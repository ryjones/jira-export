<Action id="51711" issue="34278" author="spivachuk" type="comment" created="2018-10-03 18:04:18.0" updateauthor="spivachuk" updated="2018-10-03 18:04:18.0"> <body><! CDATA *Problem reason:* - {{HasActionQueue._cancel}} method was called from {{LedgerManager._cancel_request_ledger_statuses_and_consistency_proofs}} incorrectly. The ID of the action which must be canceled was passed to {{HasActionQueue._cancel}} as the sole positional argument. Thus it was taken as the first parameter of the formal parameter list which is {{Callable}} rather than the concrete action ID.  *Changes:* - Corrected the test of cancellation of scheduled re-asking for ledger statuses and maximal consistency proofs on reception of sufficient count of messages. - Fixed cancellation of scheduled re-asking for ledger statuses and maximal consistency proofs in {{LedgerManager._cancel_request_ledger_statuses_and_consistency_proofs}} method.  *PRs:* -  https://github.com/hyperledger/indy-plenum/pull/933  -  https://github.com/hyperledger/indy-node/pull/961   *Version:* - indy-node 1.6.621-master - indy-plenum 1.6.554-master  *Risk factors:* - Nothing is expected.  *Risk:* - Low  *Covered with tests:* - {{test_cancel_request_cp_and_ls_after_catchup}}  ></body> </Action>
<Action id="51767" issue="34278" author="zhigunenko.dsr" type="comment" created="2018-10-04 15:20:07.0" updateauthor="zhigunenko.dsr" updated="2018-10-04 16:08:52.0"> <body><! CDATA *Environment:* indy-node 1.6.620  *Step to Reproduce:* 1. Install pool of 4 nodes. 2. Run load test for ~5k NYMs. 3. Add 5th node. 4. Run load test for ~5k NYMs. 5. Restart 3st node to change the primary (at that moment) 6. Stop 3st node. 7. Run load test for ~25k NYMs. 8. Add 6th node (without start) 9. Start service at 6th node and start 3st node simultaneously to check their catchup under load.  *Actual Results:* After start nodes 3 and 6 didn't complete catchup and so didn't complete primary propogation view change. Catchup runs once and cannot finished. During first 5 minutes Node6 received only one LedgerStatus and one Consistency Proof.  view_no = 2 before step 5 view_no = 3 right after step 6 view_no = 5 before step 9  Node 6 has been added in 2018-10-04 14:06:28  *Additional info:* Logs are in _indy-1740-recheck/_  ></body> </Action>
<Action id="51795" issue="34278" author="spivachuk" type="comment" created="2018-10-05 11:18:15.0" updateauthor="spivachuk" updated="2018-10-05 11:20:42.0"> <body><! CDATA As we can see in logs, *under load there were issues with connections between nodes in the Docker pool*. This hindered the new node (*Node6*) and the restarted node (*Node3*) from catch-up completion:  Here are the moments of connections establishment on *Node6*: {code:java} 2018-10-04 14:07:06.074000 | Node6:- | INFO | looper.py | Starting up indy-node 2018-10-04 14:07:28.173000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node2 2018-10-04 14:10:51.111000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node4 2018-10-04 14:14:43.281000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node1 2018-10-04 14:15:58.011000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node5 {code} Here are the moments of connections establishment on *Node3* after restart: {code:java} 2018-10-04 14:06:48.006000 | Node3:- | INFO | looper.py | Starting up indy-node 2018-10-04 14:07:18.967000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node2 2018-10-04 14:07:33.915000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node5 2018-10-04 14:09:37.851000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node1 2018-10-04 14:09:37.851000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node4 {code} Please retest the scenario on AWS pool.  ></body> </Action>
<Action id="51798" issue="34278" author="zhigunenko.dsr" type="comment" created="2018-10-05 12:43:27.0" updateauthor="zhigunenko.dsr" updated="2018-10-05 12:43:27.0"> <body><! CDATA *Environment:* indy-node                  1.6.623  *Step to Validate:* 1. Install pool of 4 nodes on AWS. 2. Run load test for ~5k NYMs. 3. Add 5th node. 4. Run load test for ~5k NYMs. 5. Restart 2st node to change the primary (at that moment) 6. Stop 2st node. 7. Run load test for ~25k NYMs. 8. Add 6th node (without start) 9. Start service at 6th node and start 2st node simultaneously to check their catchup under load.  *Actual Results:* Both nodes finished their catchup successfully.  ></body> </Action>
