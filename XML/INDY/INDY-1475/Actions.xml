<Action id="47357" issue="31860" author="esplinr" type="comment" body="This information will help us refine the minimum hardware requirements for validator nodes run by network stewards." created="2018-07-16 14:14:15.0" updateauthor="esplinr" updated="2018-07-16 14:14:15.0"/>
<Action id="48356" issue="31860" author="sergey.khoroshavin" type="comment" created="2018-08-03 12:32:18.0" updateauthor="sergey.khoroshavin" updated="2018-08-03 12:36:45.0"> <body><! CDATA I've attached some data gathered from 25-node pool under load of ~10 writes (nym, attrib, schema, claim_def) and 100 reads per second from 125 clients in total. Timestamps are in UTC. Revocations were not tested because they were causing out of memory errors very quickly, this is already being investigated in INDY-1546. Most interesting moments were (time is approximate): - 16:10 - required load parameters achieved - 16:30 - something happened in pool leading to 2-fold increase in amount of incoming traffic, although there was almost no impact on ordering performance - 16:40 - view change as a result of manual restart of master primary node - 16:50 - pool fully recovered from view change and continued performing with required rate - 20:10 - another increase of incoming traffic, but then pool recovered - 21:10 - another view change happened, and at the same time writing load script crashed, aftewards only reads were executed (100 per second)  I believe some number require explanation to avoid confusion: 1) request_count_per_second are counted for all instances, so given that there are 9(f+1) total instances in this pool real request number should be divided by 9 2) despite large number of read per second only 4 incoming client requests are seen - this is expected, because reads are spread across all (25) nodes in pool, 100/25 = 4  ></body> </Action>
<Action id="48358" issue="31860" author="sergey.khoroshavin" type="comment" created="2018-08-03 13:05:03.0" updateauthor="sergey.khoroshavin" updated="2018-08-03 13:10:43.0"> <body><! CDATA Now to the topic of  recommendations for network node resourcing. 1) looking at average message sizes during steady states it can be seen that node-to-node messages average to a little more than 3kB, and client-to-node message a little less than 2 kB. Given that currently we have node queue of 20000 messages, client queue of 3000 and it seems like ZMQ is keeping separate queue for each connection it can be seen that node queues can require up to 3.5 * 20000 * 24 * 2(in/out) ~ 3.2 Gb of RAM, client queues (if we allow 1000 connected clients) 2.0 * 3000 * 1000 * 2 = 11 Gb of RAM. Besides this there are also different internal queues in node which can also take significant amount of RAM 2) however sometimes node-to-node messages max out to about 700 Kb (and there is already PR to reduce this limit to 128 Kb), and during testing with heavier load I've seen average 500 Kb per message sustained for tens of minutes, which raises potential RAM requirements for node queues to 110 Gb (with PR merged). Also some client messages were as large as 10 Kb, which means that potential RAM requirements for client queues can be up to 55 Gb 3) on the other hand looking at real node-to-node traffic (~1 Mb per second) during moderate load it's unlikely that queues can grow that large in short amount of time (it would take them more than 30 hours to max out at 110 Gb). Also we have quota of 100 messages processed per one looper run, and it can be seen (from metrics.log) that most of time queues were under 100 messages  Given all that it can be seen that node shouldn't take huge amount of RAM during normal operation, but in some cases can potentially require more than *165 Gb*. I would recommend to *increase minimum RAM requirement to 32 Gb*, create 25-node test pool with that amount of RAM on each node and run DDoS scenario to see if can handle such load. Also further research should be done to see if we can reduce queues size without major impact on performance.   ></body> </Action>
