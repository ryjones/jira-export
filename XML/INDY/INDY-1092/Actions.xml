<Action id="39107" issue="26877" author="vladimirwork" type="comment" created="2018-01-25 14:57:33.0" updateauthor="vladimirwork" updated="2018-01-29 15:43:32.0"> <body><! CDATA that view is changed on performance degradation; - run load script on primary node (in docker for faster node degradation): 1 node of 4 tries to  make view change due to primary performance degradation only so unable to change primary this way (docker logs  ^docker_no_view_change_due_to_degradation.tar.gz ) - run load script on separate client (for continuous equal load for the whole pool): unable to force view change this way - run cpuburn on primary: unable to force view change this way  that view is changed on disconnections of primary; - works properly after many view changes done on long lived pool: ok - the next possible primaries (1, 2) are stopped/demoted (more than 4 nodes are needed): ok - add/demote/promote nodes of the pool (AWS logs  ^AWS_the_same_primary_for_both_instances_on_not_demoted_nodes.7z ):  {noformat} 2018-01-24 15:34:59,386 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node1 changed to view 1, will start catchup now 2018-01-24 15:34:59,459 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node1:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is  (0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')  2018-01-24 15:34:59,460 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node1:1 declares view change 1 as completed for instance 1, new primary is Node4:1, ledger info is  (0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')   2018-01-24 14:33:43,092 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node2 changed to view 1, will start catchup now 2018-01-24 14:33:43,165 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node2:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is  (0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')  2018-01-24 15:34:59,340 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node2:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is  (0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')   2018-01-24 14:33:43,085 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node3 changed to view 1, will start catchup now 2018-01-24 14:33:43,174 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node3:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is  (0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')  2018-01-24 14:33:45,094 | INFO     | view_changer.py      (380) | sendInstanceChange | VIEW CHANGE: Node3 sending an instance change with view_no 2 since Primary of master protocol instance disconnected 2018-01-24 15:34:59,338 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node3:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is  (0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')   2018-01-24 14:33:43,088 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node4 changed to view 1, will start catchup now 2018-01-24 14:33:43,177 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node4:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is  (0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')  2018-01-24 15:34:59,313 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node4:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is  (0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')   2018-01-24 15:35:54,923 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node5 changed to view 1, will start catchup now 2018-01-24 15:35:55,048 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node5:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is  (0, 9, '8MMN1HuP2XrkiCubNXk9Cqw685UUCJL93SGFksaq1gEd'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')  2018-01-24 15:35:55,049 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node5:1 declares view change 1 as completed for instance 1, new primary is Node4:1, ledger info is  (0, 9, '8MMN1HuP2XrkiCubNXk9Cqw685UUCJL93SGFksaq1gEd'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')   {noformat}  that view change works properly with random delays; - added `sleep` in  process_vchd_msg of ~/plenum/server/view_change/view_changer.py: 1st node can't reselect primary when 4th (previous primary) is restarting/disconnecting but 2nd and 3rd select 3rd as primary (and pool can'r write NYMs (can read them only) in this case, see logs  ^issue_with_view_changer_timeout.tar.gz )  that view change works properly when some nodes in the pool are down; - node 1(2) of 4(7) is shutted down: unable to rotate primary between the remaining at n-f: clarification is needed - nodes 2(3) of 4(7) is shutted down: unable to elect primary at less than n-f: ok  that view change happens when primary behaves maliciously (sends incorrect data for example); - added malformed data in create3PCBatch of ~/plenum/server/replica.py: nodes reselect primary with "since Primary of master protocol instance degraded the performance" message during NYMs sending, ok  we need to test it also in a quite large and distributed pool - cases with view change caused by primary disconnecions (with some switched off nodes) on QA Live Pool: ok  ></body> </Action>
<Action id="39426" issue="26877" author="ashcherbakov" type="comment" created="2018-01-31 13:52:00.0" updateauthor="ashcherbakov" updated="2018-01-31 13:52:00.0"> <body><! CDATA As for `that view is changed on performance degradation;` items:  we have INDY-34 which may be a cause of the issues.  ></body> </Action>
<Action id="39433" issue="26877" author="vladimirwork" type="comment" body="Separate tickets for found issues are created and linked to this ticket. Also exploratory INDY-1115 is created." created="2018-01-31 14:45:59.0" updateauthor="vladimirwork" updated="2018-01-31 14:45:59.0"/>
