<Action id="42243" issue="28264" author="anikitindsr" type="comment" created="2018-03-28 11:26:24.0" updateauthor="anikitindsr" updated="2018-03-28 11:26:24.0"> <body><! CDATA Problem reason:  - When n-f node start view change (on primary loss) pool can lose quorum  Changes:  - added resend InstanceChange messages  PR: - https://github.com/hyperledger/indy-plenum/pull/594  Version: - 1.3.354-master  Risk: - Low  Covered with tests: - https://github.com/hyperledger/indy-plenum/blob/1.2.299-master/plenum/test/view_change/test_resend_instance_change_messages.py  Recommendations for QA - Set up pool of 7 nodes - disconnect not primary for all replica node - disconnect primary - check, that view change completed - check that pool works normally  ></body> </Action>
<Action id="42314" issue="28264" author="zhigunenko.dsr" type="comment" created="2018-03-29 13:47:43.0" updateauthor="zhigunenko.dsr" updated="2018-03-29 13:48:47.0"> <body><! CDATA *Environment:* indy-node 1.3.354 indy-plenum 1.2.299 libindy 1.3.1~439  *Case 1:* Disconnect primary node only (7 nodes in pool) *Actual results:* view change completed, pool works normally, node reconnected successfully  *Case 2:* Disconnect the last of backup instance and then disconnect primary node (7 nodes in pool) *Actual results:* view change completed, pool works normally, node reconnected successfully *Additional info:*  *Case 3:* Disconnect node which following up the last of backup instance and then disconnect primary node (7 nodes in pool) *Actual results:* view change completed, pool works normally, node reconnected successfully  *Case 4:* 1) create pool with 15 nodes 2) apply  ^suppressor.sh  for node1, node2, node3, node7, node13 (started with $S2=6sec delay, not in the single moment) 3) start load test 100threads*100txns *Actual results:* Pool collapsed after 1164 txns, load test failed with 307 error. Consensus was not reached after suppressed nodes reconnection   Logs|https://drive.google.com/open?id=13ifBNFC0F3pgl6jLNxKHqCTKp_e4yW4S   ></body> </Action>
<Action id="42374" issue="28264" author="zhigunenko.dsr" type="comment" created="2018-03-30 08:09:53.0" updateauthor="zhigunenko.dsr" updated="2018-03-30 08:09:53.0"> <body><! CDATA *Case 5:* Disconnect node which following up the last of backup instance and then disconnect primary node (15 nodes in pool) *Actual results:* view change completed, pool works normally, but reconnected nodes are both unreachable ^1210-2.7z   ></body> </Action>
<Action id="42377" issue="28264" author="anikitindsr" type="comment" created="2018-03-30 11:17:25.0" updateauthor="anikitindsr" updated="2018-03-30 11:17:25.0"> <body><! CDATA Recheck case 4 and case 5 without "docker network disconnect". "docker network disconnect" will recreate network interface, that's not "cleanly". Instead of docker way, try to use "ifconfig <network name> down" for disconnect emulating. Steps: - run containers in "privileged" mode (--privileged) - inside selected node's containers (which should be disconnected) run : - install "net-utils" package - ifconfig <network name> down (usually network name is "eth0")  ></body> </Action>
<Action id="42438" issue="28264" author="zhigunenko.dsr" type="comment" created="2018-04-02 14:34:26.0" updateauthor="zhigunenko.dsr" updated="2018-04-02 14:34:26.0"> <body><! CDATA *Steps to reproduce (case4):* 1) create pool with 15 nodes (privileged mode) 2) apply  ^suppressor.sh  (updated) for node1, node2, node3, node7, node13 (started with $S2=1.97..2.02 sec delay, not in the single moment) 3) start load test 100threads*100txns *Actual results:* Pool finished with 5072 txns, but node1, node2, node3 have only 592 txns. All nodes shows each of others as reachable. After connection reestablishing these nodes couldn't catch up with the pool (without or with node restarting)   Logs|https://drive.google.com/file/d/1lGtyB4X3tub3STMhjFGwY055RwYVDotT/view?usp=sharing   ></body> </Action>
<Action id="42504" issue="28264" author="zhigunenko.dsr" type="comment" created="2018-04-04 10:26:25.0" updateauthor="zhigunenko.dsr" updated="2018-04-04 10:26:25.0"> <body><! CDATA *Reason for closing:* Basic case has been fixed. Additional case has been moved in INDY-1258  ></body> </Action>
