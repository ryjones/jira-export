<Action id="47706" issue="31917" author="spivachuk" type="comment" created="2018-07-24 10:49:03.0" updateauthor="spivachuk" updated="2018-07-24 11:01:39.0"> <body><! CDATA _Below please find our discussion of the message recorder / replayer tool with_  ~ckochenower  _and_  ~devin-fisher  _(copied from berdyev6 channel in Slack):_   ~spivachuk : @corin.kochenower @dfisher @lovesh I've sent you our questions on the message recorder / replayer tool by email. I'm also duplicating these questions here to share them with the team. Could you please clarify them? - Can nscapture be used for getting a record in case indy-node services was terminated abnormally, for example, in case of abrupt power-off or in case of out-of-memory? - Can nscapture be used for getting a record of multiple node sessions (start-stop / start-terminate cycles)? If yes, how does nsreplay replay them: as one session or as multiple sessions? - How to reproduce an issue occurred on a pool with a big count of nodes when it is unknown which node caused the issue or the issue was not caused by a particular node? In other words, how to reproduce the issue on a whole pool? - How to reproduce an issue on a node from a pool working under load which revealed after a significant period after start (for example, a week), taking into account that we do not know when the issue reason originated? Do we need to replay the captured record during the same period?  In general, most of issues that we face occur on a large pool (25 nodes) during load testing running for hours or days and writing multiple thousands (and sometimes millions) of transaction to the ledger. When an issue occurs, it is unknown / unclear at what time and on what node(s) it occurred. Replaying all the records of all 25 nodes running for days can take months. How can the message recorder / replayer tool be efficiently used in such cases? Or is it assumed that this tool is used for dealing with bugs occurred only on not large pools and after not long period after start? Can you please advise what is the expected usage of the recorder / replayer tool?   ~ckochenower : Very good questions @nikita.spivachuk.  Q: - Can nscapture be used for getting a record in case indy-node services was terminated abnormally, for example, in case of abrupt power-off or in case of out-of-memory?  A: It sounds like you would like to debug (root cause analysis) "abend" and "abrupt power-off" scenarios with nsreplay. Debugging the "abend" scenario will be possible, if we can figure out how to get the operating system to call nscapture when indy-node is terminated abnormally. Perhaps the systemd script can be made smart enough to know to call nscapture before subsequent startup.  I'm not sure the OS can help us call nscapture in an "abrupt" power-off scenario.  Q: Can nscapture be used for getting a record of multiple node sessions (start-stop / start-terminate cycles)? If yes, how does nsreplay replay them: as one session or as multiple sessions?  A: Yes. Currently, nsreplay requires the indy-node service to be stopped before calling it. The recording persists across restarts. Every subsequent nscapture archive will be a superset of the previous ones. In other words, the last nscapture archive created will contain all messages from the time STACK_COMPANION = 1 was added to /etc/indy/indy_config.py  Q: How to reproduce an issue occurred on a pool with a big count of nodes when it is unknown which node caused the issue or the issue was not caused by a particular node? In other words, how to reproduce the issue on a whole pool?  A: Each node in the cluster is capable of recording it's own messages. Adding features like "skip idle time", "replay multiplier (sped up replay)", "replay from a snapshot", "replay the last N transactions/messages" will hopefully reduce months to days, hours, or minutes.  Q: How to reproduce an issue on a node from a pool working under load which revealed after a significant period after start (for example, a week), taking into account that we do not know when the issue reason originated? Do we need to replay the captured record during the same period?  A: The answer to the previous question also applies to this question.  The "capture state, then replay to find root cause of a problem" approach, seems to be analogous to explaining symptoms to a doctor and asking the doctor to diagnose the problem.  Hopefully, we are building in self-diagnosing and self-healing features (I think we are) for simple failures/faults.  The capture and replay tools seem more appropriate for the very difficult to diagnose problems.  If a patient has heart arrhythmia, a doctor will perform either a chemical stress test or a physical stress test to hopefully cause a "cardiac event" in a controlled environment where the patient can be monitored and quickly treated (QA stress and system tests). However, conditions like heart arrhythmia aren't easily triggered by these conventional "stress tests" and the doctor has the patient wear a "recorder" that monitors his/her vitals constantly day and night. Once the patient experiences a "cardiac event", the recording is then shared with the doctor and the patient helps the doctor pinpoint when the event happened.  I hope our patient, indy-node, will tell us when these sometimes hard to diagnose problems happen. Then we can replay that part of the recording and ignore the rest.  In other words, indy-node needs to experience an anomaly, we make indy-node as smart as we can to recognize symptoms associated the anomaly (abnormal heart beat, blood pressure, temperature, blood oxygen level), enable the recorder, and then have indy-node tell us when it experiences the symptoms again.  Perhaps we need to record indy-node "vitals" to help us pinpoint anomalies?  @dfisher - Do my answers *^* align with your thoughts?   ~devin-fisher : Corin thoughts are good. But I have a few of my own.  Frist, nscapture is a simple script that captures into an archive file the state of the node. Should be run when the node is stoped. The function of the script is mostly to discover what files need to be captured and to run the required tar function to archive them. nscapture captures only one node's data. Additionally, it can only capture data that is on the disk. So, if the recorder was not activated with the config setting, a replay will not be possible. Although, nscaputure can grab the logs, configuration and data files.  1) The recorder will messages and signals for a long as the node is running. The signals have to be trapable. I'm not sure (Lovesh would know more), but I'm guessing that only SIGTERM signal is traped. On the replay, the SIGTERM will be simulated (see the below info for more detail). But It is likely not to be a very interesting replay if the messages don't lead to the SIGTERM.  If the signal is not trapable, like SIGKIL from the OM killer, the recorder will record messages up to the point of being killed by the OS. The replay will likely not be of much use unless the sequence of messages lead to the conditions that caused the OS to kill the process. Similarly, in the abrupt power-off, the recorder will record all messages up to the power loss (assuming that the data had been flushed to disk in the recorder).  2) The recorder does record the node sessions. There is a file called 'start_times' that records when these events happen. The start and stops are simulated by removing the node from the lopper. And a new node object is created and added back the looper and execution is resumed. This process is time to happen in a similar timing as was seen by the recorder.  3) Each node is recorded individually and so the number of nodes does not change much. The larger the cluster will cause more messages to be recorded since the size of the pool affect the number of redundant messages sent and also the number of replicas that are run. Running a recording on a large pool should be totally possible.  If all nodes are recording, the choice of which node to replay can be decided after the fault is detected (or pool failure).  4) Currently, the recorder and replayer are limited in their ability to replay long recordings. We had hoped to find a way to snapshot the recording but that proved too difficult for our small team.  Generally, if you think that the only issue we will be diagnosting will take days to weeks to trigger, then the recorder and replayer in their current form will not be that useful. The recorder and replayer should work fine for pools with a large number of nodes and for heavy load scenarios.   ~ashcherbakov : Thanks for the answers. Actually for me it's still not clear how to use the tools for large number of nodes and heavy load scenarios. {quote}If all nodes are recording, the choice of which node to replay can be decided after the fault is detected (or pool failure). {quote} This is usually the main unknown we have to detect. And it looks like it's hard to do it with Recorder/Replayer in the current state...   ~spivachuk : @corin.kochenower @dfisher Thank you for the answers.  Let me share our thoughts about the features intended for reducing replay time. The checkpoints feature would be useful in cases when we know the time frame when the issue reason originated.  But the replay speed multiplier feature does not seem to be useful because a significant part of incoming messages are sent during a dialog of the node with other nodes. So the speed of the message workflow depends on the node performance, among other things. If the replay is sped up, the order of events may be broken.  ></body> </Action>
<Action id="47711" issue="31917" author="spivachuk" type="comment" created="2018-07-24 12:26:01.0" updateauthor="spivachuk" updated="2018-07-24 13:03:49.0"> <body><! CDATA Completed the steps listed in the ticket description. Found several issues with the message recorder / replayer tool and discussed them with Berdyaev6 team in Slack.  Provided our feedback onÂ the message recorder / replayer tool with a number of questions and discussed them with Berdyaev6 team (this discussion has been copied from Slack to the previous comment to this ticket).  ></body> </Action>
<Action id="47713" issue="31917" author="anikitindsr" type="comment" created="2018-07-24 12:42:06.0" updateauthor="anikitindsr" updated="2018-07-24 12:42:06.0"> <body><! CDATA As for me, i ran nsreplay for reproducing problems in INDY-1470. Some problems was described in comments for INDY-1470: https://jira.hyperledger.org/browse/INDY-1470?focusedCommentId=47616&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-47616 and https://jira.hyperledger.org/browse/INDY-1470?focusedCommentId=47619&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-47619  Also,Â Â i were chatting with team in the berdyaev6 channel in the slack.  ></body> </Action>
<Action id="47771" issue="31917" author="ashcherbakov" type="comment" created="2018-07-25 07:17:19.0" updateauthor="ashcherbakov" updated="2018-07-25 07:17:19.0"> <body><! CDATA My feedback: 1) I don't see how Replayer (even with snapshot feature) can be the only tool to find the issue. We simply don't know when and on what nodes the issues happened until run the Replayer. However, Replayer (with snapshot feature) can be an additional tool used *after* initial analysis of logs and validator-info input. 2) I don't understand how can I easily analyse a problem (on my dev machine) if it occurred on all 25 nodes. I need to run Replayer at least 25 time (for each node), and it can be that I will need to run it multiple times for each node.  3) The requirement for Steward machine will be increased, and can be about 32 GB of RAM. In order to run Replayer on my dev machine and not fail with OutOfMemory, I will have to have even more than 32 GB RAM. 4) I'm not sure that snapshot feature can be easily implemented.  So, I think that Recorder/Replayer can be useful tools in some cases (especially in combination with chaos testing). But I'm not sure that it can be the primary tool for analysis of issues on large pools and heavy scenarios. However, it can be used in addition (after initial analysis of logs and validator info output)  ></body> </Action>
