<Action id="48714" issue="32731" author="vladimirwork" type="comment" body="All logs/journals/node_info files are in evernymr33:/home/ev/logs/1574.tar.gz" created="2018-08-10 13:19:17.0" updateauthor="vladimirwork" updated="2018-08-10 13:19:17.0"/>
<Action id="50242" issue="32731" author="derashe" type="comment" created="2018-09-11 11:41:14.0" updateauthor="derashe" updated="2018-09-11 11:41:14.0"> <body><! CDATA Problem reason: * Pool stopped writing  Research: * Pool doesn't write because it have repeatable restarts. Because nodes raises and fall because of OOM exception * Nodes became restart because of OOM, that was caused by stashing large amount of requests and affiliated client/node traffic. So node even can't exchange it's LEDGER_STATUS messages * Stashing of requests were caused by view_change, which was called because of primary degraded (case with throughtput master == 0, backup == 0.1) * We also can define problem about long view_change (~ 1 hour), which will lasts untill reason "could not complete in time", because pool cannot collect quorum of view_change_done because of   Conclusion: * We can distinguish two problems: ** false positive view_changes ** OOM error * These problems should be fixed is scope of: ** https://jira.hyperledger.org/browse/INDY-1639 ** https://jira.hyperledger.org/browse/INDY-1681      Recomendadtion for QA: * Retest this case when above PRs will be merged  ></body> </Action>
<Action id="51531" issue="32731" author="vladimirwork" type="comment" created="2018-10-02 15:22:07.0" updateauthor="vladimirwork" updated="2018-10-02 15:22:07.0"> <body><! CDATA Build Info: indy-node 1.6.619  Steps to Reproduce: 0. Preload pool with 300k txns. 1. Run 20 writing and 200 reading txns/sec load test. 2. Check validator-info/logs/metrics.  Actual Results: Pool stopped writing due to OOM (see screenshot).  !INDY-1574_OOM.PNG|thumbnail!   ></body> </Action>
<Action id="51614" issue="32731" author="vladimirwork" type="comment" body="All logs and validator-infos are in ev@evernymr33:logs/1574_NEW.tar.gz. Metrics with request_sizes:  !INDY-1574_REQ_QUEUE_SIZES.PNG|thumbnail! " created="2018-10-03 09:50:11.0" updateauthor="vladimirwork" updated="2018-10-03 09:50:11.0"/>
<Action id="51846" issue="32731" author="derashe" type="comment" created="2018-10-08 12:59:28.0" updateauthor="derashe" updated="2018-10-08 12:59:28.0"> <body><! CDATA Results of the last load testing: * Pool went down because of OOM and could not restore consensus. * Memory started to grow because one of backup primary nodes restarted and could not work as a primary (seems to be a known issue) * This node(a) was restarted because of connection problems with one(b) of another backup primary nodes, which led to OOM just on this node(a). (no problems have seen for other nodes in pool). But in the same time replica with primary node(b) did not turned off on node(a), what is strange. After connection restored, node(a) cannot write any txns in this replica.  ></body> </Action>
