<Action id="50959" issue="33502" author="toktar" type="comment" created="2018-09-21 15:15:59.0" updateauthor="toktar" updated="2018-09-21 15:15:59.0"> <body><! CDATA The implementation of this task allows a situation when the nodes will have a different number of instances. In this case when the instance contains an insufficient number of replicas for the transactions ordering, there will be an error out_of_memory. This problem will be fixed in  tasks INDY-1682,  INDY-1683.  ></body> </Action>
<Action id="51396" issue="33502" author="spivachuk" type="comment" created="2018-09-28 21:43:10.0" updateauthor="spivachuk" updated="2018-09-28 21:44:04.0"> <body><! CDATA *Changes:* - Implemented removal of a backup replica on its primary loss. - Added tests for removal of a backup replica on its primary loss. - Reworked the tests of the replica removal mechanism. - Fixed an issue with lack of freeing ordered requests on a replica removal.  *PRs:* - https://github.com/hyperledger/indy-plenum/pull/925 - https://github.com/hyperledger/indy-plenum/pull/931 - https://github.com/hyperledger/indy-node/pull/958  *Version:* - indy-node 1.6.618-master - indy-plenum 1.6.553-master  ></body> </Action>
<Action id="51789" issue="33502" author="nataliadracheva" type="comment" created="2018-10-05 08:06:08.0" updateauthor="nataliadracheva" updated="2018-10-05 08:06:27.0"> <body><! CDATA *Scenario 1:* *Build version:* indy-node: 1.6.618 indy-plenum: 1.6.553 *Test description:* Verify if pool keeps writing after all replicas switching off and forced View Change. *Steps to Validate:* 1. Run perf_processes.py from 1 AWS agent with the following parameters: {code:java} perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 20 -l 10 -y one -k nym {code} 2. 10 mins later ssh to Node2 and do 'sudo systemctl stop indy-node'. 3. Do the same to all node up to Node8 (including) every 10 mins. => Instances are turned off, pool keeps writing, no VC by master degraded, no OOM. 4. Turn on all nodes. 5. Turn off Node1. ~/logs/1681/618-553 *Expected results:* VC happens, all replicas start, pool keeps writing, all replicas catch up data. *Actual results:* VC happens, all replicas start, pool keeps writing, all replicas catch up data. (/) *Additional info:* ~/logs/1681/618-553  *Scenario 2:* *Build version:* indy-node: 1.6.622 indy-plenum: 1.6.554 *Test description:*  *Steps to Validate:* 1. Run perf_processes.py from 1 AWS agent with the following parameters: {code:java} perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 20 -l 10 -y one -k nym {code} => Instance8 is removed automatically as f = 7 now. 2. Turn off node9. 3. Wait 1.5 minutes. *Expected results:* No errors in logs, pool keeps writing, all other nodes are reachable. *Actual results:* No errors in logs, pool keeps writing, all other nodes are reachable. (/) *Additional info:* ~/logs/1681/additional  ></body> </Action>
