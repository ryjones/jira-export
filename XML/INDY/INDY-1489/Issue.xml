<Issue id="31956" key="INDY-1489" number="1489" project="10303" reporter="ozheregelya" assignee="toktar" creator="ozheregelya" type="10004" summary="Pool stopped working under load without view changes" priority="3" resolution="10000" status="10001" created="2018-07-17 11:08:39.0" updated="2019-03-29 20:32:26.0" resolutiondate="2019-03-29 20:32:26.0" votes="0" watches="2" workflowId="31987"> <environment><! CDATA AWS pool of 25 nodes (QA Live), default config indy-node 1.4.504 libindy 1.5.0~613  ></environment> <description><! CDATA *Steps to Reproduce:* 1. Restore the pool from backup  https://s3-us-west-2.amazonaws.com/qanodelogs/backups/qa_large/node1.7z 3. Start the load test with all types of writing txns.  *Actual Results:* Pool was writing during ~5 hours with throughput ~13txns/sec. 9 of 25 nodes were lagged. No View Changes were happened. Domain ledger sizes: Node4,9,10,17,21,24: 752253 Node5: 732253 Node7: 792253 Node12,23: 782253 Node25: 675282  *Expected Results:* Pool should work.  Logs:  https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1489   *Case 2:* Looks like the same issue:  https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1489-2  The only difference is that there are more than f+1 node is out of sync (Only 13 of 25 nodes have the same amount of domain txns). UPD: case 2 was reproduced not with default config: {code:java} MAX_CONNECTED_CLIENTS_NUM = 250 # connections limit MIN_STACK_RESTART_TIMEOUT = 30 # seconds MAX_STACK_RESTART_TIME_DEVIATION = 10 # seconds TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True CLIENT_STACK_RESTART_ENABLED = True{code}  ></description> </Issue>
