<Action id="48834" issue="32663" author="derashe" type="comment" created="2018-08-14 08:05:04.0" updateauthor="derashe" updated="2018-08-14 08:05:04.0"> <body><! CDATA Problem reason:  Pool stopped write txns under heavy load   Research: * Pool started lagging because of multiple view changes (50 view changes in 12 hours) ** view changes were called by "master degraded" reason, which was initiated because of low master throughput. In this particullar case master throughtput was low, because of uneven txn handling by different replicas. This can happen when backup replicas slow ordering txns for some time, and then trying to order plenty of them. This leads to "thoughtput spikes" which can cause view change. ** The reason why view change slow node is that sometimes (50% probability) view change can lasts for 5 minutes, stopping any other activity on node (this case described in https://jira.hyperledger.org/browse/INDY-1473).   ** The resolving for problem of "throughput spikes" is in progress now and will be solved in scope of another tickets (as https://jira.hyperledger.org/browse/INDY-1582). ** To lower the network load we can discard any client messages while view_change is in progress (https://jira.hyperledger.org/browse/INDY-1564). * There are 5 nodes that had out of memory error.  ** Every of these nodes had slowed ordering txns and got overflowed thier network queues ** 2 of these nodes slowed and started cathing up txns. And as a result of a continuing heavy load, nodes started making more catchups (the longest took ~1 hour) which leads to out of memory *** Catchup took so long because of node-to-node network stack crowded. On applied image (Figure_1), where "y axis" is timeline and "x axis" is requests we need to catch up, we can see that catchup replies came non sequentially and with a huge delays. Problem with overloaded node-to-node stack must be resolved in https://jira.hyperledger.org/browse/INDY-1472.  Also, problem with out of memory while catching up a big ledgers will be discovered in scope of https://jira.hyperledger.org/browse/INDY-1557.  ** 3 of these nodes lagged and could not make any view_change because of incorrect throughtput measurement and heavy network load which leads to out of memory error. There are a few tickets that must improve network stability: https://jira.hyperledger.org/browse/INDY-1549,  https://jira.hyperledger.org/browse/INDY-1569.     Recommendations for QA:   Retest pool with this or similar config and load after fixes for throughtput spikes.  ></body> </Action>
