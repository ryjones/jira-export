<Action id="31657" issue="21334" author="krw910" type="comment" body="It looks like the issue is the node wrote the entry to its own ledger twice. What could cause that to happen? We believe it was due to using the &quot;force=True&quot; parameter on the upgrade transaction." created="2017-10-05 21:31:51.0" updateauthor="krw910" updated="2017-10-09 04:38:21.0"/>
<Action id="31957" issue="21334" author="mgbailey" type="comment" created="2017-10-09 20:54:33.0" updateauthor="mgbailey" updated="2017-10-09 20:54:33.0"> <body><! CDATA I got with the steward and tried these steps: # stop the sovrin-node and sovrin-node-control services # delete the config_transactions directory # start the sovrin-node service  The theory was that this would rebuild the config ledger, and it would match the ledgers in the pool.  It didn't work.  It rebuilt the ledger exactly as it appears above, which does not match the config ledger of any other node in the pool.  Digging deeper, I see in journalctl that the node "upgraded" to 1.1.33 from 1.1.37.  So now I have a new theory.  When the node is coming up with an empty config ledger, it gets a transaction from somewhere (either something in sovrin-node-control or via catchup from another node) to upgrade to 1.1.33. This was an old transaction that was supposed to execute back in early September. The node is already at 1.1.37, so this is actually a downgrade at this point.  The node then does the downgrade, and reverts back to 1.1.37, and somewhere along the way a second copy of the 1.1.33 upgrade transaction is written to the config ledger, causing the mismatch and the lack of ability to catch up on all ledgers.  I am attaching the journalctl and logs from today.  The downgrade is at 12:10.  Here is another note.  The downgrade only happened once.  The duplicate ledger entries happened on other restarts as well, so this does not explain everything.  The difference when the downgrade occurred was that the sovrin-node-control service was restarted as well.  ></body> </Action>
<Action id="33208" issue="21334" author="dsurnin" type="comment" created="2017-10-26 10:21:41.0" updateauthor="dsurnin" updated="2017-10-26 10:21:41.0"> <body><! CDATA  ~mgbailey   The issue with downgrade to 1.1.33 was fixed in  INDY-869|https://jira.hyperledger.org/browse/INDY-869 .   However the main issue of the Bug still requires research.      ~krw910   ~mgbailey   It requires a lot time to research. Is it still a high priority bug?  Also we did several important fixes to forced upgrade recently and probably it should be retested with the new version before continue.  Could you please prioritize it against the rest tasks?  ></body> </Action>
<Action id="33312" issue="21334" author="mgbailey" type="comment" body="This issue happened in the config ledger, so it was not part of an upgrade script. Since we are continuing to use force=True for upgrades, this continues to be a concern.  " created="2017-10-27 15:45:13.0" updateauthor="mgbailey" updated="2017-10-27 15:45:13.0"/>
<Action id="34676" issue="21334" author="ashcherbakov" type="comment" created="2017-11-16 15:37:48.0" updateauthor="ashcherbakov" updated="2017-11-16 15:37:48.0"> <body><! CDATA  ~mgbailey  Are you sure that config_ledger of Metis node was deleted in trying to catch it up? I can see from the log that Metis always has 2 txns in its Config ledger, so it looks like this is the two duplicated txns that were there from the very beginning. I think deleting the config ledger should help.  ></body> </Action>
<Action id="34677" issue="21334" author="ashcherbakov" type="comment" created="2017-11-16 15:53:45.0" updateauthor="ashcherbakov" updated="2017-11-16 15:54:32.0"> <body><! CDATA  ~mgbailey  I can see some strange difference between txns on Metis and other Nodes:  Both duplicated txns on Metis have the following schedule for Metis ('7Mxn7MzXTGRbUBWwaGzpU2j7d58kSpjpfQnjoMdLV3xZ' id) # "7Mxn7MzXTGRbUBWwaGzpU2j7d58kSpjpfQnjoMdLV3xZ":"*2017-09-06T18:55:33*.555000-06:00",  while in the one and only POOL_UPGRADE txn on other Nodes we have the following for Metis: #  "7Mxn7MzXTGRbUBWwaGzpU2j7d58kSpjpfQnjoMdLV3xZ":"*2017-09-05T15:05:33*.555000-06:00"     ></body> </Action>
<Action id="34678" issue="21334" author="ashcherbakov" type="comment" body="Can it be that there were more than one POOL_UPGRADE txn? " created="2017-11-16 15:55:39.0" updateauthor="ashcherbakov" updated="2017-11-16 15:55:39.0"/>
<Action id="34679" issue="21334" author="mgbailey" type="comment" created="2017-11-16 16:32:35.0" updateauthor="mgbailey" updated="2017-11-16 16:32:35.0"> <body><! CDATA  ~ashcherbakov , I am sure that we deleted the config ledger.  That was one of the mysteries of this thing: where were these 2 transactions coming from?  I don't recall: do we see a catchup of ledger 2 happening in the logs?  If so, why do the transactions not match exactly, as you noted above?  Another odd thing: only the transaction time for metis is different, not for the other nodes. and that is for an entirely different hour, on another day.  We certainly did not post a transaction like this!  ></body> </Action>
<Action id="34714" issue="21334" author="ashcherbakov" type="comment" created="2017-11-17 13:23:00.0" updateauthor="ashcherbakov" updated="2017-11-17 13:23:20.0"> <body><! CDATA Still have no idea why a wrong POOL_UPGRADE txn (with incorrect time) appeared in Metis's config ledger twice.  But found the reason why Metis wasn't able to catch-up after Config ledger was removed (it was really removed): 1) Metis started catch-up of removed config ledger (from size 0 to size 29) 2) It caught up just fine 3) Once config ledger is caught up, but before domain ledger catch up starts, Node checks config ledger to see whether a Node needs to perform scheduled Upgrade. 4) Metis sees POOL_UPGRADE txn to version 1.1.33. Because of the problem which was fixed in INDY-869, metis schedules downgrade to 1.1.33 (it should not happen anymore because of the fix in INDY-869) 5) Metis sends NODE_UPGRADE txn for his upcoming Upgrade 6) Before Upgrade (downgrade to 1.1.33) is started for Metis, Metis starts domain ledger catch-up (from size 40801 to 43800) 7) It gets the first batch of missing domain transactions (1500). 8) Metis started applying these txns, and *downgrade to 1.1.33 happens in between this applying. Moreover, it happened in between writing a txn to ledger transaction log and the tree. It's possible since this operation is not atomic. As a result, Metis's domain ledger transaction log has 41076 txns, and Metis's domain ledger hash tree has 41075 txns*. 9) Metis re-started, and tries to restore the ledger tree from the hash store. The hash store size differs from transaction log size (41075 != 41076), so it fails. 10) Then Metis falls back to restoration of the ledger tree from transaction log. *It tries to reset the tree, but looks like doesn't do it correctly. So, the tree is restored with 41075 (from hash store) + 41076 (from txn log) = 82151 txns*. 11) When Metis starts to catch-up domain ledger, it fails, since *it assumes it has 82151, but really has only 41075*   ></body> </Action>
<Action id="34715" issue="21334" author="ashcherbakov" type="comment" created="2017-11-17 13:24:57.0" updateauthor="ashcherbakov" updated="2017-11-17 13:24:57.0"> <body><! CDATA Further steps: 1) Fix the problem with recovering of the ledger tree  2) Create a ticket for trying to make ledger add operation atomic 3) Analysing possibility of having POOL_UPGRADE txn with incorrect  time further.  ></body> </Action>
<Action id="34773" issue="21334" author="ashcherbakov" type="comment" body="Created https://jira.hyperledger.org/browse/INDY-955" created="2017-11-20 16:33:50.0" updateauthor="ashcherbakov" updated="2017-11-20 16:33:50.0"/>
<Action id="35300" issue="21334" author="ashcherbakov" type="comment" created="2017-11-21 13:09:25.0" updateauthor="ashcherbakov" updated="2017-11-21 13:09:25.0"> <body><! CDATA 1) The problem with downgrade is already fixed in INDY-869 2) Fixed the problem with ledger recovery:  - PR: https://github.com/hyperledger/indy-plenum/pull/451 3) Created a ticket in backlog for atomic operations in ledger: INDY-955 4) I still have no idea how incorrect txns (with incorrect time) appeared on Metis.  I think 1) and 2) should be enough to declare the problem fixed. Let's monitor if we face Issue 4 (incorrect POOL_UPGRADE txn times) again.  Build: - master 1.2.216  ></body> </Action>
<Action id="35781" issue="21334" author="ashcherbakov" type="comment" body=" ~mgbailey  will you be able to validate this one?" created="2017-12-04 15:37:59.0" updateauthor="ashcherbakov" updated="2017-12-04 15:37:59.0"/>
<Action id="35791" issue="21334" author="mgbailey" type="comment" body=" ~ashcherbakov  I am unable to validate this.  Metis has been removed from the ledger and destroyed." created="2017-12-04 18:12:05.0" updateauthor="mgbailey" updated="2017-12-04 18:12:49.0"/>
<Action id="35803" issue="21334" author="ozheregelya" type="comment" body=" ~ashcherbakov ,  ~mgbailey , today I tried to verify this issue on test pool, but I have some problems with catch up after sending lots of transactions. It probably may relate to INDY-911. I&apos;ll discuss this problem with team tomorrow." created="2017-12-04 19:30:27.0" updateauthor="ozheregelya" updated="2017-12-04 19:30:27.0"/>
<Action id="35907" issue="21334" author="ashcherbakov" type="comment" body=" ~ozheregelya  ok, then I think you can close the ticket once you test it." created="2017-12-06 07:52:24.0" updateauthor="ashcherbakov" updated="2017-12-06 07:52:24.0"/>
<Action id="36005" issue="21334" author="ashcherbakov" type="comment" body="As INDY-911 and INDY-960 cover the fixed issue in some sense, I suggest that we close this ticket and continue validation in the scope of INDY-911" created="2017-12-08 08:52:51.0" updateauthor="ashcherbakov" updated="2017-12-08 08:52:51.0"/>
