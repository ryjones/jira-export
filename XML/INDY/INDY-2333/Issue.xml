<Issue id="44143" key="INDY-2333" number="2333" project="10303" reporter="anikitindsr" creator="anikitindsr" type="10004" summary="Reordering after view change caused by adding new node." priority="3" status="10200" created="2020-01-27 10:50:41.0" updated="2020-01-27 10:50:41.0" votes="0" watches="1" workflowId="58044" archived="N"> <description><! CDATA There is edge-case (hard reproducible) then lagged behind node have to reorder txn with adding new node which would be selected as new primary.  In this case, `old view preprepares` will be requested and during this process other pool will finish reordering (because they don't need to real ordering) and first batch in new view will be ordered too. For now, ReAppliedInNewView internal message will not be sent and lagged node will not unstash all the 3PC message received while requesting `old_view_preprepares`.  Also, there is a problem, that lagged node doesn't know new added node and will reject all messages from it and first PrePrepare in new view too.  Possible fixes: # Make ReAppliedInNewView logic more clear. For example, this action can be added directly after applying `prev_view_prepare_certificate`. # We need to add requesting preprepare logic for case if node received not the first batch in new view (the second, the third etc..) and didn't have PrePrepare for the first batch (it would be STASH_WAITING_FIRST_BATCH_IN_VIEW reason)  There is a test that can be used for debugging (need to turn off 2 checkpoints requests sending):  plenum/test/view_change/test_vc_with_incorrect_primary_in_promote.py  ></description> </Issue>
