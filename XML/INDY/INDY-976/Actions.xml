<Action id="36609" issue="24539" author="dsurnin" type="comment" body="Kelly is going to attach some logs soon" created="2017-12-12 08:43:25.0" updateauthor="dsurnin" updated="2017-12-12 08:43:25.0"/>
<Action id="37024" issue="24539" author="dsurnin" type="comment" created="2017-12-13 07:13:09.0" updateauthor="dsurnin" updated="2017-12-13 07:13:09.0"> <body><! CDATA from dialog with Kelly  kelly.wilson  1:20 AM   I have not been able to reproduce the issue. I had one node fall out of sync with the pool during the testing. When I logged the issue the pool had the same ledger count of 6357 txns, but would not accept any new ones. I guess assign the ticket back to me in the backlog and I will have to keep an eye on it to see if it happens again. While trying to reproduce the issue I ran into another one that I am looking into. I sent 25,000 txns and the ledger wrote 25,082 txns. So it appears I ended up with 82 duplicates. I am working through that information now before I log anything.  ></body> </Action>
<Action id="37027" issue="24539" author="ashcherbakov" type="comment" created="2017-12-13 08:06:07.0" updateauthor="ashcherbakov" updated="2017-12-13 08:06:07.0"> <body><! CDATA  ~krw910   ~dsurnin  I think the issue may be already fixed in the scope of INDY-911 and INDY-960.  ></body> </Action>
<Action id="37101" issue="24539" author="sergey-shilov" type="comment" created="2017-12-14 15:41:20.0" updateauthor="sergey-shilov" updated="2017-12-14 15:41:20.0"> <body><! CDATA  ~krw910   ~danielhardman   ~ashcherbakov   Hello all, firstly let me please introduce my vision of performance testing as I have some experience with it.  ============================================================================ ABSTRACT As a PhD I can say that one of the most important (and difficult) things in measurements is to get "true results", i.e. results that really correspond to things that are in scope of the investigation. For example, we want to investigate the optimality of implementation of some algorithm by measuring the time of its execution. Also current implementation of algorithm does some I/O operations with hard disk. So in this case in fact we test disk performance instead of optimality of algorithm implementation as time of disk I/O wait may be several (x10, x100, x1000) times greater than time spent on execution of algorithm instructions itself. The same situation occurs for distributed systems that use network for communication as network I/O wait may take a long time. So that each measurement should be done taking into account all causes that affect performance. If some cause may unexpectedly influence gathered results then this cause should be excluded from the experiment, especially if this cause is not under control (hard disk read/write speed, global network routing etc.).   POOL MEASUREMENTS Our pool setup is a globally dispersed system. The main causes affecting pool performance are: - network I/O wait - hard disk I/O wait - nodes consensus algorithm - implementation of algorithms that work with tree structure and levelDB settings Measurements using global pool setup can just show the fact that there are performance problems, but they can not determine the root causes of performance degradation. I propose to divide performance testing into several stages with different isolation level. Each stage should exclude causes of the performance degradation one by one. In this step by step performance testing the measurements using global pool setup should be the last stage. The first stages should exclude at least global network and global routing.  I propose the following test plan: 1. Some static code analysis. 2. Single-node read/write ledger operations without hard disk (in-memory). 3. Single-node read/write ledger operations with hard disk (including levelDB). 4. Multi-node read/write ledger operations using isolated local network. 5. Multi-node read/write ledger operations using global network.  Some details for each stage: 1. Short stage to figure out obvious implementation inaccuracy like unnecessary iteration over all records and so on. 2. At this stage we can determine problems in implementation of tree algorithms by analysis of decreasing of number of read/write operations per second caused by growing number of written records. 3. Analysis of levelDB settings and hard disk usage, try to find the ways to minimise disk I/O operations (use caches, I/O batching etc.). 4. Investigation of functionality of isolated pool with different number of nodes without influence of global routing, here we can check implementation of requests processing and RBFT influence. 5. Investigation of influence of global routing and AWS infrastructure, comparing throughput degradation with isolated pool.   DISCUSSION: THRESHOLDS The main question related to gathered results is a question of thresholds: which throughput is acceptable and which is not? I think that it is a topic of a separate discussion. Another question is a consequence or RBFT design. It is obvious that increasing of number of pool nodes leads to the performance degradation due to the consensus algorithm: more nodes => more requests. So that the questions is: which performance degradation is acceptable and Which is not? I think that we can determine theoretical dependence graph of throughput and number of nodes as we know the number of requests to get the consensus for N nodes. So we can compare experimental results with these theoretical results to determine how far/close we are from/to acceptable results. Of course this is another topic for discussion. ============================================================================  ></body> </Action>
<Action id="37606" issue="24539" author="krw910" type="comment" body="I am closing this bug and opening a new one. I was not able to reproduce this ticket as it was written, but did find the issue another way. This looks like a view change issue and I will be logging a new ticket with logs around that issue." created="2017-12-18 20:43:34.0" updateauthor="krw910" updated="2017-12-18 20:43:34.0"/>
