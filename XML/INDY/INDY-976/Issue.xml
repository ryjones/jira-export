<Issue id="24539" key="INDY-976" number="976" project="10303" reporter="krw910" assignee="krw910" creator="krw910" type="10004" summary="Validator pools over 19 nodes lose consensus" priority="2" resolution="10001" status="10001" created="2017-11-28 21:20:15.0" updated="2019-03-29 20:35:42.0" resolutiondate="2019-03-29 20:35:42.0" votes="0" watches="4" workflowId="24539"> <description><! CDATA Once I had validator pools over 19 nodes I would start to lose consensus after 6,000 transactions. The pool would stop working and the fix is to restart all nodes in the pool. Restarting 19 nodes in a global pool by having to contact all the stewards is not acceptable.  *Testing Setup* Global pool setup through AWS across as many as 13 regions. 5 Globally dispersed client machines each running Libindy and load testing scripts.  Each client machine using Libindy ran 40 threads each thread sending 10 transactions. So with 5 Libindy machines it was simulating 200 clients each sending 10 transactions for a total of 2,000 transactions.  I would run this test 4 times and take the average transaction per second.  The measurement of transactions per second is done be getting the epoch time stamp from the first transaction that was sent and subtracting it from the epoch time stamp of the last transaction that was sent. The difference in the time stamps gives you the total number of seconds.  Dividing the total transactions by the total seconds of what was committed to the ledger gives the number of transactions per second.  I started with a 7 node pool. After 4 successful runs I would wipe out the ledger and add 3 more nodes. So each run using a different pool size started with basically the same fresh ledger with the exception of the few node transactions to add the new ones to the pool.  *Results of Test* Each test run should produce a little over 8.000 transactions. As you can see below once I got over 6,000 transactions I started losing consensus.   || Ledger Size || Pool Size || Avg Txns / Sec || | 8012 | 7 | 41 | | 8023 | 10 | 35 | | 8021 | 13 | 27 | | 8024 | 16 | 23 | | 6357 | 19 | 21 | | 6705 | 22 | 18 | | 6836 | 25 | 16 |    ></description> </Issue>
