<Action id="56103" issue="36881" author="ozheregelya" type="comment" created="2019-01-23 14:15:57.0" updateauthor="ozheregelya" updated="2019-01-23 14:15:57.0"> <body><! CDATA New test-run results: indy-node 1.6.769  Steps to reproduce are exactly the same as in description. Stopped nodes: Node 19, Node 24. At the end of the test Node1 (ex-primary) still didn't completed catch up, but the initial problem is reproducing well on Node 24.  Logs and metrics: s3://qanodelogs/indy-1955/additional_logging_23_01_2019 To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1955/additional_logging_23_01_2019/ /home/ev/logs/qanodelogs/indy-1955/additional_logging_23_01_2019/  ></body> </Action>
<Action id="56196" issue="36881" author="ashcherbakov" type="comment" created="2019-01-25 07:26:48.0" updateauthor="ashcherbakov" updated="2019-01-25 07:26:48.0"> <body><! CDATA *Problem reason* * One of the reasons for this is that if there are a lot of txns ordered by the pool when a Node is doing the view change (and hence a catch-up), then this Node may get enough Checkpoints to start catch-up. * Once the view-change/catch-up is finished, the Checkpoints were processed and a new catch-up was started. * However, there is no need to start a new catch-up since actually the quorum of stashed checkpoints is for the already ordered or caught up transactions.  *Fix* * Remove a stashed Checkpoint if it's already ordered  *PR* *  https://github.com/hyperledger/indy-plenum/pull/1061   *Tests* * test_3pc_while_catchup_with_chkpoints_only * test_checkpoints_after_view_change  *Build* * 1.6.771  Â   ></body> </Action>
<Action id="56209" issue="36881" author="toktar" type="comment" created="2019-01-25 13:50:27.0" updateauthor="toktar" updated="2019-01-28 21:03:00.0"> <body><! CDATA Another problem was found.  *Problem reason* * In catch-up a node sends CatchupReq to all nodes and doesn't check them for disconnection and malicious behavior. The catch-up will be finished but after a long time.  *Fix* * A node shouldn't send CatchupReq to disconnected nodes and nodes which didn't answer CatchupRep in the previous round of transactions request.  *PR* *  https://github.com/hyperledger/indy-plenum/pull/1063   *Tests* *  test_catchup_with_one_slow_node.py|https://github.com/hyperledger/indy-plenum/pull/1063/files#diff-a9bd82da7b85c96e8a463678809ac513  *  test_catchup_with_disconnected_node.py|https://github.com/hyperledger/indy-plenum/pull/1063/files#diff-6290ecccaaac916bdcab5264aee89cf3   *Build* * 1.6.774  ></body> </Action>
<Action id="56370" issue="36881" author="vladimirwork" type="comment" created="2019-01-30 08:59:36.0" updateauthor="vladimirwork" updated="2019-01-30 09:01:14.0"> <body><! CDATA Build Info Case 1: indy-node 1.6.772 Case 2: indy-node 1.6.774  Steps to Reproduce: 1. Run load test with load near to production. 2. Stop several nodes (Node20 and Node25). 3. Stop the primary (Node2 in Case 1 and Node1 in Case 2) to initiate View Change. 4. Start stopped nodes (one by one, including primary). 5. Stop the load. 6. Wait for the end of catch up on nodes which were stopped.  Actual Results: Stopped nodes don't order after start (but catch up txns). Case 1:  !INDY-1955_25_01_2019.PNG|thumbnail!  !INDY-1955_25_01_2019_another_nodes.PNG|thumbnail!  Case 2:  !INDY-1955_29_01_2019.PNG|thumbnail!   Logs and metrics: Case 1: ev@evernymr33:logs/1949_25_01_2019_metrics.tar.gz ev@evernymr33:logs/1949_25_01_2019.tar.gz Case 2: ev@evernymr33:logs/1955_29_01_2019_metrics.tar.gz ev@evernymr33:logs/1955_29_01_2019_logs.tar.gz  ></body> </Action>
<Action id="56436" issue="36881" author="toktar" type="comment" created="2019-01-31 12:09:49.0" updateauthor="toktar" updated="2019-01-31 12:09:49.0"> <body><! CDATA *Problem reason* * Nodes remove first messages in stash deque upon reaching the limits.  *Fix* * Don't add messages in stash deque when it more than limit.  *PR* *  https://github.com/hyperledger/indy-node/pull/1148  *  https://github.com/hyperledger/indy-plenum/pull/1067   *Tests* *  test_limited_stashing_3pc_while_catchup.py|https://github.com/hyperledger/indy-plenum/pull/1067/files#diff-aa7ee199f8b26780f7fd6b64bc93e06f   *Build* * 1.6.776  ></body> </Action>
<Action id="56443" issue="36881" author="vladimirwork" type="comment" created="2019-01-31 14:11:16.0" updateauthor="vladimirwork" updated="2019-02-01 07:17:24.0"> <body><! CDATA Build Info indy-node 1.6.776  Steps to Reproduce: 1. Run load test with load near to production. 2. Stop several nodes (Node20 and Node25). 3. Stop the primary (Node1) to initiate View Change. 4. Start stopped nodes (one by one, including primary). 5. Stop the load. 6. Wait for the end of catch up on nodes which were stopped.  Actual Results: Stopped nodes *don't order* after start but catch up successfully (except Node20). !INDY-1955_31_01_2019.PNG|thumbnail!  ev@evernymr33:logs/1955_31_01_2019_logs.tar.gz ev@evernymr33:logs/1955_31_01_2019_metrics.tar.gz   ></body> </Action>
<Action id="56494" issue="36881" author="vladimirwork" type="comment" body="All stopped and started nodes perform catch-up but still don&apos;t order so https://jira.hyperledger.org/browse/INDY-1983 has been reported for additional fixes." created="2019-02-01 12:27:59.0" updateauthor="vladimirwork" updated="2019-02-01 12:28:23.0"/>
