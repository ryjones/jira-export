<Action id="26514" issue="17793" author="dsurnin" type="comment" created="2017-06-16 12:40:59.0" updateauthor="dsurnin" updated="2017-06-16 12:40:59.0"> <body><! CDATA Initial discussion from slack     dsurnin  5:48 PM   Jason, could you please review? we would like to hear what you think about it   5:48   dsurnin  5:23 PM   so, about 201   5:23   descriptions says "If the ledger is unable to process ordinary identity transactions"   5:24   if we still need some nodes then we probably needs to identify the reasons why we cannot process transactions   5:25   what do you think?  alexander.shcherbakov  5:31 PM   Also the question what do we mean by *stalled* pool. POOL_UPGRADE is a txn (in config ledger), so currently common rules are applied to it. Should we make an exception, and process it differently?  daniel.hardman  5:32 PM   Here is the reality that we are wrestling: if the pool is broken in some way (for example, the domain/identity ledger has been corrupted), we need to be able to upgrade the ledger as a way to possibly get it working again.  lovesh  5:32 PM   If we want system transactions to be processed any different than other transactions then we can do that since the ledger and state are different but then you put an exception in the protocol   5:33   Also pool stalled is not same as nodes crashed, so need to address that too  daniel.hardman  5:34 PM   We don't want a malicious actor to be able to force a pool upgrade, so that's a reason not to accept a pool upgrade without traditional consensus. But if an upgrade would fix consensus, that's a reason to allow it... Jason is the one who asked for this--not me.  lovesh  5:38 PM   Well if we have transactions for each ledger going through a separate instance of consensus protocol so if we had 3 ledgers, we have 3 RBFT running, one for each ledger then this brings a clean separation and can help in stalls but this takes away the ability to have a total order among transactions (we are not using that total order today), so is this total order really important  daniel.hardman  5:42 PM   I think one of us needs to discuss this with Jason and understand his vision better. I think the way Dmitry originally framed the question ("If I have 4 nodes and 3 are down, should I upgrade just the 1 node"), the answer has to be "No." But if there are 26 nodes and 7 are down, it might make sense to say, "Well, down nodes are not the same as malicious nodes for the purpose of upgrade. With 19 nodes up, we have a 3f+1 scenario where f = 8, and we so no evidence of any malicious nodes--only of down nodes. So we have perfect consensus on upgrade." Something like that.  lovesh  5:43 PM   "we have a 3f+1 scenario where f = 6", so you are saying if we have a high enough f then its ok  daniel.hardman  5:44 PM   Yes, that's the theory I'm playing with.  lovesh  5:44 PM   where f=6 is high enough but f=2 is not   5:44   Ok  daniel.hardman  5:44 PM   Maybe as long as the new f is at least a majority of the old f   5:44   Again, that's just a theory. I want to hear what Jason thinks.  lovesh  5:44 PM   By i think practically if we have a bug like dead pool it will affect majority of nodes   5:45   Ok  dsurnin  5:45 PM   I will add Jason to the thread now  alexander.shcherbakov  5:45 PM   @daniel.hardman 1) what if we have 2f+1 nodes up, but their domain ledger is out of sync while config ledger is sync? -> looks like we should be able to upgrade (taken into account Lovesh's comment about possibility for separate consensus for each ledger) 2) What if we have 2f+1 nodes up, but theire config ledger is out of sync? -> No upgrade????? 3) What if we have less than 2f+1 nodes up (>f down) -> No upgrade? (edited)  daniel.hardman  5:46 PM   All of my thinking is predicated on the notion that having a node that's not responding at all is different from having a node that *is* responding.  dsurnin  5:46 PM   lets move the discussion to the new thread  new messages lovesh  5:47 PM   Yes, my assumptions are same as your, if a node is down, there is nothing we can do unless the agent is capable enough  dsurnin  5:59 PM   and about blacklisted nodes what if all nodes blacklisted one node   5:59   should we updated blacklisted one?  lovesh  6:11 PM   for the correct solution we need to have a very comprehensive strategy where the consequences depend on type of blacklisting, but a naive that works is that you send messages to blacklisted nodes but not rely on received messages from blacklisted nodes, that way the blacklisted node will have consensus on upgrade and get upgraded. I am assuming the node is not really malicious and maybe some bug or a temporary compromise that like changing the ledger is causing it to be this way, if its really controlled by an ill-intentioned guy, why bother? (edited)  daniel.hardman  6:14 PM   It's also important to consider that if a node is malicious, upgrading it would be likely (though not guaranteed) to disrupt the maliciousness (since the upgrade may clean out data or do other housekeeping, in addition to just laying down bits). I'm not sure what the implications of this are, but I wanted to point out that the operation we're talking about approving by consensus will have already been approved by a consensus of human beings, and may be a way to fix maliciousness...   ----- Today June 16th, 2017 ----- jlaw  7:53 AM   OK, sorry I'm just now responding. I'm going to give my take. If you disagree strongly with anything I've written, please speak up. :slightly_smiling_face:  *Nodes upgrade eagerly.* When a node is getting caught up, and it processes an upgrade txn, it should schedule it just like it normally would. If the date/time is in the past, then it upgrades immediately. Possibly a new story.  *An upgrade is idempotent.* A txn to upgrade to version 5 should be allowed even if an earlier upgrade to v.5 has gone through. Given four nodes, if three are at v.5 and one is at v.4, then the one at v.4 will be upgraded. When it comes time for a particular node to upgrade, and it's already at v.5, it doesn't do anything.  *Support "reupgrade".* That said, perhaps we should add 'reupgrade' field that defaults to False. When it is True, it will upgrade to v.5 even if it is already v.5. This allows for a case where a node thinks it upgraded properly, but it really didn't.  *Consensus may not be required.* Because all upgrades are backward compatible, it is OK if one node is upgraded to v.5 when all the other nodes are on v.4. A node can make a decision as to the validity of an upgrade txn independent of consensus. After validation of the upgrade txn, Perhaps if force=True, then a node should not wait for consensus on a POOL UPGRADE txn, and it should schedule the upgrade anyway. Of course if consensus can be reached, then txn still gets written.  This breaks the rule that there needs to be a POOL UPGRADE txn written before a NODE UPGRADE. In this case, a POOL UPGRADE txn won't be recorded. And because there is no consensus, the NODE UPGRADE won't be recorded. When the network comes back online and consensus can happen, the Node should submit it's NODE UPGRADE for historical purposes.  Very soon in the future, a single TRUSTEE will not be able to initiate an upgrade. It will need to be proposed first, and then voted on by TRUSTEEs and/or STEWARDS with some minimum threshold before it's considered approved. We need a story for this.  *Ledgers "stall" gracefully.* When a node is applying ledger txns to state, and runs into a ledger txn that causes an exception during deserialization or validation or processing, then the failure will be handled gracefully. A nice error message outlining the specific issue will be put to the log. That ledger will be set to a "STALLED" state. On startup, or on upgrade, or maybe even periodically, the node will reattempt to restart a stalled ledger, that is, process the ledger entry that failed. It will not proceed past the current state, and it will not participate in 3pc on any txns for that ledger. With this stalled concept, we don't need a separate consensus protocol for each ledger.  The combination of *Ledgers "stall" gracefully* and *Consensus may not be required* means we can recover from some nasty bugs.  Daniel's point about the consensus of humans already having happened is an important one. The ledger should allow an upgrade txn that 'doesn't make sense'. For example, we should not have some arbitrary 5 minute minimum space between node upgrades. Trust the upgrade txn if approved is sound.  Finally, I'm a little concerned that we're talking about upgrades in the enterprise computing frame of reference. We're not upgrading nodes. We don't upgrade nodes. Rather, Sovrin supports txns that each node independently verifies and validates and if everything looks good to the node, it upgrades itself. It's a subtle distinction, but an important one that will help us make smart choices as we work on improving the upgrade feature.  lovesh  9:27 AM   We have what's needed for "Nodes upgrade eagerly".  We partially have "An upgrade is idempotent", i say partially since node on upgrade can run an additional script, we need to ensure changes done as part of the script are idempotent, we need a test which before any pool upgrade is sent applies the upgrade twice on each node and verifies the update to be idempotent. We need to build "Support "reupgrade", but should we have downgrade too? I understand that for now we can assume a Trustee is not malicious thus make consensus optional on POOL_UPGRADE, but when we have voting by Trustees for POOL_UPGRADE and the ledger is stalled, how would the voting happen since voting is done through a transaction. Or do we make an exception for voting txns? Regarding "talking about upgrades in the enterprise computing frame of reference", doesn't this contradict the above point "Trust the upgrade txn if approved is sound.", because trusting the upgrade txn is similar to trusting the group of network admins/owners of the enterprise. Or do we say we have diffused trust not just between Sovrin validators but between Trustees too? (edited)  jlaw  9:52 AM   Just like we decreed that *all upgrades must be backward compatible*, we need to decree that *all upgrades are idempotent*.  We need to support downgrades. Til now, I've been assuming a downgrade is just another upgrade, but that's probably not good enough. Open to your thoughts.  Good catch on the voting. Option is to have each voter sign an approval out of band and the submitter bundles the votes with the upgrade txn. Imagine something like a CSR where the serialized upgrade proposal is emailed to Trustees or Stewards who load it in the CLI, review it, and then approve it, which spits out an approval .sovrin document that they email back to the submitter. Certainly adds complexity (human and machine), but it's a workable solution. Let's bat this one around and see if we can get something a bit more tight.  Yes, we have diffuse trust with trustees as well.  lovesh  10:04 AM   Ok, since the submitted upgrade txn is bundled with signed votes, nodes do not try to achieve consensus but just apply the txn to ledger, do the upgrade and get "un-stalled"  alexander.shcherbakov  11:48 AM   @lovesh @jlaw and everyone. Thank you for your thoughts. I think all *bold* items need to be represented as separate stories. I think the only item that should be done right now in the scope of INDY-201 is *Consensus may not be required*.  But my understanding of this item is that we should just make *scheduling of Update* and *processing POOL_UPGRADE txn* separate. That is we schedule Upgrade immediately regardless of consensus (probably only if force=True), and then we continue processing POOL_UPGRADE txn as usual (that is propagate, wait for consensus, and write into Config Ledger, etc.).  Dmitry is going to write a PoA for this item with some details.  ></body> </Action>
<Action id="26516" issue="17793" author="dsurnin" type="comment" created="2017-06-16 12:42:15.0" updateauthor="dsurnin" updated="2017-06-16 12:42:15.0"> <body><! CDATA Document for the further discussion here  https://docs.google.com/document/d/1h2bRdzmoNf-XcUNR-vmDR30Svcin8GAvChrulV-tC54/edit  ></body> </Action>
<Action id="27191" issue="17793" author="dsurnin" type="comment" created="2017-06-22 13:01:44.0" updateauthor="dsurnin" updated="2017-06-22 13:02:34.0"> <body><! CDATA In scope of this task will be added optional parameter “Force=true|false” (false by default) to the POOL_UPGRADE command. Force=false - no changes needed, works the same way as now. Force=true - basicly all the second level checks are ignored, transaction does not wait for consensus, node schedule upgrade right after receiving and then tries to process it as a normal transactionIgnore 5 min timeout in case of force is true and timestamp is now or in the past  Client will send forced upgrade to the nodes even if the number of nodes is small, for now client does not send anything if the number of available nodes is insufficient  ></body> </Action>
<Action id="27279" issue="17793" author="dsurnin" type="comment" created="2017-06-23 15:18:56.0" updateauthor="dsurnin" updated="2017-06-27 12:26:32.0"> <body><! CDATA Implemented in   https://github.com/evernym/plenum/commit/572090e9c4c4fe6671526824591c8d68287bb6fe    https://github.com/sovrin-foundation/sovrin-common/commit/7827d886fe3f67263646ff6f7a0a71650e9a67ab    https://github.com/sovrin-foundation/sovrin-node/commit/4b46a1e67611dfd26d5545542bd7a9118b83bed9    https://github.com/sovrin-foundation/sovrin-client/commit/66575aebbf8d63e336844d0a2c6dcd3c85586842     * optional parameter force * upgrade scheduled before consensus * send forced request from client to any number of connected nodes * forced upgrade allows to omit some nodes in schedule map * forced upgrade allows to send time in schedule map less or equal to current time     tests   https://github.com/evernym/plenum/blob/master/plenum/test/input_validation/fields_validation/test_bool_field.py    https://github.com/sovrin-foundation/sovrin-client/blob/master/sovrin_client/test/cli/test_pool_upgrade.py    https://github.com/sovrin-foundation/sovrin-node/blob/master/sovrin_node/test/upgrade/test_pool_upgrade.py   ></body> </Action>
<Action id="27832" issue="17793" author="ozheregelya" type="comment" created="2017-07-04 13:44:43.0" updateauthor="ozheregelya" updated="2017-07-04 13:44:43.0"> <body><! CDATA Build Info: sovrin-client version: 0.4.19 sovrin-node version: 0.4.7  OS/Platform: Ubuntu 16.04.2 LTS  *Reason for Reopen:* Necessary validation is absent.  *Case 1:* No validation for role. *Steps to Reproduce:* 1. Open the CLI. 2. Connect to test environment. 3. Do not use any role, or use Steward role. 4. Send POOL_UPGRADE with force=True parameter.  *Actual Results:* Upgrade is successfully scheduled, node is broken after upgrade. Services are not able to start. Following errors are in journalctl:  ^jout.txt   *Expected Results:* Only Trustee should be able to upgrade the node. Node should not be broken.  *Case 2:* No validation for uniqueness of upgrade name. *Steps to Reproduce:* 1. Open the CLI. 2. Connect to test environment. 3. Send POOL_UPGRADE with force=True parameter and not unique upgrade name.  *Actual Results:* Upgrade is successfully scheduled.  *Expected Results:* Error message should appear.  ></body> </Action>
<Action id="27953" issue="17793" author="dsurnin" type="comment" created="2017-07-06 15:50:53.0" updateauthor="dsurnin" updated="2017-07-06 15:50:53.0"> <body><! CDATA case 1 is fixed  node 051470bc9444acc058d2726ce166c3cbd94fa0d2  added test for this case  sovrin_node/test/upgrade/test_pool_upgrade_reject.py     case 2 were considered as a valid case  ></body> </Action>
<Action id="28327" issue="17793" author="vladimirwork" type="comment" body="New features of force parameter are checked. Case 1 fix is checked. Case 2 is not reproducing despite of comment about &quot;case 2 were considered as a valid case&quot;. Single finding is in INDY-415." created="2017-07-13 16:13:13.0" updateauthor="vladimirwork" updated="2017-07-13 16:13:13.0"/>
