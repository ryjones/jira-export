<Action id="42353" issue="27713" author="mgbailey" type="comment" body="Who should have access to these logs? I don&apos;t think that the general public should, because it might inadvertently provide data that could be used by a bad actor to find an exploit, but I think that stewards should all have access.  When it comes down to it, all stewards are peers, and we have no more right to this information than any other steward" created="2018-03-29 19:37:51.0" updateauthor="mgbailey" updated="2018-03-29 19:37:51.0"/>
<Action id="42376" issue="27713" author="sergey.khoroshavin" type="comment" created="2018-03-30 10:37:45.0" updateauthor="sergey.khoroshavin" updated="2018-03-30 11:00:59.0"> <body><! CDATA We have several problems now which need to be solved, and hardest one is that devs from indy core team don't have access to logs from live pool. Without this information it's impossible to debug issues that may arise in production. I understand concerns about exposing logs to general public, so I believe that there should be some form of authorization. The main point is that authorized team members should be able to get these logs as soon as possible, without human intervention.  Other problems are that there's a lot of information in logs (for example a few hours of load on 25 nodes currently generates ~100Gb of raw text data with some spikes of almost 1 Gb/minute), which needs to be stored and indexed somehow, so that queries on this data don't run for ages. I've already made a small tool which tries to solve problem of extracting useful data from logs, but as it grows in capabilities it already starts to resemble some of ELK tools, and I think that moving to ELK stack is the most cost-efficient solution for our monitoring and debugging problems.  I have three propositions on architecture to choose from: 1. One central ELK pool hosted by Sovrin, where logs from all Stewards are gathered.  Pros: - one data source to run queries, simplifying debugging node communication issues - trivial to setup dashboards showing health of entire pool - guaranteed isolation from validator nodes - relatively simple to setup from technical point of view - minimal intervention needed from Stewards (just install filebeat and point it to ELK)  Cons: - needs to be hosted by Sovrin - is centralized (but I don't think this is a real problem as it just for logs and health monitoring and doesn't affect consensus process)  2. ELK pools hosted by Stewards, containing their own logs. Pros: - decentralized  Cons: - devs need to get authorization from each steward to get logs - more complex for Stewards to setup (and more chances for mishaps) - no easy way to run queries concerning multiple nodes - no easy way to setup a dashboard showing health of entire pool  3. ELK pools hosted by Stewards, each containing logs from all Stewards Pros: - decentralized - queries concerning multiple nodes possible - trivial to setup dashboards showing health of entire pool (from point of view of each Steward!) - devs can get all logs from any Steward reducing problems with authorization  Cons: - even more complex for Stewards to setup - massive data duplication  I want to stress that none of the proposed solutions require changes to existing plenum code nor removes simple logs files as currently written by nodes, so even if we go with centralized solution and something bad happens it won't affect request processing, and no valuable data (including logs) will be lost.  ></body> </Action>
<Action id="42403" issue="27713" author="tharmon" type="comment" created="2018-03-30 21:01:34.0" updateauthor="tharmon" updated="2018-03-30 21:01:34.0"> <body><! CDATA I'm having a hard time with this ticket/requirement in general. Here are a few thoughts around it. First off, there are some statements that are being taken to be givens that I don't necessarily agree with.  {quote}We have several problems  ...  one is that devs from indy core team don't have access to logs from live pool.{quote}  This was a design tradeoff from the very beginning. My belief is that no dev should believe they will have access to any or all of the logs from the live pool. That is a centralized administration mindset. Even if you wanted to try and enforce this, Stewards are free agents and under existing trust frameworks are not compelled to participate, so getting all logs is unlikely.  {quote}Without this information it's impossible to debug issues that may arise in production.{quote}  Difficult, yes. Impossible, no. That's a very strong word. I would like to see if we can come up with some more ingenious ways to handle this particular problem. Don't get me wrong, I fully understand the difficulty of what I'm suggesting here.  {quote}The main point is that authorized team members should be able to get these logs as soon as possible, without human intervention.{quote}  I believe there is a strong desire for this from the development point of view, but I'm not convinced it's okay. There may very well need to be some human intervention. In short, I question the very premise of this ticket.  I also have a problem with the apparent amount of outbound traffic these suggestions would create from any given node. Under load, the transmission pipes are already going to be quite full. I'm not particularly interested in filling them even more with what appears to be quite a bit of data (1Gb/minute uncompressed was mentioned), especially if it's just a convenience thing.  I suspect this particular view will likely cause some consternation and some follow-on conversations.   ~sergey.khoroshavin  :  ~mgbailey  :  ~danielhardman   ></body> </Action>
<Action id="42405" issue="27713" author="mgbailey" type="comment" created="2018-03-30 23:37:06.0" updateauthor="mgbailey" updated="2018-03-30 23:37:06.0"> <body><! CDATA  ~tharmon  These are valid concerns that caused us to mothball the system for log aggregation that we had in place when we promoted the network from 'alpha' to 'provisional'.  In view of our experiences in debugging the provisional network, maybe we should revisit this.  First I will describe what we did then:  We set up a VM (aggregator) whose only purpose was to store logs from the nodes.  We then provided to the stewards a script that uses rsync to push the logs and the transaction count to the aggregator. This was configured with a ssl cert with narrowly defined write access to a config directory on the aggregator.  We also provided instructions for setting up a cron job to trigger the script periodically.  If we were to do this today, I would have it give the output of validator-info instead of just the transaction count.  Advantages of this approach is that it is entirely separate from indy & sovrin.  It is something that stewards can opt to do so that we don't have to bug them for logs, or they can choose not to and we will continue to harass as needed.  We can also provide read-only credentials to all the stewards who want to see the logs themselves.  It is simple and provides the autonomy that the stewards need, and we have used it successfully in the past.  The same reasoning regarding the cleanliness of the logs can be used to justify this approach, as with the other approach that is discussed here.  ></body> </Action>
<Action id="42414" issue="27713" author="danielhardman" type="comment" created="2018-03-31 23:06:01.0" updateauthor="danielhardman" updated="2018-03-31 23:06:01.0"> <body><! CDATA It is not acceptable to build any solution to this problem that violates the following principle:  ...Developers should have access to no more data about the running system than the general public, UNLESS the stewards give them that data or the developers are stewards themselves...  Period. This is not negotiable; it is built into the trust framework for the network itself. Doing otherwise makes the system centralized and gives developers–who haven't signed any legal agreements with anybody in the trust framework--a back door. Ain't gonna happen.  I don't think this means that the title of this ticket is invalid, but it is close to crossing the line--we have to be very, very careful how we accomplish the goal.  It might be acceptable for stewards to publish some log data that they are willing to share with the whole world. Such data could be published and indexed automatically, and it could be useful for developer debugging. But we cannot put authorization around it. It must be world visible, and not dangerous in the hands of hackers. If we think we can accomplish our goal by doing this, by all means let's do it.  We can publish metrics and alerts to the world (but not just to developers). For example, there is nothing that prevents us from writing code in the network that puts a message in SQS announcing a view change or a troubled catchup. But anybody in the world would have to be allowed to subscribe to it.  We can add commands to the network that anybody in the world, not just developers, can run, to find out more about what might be going wrong.  We can do elaborate things to capture specific kinds of error conditions and dump them to disk–and then beg stewards to send such things to us to help us debug. But we can't get private data without going through stewards.  We could become stewards ourselves. Whatever happened to the conversation that Anatoli and Alex started, about DSR becoming a steward? There is no reason why stewards can't be developers. They'd then have access to their own box.  If we can't figure out a way to understand the behavior of the network without logs, then I claim we have a more fundamental problem than just lack of monitoring. We have relied on a log crutch when we should be building visibility features in, and testing in such a way, that we truly understand what's going on without needing privileged access.        ></body> </Action>
<Action id="42420" issue="27713" author="sergey.khoroshavin" type="comment" created="2018-04-01 17:04:41.0" updateauthor="sergey.khoroshavin" updated="2018-04-01 17:04:41.0"> <body><! CDATA Okay, I see this is both political and technical issue, and I was mostly thinking about technical side. First of all, thanks for making this clear {quote}Developers should have access to no more data about the running system than the general public, UNLESS the stewards give them that data or the developers are stewards themselves {quote} So, as I see there are just two options from political point of view: 1. Make logs public, so everyone can see what happens on live pool. Since https://jira.hyperledger.org/browse/INDY-1176 is done I assume that there is no sensitive information in logs, and the only concern is that too much information increases surface of possible attack. While this is a valid concern there is also another one: hiding what happens in the pool looks like security through obscurity, which is often considered not good in the long run. Also, one possible option is to have realtime logging solution, but for general public (including devs) make it lag for an hour or even a day. That way attacks exploiting timings will be much harder, but developers will still get access to full logs eventually.  2. Make developers stewards themselves, but I'm afraid this is completely out of scope of my competence, we need Alex response here.  There is also technical side of this issue. Let me try to explain what is happening now and why I think ELK (or something like ELK) is the way to go: 1. During stress testing our test pools generate about 100 Gb of logs per session, and they could generate more if there was enough space. As part of https://jira.hyperledger.org/browse/INDY-1225 I'm also implementing log compression during rotation, so when this work will be done there will be even more data. 2. In order to process these logs I proposed to write a simple log processor, now it's already in indy-plenum master, sort of grew and helped to find some issues. The only problem is that it's quite slow to process these amounts of data, that's why after a talk with Alex I asked Trev for CPU-packed spot instace. 3. In order to accelerate processing one possible thing to do is to reduce amount of data logged, and there was already some work done in that area ( https://github.com/hyperledger/indy-plenum/pull/592 ), but reduction is just 1.5-2-fold. Further reduction could be done in future after thorough analysis, but not now. For example, logging this much information helped us to identify that during one load test session one of nodes started effectively DoSing other nodes for no particular reason which decreased processing throughput to a halt (I'm talking about that 1 Gb of logs per minute episode, and it's clearly not a normal situation we expect in healthy pool). 4. Other option to accelerate processing is indexing. It can be implemented as part of this log processor utility (making it even more complex), or some existing solution could be used leaving log processor relatively simple and suitable for simpler cases. Both me and Alex think that using existing solution (like ELK stack) is preferable. 5. Now if we start using ELK stack for testing why not make this solution suitable for live pool? Filebeat doesn't listen on some network port, it just watches files and sends data over mutually authenticated and encrypted channel to logstash, so I believe this is no more complex or dangerous than rsync script mentioned by Mike. Also while bringing up central (publicly available) elasticsearch pool is more complex than simple VM for storing logs semantically it's not that different and can provide additional benefits.  So, what I'm proposing now is to make PoC of ELK-based logging solution, then apply it to testing pools, and after resolving political issues one or another way apply it to live pool.  ></body> </Action>
<Action id="42421" issue="27713" author="sergey.khoroshavin" type="comment" created="2018-04-01 17:44:55.0" updateauthor="sergey.khoroshavin" updated="2018-04-01 17:48:46.0"> <body><! CDATA Also, I want to clarify some more things: {quote}Difficult, yes. Impossible, no. That's a very strong word. I would like to see if we can come up with some more ingenious ways to handle this particular problem. Don't get me wrong, I fully understand the difficulty of what I'm suggesting here. {quote} Probably I've used wrong words. I mean that when we don't have any logs we can only try to make guess what happened, try to reproduce situation in tests, fix that and only hope that it was indeed the same situation that happened in live pool. Almost no way to prove that issue is really fixed. {quote}I also have a problem with the apparent amount of outbound traffic these suggestions would create from any given node. Under load, the transmission pipes are already going to be quite full. I'm not particularly interested in filling them even more with what appears to be quite a bit of data (1Gb/minute uncompressed was mentioned), especially if it's just a convenience thing. {quote} As I stated above this was clearly not a normal situation, and amount of logs written was not the biggest problem. Also, there was some work to reduce amount of things to log, and I believe it could continued in the future. {quote}We set up a VM (aggregator) whose only purpose was to store logs from the nodes.  ...  Advantages of this approach is that it is entirely separate from indy & sovrin. {quote} Who hosted that VM? I thought it was Sovrin, so it's hard to understand how it can be separate from it. If it wasn't Sovrin and that was ok next question is can this enitity host elasticsearch pool? {quote}It must be world visible, and not dangerous in the hands of hackers. If we think we can accomplish our goal by doing this, by all means let's do it. {quote} To be honest personally I like this solution the most. {quote}If we can't figure out a way to understand the behavior of the network without logs, then I claim we have a more fundamental problem than just lack of monitoring. {quote} It's only one month passed since I joined this project so my opinion could be totally wrong, but I have a feeling that we have too many integration tests and too little truly unit ones. This has led to sometimes quite convoluted code that's hard to reason about (and hard to write unit tests). But again - this is just my feeling.     ></body> </Action>
<Action id="42457" issue="27713" author="gudkov" type="comment" created="2018-04-03 10:51:54.0" updateauthor="gudkov" updated="2018-04-03 11:04:17.0"> <body><! CDATA  ~danielhardman   ~tharmon   ~ashcherbakov   ~nage   ~SeanBohan_Sovrin   What is about the following approach:  - Node can support custom logger configuration. It can write logs locally and also optionally support uploading logs to ELK instance provided in config - Logs will contain public data only. At least logs that will be transferred to ELK - Sovrin will host ELK instance for logging - Stewards can optionally configure Nodes to post logs to Sovrin's ELK instance by some support agreement - Access to Sovrin's ELK instance will be limited to Sovrin's developers team  This solution doesn't break any decentralization assumptions (as logging is optional) and for me it corresponds to what we do now: We ask stewards for logs and some stewards send logs to us. But with automation of this process.  Implementation of really public decentralized logging infrastructure doesn't look reasonable for now for the following reasons: - It will be hard (impossible) to implement before TDE - It simplifies discovery of approaches to hack our system in a short perspective - It will reduce our stability and hacking-resistance in a short perspective  I understand that it looks as moving a bit out of "ideal" way, but we need some solution to our real problem: - We have limited time and resources before TDE - Our stability and performance require to be better - Our reaction time to pool problems is too big now for TDE needs  For now this solution looks like the only feasible way to solve this. It significantly decreases risk of big pool problems after TDE. So it is a more question to Product team.  ></body> </Action>
<Action id="42466" issue="27713" author="tharmon" type="comment" created="2018-04-03 14:59:00.0" updateauthor="tharmon" updated="2018-04-03 14:59:00.0"> <body><! CDATA At this time, I'm opposed to live streaming logs to an ELK instance because of the overhead it creates and my feeling that we are too close to the line with the trust framework. I'm opposed to all forms of enforced automation of this for similar reassons. As noted, we are trying to address some real problems.  {quote}We have limited time and resources before TDE{quote}  That certainly needs to be discussed.  {quote}Our stability and performance require to be better{quote}  That's independent of this ticket, to some extent. I know we _want_ this information to help with this, and that it would be helpful. But, stability and performance improvements does not have this as a prerequisite.  {quote}Our reaction time to pool problems is too big now for TDE needs{quote}  This needs to be addressed both from a technical and a political point of view. This ticket feels like we are trying to create a purely technical solution.  {quote}So it is a more question to Product team.{quote}  No, this is a question for the Technical Governance Board. ^^  ~danielhardman   ></body> </Action>
<Action id="42467" issue="27713" author="gudkov" type="comment" created="2018-04-03 15:09:03.0" updateauthor="gudkov" updated="2018-04-03 15:09:03.0"> <body><! CDATA >  I'm opposed to live streaming logs to an ELK instance because of the overhead it creates  I am not sure that this overhead will be really visible especially if we limit log level  ></body> </Action>
<Action id="42770" issue="27713" author="seanbohan_sovrin" type="comment" body=" ~nage   ~danielhardman  - can you weigh in on this" created="2018-04-10 13:11:36.0" updateauthor="seanbohan_sovrin" updated="2018-04-10 13:11:36.0"/>
<Action id="42780" issue="27713" author="danielhardman" type="comment" created="2018-04-10 15:59:46.0" updateauthor="danielhardman" updated="2018-04-10 16:03:29.0"> <body><! CDATA Slava's proposal could be entertained except for his last bullet, "Access to Sovrin's ELK instance will be limited to Sovrin's developers team". This is unacceptable, even if logging is optional. Let me explain why.  Sovrin's solution for identity is supposed to be decentralized, permissioned, and public. There are other blockchain-based solutions to identity, such as uPort and Blockstack. We have differentiated from them by saying that we can get better scale/performance/security by permissioning which nodes are running. Their critique of us is, "Why should the world trust a few Sovrin stewards who claim they are being honest and noble with blockchain data?" Our response is, "Everything that the stewards do is transparent. They conduct their meetings in public, the upgrades that they schedule are done in public, the code they run is public..."  What Slava is proposing here would undermine that story. Stewards would have some specially privileged friends - Sovrin developers - that get to see more about what's going on in the system than the public itself. It doesn't make any difference that this is an optional system; we'd be trying hard to convince stewards to use it, and we'd be depending on that system to help us troubleshoot. And we'd be telling the world that this isn't a security risk, because Sovrin developers are trustworthy and wouldn't manipulate the system for their own purposes.  This is essentially centralization. We are centralizing the coding and troubleshooting of the system, and the knowledge about what is happening inside the system. And we are explicitly not being transparent; that's what "access would be limited" means. Critics of Sovrin from the permissionless world would be right to doubt the trustworthiness and transparency of such a network. There is a reason that a role with special access to the system internals isn't a construct supported by the Trust Framework; it's because it's not trustable. Any solutions that have this characteristic will be vetoed by the TGB if they get that far.  I think the reason we keep proposing variations on this idea is that we don't believe, in our heart of hearts, that it's going to be safe for the general public to know what the system is doing internally. Even if logs are scrubbed of especially sensitive items, it will be too easy for hackers to use such knowledge to refine their attempts at a DDoS attack.  So let's make it so we can troubleshoot without logs...  The amount of struggle we're having on this ticket has convinced me that we have a more fundamental problem than how to handle logs intelligently. The problem is this:          Logging is our only visibility mechanism.  The majority of questions we need to resolve during troubleshooting center on state transitions: when did a view change happen, and why? Is catchup happening as we expected? And so forth. We observe these events in logs that contain thousands or millions of other events. So now we're dreaming up a mechanism to stream these logs somewhere, and index them so we can find the few key pieces of info that are most relevant to troubleshooting.  Maybe what we need instead is alerting (NOT logging) for the 0.01% of events that are especially helpful for troubleshooting. Maybe we should build a system where any steward can do specialized troubleshooting, but developers cannot–and then try much harder to make DSR a steward. Maybe we should build a system where we can ask the system at any time when its last view change or catchup happened, and what its status is, and what triggered it–but only if you're a steward or trustee?   If that isn't something we can afford to build right now, then we can entertain more pragmatic ideas, such as Slava's idea minus special access for developers. But those pragmatic workarounds should not be confused with the right answer, and they cannot sacrifice trust. There is no point in building the system in the first place, if we give up trust.  ></body> </Action>
<Action id="42784" issue="27713" author="devin-fisher" type="comment" created="2018-04-10 16:57:38.0" updateauthor="devin-fisher" updated="2018-04-10 16:57:38.0"> <body><! CDATA Do we need logs from multiple nodes in the pool for them to be useful to Indy development or Sovrin DevOps?  If not, evernym is a steward and as a steward, we should own and control the logs from our node. Surely we could send them to ELK stack for our own operational needs without breaking trust.  Of course, having only one node logs will not be as helpful and all the node's logs. But using just Evernym's logs could it be a middle ground that is pragmatic and fits the trust framework.  So is one node's log good enough?  ></body> </Action>
<Action id="42805" issue="27713" author="danielhardman" type="comment" created="2018-04-11 03:37:00.0" updateauthor="danielhardman" updated="2018-04-11 03:37:00.0"> <body><! CDATA What if we supported troubleshooting via transactions, including stuff like the following: * A new param to any existing transaction, called "enable_debug", that would cause the transaction to be returned with a lot of debug info that would help diagnose how it was handled by the pool. * A new read-only transaction, DEBUG_CONSENSUS, that would specifically test consensus and report what was learned.  We could make these things usable by anybody, or we could make them usable by stewards only. I don't think that would violate our principles of decentralization.  We could also change the behavior of the nodes such that, any time any of them requests a view change or begin a catchup, they automatically go into debug mode for 5 minutes, then revert back.  ></body> </Action>
<Action id="42825" issue="27713" author="gudkov" type="comment" created="2018-04-11 14:52:48.0" updateauthor="gudkov" updated="2018-04-11 14:52:48.0"> <body><! CDATA  ~danielhardman   > A new param to any existing transaction, called "enable_debug", that would cause the transaction to be returned with a lot of debug info that would help diagnose how it was handled by the pool.  The main our problems don't directly related to transactions. They are related to "view change" and "catch up" processes corner cases. Not sure that info we got during transaction execution can help a lot.  > A new read-only transaction, DEBUG_CONSENSUS, that would specifically test consensus and report what was learned.  We plan to implement VALIDATOR_INFO transaction that behaves very similar. Take a look to https://jira.hyperledger.org/browse/INDY-1184 Also interesting ticket is https://jira.hyperledger.org/browse/INDY-1175  > If that isn't something we can afford to build right now, then we can entertain more pragmatic ideas, such as Slava's idea minus special access for developers. But those pragmatic workarounds should not be confused with the right answer, and they cannot sacrifice trust. There is no point in building the system in the first place, if we give up trust.  I believe we need to do this. It can help a lot in stability goal right now. In the future we can replace this with more elegant solution.  ></body> </Action>
<Action id="42867" issue="27713" author="nage" type="comment" created="2018-04-12 13:19:11.0" updateauthor="nage" updated="2018-04-12 13:19:11.0"> <body><! CDATA There are a few steps here to avoid a centralization issue: 1) add an API for retrieving logs with the right permission settings 2) create the ELK or similar process to aggregate the data and report it on Sovrin.org 3) publish that code so that any entity with the right permissions can build their own versions of that site  ></body> </Action>
<Action id="42882" issue="27713" author="sergey.khoroshavin" type="comment" created="2018-04-12 17:11:30.0" updateauthor="sergey.khoroshavin" updated="2018-04-12 17:11:30.0"> <body><! CDATA  ~nage  {quote}1) add an API for retrieving logs with the right permission settings{quote} Do you require this to be some transaction implemented by validator node or it can be arbitrary solution like some (possibly replicated) service with REST API?  All in all I think that approach you propose will solve our problem, but I have concerns that service exposing API for retrieving logs can experience quite a high load due to possible amount of logs requested, and it will require some indexing implementation so that logs can be retrieved at least by time range. This in turn makes me think that this service better be separate from node, and it should be based on some existing tool. Probably this could end up with each steward hosting their own ElasticSearch+LogStash instance, and Sovrin.org hosting some lightweight front-end service.  ></body> </Action>
<Action id="43004" issue="27713" author="ashcherbakov" type="comment" created="2018-04-16 13:10:09.0" updateauthor="ashcherbakov" updated="2018-04-16 13:14:28.0"> <body><! CDATA There is a quick fix of a simple publishing of all log files to a server (using cron job). This quick-fix matches our first thinking and the first option to achieve the requirement of INDY-1177. But we started to analyse it further, and came to the conclusion that in addition to just getting logs, it would be great to have some build-in *analyses* of the logs (not re-inventing a wheel). You can find more details in Sergey Kh.'s comments in the ticket. Actually we already have some custom scripts (written by Sergey Kh.) that can analyze the logs. The script works pretty good for common cases, but they don't work well with a huge amount of data, and making them do so looks like re-inventing a wheel. The ELK approach allows to perform log analysis more efficiently which means that *efficiency of stability bug fixing will be increased.*  I believe we need to do the following: # Propose a short-term solution as a hot-fix we already have:  * Use cron job and publish the logs to a public server if Steward wants * Use custom script to analyze the logs * No dev work is required *  ~mgbailey  /  ~tharmon  need to work with Stewards to configure such publishing of logs  # Propose a long-term solution to use ELK (Sergey Kh. can provide more details):  * Each Steward may have (optionally) ELK instance * Each instance is mapped (optionally) to the main Sovrin ELK instance with Dashboard * Use ELK for log analyses (not custom scripts) * Allows to perform log analysis more efficiently which means that *efficiency of stability bug fixing will be increased* * May require about *1 Sprint for 1 engineer for PoC* of this approach.  I would prefer to implement Option 2 (ELK) since it has an advantage of bug fixing efficiency. However, we may consider using a short-term solution for now, and implement long-term a bit latter taking into account a lot of other (quite critical) issues.      ~sergey.khoroshavin   ~danielhardman   ~nage   ~tharmon   ~mgbailey  Do you agree with proposed short- and long-term solutions? If so, I believe we can close the ticket.  Would you prefer to implement short solution first, or start with a long-term solution right now (taking into account the advatnage of long-term solution for log analysis and efficiency of stability fixing)?  ></body> </Action>
<Action id="43008" issue="27713" author="ashcherbakov" type="comment" created="2018-04-16 14:00:49.0" updateauthor="ashcherbakov" updated="2018-04-16 14:00:49.0"> <body><! CDATA  ~sergey.khoroshavin  Please provide more details regarding Long-term option if needed (in particular, the difference between push from Stewards to their local ELK and pull from global ELK to local ELKs).  ></body> </Action>
<Action id="43013" issue="27713" author="sergey.khoroshavin" type="comment" created="2018-04-16 15:33:53.0" updateauthor="sergey.khoroshavin" updated="2018-04-16 15:33:53.0"> <body><! CDATA  ~ashcherbakov  Probably not much new details here, but a sum up of architecture I propose given latest input: 1. Each node (willing to provide logs) will contain filebeat service which will watch node log file and send data to logstash instance hosted by steward. This can be done over local network, and compression can be applied if needed (filebeat supports it out of the box). Also, this channel could be authenticated and encrypted which is also supported by filebeat. 2. Logstash instance hosted by steward will process logs adding useful metadata and store them in elasticsearch also hosted by steward. Actual processing rules will be ported from current log processing utility and provided in some open source repository (to be decided). Since logstash instance is separate from validator node and even can be denied from connecting to it (filebeat connects to logstash, not vice versa) it should be relatively safe to update these rules automatically. If this is still a problem it's possible to define only minimal set of rules which won't change much, in this case further processing will be mostly based on full text search rather than metadata. 3. Elasticsearch instance hosted by steward will store logs and provide external (possibly permissioned) API for ther retrieval. This API can be used by steward to implement some local dashboard/monitoring/alerting solution, or by some external entity including Sovrin and developers. 4. Sovrin can implement global dashboard by hosting kibana and elasticsearch instance with cross cluster search feature enabled which will pull data from stewards as needed.  This way following goals should be achieved: 1. Minimal interference and security risks for validator node 2. Log processing burden is spread across stewards 3. Stewards are in full control of what information they give away 4. Stewards have almost ready-to-use solution for monitoring and alerting 5. Any entity can have access to indexed logs for fast analysis  ></body> </Action>
<Action id="43068" issue="27713" author="danielhardman" type="comment" created="2018-04-17 22:13:47.0" updateauthor="danielhardman" updated="2018-04-17 23:38:17.0"> <body><! CDATA Some feedback: # I think that an ELK solution has many good characteristics, and that Sergey K's summary of achieved goals (1-5) in the preceding comment is nice. I also believe that the proposal would alleviate some pressure for us for a while. I recognize that a lot of effort has gone into the current proposal, and I appreciate it. # I do not believe that it will scale. My belief is that it will be adequate until late 2018 at most. This is based on the observation that A) ledger traffic is likely to spike by mid year due to increasing interest and adoption; B) ledger traffic will also grow substantially due to the way token features use the ledger; C) the number of stewards we are wanting to work with is growing linearly, which will cause the number of logged events to grow in an N-squared fashion; D) we are likely to increase the verbosity of logs as we drill into events that might be causing problems we want to troubleshoot. # I believe that the heavy reliance on logging introduces new failure modes for the network, where disk space for logs gets exhausted, or the network pipe over which logs are streamed elsewhere has brownouts or disconnections. # I do not believe it accomplishes the overarching goal for developers, because in order for developers to get the insight they are hoping for, they need stewards to run with log level = DEBUG. I don't consider this reasonable. Production software should run at INFO or WARN levels. # I do not like the way this turns a project about a ledger into a project with deep investment in dashboards and monitoring technology. Dashboards are a legitimate feature for Sovrin, but not for indy. # I don't believe a deep archive of log events is particularly useful. 99% of the time, if we experience an interruption of service, we should be interested in what the last hour of the logs tells us. If we are not noticing the problem for more than an hour, then we have other problems and should not confuse them with logging. Thus, building and maintaining an index over a vast corpus of log data is overkill. # I am fairly certain that the Ops and OpsSec departments of large stewards will reject this approach because it requires greater visibility into their infrastructure than they prefer. # I do not think we are leveraging the monitoring plugin feature that we already wrote. Farooq gave us a sample monitoring plugin that would raise alerts on SQS. It feels like we are ignoring it. # I believe the current proposal is fundamentally centralized. We are saying that we want to think about this system as a centralized system with a common view across all of it, and we want to invest in maintaining that common, centralized view as a permanent feature. I don't think it's tenable. I admit that the proposal gets past some of my objections about conflicts with the trust framework, but it assumes that the way to diagnose and troubleshoot a failure is to create and maintain a centralized view.  What I would recommend is a fundamentally different approach to visibility for developers. Let's do the quick-and-dirty fix that doesn't involve ELK, and close this ticket. Then let's rethink our whole approach and write a new ticket or set of tickets that contemplates things like this: * How can we detect a looming crisis BEFORE it happens, and put the system into a proactive "capture mode" that will help developers diagnose and troubleshoot when the inevitable failure actually happens? For example, can we detect a DDoS and start logging differently, assuming we might go down? Can we detect nagging patterns of near failure in consensus? Instead of noticing that the network is no longer achieving consensus, can we start troubleshooting as soon as any node routinely fails to participate in consensus? Could we agree that when a node isn't achieving consensus, we automatically switch into a debug mode where logging gets more aggressive? * Can we develop a dashboard that is NOT populated from logs, but rather is populated from measures like network connection quality, average time to process a transaction, etc? * Can we develop some tools that will quickly eliminate common failure cases as explanations for a problem? * Can we work on scheduled downtime for nodes, so we promote routine maintenance instead of accidental interruptions? * Doctors take people's temperature during a visit, because it tells them a lot about the health of the system. What is the equivalent for us? If we don't know how to take the system's "temperature", why not, and how can we fix it? * Can we instrument node state machines, such that just the sequence of state transitions is reported to developers?  * Can we develop a node selection algorithm that lets us identify problem nodes and remove them from the consensus pool? * Can we ask nodes to maintain a more verbose debug log for developers in a circular queue of, say, 1 hour–plus a command that would pull this data on demand? This would give us an hour to notice and pull data. The data could be pulled by a pre-authorized person, but only distributed to developers on request. In this way, we could avoid having massive logs to analyze.  ></body> </Action>
<Action id="43087" issue="27713" author="ashcherbakov" type="comment" created="2018-04-18 07:05:46.0" updateauthor="ashcherbakov" updated="2018-04-18 07:05:46.0"> <body><! CDATA Ok, then let's close this ticket and go with short-term solution for now.   ~mgbailey   ~tharmon  Can you please make sure that we configure and apply short-term solution (cron job to upload logs to a server) to STN and Live Pools?  ></body> </Action>
<Action id="43088" issue="27713" author="ashcherbakov" type="comment" created="2018-04-18 07:22:39.0" updateauthor="ashcherbakov" updated="2018-04-18 07:22:39.0"> <body><! CDATA  ~danielhardman   I believe we need to create a set of Stories based on your comments and proposals ( ~esplinr   ~SeanBohan_Sovrin   ~resplin ).     Some comments on your items:  {quote}I don't believe a deep archive of log events is particularly useful. 99% of the time, if we experience an interruption of service, we should be interested in what the last hour of the logs tells us. If we are not noticing the problem for more than an hour, then we have other problems and should not confuse them with logging. Thus, building and maintaining an index over a vast corpus of log data is overkill. {quote} I agree that this is true for most of the cases, and that we need some mechanisms to receive notifications about events in the system (other than logs). However, I think that some history of events or logs may still be needed in some cases. For example, an event that something is wrong with the Node may be lost, or not processed by a Steward (or someone else who monitors it), or it just occurred in the middle of the night, so we will need more info than just the last hour.  {quote} I do not think we are leveraging the monitoring plugin feature that we already wrote. Farooq gave us a sample monitoring plugin that would raise alerts on SQS. It feels like we are ignoring it. {quote} I'm not aware of such a plugin at all. Can you please provide more info? There are some plugins that can help gathering statistics, but I'm not aware of SQS one done by Farooq.  {quote} I believe the current proposal is fundamentally centralized. {quote} I don't agree with this assumption. The short-term solution is centralized, but the architecture of the long-term solution is that each steward individually and independently configures ELK instance, and there is an API/way to pull logs from there (not necessary centralized). Centralization there appears only when Sovrin also creates a global ELK instance pulling data from Stewards (in fact from Steward's ELK instances). But this is rather optional and not essential part.    ></body> </Action>
<Action id="43090" issue="27713" author="ashcherbakov" type="comment" created="2018-04-18 07:46:24.0" updateauthor="ashcherbakov" updated="2018-04-18 07:46:24.0"> <body><! CDATA  ~danielhardman   ~esplinr   ~tharmon   ~mgbailey  I created a new Monitoring story (we were discussing it before + it matches some of your proposals): https://jira.hyperledger.org/browse/INDY-1278  ></body> </Action>
<Action id="43125" issue="27713" author="mgbailey" type="comment" body="Regarding the comment about scheduled downtime.  Some of the stewards have been asking about this, for example, for a kernel patch that is released that requires a reboot. This could be as sophisticated as a steward writing a transaction to the config ledger to schedule a down time for his node, along with a mechanism for a human-readable mechanism to display this.  It could be as simple as a shared google spreadsheet where stewards enter their downtimes." created="2018-04-18 17:32:40.0" updateauthor="mgbailey" updated="2018-04-18 17:32:40.0"/>
