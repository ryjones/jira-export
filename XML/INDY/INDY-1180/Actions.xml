<Action id="40288" issue="27763" author="ozheregelya" type="comment" created="2018-02-16 13:11:47.0" updateauthor="ozheregelya" updated="2018-02-16 13:51:47.0"> <body><! CDATA The issue was reproduced on the latest RC (indy-node 1.3.52).  Debug logs and journalctl:  https://drive.google.com/file/d/1wK5kj8yHLZ-SDiUP8Ip_OuIMjegnD6XA/view?usp=sharing   Node 9 data:  https://drive.google.com/file/d/1QO-ZEPq8vwpoOIrjRQWZM5Ax8XZnFvwg/view?usp=sharing   ></body> </Action>
<Action id="40374" issue="27763" author="ashcherbakov" type="comment" created="2018-02-19 08:13:14.0" updateauthor="ashcherbakov" updated="2018-02-19 09:03:34.0"> <body><! CDATA *Issue 1: Not updated watermarks on backup instances*  _Problem reason:_ When a master replica receives a quorum of stable checkpoints, and realizes that it's behind, it start catch-up and updates the watermarks.  Backup replicas should also update watermarks in this situation.  _Fix:_  Update the watermarks for delayed replicas regardless whether it's a master or backup.  _PR:_   https://github.com/hyperledger/indy-plenum/pull/536   *Issue 2: It looks like view change is not finished on some of the nodes*  _Problem reason:_  A node didn't finish view change for some reasons (not enough logs to say why).  _Fix:_  A Node needs to check whether it received a quorum of ViewChangeDone (Current State) messages, and request them if needed.  _PR:_  TBD  *Issue 3: One of the Backup Primaries is disconnected*  _Problem reason:_ We initiate a View Change only if master Primary is disconnected. If a backup replica's Primary is disconnected, then the requests are just stashed.  It leads to increasing of queues size.  _PR:_  TBD  *Issue 4: Two Nodes couldn't restart*  Two Nodes hanged up on 'Recovering tree from hash store of size'. Recovering didn't finish.  Stop/restart of indy-node service didn't work.  The ledger data seems to be correct, it was able to recover on another machine.  Also the Node started properly after restart (killing indy-node service).  _PR:_  TBD     ></body> </Action>
<Action id="40794" issue="27763" author="anikitindsr" type="comment" created="2018-02-26 12:17:16.0" updateauthor="anikitindsr" updated="2018-02-26 12:17:16.0"> <body><! CDATA Recomendation for QA: - If this issue will not reproduced while testing INDY-1141 then ticket may be closed.  ></body> </Action>
<Action id="40799" issue="27763" author="ashcherbakov" type="comment" created="2018-02-26 12:53:40.0" updateauthor="ashcherbakov" updated="2018-02-26 12:53:40.0"> <body><! CDATA * INDY-1191 is created for Issue 2 *  ~anikitinDSR Please create a ticket for Issue 3  * Let's see if Issue 4 is reproduced again, it looks like some problems with systemd.  ></body> </Action>
<Action id="41184" issue="27763" author="ozheregelya" type="comment" created="2018-03-05 12:01:38.0" updateauthor="ozheregelya" updated="2018-03-13 13:47:53.0"> <body><! CDATA Environment: 25-nodes AWS pool indy-node 1.3.324  Steps to Validate: 1. Check basic load: {{for s in `seq 1 2000` ; do python3 Perf_Add_nyms.py -n 500 -s 20 ; done}} - {color:#d04437}{color:#14892c}failed{color}{color}. 2. Check load with greater count of threads (like in INDY-1141).  ></body> </Action>
<Action id="41452" issue="27763" author="ozheregelya" type="comment" body="Exploratory ticket INDY-1216 was created to check how many transactions can be written with different count of threads. This ticket will be partially tested by exploration of INDY-1216." created="2018-03-12 12:39:47.0" updateauthor="ozheregelya" updated="2018-03-12 12:39:47.0"/>
<Action id="41529" issue="27763" author="ozheregelya" type="comment" created="2018-03-13 13:59:34.0" updateauthor="ozheregelya" updated="2018-03-15 14:50:44.0"> <body><! CDATA Environment: indy-node 1.3.331 libindy 1.3.1~371 AWS 25 nodes pool  Reason for Reopen: Problem with nodes lagging reproduces with 1 thread in load test.  Steps to Reproduce: 1. Run load test with 1 thread: for s in `seq 1 2000` ; do python3 Perf_Add_nyms.py -n 500 -s 1 ; done  Actual Results: Part of nodes lagged. Most of nodes wrote 33509 transactions. Node5: 29576 Node7: 29369 Node12: 32648 UPD: Node14: 24074 Node17: 29697 Node21: 26812  Expected Results: Nodes should not lag.  Logs:  https://drive.google.com/file/d/1ZAoULbXf-P_Y-umXJkKJDimO0vgebarT/view?usp=sharing   ></body> </Action>
<Action id="41797" issue="27763" author="ozheregelya" type="comment" body="Logs after first  ~dsurnin &apos;s fix (indy-node 1.3.339):  https://drive.google.com/file/d/16kD3pWpkg3tTowcWaNfMgk5QVC_u5TYU/view?usp=sharing " created="2018-03-19 15:40:43.0" updateauthor="ozheregelya" updated="2018-03-19 15:41:08.0"/>
<Action id="41834" issue="27763" author="ozheregelya" type="comment" created="2018-03-20 14:21:43.0" updateauthor="ozheregelya" updated="2018-03-20 14:21:43.0"> <body><! CDATA Additional logs with memory monitoring (empty statistic_NodeX.log were added by mistake): https://drive.google.com/file/d/1SiEzVZACd-Dmdh74QqrIKoci0sE3qjXT/view?usp=sharing  Environment: AWS pool of 25 nodes (t2.medium) indy-node 1.3.339  Steps to Reproduce: 1. Run load test with 5 threads for ~16-20 hours.  Actual Results: One of nodes was killed because of out of memory after ~6 hours. Most part of nodes wrote 46268 txns. Node 6: 43931; Node 7: 44903; Node 8: 46230; Node 14: 45842; Node 16: 38420; Node 17: 46230; Node 18: 40315. Note that there were problems with connection to node6 and it was restarted before logs collecting.  ></body> </Action>
<Action id="41924" issue="27763" author="ozheregelya" type="comment" created="2018-03-22 13:54:37.0" updateauthor="ozheregelya" updated="2018-03-22 13:54:37.0"> <body><! CDATA More logs: https://drive.google.com/file/d/1Aqop3BzlbPuTQlvJ-aHgoK46C1dL8JxR/view?usp=sharing  ></body> </Action>
<Action id="42146" issue="27763" author="dsurnin" type="comment" created="2018-03-26 13:08:14.0" updateauthor="dsurnin" updated="2018-03-26 13:08:14.0"> <body><! CDATA There were 3 fixes In scope of this issue. 1 - Fix of invalid batch splitting 2 - Cleaning of queue of stashed_during_chatchup commits 3 - Fix processing of non-finalized requests  At the moment pool is able to restore its state after the load is gone. For the moment several nodes are being killed during the test with Out of mem error. Separate issue for memory issue is created  INDY-1238|https://jira.hyperledger.org/browse/INDY-1238   ></body> </Action>
