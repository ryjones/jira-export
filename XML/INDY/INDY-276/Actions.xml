<Action id="28008" issue="18359" author="lovesh" type="comment" body=" ~krw910  It cannot get back to original state, but if you delete the state database and restart the node, the state database will be correctly populated" created="2017-07-07 13:07:38.0" updateauthor="lovesh" updated="2017-07-07 13:07:38.0"/>
<Action id="49407" issue="18359" author="ozheregelya" type="comment" created="2018-08-27 11:53:51.0" updateauthor="ozheregelya" updated="2018-08-27 11:53:51.0"> <body><! CDATA Actual behavior is exactly the same as Lovesh said. It was tested and described as part of case 3 from INDY-273: {quote} To get pool functioning after this stewards of broken nodes should perform following steps: 1. Run _sudo systemctl stop indy-node && sudo systemctl stop indy-node-control_ 2. Run _sudo rm -r /var/lib/indy/<network>/data/* && sudo rm -r /var/log/indy/<network>/*_ 3. Get _/var/lib/indy/<network>/_data from one of working nodes. 4. Put it to the according folder of his node and rename _data/<alive_node_name>_ to _data/<steward's_node_name>_ 5. Remove file _data/<steward's_node_name>node_info_ 6. Start the indy-node. {quote} If there are enough nodes for catch up, steward can start node after step 2 and node will complete catch up.  ></body> </Action>
