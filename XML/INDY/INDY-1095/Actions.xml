<Action id="38869" issue="26936" author="krw910" type="comment" created="2018-01-18 17:35:22.0" updateauthor="krw910" updated="2018-01-18 17:35:22.0"> <body><! CDATA This is blocking the following tickets: INDY-1054 INDY-1025 INDY-1034  ></body> </Action>
<Action id="38907" issue="26936" author="ozheregelya" type="comment" created="2018-01-19 19:56:12.0" updateauthor="ozheregelya" updated="2018-01-19 19:58:53.0"> <body><! CDATA The issue is reproducing on the latest master 1.2.279. DEBUG logs and journalctl are attached in archives.  ></body> </Action>
<Action id="38908" issue="26936" author="ozheregelya" type="comment" created="2018-01-19 19:58:13.0" updateauthor="ozheregelya" updated="2018-01-19 19:58:13.0"> <body><! CDATA  ~krw910 ,  After restart of whole pool load test works fine, so this problem doesn't block testing with load test fully.  ></body> </Action>
<Action id="38967" issue="26936" author="krw910" type="comment" body=" ~ozheregelya  If we can not get past 400 transactions then it does block the other tickets I have. Restarting the pool has always seemed to fix the issues, but we have to be able to run thousands not hundreds of transactions." created="2018-01-22 15:25:51.0" updateauthor="krw910" updated="2018-01-22 15:25:51.0"/>
<Action id="38977" issue="26936" author="ozheregelya" type="comment" created="2018-01-22 17:33:47.0" updateauthor="ozheregelya" updated="2018-01-22 17:33:47.0"> <body><! CDATA  ~krw910 ,  Last week I was able to send ~4500 transactions after restart the pool. I had stopped on this value only because of "No space left on device" error which was caused by too big syslog file.  ></body> </Action>
<Action id="39447" issue="26936" author="spivachuk" type="comment" created="2018-01-31 19:03:41.0" updateauthor="spivachuk" updated="2018-01-31 19:03:41.0"> <body><! CDATA *Problem reason:*  When a replica orders a 3PC-batch, it checks whether it has an own checkpoint with the bounds including {{ppSeqNo}} of this batch. If it does not have such one, it creates a new checkpoint with the lower bound equal to {{ppSeqNo}} of the batch and the upper bound equal to {{ppSeqNo + CHK_FREQ - 1}}. So while a replica orders every next batch in a view, bounds of checkpoints which it creates are predictable. Under these conditions an upper bound of any checkpoint is divisible by {{CHK_FREQ}}.  Bounds of checkpoints may be shifted if a replica did not participate in ordering of some 3PC-batches in a view, the node got the transactions contained in these 3PC-batches via catch-up and after that the replica orders next 3PC-batches in the same view. Shifted bounds of a checkpoint prevent to stabilize it because the replica does not receive checkpoints with the same bounds from other replicas in the instance. Other replicas, in turn, may not stabilize their checkpoints with regular bounds since they may not gather a quorum of checkpoints with the same bounds from other replicas in the instance because some replicas send checkpoints with shifted bounds.  In the case described in the ticket more than {{f}} nodes in the pool caught up in the the same view. So checkpoints on replicas of these nodes were created with shifted bounds. Replicas did not gather quorum of checkpoints to stabilize them, so did not move the watermarks forward and eventually stopped to process 3PC-messages when their {{ppSeqNos}} exceeded the high watermark.  *Changes:* - Fixed an issue with possible shift of checkpoint bounds that had resulted in inability to stabilize the current and all the following checkpoints in the current view. Now the upper bound of a checkpoint is always set to a value divisible by {{CHK_FREQ}}. So the second checkpoint after a catch-up can always be stabilized and so the watermarks will be moved forward. (With the default configuration settings, ordering of 3PC-batches can survive 2 not stabilized checkpoints because the watermarks window size - {{LOG_SIZE}} - is {{3 * CHK_FREQ}} by default.) - Added a test verifying that the upper bound of the checkpoint after a catch-up is divisible by {{CHK_FREQ}} config parameter. - Added a test verifying that the second checkpoint after a catch-up can be stabilized. - Added a test from  ~anikitinDSR  which verifies that more than {{LOG_SIZE}} batches can be ordered in one view after more than {{f}} nodes caught up in this view when some 3PC-batches had already been ordered in this view. - Made minor corrections in existing tests of checkpoints.  *PRs:* - https://github.com/hyperledger/indy-node/pull/544 - https://github.com/hyperledger/indy-plenum/pull/510  *Version:* - indy-node 1.2.290-master - indy-plenum 1.2.231-master  *Risk factors:* - Nothing is expected.  *Risk:* - Low  *Covered with tests:* - {{test_upper_bound_of_checkpoint_after_catchup_is_divisible_by_chk_freq}} - {{test_second_checkpoint_after_catchup_can_be_stabilized}} - {{test_ordering_after_more_than_f_nodes_caught_up}}  *Recommendations for QA:* - To verify the fix it is sufficient to deploy a pool of 4 nodes, add 2 new nodes to the pool and then send more than 300 domain transactions one by one with one second interval. In case if all the transactions have been successfully committed into the ledger, it makes sense to ensure that all the nodes in the pool are in the view 0.  ></body> </Action>
<Action id="39566" issue="26936" author="zhigunenko.dsr" type="comment" created="2018-02-02 08:05:59.0" updateauthor="zhigunenko.dsr" updated="2018-02-05 08:47:04.0"> <body><! CDATA *Environment:* * indy-node 1.2.291 * indy-plenum 1.2.231  *Steps to Reproduce:* CASE 1 1. Setup the pool of 4 nodes 2. Send 1 transaction manually. 3. Run python3 Perf_Add_nyms.py -n 10 => transactions successfully written. 4. Run python3 Perf_Add_nyms.py -s 5 -n 100 => transactions successfully written. 5. Run python3 Perf_Add_nyms.py -s 5 -n 10000 6. add node5 and node6 7. Run another python3 Perf_Add_nyms.py -s 5 -n 10000 CASE 2 1. Setup the pool of 4 nodes 2. Add 2 nodes manually one by one 3. Run python3 Perf_Add_nyms.py -s 5 -n 10000  *Actual results:* All transactions (before and after nodes addition) successfully recordered in all nodes _read_ledger --type domain --count_ show the same values in all nodes _validator-info_ show the same values in all nodes  *Additional info:* Suspicious differences in the logs of nodes will allocated into INDY-1146  ></body> </Action>
