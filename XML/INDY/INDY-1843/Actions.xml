<Action id="53374" issue="35285" author="toktar" type="comment" created="2018-11-13 09:03:32.0" updateauthor="toktar" updated="2018-11-13 09:22:34.0"> <body><! CDATA In 00:00 on the Node8 the node stack had a high load. With INFO logs we don't know what messages were it. But with metrics we see that it were messages with type MessageRep.  !image-2018-11-13-12-04-18-072.png|width=573,height=293! In this metric we see count of processing messages per second for all types of messages witch can be asked via message requests (MessageReq). Node8 asked Propagate messages. * red line - count of processed propagates per second * orange line - count of processed message replies per second  As a result, Node8 is slow due to the large number of processed messages. It does not have time to order incoming messages and after 2 hours the node shot down with out of memory. And the instance 7 with primary Node8 has been removed with backup primary degradation.  ></body> </Action>
<Action id="53682" issue="35285" author="toktar" type="comment" created="2018-11-21 10:08:45.0" updateauthor="toktar" updated="2018-11-21 11:13:11.0"> <body><! CDATA Logs with level INFO and csv from 1, 6, 8, 16 nodes are not enough.  We know that in 22:00 nodes processed a lot of request messages. We don't know which node send it but message requests ask propagates messages. Node24 received twice PrePrepares from Node9 (instance 8) at 23:59:29. It means that some other node may already ordered this batch and asked propagate from other nodes. What can we do: # Fix problem with requesting Propagates messages for already ordered batches (INDY-1709) # Add logging for all message requesting. # Retest with Debug logging level and new feature.  ></body> </Action>
<Action id="54059" issue="35285" author="toktar" type="comment" body="Similar problem with propagates re asking was found in the logs for another case with the same INDY-1865 load and 400 MAX_CONNECTED_CLIENTS_NUM \ 500 iptables client connections limit restrictions: ev@evernymr33:logs/1865_23_11_2018_400_500_restrictions.7z" created="2018-11-30 09:15:26.0" updateauthor="toktar" updated="2018-11-30 09:15:26.0"/>
<Action id="54062" issue="35285" author="toktar" type="comment" created="2018-11-30 09:44:26.0" updateauthor="toktar" updated="2018-11-30 11:20:37.0"> <body><! CDATA Problem reason: - 4 nodes restarted with out of memory and have a problem with ordering. In theory this problem relates to so big traffic with Propagate messages re-asking. In this case slow nodes should receive a lot of Pre-Prepare messages with already ordered requests.    Changes: - In this step bug with re-asking ordered requests is fixed. If it solves problem with the pool in this ticket, task can be closed.   PR: *  https://github.com/hyperledger/indy-node/pull/1067  *  https://github.com/hyperledger/indy-plenum/pull/990   Version: * indy-node 1.6.711 -master * indy-plenum 1.6.617  -master  Risk factors: - Node ordering, backup instances can be stop ordering in view change or catchup if task is fixed incorrect.  Risk: - Low  Covered with tests: *  test_replica_received_preprepare_with_unknown_request.py|https://github.com/hyperledger/indy-plenum/pull/990/files#diff-4a9676e3338b499da78166553598c538  * test_process_pre_prepare_with_not_final_request * test_process_pre_prepare_with_ordered_request  Recommendations for QA: * Repeat test from this task with DEBUG logging level for 8-15 hours. If logs will contain "Pre-Prepare message has already ordered requests" and no one node will not restart with out of memory, ticket may be close.  ></body> </Action>
<Action id="54362" issue="35285" author="vladimirwork" type="comment" created="2018-12-05 09:43:02.0" updateauthor="vladimirwork" updated="2018-12-05 09:43:02.0"> <body><! CDATA Build Info: indy-node 1.6.713  Steps to Validate: 1. Run 20 nyms/sec for ~8 hours with debug logs. 2. Run 20 nyms/sec for ~12 hours with info logs. 3. Check metrics and logs for OOM reasons at any node.  Actual Results: It looks like there are no "Pre-Prepare message has already ordered requests" entries in the logs in both cases. Pool has been broken it the first case but since the second case works well the root cause of the issue is debug level logs. !image (3).png|thumbnail!  !image (4).png|thumbnail!  !image (5).png|thumbnail!  !image (6).png|thumbnail!   ></body> </Action>
<Action id="54376" issue="35285" author="toktar" type="comment" created="2018-12-05 13:57:30.0" updateauthor="toktar" updated="2018-12-06 09:13:07.0"> <body><! CDATA In the first test with debug logs we have a lot of view changes, because master instance writes more logs in debug logging level then backup instances and works slower. * 1-3 View changes happened with master instance degraded * 4-7 View changes happened because a previous View change didn't finish in time.   In this test, message requests for Propagates were found but it's an expected behavior. If Node2 is farther from primary node than from other nodes, it can receives PrePrepare message early than will has a quorum of Propogates messages to finalize request.  ></body> </Action>
