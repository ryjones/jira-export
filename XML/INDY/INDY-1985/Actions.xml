<Action id="56680" issue="37383" author="vladimirwork" type="comment" created="2019-02-05 09:14:49.0" updateauthor="vladimirwork" updated="2019-02-05 09:14:49.0"> <body><! CDATA Build Info: indy-node 1.6.779  Steps to Reproduce: 1. Run load test with 1 writing txn per second and 10 reading txns per second (1/10 of production load with the same txn types). 2. Stop several nodes (Node20 and Node25). 3. Stop the primary (Node1) to initiate View Change. 4. Start stopped nodes (one by one, including primary). 5. Stop the load. 6. Wait for the end of catch up on nodes which were stopped. 7. Start the same load for a short period to check ordering.  Actual Results: There is the same issue as for full production load (also it looks like Node 25 has started ordering at Step 7 but Node 1 and Node 20 have not). ev@evernymr33:logs/1983_04_02_2019_metrics.tar.gz ev@evernymr33:logs/1983_04_02_2019_logs.tar.gz  ></body> </Action>
<Action id="56681" issue="37383" author="ashcherbakov" type="comment" created="2019-02-05 11:03:00.0" updateauthor="ashcherbakov" updated="2019-02-05 11:03:00.0"> <body><! CDATA *Problem reason* * This is caused by the fact that ** we write to multiple ledgers (domain and payment) during catch-up ** catch-up is not synced between ledgers ** last_ordered_3pc is set according to the last caught-up ledger * So, we caught-up domain ledgers first till last_ordered_3pc=(0, X), and then payment ledger till last_ordered_3pc=(0, X+Y). * Last ordered is set to (0, X+Y) * Once we try apply stashed 3pc messages for domain ledger, we will discard all 3pc messages below (0, X+Y), although they are not actually ordered * Once we start applying (0,X+Y+1) PrePrepare, state root will not be equal, since we actually have a gap  *Fixes* * Audit Ledger will synchronize the catch-up for all the ledgers and the issue will gone * It will be done in the scope of INDY-1945     *Proof from the logs about the reason:*  2019-02-04 14:33:14,755|INFO|node.py|Node25 is updating txn to batch seqNo map after catchup to *(1, 9562)* for *ledger_id 1*  2019-02-04 14:33:14,755|INFO|ledger_manager.py|CATCH-UP: Node25 completed catching up ledger 1, caught up 530 in total  ..... 2019-02-04 14:35:44,476|INFO|node.py|Node25 is updating txn to batch seqNo map after catchup to *(1, 9712)* for *ledger_id 1001*   2019-02-04 14:35:44,476|INFO|ledger_manager.py|CATCH-UP: Node25 completed catching up ledger 1001, caught up 480 in total 2019-02-04 14:35:44,476|INFO|node.py|Node25 caught up to 1010 txns in the last catchup  2019-02-04 14:35:44,476|INFO|replica.py|Node25:0 set last ordered as *(1, 9712)*  .....  2019-02-04 14:35:46,640|WARNING|node.py|Node25 raised suspicion on node Node2 for Pre-Prepare message has incorrect state trie root; suspicion code is 21  ></body> </Action>
<Action id="56683" issue="37383" author="ashcherbakov" type="comment" created="2019-02-05 14:00:51.0" updateauthor="ashcherbakov" updated="2019-02-05 14:00:51.0"> <body><! CDATA *Recommendation for QA*  In order to double-check the facts, I recommend the following load test: * Acceptance load with domain txns only (all domain txns): 1 txn per sec * Acceptance load with domain txns only (all domain txns): 10 txns per sec  ></body> </Action>
<Action id="56715" issue="37383" author="vladimirwork" type="comment" created="2019-02-06 12:18:29.0" updateauthor="vladimirwork" updated="2019-02-06 12:26:41.0"> <body><! CDATA Build Info: indy-node 1.6.781  Steps to Validate: 1. Acceptance load with domain txns only (all domain txns): 1 txn per sec + stop 20, 25 then stop primary then start all stopped. 2. Acceptance load with domain txns only (all domain txns): 10 txns per sec + stop 20, 25 then stop primary then start all stopped.  Actual Results: 1. All nodes catched up and ordered successfully. (/) 2. All nodes catched up and ordered successfully *except Node 25 that didn't catch up and didn't order*. (!) Logs and metrics: ev@evernymr33:logs/1985_06_02_2019_metrics.tar.gz ev@evernymr33:logs/1985_06_02_2019_logs.tar.gz !INDY-1985_06_02_2019_bad_25th_node.PNG|thumbnail!   ></body> </Action>
<Action id="56719" issue="37383" author="ashcherbakov" type="comment" created="2019-02-06 14:09:40.0" updateauthor="ashcherbakov" updated="2019-02-06 14:09:40.0"> <body><! CDATA For some reasons, Node25 was disconnected from most of the nodes after restart, so that it wasn't able to start operating.  So, the initial issues is not reproduced (that proves the theory).  ></body> </Action>
<Action id="56720" issue="37383" author="vladimirwork" type="comment" body="Second case will be re-tested in scope of INDY-1945 after audit ledger implementation." created="2019-02-06 14:16:27.0" updateauthor="vladimirwork" updated="2019-02-06 14:16:27.0"/>
