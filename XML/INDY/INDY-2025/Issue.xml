<Issue id="38513" key="INDY-2025" number="2025" project="10303" reporter="ashcherbakov" assignee="vladimirwork" creator="ashcherbakov" type="10002" summary="Debug and Validation: Restore current 3PC state from audit ledger - Phase 1" priority="3" resolution="10000" status="10001" created="2019-03-15 12:26:07.0" updated="2019-04-15 09:55:35.0" resolutiondate="2019-04-12 09:35:48.0" votes="0" watches="5" workflowId="50156"> <description><! CDATA PR:  https://github.com/hyperledger/indy-plenum/pull/1096   *The following needs to be tested* # Regression testing ** Load tests: *** acceptance load (1943) no fees *** NYM txns (10 writes per sec) with forced view change *** acceptance load (1943) with forced view change *** load to all ledgers with forced view change # Non-primary Node(s) join the pool after restart ** Docker / system tests (pool with 7 nodes): *** Restart 1 Node no load: **** send 5 txns; viewNo=0; restart the 6th node  => node set correct viewNo=0 and ppSeqNo=5; send more txns => node participates in ordering **** do view change so that viewNo=1; send 5 txns; restart the 6th node => node set correct viewNo=1 and ppSeqNo=5; send more txns => node participates in ordering **** do view change so that viewNo=1; stop the 4th node; send5 txns; start 6th Node => node set correct viewNo=1 and ppSeqNo=5;  send more txns => node participates in ordering *** Restart 1 Node with load: **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart the 6th node=> node set correct viewNo=1 and ppSeqNo; node participates in ordering with the current load **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; stop 6th node; wait for 1 minute; start the 6th Node=> node set correct viewNo=1 and ppSeqNo; node participates in ordering with the current load *** Restart master primary no load **** do view change so that viewNo=1; send 5 txns; restart the 2d node (primary) => node set correct viewNo=1 and ppSeqNo=5; send more txns => node participates in ordering *** Restart master primary with load **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart the 2d node (primary) => node set correct viewNo=1 and ppSeqNo; node participates in ordering with the current load *** Restart all nodes at the same time no load: **** do view change so that viewNo=1; send 5 txns; restart all nodes (simultaneously)  => all nodes set correct viewNo=1 and ppSeqNo=5; send more txns => nodes participate in ordering *** Restart n-f-1 nodes no load: **** do view change so that viewNo=1; send 5 txns; restart Nodes4-7 (simultaneously)  => all nodes set correct viewNo=1 and ppSeqNo=5; send more txns => nodes participate in ordering *** Restart f nodes with load **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart Nodes 6 and 7 (simultaneously) => nodes set correct viewNo=1 and ppSeqNo; nodes participates in ordering with the current load *** Restart n-f-1 nodes with load: **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart Nodes4-7  (simultaneously) => all nodes set correct viewNo=1 and ppSeqNo; nodes participates in ordering with the current load *** Restart all nodes at the same time with load: **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart all nodes  (simultaneously) => all nodes set correct viewNo=1 and ppSeqNo; nodes participates in ordering with the current load *** Restart all nodes one by one no load: **** do view change so that viewNo=1; send 5 txns; restart all nodes 1 by 1 (from Node1 till Node 7) => all nodes set correct viewNo=1 and ppSeqNo; nodes participates in ordering with the current load *** Restart all nodes one by one with load: **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec;; restart all nodes 1 by 1 (from Node1 till Node 7) => all nodes set correct viewNo=1 and ppSeqNo; nodes participates in ordering with the current load ** Load testing on 25 Nodes AWS pool *** Restart all nodes (/) **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart all nodes at the same time **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data *** Restart 1 by 1 (/) **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart nodes 1 by 1 (Node1 - Node25) **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data *** Restart f non-primary nodes (/) **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart f nodes at the same time (Nodes 18-25) **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data *** Restart f primary nodes (/) **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart f nodes at the same time (Nodes 1-8) **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data *** Restart n-f-1 non-primary nodes **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart n-f-1 nodes at the same time (Nodes 10-25) **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data *** Restart n-f-1 primary nodes **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart n-f-1 nodes at the same time (Nodes 1-16) **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data *** Restart more than n-f nodes with disabled watchdog **** Set ENABLE_INCONSISTENCY_WATCHER_NETWORK to False **** Start acceptance load as in 1983 **** Wait for 1 min **** Provoke view change (stop 1st Node; make sure view has changed; start it back) **** Restart more than n-f nodes at the same time (Nodes 1-20) **** Make sure that every node participates in consensus **** Stop the load **** Make sure that all nodes have equal data # Backup primary continues ordering after restart ** Docker / system tests (pool with 7 nodes): *** do view change so that viewNo=1; send 5 txns; restart the 3d node (primary on instance 1) send more txns => instance 1 orders transactions ** Load testing on 25 Nodes AWS pool *** Start acceptance load as in 1983 *** Wait for 1 min *** Provoke view change (stop 1st Node; make sure view has changed; start it back) *** Restart the 3d node (primary on instance 1) *** Make sure that instance 1 orders transaction and not removed # Backup non-primary continues ordering after restart ** Docker / system tests (pool with 7 nodes): *** do view change so that viewNo=1; send 5 txns; restart the 6th node (non-primary on instance 1) send more txns => Node 6 orders on instance 1 ** Load testing on 25 Nodes AWS pool *** Start acceptance load as in 1983 *** Wait for 1 min *** Provoke view change (stop 1st Node; make sure view has changed; start it back) *** Restart the 6th node (non-primary on instance 1) *** Make sure that Node 6 orders on instance 1 # Correct primaries after demotion and promotion leading to changes of F (6 -> 7; 7 ->6; etc.) ** See INDY-1720 ** Docker / system tests (pool with 7 nodes): *** Demote non-primary **** do view change so that viewNo=1; send 5 txns; **** Demote Node 7 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order *** Demote master primary **** do view change so that viewNo=1; send 5 txns; **** Demote Node 2 (master primary) **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order *** Demote backup primary **** do view change so that viewNo=1; send 5 txns; **** Demote Node 3 (primary on instance 1) **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order ** Docker / system tests (pool with 6 nodes): *** Add a node **** do view change so that viewNo=1; send 5 txns; **** Add 7th node **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order *** Promote a non-primary Node **** do view change so that viewNo=1; send 5 txns; **** Demote Node 7 **** Promote Node 7 **** send more txns => make sure that view has changed twice, all nodes have the same primaries on all instances, and the pool can order *** Promote master primary **** do view change so that viewNo=1; send 5 txns; **** Demote Node 3 (master primary for viewNo 2) **** Wait till view changed to 2 **** Promote Node 3 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order *** Promote backup primary **** do view change so that viewNo=1; send 5 txns; **** Demote Node 4 (backup primary for viewNo 2) **** Wait till view changed to 2 **** Promote Node 4 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order  ></description> </Issue>
