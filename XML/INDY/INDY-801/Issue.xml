<Issue id="20577" key="INDY-801" number="801" project="10303" reporter="mgbailey" assignee="vladimirwork" creator="mgbailey" type="10004" summary=" POOL_UPGRADE  Sovrin logs are insufficient for failed upgrade" environment="STN, running 1.0.28, being upgraded to 1.1.33" priority="3" resolution="10000" status="10001" created="2017-09-01 15:46:46.0" updated="2019-03-29 20:32:49.0" resolutiondate="2019-03-29 20:32:49.0" votes="0" watches="4" workflowId="20581"> <description><! CDATA An upgrade of the STN was performed via ledger.  One of the nodes, owned by an external steward, failed to upgrade.  There is a single-line entry in the sovrin log on that node stating that the upgrade failed, with no other explanation: {quote}2017-08-31 17:05:04,843 | INFO | upgrader.py ( 369) | _callUpgradeAgent | 2VMDBjJCqxEonfaTcwvyBy5J473H2u65ZqQhPHx47iDG's upgrader calling agent for upgrade 2017-08-31 17:05:49,672 | INFO | log.py ( 79) | setupRaet | Setting RAET log level 2 2017-08-31 17:05:52,101 | INFO | plugin_loader.py ( 116) | _load | plugin FirebaseStatsConsumer successfully loaded from module plugin_firebase_stats_consumer 2017-08-31 17:05:52,102 | INFO | replica.py ( 300) | h | icenode:0 set watermarks as 0 300 2017-08-31 17:05:52,102 | DISPLAY | node.py (1034) | addReplica | icenode added replica icenode:0 to instance 0 (master) 2017-08-31 17:05:52,102 | INFO | replica.py ( 300) | h | icenode:1 set watermarks as 0 300 2017-08-31 17:05:52,102 | DISPLAY | node.py (1034) | addReplica | icenode added replica icenode:1 to instance 1 (backup) 2017-08-31 17:05:52,103 | INFO | replica.py ( 300) | h | icenode:2 set watermarks as 0 300 2017-08-31 17:05:52,103 | DISPLAY | node.py (1034) | addReplica | icenode added replica icenode:2 to instance 2 (backup) *2017-08-31 17:05:52,156 | ERROR | upgrader.py ( 109) | __init__ | Failed to upgrade node 'icenode' to version 1.1.33* 2017-08-31 17:05:52,156 | INFO | node.py (2408) | initStateFromLedger | icenode found state to be empty, recreating from ledger 2017-08-31 17:05:52,157 | INFO | zstack.py ( 312) | start | icenode starting with restricted as True and reSetupAuth as True 2017-08-31 17:05:52,158 | INFO | stacks.py ( 76) | start | icenode listening for other nodes at 0.0.0.0:9791 2017-08-31 17:05:52,159 | INFO | zstack.py ( 312) | start | icenodeC starting with restricted as False and reSetupAuth as True 2017-08-31 17:05:52,159 | INFO | node.py ( 594) | start | icenode first time running... 2017-08-31 17:05:52,163 | INFO | zstack.py ( 580) | connect | icenode looking for england at 52.56.191.9:9701 2017-08-31 17:05:52,165 | INFO | zstack.py ( 580) | connect | icenode looking for singapore at 13.228.62.7:9701 2017-08-31 17:05:52,166 | INFO | zstack.py ( 580) | connect | icenode looking for brazil at 54.233.203.241:9701 {quote} A reason for the failure should be included as well.  A message instructing the admin to look in the syslog for more detail would also be appropriate.   ></description> </Issue>
