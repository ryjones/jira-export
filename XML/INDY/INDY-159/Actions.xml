<Action id="25502" issue="17558" author="vladimirwork" type="comment" body="FYI  ~kelly.wilson ,  ~stevetolman ,  ~tylerq ,  ~ashcherbakov " created="2017-06-06 12:50:00.0" updateauthor="vladimirwork" updated="2017-06-06 12:50:00.0"/>
<Action id="25503" issue="17558" author="vladimirwork" type="comment" created="2017-06-06 13:04:35.0" updateauthor="vladimirwork" updated="2017-06-06 13:04:35.0"> <body><! CDATA Workaround: Restart all nodes in the pool.  ></body> </Action>
<Action id="25505" issue="17558" author="aleksey-roldugin" type="comment" body="After restarting service on all nodes every node can make catch up." created="2017-06-06 13:31:01.0" updateauthor="aleksey-roldugin" updated="2017-06-06 13:31:01.0"/>
<Action id="25728" issue="17558" author="alexander.shekhovcov" type="comment" created="2017-06-08 11:50:52.0" updateauthor="alexander.shekhovcov" updated="2017-06-08 11:50:52.0"> <body><! CDATA    Actually there are two issues here  # Lots lines – "_missing PRE-PREPAREs between 7 and 2"_ in the log which should not break the pool # Though the pool does not work well  +Node1 got client request:+ {code:java} 2017-06-06 11:58:17,329 | TRACE | node.py (1336) | validateClientMsg | Node1C received CLIENT message: SafeRequest: {'reqId': 1496750297296355, 'identifier': 'CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtx wEYZkmEB', 'signature': '4UdHxtPUsCAkcTLKFLc2LroFbqHSNEFWct4YHh82BfjBkyJ8trHM8iKg8KaVBLVrUz7btB7CCfJtVeaMiSn7YvxB', 'operation': {'role': '2', 'dest': 'FE1yjAc4ewAs8EDA7kFiAURxu7AeHFa2xMV63oX5T2wY', 'type': '1'} } ...{code} +the request is ordered:+    {code:java} ... 2017-06-06 11:58:17,386 | TRACE | replica.py (1627) | send | Node1:1 sending ORDERED{'stateRootHash': None, 'instId': 1, 'ppSeqNo': 10, 'viewNo': 2, 'txnRootHash': None, 'ledgerId': 1, 'ppTime': 1496750297325.4019, 'reqIdr':  ('CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtxwEYZkmEB', 1496750297296355) } 2017-06-06 11:58:17,386 | DEBUG | replica.py (1261) | doOrder | Node1:1 ordered request (2, 10) 2017-06-06 11:58:17,387 | DEBUG | replica.py ( 748) | processCommit | Node1:1 processed incoming COMMIT(2, 10) 2017-06-06 11:58:17,387 | DEBUG | replica.py ( 737) | processCommit | Node1:1 received COMMIT(2, 10) from Node2:1 2017-06-06 11:58:17,387 | DEBUG | replica.py ( 772) | tryOrder | Node1:1 cannot return request to node: already ordered 2017-06-06 11:58:17,387 | DEBUG | replica.py ( 748) | processCommit | Node1:1 processed incoming COMMIT(2, 10) 2017-06-06 11:58:17,389 | TRACE | node.py (1640) | processOrdered | Node1 got ordered requests from backup replica 1 2017-06-06 11:58:20,080 | DEBUG | zstack.py (1161) | reconcileNodeReg | Node1 matched remote Node2 HA(host='10.0.0.102', port=9703) 2017-06-06 11:58:20,080 | DEBUG | zstack.py (1161) | reconcileNodeReg | Node1 matched remote Node4 HA(host='10.0.0.104', port=9707) 2017-06-06 11:58:20,080 | DEBUG | zstack.py (1161) | reconcileNodeReg | Node1 matched remote Node3 HA(host='10.0.0.103', port=9705) ...{code} +but the client does not get a reply so the client resend the request+ {code:java} 2017-06-06 12:00:00,266 | TRACE | node.py (1336) | validateClientMsg | Node1C received CLIENT message: SafeRequest: {'reqId': 1496750370218480, 'identifier': 'CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtxwEYZkmEB', 'signature': '2AXHWXTd2JtFngS88DbA8aSxmmFjfg4ktWHDEwPLRaDjwwNMaFj1BJ9cixjyRs95sUG7amjt8DRWXS2EQkQTp64N', 'operation': {'role': '2', 'dest': 'FE1yjAc4ewAs8EDA7kFiAURxu7AeHFa2xMV63oX5T2wY', 'type': '1'}} 2017-06-06 12:00:00,266 | DISPLAY | node.py (1382) | processClientInBox | Node1C processing b'xDsx(LV7P9<gN15vR3AL+NT=KTPslumTu}e7uHMk' request SafeRequest: {'reqId': 1496750370218480, 'identifier': 'CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtxwEYZkmEB', 'signature': '2AXHWXTd2JtFngS88DbA8aSxmmFjfg4ktWHDEwPLRaDjwwNMaFj1BJ9cixjyRs95sUG7amjt8DRWXS2EQkQTp64N', 'operation': {'role': '2', 'dest': 'FE1yjAc4ewAs8EDA7kFiAURxu7AeHFa2xMV63oX5T2wY', 'type': '1'}} 2017-06-06 12:00:00,266 | DEBUG | node.py (1537) | processRequest | Node1 received client request: SafeRequest: {'reqId': 1496750370218480, 'identifier': 'CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtxwEYZkmEB', 'signature': '2AXHWXTd2JtFngS88DbA8aSxmmFjfg4ktWHDEwPLRaDjwwNMaFj1BJ9cixjyRs95sUG7amjt8DRWXS2EQkQTp64N', 'operation': {'role': '2', 'dest': 'FE1yjAc4ewAs8EDA7kFiAURxu7AeHFa2xMV63oX5T2wY', 'type': '1'}} from b'xDsx(LV7P9<gN15vR3AL+NT=KTPslumTu}e7uHMk' 2017-06-06 12:00:00,269 | TRACE | propagator.py ( 130) | propagate | Node1 already propagated SafeRequest: {'reqId': 1496750370218480, 'identifier': 'CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtxwEYZkmEB', 'signature': '2AXHWXTd2JtFngS88DbA8aSxmmFjfg4ktWHDEwPLRaDjwwNMaFj1BJ9cixjyRs95sUG7amjt8DRWXS2EQkQTp64N', 'operation': {'role': '2', 'dest': 'FE1yjAc4ewAs8EDA7kFiAURxu7AeHFa2xMV63oX5T2wY', 'type': '1'}} 2017-06-06 12:00:00,269 | DEBUG | propagator.py ( 224) | tryForwarding | Node1 not forwarding request SafeRequest: {'reqId': 1496750370218480, 'identifier': 'CqH1opg44oVnX9QZA1PEL9VwnPnukviPNYtxwEYZkmEB', 'signature': '2AXHWXTd2JtFngS88DbA8aSxmmFjfg4ktWHDEwPLRaDjwwNMaFj1BJ9cixjyRs95sUG7amjt8DRWXS2EQkQTp64N', 'operation': {'role': '2', 'dest': 'FE1yjAc4ewAs8EDA7kFiAURxu7AeHFa2xMV63oX5T2wY', 'type': '1'}} to its replicas since already forwarded {code}        ~lovesh   ~ashcherbakov  any thoughts?   There were 2 view change in the log but I have not found any signs of the 'view change' problem.           ></body> </Action>
<Action id="25853" issue="17558" author="ashcherbakov" type="comment" created="2017-06-09 13:18:23.0" updateauthor="ashcherbakov" updated="2017-06-09 13:18:23.0"> <body><! CDATA We investigated the logs and came to the following conclusions:  There are two issues: *Issue 1* - Reproduced locally - How is shown: lots of missing PRE-PREPAREs between X and Y entries in the log - Cause: The node doesn't delete nor ignore 3PC requests with pp_seq_no < last_ordered_pp_seq_no, that is requests that are already ordered, if they come after  last_ordered_pp_seq_no is set. When a node is discinnected and then connected again, it does catch-up and sets last_ordered_pp_seq_no according to the latest state (obtained from previous nodes), but later on it receives old 3PC messages (which is ok).  - Going to fix in this ticket: Yes - How to fix: -- do not process new 3PC messages with ppSeqNo <= last_ordered_pp_seq_no;  -- GC prePreparesPendingPrevPP queue  *Issue 2* - Not Reproduced locally - How is shown: the disconnected node (and probably the whole pool) can not process any new requests - Cause:  When a node is connected back, it does catch-up, and other nodes tell that  last_ordered_pp_seq_no==2. But then it receives PrePrepare with  last_ordered_pp_seq_no==7, so it goes to infinite loop of waiting missing prep-prepares (between 2 an 7). However, it doesn't receive them. We can see two possible issues why it doesn't receive them: Issue 2.1: 2 view changes were done, so the nodes may be in a broken state (that's why the pool doesn't process any new requests). The pool may send a wrong last_ordered_pp_seq_no. Issue 2.2: The connected node does catch-up, and by that time last_ordered_pp_seq_no==2. But other nodes still process new requests at this time, and come to last_ordered_pp_seq_no>2. However, 3PC messages for this state are not sent to the connected node, since it was disconnected at that time. In other words:  nodes process res for ppSeqNo>2 and most of 3PC reqs needed for consensus  ->  Node is connected  ->  Node did a catch-up and get last_ordered_pp_seq_no==2  ->  other nodes finish processing reqs and come to the state with ppSeqNo>2  ->  The node is still at state with ppSeqNo==2 . As we don't re-run catch-up as of now, the node will remain in old state. - Going to fix in this ticket: No - How to fix: -- Issue 2.1: caused INDY-13 -- Issue 2.2: caused by INDY-103   So, the PoA: - Finish the fix for Issue1 - Disable view change and make sure that Issue 2 is not reproduced  ></body> </Action>
<Action id="26133" issue="17558" author="alexander.shekhovcov" type="comment" created="2017-06-13 11:36:14.0" updateauthor="alexander.shekhovcov" updated="2017-06-13 11:36:14.0"> <body><! CDATA (/)   *Problem reason:* - the node stashes outdated pre-prepare messages  *Changes:* - the node ignores outdated pre-prepare messages  *Committed into:* https://github.com/evernym/plenum/commit/6b1cf9b4e353fe62c5b6c237cd5cfce0e628f8e1 sovin-node 0.3.135+  *Risk factors:* Nothing is expected.  *Risk:* Low  *Covered with tests:* _test_ignore_pre_prepare_pp_seq_no_less_than_expected_  *Recommendations for QA:* The fix covers Issue 1 (see Alex's comment). The issue 2 will be fixed in INDY-13 INDY-103. # Stop a node # Send 3 NYM txns  # Start the node # Send 1 NYM txn # Make sure the node log does not contain lots "... missing PRE-PREPARE ..." (see attached logs) # Repeat 1-5 at most 10 times        ></body> </Action>
<Action id="26222" issue="17558" author="krw910" type="comment" created="2017-06-14 03:00:28.0" updateauthor="krw910" updated="2017-06-14 03:00:28.0"> <body><! CDATA  ~alexander.shekhovcov  we have a problem with Master build 0.3.138. If I stop the service on one out of 4 nodes I can send one successful transaction. The second transaction does not go through. I restarted the node that was down and the client connected, but still cannot completed a transaction. I restarted all the nodes but that still did not fix it.  Fix - I had to bring all 4 nodes services down and start them up one at a time. When I saw the client connect to one I brought up the next one. Doing one at time after having them all off worked.  ></body> </Action>
<Action id="26258" issue="17558" author="alexander.shekhovcov" type="comment" created="2017-06-14 10:21:57.0" updateauthor="alexander.shekhovcov" updated="2017-06-14 10:21:57.0"> <body><! CDATA  ~krw910  I see sovrin-node 0.3.134 installed on the Shakedown pool 4 which is affected. Have you tried 0.3.135+? Have you used another pool for the tests?  Do you still have the logs?  ></body> </Action>
<Action id="26303" issue="17558" author="stevetolman" type="comment" created="2017-06-14 17:25:30.0" updateauthor="stevetolman" updated="2017-06-14 17:25:30.0"> <body><! CDATA Sasha, please stay on this ticket until it is completed.  Once you have an idea of an ETA, please share it with us in the ticket.   We are going to start an rc build now but if you have this fixed by your end of day tomorrow (15 June), we will request and build another stable (rc) build and use that as our H1 release.  ></body> </Action>
<Action id="26400" issue="17558" author="alexander.shekhovcov" type="comment" created="2017-06-15 14:00:39.0" updateauthor="alexander.shekhovcov" updated="2017-06-15 14:12:23.0"> <body><! CDATA  ^node-2017-06-14.tar.gz   Findings: * Node2 did not forward the request 1497408485465294 to the replicas after the node gets 2 propagates from node3 and node4 * so 3pc is stopped because no quorum (node1 is not participating) * the pool sends ppSeqNo=1 in a CONSISTENCY_PROOF replay to node1 after the node1 is started * but next pre-prepare comes with ppSeqNo=4 from the primary * so node1 prints "_missing PRE-PREPAREs between 4 and 1"_  * PROPAGATE for 1497408485465294 comes to node3 and node4 before node3 and node4 receive 1497408485465294 from client  ></body> </Action>
<Action id="26568" issue="17558" author="alexander.shekhovcov" type="comment" created="2017-06-16 15:11:07.0" updateauthor="alexander.shekhovcov" updated="2017-06-16 15:11:07.0"> <body><! CDATA We have an other issue here.   Let's track the new issue here https://jira.hyperledger.org/browse/INDY-245. I suggest to close this ticket.   ></body> </Action>
<Action id="26606" issue="17558" author="krw910" type="comment" body="This ticket has been fixed and can work. I ran into the same symptoms but from a different cause and that was captured in INDY-245" created="2017-06-16 18:46:38.0" updateauthor="krw910" updated="2017-06-16 18:46:38.0"/>
