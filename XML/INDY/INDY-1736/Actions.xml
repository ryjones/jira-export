<Action id="51538" issue="34224" author="spivachuk" type="comment" created="2018-10-02 17:04:07.0" updateauthor="spivachuk" updated="2018-10-02 18:29:36.0"> <body><! CDATA As we see in the logs, *Node3* (one of the nodes which were performing the view change to view 1 longer than most nodes in the pool) started a catch-up because of a checkpoints lag in less than 2 minutes after the view change 1 completion. Then on reception of some next {{Checkpoint}} message *Node3* *started another catch-up* caused by a checkpoints lag - {color:#d04437}*right during the first catch-up being in progress*{color}. When a node starts a catch-up, it resets {{LedgerInfos}} for all the ledgers. *Thus the new catch-up abandoned the previous incomplete one.* Then again and again on reception of {{Checkpoint}} messages *Node3* started new catch-ups abandoning previous incomplete ones. None of these catch-up was completed. So *Node3* was in permanent catch-up phase and did not participate in ordering due to this. The other nodes which were performing the view change to view 1 during a long time showed the same behavior. All these nodes - *Node3*, *Node11*, *Node13*, *Node15*, *Node24*, *Node25* - were in permanent catch-up phase and did not participate in ordering due to this during the period since they started a catch-up on a checkpoints lag (21:20 - 21:23) (after the view change 1 completion) until they were restarted (22:41 - 22:47). Created INDY-1739 for fixing this issue.  Also we have found repeated requests for ledger statuses in logs. As it has turned out, scheduled re-asking for ledger statuses and maximal consistency proofs is not actually canceled when sufficient count of messages are gathered. Created INDY-1740 for fixing this issue.  ></body> </Action>
