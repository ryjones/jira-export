<Action id="38606" issue="18356" author="krw910" type="comment" body=" ~ozheregelya  I think this was covered in test we ran last year. Can you put your findings into this ticket and mark it done?" created="2018-01-15 03:43:15.0" updateauthor="krw910" updated="2018-01-15 03:43:15.0"/>
<Action id="49405" issue="18356" author="ozheregelya" type="comment" created="2018-08-27 11:39:23.0" updateauthor="ozheregelya" updated="2018-08-27 11:39:23.0"> <body><! CDATA There are several cases:  Case 1: Less than F+1 nodes alive. To get pool functioning after this we need to get stopped nodes functioning again, and start them simultaneously. Alive nodes will be restarted automatically (see INDY-1199). Or as an option, nodes which were not working may be started at any time, but when at least N-F nodes will be started, Trustee should register pool_restart using indy-cli.  Case 2: More than or equal F+1 node alive. To get pool functioning after this we need to get stopped nodes functioning again, and start them at any time. Nodes which were not working will restore state from alive ones. Pool will start as soon as at least N-F nodes will work.  Case 3: More than F nodes were broken (lagged behind others and can't catch up) because of problems during View Change. To get pool functioning after this stewards of broken nodes should perform following steps: 1. Run _sudo systemctl stop indy-node && sudo systemctl stop indy-node-control_ 2. Run _sudo rm -r /var/lib/indy/<network>/data/* && sudo rm -r /var/log/indy/<network>/*_ 3. Get _/var/lib/indy/<network>/_data from one of working nodes. 4. Put it to the according folder of his node and rename _data/<alive_node_name>_ to _data/<steward's_node_name>_ 5. Remove file _data/<steward's_node_name>node_info_ 6. Start the indy-node.  ></body> </Action>
