<Action id="26593" issue="18241" author="ashcherbakov" type="comment" created="2017-06-16 16:56:07.0" updateauthor="ashcherbakov" updated="2017-06-16 16:56:07.0"> <body><! CDATA  ~lovesh  For me it looks like there is a problem in either RBFT, or in our implementation (especially with batching): - Once a non-primary misses PRE_PREPARE, it is stalled until the next catch-up is done - It seems for me that PREPARE needs to be like a 'Propagate of Pre-Prepares'. So, even if we miss a Pre-Prepare, but received 2f Prepares for a request, then we can send COMMIT. Currently we wait for the corresponding PrePrepare in this case (which corresponds to what RBFT paper says, but is it really correct?) - Looks like we need to move all batching-related logic from PrePrepare to Prepare. That's we should validate and apply uncommitted state on recieves sufficient number of Prepares, not on receiving of PrePrepare (consider prepare as just a propagation of pre-prepares).     ></body> </Action>
<Action id="26595" issue="18241" author="lovesh" type="comment" created="2017-06-16 17:16:06.0" updateauthor="lovesh" updated="2017-06-16 17:16:06.0"> <body><! CDATA This is intentional, A node will not accept a PREPARE without a PRE-PREPARE, if we act on this assumption that success indication of a later phase means we ignore problems with previous phase, then you really don't need to wait for PRE-PREPARE+2f PREPAREs, once you have 2f+1 COMMITs if you put roots in COMMITs too. Not having a PRE-PREPARE indicates a problem, either primary did not send you, or you lost it in network or it's just delayed. So if you miss PRE-PREPARE, you should not really start the whole catchup but just request that PRE-PREPARE from one node, it was a suggestion from the author of RBFT when i asked these questions that you do not send messages speculatively. If we want then if we have PRE-PREPARE for some batches but we miss PREPAREs and COMMITs (poor network) but start getting all 3PC messages then we can quickly check if these PRE-PREPARES were ordered or not and execute them if they were  ></body> </Action>
<Action id="26597" issue="18241" author="ashcherbakov" type="comment" created="2017-06-16 17:31:01.0" updateauthor="ashcherbakov" updated="2017-06-16 17:31:01.0"> <body><! CDATA Yes, I agree that catch-up (or even just a catch-up of missing Pre-Prepares) will help.   But for me it's still not obvious that having 2f PREPARES for a request with f+1 PROPAGATE is not enough to assume that a request can be committed.  We should not send PREPAREs to other nodes if we don't have the corresponding PRE-PREPARES. But I think we can process (send COMMIT), if we have >2f PREPARES. It means that we have consensus about the PRE_PREPARE.  ></body> </Action>
<Action id="26598" issue="18241" author="ashcherbakov" type="comment" body="Also, the pool becomes very fragile without such a catch-up. It will be quite common than a PrePrepare is not received." created="2017-06-16 17:32:36.0" updateauthor="ashcherbakov" updated="2017-06-16 17:32:36.0"/>
<Action id="26601" issue="18241" author="mzk-vct" type="comment" created="2017-06-16 17:44:20.0" updateauthor="mzk-vct" updated="2017-06-16 17:44:44.0"> <body><! CDATA PrePrepare has two goals: 1. Introduce request for ordering. 2. Ensure that we have only one (since only Primary can send PrePrepares) source of requests to exclude possible concurrent tricks.  When 2f+1 backup nodes receive PrePrepare we can declare that PrePrepare played its role and its life is over. That's why I see no reason why node cannot join ordering if it has no PrePrepare, but received 2f+1 Prepares. There is only one limitation - since nodes can forward Prepares malicious node can try to introduce request for ordering bypassing primary. To avoid this we just need to prohibit node to propagate Prepares if it has no PrePrepare.    ></body> </Action>
<Action id="26602" issue="18241" author="lovesh" type="comment" body="PREPAREs and PROPAGATEs are independent, former is for batch which may have more than 1 request while the latter is for request, also PROPAGATE has nothing to do with consensus, a request might be invalid (might invalidate state, double spend) but will be propagated. If COMMIT is the third phase, if we can send COMMITs then no reason why we cant send PREPAREs" created="2017-06-16 17:46:23.0" updateauthor="lovesh" updated="2017-06-16 17:46:23.0"/>
<Action id="26603" issue="18241" author="ashcherbakov" type="comment" created="2017-06-16 17:54:50.0" updateauthor="ashcherbakov" updated="2017-06-16 17:54:50.0"> <body><! CDATA  ~lovesh  Yes, they are independent in general, but I mean the following from RBFT paper: _It  then  replies  to  the PRE-PREPARE message  by sending a PREPARE message to all other replicas, only if the node  it  is  running  on  already  received f+1  copies  of  the request._ So, it can have f+1 copies of the request only because of PROPAGATE (that's why they are related in some sense).  ></body> </Action>
<Action id="27417" issue="18241" author="lovesh" type="comment" created="2017-06-27 10:31:17.0" updateauthor="lovesh" updated="2017-06-27 10:41:02.0"> <body><! CDATA If we accept a lack of PRE-PREPARE and try to compensate that with a PREPARE (>2f PREPARE) then PREPARE needs to include which requests are part of that batch (by including all identifier+requestId) since only PRE-PREPARE contains this information, the PREPARE only has a digest for them and post roots after applying those requests.  I would suggest we simply do a request PRE-PREPARE in absence of PRE-PREPARE, it solves the problem. Missing PRE-PREPARE is not very common and to solve this uncommon problem, increasing size of each PREPARE is not acceptable. The request for PRE-PREPARE does not necessarily need to be made to the primary, it can be made to any non-primary since quorum of PREPAREs will tell if the PRE-PREPARE is correct or not. So our protocol works even if a minority (<f) of replicas is partitioned from primary.  ></body> </Action>
<Action id="27476" issue="18241" author="ashcherbakov" type="comment" created="2017-06-28 08:32:19.0" updateauthor="ashcherbakov" updated="2017-06-28 08:32:19.0"> <body><! CDATA  ~lovesh  Yes, I was assuming that we OK with lack of PRE_PREPARES, then # The PREPARE message must be indentical to PRE-PREPARE (PREPARE is just Propagation of PRE-PREPARE) # We need to move the logic for processing and validating Batches to the point, when we have a consensus of PREPAREs.    ></body> </Action>
<Action id="27513" issue="18241" author="ashcherbakov" type="comment" created="2017-06-28 15:30:31.0" updateauthor="ashcherbakov" updated="2017-06-28 15:30:31.0"> <body><! CDATA A doc from Lovesh: https://docs.google.com/document/d/1tH7LweHiV9eugbc_6TxftWquae4WYgmGpmBobeOlbq4/edit?ts=5953c82d#  ></body> </Action>
<Action id="27913" issue="18241" author="danielhardman" type="comment" body="I know we are blocked on this until someone (me, probably) specifies whether we should accept Lovesh&apos;s generic message request mechanism, or use the more specific variant that Alex favors. I sent an email about how I&apos;d like to resolve the impasse. Let&apos;s talk after Thursday&apos;s standup." created="2017-07-06 03:27:56.0" updateauthor="danielhardman" updated="2017-07-06 03:27:56.0"/>
<Action id="28073" issue="18241" author="lovesh" type="comment" body="PR for the change: https://github.com/hyperledger/indy-plenum/pull/251. Relevant `tests test_node_requests_missing_preprepare`, `test_node_requests_missing_preprepare_malicious`, `test_node_request_preprepare`, `test_no_preprepare_requested`, `test_handle_delayed_preprepares`, `test_node_reject_invalid_req_resp_type`, `test_node_reject_invalid_req_resp_params`. Also some of the tests were updated which relied on delaying the PRE-PREPARE for simulating slowness, in those PREPARE was also delayed to since now PRE-PREPAREs can be requested" created="2017-07-10 09:45:20.0" updateauthor="lovesh" updated="2017-07-10 09:45:20.0"/>
<Action id="28275" issue="18241" author="krw910" type="comment" body="Blocked by INDY-406" created="2017-07-12 20:25:37.0" updateauthor="krw910" updated="2017-07-12 20:25:37.0"/>
<Action id="28547" issue="18241" author="krw910" type="comment" created="2017-07-18 03:37:31.0" updateauthor="krw910" updated="2017-07-18 03:37:31.0"> <body><! CDATA Pool stopped working at running load test after 33,000 transactions repeating "missing PRE-PREPARE" in some of the log files.  Three of the nodes fell out of sync at about 26,000 transactions, but the pool kept working for another 7,000 transaction before it stopped allowing any new transactions to occur.  The log level was set to debug  and with that many transactions there are over 1GB of logs so I cannot attach them. Here is a snippet from Node1.log showing "missing PRE-PREPARE".  {code} 2017-07-18 00:10:16,239 | DEBUG    | replica.py           ( 950) | __is_next_pre_prepare | Node1:0 missing PRE-PREPAREs between 6 and 4 2017-07-18 00:10:16,241 | DEBUG    | has_action_queue.py  (  61) | _serviceActions | Node1 running action <function HasActionQueue.startRepeating.<locals>.wrapper at 0x7f3436645a60> with id 3478 2017-07-18 00:10:16,241 | TRACE    | node.py              (1969) | checkPerformance | Node1 checking its performance 2017-07-18 00:10:16,241 | DEBUG    | notifier_plugin_manager.py (  69) | sendMessageUponSuspiciousSpike | NodeRequestSuspiciousSpike: New value 10 is within bounds. Average: 20.178996228604614 2017-07-18 00:10:16,241 | INFO     | monitor.py           ( 296) | isMasterThroughputTooLow | Node1 master throughput ratio 0.0 is lower than Delta 0.4. 2017-07-18 00:10:16,241 | DEBUG    | throttler.py         (  30) | acquire | now: 265336.688094334, len(actionsLog): 6 2017-07-18 00:10:16,241 | DEBUG    | throttler.py         (  32) | acquire | after trim, len(actionsLog): 5 2017-07-18 00:10:16,241 | DEBUG    | throttler.py         (  42) | acquire | timeToWaitAfterPreviousTry: 3.568050833375483, timePassed: 10.00919619295746 2017-07-18 00:10:16,241 | DEBUG    | throttler.py         (  47) | acquire | timeToWaitAfterPreviousTry < timePassed was true, after append, len(actionsLog): 6 2017-07-18 00:10:16,241 | INFO     | node.py              (2035) | sendInstanceChange | Node1 sending an instance change with view_no 4 since Primary of master protocol instance degraded the performance 2017-07-18 00:10:16,242 | INFO     | node.py              (2037) | sendInstanceChange | Node1 metrics for monitor: Node1 Monitor metrics:: None Delta: 0.4 Lambda: 60 Omega: 5 instances started:  235292.793769927, 235292.794086392, 235292.79439418, 235292.794818582  ordered request counts: {0: 0, 1: 27, 2: 27, 3: 27} ordered request durations: {0: 0, 1: 14.578874578030081, 2: 14.874403344118036, 3: 16.25337524403585} master request latencies: {} client avg request latencies:  {}, {'ATHPhYMbd8GVmMH53gobC1': (5, 0.5404794633970595), 'Q3FnaYVnAk3A1oQfvGJY7v': (5, 0.5594268773973454), 'HBtdADDEL9SYpDgGDdXxXg': (5, 0.5559135610004887), 'V4SGRU86Z58d6TV7PBUe6f': (1, 0.3419497249997221), 'SJ9Akdc6vNkBsNiNhTPEhG': (5, 0.5686641380016226), '5WznHUkz36Tb6TFVYTpJed': (6, 0.519084109007963)}, {'ATHPhYMbd8GVmMH53gobC1': (5, 0.5329394946049433), 'Q3FnaYVnAk3A1oQfvGJY7v': (5, 0.5830360744032077), 'HBtdADDEL9SYpDgGDdXxXg': (5, 0.5631893420009874), 'V4SGRU86Z58d6TV7PBUe6f': (1, 0.3942903550050687), 'SJ9Akdc6vNkBsNiNhTPEhG': (5, 0.5639786391984671), '5WznHUkz36Tb6TFVYTpJed': (6, 0.54406587301249)}, {'ATHPhYMbd8GVmMH53gobC1': (5, 0.6473643529985565), 'Q3FnaYVnAk3A1oQfvGJY7v': (5, 0.6009648302046117), 'HBtdADDEL9SYpDgGDdXxXg': (5, 0.5930604616005439), 'V4SGRU86Z58d6TV7PBUe6f': (1, 0.48810275198775344), 'SJ9Akdc6vNkBsNiNhTPEhG': (5, 0.626101103995461), '5WznHUkz36Tb6TFVYTpJed': (6, 0.571303124675372)}  throughput: {0: 0, 1: 1.8519948062855396, 2: 1.8151988604421523, 3: 1.6611934194965199} master throughput: 0 total requests: 21007 avg backup throughput: 1.7721708851771232 master throughput ratio: 0.0 2017-07-18 00:10:16,242 | DEBUG    | node.py              (2543) | send | Node1 sending message INSTANCE_CHANGE{'viewNo': 4, 'reason': 25} to all recipients:  'Node3', 'virginaPerf10', 'seoulPerf8', 'Node2', 'Node7', 'saopauloPerf9', 'Node6', 'Node4', 'Node5'  {code}   *Setup* 10 Node pool each on their own machine spread globally.  4 Client machines each running the CLI  *Running load tests* I already had 246 transactions on my ledger before starting the load tests  *From one Client run the following one time (+200)* python3 add_keys.py Steward1 000000000000000000000000Steward1  *From one Client run the following in order* *Add 500* python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 *Add 500* python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once *Add 750* python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 750 --at-once *Add  1,000* python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 1000 --at-once *Add  2,000* python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 2000 --at-once  *Add 15,000* ./load_test_overmind.sh 5 3000 NYM ./load_test_clients.list   *Start at the same time from 4 separate client machines (+2,000)* Client 1 - python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once Client 2 - python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once Client 3 - python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once Client 4 - python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once  *Start at the same time from 4 separate client machines (+3,000)* *{color:#d04437}(Dropped 2 transactions){color}* Client 1 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 750 --at-once Client 2 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 750 --at-once Client 3 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 750 --at-once Client 4 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 750 --at-once   *Start at the same time from 4 separate client machines (+8,000)*   *{color:#d04437}(Became out of sync on Nodes 1,3,6){color}* *{color:#d04437}Nodes 1,3,6 - 25,596{color}* *{color:#d04437}Nodes 2,4,7,8,9,10 - 33,196{color}* Client 1 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 2000 --at-once Client 2 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 2000 --at-once Client 3 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 2000 --at-once Client 4 -  python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 2000 --at-once  I was still able to add new transactions one at a time. I then kicked off a larger test and only a few transaction were written before the pool stopped taking any new transactions.  *Start at the same time from 4 separate client machines (+60,000)* Client 1 - ./load_test_overmind.sh 5 3000 NYM ./load_test_clients.list Client 2 - ./load_test_overmind.sh 5 3000 NYM ./load_test_clients.list Client 3 - ./load_test_overmind.sh 5 3000 NYM ./load_test_clients.list Client 4 - ./load_test_overmind.sh 5 3000 NYM ./load_test_clients.list  *Final ledger counts show the following* Node 1 has  25596 total transactions Node 2 has  33219 total  transactions Node 3 has  33219 total  transactions Node 4 has  33221 total  transactions Node 5 has  33219 total  transactions Node 6 has  25596 total transactions Node 7 has  33219 total transactions Node 8 has  33219 total transactions Node 9 has  33221 total transactions  Node 10 has  33221 total transactions    ></body> </Action>
<Action id="28583" issue="18241" author="lovesh" type="comment" body=" ~krw910  Due to a bug with log rollover INDY-435 i am not able to see logs before this started happening." created="2017-07-18 14:34:02.0" updateauthor="lovesh" updated="2017-07-18 14:34:02.0"/>
<Action id="28748" issue="18241" author="krw910" type="comment" created="2017-07-20 20:43:40.0" updateauthor="krw910" updated="2017-07-20 20:43:40.0"> <body><! CDATA Killed the pool again after 5,555 transactions.   *Setup* 10 Node Global Pool 10 Clients  *Commands sent for the load tests* {color:#205081}Add Trust Anchors using one client{color} python3 add_keys.py Steward1 000000000000000000000000Steward1  {color:#205081}From each of the 10 clients{color} python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once  I had three transactions that did not go through out of 5,000  At this point I had 5,221 transaction in each of the nodes ledger  {color:#205081}From each of the 10 clients{color} ./load_test_overmind.sh 5 3000 NYM ./load_test_clients.list  This should run 50 clients sending 150,000 total requests. They are not sending all at once, but 1 at a time. So the pool should be getting about 50 requests per second.  *{color:#d04437}After 334 transactions the pool stopped taking new transactions. {color}*  *The logs show the following:*  *{color:#205081}sovrin_config.py{color}* logLevel=0 logRotationBackupCount=20 ACCEPTABLE_DEVIATION_PREPREPARE_SECS=300 enableStdOutLogging=False  *{color:#205081}Primary - All nodes show{color}* Node5:0 Node6:1 Node7:2 seoulPerf8:3  *{color:#205081}Transactions{color}* {color:#14892c}Nodes 1,3,4,5,6,7,8,10{color} 5555  {color:#d04437}Node 2{color} 5528  {color:#d04437}Node 9 (saopauloPerf9){color} 5542   *{color:#205081}Pre-PRE-PREPAREs{color}* grep -i "missing PRE-PREPAREs" *.log*  *Node1* Node1.log:0 Node1.log.2017-07-20:87 Node1.log.2017-07-20.1:0 Node1.log.2017-07-20.2:0 Node1.log.2017-07-20.3:0  *Node2* Node2.log:0 Node2.log.2017-07-20:3 Node2.log.2017-07-20.1:0 Node2.log.2017-07-20.2:0 Node2.log.2017-07-20.3:0  *Node3* Node3.log:0 Node3.log.2017-07-20:35 Node3.log.2017-07-20.1:0 Node3.log.2017-07-20.2:0 Node3.log.2017-07-20.3:0  *Node4* Node4.log:0 Node4.log.2017-07-20:0 Node4.log.2017-07-20.1:0 Node4.log.2017-07-20.2:0 Node4.log.2017-07-20.3:0  *Node5* Node5.log:0 Node5.log.2017-07-20:64 Node5.log.2017-07-20.1:0 Node5.log.2017-07-20.2:0 Node5.log.2017-07-20.3:0  *Node6* Node6.log:0 Node6.log.2017-07-20:0 Node6.log.2017-07-20.1:0 Node6.log.2017-07-20.2:0 Node6.log.2017-07-20.3:0  *Node7* Node7.log.2017-07-20:0 Node7.log.2017-07-20.1:0 Node7.log.2017-07-20.2:0 Node7.log.2017-07-20.3:0 raet.log:0  *Node8* seoulPerf8.log:0 seoulPerf8.log.2017-07-20:25 seoulPerf8.log.2017-07-20.1:0 seoulPerf8.log.2017-07-20.2:0 seoulPerf8.log.2017-07-20.3:0  *Node9* saopauloPerf9.log:0 saopauloPerf9.log.2017-07-20:57 saopauloPerf9.log.2017-07-20.1:0 saopauloPerf9.log.2017-07-20.2:0 saopauloPerf9.log.2017-07-20.3:0  *Node10* virginaPerf10.log:0 virginaPerf10.log.2017-07-20:72 virginaPerf10.log.2017-07-20.1:0 virginaPerf10.log.2017-07-20.2:0 virginaPerf10.log.2017-07-20.3:0    ></body> </Action>
<Action id="28775" issue="18241" author="lovesh" type="comment" body="Node2 and Node9 had encountered a KeyError which is resolved in https://github.com/hyperledger/indy-plenum/pull/297" created="2017-07-21 09:26:10.0" updateauthor="lovesh" updated="2017-07-21 09:26:10.0"/>
<Action id="28812" issue="18241" author="alexander.shekhovcov" type="comment" created="2017-07-21 16:59:16.0" updateauthor="alexander.shekhovcov" updated="2017-07-21 16:59:16.0"> <body><! CDATA My findings: * the pool is broken because not only Node2 and Node9 crashed with KeyError but at least Node6 and Node7 crashed too * Node6, Node7 (and maybe some others) finished catchup successfully * Node2, Node9 did not finished catchup because they got enough ledger status messages from others crashed nodes and they stopped their catchup  I am working on solution which increases the quorum for LedgerStatus messages. https://github.com/hyperledger/indy-plenum/pull/299  ></body> </Action>
<Action id="28893" issue="18241" author="alexander.shekhovcov" type="comment" body="*&quot;Node stops the catchup procedure if gets f+1 old LedgerStatus&quot;* INDY-454 was created." created="2017-07-24 13:10:32.0" updateauthor="alexander.shekhovcov" updated="2017-07-24 13:10:32.0"/>
<Action id="28896" issue="18241" author="alexander.shekhovcov" type="comment" created="2017-07-24 13:26:58.0" updateauthor="alexander.shekhovcov" updated="2017-07-24 13:26:58.0"> <body><! CDATA Just summarizing: * there was an issue "Missing pre-prepare hangs 3pc processing" which was fixed Lovesh * during testing Kelly got "KeyError" which was fixed Lovesh too * "KeyError" made the pool in state when INDY-454 happened. INDY-454 was fixed and move to test  So the ticket ready for testing again.   ></body> </Action>
<Action id="29046" issue="18241" author="krw910" type="comment" body="This appears to be working. I am not getting the pre-prepare issues or KeyError. We have other issues around getting a burst of transactions, but this ticket is fixed." created="2017-07-26 15:33:42.0" updateauthor="krw910" updated="2017-07-26 15:33:42.0"/>
