<Action id="59184" issue="39180" author="sergey.khoroshavin" type="comment" created="2019-04-12 07:34:57.0" updateauthor="sergey.khoroshavin" updated="2019-04-12 07:34:57.0"> <body><! CDATA *Problem reason* As in description  *Changes made* If there were no transactions in audit ledger, but some transactions were caught up in other ledger (using legacy method) continue catchup until either: * some transactions appear in audit ledger * no more transactions are caught up  *Versions* indy-node:  1.7.0.dev891   *PR* https://github.com/hyperledger/indy-plenum/pull/1159  *Covered by tests* https://github.com/skhoroshavin/indy-plenum/blob/64b8c189a72677bfcc8bd2ab4c8e02ad87236006/plenum/test/audit_ledger/test_first_audit_catchup_during_ordering.py  *Risk* Low  *Risk factors* Some tricky failure cases may still get through  *Recommendations for QA* Try the following test case on 25 nodes AWS pool: * install node version that doesn't have audit ledger * start production load test and run in for 3 minutes * stop some non-primary nodes (for example Node10, Node15, Node20) * after 1 minute send a forced upgrade transaction to pool and start stopped nodes * check that pool successfully performs upgrade and continues ordering  ></body> </Action>
<Action id="59239" issue="39180" author="sergey.khoroshavin" type="comment" created="2019-04-15 14:57:46.0" updateauthor="sergey.khoroshavin" updated="2019-04-15 14:57:46.0"> <body><! CDATA During tests some of nodes crashed with following traceback (including some of those NOT in lagged list): {code} Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py", line 104, in process_catchup_rep Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     num_processed = self._process_catchup_txns(txns_already_rcvd_in_catchup) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py", line 338, in _process_catchup_txns Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     self._add_txn(txn) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py", line 418, in _add_txn Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     self._provider.notify_transaction_added_to_ledger(self._ledger_id, txn) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/node_catchup_data.py", line 58, in notify_transaction_added_to_ledger Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     info.postTxnAddedToLedgerClbk(ledger_id, txn) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 2188, in postTxnFromCatchupAddedToLedger Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     self.updateSeqNoMap( txn , ledger_id) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 3350, in updateSeqNoMap Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     for txn in committedTxns) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/plenum/persistence/req_id_to_txn.py", line 30, in addBatch Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     self._keyValueStorage.setBatch(payload) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py", line 126, in setBatch Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :     b.put(key, value) Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "rocksdb/_rocksdb.pyx", line 1347, in rocksdb._rocksdb.WriteBatch.put Apr 15 14:08:01 londonQALive13.qatest.evernym.com env 1049 :   File "rocksdb/_rocksdb.pyx", line 103, in rocksdb._rocksdb.bytes_to_slice {code}  ></body> </Action>
<Action id="59241" issue="39180" author="vladimirwork" type="comment" created="2019-04-15 15:23:06.0" updateauthor="vladimirwork" updated="2019-04-15 15:23:06.0"> <body><! CDATA Build Info: indy-node 1.7.0~dev892  Steps to Reproduce: 1. Install node version that doesn't have audit ledger. 2. Start production load test and run in for 3 minutes. 3. Stop some non-primary nodes (for example Node10, Node15, Node20). 4. After 1 minute send a forced upgrade transaction to pool and start stopped nodes.  Actual Results: 15th and 20th nodes don't catch up and order. There is a stacktrace above at 13th node.  Logs and metrics: ev@evernymr33:logs/INDY-2047_15_04_2019_logs.tar.gz ev@evernymr33:logs/INDY-2047_15_04_2019_metrics.tar.gz  ></body> </Action>
<Action id="59242" issue="39180" author="sergey.khoroshavin" type="comment" body="Some nodes failed to reach consensus after upgrade due to unrelated bug, created a separate issue INDY-2055" created="2019-04-15 15:48:04.0" updateauthor="sergey.khoroshavin" updated="2019-04-15 15:50:11.0"/>
<Action id="59311" issue="39180" author="ashcherbakov" type="comment" body="Fixed in Build: indy-node 1.7.0.dev895Â " created="2019-04-18 07:32:58.0" updateauthor="ashcherbakov" updated="2019-04-18 07:32:58.0"/>
<Action id="59318" issue="39180" author="vladimirwork" type="comment" created="2019-04-18 10:20:09.0" updateauthor="vladimirwork" updated="2019-04-18 10:20:09.0"> <body><! CDATA Build Info: indy-node 1.7.0~dev895  Steps to Reproduce: 1. Install node version that doesn't have audit ledger. 2. Start production load test and run in for 3 minutes. 3. Stop some non-primary nodes (for example Node10, Node15, Node20). 4. After 1 minute send a forced upgrade transaction to pool and start stopped nodes.  Actual Results: All stopped nodes don't catch up and order under load but catch up and order after load stopping.  ></body> </Action>
