<Action id="50614" issue="33296" author="spivachuk" type="comment" created="2018-09-18 16:56:35.0" updateauthor="spivachuk" updated="2018-09-18 16:56:35.0"> <body><! CDATA Node25 stopped ordering because it detected an *incorrect state trie root hash* at 2018-08-28 12:32:27 when it tried to apply the first 3PC-batch after the huge catch-up. However, Node25 was still able to catch up missed transactions because catch-up process does not verify matching of states.  The merkle tree root hashes before applying the transaction that caused state mismatch and after applying it (before reverting) were the same on Node25 as on other nodes. But the state trie root hashes on Node 25 were different from the hashes on other nodes both before reverting and after reverting the transaction.  There were no logged state root hashes on Node25 before attempts to apply 3PC-batches that were done after the huge catch-up because state root hashes are logged during ordering only _but not during or after a catch-up and not on a node initialization_.  However, we see in logs of Node25 that there were multiple attempts to catch up the domain ledger. They were interrupted by multiple restarts of this node until the last attempt succeeded at 2018-08-27 22:57:18 to catch up the domain ledger till its end. We don't have systemd journal of this node for the period of these restarts, so we don't know the reasons for these restarts. During the successfully completed last attempt of catch-up the node caught up ~162K transactions of ~633K total number caught up. So other transactions were caught up during the previous interrupted attempts of catch-up. Not all the shutdowns of the node on these restarts were graceful, some of them were abnormal as it is seen from the logs (for these abnormal shutdowns there are no log messages about the looper shutdown). We suppose that for some transaction added to the ledger (during a catch-up) the state was not updated due to an interruption by such an abnormal shutdown of the node. But to determine the cause of the issue for sure, we have added logging of states to node initialization and catch-up and will re-test the case.  Logging of states have been enhanced in scope of the following PRs: -  https://github.com/hyperledger/indy-plenum/pull/916  -  https://github.com/hyperledger/indy-node/pull/947   The version containing these changes: - indy-node 1.6.608-master - indy plenum 1.6.543-master  ></body> </Action>
<Action id="50923" issue="33296" author="ozheregelya" type="comment" created="2018-09-21 10:39:49.0" updateauthor="ozheregelya" updated="2018-09-21 11:01:00.0"> <body><! CDATA *Environment:* indy-node 1.6.608 indy plenum 1.6.543  *Steps to Reproduce:* 1. Setup the pool of 24 nodes. 2. Run load test to get ~600K txns in domain ledger. 3. Add 25th node. ~ First time node was added by mistake with wrong configuration. After that it was demoted from the pool, stopped cleaned up, initialized again with right configuration and promoted back. 4. Wait for the end of catch up. => Node completed catch up successfully. All nodes haveÂ 607129 in the ledger. 5. Write 1 NYM using indy-cli. => NYM was written on all nodes exclude 25th. 6. Run load test.  *Actual Results:* NYM was not written by 25th node after first catch up (step 5). After load test and second catch up of 25th node all nodes exclude 25th have 616280 txns. 25th node has 613330.  *Expected Result:* Node should write after catch up.  Logs and metrics: s3://qanodelogs/INDY-1657-additional-logging/NodeXX/ To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/INDY-1657-additional-logging/ /home/ev/logs/INDY-1657-additional-logging/  ></body> </Action>
<Action id="50997" issue="33296" author="spivachuk" type="comment" created="2018-09-22 21:38:35.0" updateauthor="spivachuk" updated="2018-09-22 21:55:51.0"> <body><! CDATA After retesting the scenario with the enhanced set of cases when the state trie root hash is logged, we have got the same symptoms as when testing the scenario for the first time. Now we see that the reasons of the spontaneous restarts of the node during a huge catch-up are the node process falls caused by the Python *call stack overflow*. The state root of Node25 diverged from the state root of other nodes in the pool after the first such fall of Node25. All these call stack overflows were caused by a big depth of {{LedgerManager._processCatchupReplies}} recursion but the overflows themselves occurred deeper - in functions called from {{DomainRequestHandler.updateState}}. Thus the state was not updated completely at those moments.  The call stack overflows can be seen in the systemd journal of Node25. There are error tracebacks for all the node process falls. For all these tracebacks *the call stack depth is 1000* which is the *default* value of *the maximum depth of the Python interpreter stack*.  For example, here is the traceback of the first fall of the node process on Node25:  {code} Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 : Traceback (most recent call last): Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/bin/start_indy_node", line 19, in <module> Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     client_ip=sys.argv 4 , client_port=int(sys.argv 5 )) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py", line 54, in run_node Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     looper.run()  ...  Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 516, in processCatchupRep Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     ledgerId, ledger, txns_already_rcvd_in_catchup) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 554, in _processCatchupReplies Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     catchUpReplies toBeProcessed: ) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 554, in _processCatchupReplies Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     catchUpReplies toBeProcessed: ) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 554, in _processCatchupReplies Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     catchUpReplies toBeProcessed: )  ...  Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 554, in _processCatchupReplies Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     catchUpReplies toBeProcessed: ) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 554, in _processCatchupReplies Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     catchUpReplies toBeProcessed: ) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 550, in _processCatchupReplies Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     ledgerInfo, txn) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py", line 571, in _add_txn Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     ledgerInfo.postTxnAddedToLedgerClbk(ledgerId, txn) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 1985, in postTxnFromCatchupAddedToLedger Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     rh.updateState( txn , isCommitted=True) Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/local/lib/python3.5/dist-packages/plenum/server/domain_req_handler.py", line 69, in updateState Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     self._updateStateWithSingleTxn(txn, isCommitted=isCommitted)  ...  Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :   File "/usr/lib/python3.5/_weakrefset.py", line 75, in __contains__ Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env 2223 :     return wr in self.data {code}  The recursive calls of {{LedgerManager._processCatchupReplies}} took the most part of the call stack.  Created INDY-1711 about this issue.  ></body> </Action>
