<Action id="39574" issue="27270" author="anikitindsr" type="comment" created="2018-02-02 09:53:50.0" updateauthor="anikitindsr" updated="2018-02-02 09:53:50.0"> <body><! CDATA The fateful events that led to the broken pool was: # At 2018-01-26 15:19:51 node ev1 was started. # After timeout (1 minute) it sent INSTANCE_CHANGE from 0 to 1 viewNo message with reason "Primary of master protocol instance disconnected". It's "viewNo 0" logic. # Node was connected with 13 other nodes, than "number of nodes" was 14, "f"  4. Therefore, quorum for instance change was 14 - 4 = 10. # "ev1" received 8 instance change messages from other nodes and "ev1" as self. Now, we have 9 instance change messages. # "ev1" received quorum of view change done messages and approved viewNo=0. When new viewNo was applied, queue with instance change message wasn't cleaned. Therefore, viewNo=0 and 9 instance_change messages for the viewNo=1 in queue. # At 2018-01-26 19:17:18 node ev1 selected ev1:0 as primary for the instance 0. # Than pool worked fine, transaction was ordered. # At 2018-01-31 07:17:07 was received 10th instance change from zaValidator and initiated view change from 0 to 1. primaryName set to None. # Therefore was not received view change done messages for view 1, ev1 sent instance change message for change view from 1 to 2. This message was not replied. # Now, ev1 has viewNo = 1 and not participating mode and other node use viewNo=0 and expect that primary node is ev1. # Client request, which was sent to ev1 was propagated but PREPREPARE wasn't sent because ev1 not choose primary for it current viewNo = 1.  What we can do for restore consensus: We propose fast restart for ev1 node (like "systemctl restart indy-node"). When ev1 would restarted, he will ask for CURRENT_STATE from other nodes and when received quorum of VIEW_CHANGE_DONE message set viewNo as 0 and choose ev1 as primary (reset viewNo state)  ></body> </Action>
<Action id="39579" issue="27270" author="mgbailey" type="comment" created="2018-02-02 12:39:38.0" updateauthor="mgbailey" updated="2018-02-02 12:39:38.0"> <body><! CDATA  ~anikitinDSR  After restarting indy-node on ev1, zaValidator is now the primary.  Since this node's status is that it has BLS keys configured locally, but it does not have its BLS keys on the ledger, this is probably a problem. {code:java} indy@sao-sov-p001:/var/log/indy/live$ grep 'selected primary' * ev1.log:2018-02-02 12:30:23,729 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:0 selected primary zaValidator:0 for instance 0 (view 1) ev1.log:2018-02-02 12:30:23,753 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:1 selected primary danube:1 for instance 1 (view 1) ev1.log:2018-02-02 12:30:23,763 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:2 selected primary royal_sovrin:2 for instance 2 (view 1) ev1.log:2018-02-02 12:30:23,776 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:3 selected primary digitalbazaar:3 for instance 3 (view 1) ev1.log:2018-02-02 12:30:23,788 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:4 selected primary OASFCU:4 for instance 4 (view 1) {code}  ></body> </Action>
<Action id="39581" issue="27270" author="ashcherbakov" type="comment" created="2018-02-02 12:43:32.0" updateauthor="ashcherbakov" updated="2018-02-02 12:43:32.0"> <body><! CDATA  ~mgbailey  This is not a problem until a `NYM` txn is sent. So, the pool should be able to process txns from the Pool ledger (NODE txns), that is continue initializing BLS. But can not process any NYM txns, and may get broken once NYM txn is sent.  ></body> </Action>
<Action id="39588" issue="27270" author="mgbailey" type="comment" body="After ev1 restarted, the zaValidator BLS transactions that were previously sent appeared on the ledger.  However, we then tried sending a NYM transaction to the ledger since we think that there are now 11 properly configured validators.  The send NYM failed. Logs of ev1 are attached. " created="2018-02-02 13:51:00.0" updateauthor="mgbailey" updated="2018-02-02 13:51:00.0"/>
<Action id="39597" issue="27270" author="mgbailey" type="comment" created="2018-02-02 16:31:29.0" updateauthor="mgbailey" updated="2018-02-02 16:31:29.0"> <body><! CDATA I also attempted to re-send the ev1 NODE transaction, and that was also not posted.     ></body> </Action>
<Action id="39644" issue="27270" author="anikitindsr" type="comment" created="2018-02-05 07:57:55.0" updateauthor="anikitindsr" updated="2018-02-05 07:57:55.0"> <body><! CDATA Current situation on live pool. It's look like problem, described in INDY-1081. Node ev1 got PREPREPARE with ppseqno 4 for view 1 (1,4) from zaValidator and then requested missing PREPREPARE and PREPARE for previous ppseqno 2, 3 and for 4. All requests from (1, 1) was ordered, but when we get PREPREPARE from zaValidator again, this node was suspected. This issue will be fixed in INDY-1081. For now, restart zaValidator and change view number can help with the live pool.  ></body> </Action>
<Action id="42212" issue="27270" author="ashcherbakov" type="comment" body="We can reopen any that reappear, but all is good on the live pool now." created="2018-03-27 13:48:54.0" updateauthor="ashcherbakov" updated="2018-03-27 13:48:54.0"/>
