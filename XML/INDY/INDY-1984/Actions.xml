<Action id="56869" issue="37382" author="sergey.khoroshavin" type="comment" created="2019-02-11 16:18:20.0" updateauthor="sergey.khoroshavin" updated="2019-02-11 16:18:20.0"> <body><! CDATA Required additional integration *tests scenarios*  *Configuration* * Set primary disconnection timeout to some large value, so that view change is not triggered by primary restarts * Set freshness check interval to 20-30 seconds, so that it will happen during test  *Scenario 1* * Beta and Gamma send InstanceChange for all nodes. * Restart Alpha * Restart Gamma and Delta * Send InstanceChange from Delta for all nodes * At this point Delta will enter view change, while all other nodes continue normal ordering (unless InstanceChanges are persisted) * Ensure that pool is still functional (ordering some transactions in process) * Beta and Gamma send InstanceChange for all nodes again, which will lead to view change by other nodes * Ensure that all nodes have same data and pool is functional (this will most probably fail)  *Scenario 2* * Beta and Gamma send InstanceChange for all nodes. * Restart Alpha (while setting primary disconnection timeout to some large value, so that view change is not triggered) * Restart Gamma and Delta * Send InstanceChange from Delta for all nodes * At this point Delta will enter view change, while all other nodes continue normal ordering (unless InstanceChanges are persisted) * Ensure that pool is still functional (ordering some transactions in process) * Beta send InstanceChange for all nodes again * Restart Alpha and Gamma * Send InstanceChange from Gamma for all nodes * At this point Gamma will enter another view change, leaving two nodes in view change which breaks consensus * Ensure that all nodes have same data and pool is functional (this will most probably fail)   ></body> </Action>
<Action id="56966" issue="37382" author="toktar" type="comment" created="2019-02-13 13:13:57.0" updateauthor="toktar" updated="2019-02-13 14:09:27.0"> <body><! CDATA *PoA:* Instance Change messages are lost after a node restart. As a result, a restarted node can't start view change and will not order batches. This leads to case when a pool can't order because more then f+1 nodes can't order. To solve this problem InstanceChanges will store in a KeyValueStorageType.Rocksdb. InstanceChanges should be store in the follow structure: {code:java} { <view_no>: { <node_name>: {<timestamp>, <reason>}, <node_name>: {<timestamp>, <reason>}, }, ... }{code} +First option.+ Store serialized InstanceChangesVotes in Rocksdb and deserialize it in every case when we need to read value. +Second option.+ Deserialize InstanceChangesVotes from Rocksdb only in a node initialization and save it in an internal structure.  *Implementation.* Add two tests from the comment above. Create InstanceChangeProvider with public functions as in models.InstanceChanges: * add_vote * has_view * has_inst_chng_from * has_quorum *  + remove_votes - to remove votes for all last view before View Change  Implement in InstanceChangeProvider one of options to store InstanceChange messages and to remove messages when the quorum has been reached or InstanceChange was received too long ago. Integrate InstanceChangeProvider in the current system. Change calls models.InstanceChanges methods to calls same InstanceChangeProvider methods.  ></body> </Action>
<Action id="57353" issue="37382" author="toktar" type="comment" created="2019-02-20 12:38:05.0" updateauthor="toktar" updated="2019-02-21 09:59:59.0"> <body><! CDATA *Problem reason:* - Nodes lose InstanceChange messages in restart and can't start a view change.  *Changes:* - Store  InstanceChange messages in nodeStatusDB  *PR:* *  https://github.com/hyperledger/indy-node/pull/1178  *  https://github.com/hyperledger/indy-plenum/pull/1087   *Version:* * indy-node 1.6.811 -master * (indy-plenum 1.6.690 -master)  *Risk factors:* - More than f nodes will start view change and can't finish it.  *Risk:* - Medium  *Test:* *  test_vc_started_in_different_time.py|https://github.com/hyperledger/indy-plenum/pull/1087/files#diff-fe1d1e3ed51dc1314a76d1256ee9aa0e  *  test_vc_finished_when_less_than_quorum_started.py|https://github.com/hyperledger/indy-plenum/pull/1087/files#diff-683019ed07c17671377ff7b2bb70597d   *Recommendations for QA:*  Docker pool for 4 nodes (can scale to more). * Node3, Node4 send InstanceChange with any reason * Restart Node2 * Restart Node3 * Restart Node4 * Node2 send InstanceChange with any reason * Check that pool done view change after some time(a few minutes).  ></body> </Action>
<Action id="57436" issue="37382" author="ozheregelya" type="comment" created="2019-02-21 16:56:31.0" updateauthor="ozheregelya" updated="2019-02-21 16:56:31.0"> <body><! CDATA *Environment:*  indy-node 1.6.819  *Steps to Validate:* 1. Setup the pool. 2. Send n-f-1 instance changes. 3. Restart more than f+1 nodes. 4. Send 1 more instance change.  *Actual Results:* View change was happened.  ></body> </Action>
