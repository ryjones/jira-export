<Action id="52170" issue="33547" author="sergey-shilov" type="comment" created="2018-10-12 15:50:43.0" updateauthor="sergey-shilov" updated="2018-10-12 15:50:43.0"> <body><! CDATA While looking for possibility of monitoring of ZMQ inner queues I found a feature of ZMQ called MonitoredQueue. MonitoredQueue is a kind of ZeroMQ device. In fact it is a combination of three ZMQ sockets that is something like a proxy. Using this proxy you can monitor all incoming/outgoing traffic.  Some docs and examples for that: https://pyzmq.readthedocs.io/en/latest/devices.html https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/pyzmqdevices/monitorqueue.html  In fact this is not that I've been looking for as it can not be used for monitoring of inner ZMQ queues. Also I had a trouble with combination of this proxy with Router socket, I got an issue with one more added ident to a message. But anyway, may be it is useful information for future investigation.  ></body> </Action>
<Action id="52601" issue="33547" author="sergey-shilov" type="comment" created="2018-10-24 16:33:26.0" updateauthor="sergey-shilov" updated="2018-10-25 15:25:48.0"> <body><! CDATA During working on this ticket we did various tests with ZMQ-based sample servers written in C and in Python (libzmq-4.1.4). During these tests we used a sample ZMQ-based client written in Python which emulates libindy's behaviour.  *Results* * There is no big difference between C and Python servers in memory consumption under equal load (the server written in Python consumes slightly more memory, but it's expected) * ZMQ is a very big memory consumer as: ** it creates a separate queue for each connection ** it reads from these queues in a Round-Robin manner (when recv() for listener is called), so these queues are kept in memory for a long time in case of big number of connections (note our receive quotas for client stack) ** these inner queues are not cleaned if corresponding connection socket has been closed from the client side (for example by time out), such queue is kept in memory until the last packet is read ** under high load the memory grows fast, but it is freed very slow even after load stop, looks like there is some additional memory management ** during one of long tests of high load with C-server when clients throughput exceeds server's throughput (40 reqs/seq vs 25 reqs/sec) memory usage grew up to ~12GB VSZ and ~5.5GB RSS. Then load was stopped and we let server read all messages from queues. When no messages left (i.e. non-blocking recv() finished with EAGAIN) we observed that the C-server still keeps ~4GB VSZ and ~1.5GB RSS. After some idle time and small and short load the C-server abruptly reduced memory consumption to ~250MB VSZ and 64MB RSS. * Monitor does not affect memory consumption.  *Conclusion* Seems like ZMQ library is not designed for high load with big numbers of connections. The inner queues management and such long keeping of memory by ZMQ library may cause very large memory consumption under long high load. For now we consider restart of client stack depending on memory consumption by indy-node process and reducing of allowed number of connections (INDY-1777), these options are under research now.  ></body> </Action>
