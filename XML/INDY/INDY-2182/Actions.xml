<Action id="62076" issue="41378" author="anikitindsr" type="comment" created="2019-07-24 11:30:00.0" updateauthor="anikitindsr" updated="2019-07-24 11:30:00.0"> <body><! CDATA Reasons: * need to remove memory_profiler from GET_VALIDATOR_INFO cmd output  Changes: * Output of GET_VALIDATOR_INFO cmd includes only empty value for 'Memory_profiler' field. In the other words, we don't yet collect memory profiler info.  Version: * indy-node: 1.9.0.dev1039  Recommendation for QA: * setup pool with trace logs * send GET_VALIDATOR_INFO cmd * check logs for time, spending for processing GET_VALIDATOR_INFO action (string between 'Transaction .* with type 119 started' and 'Transaction .* with type 119 finished') * Also, take a look at the nightly builds for `test_vc.py` fails  ></body> </Action>
<Action id="62100" issue="41378" author="esplinr" type="comment" created="2019-07-24 21:12:48.0" updateauthor="esplinr" updated="2019-07-24 21:12:48.0"> <body><! CDATA I'm glad that the team looked into INDY-2181 and has a good theory for what the cause is.  However, we shouldn't be hasty in deciding on the right fix. Specifically, there are trade-offs to this change that we should discuss as a group. I am nervous about losing this visibility into how the production network functions. Has anyone checked with  ~dfarns  for his monitoring of the Sovrin Network?  I am also nervous about our team making unilateral changes without collaborating with other maintainers of Indy networks.  Some alternative approaches that should be considered: 1. Stop returning this data. 2. Do nothing, i.e. allow some validator-info responses to be slow. 3. Have a separate command that collects and returns this data at an appropriate interval.  ></body> </Action>
<Action id="62105" issue="41378" author="ashcherbakov" type="comment" created="2019-07-25 08:17:02.0" updateauthor="ashcherbakov" updated="2019-07-25 08:17:02.0"> <body><! CDATA  ~esplinr  I totally agree that this should not be a silent change, and it needs to be communicated. That's why we started the discussion with  ~lbendixsen  and  ~mgbailey . We will talk with Dan as well.  A couple of things about the `Memory_profiler`:  1) This field is *not* for showing a total RAM. We have another fields for this: `_RAM_all_free_`, `_RAM_used_by_node_`. *These RAM fields are not removed* *and can (and should) be used for memory analysis.* HDD fields were also *not* removed. Memory_profiler fields are used for showing sizes of low level python objects used by the application. Understanding and using this info requires good understanding of code internals. 2) This field was added there as rather a debug information when we were hunting OOM issues. It wasn't a request from the customer, and it wasn't announced to anyone. So, I'll be really surprised if anyone actively uses this info. So, from my point of view, the issue is that we forgot to remove the field that time. 3) The concern with having this field is not minor at all. Keeping the field can lead to - This stops node from doing anything for 6 secs on a rather small system tests environment. I would expect the delay can be more significant if we have a production Net running for days and weeks. - Although only specific roles can run the command, this is an easy to use attack vector. One can shot nodes one by one making the pool unavailable at all, or leading to constant view changes (plus remember that we still don't have PBFT View change in stable).   - It can lead to node disconnections.  As a conclusion, I agree that this is a trade off, and it must be discussed with other Maintainers, but taking into account the history of this field and reasoning, it's safer to remove it in the next Release. We can discuss if we need an Option 3 in further releases if it's needed.  ></body> </Action>
<Action id="62116" issue="41378" author="esplinr" type="comment" created="2019-07-25 15:19:21.0" updateauthor="esplinr" updated="2019-07-25 15:19:21.0"> <body><! CDATA Thank you Alex for the explanation about memory_profiler. You are correct that RAM_all_free and RAM_used_by_node are the fields I was worried about losing.  Also, thank you for the explanation about why collecting memory_profiler data is such a problem. I agree that it shouldn't be available in production. Do we want to keep the code for generating the report on the python objects in debug builds of Indy Node for the next time we need to do memory profiling?  I hadn't noticed that the issue doesn't discuss the HDD fields. Those were only mentioned in the chat. I'm glad we decided to keep them.  ></body> </Action>
<Action id="62118" issue="41378" author="mgbailey" type="comment" body="Could this be related to INDY-2175 as well?" created="2019-07-25 15:25:16.0" updateauthor="mgbailey" updated="2019-07-25 15:25:16.0"/>
<Action id="62120" issue="41378" author="ashcherbakov" type="comment" created="2019-07-25 15:33:13.0" updateauthor="ashcherbakov" updated="2019-07-25 15:33:13.0"> <body><! CDATA  ~mgbailey  In theory yes, it can be the reason of INDY-2175. But we need to investigate the logs to say for sure.  ></body> </Action>
<Action id="62123" issue="41378" author="lbendixsen" type="comment" body="Thanks to all for the comments and explanations.  I am in agreement with this fix and I appreciate being consulted to make sure we do the right thing with items of this nature." created="2019-07-25 17:10:27.0" updateauthor="lbendixsen" updated="2019-07-25 17:10:27.0"/>
<Action id="62166" issue="41378" author="ashcherbakov" type="comment" created="2019-07-26 12:56:39.0" updateauthor="ashcherbakov" updated="2019-07-26 12:56:39.0"> <body><! CDATA GET_VALIDATOR_INFO execution time changed from 6 secs to 0.1-0.2 secs.  It also fixed the mentioned system test from INDY-2181 (at least the part affected by this issue).  ></body> </Action>
