<Action id="28381" issue="19249" author="ashcherbakov" type="comment" created="2017-07-14 09:50:08.0" updateauthor="ashcherbakov" updated="2017-07-14 09:50:08.0"> <body><! CDATA There is a possible problem with re-starting nodes if we have just 4 nodes. It will be fixed soon in the scope of INDY-389.  We don't 'elect' primary now, we 'select' it in round-robin style. So, initially the Primary for master (instance 0) is Node1, for instance 1 is Node2, for instance 2 is Node3, etc. When a new Node comes online, all other nodes propagate the existing Primary to it (so, no new election, the new node will just get know the primary). Currently a consensus of n-f is required for a newly added node to get the existing Primary. So, if you had 4 nodes, send some txns, then one node is down, and then it is back up, then there is chance that it will not get the consensus and will not get the Primary.   ></body> </Action>
<Action id="28421" issue="19249" author="mzk-vct" type="comment" created="2017-07-14 16:34:38.0" updateauthor="mzk-vct" updated="2017-07-14 16:34:38.0"> <body><! CDATA Cannot reproduce. Seems that it is fixed by https://jira.hyperledger.org/browse/INDY-389 Builds: node==0.4.35 plenum==0.4.47  ></body> </Action>
<Action id="28441" issue="19249" author="slafranca" type="comment" created="2017-07-14 21:44:26.0" updateauthor="slafranca" updated="2017-07-14 22:08:11.0"> <body><! CDATA I had 7 nodes.  Node 1 was the primary node.  I stopped the service on node 1, node 2 became the primary node.  I ran "grep "selected primary" *.log" on node 2 and I see:  {code:java} Node2.log:2017-07-14 17:20:04,534 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node2:0 selected primary Node1:0 for instance 0 (view 0) Node2.log:2017-07-14 17:20:04,535 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node2:1 selected primary Node2:1 for instance 1 (view 0) Node2.log:2017-07-14 17:20:04,536 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node2:2 selected primary Node3:2 for instance 2 (view 0) Node2.log:2017-07-14 20:28:20,206 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node2:0 selected primary Node2:0 for instance 0 (view 1) Node2.log:2017-07-14 20:28:20,208 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node2:1 selected primary Node3:1 for instance 1 (view 1) Node2.log:2017-07-14 20:28:20,209 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node2:2 selected primary Node4:2 for instance 2 (view 1) {code} I brought node 1 back online and ran the same grep command (I also searched for 'primary is') on node 1 and there are no new entries.  The output was the same before and after the service was stopped.  I see: {code:java} Node1.log:2017-07-14 17:19:39,906 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node1:0 selected primary Node1:0 for instance 0 (view 0) Node1.log:2017-07-14 17:19:39,907 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node1:1 selected primary Node2:1 for instance 1 (view 0) Node1.log:2017-07-14 17:19:39,908 | DISPLAY  | primary_selector.py  ( 276) | _startSelection | Node1:2 selected primary Node3:2 for instance 2 (view 0) {code} After node 1 was back online, I shut node 2 down and sent a transaction to the pool.  The transaction did not get processed.  I started the service on node 2.  When node 2 came online, the transaction I sent before I restarted the service on node 2 was processed and node 2 was still the primary.  +Expected results:+ * I expected the other nodes to propagate the the log entries for the existing primary node back to node 1 when it came online, I saw nothing in the logs that would indicate this happened * After stopping and starting node 1, then stopping node 2.  I expected the pool to process transactions and node 3 would be the primary node.  Instead, the cli shows the following message: {code:java} Remote Node2C is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings {code}  * While node 2 was down, I expected the transaction I sent to get processed.   I also expected the status message shown above to keep scrolling in the client view until node 2 came back online.  Until node 2 came back online, I could not process transactions.  It appears like the pool will only trigger one view change.  It didn't matter which node I dropped, node 2 remained as the primary node.   ></body> </Action>
<Action id="28458" issue="19249" author="krw910" type="comment" created="2017-07-16 03:33:30.0" updateauthor="krw910" updated="2017-07-16 03:33:30.0"> <body><! CDATA Debug logs from each of the 10 nodes are attached. I had been deleting logs between test runs and on this last one Node5 has a much larger log file. The time stamps seem to line up so I am not sure why it is so much larger.  ^LogsSet_Debug_ServiceIssue.7z    We are seeing some strange behvior with view change where the pool can stop working when node become disconnected in some fashion. I have reproduced the pool failing by both stopping the node service and both just shutting off the ports. My concern here being that we already know of 3 Steward nodes that have been experiencing random disconnects and reconnects. What I am going to describe below is a consistent way to see the pool stop working, but it probably is not the only way.  I have done this 3 times and every time I get the pool to stop working unless every nodes service is restarted in the pool.  *Version* indy-plenum=0.4.50 indy-anoncreds= 0.4.15 indy-node= 0.4.36  *Legend* :0 = Primary Instance :1 = Secondary Instance :2 = Next Instance :3 = Next Instance  The larger the pool the more instances.  It appears that when the pool starts up* it sets the instance order based off the order of nodes in the pool_transactions_sandbox(or live) ledger*. So when the pool starts or if the entire pool is restarted Node1 is always the primary node (Node1:0) and node 2 is always the secondary instance (Node2:1) and so on depending on the size of the pool.  With that background here is what we have seen in testing.  *7 Node Pool* With a 7 node pool we should be able to tolerate two nodes in a failed state. We found the following issue Pool starts with Node1:0 (view 0) Node2:1 (view 0) Node3:2 (view 0)  We stopped the service on Node1 which was the primary node. A view change happened showing: Node2:0 (view 1) Node3:1 (view 1) Node4:2 (view 1)  You could still send transactions. Since with 7 nodes you should be able to lose 2 nodes we shut down the service on Node2. A new view change did not occur. We expected to see: Node3:0 (view 2) Node4:1 (view 2) Node5:2 (view 2)  *{color:#d04437}Issues{color}* With Node1 and Node2 down the pool stopped accepting transactions. Until Node2 came back on line the pool will not accept any transactions even with 6 of the 7 nodes running once we started Node1 again and left Node2 off.   --------------------------------------------  *10 Node Pool* With a 10 node pool chasing the primary like in the 7 node scenario seem to work just fine. The view changes kept happening as long as you still had 7 out of the 10 nodes running which is what is expected. We would shut down Node1, then primary Node2, then primary Node3, then bring back up Node1, then shut down primary Node4, then bring back up Node2, and so on always keeping 7 nodes active.  *10 nodes show 4 instances* Node1:0 (view 0) Node2:1 (view 0) Node3:2 (view 0) Node4:3 (view 0)  *{color:#d04437}Failures{color}* If you restart the entire pool for some reason you will not be able to perform any transactions until the primary node comes online. After I had been running a few transactions I stopped the entire pool. I started with Node10 and brought the nodes online one node at a time in reverse order. Once I had 7 out of 10 nodes running (Node10 - Node4) I expected to be able to start sending transactions. I was unable to send any transactions. I then started Node3 and sent transactions with nothing going through so I had 8 out of 10 nodes running. I then started Node2 and sent transactions with nothing going through so I now had 9 out of 10 nodes. It was not until I started Node1 that transactions went through and once Node1 started you could see in the CLI all the transactions I had sent all the sudden went through like they were waiting in a queue.  If you stop the secondary node then the primary node the pool will sometimes stop accepting transactions and other times if you wait about 30 seconds a view change would happen and transactions would go through. *{color:#d04437} Here is how I got the entire pool to go down{color}* Started with the following in a 10 node setup Node1:0 (view 0) Node2:1 (view 0) Node3:2 (view 0) Node4:3 (view 0)  Remember the nodes change in a round robin fasion. In the following steps you can *+{color:#d04437}either stop the sovrin-node service or block the node ports 9701-9799{color}+* *Either way as long as there is a disconnect*  *Step 1* - Node2:1 (view 0) -- Stop this node and transactions still go through *Step 2* - Node1:0 (view 0) -- Stop this node and transactions still go through as well as a view change  View Change Node3:0 (view 1) Node4:1 (view 1) Node5:2 (view 1) Node6:3 (view 1)  *Step 3* - Node3:0 (view 1) -- Stop this node and transactions still go through as well as a view change  View Change Node4:0 (view 2) Node5:1 (view 2) Node6:2 (view 2) Node7:3 (view 2)  *Step 4* - Node1 -- Start and searched for "primary is", but it still showed that Node1:0 was the primary from (view 0) *Step 5* - Node2 -- Start and searched for "primary is", but it still showed that Node1:0 was the primary from (view 0) *Step 6* - Node5:1 (view 2) -- Stop this node and the entire pool stopped working.  After step 6 *{color:#d04437}the only recover was to restart the entire pool{color}* which set the view back to the following and *{color:#d04437}transactions would not occur until Node1 was up and running{color}* Node1:0 (view 0) Node2:1 (view 0) Node3:2 (view 0) Node4:3 (view 0)    ></body> </Action>
<Action id="28480" issue="19249" author="lovesh" type="comment" body="Same cause as INDY-430" created="2017-07-17 08:55:30.0" updateauthor="lovesh" updated="2017-07-17 08:55:30.0"/>
<Action id="28488" issue="19249" author="lovesh" type="comment" body="PR, https://github.com/hyperledger/indy-plenum/pull/283" created="2017-07-17 10:55:28.0" updateauthor="lovesh" updated="2017-07-17 10:55:28.0"/>
<Action id="28625" issue="19249" author="krw910" type="comment" body="Blocked by INDY438. I am unable to throw a sufficant load at the pool until this ticket has been addressed. Lovesh is looking at the pool." created="2017-07-18 21:37:34.0" updateauthor="krw910" updated="2017-07-18 21:37:34.0"/>
<Action id="28932" issue="19249" author="slafranca" type="comment" body="I tested this with node version 0.4.61.  The nodes processed the transaction while node 1 went down.  When node 1 came back online, the logs were updated and the transaction was processed on node 1.  Each time I stopped the service on the primary node, a different node was promoted.  The logs appear to contain the data about the new primary and backup nodes.  The transaction logs show the NYM transaction was processed as well.  As far as I can tell, the expectation listed in my comments above are met and the bug has been fixed." created="2017-07-24 21:08:33.0" updateauthor="slafranca" updated="2017-07-24 21:08:33.0"/>
