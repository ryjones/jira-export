<Action id="42247" issue="28802" author="dsurnin" type="comment" created="2018-03-28 13:13:25.0" updateauthor="dsurnin" updated="2018-03-28 13:15:03.0"> <body><! CDATA problem nym were added to ledger. it was checked later with additional get command. be default sdk has 100 sec timeout, but adding this nym took 2.5 mins, so it was expected behavior.  there were fix in sdk that waits REQACK for command for 10 sec, if REQACK received sdk will wait longer. https://github.com/hyperledger/indy-sdk/pull/608  ></body> </Action>
<Action id="42402" issue="28802" author="ckochenower" type="comment" created="2018-03-30 20:39:27.0" updateauthor="ckochenower" updated="2018-03-30 20:41:05.0"> <body><! CDATA  ~sergey.khoroshavin  - You now have permission to create AWS spot instances appropriately sized to process logs. Please login to your home directory on ssh01.corp.evernym.com and read the awscli.howto document in your home directory.  A change had to be made to sshuttle-helper today to get all of this to work. Please pull the latest changes if you have already cloned the evernym/sshuttle-helper project. {code} git clone https://github.com/evernym/sshuttle-helper cd ./sshuttle-helper ./sshuttle-helper -u sergeykhoroshavin {code}  Evernym Technical Enablement Jira issue tracking this effort: https://evernym.atlassian.net/browse/TE-75  ></body> </Action>
<Action id="43044" issue="28802" author="ashcherbakov" type="comment" created="2018-04-17 08:12:33.0" updateauthor="ashcherbakov" updated="2018-04-17 08:12:33.0"> <body><! CDATA  ~dsurnin   Is it correct that sometimes pool processed requests too long (more than 100 seconds) because of a flood of MessageRequets/Responses? Can you please provide more detail?  ></body> </Action>
<Action id="43096" issue="28802" author="dsurnin" type="comment" created="2018-04-18 12:27:12.0" updateauthor="dsurnin" updated="2018-04-18 12:27:12.0"> <body><! CDATA According to logs there lots of message requests and nodes most of the time generates message responses. It could lead to performance degradation. I cannot tell for sure if this the only reason.  ></body> </Action>
<Action id="43393" issue="28802" author="toktar" type="comment" body="When the node is behind, it requests preparare and preprepare for all lost transactions. It is necessary to change this logic." created="2018-04-25 10:56:03.0" updateauthor="toktar" updated="2018-04-25 10:56:03.0"/>
<Action id="43548" issue="28802" author="toktar" type="comment" created="2018-04-27 13:28:52.0" updateauthor="toktar" updated="2018-04-27 13:48:38.0"> <body><! CDATA In the procedure of exchanging messages requests with PREPARE and PREPREPARE, the following problems were identified: - When node receive pre-prepare with ppSeqNo greater than its have, it send MessageRequest with prepere and preprepare to all nodes. - Node send MessageRequest with prepere and preprepare to nodes in range  self.ppSeqNo, receivedPpSeqNo+1  - Sending message requests for every case when the order of obtaining transactions was violated with respect to their sending.  Ways for solve problems when recieved ppSeqNo greater than on the current node: - When node recieves *pre-prepare* message, sends message request only with pre-prepare and only for the primary node with 15 sec timeout. - When node recieves *prepare* message, doesn't do anything. - When node recieves *commit* message, doesn't do anything. - When node has *quorum* *of commits*, sends message request only with pre-prepare and only for the primary node.  The general idea of this logical changes in the decrease numbers of request messages, when node don't interested in response.  ></body> </Action>
<Action id="44076" issue="28802" author="toktar" type="comment" body="PR: https://github.com/hyperledger/indy-plenum/pull/664" created="2018-05-08 15:31:37.0" updateauthor="toktar" updated="2018-05-08 15:31:37.0"/>
<Action id="44726" issue="28802" author="toktar" type="comment" created="2018-05-21 13:05:28.0" updateauthor="toktar" updated="2018-05-21 13:05:28.0"> <body><! CDATA *Problem reason:* * Load test fails with timeout error because backup replicas send a lot of message requests.  *Changes:* * remove _setup_for_non_master from catchup * added condition for case when node receives prepare or pre-prepare after catchup. We need in _setup_for_non_master only in this case without catchup. * added logic for queues cleaning in catchup. * moves _setup_for_non_master() from case of receive preprepare with greater ppSeqNo to receive prepare and preprepare * added condition to ask pre-prepare only from primary node * added update watermarks in _setup_for_non_master()  *PR:* *  https://github.com/hyperledger/indy-node/pull/705  *  https://github.com/hyperledger/indy-plenum/pull/664   *Version:* * indy-node 1.3.418-master * indy-plenum 1.2.364-master  *Risk factors:* * Problems with node fall behinds * Probems with catchup  *Risk:* * Medium  *Covered with tests:* *  test_unit_setup_for_non_master.py|https://github.com/hyperledger/indy-plenum/pull/664/files#diff-0716bdd02759d588de5ea35b76663592  *  test_setup_for_non_master.py|https://github.com/hyperledger/indy-plenum/pull/664/files#diff-64cda64c306348a0d3677a3d1e09ffa9   ></body> </Action>
<Action id="45070" issue="28802" author="ozheregelya" type="comment" body="Testing of this ticket will be performed in scope of INDY-1367 because of current problems with load testing (INDY-1365)." created="2018-05-24 15:36:32.0" updateauthor="ozheregelya" updated="2018-05-24 15:36:32.0"/>
