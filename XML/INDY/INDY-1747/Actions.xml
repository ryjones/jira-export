<Action id="53192" issue="34563" author="ashcherbakov" type="comment" created="2018-11-08 07:53:07.0" updateauthor="ashcherbakov" updated="2018-11-09 11:30:23.0"> <body><! CDATA The main suspicion here is that it's caused by GC calls which take more and more time.  *Ideas for investigation:* * Add GC-related metrics and track the number of objects * Run GC in debug mode * Try to disable GC (reference one) `{{gc.disable()}}` and check memory and performance * Call `{{sys._debugmallocstats()}}` to output more statistics  ></body> </Action>
<Action id="53275" issue="34563" author="sergey.khoroshavin" type="comment" created="2018-11-09 16:33:12.0" updateauthor="sergey.khoroshavin" updated="2018-11-09 16:33:12.0"> <body><! CDATA *PoA* * Add GC-related metrics (actually done) * Run load test with metrics enabled (also done - number of objects tracked by GC is steadily increasing) * Write small experimental scripts to figure out GC behavior ** Check growth patterns of _gc.garbage_ ** Check _sys._debugmallocstats()_ * add more GC-related metrics to plenum as a result of experiments on small scripts and run more load tests in docker * check most popular object types in _gc.get_objects()_ and _gc.garbage_ lists * run load test in docker with some GC disabled on some nodes to check if we really have a problem with cyclic references   ></body> </Action>
<Action id="53474" issue="34563" author="sergey.khoroshavin" type="comment" created="2018-11-15 09:11:50.0" updateauthor="sergey.khoroshavin" updated="2018-11-15 09:11:50.0"> <body><! CDATA Load test in docker with additional metrics and statistics of most popular object types:  !Screenshot_2018-11-15_10-08-11.png|thumbnail!   {code} 207c4b5a2e87f:/home/indy# cat /var/log/indy/sandbox/Node2.log | grep garbage -A 2 2018-11-14 17:26:58,691|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 17:26:58,691|INFO|node.py|    <class 'function'>: 14998 2018-11-14 17:26:58,692|INFO|node.py|    <class 'dict'>: 11464 -- 2018-11-14 18:26:58,947|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 18:26:58,947|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 27795 2018-11-14 18:26:58,947|INFO|node.py|    <class 'dict'>: 15156 -- 2018-11-14 19:26:59,264|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 19:26:59,265|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 30112 2018-11-14 19:26:59,265|INFO|node.py|    <class 'dict'>: 15719 -- 2018-11-14 20:26:59,546|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 20:26:59,547|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 55246 2018-11-14 20:26:59,547|INFO|node.py|    <class 'dict'>: 15551 -- 2018-11-14 21:26:59,863|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 21:26:59,863|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 87169 2018-11-14 21:26:59,863|INFO|node.py|    <class 'function'>: 14998 -- 2018-11-14 22:27:00,214|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 22:27:00,215|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 90586 2018-11-14 22:27:00,215|INFO|node.py|    <class 'dict'>: 15098 -- 2018-11-14 23:27:00,579|INFO|node.py|Top objects tracked by garbage collector: 2018-11-14 23:27:00,580|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 64413 2018-11-14 23:27:00,580|INFO|node.py|    <class 'dict'>: 15937 -- 2018-11-15 00:27:00,907|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 00:27:00,908|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 74832 2018-11-15 00:27:00,908|INFO|node.py|    <class 'function'>: 14998 -- 2018-11-15 01:27:01,288|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 01:27:01,289|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 86350 2018-11-15 01:27:01,289|INFO|node.py|    <class 'dict'>: 17518 -- 2018-11-15 02:27:01,642|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 02:27:01,643|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 96967 2018-11-15 02:27:01,644|INFO|node.py|    <class 'function'>: 14998 -- 2018-11-15 03:27:01,955|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 03:27:01,955|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 71280 2018-11-15 03:27:01,955|INFO|node.py|    <class 'dict'>: 16518 -- 2018-11-15 04:27:02,347|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 04:27:02,347|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 156374 2018-11-15 04:27:02,348|INFO|node.py|    <class 'dict'>: 17048 -- 2018-11-15 05:27:02,752|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 05:27:02,752|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 170481 2018-11-15 05:27:02,752|INFO|node.py|    <class 'function'>: 14998 -- 2018-11-15 06:27:03,161|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 06:27:03,162|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 139272 2018-11-15 06:27:03,162|INFO|node.py|    <class 'function'>: 14998 -- 2018-11-15 06:37:03,246|INFO|node.py|Top objects tracked by garbage collector: 2018-11-15 06:37:03,247|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 186832 2018-11-15 06:37:03,247|INFO|node.py|    <class 'function'>: 14998 {code}  ></body> </Action>
<Action id="53475" issue="34563" author="sergey.khoroshavin" type="comment" created="2018-11-15 09:16:44.0" updateauthor="sergey.khoroshavin" updated="2018-11-15 09:16:44.0"> <body><! CDATA It can be seen that: * number of live objects grows steadily * Gen2 collection collects lots of objects, but list of live objects doesn't shrink as much * the only type of object that seem to leak is _orderedset._orderedset.entry_  *Updated PoA* * add metrics for all OrderedSet-based collections (there aren't many) to check if they are cleared * try to use standard OrderedDict instead of 3rd party OrderedSet (written in Cython) to check if OrderedSet is leaky  ></body> </Action>
<Action id="53496" issue="34563" author="sergey.khoroshavin" type="comment" created="2018-11-15 15:15:28.0" updateauthor="sergey.khoroshavin" updated="2018-11-15 15:15:28.0"> <body><! CDATA  !Screenshot from 2018-11-15 16-08-36.png|thumbnail!  It turned out that problem was in _Replica.ordered_ collection which was not cleared until view change, so this is a real memory leak.  However, fix is quite easy, PR is already in progress: https://github.com/hyperledger/indy-plenum/pull/979  ></body> </Action>
<Action id="53604" issue="34563" author="sergey.khoroshavin" type="comment" created="2018-11-19 16:37:26.0" updateauthor="sergey.khoroshavin" updated="2018-11-19 16:43:25.0"> <body><! CDATA  !Screenshot from 2018-11-19 19-20-54.png|thumbnail!  Results of load test after fix was applied. It can be  seen that number of objects tracked by GC is no longer grows, actually now it matches number of requests in queue. Also GC gen2 events no longer become more frequent, and _max_prod_time_ seems stable. So it seems like now there are no leaks in python code under sustainable load.  *Problem description* It turned out that _Replica.ordered_ collection was not cleared until view change. This led to ever increasing number live objects, which in the long run significantly degraded GC performance. Also it was a memory leak  *Solution* _Replica.ordered_ was basically a set containing contiguous ranges of integers. Replacing it with _IntervalList_ with automatically merging ranges reduced storage requirements from O(N) to O(1) (where N is number of pp_seq_nos tracked).  *Versions* Indy-node: 1.6.684 Indy-plenum: 1.6.594  *PR* https://github.com/hyperledger/indy-plenum/pull/979  *Covered by tests* https://github.com/hyperledger/indy-plenum/pull/979/files#diff-4eecd62b97800f596d964dc888f71cd4  *Risk* Low   ></body> </Action>
