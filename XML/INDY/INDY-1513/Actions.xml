<Action id="48940" issue="32141" author="ckochenower" type="comment" created="2018-08-16 15:02:37.0" updateauthor="ckochenower" updated="2018-08-16 16:34:37.0"> <body><! CDATA  ~krw910  - The description/requirements for this ticket made an invalid assumption about replica selection. The assumption being that if the indy-node service was stopped or made unreachable (i.e. block the node port) long enough, a new replica would be selected/elected. This invalid assumption makes scenarios 1 and 2 in the description uninteresting.  By design, nodes that, according to the pool ledger, are validator nodes (services =  "VALIDATOR" ) are eligible to play the role of replica if the indy-node service is running/reachable or not.  The third scenario, however, is interesting, because the cluster's f_value and Count_of_replicas (f_value + 1) changes when demoting one of the backup primaries (non-master replica), but the remaining 9 validator nodes do not elect a replacement.  The following screenshot depicts a 10 node cluster with Node1 as the primary and Node2, Node3 as backup primaries. Node1 and Node3-Node10 are validators and Node2 was demoted using indy-cli. The annotations in the screenshot make it very busy. Following the circled numbers 1 through 3 and the associated colors and arrows will help.  !Screen Shot 2018-08-16 at 8.57.56 AM.png|thumbnail!   Within a minute of demoting Node2, the remaining validator nodes began reporting f_value = 2 and Count_of_replicas = 3, which is good. However, Node4 was dropped as a replica instead of Node2.  Nothing changes if Node2 is both demoted and indy-node is stopped on Node2. In other words, a node can be demoted and effectively taken offline (service unreachable) and validator nodes continue to label it as a backup replica and validator info from Node2 continues to report the node's Mode as "participating".  I generated some load (7200 NYM transactions across 18 clients in batch sizes of 400 NYMs) with no change in state except the domain ledger txn count.  Stopping indy-node on the primary triggered a view change and Node2, even though it was next in line according to validator-info (backup replica 1 - R1 in above screenshot), as expected, was NOT selected as the new primary. Node3 became the new primary and Node4 and Node5 became backup primaries.  !Screen Shot 2018-08-16 at 9.55.05 AM.png|thumbnail!   Restarting indy-node on Node1 (still a validator node) and Node2 (still NOT a validator node) results in the following. Note that Node2's f_value and Count_of_replicas gets updated and it's Mode says "starting" indefinitely.  !Screen Shot 2018-08-16 at 9.59.11 AM.png|thumbnail!   Promoting Node2 using indy-cli while it is in "starting" mode causes the pool's f_value and Count_of_replicas values to increase by 1 on all nodes but Node2. Node2's mode changes to "participating", but it's f_value and Count_of_replicas values remain 2 and 3 respectively until Node2 is restarted. Starting or restarting indy-node on a node AFTER promoting the node (setting it's services to "VALIDATOR") is required in the "promotion workflow". I just thought you might be interested in the observed behavior.  ></body> </Action>
<Action id="48946" issue="32141" author="ckochenower" type="comment" body="I think that INDY-1541 and INDY-1560 cover Scenario 3 well enough and this issue can be closed as &quot;Done&quot;.  ~krw910  - please reopen this issue if you disagree." created="2018-08-16 16:41:35.0" updateauthor="ckochenower" updated="2018-08-16 16:41:35.0"/>
<Action id="48961" issue="32141" author="ckochenower" type="comment" created="2018-08-16 19:30:23.0" updateauthor="ckochenower" updated="2018-08-17 15:22:10.0"> <body><! CDATA I did not write a Chaos test to test round-robin selection of backup primaries when nodes are demoted. It seems to be more of a system test than a Chaos test. The results are depicted in the following screenshot show that demoted nodes are excluded from the round-robin selection of backup primaries.  !Screen Shot 2018-08-16 at 1.12.31 PM.png|thumbnail!   In a nutshell:  1. Start with 10 nodes, f_value = 3, Count_of_replicas = 4, master = Node1, backup replicas are Node2 - Node4. 2. Demote Node10 to cause  f_value = 2, and Count_of_replicas = 3, master = Node1, backup replicas are Node2 - Node3. Node4 is no longer needed as a backup replica. 3. Demote Node4 - Node4 would be the next node in round-robin to be selected as a backup replica. With 9 nodes in the pool, 3 more nodes would have to be demoted to reduce f_value/Count_of_replicas any further. Therefore, we don't have to worry about Count_of_replicas shrinking if we demote Node4. 4. f_value and Count_of_replicas remain 2 and 3 respectively. 5. Force a view change to cause Node2 (next backup primary in round-robin) to become the new master. Node3 and Node5 become the backup replicas. Node4 is excluded, because it has been demoted. 6. Promoting Node4 (including restart of indy-node) does not change the backup primaries. The master is still Node2 and the backup primaries remain Node3 and Node5. 7. Promoting Node10 back into the pool (including restart of indy-node) changes f_value to 3 and Count_of_replicas to 4.  I'm not sure if this is a problem, but Node4 is selected as the third backup primary for Node1 - 9 and Node6 is selected as the third backup primary for Node10. Node10's third backup primary remains Node6 even after restarting indy-node on Node10.  !Screen Shot 2018-08-16 at 1.40.26 PM.png|thumbnail!    ~krw910  - Is this a bug that needs to be reported? I guess I need to read up on the RBFT for the 3rd time to really know for myself.  ></body> </Action>
<Action id="48968" issue="32141" author="ckochenower" type="comment" created="2018-08-16 21:22:01.0" updateauthor="ckochenower" updated="2018-08-17 14:53:58.0"> <body><! CDATA  ~krw910  wanted me to try the following: Step 1 - Demote Node10 !Step1-DemoteNode10.png|thumbnail!  Step 2 - Demote Node4 !Step2-DemoteNode4.png|thumbnail!  Step 3 - Stop indy-node service on Node1 - force a view change !Step3-StopNode1-Force-View-Change.png|thumbnail!  Step 4 - Promote Node4 (including restart of indy-node) !Step4-PromoteNode4-Including-Restart.png|thumbnail!  Step 5 - Promote Node10 (without restart of indy-node) !Step5-PromoteNode10-Without-Restart.png|thumbnail!  Step 6 - Start indy-node service on Node1 (old master) !Step6-StartNode1.png|thumbnail!  Step 7 - Restart indy-node service on Node10 !Step7-RestartNode10.png|thumbnail!  Step 8 - Stop indy-node service on Node3 and Node5 - No visible change. !Step8-StopNode3and5-No-Visable-Change.png|thumbnail!  Step 9 - Stop indy-node service on Node2 - force a view change !Step9-StopNode2-Force-View-Change.png|thumbnail!  Step 10 - Check indy-node services state on each node. Inactive on Node2, Node3, and Node5 !Step10-Check-Indy-Node-Services.png|thumbnail!  Step 11 - Start indy-node service on Node2 !Step11-StartNode2.png|thumbnail!  Step 12 - Start indy-node service on Node3 and Node5 !Step12-StartNode3and5.png|thumbnail!  The problem I see during this test scenario is that consensus cannot be reached who the new master will be. A majority of nodes (6 of 10) cannot come to consensus on who the new master will be. Node3 and 5 were stopped to force R3 (the third replica) to become the next primary and Node2 is stopped to force a view change. Because indy-node is not running on 3 of 10 nodes 6 of the 7 remaining nodes must agree who the new master is. The 7 nodes voting have the following votes:   ||Alias||Vote|| |Node1|Node6| |Node4|Node4| |Node6|Node4| |Node7|Node4| |Node8|Node4| |Node9|Node4| |Node10|Node6|  5 vote Node4 2 vote Node6  Neither are a majority (>= 6 out of 10)  Because a new master cannot be chosen, the pool falls out of consensus.  Starting the indy-node service on Node2 allows the pool to come to consensus on who the new master is and Node4 is elected master.  Restarting indy-node on Node3 and Node5 brings all nodes back into sync with Node4 as the master and Node5, Node6, and Node7 as replica 1, 2, and 3 respectively.   ></body> </Action>
<Action id="49045" issue="32141" author="ckochenower" type="comment" created="2018-08-17 22:03:37.0" updateauthor="ckochenower" updated="2018-08-17 22:03:37.0"> <body><! CDATA I am seeing some unexplained behavior promoting and demoting nodes w/o view change. The primary is never demoted, because that would trigger a view change.  I was repeating a variation of the test in the previous comment when I stumbled on to this behavior.  My notes:  Pool starts with the following configuration: Node1 - Primary and replica :0 Node2 - Replica :1 Node3 - Replica :2 Node4 - Replica :3 Node5-10 -  # Shrink the pool to 7 nodes ## Demote Node10 - this reduces f_value from 3 to 2 and Count_of_replicas from 4 to 3 ## Demote Node9 ## Demote Node8 # Force a view change - Stop indy-node on Node1 # Wait for view change and then start indy-node on Node1 - Note: starting Node1 after Demoting Node7 doesn't make a difference. # Write Nym - Passed # Demote Node7 - this reduces f_value from 2 to 1 and Count_of_replicas from 3 to 2 # Write Nym - Passed # Stop Node3 # Write Nym - Fails with a timeout! - {color:#0747A6}TODO: log Jira issue stating that stopped or demoted backup replicas don't get supplanted until a view change. Stop indy-node on all backup replicas OR demote all backup replicas and try to write a NYM. The transaction will timeout with "LoadClient_0 run_test error ErrorCode.PoolLedgerTimeout"{color} # Start Node3 # Write Nym - Passed # Promote Node7 (without restart of indy-node) # Write Nym - Fails with a timeout! "run_test error ErrorCode.PoolLedgerTimeout" # Restart all nodes in the pool. This will cause a view change. Node1 will be selected with Node2 and Node3 as backup replicas. # Stop indy-node service on Node3 # Write Nym - Fails with timeout! Should pass, because 6 of 7 nodes should participate in consensus. Only Node3 should be down at this point. # Demote Node7 # Write Nym - Passed # Demote Node3 # Write Nym - Failed! # Force a view change - Stop indy-node on Node1 # Write Nym - Failed! # Promote Node7 including restart of indy-node # Write Nym - Failed! # Promote Node8 including restart of indy-node # Write Nym - Failed! # Promote Node9 including restart of indy-node # Write Nym - Passed # Demote Node9 # Write Nym - Passed # Demote Node8 # Write Nym - Passed # Demote Node7 # Write Nym - Failed! # Promote Node3 including indy-node restart # Write Nym -Failed! # Promote Node7 including indy-node restart # Write Nym - Passed # Demote Node7 # Write Nym - Passed # Demote Node1 # Write Nym - Failed! # Promote Node1 # Write Nym - Failed!  The following shows what should be an impossible state. There are only 6 validator nodes (Node7-Node10 are demoted), yet Node1-Node10 say f_value is 2 and Count_of_replicas is 3 when it should be 1 and 2 respectively.  !Screen Shot 2018-08-17 at 3.35.45 PM.png|thumbnail!   Restarting (`systemctl restart indy-node`) indy-node on the master (Node4) to force a view change resulted in the following:  !Screen Shot 2018-08-17 at 3.49.43 PM.png|thumbnail!   Stopping (`systemctl stop indy-node`) indy-node on the master (Node4) to force a view change resulted in the following. Note that some nodes continue to report f_value = 2 and Count_of_replicas = 3  !Screen Shot 2018-08-17 at 3.55.03 PM.png|thumbnail!   A `systemctl restart indy-node` on all nodes but the master (to avoid a view change) does NOT clear up the problem.  !Screen Shot 2018-08-17 at 3.55.03 PM.png|thumbnail!   Restarting the master (Node4) clears up the problem. In other words, Node4 had to be restarted before a Nym could be written to the ledger even though all other participating nodes (Node1-Node6 in the following screenshot) were reporting Node5 as the master.  !Screen Shot 2018-08-17 at 3.55.03 PM.png|thumbnail!   This is technically a split-master. I will see if I can reproduce this behavior next Monday.    ></body> </Action>
