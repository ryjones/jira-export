<Action id="56496" issue="36099" author="sergey.khoroshavin" type="comment" created="2019-02-01 13:46:33.0" updateauthor="sergey.khoroshavin" updated="2019-02-01 13:46:33.0"> <body><! CDATA *Analysis* * >f and <n-f nodes start a view change ** View change will not finish unless more nodes also enter view change ** But since ordering will be stalled state signatures won't be updated so other nodes will send more INSTANCE_CHANGE messages, which should help them reach quorum and also join view change ** There was a concern that by the time other nodes find out that they should also enter view change nodes that already entered view change will time out and start next view change, making this fix futile. However this is not the case, since in order to switch to next view change they need to reach quorum of n-f INSTANCE_CHANGE messages for next view, which is not possible since there are less than n-f nodes stuck in a view change in the first place * <=f nodes start a view change ** View change will not finish unless more nodes also enter view change ** Since ordering won't be stalled state signatures will be updated as usual so other nodes won't have incentive to join view change ** However this is not a problem since ordering will continue as usual ** And if something stops ordering freshness check will kick in, forcing more nodes to join and finally finish view change ** However since there was ordering in progress and a lot of time might have passed between start of view change by different nodes its possible that small number of nodes that started view change earlier might end up with different ledger states  *Plan of Attack* * Add one more reason code for INSTANCE_CHANGE (signatures_are_not_fresh) * Implement periodic check of state signatures timestamps, send INSTANCE_CHANGE if signatures are not fresh enough * Remove skip mark from test_view_change_with_different_ic, check that it passes * Implement test to check behavior of case when <=f nodes start a view change much earlier    ></body> </Action>
<Action id="56505" issue="36099" author="sergey.khoroshavin" type="comment" created="2019-02-01 16:02:59.0" updateauthor="sergey.khoroshavin" updated="2019-02-01 16:02:59.0"> <body><! CDATA *Failing case found* * Network outage happens between primary and nodes 2 and 3 ** Nodes 2 and 3 broadcast INSTANCE_CHANGE messages, which get accumulated on nodes 2, 3 and 4 * Node 4 gets restarted, losing accumulated INSTANCE_CHANGE messages * Network outage happens between primary and node 4 ** Node 4 broadcasts INSTANCE_CHANGE ** Node 2 and 3 now have quorum of INSTANCE_CHANGE messages, which start a view change ** Node 4 has only INSTANCE_CHANGE from self, which is insufficient to start a view change * Time passes and freshness check is triggered on Node1 and Node4 ** Nodes 1 and 4 broadcast INSTANCE_CHANGE, which get accumulated on nodes 1 and 4 ** However this is still insufficient to start a view change on Node4 and Node1 * Consensus is lost  Possible solutions are: * when performing freshness check send INSTANCE_CHANGE even if view change to this view is already in progress * don't do anything with freshness, just periodically resend INSTANCE_CHANGE while view change is in progress   ></body> </Action>
<Action id="56811" issue="36099" author="sergey.khoroshavin" type="comment" created="2019-02-08 16:30:26.0" updateauthor="sergey.khoroshavin" updated="2019-02-11 09:09:29.0"> <body><! CDATA *Changes* * Added new reason for INSTANCE_CHANGE: state signatures are not fresh enough, code 43 * Implemented sending INSTANCE_CHANGE messages when node discover that state of some of its ledgers doesn't have fresh enough signatures for too long * Implemented contiguous sending of INSTANCE_CHANGE (with same target view_no) until all ledgers have fresh enough signature  *PR* https://github.com/hyperledger/indy-plenum/pull/1078  *Versions* indy-plenum 1.6.675 indy-node 1.6.799  *Covered with tests* *  unit tests + test_view_change_doesnt_happen_if_pool_is_left_alone|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-286c6136421042b6056063c91c03f22e  *  test_freshness_instance_changes_are_sent_continuosly|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-c5870aa7a382d90b9bffe7b56baf4928  *  test_view_change_happens_if_ordering_is_halted|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-9f6c8921187de21f7703e7dbf60b4c8e  *  test_view_change_happens_if_primary_is_slow_to_update_freshness|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-e2fcfc49802c877d17fd4a19fbe8179a  *  test_view_change_not_happen_if_ic_is_discarded|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-4151554ee1cac10387e880c08964071a   *Risk* Medium  *Risk factors* Additional reason for view change can lead to more view changes, watch out for false or even perpetual view changes  *Recommendations for QA* Run load test on 25-nodes pool with moderate load - pool should work. Try enabling regular view change (every 30 minutes) - pool should still work under load. Stop load script Try slowing down primary - pool should do a view change even without load.  Try reproducing INDY-1903 - pool should recover after entering a view change on only some of nodes, even without load.  ></body> </Action>
<Action id="56913" issue="36099" author="sergey.khoroshavin" type="comment" created="2019-02-12 11:19:45.0" updateauthor="sergey.khoroshavin" updated="2019-02-12 11:33:18.0"> <body><! CDATA *Additional changes* * Respect ACCEPTABLE_FRESHNESS_INTERVALS_COUNT when detecting whether to send INSTANCE_CHANGE * This greatly reduces chance of false view change since previous coefficient was 1.2 and default value of ACCEPTABLE_FRESHNESS_INTERVALS_COUNT is 2  *PR* https://github.com/hyperledger/indy-plenum/pull/1085  ></body> </Action>
<Action id="57047" issue="36099" author="ozheregelya" type="comment" created="2019-02-14 16:33:39.0" updateauthor="ozheregelya" updated="2019-02-14 16:33:39.0"> <body><! CDATA Environment: indy-node 1.6.803  Case 1: Steps to Validate: 1. Setup the pool with default config. 2. Run production load. Actual Results: 550K txns written, pool works, all nodes are in sync.   Case 2: Steps to Validate: 1. Setup the pool with forced View Changes each 0.5h. 2. Run production load. Actual Results: 400K txns written, 42 view changes happened, pool works, all nodes are in sync.   Case 3: {quote} Try slowing down primary - pool should do a view change even without load.  {quote} Can't be reproduced on real environment.  Case 4: Steps to Validate: 1. Setup the pool. 2. Block primary on half of nodes using iptables (iptables -A INPUT -s 54.176.193.152 -j DROP). 3. When the rest nodes will get INSTANCE_CHANGEs with 26 reason, unblock primary. 4. Restart the rest half of nodes and block primary on them. Actual Results: View Change was completed because of 43 reason.  ></body> </Action>
