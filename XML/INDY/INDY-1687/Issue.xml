<Issue id="33548" key="INDY-1687" number="1687" project="10303" reporter="ashcherbakov" assignee="vladimirwork" creator="ashcherbakov" type="10002" summary="We need to take as much as possible Node-to-Node messages from queues before starting View Change" priority="3" resolution="10000" status="10001" created="2018-09-06 13:07:25.0" updated="2019-03-29 20:33:43.0" resolutiondate="2019-03-29 20:33:43.0" votes="0" watches="3" workflowId="44837"> <description><! CDATA The current View Change protocol is based on catch-up and last prepared certificate calculated by each node individually before starting the View Change.  The idea is the following: * Each nodes calculates the maximum 3PC seqNo for a quorum of received prepares. * Once we have a quorum (n-f) Prepares, other nodes should also have this (in an ideal perfect world). So, we should have n-f nodes at the same state, while other f nodes can catchup to this state.  However, in reality this approach doesn't work in 100% cases and may lead to some nodes go beyond others because * The network is async and unpredictable, so all nodes may send Prepares, some nodes may receive them, but some not * *There is a message queue with node-to-node messages, and messages are grabbed from there by small portions to avoid looper iterations taking too much time*  *The problem:* * It's possible that a Node has enough Prepares to set a higher prepared certificate, but they are still queued when View ChangeÂ  is started, and prepared certificate is older than can be. * So, if we have fast and slow nodes, for fast nodes we may have higher prepared certificate than for slow ones because fast nodes process message queue faster. This may lead to a situation where fat nodes go beyond others, and we lost consistency.  *Acceptance criteria:* * Provide a configurable strategy that can process as much as possible messages form Node-to-Node message queue before starting the View Change and setting last prepared certificate.  ** We would like to grab all messages ** However, there is a risk of infinite grabbing if there is intensive load ** So, probably we still need to have some limits, or say ZMQ to not process any incoming messages while we are flushing the queue. * Test the strategy and enable it by default if everything is fine  ></description> </Issue>
