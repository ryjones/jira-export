<Action id="53276" issue="35168" author="sergey.khoroshavin" type="comment" created="2018-11-09 16:57:34.0" updateauthor="sergey.khoroshavin" updated="2018-11-21 10:32:29.0"> <body><! CDATA *PoA* *  optional  implement block cache size metric in RocksDB python wrapper *  optional  add block cache metric to set of metrics in plenum * explicitly set limits to block cache and max_files_open, run load test and check results * implement a way to destroy and recreate RocksDB instances (so all memory gets dropped) * run load test in docker with some nodes periodically resetting their RocksDB instances and check both memory consumption and GC metrics  ></body> </Action>
<Action id="53684" issue="35168" author="sergey.khoroshavin" type="comment" created="2018-11-21 11:01:22.0" updateauthor="sergey.khoroshavin" updated="2018-11-21 11:01:22.0"> <body><! CDATA Due to found and fixed memory leak in scope of INDY-1747 it was decided to run plain load test in docker, with altered block_cache and max_files_open limits. During load test run in docker nodes were configured as follows: * Node1 & 2: default config * Node3: _max_files_open_ set to 10 (default is 5000) * Node4: _block_cache_size_ set to 1 Mb (default is 8 Mb)  Default config (node2) vs _max_files_open_ set to 10 (node3): !Screenshot from 2018-11-21 12-25-39.png|thumbnail!   Default config (node2) vs _block_cache_size_ set to 1 Mb (node4): !Screenshot from 2018-11-21 12-26-35.png|thumbnail!   It can be seen that: * even with default config memory consumption became more or less stable after first 24 hours passed * reduced _max_files_open_ affects _estimate-table-readers-mem_ metric, however total memory consumption change was not so visible * reduced _block_cache_size_ didn't have any visible effects * number of objects tracked by GC is not increasing  Preliminary conclusions: * rocksdb memory consumption becomes more or less stable after initial warmup * most probably it will still increase during long run, but not significantly  Additional things to do: * run load test for a couple more days to make sure that memory consumption is really stable and see if difference between node2 and node3 becomes more pronounced * revert configuration of node4 to default and do periodic restarts to check warmup behavior with large ledger (there are already over 1.5M txns written)  ></body> </Action>
<Action id="53689" issue="35168" author="sergey.khoroshavin" type="comment" created="2018-11-21 14:45:18.0" updateauthor="sergey.khoroshavin" updated="2018-11-21 14:46:08.0"> <body><! CDATA Also, according to RocksDB  documentation|https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB#indexes-and-filter-blocks  there is no simple way to limit memory taken by indexes and filters. It can be either moved to block cache (so it will page out when hitting limit) or number of max open files can be limited (which we did in our test), however both cases are not recommended unless most data is cold and there is only limited number of hot keys, otherwise performance issues may appear. In fact, recommendation is the opposite - make max number of open files unlimited if possible.   So, given that during moderate load memory consumption is very modest it makes sense to keep RocksDB configuration default for now.  ></body> </Action>
