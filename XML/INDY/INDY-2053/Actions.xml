<Action id="59564" issue="39224" author="sergey.khoroshavin" type="comment" created="2019-04-29 09:22:56.0" updateauthor="sergey.khoroshavin" updated="2019-04-29 09:23:14.0"> <body><! CDATA *Problem* As described in issue  *Changes made* Node doing catch up takes into account information received from consistency proofs about ledger states of other nodes and sends only catchup requests that can be fulfilled. Some additional details can be found here:   ^INDY-2053.pdf    *Version* indy-node 1.7.0.dev914  *PR* https://github.com/hyperledger/indy-plenum/pull/1173  *Covered by tests* test_catchup_req_distribution_invariants test_catchup_uses_only_nodes_with_cons_proofs test_catchup_from_unequal_nodes_without_reasking test_catchup_with_only_one_available_node test_catchup_with_all_nodes_sending_cons_proofs_dead test_receive_incorrect_catchup_request_for_seq_no_zero  *Risk* Low  *Risk factors* Some yet unknown edge cases of new catchup logic may arise  *Recommendations for QA* On 25-nodes AWS pool run following tests: * Short catchup of several nodes under load ** Start production load (10 txns/seconds) ** Wait for 5 minutes ** Stop nodes 15-20 for 1 minute ** Make sure they all manage to catch up ** And there are no ERRORs in logs of other nodes indicating that incorrect catch-up request received * Long catchup of several nodes under load ** Start production load (10 txns/seconds) ** Wait for 5 minutes ** Stop nodes 15-20 for 15 minutes ** Make sure they all manage to catch up ** And there are no ERRORs in logs of other nodes indicating that incorrect catch-up request received * Forced view change ** Start production load with forced view change period set to 1800 seconds ** Check if pool survives this for 24 hours  ></body> </Action>
<Action id="59744" issue="39224" author="vladimirwork" type="comment" created="2019-05-06 10:43:20.0" updateauthor="vladimirwork" updated="2019-05-06 10:48:48.0"> <body><! CDATA Build Info: 1.8.0~dev918  Steps to Reproduce: 1. Start production load (10 txns/seconds). 2. Wait for 5 minutes. 3. Stop nodes 15-20 for 1 minute. 4. Make sure they all manage to catch up.  Actual Results: Nodes 15-20 can't reach other ones under load. There is an error on this nodes: {noformat} 2019-05-06 09:54:46,001|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 675 is more than ledger size 671 2019-05-06 09:54:53,361|ERROR|seeder_service.py|Node15 cannot build consistency proof: start 672 is more than ledger size 671 2019-05-06 09:56:05,334|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 912 is more than ledger size 791 2019-05-06 09:56:07,300|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 915 is more than ledger size 791 2019-05-06 10:08:49,132|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 1722 is more than ledger size 1702 2019-05-06 10:20:07,318|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 2816 is more than ledger size 2781 {noformat}  Expected Results: Nodes should catch up under load and there are should be no errors.  Logs: ev@evernymr33:logs/INDY_2053_06_05_2019_logs.tar.gz  ></body> </Action>
<Action id="59793" issue="39224" author="sergey.khoroshavin" type="comment" created="2019-05-07 11:21:38.0" updateauthor="sergey.khoroshavin" updated="2019-05-07 11:21:38.0"> <body><! CDATA Analysis of failed case (as well as some tests performed on earlier versions of indy-node) showed that: * it is not a regression * ERRORs arise from different code path and actually not an issue * new implementation still has a flaw, however it is easily fixed, although at the cost of lesser efficiency ( PR|https://github.com/hyperledger/indy-plenum/pull/1194  is already opened) * there is yet another issue with catch-up slowness related to gathering CONSISTENCY_PROOFs, which is outside of scope of this ticket, so another one will be created and linked to  ></body> </Action>
<Action id="59829" issue="39224" author="ashcherbakov" type="comment" body="Fixed in 1.8.0.dev923" created="2019-05-08 07:43:47.0" updateauthor="ashcherbakov" updated="2019-05-08 07:43:47.0"/>
<Action id="59830" issue="39224" author="sergey.khoroshavin" type="comment" created="2019-05-08 08:38:25.0" updateauthor="sergey.khoroshavin" updated="2019-05-08 08:38:25.0"> <body><! CDATA *Recommendations for QA* Run short catch-up case: * Start production load (10 txns/seconds) * Wait for 5 minutes * Stop nodes 15-20 for 1 minute * Start them and see how they catch up  Unfortunately due to how consistency proofs are gathered (which is a problem outside of scope of this issue) catch up is still likely to be slow, however it should perform a little better than before, and there shouldn't be any messages in logs stating that nodes are discarding catch-up requests. In other words, output of the following command should be empty on all nodes: {code} xzgrep discarding /var/log/indy/sandbox/* | grep CATCHUP_REQ {code}   ></body> </Action>
<Action id="59832" issue="39224" author="vladimirwork" type="comment" created="2019-05-08 11:31:11.0" updateauthor="vladimirwork" updated="2019-05-08 11:31:11.0"> <body><! CDATA Build Info: 1.8.0~dev923  Steps to Reproduce: 1. Start production load (10 txns/seconds). 2. Wait for 5 minutes. 3. Stop nodes 15-20 for 1 minute. 4. Make sure they all manage to catch up.  Actual Results: 2 nodes from 6 have completed catch up successfully. The rest ones have not even after 1+ hour waitng. There are no messages about discarded catch-up requests on the nodes.  Additional Info: Issues with catch up are still present and the will be fixed in scope of INDY-<>.   ></body> </Action>
