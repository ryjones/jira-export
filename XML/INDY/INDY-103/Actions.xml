<Action id="24772" issue="17229" author="andrey.goncharov" type="comment" created="2017-05-31 14:29:21.0" updateauthor="andrey.goncharov" updated="2017-05-31 14:29:29.0"> <body><! CDATA Also I have a concern about current catch up design. As of now we stash incoming transactions if we're catching up and process them after the catch up is completed. But what if we have some internet connection problems? Then we still have an unsynced ledger after applying the transactions. This scenario looks to me way to complicated anyway. We store node's state for some time in separate places and just try to merge those states at some point. Can't we stop processing any transactions at all if we're catching up (do not even stash them), complete the catch up and send ledger status to all nodes when we think  the catch up is completed? This way if there were additional transactions while we were catching up we will trigger a new catch up. After a set of recursive catch ups we will have a valid up-to-date ledger. This way the state of our ledger depends on one component of the system only and it gets easier to maintain.   ~jlaw 1   ~danielhardman   ~lovesh   ~alexander.shekhovcov  what do you think?  ></body> </Action>
<Action id="24879" issue="17229" author="danielhardman" type="comment" body="I agree with Andrey&apos;s reasoning. +1 from me." created="2017-06-01 03:21:07.0" updateauthor="danielhardman" updated="2017-06-01 03:21:07.0"/>
<Action id="25123" issue="17229" author="ashcherbakov" type="comment" created="2017-06-02 15:08:55.0" updateauthor="ashcherbakov" updated="2017-06-02 15:08:55.0"> <body><! CDATA  ~danielhardman   ~andrey.goncharov   ~lovesh   I totally agree that we should not stash too much. It's a good optimization, but unnecessary got MGL. I prefer to be more predictable and stable, although slower. I think it's better to discard some of the messages (client requests for example) during some of the process (view change, catch-up), then to have very complicated and tangled logic which lead to instability. That's a type of trade-off we use to solve INDY-13.  However, in this particular example I feel like we have to stash incoming 3PC messages during catchup (especially during high-load). Otherwise a pool will be behind again after catch-up is finished. because other nodes may process and commit lots of new txns in the meantime. I agree that there is a chance that catch-up may not help if not all 3PC messages are received/stashed. Actually I think it's rather a rare case. Recursive catch-up may also not help. There is always a chance that it can be an infinite loop.  So, I see two options here: - Send a LedgerStatus again after a catch-up is done, to check that we're up to date (after applying stashed 3PC messages received during catch-up). But don't send it infinitely, have a limited number of re-tries (configurable). - Send LedgerStatus according to some timeouts (configurable), or after Checkpoint is received.  As for INDY-13: we're going to implement a kind of recursive catch-up there, but during a view change, that is during 'pause the world', so it should not be infinite.    ></body> </Action>
<Action id="25229" issue="17229" author="lovesh" type="comment" created="2017-06-03 13:27:46.0" updateauthor="lovesh" updated="2017-06-03 13:29:53.0"> <body><! CDATA In a practical setting, not stashing will make the node repeatedly catchup, "Catchup untill the lagging node is at same state as others" is true only when the *time to catchup _n_ txns < time to 3PC _n_ txns*, which is going to be false usually. Regarding sending LedgerStatus again after catchup, the catchup process communicates the last 3PC state so if you are stashing 3PC messages, you know what to remove and what to process (this is what we do as of now). As to starting catchup whenever a node is behind, a simple (maybe naive) approach is to set a timer when the node realises  *It has a prepared certificate for a seq no. but not for the previous one(s)*  once this timer expires and the condition is still true and then it initiates a "catchup". This catchup does have to be as expensive as our current process which nodes do on startup but can be simply requesting txns from another node and checking the merkle root after applying them (to a temporary tree as we do now in catchup) and comparing the merkle root in the prepared certificate above.  ></body> </Action>
<Action id="27831" issue="17229" author="andrey.goncharov" type="comment" created="2017-07-04 13:37:42.0" updateauthor="andrey.goncharov" updated="2017-07-04 13:37:42.0"> <body><! CDATA Design added:  https://docs.google.com/document/d/1OCUsD46HLhqhqZBJmn5wLZ2gTS1mcu6CQszoAh1gUA0  ></body> </Action>
<Action id="27834" issue="17229" author="andrey.goncharov" type="comment" body="Assigned to Daniel for review. Lovesh I would also appreciate if you take a look." created="2017-07-04 14:18:32.0" updateauthor="andrey.goncharov" updated="2017-07-04 14:18:32.0"/>
<Action id="27853" issue="17229" author="lovesh" type="comment" created="2017-07-05 06:12:09.0" updateauthor="lovesh" updated="2017-07-05 07:39:53.0"> <body><! CDATA  ~andrey.goncharov  We already have a specific message called CHECKPOINT that is sent every `CHK_FREQ` batches, communicating state. Let a node use that to determine if its behind. Now, solving the problem where system does not have writes for some time or write frequency is low so CHECKPOINTs take very long: every node has a timer that expires in `t` seconds, if it does not hear about a checkpoint in `t` seconds, it requests LEDGER_STATUS (not send, only node feeling the need to know sends it) from other nodes and decides if it need to catchup. I don't see any more scenario that needs to be taken care of. It requires minimal changes. The current value of `CHK_FREQ` is set to 10000, you can reduce it to 100. If this sounds passive, then node simply uses 3PC messages to determine if it is behind, eg. If you see COMMITs for seq no 100 but your seq no is 80, you know you have missed messages and the network is moved ahead.  Another optimisation i suggest here is that the "catchup" (with consistency proofs, etc) should not be done if the number of missing transactions is small say less than 20. Use the `MessageReq` instead to request 3PC messages. In-fact, you don't request all 3PC messages, just request PRE-PREPARE+PREPARE for each batch from each node, if you have some later confirmed PRE-PREPARE.   ></body> </Action>
<Action id="27868" issue="17229" author="andrey.goncharov" type="comment" created="2017-07-05 13:11:06.0" updateauthor="andrey.goncharov" updated="2017-07-05 13:11:06.0"> <body><! CDATA  ~lovesh  thank you! Added checkpoint option to the doc and updated my final recomendation. https://docs.google.com/document/d/1OCUsD46HLhqhqZBJmn5wLZ2gTS1mcu6CQszoAh1gUA0  ></body> </Action>
