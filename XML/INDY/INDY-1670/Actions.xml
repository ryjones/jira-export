<Action id="49805" issue="33404" author="ozheregelya" type="comment" created="2018-09-04 12:43:00.0" updateauthor="ozheregelya" updated="2018-09-04 12:48:42.0"> <body><! CDATA Similar problem with normal load (10 writes/sec): s3://qanodelogs/indy-1670-normal-load/NodeXX/ Note that before normal load there was short time high load, and before high load pool was working with low load ~1 day.  ></body> </Action>
<Action id="49965" issue="33404" author="toktar" type="comment" created="2018-09-06 11:03:55.0" updateauthor="toktar" updated="2018-09-06 11:04:08.0"> <body><! CDATA After analyzing logs, two main problems were identified:  1) Often View changes with "master degraded". Problem with a long latency will fix in a new strategy in the task INDY-1639  2) One of the problems with the current implementation of View Change is different prepared certificates on different nodes. It's a known problem in PBFT. F nodes can have a larger preperd certificate for two reasons:  - Network delays. All nodes sent prepare messages, but some be in time to collect a quorum of them, some are not, as a result, a minority of nodes can have a higher or lower certificate.  - All messages from all nodes can reach us, but at the time of collecting a prepare certificate they are still in the queue, and we have not processed them yet. It's a known problem in PBFT and can be solve via change a logic of View Change. As a hot fix we can processing the maximum from the node stack before setting a prepare certificate.  ></body> </Action>
<Action id="49995" issue="33404" author="ashcherbakov" type="comment" body="One of the reasons why prepared certificates may be so different on different nodes in the pool was identified: this is because we may have different number of messaged queued in Node-to-Node message queue. This will be addressed in https://jira.hyperledger.org/browse/INDY-1687" created="2018-09-06 13:13:44.0" updateauthor="ashcherbakov" updated="2018-09-06 13:13:44.0"/>
