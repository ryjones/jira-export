<Action id="44320" issue="29682" author="ashcherbakov" type="comment" body="This needs to be fixed together with INDY-1259" created="2018-05-14 11:21:06.0" updateauthor="ashcherbakov" updated="2018-05-14 11:21:06.0"/>
<Action id="44328" issue="29682" author="ashcherbakov" type="comment" created="2018-05-14 11:52:51.0" updateauthor="ashcherbakov" updated="2018-05-14 11:52:51.0"> <body><! CDATA Our current thinking about ViewChange problems: 1) The current approach has the assumption that >f nodes will eventually commit their prepared certificates. However, it's possible only if there is no big crashes. 2) Another problem with the current protocol is that it relies on some timestamp, that is eventually the prepared certificates will be committed or caught up. However, this is not very deterministic.  Also there are probably some issues with implementation of the current View Change approach: 1) Whether we give enough possibility for prepared certificates to be ordered (gather COMMITs) before/between rounds of catch-up that we are doing. In particular, we start catch-up immediately, and it may revert some state (needed for committing prepared certificates). 2) Whether we have correct timeouts 3) Whether we should separate New Primary Selection Timeout (we need it since we use round robin for primary selection, and the new Primary can be unavailable), and All-Nodes-In-Sync timeout, that is the timeout we use for making sure that all nodes eventually in sync for the new view (prepared certificates committed, catch-up finished, etc.)   ></body> </Action>
<Action id="44538" issue="29682" author="ashcherbakov" type="comment" created="2018-05-17 09:57:12.0" updateauthor="ashcherbakov" updated="2018-05-17 09:57:52.0"> <body><! CDATA Summary of the problems in existing protocol:  # The existing protocol doesn't guarantee in a deterministic and provable way that all requests ordered at at least f+1 nodes will be ordered in the new view. # The existing protocol doesn't propagate the state of the maximum ordered request in the pool (prepared certificate) but relies on its own calculation. So, the only way to make sure that the node ordered everything that needed is to have some "positive" and "optimistic" timeouts, that is wait for some (maybe significant) time until the node ordered what it thinks needs to be ordered. Timeouts are very undetermenistic, and it can be an issue in a distributed and asynced networks.  Further work will be continued in the scope of INDY-1341, INDY-1303, INDY-1304.  ></body> </Action>
<Action id="44597" issue="29682" author="sergey.khoroshavin" type="comment" created="2018-05-17 16:35:24.0" updateauthor="sergey.khoroshavin" updated="2018-05-17 16:35:24.0"> <body><! CDATA I'd like to summarize things found during analysis of our view change protocol and PBFT and VR (precursor of PBFT) articles.  1) View Change protocol must guarantee that new view will contain all requests that were ordered in previous view by at least one correct and f potentially faulty nodes (in other words - if there is chance that for given request client received reply confirming that transaction was written to ledger it should stay in ledger after view change no matter what).  2) In a distributed system we cannot determine which requests were confirmed to user without extra phase after commit, but we can determine upper bound on requests that could have been potentially confirmed. This upper bound is largest prepare certificate that exists on n-f nodes - only for these requests some nodes could get commit quorum.  3) In order to determine this upper bound nodes need to know list of prepared certificates on other nodes. PBFT solves this deterministically by explicitly propagating this information during view change, and if it fails to do so by any reason (including timeout) it starts another view change immediately. This means that in PBFT when we finally enter some new view all non-faulty nodes are guaranteed to know all requests that should be carried from previous view (and this list is same on all nodes).  4) On the other hand current view change protocol makes an assumption that we can determine upper bound by waiting enough time for requests to commit. Given that all messages will be deivered this is valid assumption but problem is the time point when this happens is unbounded. If at some point some node doesn't receive enough commits it has no way of knowing whether messages it didn't receive are just late or won't arrive at all because of lack of prepare quorum.  5) So all in all current problems with view change can be solved in one of two ways: - just increase timeouts reducing probability of failure (but it will never reach zero) and fix any remaining issues in code - implement explicit prepare certificate propagation, which, if done properly, will lead to PBFT view change protocol   ></body> </Action>
<Action id="44714" issue="29682" author="sergey-shilov" type="comment" created="2018-05-21 08:23:02.0" updateauthor="sergey-shilov" updated="2018-05-21 08:23:02.0"> <body><! CDATA We've done testing with increased view change time out (30 minutes), analysed logs and got the following result:   - the view change finished by view change time out spent;   - after finished view change the pool continued ordering transactions.  But despite continuation of ordering transaction such a long view change time out is not a good solution, because the probability of waiting till view change time out during view changed process is very high. The reason for it is unaligned _last_prepared_certificate_ (unlike PBFT)_._ Before starting of the view change process each node calculates it's own _last_prepared_certificate_ that corresponds to the last prepare message that has a quorum, so it's highly likely that these certificates are different for different nodes. If more than _f_ but less then _N-f_ nodes have higher prepared seqno than others then they can not complete a catch up that is sub process of view change. This leads to waiting till the view change time out. For example, in our test we observed the following situation:  ======================================================================  2018-04-28 14:49:45.537000 |  Node11:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:58.897000 |  Node12:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:49.527000 |  Node15:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:52.869000 |  Node16:0 setting last prepared for master to (0, 148) 2018-04-28 14:49:48.189000 |  Node18:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:57.800000 |  Node2:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:48.270000 |  Node23:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:53.251000 |  Node24:0 setting last prepared for master to (0, 148) 2018-04-28 14:49:53.727000 |  Node25:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:48.534000 |  Node3:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:51.814000 |  Node7:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:47.264000 |  Node8:0 setting last prepared for master to (0, 149)     2018-04-28 14:49:44.947000 |  Node1:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:46.604000 |  Node10:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:50.750000 |  Node13:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:58.539000 |  Node14:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:51.600000 |  Node17:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:47.240000 |  Node19:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:50.815000 |  Node20:0 setting last prepared for master to (0, 151) 2018-04-28 14:49:55.239000 |  Node21:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:53.077000 |  Node22:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:51.266000 |  Node4:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:57.592000 |  Node5:0 setting last prepared for master to (0, 151) 2018-04-28 14:49:51.674000 |  Node6:0 setting last prepared for master to (0, 150) 2018-04-28 14:50:16.465000 |  Node9:0 setting last prepared for master to (0, 150)  ======================================================================  As a result the nodes from the first group were waiting for _VIEW_CHANGE_DONE_ messages from the nodes from the second group until view change time out have spent.  So the risks of long view change time out are the following: * high memory usage as we stash all incoming messages during the catch up process that may lead to Out Of Memory; * unavailable pool for a long time equal to view change time out * potential kind of attack to keep the pool infinitely unavailable  So one of the core problems of current view change protocol is unaligned _last_prepared_certificate_ that leads to incomplete catch up during the view change process. It's not very critical as pool nodes complete the view change process by time out and continue ordering, but it adds risks described above.  One of the possible solution is to increase view change time out up to 5 minutes (it's approximate, we will clarify it in further related tickets) as current 1 minute is too low.  There is another problem with inconsistent state trie that is not reproduced with long time out, we'll continue trying to reproduce it in scope of ticket INDY-1350. I think that this ticket may be closed.  ></body> </Action>
<Action id="45042" issue="29682" author="sergey.khoroshavin" type="comment" created="2018-05-24 11:28:25.0" updateauthor="sergey.khoroshavin" updated="2018-05-24 11:28:25.0"> <body><! CDATA There were proposals for adding _VIEW_CHANGE_START_ messages to current view change protocol to mitigate found problems without increasing time needed for view change. Basic idea is that if we propagate last prepared certificate this way all nodes can select same last prepared certificate that was reached on _N-f_ nodes and be sure that they will get commits eventually, making possible to increase timeout for receiving COMMITs indefinitely.  Unfortunately in case of malicious nodes it won't work.  Suppose we have 4 nodes: _Alpha_, _Beta_, _Gamma_, _Delta_, with _Delta_ being malicious.  *Attack #1* 1) _Alpha_ reach prepared certificate 1 (and send corresponding commits), _Beta_ and _Gamma_ reach prepared certificate 2, _Delta_ state is irrelevant 2) View change starts, _Alpha_, _Beta_ and _Gamma_ report their prepared certificates normally, _Delta_ reports to _Beta_ that it reached prepared certificate 2, but at the same time it reports to _Alpha_ and _Gamma_ that it reached prepared certificate 1 3) Now _Alpha_ and _Gamma_ think that last prepared certificate on _N-f_ is 1, but _Beta_ thinks that its 2 4) _Alpha_ and _Gamma_ order requests till 1 and finish view change with ledger state 1 5) _Beta_ waits for commits quorum till 2, and given that it has it's own commit, it can receive commits from _Gamma_ and malicious _Delta_ and finish view change with ledger state 2 6) _Delta_ finish view change with ledger state 1 7) View change finishes with _Beta_ thrown out of consensus  This attack can be mitigated by adding _VIEW_CHANGE_START_ACK_ messages to make sure that malicious node cannot trick others into ordering different requests. However this still leaves pool open for denial of service attack.  *Attack #2* 1) _Alpha_ reach prepared certificate 1 (and send corresponding commits), _Beta_ and _Gamma_ reach prepared certificate 2, _Delta_ state is irrelevant 2) View change starts, _Alpha_, _Beta_ and _Gamma_ report their prepared certificates normally, _Delta_ reports that it reached prepared certificate 2 3) All nodes wait for commits quorum till 2, but _Delta_ doesn't send them, and 2 commits from _Beta_ and _Gamma_ are insufficient 4) As a result pool is halted for maximum wait time for commits which theoretically should be infinite  Further investigation is needed to determine if allowing processing prepares till last prepared certificate can mitigate this attack, but there is still high probability that it will still leave pool open for more complicated attacks.  On the other hand PBFT view change protocol is not susceptible to these attacks and has a formal proof of correctness.   ></body> </Action>
