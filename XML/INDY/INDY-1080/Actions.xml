<Action id="38506" issue="26683" author="ashcherbakov" type="comment" created="2018-01-12 16:22:18.0" updateauthor="ashcherbakov" updated="2018-01-15 14:45:01.0"> <body><! CDATA We need to separate ViewChange protocol (how all instances and replicas move to the next view and become in sync) and next Primary selection (how next primary is selected). - I believe we should use ViewChange protocol as described in PBFT paper (with application to RBFT as mentioned in Option2 of the doc, in particular apply it to all instances). We should do it the same way as described in PBFT (using stable checkpoints).  - I propose not changing Primary selection logic, so continue using round robin. We can move to selection based on Performance as suggested in Option 1 of the doc later.     Why I prefer approach 2 (PBFT) for View Change: - It's defined in PBFT paper => contains lots of thinking from authors, BFT experts and community - Looks like it solves the main problem we were facing with view change: how to get all replicas in sync in the new view. Using checkpoints and re-applying requests look like relatively easy and stable way to do it.  - Option 2 (PBFT) is easier to implement than Option 1. Moreover, we already have checkpoints logic. - Our main goal now is stability, not performance. Option 1 may be better from performance point of view (BTW it needs to be measured), but I think it may be worse from stability point of view since it's a custom algorithm, not reviewed by BFT experts.  ></body> </Action>
<Action id="38507" issue="26683" author="ashcherbakov" type="comment" created="2018-01-12 16:29:48.0" updateauthor="ashcherbakov" updated="2018-01-15 15:19:28.0"> <body><! CDATA PoA for PBFT View Change Protocol: * All view change protocol  related logic should be implemented in separate classes following Actor model approach and using State machine (see INDY-970 and  State refactoring|https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.eybc53im5j6s ).  We should do it the same way as described in PBFT (using stable checkpoints). * Primary selection logic should be independent (may still be the same round robin as of now) * Write a doc describing View Change (in fact it will be a short doc referencing PBFT paper and mentioning RBFT and multiple instances specific)  ></body> </Action>
<Action id="38678" issue="26683" author="lovesh" type="comment" created="2018-01-16 13:39:07.0" updateauthor="lovesh" updated="2018-01-16 13:44:16.0"> <body><! CDATA My suggestion is not change the overall algorithm but refactor the code since we have not seen a flaw in the algorithm but rather than the implementation where there are so many state variables to be updated and some of them are not  Some inefficiencies for PBFT style view change VIEW-CHANGE message requires each node to store received CHECKPOINT messages from each node for the last stable checkpoint, it is not needed. VIEW-CHANGE-ACK is sent to primary for each replica's VIEW-CHANGE, so a node receiving 3 VIEW-CHANGE sends VIEW-CHANGE-ACKs to the primary. This is chatty. The primary has to sends null requests for pre-prepared requests and primary starts the requests again for already prepared requests. Moreover it will take considerable time   Refactoring required: 	Checkpoints are not needed so get rid of watermarks and checkpoints. We can use multi-sig rounds to remove processed requests. 	Use state machine for view change   ></body> </Action>
<Action id="38679" issue="26683" author="ashcherbakov" type="comment" created="2018-01-16 13:52:18.0" updateauthor="ashcherbakov" updated="2018-01-16 13:54:04.0"> <body><! CDATA So, in fact we consider the following two Options:  *Option1*: re-factor existing View Change code as suggested in  State Machine Refactoring|https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r  - cons: ** even if we have clean implementation of this approach, we are not sure for 100% that this ViewChange approach works in all cases. This is our custom algorithm not reviewed by BFT experts and community. So, there is chance that we spent time for re-factoring and will still come to the conclusion that we need to implement PBFT or something else. - pros: ** may be less work (although not significantly less) ** will improve stability ** will improve code quality  *Option2*: write View Change logic from scratch using PBFT algorithm * cons: ** May require more time for implementation (although not significantly more) ** May require some time for debugging and testing the new approach * pros: ** a well defined approach by BFT experts and community, so more confidence that it will work ** stable view change ** can have good code quality if we implement it properly from scratch  ></body> </Action>
<Action id="38755" issue="26683" author="ashcherbakov" type="comment" created="2018-01-17 16:45:25.0" updateauthor="ashcherbakov" updated="2018-01-17 16:46:41.0"> <body><! CDATA The current problems with View Change code:  1. There are multiple files dealing with View Change - related logic: * node.py * view_changer.py * replica.py * ledger_manager.py * primary_selector.py  Please note that `view_changer.py` was created recently as one of the refactorings (see INDY-480 and  Plenum States Refactoring|https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit .  I believe our intention is that all view-change related logic should be in ViewChanger class, and this class doesn't call/use any variables/state from other classes directly.  It should communicate with other entities via messages as a step towards Actor model approach.  2. Many of the files below are really huge (node.py contains 3000 lines; replica.py contains 2500 lines), so it's quite hard to find out what is related to view change and what is not.  3. View Change process affects the state of replicas and 3PC. But because we don't have clear states and state machine, it's done quite implicitly and can be a source of issues (see  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L466 )  4. View Change logic is very tangled with catch-up logic. Some of catch-up conditions are mixed with the View change ones (see  link|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L1746  as example)  5. View Change logic is also quite tangled with Primary selection logic (although primary selection now is just a round robin) (see  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/view_change/view_changer.py#L484 ).  6. View Change logic is used in two scenarios: real view change (when participating replicas change a primary), and propagation of the current state (and current view in particular) to a newly joined node. They have some common, but also have differences. It leads to an ugly code with special variables and conditions (see  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/view_change/view_changer.py#L49  and  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/view_change/view_changer.py#L92  and  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/replica.py#L492 ) and a source of bugs.  7. There are many different variables in many classes. It's quite often that classes use variables from other classes (see  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/replica.py#L262  and  this |https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L398 for example).  8. Many methods do not what is expected from them. For example, methods like `is_something_done` or `can_do_something` don't just check conditions, but perform some actions which is not expected and makes workflow not so clear (see  this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L398  as example).  ></body> </Action>
<Action id="38756" issue="26683" author="ashcherbakov" type="comment" body="I believe INDY-1092 can help us to check the stability of the current implementation and the protocol better." created="2018-01-17 16:47:25.0" updateauthor="ashcherbakov" updated="2018-01-17 16:47:25.0"/>
<Action id="39373" issue="26683" author="ashcherbakov" type="comment" created="2018-01-30 15:55:44.0" updateauthor="ashcherbakov" updated="2018-01-30 15:55:44.0"> <body><! CDATA  ~danielhardman   ~nage   ~lovesh   ~gudkov  So, what is our final decision here, especially taken into account INDY-970. We can do refactoring of existing code and protocol, or implement checkpoint-based one.     ></body> </Action>
<Action id="43422" issue="26683" author="ashcherbakov" type="comment" created="2018-04-25 16:02:17.0" updateauthor="ashcherbakov" updated="2018-04-25 16:02:17.0"> <body><! CDATA The final decision was as follows: * Continue analysis of the current protocol and write more tests around it (these tests will help to prove that the new implementation is good): INDY-1296 * Start implementing PBFT in parallel. Take the algorithm from PBFT as a basis, modify it for Plenum needs (take into account 3PC in particular), and have a look at the Fabric's PBFT experience and Sawtooth's RBFT one: INDY-1290  ></body> </Action>
