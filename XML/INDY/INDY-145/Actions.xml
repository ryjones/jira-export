<Action id="25196" issue="17414" author="krw910" type="comment" created="2017-06-02 22:11:47.0" updateauthor="krw910" updated="2017-06-02 22:11:47.0"> <body><! CDATA I have reproduced this again with two triggers: # If I demote a node and add it back to the pool it won't catch up or take new transactions. # If I stop the service on a node and then start the service again it won't catch up or take new transactions.  I did the first trigger on Node 5 and the second trigger on Node 6. *{color:#d04437}Now I cannot connect to the pool at all.{color}*  I am going to try again with a new pool where I restart the service on a node and see if it has the same issue.  ></body> </Action>
<Action id="25234" issue="17414" author="krw910" type="comment" created="2017-06-03 17:50:24.0" updateauthor="krw910" updated="2017-06-04 00:28:24.0"> <body><! CDATA Today I setup 3 pools *Pool 1 Scenario 1* - I am just going to let this one set idol for the day to see if we lose any connections. *Pool 2* *Scenario 2*- I am going to shut down the sovrin-node service on one node for 5 minutes and not send any transactions. After 5 minutes I will start the service again and send some transactions. *Pool 3* *Scenario 3*- I am going to send one "send NYM" transaction every 15 seconds so 4 per minute for 48 hours. I will check about every 5 hours to see how it is going.  *{color:#333333}Results on Pool 2 Scenario 2{color}* * Run 20 transactions on Pool 2 successfully to all nodes. * Shut down the sovrin-node service on Node 4 for 5 minutes. * Do not send any transactions * Now start the sovrin-node service on Node 4 * Send 10 transactions  {color:#FF0000}At this point when I went to send the 10 transactions I found I was only connected to Node 4. During the 5 minutes I had the sovrin-node service shut off on Node 4 the other 3 nodes disconnected.{color}  {color:#14892c}Workaround was to restart the sovrin-node service on all 4 nodes. Once I did that I was able to send more transactions.{color}  I have added the log files for the issue on the Pool 2 scenario. They are named Node1_ServiceShutDown.log and so on for all 4 nodes.  ></body> </Action>
<Action id="25237" issue="17414" author="krw910" type="comment" created="2017-06-03 21:12:59.0" updateauthor="krw910" updated="2017-06-04 00:27:41.0"> <body><! CDATA I reproduced scenario 2 in a global pool setup with 4 nodes and 1 client.  This time when I first setup the nodes the client could connect to the pool, but NYMs could not be added.  I restarted all the nodes and then I was able to add NYMs.  I sent 20 transactions and confirmed that all nodes were in sync. I then shut the sovrin-node service off on Node 4 for 5 minutes. I did not perform any transactions. After 5 minutes I started the sovrin-node service on Node 4. At this point I tried to send transactions and was only able to connect with Node 4 from the Client. Nodes 1-3 were disconnected.  I am leaving that pool in the current state and will email the connection information.  ></body> </Action>
<Action id="25274" issue="17414" author="ashcherbakov" type="comment" created="2017-06-04 18:40:22.0" updateauthor="ashcherbakov" updated="2017-06-04 18:40:22.0"> <body><! CDATA Looks like we had the following problems:  *     A node can not process txns after it’s disconnected and re-connected (catch-up is ok, but new reqs are not ordered) FIXED in https://github.com/evernym/plenum/pull/201 - MERGED into plenum master  *     A node can not process txns after it’s demoted and added back (catch-up is ok, but new reqs are not ordered) FIXED in https://github.com/evernym/plenum/pull/201 - MERGED into plenum master  *     Connection problems after a node is demoted and added back FIXED in https://github.com/evernym/plenum/pull/201 - MERGED into plenum master  *     Connection problems after a node is disconnected and re-connected back IDEA: https://github.com/evernym/stp/tree/fix-re-connection Added reconnection tests https://github.com/evernym/stp/blob/fix-re-connection/stp_zmq/test/test_reconnect.py - test_reconnect_long FAILS on teh current master If I always re-connect (https://github.com/evernym/stp/blob/fix-re-connection/stp_zmq/zstack.py#L1173), then the test starts passing. Try to send ping if socket is not closed for a limited number of times only. Close the socket after if sending ping failed for X times.  TODO:  #     Create an RC build with the fixes (Andrey G.): Merge https://github.com/evernym/plenum/pull/201 - DONE Pin versions Merge to Stable branch Create an RC #    Fix Issue 4 (disconnection) (Victor): Try to send ping if socket is not closed for a limited number of times only. Close the socket after if sending ping failed for X times. Make sure that https://github.com/evernym/stp/blob/fix-re-connection/stp_zmq/test/test_reconnect.py  tests pass     ></body> </Action>
<Action id="25337" issue="17414" author="ashcherbakov" type="comment" created="2017-06-05 13:56:47.0" updateauthor="ashcherbakov" updated="2017-06-05 13:56:47.0"> <body><! CDATA Fixed in RC buils: - sovrin-node==0.3.17 - sovrin-client==0.3.19  ></body> </Action>
<Action id="25396" issue="17414" author="spivachuk" type="comment" created="2017-06-05 17:16:43.0" updateauthor="spivachuk" updated="2017-06-05 17:16:43.0"> <body><! CDATA Tried to reproduce the issue with lost connections to nodes (Pool 2 Scenario 2 from the comments to this ticket) on a local pool generated by Vagrant script for the latest deb packages of sovrin-node and sovrin-client from "xenial master" APT repositories: * The issue is not reproduced for the case when one node is stopped and then started. * The issue is not reproduced for the case when one node is blacklisted and then enabled back.  ></body> </Action>
<Action id="25463" issue="17414" author="krw910" type="comment" body="As this ticket is written dealing with demoting a node I have to set this blocked. In the latest RC build we cannot demote a node. I have reopened INDY-9 which is blocking this ticket." created="2017-06-06 04:05:16.0" updateauthor="krw910" updated="2017-06-06 04:05:16.0"/>
<Action id="25475" issue="17414" author="ashcherbakov" type="comment" created="2017-06-06 09:15:18.0" updateauthor="ashcherbakov" updated="2017-06-06 09:15:18.0"> <body><! CDATA Problem reason:  - Issue1: The nodes didn't disconnect suspended (demoted) node - FIXED - Issue2: lats_ordere_pp_seq_no wasn't set for a replica after catch-up, that's why cought-up node didn't receive any new txns - FIXED - Issue3: We didn't close connection (socket) for disconnected Nodes, and sent pings - FIXED  Covered with tests: - https://github.com/evernym/plenum/blob/master/plenum/test/node_catchup/test_catchup_demoted.py - https://github.com/evernym/plenum/blob/master/plenum/test/node_catchup/test_node_catchup_after_disconnect.py - https://github.com/evernym/stp/blob/master/stp_zmq/test/test_reconnect.py   ></body> </Action>
<Action id="25887" issue="17414" author="aleksey-roldugin" type="comment" created="2017-06-09 17:37:26.0" updateauthor="aleksey-roldugin" updated="2017-06-09 17:37:26.0"> <body><! CDATA h6. build sovrin-node 0.3.19  h6. verification - Scenario 1 (added nodes stop making catch up after demotion-promotion) is verified. - Scenario 2 (nodes didn't perform catch up after adding) is verified. - Scenario 3 (node didn't perform catch up after stop-start sovrin-node.service) is reproduced again and moved to separate ticket  INDY-191|https://jira.hyperledger.org/browse/INDY-191 . It also linked with  INDY-159|https://jira.hyperledger.org/browse/INDY-159 .  ></body> </Action>
