<Action id="41202" issue="27536" author="spivachuk" type="comment" created="2018-03-05 17:31:26.0" updateauthor="spivachuk" updated="2018-03-05 17:31:26.0"> <body><! CDATA *PoA:* The backup replicas must be able to restore their state in a redundant way as well as the master replica is. For this {{viewNo}} and {{ppSeqNo}} fields in LEDGER_STATUS and CONSISTENCY_PROOF messages must be replaced with a list of pairs ({{viewNo}}, {{ppSeqNo}}) for each replica. This, in turn, will require to track 3PC-keys for {{txnSeqNos}} not only for the master replica but for all the replicas.  ></body> </Action>
<Action id="41290" issue="27536" author="spivachuk" type="comment" body="In a detailed discussion with  ~ashcherbakov  of the approach proposed in the previous comment we agreed that the 3PC-keys for backup replicas being reported by LEDGER_STATUS and CONSISTENCY_PROOF messages must correspond to some synchronization point as well as the 3PC-key for the master replica now corresponds to the last transaction in a ledger. We cannot just report the last ordered 3PC-keys for replicas because, for example, in case of significant load, lacking CONSISTENCY_PROOFs are requested for a specific end transaction {{seqNo}} in order to gather the quorum of the same messages. However, for a backup instance we cannot use the last transaction {{seqNo}} as a synchronization point for the 3PC-keys in LEDGER_STATUSes or CONSISTENCY_PROOFs. This is so because the backup instance may not have ordered this transaction for the moment or even may not be going to order it (if the primary of the instance is offline). This means that we should use an individual synchronization point for each instance. To make this possible, we can emulate commitment of transactions to a ledger on backup replicas. With this we will be able to use {{seqNo}} of such a virtual transaction on a backup instance as a synchronization point for the 3PC-keys being reported by LEDGER_STATUSes or CONSISTENCY_PROOFs. However, in such the case each replica of a node will have its own target state when the node is performing catch-up. These states will not be synchronized between the replicas of the node. This makes it reasonable to divide the whole node catch-up procedure into separate procedures for each of the replicas. This work must be included into the scope of INDY-971." created="2018-03-07 13:20:06.0" updateauthor="spivachuk" updated="2018-03-07 13:20:06.0"/>
<Action id="43454" issue="27536" author="spivachuk" type="comment" body="Created the separate task INDY-1301 for a design of the division of the whole node catch-up procedure into separate procedures for each of the replicas." created="2018-04-26 09:10:54.0" updateauthor="spivachuk" updated="2018-04-26 09:10:54.0"/>
<Action id="64542" issue="27536" author="ashcherbakov" type="comment" body="We implemented fixes for this. Also we are going to get rid of Backup Instances after moving to Aardvark." created="2019-10-10 13:51:49.0" updateauthor="ashcherbakov" updated="2019-10-10 13:51:49.0"/>
