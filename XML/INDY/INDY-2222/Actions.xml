<Action id="63560" issue="42195" author="vladimirwork" type="comment" body="There was the same issue with 9th node but during node disconnection and connection against 1.10.0~dev1073 version: https://build.sovrin.org/blue/rest/organizations/jenkins/pipelines/indy-node/pipelines/indy-node-nightly/runs/92/nodes/90/steps/464/log/?start=0." created="2019-09-05 07:30:07.0" updateauthor="vladimirwork" updated="2019-09-05 07:30:07.0"/>
<Action id="63562" issue="42195" author="toktar" type="comment" body="Node8 after promote connected only to Node5 and Node7. And didn&apos;t catchup because didn&apos;t  have a quorum of LedgerStatuses." created="2019-09-05 08:17:49.0" updateauthor="toktar" updated="2019-09-05 08:17:49.0"/>
<Action id="63589" issue="42195" author="vladimirwork" type="comment" created="2019-09-06 07:39:57.0" updateauthor="vladimirwork" updated="2019-09-06 07:42:21.0"> <body><! CDATA Issues with catchup are still present in 1.10.0~dev1078 indy-node setup: https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-nightly/detail/indy-node-nightly/94/pipeline/94  ^node9.2019-09-05T233939.tgz   ^node8.2019-09-05T233938.tgz   ^node7.2019-09-05T233937.tgz   ^node6.2019-09-05T233937.tgz   ^node5.2019-09-05T233936.tgz   ^node4.2019-09-05T233935.tgz   ^node3.2019-09-05T233934.tgz    ^node2.2019-09-05T233933.tgz   ^node1.2019-09-05T233932.tgz   and 1.0.3~dev86 plugins + 1.10.0~dev1077 indy-node setup: https://build.sovrin.org/blue/organizations/jenkins/sovrin%2Fsovrin-nightly/detail/sovrin-nightly/51/pipeline/85  ^node9.2019-09-06T015252.tgz   ^node8.2019-09-06T015251.tgz   ^node6.2019-09-06T015249.tgz   ^node5.2019-09-06T015249.tgz   ^node4.2019-09-06T015248.tgz   ^node3.2019-09-06T015247.tgz   ^node2.2019-09-06T015246.tgz   ^node1.2019-09-06T015245.tgz    ></body> </Action>
<Action id="63716" issue="42195" author="vladimirwork" type="comment" body="It looks like we have more intermittent failures of TestCatchUpSuite (especially test_case_out_of_network) after indy-node ..1077 and plugins ..86 versions according to CD pipelines and local runs." created="2019-09-12 14:08:07.0" updateauthor="vladimirwork" updated="2019-09-12 14:08:07.0"/>
<Action id="63904" issue="42195" author="vladimirwork" type="comment" created="2019-09-20 12:47:28.0" updateauthor="vladimirwork" updated="2019-09-20 12:47:28.0"> <body><! CDATA Test failed against the latest master tonight: {noformat}  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node1 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node2 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node3 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node4 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node5 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node6 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node7 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node8 'read_ledger --type=domain --count'", exit_status=0, stdout=b'263\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node9 'read_ledger --type=domain --count'", exit_status=0, stdout=b'13\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node1 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node2 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node3 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node4 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node5 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node6 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node7 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node8 'read_ledger --type=audit --count'", exit_status=0, stdout=b'259\n', stderr=None)  INFO     testinfra:base.py:241 RUN CommandResult(command=b"ssh node9 'read_ledger --type=audit --count'", exit_status=0, stdout=b'2\n', stderr=None)  ERROR    system.utils:utils.py:244 check_pool_is_in_sync failed; not trying any more because 200 seconds have passed; args were () {noformat}  9th node doesn't catch up and order anything (it has 13 domain txns and 2 audit).  Full logs:  ^node9.2019-09-19T235908.tgz    ^node8.2019-09-19T235906.tgz    ^node7.2019-09-19T235905.tgz    ^node6.2019-09-19T235904.tgz    ^node5.2019-09-19T235902.tgz    ^node2.2019-09-19T235858.tgz    ^node1.2019-09-19T235857.tgz    ^node3.2019-09-19T235900.tgz    ></body> </Action>
<Action id="64018" issue="42195" author="vladimirwork" type="comment" body="All 4 catchup tests pass if we reduce the number of nyms written and read between nodes&apos; operations to 1 (4 total in test case)." created="2019-09-24 09:56:07.0" updateauthor="vladimirwork" updated="2019-09-24 09:56:07.0"/>
<Action id="64022" issue="42195" author="kithat" type="comment" created="2019-09-24 11:35:42.0" updateauthor="kithat" updated="2019-09-24 11:35:42.0"> <body><! CDATA Processed logs from the latest nightly build. Looks like Node9 finished connection:  {noformat} 2019-09-23 23:51:57,070|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node8 2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node2 2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node6 2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node7 2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node3 2019-09-23 23:52:19,030|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node1 2019-09-23 23:52:19,030|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node5 2019-09-23 23:52:19,030|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node4 {noformat}  But has not received quorum of LEDGER_STATUS messages to start catch-up:  {noformat} 2019-09-23 23:51:56,098|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8 2019-09-23 23:52:01,943|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8 2019-09-23 23:52:01,943|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8 2019-09-23 23:52:01,944|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8 2019-09-23 23:52:01,945|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8 2019-09-23 23:52:18,404|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,405|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,405|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,416|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,789|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,800|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,801|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,802|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2 2019-09-23 23:52:18,966|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4 2019-09-23 23:52:20,232|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,234|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,234|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,235|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,236|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,237|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,772|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,773|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,774|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,775|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,776|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,785|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4 2019-09-23 23:52:20,786|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4 2019-09-23 23:52:20,787|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4 2019-09-23 23:52:20,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4 2019-09-23 23:52:20,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4 2019-09-23 23:52:20,789|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1 2019-09-23 23:52:20,828|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,830|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,831|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,836|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,841|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,842|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 2019-09-23 23:52:20,843|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5 {noformat}  So the the LEDGER_STATUS message has been received only from Node1, Node2, Node4, Node5 and Node8 -- and we need 6 for quorum. Node 9 requested LEDGER_STATUS from all nodes but it was lost somehow on a way back: {noformat} 434730 2019-09-23 23:56:54,843|TRACE|batched.py|Node3 sending msg b'{"msg_type":"LEDGER_STATUS","op":"MESSAGE_RESPONSE","params":{"ledgerId":0},"msg":{"ledgerId":0,"merkleRoot":"9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER","ppSeqNo":null,"protocolVersion":2,"txnSeqNo":9,"viewNo":null}}' to Node9 434731 2019-09-23 23:56:54,843|TRACE|zstack.py|Node3 transmitting message b'{"msg_type":"LEDGER_STATUS","op":"MESSAGE_RESPONSE","params":{"ledgerId":0},"msg":{"ledgerId":0,"merkleRoot":"9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER","ppSeqNo":null,"protocolVersion":2,"txnSeqNo":9,"viewNo":null}}' to Node9 by socket 114 35992384   {noformat}  ></body> </Action>
<Action id="64125" issue="42195" author="vladimirwork" type="comment" created="2019-09-25 14:42:37.0" updateauthor="vladimirwork" updated="2019-09-25 14:46:26.0"> <body><! CDATA Test with 100 nyms sent on each step that checks all nodes reachability (after 9th node connection) fails in the same place as test with 100 nyms sent on each step that doesn't check reachability: !catchup_failures.PNG|thumbnail!  So it looks like with 1 and 10 nyms sent we have reachability and perform catchup and with 100 nyms *we have reachability but don't perform catchup*.  FYI  ~ashcherbakov   ~KitHat   ></body> </Action>
<Action id="64491" issue="42195" author="kithat" type="comment" created="2019-10-09 13:28:17.0" updateauthor="kithat" updated="2019-10-09 13:28:17.0"> <body><! CDATA Problem reason:  - Some nodes can not connect to reconnected (after different occasions) nodes  Changes:  - after 4 consecutive pings node will reconnect connected remote.  PR: - https://github.com/hyperledger/indy-plenum/pull/1363   Version: - indy-node master #1102  Risk factors: - reconnection cycles  Risk: - Med  Covered with tests: - https://github.com/KitHat/indy-plenum/blob/1d0e1ac8368f877dcec890aa3c68c78963ad33f0/plenum/test/zstack_tests/test_ping_reconnection.py  Recommendations for QA - The best test would be reconnections under load with checks of reachability (but without viewchanges)  ></body> </Action>
<Action id="64525" issue="42195" author="vladimirwork" type="comment" created="2019-10-10 11:20:28.0" updateauthor="vladimirwork" updated="2019-10-10 11:21:19.0"> <body><! CDATA Catchup tests have failed at 50 txns sent locally and at 25 and 50 txns sent in indy-node nightly pipeline so it looks like we still have the issue here.  https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-nightly/detail/indy-node-nightly/129/tests  ></body> </Action>
<Action id="64668" issue="42195" author="kithat" type="comment" created="2019-10-11 16:10:54.0" updateauthor="kithat" updated="2019-10-11 16:10:54.0"> <body><! CDATA So, I have investigated the logs and it is another issue (or an issue with tests) It looks like the node that is turned on first isn't awaited to get all connections before writing new transactions. (However nodes that have not restarted in some way receive messages) As a result Node8 does get some 3PC messages but not enough to collect strong consensus. Then the new mechanism of reconnection fires off and old 3PC messages are not resent -- so Node8 stays behind. We may have it caught up if we will send more messages. I suggest to modify the test or to move the work to another story -- the problem with connections looks to be fixed.  ></body> </Action>
