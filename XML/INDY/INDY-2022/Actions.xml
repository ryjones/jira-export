<Action id="58435" issue="38471" author="derashe" type="comment" created="2019-03-22 09:33:38.0" updateauthor="derashe" updated="2019-03-22 09:33:38.0"> <body><! CDATA    *Results of inverstigation.* * As we can see from validator info output and log, after primary was restarted, there were no pool consesus lost. * But there were problems with two nodes: regioit01 and Absa. * Absa was able to finish view_change and start ordering, but stopped after some time. * regioit01 wasn't able to correctly start and finish view change, so it couldn't order till it was restarted.  *Action items.* * We will continue to do research on reasons of regioit01 fault *  ~mgbailey  We need to get logs and journalctl info from Absa node, so we could understand what happened to it.  ></body> </Action>
<Action id="58463" issue="38471" author="derashe" type="comment" body=" ~mgbailey  It also would be very good if we could get journalctl  from regioit01." created="2019-03-22 14:47:17.0" updateauthor="derashe" updated="2019-03-22 14:47:17.0"/>
<Action id="58466" issue="38471" author="mgbailey" type="comment" body="I will reach out to the stewards of Absa and regioit01 and request logs from them" created="2019-03-22 15:06:59.0" updateauthor="mgbailey" updated="2019-03-22 15:06:59.0"/>
<Action id="58508" issue="38471" author="derashe" type="comment" created="2019-03-25 12:12:36.0" updateauthor="derashe" updated="2019-03-25 12:12:36.0"> <body><! CDATA *Additional results:* * As for regioit01, it could not start view change, because of minor bug, which is already fixed in master branch ** _startViewChange_ function in view_changer.py need to have condition, which allows it to call _on_strategy_complete_, so it could correctly set _is_preparing_ flag. * We've also mentioned INSTANCE_CHANGE, that was initiated by 21 reason (incorrect state tree). That happened because master primary last sent batch was freshness batch. And it wasn't saved in _txn_seq_range_to_3phase_key_ dict. This made primary sent pp with already ordered 3pc number and changed self root hash. This pp was discarded by other nodes and did not changed its root hash. Next 3pc batch will be declined with suspicion code 21.  ></body> </Action>
<Action id="58519" issue="38471" author="mgbailey" type="comment" created="2019-03-25 16:00:35.0" updateauthor="mgbailey" updated="2019-03-25 16:00:35.0"> <body><! CDATA From the Absa steward:    unfortunately I won’t be able to recover these logs due to the fore-mentioned incident which happened on March 14^th^afternoon. Specifically we’ve lost logs in interval from 7^th^ March  to 14^th^ 15:11GMT of March.  What exactly happened: On March 14^th^ afternoon I’ve copy-pasted tar command for getting the validator logs and didn’t try to run it against some testfile first – directly tried to package up the validator logs. Maybe the tar command was for a different tar implementation - it failed and wiped out the log file. I am sorry I can’t help with the logs and I’ve learnt some lesson here too.   However, here’s journalctl logs: -- Logs begin at Wed 2019-03-13 11:19:05 UTC, end at Mon 2019-03-25 08:50:42 UTC. --  Mar 13 11:19:16 validator systemd 1 : Started Indy Node.  Mar 13 11:21:24 validator systemd 1 : Stopping Indy Node...  Mar 13 11:21:24 validator systemd 1 : Stopped Indy Node.  Mar 13 11:36:18 validator systemd 1 : Stopped Indy Node.  Mar 13 11:36:44 validator systemd 1 : Started Indy Node.  Mar 14 14:26:51 validator systemd 1 : Started Indy Node.  Mar 14 14:26:51 validator systemd 1 : Stopping Indy Node...  Mar 14 14:26:51 validator systemd 1 : Stopped Indy Node.  Mar 14 14:26:51 validator systemd 1 : Started Indy Node.  If it helps with the investigation, note that on March 13^th^ 11:36AM GMT we’ve split up our client/node interface https://indyscan.io/tx/SOVRIN_TESTNET/pool/123  and the node was able to reach other nodes after the operation. Then on March 14^th^, I temporarily enabled new firewall rules, which is when we noticed issues.  Also, as I’ve mentioned before, the firewall rules which seemed to be causing some troubles to Ibm node are not currently activated. So unless something has changed since then, re-enabling these firewall rules may reproduce issues observed on March 14^th^.      ></body> </Action>
<Action id="58558" issue="38471" author="derashe" type="comment" body=" ~mgbailey  thanks for the update." created="2019-03-26 11:59:46.0" updateauthor="derashe" updated="2019-03-26 11:59:46.0"/>
<Action id="58571" issue="38471" author="derashe" type="comment" created="2019-03-26 14:28:05.0" updateauthor="derashe" updated="2019-03-27 07:19:55.0"> <body><! CDATA Problem reason/description: - Freshness and empty batches do not update _txn_seq_range_to_3phase_key_  Changes: - Now, freshness and empty batches updates _txn_seq_range_to_3phase_key_  PR: -  https://github.com/hyperledger/indy-plenum/pull/1140  -  https://github.com/hyperledger/indy-plenum/pull/1141   Version: -  h4.  indy-node: 1.6.875-master|https://github.com/hyperledger/indy-node/releases/tag/1.6.875-master   Risk factors: - no  Risk: - Low  Covered with tests: -  https://github.com/hyperledger/indy-plenum/pull/1140/files#diff-1ec4662ba693350e6f170e310d7d9414   Recommendations for QA * start 4 nodes pool * send some txns (5) * wait untill few freshness batch will be sent (5 at least) * fast restart primary, so that no view_change happened * send some txns asap after restart and ensure that they will be written  ></body> </Action>
<Action id="58634" issue="38471" author="vladimirwork" type="comment" created="2019-03-27 13:56:35.0" updateauthor="vladimirwork" updated="2019-03-27 13:56:35.0"> <body><! CDATA Build Info: indy-node 1.6.876  Steps to Validate: 1. Start the pool. 2. Send some txns (5-50). 3. Wait untill few freshness batch will be sent (5 at least). 4. Fast restart primary, so that no view_change happened. 5. Send some txns (10-100) asap after restart and ensure that they will be written.  Actual Results: All txns have written successfully.  ></body> </Action>
