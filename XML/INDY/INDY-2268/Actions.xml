<Action id="65002" issue="42992" author="ashcherbakov" type="comment" created="2019-10-25 16:09:41.0" updateauthor="ashcherbakov" updated="2019-10-28 07:51:36.0"> <body><! CDATA *PoA* * Analyze old load tests results ** Analyze 1.9.2 acceptance load results ** Analyze other old load results ** For every load test we need the following information: *** How many nodes lagged behind eventually *** How many nodes lagged behind during the load *** How much time did the pool stay after the load had been stopped and before the pool had been shut down (that is how much time lagged nodes had to recover) *** How many Replicas have been deleted by the end of the load *** Were there any node crashes and restarts * Do more load tests ** prod load 10 txns per sec on 1.9.2 stable (runs right now on Pool 1), stop it, and see if lagged nodes can catchup ** prod load 10 txns per sec on 1.9.2 stable with the same client version as was at the time of 1.9.2 RC acceptance (Pool 1) ** prod load 10 txns per sec on the latest master with Max3PCBatchWait=3 (TBD on Pool 2) ** prod load 10 txns per sec on the latest master with PRE_PREPARE_REQUEST_ENABLED=False, PREPARE_REQUEST_ENABLED=False, COMMIT_REQUEST_ENABLED=False, PROPAGATE_REQUEST_ENABLED=False (Pool 1) * Compare metrics ** Get and compare 10 min interval metrics from the latest 1.9.2 load and from the "good" master one (at time when no lagged nodes) ** Get and compare 10 min interval metrics from the latest 1.9.2 load and from the "bad" master one (for lagged nodes) ** Get and compare 10 min interval metrics from  "good" new and from the "bad" new one ** Get and compare 10 min interval metrics from old 1.9.2 load (at time of RC) and from the "good" new one ** Get and compare 10 min interval metrics from old 1.9.2 load (at time of RC)  and from the "bad" new one  ></body> </Action>
<Action id="65009" issue="42992" author="sergey.khoroshavin" type="comment" created="2019-10-26 09:13:30.0" updateauthor="sergey.khoroshavin" updated="2019-10-26 09:13:30.0"> <body><! CDATA Some preliminary results: * in old load on 1.9.2 (that was performed about two months ago) only one node started lagging behind, which happened more than after 3 hours of load * in all current loads on 1.9.2 about 5 nodes started lagging behind, with 1-2 nodes starting lagging just in 2 hours from start * current 1.9.2 load results seem to be the same both with old and freshly created pools * during load test on latest master with Max3PCBatchWait=3 on Pool2 no nodes lagged behind after more than 8 hours of load * currently we have load on latest master with Max3PCBatchWait=3 on Pool1 and so far (2 hours) no nodes are lagging behind  ></body> </Action>
<Action id="65010" issue="42992" author="sergey.khoroshavin" type="comment" created="2019-10-26 09:19:56.0" updateauthor="sergey.khoroshavin" updated="2019-10-26 09:19:56.0"> <body><! CDATA Next things which I'm going to analyse and report: * reason and exact time when node started lagging behind in old load on 1.9.2 that was conducted 2 months ago * reason and exact times when nodes started lagging behind in fresh load tests on 1.9.2  ></body> </Action>
<Action id="65014" issue="42992" author="toktar" type="comment" created="2019-10-27 23:41:38.0" updateauthor="toktar" updated="2019-10-28 07:31:49.0"> <body><! CDATA +*indy-node 1.9.1*+  ev@evernymr33:logs/acc_prod_load_logs_metrics_1_1_51.tar.gz - start time of the load: 2019-08-01 07:20  *Behind nodes:* - 2019-08-01 07:46:59 - node4 - long work of the looper (up to 1000 sec), suspicion of a long verification of bls signatures - did not start ordering after restart - 2019-08-01 07:49:26 - node19 - propagates didn't finalize, lots re-asks - began to ordering after restart.  *Result at the end of the load test:* - 1 node behind (node4) - 1 node stopped ordering (node4) - 2 restarts due to memory overflow (node4, node19)  +*indy-node 1.9.2*+  ev@evernymr33:logs/prod_load_1_9_2_new_clients_25_10_2019.tar.gz - start time of the load: 2019-10-25 10:49:59  *Behind nodes:*  * 2019-10-25 13:30:37 - node6 - too many requests for propagates(not finalized) and pre-prepares(incorrect time) * 2019-10-25 12:52:42 - node8 - too many requests for propagates and pre-prepares * 2019-10-25 14:59:49 - node12 - too many requests for propagates and pre-prepares  * 2019-10-25 12:59:42 - node19 - too many requests for propagates and pre-prepares * 2019-10-25 12:52:42 - node23 - too many requests for propagates and pre-prepares * 2019-10-25 14:28:54 - node25 - too many requests for propagates and pre-prepares  *Result at the end of the load test:* - 0 node behind  - 0 node stopped ordering - 0 restarts  +*indy-node 1.9.2*+ - in progress ev@evernymr33:logs/prod_load_1_9_2_another_old_clients_26_10_2019.tar.gz - start time of the load: 2019-10-26 07:19  *Behind nodes:*  * 2019-10-25  - node3 - too many requests for propagates(not finalized) and pre-prepares(incorrect time) * 2019-10-25  - node4 - too many requests for propagates and pre-prepares * 2019-10-25  - node10 - too many requests for propagates and pre-prepares  * 2019-10-25  - node13 - too many requests for propagates and pre-prepares * 2019-10-25  - node20 - too many requests for propagates and pre-prepares * 2019-10-25  - node21 - too many requests for propagates and pre-prepares     ></body> </Action>
<Action id="65022" issue="42992" author="sergey.khoroshavin" type="comment" created="2019-10-28 10:37:44.0" updateauthor="sergey.khoroshavin" updated="2019-10-28 10:37:44.0"> <body><! CDATA Proposing analysis plan for each load: * pool start time (first "Starting up indy-node") * load stop time (grep "0 ordered batch request" with non-zero requests) * how many nodes were lagging after ~15 minutes from load stop (grep "0 ordered batch request" around required time) * how many and when were replica removal events (grep "removed replica") * how many and when were restart events (grep "Starting up indy-node") * what were avg/max_node_prod_time and avg_monitor_avg_latency like (looking at graphs) * how many seconds it took for node_prod_time,  * when nodes started lagging behind (walk in 2000 batch steps with {code}xzgrep "0 ordered batch request" Node* | grep "ppSeqNo 2000,"{code}) * when nodes started having non-equal input queues (walk in 200 batch steps backward with {code}xzgrep -E "0 ordered batch request|0 - checkpoint_service processed" NodeN* | grep 3800 | sort -k '2,4' -t ':' | head -n 25{code}) * check for suspicious activity near start time of having non-equal input queues ** non-zero max_send_message_req_time/send_message_req_time_count_per_sec ** reconnections (grep "reconnect") ** ansible check ins (grep "ansible" in journalctl)  ></body> </Action>
<Action id="65029" issue="42992" author="ashcherbakov" type="comment" body="Let&apos;s put results into https://docs.google.com/spreadsheets/d/1ExApShBQ1mLgAtpk2w9scROu7ukfU51ZTqPkSp0wLLk/edit#gid=0" created="2019-10-28 11:41:52.0" updateauthor="ashcherbakov" updated="2019-10-28 11:41:52.0"/>
<Action id="65204" issue="42992" author="toktar" type="comment" created="2019-10-31 09:21:10.0" updateauthor="toktar" updated="2019-10-31 09:21:10.0"> <body><! CDATA In scope of the task, the logs of 16 load tests were analyzed. The results can be seen in the table: https://docs.google.com/spreadsheets/d/1ExApShBQ1mLgAtpk2w9scROu7ukfU51ZTqPkSp0wLLk And change time to waiting a new batch from 1 sec to 3 sec: https://github.com/hyperledger/indy-plenum/pull/1388 *Conclusion* 1. New features for checking BLS signatures have expected increase of the processing time of commit messages. 2. Starting from version 1.9.2 we receive more commit messages. 3. The new versions of the indy-node do not contain degradation, but have lower performance due to BLS changes. 4. The solution with the formation of a batch every 3 seconds solves the performance problem, but increases the message delay time to 3.9 seconds. This is acceptable, but not well. 5. During many tests, 1-3 replicas are deleted, which positively affects performance and allows lagging nodes to resume work due to a decrease the number of 3pc messages.  It turns out that removing replicas will significantly improves performance and allows to return the formation time of the batch to 1 second. That is, this change will allow a pool to have high performance and fast response speed to the client.  ></body> </Action>
