<Action id="25470" issue="17478" author="lovesh" type="comment" body="If its just the node that crashed, it&apos;s lower priority problem. We do not need to guard against people arbitrarily messing up data/configuration on their machines." created="2017-06-06 06:36:24.0" updateauthor="lovesh" updated="2017-06-28 12:41:59.0"/>
<Action id="26337" issue="17478" author="stevetolman" type="comment" body="Mark, please invite Doug to help you with this code review." created="2017-06-14 20:50:36.0" updateauthor="stevetolman" updated="2017-06-14 20:50:36.0"/>
<Action id="26340" issue="17478" author="mark.hadley" type="comment" body="Code reviewed with Doug." created="2017-06-14 21:05:22.0" updateauthor="mark.hadley" updated="2017-06-14 21:05:22.0"/>
<Action id="26388" issue="17478" author="vladimirwork" type="comment" body="There is no info about build version with this fix. Bug is still reproducing on the latest master (0.3.139)." created="2017-06-15 13:18:17.0" updateauthor="vladimirwork" updated="2017-06-15 13:18:17.0"/>
<Action id="26894" issue="17478" author="stevetolman" type="comment" body="Let us know when this is merged into master and ready to build." created="2017-06-20 21:05:44.0" updateauthor="stevetolman" updated="2017-06-20 21:05:44.0"/>
<Action id="27222" issue="17478" author="mark.hadley" type="comment" body=" ~DouglasWightman : I assigned to you. The PR failed." created="2017-06-22 20:45:20.0" updateauthor="mark.hadley" updated="2017-06-22 20:45:20.0"/>
<Action id="27684" issue="17478" author="lovesh" type="comment" body=" ~danielhardman   ~krw910  I think building guards against people arbitrarily messing up data/configuration on their machines is a lower priority issue, but i would suggest a general solution to the problem where every data store has an integrity check, if that fails we take appropriate action." created="2017-06-30 08:27:33.0" updateauthor="lovesh" updated="2017-06-30 08:27:33.0"/>
<Action id="27705" issue="17478" author="douglaswightman" type="comment" body="Based on a discussion today with  ~danielhardman  I will rework this patch so that a corruption in the pool ledger stops the node and prints out an actionable error message." created="2017-06-30 14:36:57.0" updateauthor="douglaswightman" updated="2017-06-30 14:36:57.0"/>
<Action id="27901" issue="17478" author="stevetolman" type="comment" body="Mark, please review this ticket/PR. You can ask Daniel to do the actual merge when the review is complete." created="2017-07-05 21:06:31.0" updateauthor="stevetolman" updated="2017-07-05 21:06:31.0"/>
<Action id="28256" issue="17478" author="ozheregelya" type="comment" created="2017-07-12 16:34:56.0" updateauthor="ozheregelya" updated="2017-07-12 16:34:56.0"> <body><! CDATA *Build Info:*   sovrin-client version: 0.4.26   sovrin-node version: 0.4.19  OS/Platform: Ubuntu 16.04.2 LTS Setup: 4 nodes, 1 client running locally.  Reason for Reopen: Initial problem still reproduces.  Case 1: Invalid character in dest field. Steps to Reproduce: 1. Configure nodes to running locally. 2. Change .sovrin/data/nodes/Node1/pool_transactions_sandbox/1 to following: {code:java} {"data":{"alias":"Node1","client_ip":"127.0.0.1","client_port":9702,"node_ip":"127.0.0.1","node_port":9701,"services": "VALIDATOR" },"dest":"#w6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv","identifier":"Th7#pTaRZVRYnPiabds81Y","txnId":"fea82e10e894419fe2bea7d96296a6d46f50f93f9eeda954ec461b2ed2950b62","type":"0"} {"data":{"alias":"Node2","client_ip":"127.0.0.1","client_port":9704,"node_ip":"127.0.0.1","node_port":9703,"services": "VALIDATOR" },"dest":"8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb","identifier":"EbP4aYNeTHL6q385GuVpRV","txnId":"1ac8aece2a18ced660fef8694b61aac3af08ba875ce3026a160acbc3a3af35fc","type":"0"} {"data":{"alias":"Node3","client_ip":"127.0.0.1","client_port":9706,"node_ip":"127.0.0.1","node_port":9705,"services": "VALIDATOR" },"dest":"DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya","identifier":"4cU41vWW82ArfxJxHkzXPG","txnId":"7e9f355dffa78ed24668f0e0e369fd8c224076571c51e2ea8be5f26479edebe4","type":"0"} {"data":{"alias":"Node4","client_ip":"127.0.0.1","client_port":9708,"node_ip":"127.0.0.1","node_port":9707,"services": "VALIDATOR" },"dest":"4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA","identifier":"TWwCRQRZ2ZHMJFn9TzLp7W","txnId":"aa5e817d7cc626170eca175822029339a444eb0ee8f0bd20d3b0b76e566fb008","type":"0"}{code} 3. Try to start node.     *Actual Results:* Following traceback is shown in console: {code:java} sovrin@sovrin-VirtualBox:~$ start_sovrin_node Node1 9701 9702 2017-07-12 18:37:21,753 | INFO | log.py ( 79) | setupRaet | Setting RAET log level 2 2017-07-12 18:37:34,666 | INFO | looper.py ( 267) | shutdown | Looper shutting down now... 2017-07-12 18:37:34,700 | INFO | looper.py ( 274) | shutdown | Looper shut down in 0.029 seconds. Traceback (most recent call last): File "/usr/local/lib/python3.5/dist-packages/plenum/common/stack_manager.py", line 96, in parseLedgerForHaAndKeys verkey = cryptonymToHex(txn TARGET_NYM ) File "/usr/local/lib/python3.5/dist-packages/plenum/common/util.py", line 287, in cryptonymToHex return hexlify(base58.b58decode(cryptonym.encode())) File "/usr/local/lib/python3.5/dist-packages/base58.py", line 66, in b58decode acc += p * alphabet.index(c) ValueError: substring not found During handling of the above exception, another exception occurred: Traceback (most recent call last): File "/usr/local/bin/start_sovrin_node", line 48, in <module> cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/node.py", line 83, in __init__ config=self.config) File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 152, in __init__ self.initPoolManager(nodeRegistry, ha, cliname, cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/node.py", line 103, in initPoolManager HasPoolManager.__init__(self, nodeRegistry, ha, cliname, cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/pool_manager.py", line 12, in __init__ cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/pool_manager.py", line 23, in __init__ super().__init__(node=node, ha=ha, cliname=cliname, cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/plenum/server/pool_manager.py", line 118, in __init__ cliname=cliname, cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/plenum/server/pool_manager.py", line 158, in getStackParamsAndNodeReg nodeReg, cliNodeReg, nodeKeys = self.parseLedgerForHaAndKeys(self.ledger) File "/usr/local/lib/python3.5/dist-packages/plenum/common/stack_manager.py", line 98, in parseLedgerForHaAndKeys raise ValueError("Invalid verkey. Rebuild pool transactions.") ValueError: Invalid verkey. Rebuild pool transactions. {code}    *Expected Results:* Traceback should not be shown, human readable error should logged to log file.  *Case 2:* Invalid character out of any field. Steps to Reproduce: 1. Configure nodes to running locally. 2. Change .sovrin/data/nodes/Node1/pool_transactions_sandbox/1 to following: {code:java} {"data":{"alias":"Node1","client_ip":"127.0.0.1","client_port":9702,"node_ip":"127.0.0.1","node_port":9701,"services": "VALIDATOR" },"dest":#"Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv","identifier":"Th7MpTaRZVRYnPiabds81Y","txnId":"fea82e10e894419fe2bea7d96296a6d46f50f93f9eeda954ec461b2ed2950b62","type":"0"} {"data":{"alias":"Node2","client_ip":"127.0.0.1","client_port":9704,"node_ip":"127.0.0.1","node_port":9703,"services": "VALIDATOR" },"dest":"8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb","identifier":"EbP4aYNeTHL6q385GuVpRV","txnId":"1ac8aece2a18ced660fef8694b61aac3af08ba875ce3026a160acbc3a3af35fc","type":"0"} {"data":{"alias":"Node3","client_ip":"127.0.0.1","client_port":9706,"node_ip":"127.0.0.1","node_port":9705,"services": "VALIDATOR" },"dest":"DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya","identifier":"4cU41vWW82ArfxJxHkzXPG","txnId":"7e9f355dffa78ed24668f0e0e369fd8c224076571c51e2ea8be5f26479edebe4","type":"0"} {"data":{"alias":"Node4","client_ip":"127.0.0.1","client_port":9708,"node_ip":"127.0.0.1","node_port":9707,"services": "VALIDATOR" },"dest":"4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA","identifier":"TWwCRQRZ2ZHMJFn9TzLp7W","txnId":"aa5e817d7cc626170eca175822029339a444eb0ee8f0bd20d3b0b76e566fb008","type":"0"}{code} 3. Try to start node.     *Actual Results:* Following traceback is shown in console: {code:java} sovrin@sovrin-VirtualBox:~$ start_sovrin_node Node1 9701 97022017-07-12 18:58:41,538 | INFO | log.py ( 79) | setupRaet | Setting RAET log level 2 2017-07-12 18:58:46,227 | INFO | looper.py ( 267) | shutdown | Looper shutting down now... 2017-07-12 18:58:46,238 | INFO | looper.py ( 274) | shutdown | Looper shut down in 0.010 seconds. Traceback (most recent call last): File "/usr/local/bin/start_sovrin_node", line 48, in <module> cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/node.py", line 83, in __init__ config=self.config) File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 152, in __init__ self.initPoolManager(nodeRegistry, ha, cliname, cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/node.py", line 103, in initPoolManager HasPoolManager.__init__(self, nodeRegistry, ha, cliname, cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/pool_manager.py", line 12, in __init__ cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/sovrin_node/server/pool_manager.py", line 23, in __init__ super().__init__(node=node, ha=ha, cliname=cliname, cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/plenum/server/pool_manager.py", line 118, in __init__ cliname=cliname, cliha=cliha) File "/usr/local/lib/python3.5/dist-packages/plenum/server/pool_manager.py", line 158, in getStackParamsAndNodeReg nodeReg, cliNodeReg, nodeKeys = self.parseLedgerForHaAndKeys(self.ledger) File "/usr/local/lib/python3.5/dist-packages/plenum/common/stack_manager.py", line 79, in parseLedgerForHaAndKeys for _, txn in ledger.getAllTxn(): File "/usr/local/lib/python3.5/dist-packages/ledger/ledger.py", line 213, in getAllTxn for seq_no, txn in self._transactionLog.get_range(frm, to)) File "/usr/local/lib/python3.5/dist-packages/ledger/ledger.py", line 213, in <genexpr> for seq_no, txn in self._transactionLog.get_range(frm, to)) File "/usr/local/lib/python3.5/dist-packages/ledger/serializers/json_serializer.py", line 75, in deserialize return self.loads(data) File "/usr/local/lib/python3.5/dist-packages/ledger/serializers/json_serializer.py", line 65, in loads return json.loads(data) File "/usr/lib/python3.5/json/__init__.py", line 319, in loads return _default_decoder.decode(s) File "/usr/lib/python3.5/json/decoder.py", line 339, in decode obj, end = self.raw_decode(s, idx=_w(s, 0).end()) File "/usr/lib/python3.5/json/decoder.py", line 355, in raw_decode obj, end = self.scan_once(s, idx) json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 134 (char 133){code}    *Expected Results:* Traceback should not be shown, node should be stopped, human readable error should logged to log file.  *Case 3:* Invalid characters in identifier field. Steps to Reproduce: 1. Configure nodes to running locally. 2. Change .sovrin/data/nodes/Node1/pool_transactions_sandbox/1 to following: {code:java} {"data":{"alias":"Node1","client_ip":"127.0.0.1","client_port":9702,"node_ip":"127.0.0.1","node_port":9701,"services": "VALIDATOR" },"dest":"Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv","identifier":"Th7MpTaRZVRYnPiabds81Y","txnId":"fea82e10e894419fe2bea7d96296a6d46f50f93f9eeda954ec461b2ed2950b62","type":"0"} {"data":{"alias":"Node2","client_ip":"127.0.0.1","client_port":9704,"node_ip":"127.0.0.1","node_port":9703,"services": "VALIDATOR" },"dest":"8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb","identifier":"EbP4aYNeTHL6q385GuVpRV","txnId":"1ac8aece2a18ced660fef8694b61aac3af08ba875ce3026a160acbc3a3af35fc","type":"0"} {"data":{"alias":"Node3","client_ip":"127.0.0.1","client_port":9706,"node_ip":"127.0.0.1","node_port":9705,"services": "VALIDATOR" },"dest":"DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya","identifier":"4cU41vWW82ArfxJxHkzXPG","txnId":"7e9f355dffa78ed24668f0e0e369fd8c224076571c51e2ea8be5f26479edebe4","type":"0"} {"data":{"alias":"Node4","client_ip":"127.0.0.1","client_port":9708,"node_ip":"127.0.0.1","node_port":9707,"services": "VALIDATOR" },"dest":"4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA","identifier":"TW#CRQRZ2ZHMJFn9TzLp7W","txnId":"aa5e817d7cc626170eca175822029339a444eb0ee8f0bd20d3b0b76e566fb008","type":"0"}{code} 3. Try to start node.     *Actual Results:* Node works without any issues.  *Expected Results:* Expected behavior is unclear. Is it correct behavior that the node works with invalid pool_transactions_sandbox file?        *Additional Information:* Expected ** Results were described based on discussion with  ~dsurnin . Please, let us know if implemented behavior will differ from described in Expected Results.  ></body> </Action>
<Action id="28906" issue="17478" author="ryanmarsh" type="comment" created="2017-07-24 15:28:22.0" updateauthor="ryanmarsh" updated="2017-07-24 15:28:22.0"> <body><! CDATA Kelly says that he wants this behavior with any of the mentioned cases:  1-The nodes still starts up 2-We give a message, probably a warning, in the log file showing there is a problem with the ledger file. 3-The node checks with the other nodes and sees its file is incorrect or corrupt so it syncs with the pool and corrects the entry.     The question I have is, if node1 starts up with a corrupted file, where in the code can I trigger the event that Node1 checks with the other three nodes to see what they have in their ledgers so that it can sync up? I'm thinking that I'll call the processConsistencyProof function in ledger_manager.py but I'm not sure where the appropriate place to call that is. Any ideas/suggestions?   ></body> </Action>
<Action id="29037" issue="17478" author="ryanmarsh" type="comment" body="Current Pull Request:   https://github.com/hyperledger/indy-plenum/pull/306 " created="2017-07-26 14:08:06.0" updateauthor="ryanmarsh" updated="2017-07-26 14:08:06.0"/>
<Action id="29352" issue="17478" author="ozheregelya" type="comment" created="2017-08-02 15:05:18.0" updateauthor="ozheregelya" updated="2017-08-02 15:07:47.0"> <body><! CDATA *Build Info:*   indy-node 1.0.69   indy-anoncreds 1.0.22   indy-plenum 1.0.78   sovrin 1.0.15   python3-rlp 0.5.1   python3-sha3 0.2.1   python3-pyzmq 16.0.2 OS/Platform: Ubuntu 16.04.2 LTS Setup: 4 nodes (running locally), 1 client  *Case 1:* *Steps to Validate:* 1. Configure nodes to running locally. 2. Add invalid character to dest field in .sovrin/data/nodes/Node1/pool_transactions_sandbox/1. 3. Try to start node.  *Actual Results:* 2017-08-02 17:57:02,564 | INFO | looper.py ( 267) | shutdown | Looper shutting down now... 2017-08-02 17:57:02,575 | INFO | looper.py ( 274) | shutdown | Looper shut down in 0.011 seconds. Invalid verkey. Rebuild pool transactions.  *Case 2:* *Steps to Validate:* 1. Configure nodes to running locally. 2. Add invalid character out of any field in .sovrin/data/nodes/Node1/pool_transactions_sandbox/1. 3. Try to start node.  *Actual Results:* 2017-08-02 18:01:08,444 | INFO | looper.py ( 267) | shutdown | Looper shutting down now... 2017-08-02 18:01:08,456 | INFO | looper.py ( 274) | shutdown | Looper shut down in 0.012 seconds. Pool transaction file corrupted. Rebuild pool transactions.  *Case 3:* *Steps to Validate:* 1. Configure nodes to running locally. 2. Add invalid character to identifier field in .sovrin/data/nodes/Node1/pool_transactions_sandbox/1. 3. Try to start node.  *Actual Results:* 2017-08-02 18:02:59,275 | INFO | looper.py ( 267) | shutdown | Looper shutting down now... 2017-08-02 18:02:59,287 | INFO | looper.py ( 274) | shutdown | Looper shut down in 0.012 seconds. Invalid identifier. Rebuild pool transactions.  ></body> </Action>
<Action id="29357" issue="17478" author="ryanmarsh" type="comment" body="Alex informed me that there is no way for the Node to recover when the initial file is corrupted. He said to display error message and terminate the process. " created="2017-08-02 15:54:06.0" updateauthor="ryanmarsh" updated="2017-08-02 15:54:06.0"/>
