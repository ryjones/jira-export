<Action id="52314" issue="34625" author="derashe" type="comment" created="2018-10-16 10:43:04.0" updateauthor="derashe" updated="2018-10-16 15:29:37.0"> <body><! CDATA Our goals are: * to persist view_no:inst_id:seq_no every time primary replica send pre-prepare (def send3PCBatch). ** I think we need to include this code in _def send3PCBatch_ after _sendPrePrepare_ call.  * to restore this state. ** I think we need to include this code after we finished view_change and catch_up (which calls when node starts) so we dont overwrite restored state.  ** Probable place for it is in _def send3PCBatch_ before _create3PCBatch_ call.  Logic for restoring state: * If we are trying to send PP with seq_no == 0, we need to check persisted data. We need to look for an info with current view_no.  ** if we don't have, than this is first 3pc message in this view and we're ok with it  ** if we have, than it means, that we've restarted and we need to restore value (we can set _lastPrePrepareSeqNo_ value to persisted seq_no value) * We need to take into account case of restarting of whole pool. In that case valid 3pc key is (0,0). In that case we don't need to restore state and we need to prune persisted data. We can insert this logic in _allLedgersCaughtUp_ function, after we set mode to synced. Because we can send PP only when synced.           ></body> </Action>
<Action id="52623" issue="34625" author="spivachuk" type="comment" created="2018-10-25 09:31:33.0" updateauthor="spivachuk" updated="2018-11-02 13:34:43.0"> <body><! CDATA *PoA:*  Since a node can have at most one primary at a time, the node need to store only one entry (or zero if currently there is no primary replica on the node). This entry (triad) must contain primary replica index (inst_id), current view_no and pp_seq_no of the last sent PrePrepare.  The rules for using this triad must be as follows: * A primary replica must persist the triad each time after sending PrePrepare to other nodes. * The node must clear the persisted triad on completion of a regular (not propagate primary) view change. * A backup primary replica must restore {{lastPrePrepareSeqNo}} and adjust {{last_ordered_3pc}} using pp_seq_no from the persisted triad on completion of propagate primary from view 0 by CurrentStates in case the persisted triad exists, this replica index equals to inst_id from the persisted triad and the new view_no equals to view_no from the persisted triad. * Additionally, the node must clear the triad on completion of catch-up if master's {{last_caught_up_3PC}} == (0, 0) since it is a convincing indication that the whole pool has been restarted. This should prevent faulty restoration of pp_seq_no in case of the whole pool restart when a node with some backup primary is restarted with some lag after other nodes and so receives CurrentStates instead of not wrapped ViewChangeDones.  If after a node with some backup primary had been stopped, then the rest of the pool was restarted, later made several view changes and the stopped node is started again when view_no from its persisted triad coincides with the current view_no of the pool then pp_seq_no will be restored from the persisted triad while actually it is different view and the other replicas in this instance have not ordered any 3PC-batches (because the primary was absent until now). To workaround this issue we can adjust {{last_ordered_3pc}} on non-primary backup replicas on PrePrepare reception in case the replica haven't ordered any 3PC-batches in this view yet. (Currently we already do a similar adjustment of {{last_ordered_3pc}} on non-primary backup replicas but now we do it on gathering a prepared certificate rather than just on reception of a PrePrepare.)  ></body> </Action>
<Action id="52624" issue="34625" author="ashcherbakov" type="comment" created="2018-10-25 10:02:03.0" updateauthor="ashcherbakov" updated="2018-10-25 10:50:44.0"> <body><! CDATA WBS: # Find out a storage where we save (inst_id, view_no, last_pp_seq_no). Maybe use `idr_cache` for this? # Extends `_send3PCBatch_ `: if the Replica is a primary in a backup instance - persist (inst_id, view_no, last_pp_seq_no) in the cache # Clear (inst_id, view_no, last_pp_seq_no) at the end of a _regular_ view change _(not a propagate primary)_. # When propagate primary to viewNo=X is finished (`on_propagate_primary_done` in Replica): if this is a primary in a backup instance, and we have persisted (inst_id, view_no, last_pp_seq_no) where viewNo=X, then restore `{{lastPrePrepareSeqNo}} ` and `{{last_ordered_3pc}} ` accordingly, or clear persisted info. #  Create a separate ticket for this  If backup Repica sees a PrePrepare (A,B) with a gap (let's say (0, 100)), and it hasn't yet ordered anything in this view, then set `{{last_ordered_3pc}} ` to (A, B-1) and accept PrePrepare.     ></body> </Action>
<Action id="52632" issue="34625" author="spivachuk" type="comment" body="Created INDY-1779 for the last item from the previous comment." created="2018-10-25 11:49:49.0" updateauthor="spivachuk" updated="2018-10-25 11:49:49.0"/>
<Action id="53650" issue="34625" author="spivachuk" type="comment" created="2018-11-20 14:42:18.0" updateauthor="spivachuk" updated="2018-11-20 14:42:18.0"> <body><! CDATA *Changes:* - Implemented restoration of {{lastPrePrepareSeqNo}} on a backup primary after restart. - Made some renaming in the code related to view change to make it clearer. - Corrected {{sdk_send_batches_of_random}} and {{sdk_send_batches_of_random_and_check}} test helper functions. - Wrote tests for {{lastPrePrepareSeqNo}} restoration feature. - Corrected existing tests that restarted node instances which had been stopped previously. Now a new node instance is created for restarting a node. - Made advancing {{last_ordered_3pc}} on a backup replica up to master's {{last_ordered_3pc}} on completion of a regular view change (not propagate primary) conditional: now it is made only if the backup replica is not primary. - Updated tests in {{test_no_propagate_request_on_different_last_ordered_before_vc}} module according to the change in the logic of advancing {{last_ordered_3pc}} on backup replicas on a regular view change. - Fixed intermittent failures in tests in {{test_no_propagate_request_on_different_last_ordered_before_vc}} module.  *PRs:* - https://github.com/hyperledger/indy-plenum/pull/971 - https://github.com/hyperledger/indy-node/pull/1043  *Version:* - indy-node 1.6.687-master - indy-plenum 1.6.596-master  *Risk factors:* - Nothing is expected.  *Risk:* - Low  *Covered with tests:* - Tests in {{test_last_sent_pp_store_helper}} module - {{test_backup_primary_restores_pp_seq_no_if_view_is_same}} - {{test_node_erases_last_sent_pp_key_on_view_change}} - {{test_node_erases_last_sent_pp_key_on_pool_restart}} - {{test_node_erases_last_sent_pp_key_on_propagate_primary_after_pool_restart}}  *Recommendations for QA:*  Please test the following scenarios:  *I* 1. Start a pool of 4 nodes. 2. Ensure that the view is 0. 3. Send some write requests. 4. Ensure that the batches are ordered in the instance 1. 5. Restart the node which is the primary in the instance 1. 6. Send 1 write request. 7. Verify that the new batch is ordered in the instance 1.  *II* 1. Start a pool of 4 nodes. 2. Stop the master's primary. 3. Wait for a view change is started and completed. 4. Ensure that the view is 1. 5. Send some write requests. 6. Ensure that the batches are ordered in the instance 1. 7. Restart the node which is the primary in the instance 1. 8. Send 1 write request. 9. Verify that the new batch is ordered in the instance 1.  *III* 1. Start a pool of 7 nodes. 2. Send some write requests. 3. Ensure that the batches are ordered in the instances 1 and 2. 4. Stop the primary of the instance 2. 5. Stop the master's primary. 6. Wait for a view change is started and completed. 7. Start the last stopped node. 8. Start the first stopped node and ensure that it is the primary of the instance 1 now. 9. Send 1 write request. 10. Verify that the new batches are ordered in the instances 1 and 2.  *IV* 1. Start a pool of 4 nodes. 2. Send some write requests. 3. Ensure that the batches are ordered in the instance 1. 4. Stop all the nodes in the pool. 5. Start all the nodes in the pool. 6. Send 1 write request. 7. Verify that the new batch is ordered in the instance 1.  ></body> </Action>
<Action id="53733" issue="34625" author="vladimirwork" type="comment" created="2018-11-22 15:15:02.0" updateauthor="vladimirwork" updated="2018-11-22 15:49:02.0"> <body><! CDATA Build Info: indy-node 1.6.694  Steps to Reproduce (III): 1. Start a pool of 7 nodes. 2. Send some write requests. 3. Ensure that the batches are ordered in the instances 1 and 2. 4. Stop the primary of the instance 2 (3rd node). 5. Stop the master's primary (1st node). 6. Wait for a view change is started and completed. 7. Start the last stopped node (1st node). 8. Start the first stopped node (3rd node) and ensure that it is the primary of the instance 1 now. 9. Send 1 write request. 10. Verify that the new batches are ordered in the instances 1 and 2.  Actual Results: There are 3 primaries (0, 1, 2) on the 1st and 3rd node and only 2 primaries (0, 2) on the other 5 nodes. There are 4 txns ordered in the 2nd instance and only 3 txns ordered in the 1st instance. !INDY-1759.PNG|thumbnail!   ^1759.tar.gz    ></body> </Action>
<Action id="53736" issue="34625" author="spivachuk" type="comment" created="2018-11-22 16:09:30.0" updateauthor="spivachuk" updated="2018-11-22 16:10:27.0"> <body><! CDATA The behavior described in the previous comment is caused by the replica removal feature. Please re-test Scenario III with the disabled replica removal feature. To disable it, add the following parameters to the config of each node: {code:java} REPLICAS_REMOVING_WITH_DEGRADATION = None REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None {code}  ></body> </Action>
<Action id="53746" issue="34625" author="vladimirwork" type="comment" created="2018-11-23 09:09:13.0" updateauthor="vladimirwork" updated="2018-11-23 09:09:13.0"> <body><! CDATA Build Info: indy-node 1.6.694  Steps to Validate (III): 0. Set `REPLICAS_REMOVING_WITH_DEGRADATION = None` and `REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None` in the pool config. 1. Start a pool of 7 nodes. 2. Send some write requests. 3. Ensure that the batches are ordered in the instances 1 and 2. 4. Stop the primary of the instance 2 (3rd node). 5. Stop the master's primary (1st node). 6. Wait for a view change is started and completed. 7. Start the last stopped node (1st node). 8. Start the first stopped node (3rd node) and ensure that it is the primary of the instance 1 now. 9. Send 1 write request. 10. Verify that the new batches are ordered in the instances 1 and 2.  Actual Results: All nodes have the same amount of instances and the same primaries for them. All instances have ordered all requests successfully. !INDY-1759_ok_1.PNG|thumbnail!  !INDY-1759_ok_2.PNG|thumbnail!  !INDY-1759_ok_primaries.PNG|thumbnail!   ></body> </Action>
