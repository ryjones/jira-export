<Action id="56679" issue="37374" author="vladimirwork" type="comment" created="2019-02-05 06:53:23.0" updateauthor="vladimirwork" updated="2019-02-05 06:59:48.0"> <body><! CDATA Build Info: indy-node 1.6.779  Steps to Reproduce: 1. Run load test with 1 writing txn per second and 10 reading txns per second (1/10 of production load with the same txn types). 2. Stop several nodes (Node20 and Node25). 3. Stop the primary (Node1) to initiate View Change. 4. Start stopped nodes (one by one, including primary). 5. Stop the load. 6. Wait for the end of catch up on nodes which were stopped. 7. Start the same load for a short period to check ordering.  Actual Results: There is the same issue as for full production load (also it looks like Node 25 has started ordering at Step 7 but Node 1 and Node 20 have not). ev@evernymr33:logs/1983_04_02_2019_metrics.tar.gz ev@evernymr33:logs/1983_04_02_2019_logs.tar.gz  ></body> </Action>
<Action id="58939" issue="37374" author="andkononykhin" type="comment" created="2019-04-04 08:52:29.0" updateauthor="andkononykhin" updated="2019-04-04 09:00:14.0"> <body><! CDATA *PoA* # receiving timestamp is attached as *ts_rcv* attribute for any message read from network interface ** there would be a wrapper class (e.g. ZStackMessage) that hodls raw message and network metadata like sender and receiving timestamp # this value passed to upper stacks which handles incoming messages callbacks # Node class uses that value to associate with any node messages (including PrePrepares) ** there would another wrapper class that holds object presentation of the parsed message (MessageBase child) and network related metadata mentioned upper ** that would replace widely used tuples (msg, from) # once a PrePrepare is being processed that timestamp is used for estimation of message  obsolescence instead of now()  ></body> </Action>
<Action id="59123" issue="37374" author="andkononykhin" type="comment" created="2019-04-11 11:02:12.0" updateauthor="andkononykhin" updated="2019-04-11 13:09:02.0"> <body><! CDATA Problem reason: * stashed PrePrepares (e.g. received during catchup or view change) becomes obsolete if stashing time  is bigger then acceptable deviation time for PP * such case is wrong if incoming PP is not obsolete at the time when it comes  Changes: * PP obsolescence is estimated basing on the time it comes to replica instead of the time when the estimation happens  PR: *  https://github.com/hyperledger/indy-plenum/pull/1155   Version: * indy-plenum: 1.7.0.dev754​ * indy-node: 1.7.0.dev891  Risk factors: * nothing expected  Risk: * Low  Covered with tests: *  https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence.py  *  https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_fail_for_delayed.py  *  https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_pass_for_stashed.py   Recommendations for QA: * start test pool setting ACCEPTABLE_DEVIATION_PREPREPARE_SECS config parameter to lower value for one node * stop that node while other nodes are ordering * start the node after some time expecting that the node will perform catchup more than the time set for the mentioned config parameter * send set of txns * once the node completes catch up check that it's logs don't include records about suspicions on nodes with suspicion code 18  ></body> </Action>
<Action id="59193" issue="37374" author="vladimirwork" type="comment" created="2019-04-12 12:40:49.0" updateauthor="vladimirwork" updated="2019-04-12 13:31:45.0"> <body><! CDATA Build Info: indy-node 1.7.0~dev891  Steps to Reproduce: 1. Start test pool setting ACCEPTABLE_DEVIATION_PREPREPARE_SECS config parameter to lower value for one node (60). 2. Stop that node while other nodes are ordering (1 txn/sec). 3. Start the node after some time expecting that the node will perform catchup more than the time set for the mentioned config parameter send set of txns.  Actual Results: Node perfroms catch up under this load and orders but there are some suspicions codes appear (including code 18): {noformat} 2019-04-12 11:47:25,275|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,280|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,281|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,288|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,289|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,295|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,295|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,303|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,303|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,310|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,311|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,317|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:25,318|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18 2019-04-12 11:47:29,526|WARNING|node.py|Node3 raised suspicion on node Node4 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:47:29,526|WARNING|node.py|Node3 raised suspicion on node Node1 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:47:29,529|WARNING|node.py|Node3 raised suspicion on node Node4 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:47:29,530|WARNING|node.py|Node3 raised suspicion on node Node1 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:47:29,533|WARNING|node.py|Node3 raised suspicion on node Node4 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:47:29,533|WARNING|node.py|Node3 raised suspicion on node Node2 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:47:29,534|WARNING|node.py|Node3 raised suspicion on node Node1 for COMMIT message has already received; suspicion code is 8 2019-04-12 11:54:24,646|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20 2019-04-12 11:54:24,646|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20 2019-04-12 11:54:27,371|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20 2019-04-12 11:54:27,371|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20 2019-04-12 11:57:43,736|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20 2019-04-12 11:57:43,736|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20 2019-04-12 11:57:44,141|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20 2019-04-12 11:57:44,142|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20 {noformat}  Info logs:  ^1983_fail_case.tar.gz    Debug logs: ev@evernymr33:logs/1983_DEBUG.tar.gz  ></body> </Action>
<Action id="59214" issue="37374" author="andkononykhin" type="comment" created="2019-04-12 16:54:24.0" updateauthor="andkononykhin" updated="2019-04-12 16:57:05.0"> <body><! CDATA Debug logs exploration results: # all PP timestamps checks failed on Node3:1 backup replica # view change didn't take place # suspicions for incorrect PP time were raised 103 times (lowest ppSeqNo = 242, highest = 561): ## *ppSeqNo=242*: *** *ts diff ~199* secs (sent by Node2:1 at 13:11:21, stashed due to catchup by Node3:1 at 13:14:40) *** *suspicion* was raised ## *ppSeqNo=401* (as a middle of the range): *** *ts diff ~112* secs (sent by Node2:1 at 13:13:10, stashed due to catchup by Node3:1 at 13:15:02) *** *suspicion* was raised ## *ppSeqNo=561*: *** *ts diff ~61* secs (sent by Node2:1 at 13:17:20, reported as received by Node3:1 at 13:18:21) *** *suspicion* was raised ## I checked also *ppSeqNo=563*: *** *ts diff ~59* secs (sent by Node2:1 at 13:17:22, reported as received by Node3:1 at 13:13:21) *** *no suspicion* was raised  Keeping in mind that pp obsolescence interval  (ACCEPTABLE_DEVIATION_PREPREPARE_SECS) was set to 60 secs I may say that all mentioned cases fit designed logic.  ></body> </Action>
<Action id="59215" issue="37374" author="andkononykhin" type="comment" body=" ~ashcherbakov   ~VladimirWork  Please take a look at the comment above. I didn&apos;t find any issues related to the task and I don&apos;t think that it is necessary to tune the test scenario since in any case we would have probability of getting suspicions in logs. Seems the original recommendation for QA was not enough accurate: the task&apos;s goal was not to get rid of PP time suspicions but to make the PP times check more robust. Thus, the current tests results and the exploration I made are seemed as good verification for the task. Do you think we need more QA work here?" created="2019-04-12 17:08:47.0" updateauthor="andkononykhin" updated="2019-04-12 17:08:47.0"/>
