<Action id="48071" issue="28974" author="vladimirwork" type="comment" created="2018-07-28 14:24:54.0" updateauthor="vladimirwork" updated="2018-07-28 14:24:54.0"> <body><! CDATA Traffic Shaper tool allows to configure 3 network parameters: - latency - drop percent - bandwidth (upload\download)  So we need to decide: - which of this parameters we will use during load testing (only one parameter enabled at the same time or several?) - how many nodes will be affected by traffic shaping at the same time (1 bad node, f, n-f, all?) - how long we should shape the traffic imitating brownout and how long network should be good after it - what values of traffic shaping are acceptable (is it acceptable to set >5..10 seconds latency / >25..50% drop / <1kbps bandwidth ?) - what load will be applied (which txn types / how many clients / how many txns by each client) - what is the acceptance criteria (what is acceptable time for catch up / consensus recovery / % of failed requests)  FYI  ~ashcherbakov   ~ozheregelya   ~zhigunenko.dsr   ></body> </Action>
<Action id="48087" issue="28974" author="ashcherbakov" type="comment" created="2018-07-30 05:00:11.0" updateauthor="ashcherbakov" updated="2018-07-30 05:00:11.0"> <body><! CDATA I think we should start with light random conditions, ideally with random ones on all nodes. For example, latency from 0 to 5 secs, drop from 0 to 10%.  Â   ></body> </Action>
<Action id="48121" issue="28974" author="ashcherbakov" type="comment" body=" ~mgbailey  Do you have any thinking about how we should configure latency/drop to emulate the live network as much as possible?" created="2018-07-30 14:49:58.0" updateauthor="ashcherbakov" updated="2018-07-30 14:49:58.0"/>
<Action id="48155" issue="28974" author="mgbailey" type="comment" body=" ~ashcherbakov  ^network_pings.tgz  I am attaching hourly pings from our nodes on the STN to the other nodes on the network. This should get you an idea of the latency." created="2018-07-30 21:56:10.0" updateauthor="mgbailey" updated="2018-07-30 21:56:10.0"/>
<Action id="48645" issue="28974" author="vladimirwork" type="comment" created="2018-08-09 15:25:36.0" updateauthor="vladimirwork" updated="2018-08-09 15:34:40.0"> <body><! CDATA Things tried and found: Continuous load against 25 nodes AWS pool with the next network latencies:  - 250..500 ms - 500..1000 ms - 1000..2000 ms  Also there were cases with random package drop (1..10%) but since we use tcp/ip *it causes just to additional latency* (clients and nodes resend dropped packages due to response absence) so in main cases we use just latency settings without any drop. In all cases we sustain up to *10 writing txns/sec* (in cases with all txn types excluding revocation and payments) / up to *20 writing txns/sec* (in cases with NYMs only) together with up to *250 reading txns/sec* from up to *2800 client connections* (~800 writing and ~2000 reading) for *more than 8 hours* without any lagged nodes and consensus loss (we fall due to node no space left error because of large ledger and node metrics) so I think we can operate 72 hours against pool with increased HDD and it will be checked during the followng load test runs.  Thus, we see that increased pool network latency just slows down communication with client so we are forced to increase outgoing client load to provide target values of pool incoming load and its performance.  ></body> </Action>
