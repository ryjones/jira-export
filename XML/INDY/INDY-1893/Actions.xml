<Action id="54397" issue="35882" author="mgbailey" type="comment" created="2018-12-06 00:15:04.0" updateauthor="mgbailey" updated="2018-12-06 05:54:42.0"> <body><! CDATA More logs that are too big to attach:    https://drive.google.com/file/d/1lInLBdrhl8ANyLvcmi6Ex2A-gAPZLc7I/view?usp=sharing    https://drive.google.com/file/d/1gKRINx3Sy3r8urqOw0HZp-z1KWj_Wdc4/view?usp=sharing   https://drive.google.com/file/d/14MjSfhyX9UiAokG8SiJsHLdzBhbuqWtN/view?usp=sharing  ></body> </Action>
<Action id="54406" issue="35882" author="ashcherbakov" type="comment" body="Currently client requests are not processed during a view change. I think we should keep this for write requests only, while read requests need to be processed even during the view change." created="2018-12-06 07:01:00.0" updateauthor="ashcherbakov" updated="2018-12-06 07:01:00.0"/>
<Action id="54413" issue="35882" author="ashcherbakov" type="comment" created="2018-12-06 09:03:03.0" updateauthor="ashcherbakov" updated="2018-12-06 09:04:29.0"> <body><! CDATA {quote}At about 10:45 MST 6 nodes on TestNet owned by Evernym were rebooted simultaneously to apply a kernel update. {quote}  ~mgbailey  Taken into account that the size of the pool is 18 (so f = 5), then 6 nodes is f+1. Restarting f+1 nodes at the same time (including Primary) means that the other nodes will start a View Change that can not be finished (since there are only n-f-1 nodes are left, which is less than minimal number of working nodes).  We will investigate exact reasons for this and do necessary fixes if needed, but in general I think we need to follow the rule, that *we should not have more than f nodes down (unless the whole pool is restarted) at the same time*. It's better to restart the nodes one by one.     ></body> </Action>
<Action id="54429" issue="35882" author="derashe" type="comment" created="2018-12-06 11:22:53.0" updateauthor="derashe" updated="2018-12-06 11:25:50.0"> <body><! CDATA After inverstigating the attached logs, we've found the next situation happened: * During restart after disconnecting of 6 nodes from pool (including primary) view change was inited on the rest of the pool (at 17:47). Quorum of Instance_changes (n-f) was collected because few nodes (of those 6) sent Instance_change before they've been restarted (because of slowness reason). Other Instance_change messages were collected when primary disconnected. * As a result 18-6 = 12 nodes stuck in unfinished view_change with view_no = 3 and no primary. * After "6 group" brought back to live, it saw primary as connected and did not do a view_change (stayed on view_no = 2). * So we have groups of 6 and 12 nodes. Neither of both have quorum to commit or end view_change  ></body> </Action>
<Action id="54431" issue="35882" author="ashcherbakov" type="comment" created="2018-12-06 11:32:28.0" updateauthor="ashcherbakov" updated="2018-12-06 11:32:28.0"> <body><! CDATA So, the following fixes will be done as a result of analysis: # INDY-1897: View Change needs to be triggered in BFT way # INDY-1896: process read requests and actions (POOL_RESTART) during view change.  ></body> </Action>
<Action id="54692" issue="35882" author="mgbailey" type="comment" body="Another log: https://drive.google.com/file/d/17Nj54NUAdeKyn2Jx3POa86rmeeEH5AFO/view?usp=sharing" created="2018-12-11 22:17:36.0" updateauthor="mgbailey" updated="2018-12-11 22:17:36.0"/>
