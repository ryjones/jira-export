<Issue id="42589" key="INDY-2235" number="2235" project="10303" reporter="ashcherbakov" assignee="vladimirwork" creator="ashcherbakov" type="10002" summary="Save PrePrepare&apos;s BatchID in audit ledger and restore list of preprepares and prepares on node startup" priority="3" resolution="10000" status="10001" created="2019-10-02 12:52:26.0" updated="2019-10-29 15:15:36.0" resolutiondate="2019-10-29 15:15:36.0" votes="0" watches="4" workflowId="55657"> <description><! CDATA *Problem* * In order to successfully finish ViewChange (not go to infinite loop), we need a quorum of `preprepared` and `prepared` messages (P and Q) in consensus shared data. * We guarantee to get the quorum if all nodes are up and no new nodes are added * But if nodes are restarted or just added, they don't have `preprepared` and `prepared` filled up. * Please note, that this can help with requests alÐºeady ordered, as well as with the new nodes joining the pool. If there are requests in flight (not ordered on all nodes) like in INDY-2238, then this will not help and can still lead to infinite view change. Persisiting of 3PC messages (INDY-2238) looks like the only solution here.  *Acceptance criteria* * Add PrePrepare's digest to audit ledger ** See  https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/batch_handlers/audit_batch_handler.py  ** Write unit tests: ***  https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/audit_ledger/helper.py#L17  ***  https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/audit_ledger/test_audit_ledger_handler.py  * Restore `preprepared` and `prepared` in consensus shared data when a node starts up (after initial catch-up) from the audit Ledger ** See  https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/consensus/consensus_shared_data.py  ** See  https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/consensus/ordering_service.py#L2163  ** Write unit tests: ***  https://github.com/hyperledger/indy-plenum/tree/master/plenum/test/consensus/order_service   * Unskip `test_view_change_after_back_to_quorum_with_disconnected_primary`, make sure it passes * Consider writing more integration tests, for example: ** Test1: *** Delay commits on Node1 *** Send a request and make sure it's ordered on Nodes2-4 *** Restart Nodes2-4 *** Start view change *** Make sure that view change is finished, and all nodes have equal data ** Test2: *** Delay commits on Nodes1 and 2 *** Send a request and make sure it's ordered on Node3 and 4 *** Restart Nodes 3 and 4 *** Start view change *** Make sure that view change is finished, and all nodes have equal data  ></description> </Issue>
