<Action id="24268" issue="17082" author="krw910" type="comment" created="2017-05-24 21:21:56.0" updateauthor="krw910" updated="2017-05-24 21:21:56.0"> <body><! CDATA From  ~mzk-vct   Problem reason: Initial problem was in usage of _defaultStore in Ledger - it is static method and it is overrided in Plenum. There were some incompatibilities with Plenum and Sovrin Node that required modification of Ledger Changes: fixed usage of _defaultStore move copying of default transactions files from Node (Plenum) to FileStore and successors of it added some new tests Builds: sovrin-node=0.3.113 sovrin-client=0.3.105 Risk factors: There was one test failing because of timeout, I had to skip it https://evernym.atlassian.net/browse/SOV-1123 Covered with tests: ledger/test/test_chunked_file_store.py ledger/test/test_ledger_chunked_store.py ledger/test/test_file_stores_equailty.py all tests of plenum, node and client  ></body> </Action>
<Action id="24347" issue="17082" author="krw910" type="comment" body="Unable to upgrade existing nodes to the new chunked files. After upgrade the pool would not work. We need an upgrade path." created="2017-05-25 16:20:24.0" updateauthor="krw910" updated="2017-05-25 16:20:24.0"/>
<Action id="24442" issue="17082" author="krw910" type="comment" body="Note for  ~krw910  when you test this upgrade from node build 110 to the build that has the migration fixed." created="2017-05-26 13:24:06.0" updateauthor="krw910" updated="2017-05-26 13:24:06.0"/>
<Action id="24483" issue="17082" author="krw910" type="comment" created="2017-05-26 17:17:43.0" updateauthor="krw910" updated="2017-05-26 17:17:43.0"> <body><! CDATA  ~andrey.goncharov  after reviewing the implemetation with  ~tharmon  and  ~nage  we came up with some suggestions on how to improve the implementation of chunking up the files. We don't want to end up with 25 years of files in one directory as this can cause issues. We also felt that breaking up the files at a 1,000 transactions was small. Here are Nathan's suggestions. If you have questions around this please communicate with Nathan.  # We should have a file extension on each file. # Better breaking up of the files -- If possible can we break up the chunks by size like 100 MB or 200 MB files? If that is not the right solution lets at least increase the transaction count per file. # We should zero pad the files so the ordering is easier to deal with 000001, 000002, 000003 and so on. # Having all the files in one directory can also be an issue. We need to think about how this will look in 20 years of running. We should break up the files into sub-directories. Determining how many files to have in a given directory will also help determine how many digits to use in zero padding the files.  Let me know if you have questions and we can get more clarity and input from Nathan as needed.  ></body> </Action>
<Action id="24485" issue="17082" author="ashcherbakov" type="comment" body="Please include  ~lovesh  into this discussion. " created="2017-05-26 17:21:10.0" updateauthor="ashcherbakov" updated="2017-05-26 17:21:10.0"/>
<Action id="24504" issue="17082" author="lovesh" type="comment" created="2017-05-26 19:06:52.0" updateauthor="lovesh" updated="2017-05-29 08:40:05.0"> <body><! CDATA  ~krw910   ~tharmon   ~nage  # I am ok with adding extensions, something like indicating which kind of ledger it is.  # Breaking by size is not correct since that does not allow us to locate a particular transaction's file, increasing the size is ok but  1000 is a good number, since if the files are big then you are iterating over a large file to get to a transaction since our transactions are not fixed size. The way you look up a transaction with a sequence number in such an implementation is by getting 2 parts of data for a sequence number, a file name and a line offset (its a line offset, not a byte offset so you cant fseek). # I dont understand why zero pad, since its unicode data # Agree to having multiple directories.  ></body> </Action>
<Action id="24528" issue="17082" author="tharmon" type="comment" created="2017-05-26 21:07:01.0" updateauthor="tharmon" updated="2017-05-26 21:46:58.0"> <body><! CDATA  ~lovesh   *#2*  -First of all, if I understand correctly, these are the historical record of what's gone into the ledger, so there should be no case where the validator is actually iterating over them. Lookups should be happening from the database backend, not the file system.-  -Second of all, this design needs to handle the needs of a large network with hundreds of thousands of transactions a day. So, no, 1000 is not a good size. All that's going to do is create a huge number of files on the filesystem. This can cause a number of difficulties, not just with client tools, but also with the memory paging in the OS.-  -In the one case where one would be reading through the files (i.e., doing a rebuild), this should be done via a sequential read, not slurping the file into memory, so there's really not a seek issue, either.-  -I can be convinced to of fixed transactions vs. fixed size, but if the filename is recording what transaction starts at that point, the only downside is that it becomes slower to determine which file to look in (if doing a seek, which I still don't understand the need for) because the names aren't necessarily consistent.-  *#3*  This forces two different things * The zero padding approach requires that we actually sit down and decide how many transactions need to be handled. Once we have that answer, determining the strategy becomes much easier. * As as system admin, I don't want files interleaving with one another in the directory listing just because the file naming scheme doesn't zero-pad. I want to be able to sort the files simply and easily, and creation time or last access time can't be trusted. So, it needs to be done with the filename.  To be clear, the filename {{1000.log}} should be {{0000000000000001000.log}} instead.  ></body> </Action>
<Action id="24534" issue="17082" author="tharmon" type="comment" created="2017-05-26 21:49:30.0" updateauthor="tharmon" updated="2017-05-26 21:49:30.0"> <body><! CDATA Having had some additional conversations here, I'm withdrawing part of my above comment. That said, if we are, in fact, sequentially reading through these files to find specific transactions, we need to rethink this strategy. There are some techniques that we could use to remove this overhead, which will only grow as the ledger becomes larger.  Do we have any documentation that outlines exactly the interaction between the database and the files? Specifically, how is the read of these files implemented and under what circumstances is it done?  ></body> </Action>
<Action id="24577" issue="17082" author="lovesh" type="comment" created="2017-05-29 09:02:07.0" updateauthor="lovesh" updated="2017-05-29 09:02:07.0"> <body><! CDATA  ~tharmon  I did not realise that the zero pad was about file names. I still don't understand the value of zero padding the file name, is the concern that file with name `109.log` will appear before `2.log` in the output of `ls`, if yes then `ls -1v *.log` fixes that. Regarding reading from these files, we don't have documentation but here is a brief:  # A node reads from these files when another node is trying to sync with it and requests some transactions in a sequence with no gaps  # A client is asking the reply for an already processed request which will be looked up by a sequence number. The files are read into memory completely since keeping them small is the objective anyway.  ></body> </Action>
<Action id="24585" issue="17082" author="andrey.goncharov" type="comment" created="2017-05-29 15:19:40.0" updateauthor="andrey.goncharov" updated="2017-05-29 15:19:40.0"> <body><! CDATA  ~tharmon   I've been testing my migration tool and found out that we won't be able to introduce a migration tool and apply our first migration at the same time. It happens because we can not update our update manager and have the result right away. In other words we can update the update procedure but it will take effect only for the next update and so on.  Still I can create a migration for you, but you will have to apply it manually for each node in the pool to do the migration to chunked file storage. Would you be willing to do that?  ></body> </Action>
<Action id="24636" issue="17082" author="andrey.goncharov" type="comment" body=" ~tharmon   ~lovesh  Created a different story about  improving Chunked File Storage|https://jira.hyperledger.org/browse/INDY-104 . This story is about creating a migration for existing nodes" created="2017-05-30 12:56:31.0" updateauthor="andrey.goncharov" updated="2017-05-30 12:56:31.0"/>
<Action id="24649" issue="17082" author="tharmon" type="comment" body=" ~andrey.goncharov , the test pool I need toÂ upgrade is far enough behind that I&apos;m expecting that we&apos;ll have to rip and replace in order to do the upgrade. That means we&apos;ll be reseting the ledger for that test pool, and that I&apos;ll have to be touching each machine. That&apos;s fine, as this is a test pool, but we should make sure we can identify these things moving forward so we can do staged releases when necessary." created="2017-05-30 14:16:32.0" updateauthor="tharmon" updated="2017-05-30 14:16:32.0"/>
<Action id="24651" issue="17082" author="andrey.goncharov" type="comment" created="2017-05-30 14:23:27.0" updateauthor="andrey.goncharov" updated="2017-05-30 14:23:27.0"> <body><! CDATA  ~tharmon  thank you!   ~stevetolman  shouldn't we mark this ticket as invalid then?  ></body> </Action>
<Action id="24654" issue="17082" author="tharmon" type="comment" created="2017-05-30 14:27:54.0" updateauthor="tharmon" updated="2017-05-30 14:27:54.0"> <body><! CDATA  ~lovesh , I understand {{ls -1v *.log}} "fixes" this on Linux. However, in my opinion that's not a good enough reason to allow log files to interleave on a standard {{ls}} when zero padding fixes it on all operating systems. Also, when I put my sysadmin's hat on, interleaved log files is just plain sloppy.  Plus, as mentioned before, zero padding will force us to actually consider how many transactions and files this system will have to realistically support. This will then allow us to make sure we have a plan to appropriately store the number of files and transactions across multiple operating systems and file system types.  Regarding the chunk size of 1000, several of us (including  ~danielhardman  and  ~krw910 ) were discussing this the other day. The small size makes sense if we are doing a sequential read of the files from a speed perspective. However, we could greatly speed the system up if we did the following: * Each transaction file was accompanied by a lookup file. * Each lookup file would contain: ** The hash of the associated transaction file ** Fixed length records that recorded the byte offset for each record in the transaction file.  This would allow the following lookup process: # Open the appropriate lookup file. # Seek to the appropriate record in the lookup file. # Read the byte offset for the record in the transaction file. # Close the lookup file. # Open the transaction file. # Seek to the appropriate record in the transaction file. # Read the transaction record. # Close the transaction file.  In cases where we wanted to do the sequential reads, we still could. We'd also have a known hash for each transaction file that could be quickly checked. It would also allow us to have far more than just 1000 records in the transaction file, which has a number of other benefits.  ></body> </Action>
<Action id="24716" issue="17082" author="krw910" type="comment" body="This was completed as originally planned. We held this ticket up because we could not upgrade to this change. We decided not to hold up this ticket for the upgrade since we have other tickets dealing with that." created="2017-05-31 03:47:31.0" updateauthor="krw910" updated="2017-05-31 03:47:31.0"/>
<Action id="24750" issue="17082" author="lovesh" type="comment" body=" ~tharmon  What&apos;s wrong if we don&apos;t know the limit files/txns even though each file systems will have max limit on no of files. If we are maintaining metadata (lookup files) then why not maintain a BTree in the lookup file (just one lookup file for the complete ledger) and secondly is the hash needed to verify integrity of each file then we already have a merkle tree and nodes can request merkle root from any node for till a subset of txns but this is just one alternative, we dont loose much by storing hash of each file but i will rather maintain a btree like structure if we are keeping additional data, and if we are doing so much work, why not use an embedded database for ledger" created="2017-05-31 11:55:59.0" updateauthor="lovesh" updated="2017-05-31 11:55:59.0"/>
