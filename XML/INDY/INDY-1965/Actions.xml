<Action id="56313" issue="37176" author="vladimirwork" type="comment" created="2019-01-29 07:42:56.0" updateauthor="vladimirwork" updated="2019-01-29 07:42:56.0"> <body><! CDATA Build Info: indy-node 1.6.772  Steps to Reproduce: 1. Run production load test for 8 hours.  Actual Results: Pool was in consensus at the end of load test but logs and metrics should be investigated: ev@evernymr33:logs/1965_28_01_2019_metrics.tar.gz ev@evernymr33:logs/1965_28_01_2019_logs.tar.gz  ></body> </Action>
<Action id="56468" issue="37176" author="toktar" type="comment" created="2019-01-31 20:30:39.0" updateauthor="toktar" updated="2019-01-31 20:30:39.0"> <body><! CDATA *Problem reason* * Nodes are working slow with lots of stashing messages in logs.  *Fix* * Move logs about stashing to TRACE from INFO  *PR* *  https://github.com/hyperledger/indy-plenum/pull/1069   *Build* * 1.6.778 - indy-node - master * 1.6.669 - indy-plenum - master  ></body> </Action>
<Action id="56487" issue="37176" author="vladimirwork" type="comment" created="2019-02-01 08:54:04.0" updateauthor="vladimirwork" updated="2019-02-01 08:54:04.0"> <body><! CDATA Build Info: indy-node 1.6.776  Steps to Reproduce: 1. Run production load test for 8+ hours.  Actual Results: Pool was in consensus at the end of load test but logs and metrics should be investigated: ev@evernymr33:logs/1965_01_02_2019_metrics.tar.gz ev@evernymr33:logs/1965_01_02_2019_logs.tar.gz !INDY-1965_01_02_2019.PNG|thumbnail!   ></body> </Action>
<Action id="56650" issue="37176" author="derashe" type="comment" created="2019-02-04 16:40:52.0" updateauthor="derashe" updated="2019-02-04 17:16:38.0"> <body><! CDATA While researching this issue we did some optimization to propagate handling process.  PR:  https://github.com/hyperledger/indy-plenum/pull/1071   Node ver: 781  ></body> </Action>
<Action id="56689" issue="37176" author="vladimirwork" type="comment" created="2019-02-05 15:22:44.0" updateauthor="vladimirwork" updated="2019-02-05 15:22:44.0"> <body><! CDATA Build Info: indy-node 1.6.781  Steps to Reproduce: 1. Run production load test for 8+ hours.  Actual Results: Pool was in consensus at the end of load test but logs and metrics should be investigated: ev@evernymr33:logs/1965_05_02_2019_metrics.tar.gz ev@evernymr33:logs/1965_05_02_2019_logs.tar.gz !INDY-1965_05_02_2019.PNG|thumbnail!   ></body> </Action>
<Action id="56840" issue="37176" author="ozheregelya" type="comment" created="2019-02-11 12:10:17.0" updateauthor="ozheregelya" updated="2019-02-11 12:10:17.0"> <body><! CDATA Environment: indy-node 1.6.798  Steps to Reproduce: 1. Set up the pool. 2. Add to the node config Case 1: {code:java} MAX_REQUEST_QUEUE_SIZE = 10000 NODE_TO_NODE_STACK_QUOTA = 1000{code} Case 2: {code:java} MAX_REQUEST_QUEUE_SIZE = 10000 NODE_TO_NODE_STACK_QUOTA = 1000 PROPAGATE_REQUEST_DELAY = 300{code} 3. Run the production load without plugins.  Logs and metrics: s3://qanodelogs/indy-1965 To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1965/case1/ /home/ev/logs/qanodelogs/indy-1965/case1/ aws s3 cp --recursive s3://qanodelogs/indy-1965/case2/ /home/ev/logs/qanodelogs/indy-1965/case2/  ></body> </Action>
<Action id="56861" issue="37176" author="derashe" type="comment" created="2019-02-11 15:30:59.0" updateauthor="derashe" updated="2019-02-11 15:30:59.0"> <body><! CDATA Case 1 showed us working pool with stable performance and no anomalyes. It seems, that raising our node stack quotas helped node to survive bounce of propagate requests.  Example of propagate bounce:  !image-2019-02-11-18-18-20-176.png|thumbnail! . In the top red circle we can see that node stack was bounced up to 600 node messages. After that we was able to handle all of these messages and continue stable ordering.   As a result of testing, we've decided to set these configs as default:  _MAX_REQUEST_QUEUE_SIZE = 10000_ _NODE_TO_NODE_STACK_QUOTA = 1000_  ></body> </Action>
<Action id="56862" issue="37176" author="derashe" type="comment" body="PR: https://github.com/hyperledger/indy-plenum/pull/1083" created="2019-02-11 15:37:24.0" updateauthor="derashe" updated="2019-02-11 15:37:24.0"/>
<Action id="56930" issue="37176" author="derashe" type="comment" created="2019-02-12 15:16:31.0" updateauthor="derashe" updated="2019-02-12 15:16:31.0"> <body><! CDATA Problem reason: * During the load, nodes starting to ask for propagates because of getting preprepare with unfinalized requests. Some nodes get enough propagates and continue ordering, but other part is starting to ask more and more propagates, what leads us to Node falling because of OOM. Such a nodes cannot participate in consensus.  Changes: * During inverstigation, we found, that raising bound for node stack messages allow nodes to survive and succesfully request all propagates. * Bound was raised up to 1000 requests per one iteration (was 100). That number based on successfull load testing results and our assumption of approximate propagates request count. * We left maximum message size for one prod run the same to avoid probable issues.  Committed into: * changed config https://github.com/hyperledger/indy-plenum/pull/1083  Risk: * There is a risk, that we can be spammed by some unexpected traffic (from malicious node for example). But as we left messages size bound the same, this risk is minimized  Covered with tests: * load tests above  Recommendations for QA:  * Retest the case above with plugins to ensure that they don't affect this problem * Retest the case of DDOS with plugins to look if there are any performance improvements  ></body> </Action>
<Action id="57016" issue="37176" author="vladimirwork" type="comment" created="2019-02-14 07:41:08.0" updateauthor="vladimirwork" updated="2019-02-14 07:41:08.0"> <body><! CDATA Build Info: indy-node 1.6.803  Steps to Validate: 1. Run prod load with payments with {noformat} MAX_REQUEST_QUEUE_SIZE = 10000 NODE_TO_NODE_STACK_QUOTA = 1000 {noformat} in indy_config.py for 6+ hours.  Actual Results: Pool was in consensus at the end of load, domain and token ledgers txn count was consistent. More info can be found in ev@evernymr33:logs/1965_13_02_2019_metrics.tar.gz  ></body> </Action>
<Action id="57048" issue="37176" author="ozheregelya" type="comment" created="2019-02-14 16:40:11.0" updateauthor="ozheregelya" updated="2019-02-14 16:40:11.0"> <body><! CDATA Case 2: DDoS load  !ddos1965.png|thumbnail!  Load test was running with load rate 70 txns/sec (production mix) during 20 minutes. 77K txns were written, 26K txns were lost (19K of them since NACKs view change). All txhs were written during ~1 hour since the start of test. After that pool worked under production load without any issues and all nodes were in sync.  ></body> </Action>
<Action id="57065" issue="37176" author="vladimirwork" type="comment" created="2019-02-15 09:18:04.0" updateauthor="vladimirwork" updated="2019-02-15 09:18:04.0"> <body><! CDATA Build Info: indy-node 1.6.803  Steps to Validate: 1. Run prod load with payments with  MAX_REQUEST_QUEUE_SIZE = 10000 NODE_TO_NODE_STACK_QUOTA = 1000 in indy_config.py for 10+ hours.  Actual Results: Pool was in consensus at the end of load, domain and token ledgers txn count was consistent. More info can be found in ev@evernymr33:logs/1965_15_02_2019_metrics.tar.gz  ></body> </Action>
<Action id="57068" issue="37176" author="derashe" type="comment" created="2019-02-15 09:51:45.0" updateauthor="derashe" updated="2019-02-15 09:52:11.0"> <body><! CDATA Here is result of 10 tps load. It looks pretty stable. We have propagate request bounce at the end. But, unfortunatelly, we turned pool off.   Ticket can be closed because load became stable.  !image-2019-02-15-11-08-33-682.png|thumbnail!  ></body> </Action>
<Action id="57078" issue="37176" author="derashe" type="comment" body="PR:  https://github.com/hyperledger/indy-plenum/pull/1090 " created="2019-02-15 15:47:39.0" updateauthor="derashe" updated="2019-02-15 15:47:39.0"/>
