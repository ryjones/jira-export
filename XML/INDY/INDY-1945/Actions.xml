<Action id="56721" issue="36811" author="vladimirwork" type="comment" body="(i) QA: we should perform INDY-1985 steps during production load test *with and without* payment txns at 10w / 100r load rate." created="2019-02-06 14:18:28.0" updateauthor="vladimirwork" updated="2019-02-06 14:18:35.0"/>
<Action id="57538" issue="36811" author="sergey.khoroshavin" type="comment" created="2019-02-25 13:25:44.0" updateauthor="sergey.khoroshavin" updated="2019-03-15 09:10:22.0"> <body><! CDATA *Plan of Attack* *  done  Implement catchup utils module ** CatchupDataProvider - catchup specific minimalistic interface to node ** Some of catchup message builders and validators *  done  Implement ClientSeederService and NodeSeederService ** Depends on CatchupDataProvider ** Doesn't have internal state other than injectected through data provider ** Handles LEDGER_STATUS and CATCHUP_REQ messages ** Makes it possible to move most of logic not depending on current catchup state out of LedgerManager *  done  Implement ConsProofService (and probably come up with a better name for it) ** Main purpose is to find out catchup till seq_no by gathering quorum of CONSISTENCY_PROOFS ** Depends on CatchupDataProvider ** Each instance is tied to specific ledger ** Has method start to start subprotocol and some form of callback when it comes up with a safe seq_no that can be used by CatchupRepService ** Handles LEDGER_STATUS and CONSISTENCY_PROOF messages *  done  Implement CatchupRepService ** Main purpose is to actually gather transactions from other nodes once catchup till seq_no is known from ConsProofService ** Depends on CatchupDataProvider ** Each instance is tied to specific ledger ** Has method start(seq_no, root_hash) to start subprotocol and some form of callback to notify NodeOneLedgerLeecherService that ledger catchup is finished ** Handles CATCHUP_REP messages *  done  Implement LedgerLeecherService ** Depends on CatchupDataProvider ** Each instance is tied to specific ledger ** Aggregates ConsProofService and CatchupRepService ** Has method start with optional parameter last_txn *** If last_txn is provided then it will start CatchupRepService *** If last_txn is not provided then it will first try to get it using ConsProofService ** Has some form of callback to notify NodeLeecherService that ledger catchup is finished ** Makes it possible to move individual ledger catchup logic out of LedgerManager *  done  Implement NodeLeecherService ** Depends on CatchupDataProvider ** Aggregates multiple LedgerLeecherServices ** Has explicit state *** CatchingUpPoolLedger *** CatchingUpAuditLedger *** CatchingUpNormalLedger *** Done ** Has potential to catch up ledgers in parallel in CatchingUpNormalLedger state ** Has method start which enters CatchingUpAuditLedger and starts catchup of audit ledger ** When audit ledger is caught up catch up other ledgers using received data ** If audit ledger catch up fails under some timeout catch up other ledgers the old way *  optional  Implement CatchupService ** Aggregates ClientSeederService, NodeSeederService and NodeLeecherService ** Just a simple wrapper around multiple catchup-related services ** Replaces most of functionality of LedgerManager  *Integration and testing strategy*  After implementing each of new services (with corresponding unit tests) delete their functionality from LedgerManager and fix all failing integration tests. Pros: * code gets integrated to main codebase ASAP * existing integration tests can catch problems that may slip through unit tests * integrating code early can help detect design flaws and other problems early * gradually reducing amount of code in LedgerManager will make it more manageable, which in turn can help  ></body> </Action>
<Action id="58177" issue="36811" author="sergey.khoroshavin" type="comment" created="2019-03-15 09:39:04.0" updateauthor="sergey.khoroshavin" updated="2019-03-15 09:39:04.0"> <body><! CDATA *Problem reason* Old catchup logic could lead to failures when there were writes to several ledgers during catchup. This was because size to catch up was queried independently and at different times  for each ledger.  *Changes* New catchup logic is implemented. It catches up audit ledger using old logic, but sizes to catch up to for other ledgers are determined from audit ledger, so in effect size to catch up for all ledgers is determined atomically, which solves problem. Furthermore, in many cases catch up should happen significantly faster since it needs to request and gather ledger statuses and consistency proofs only for audit ledger (previously it did this for all ledgers). Also in process old code was refactored quite heavily.  *PRs:* https://github.com/hyperledger/indy-plenum/pull/1098 https://github.com/hyperledger/indy-plenum/pull/1101 https://github.com/hyperledger/indy-plenum/pull/1104 https://github.com/hyperledger/indy-plenum/pull/1108 https://github.com/hyperledger/indy-plenum/pull/1109 https://github.com/hyperledger/indy-plenum/pull/1111 https://github.com/hyperledger/indy-plenum/pull/1113 https://github.com/hyperledger/indy-plenum/pull/1114 https://github.com/hyperledger/indy-plenum/pull/1115 https://github.com/hyperledger/indy-plenum/pull/1117 https://github.com/hyperledger/indy-plenum/pull/1118 https://github.com/hyperledger/indy-plenum/pull/1120 https://github.com/hyperledger/indy-plenum/pull/1123  *Version:* indy-plenum: 1.6.726-master indy-node: 1.6.861-master  *Risk:* Medium  *Covered with tests:* https://github.com/hyperledger/indy-plenum/blob/1.6.726-master/plenum/test/node_catchup_with_3pc/test_slow_catchup_while_ordering.py Also lots of existing integration tests were applicable, since majority of work was just a refactoring  *Recommendations for QA* This is going to be tested in scope of INDY-1993  ></body> </Action>
