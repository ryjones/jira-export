<Issue id="24894" key="INDY-1025" number="1025" project="10303" reporter="krw910" creator="krw910" type="10004" summary="Pool stopped working and lost consensus while new node was performing a catch-up" priority="2" resolution="10000" status="10001" created="2017-12-08 19:08:11.0" updated="2019-03-29 20:32:29.0" resolutiondate="2019-03-29 20:32:29.0" votes="0" watches="3" workflowId="24896"> <description><! CDATA The pool lost it's ability to reach consensus while a new node was performing a catch-up. I don't have detailed logs only at the info level.  *Setup* I have pool of 13 nodes with 5,022 transactions. I was adding 3 more nodes to the pool (14, 15, 16)  *Steps* # I added Node14 and let it perform a catch up before adding the next node # I added Node15 to the pool after Node14 was at 5,022 transactions # I then added Node16 after Node15 was at 5,022 transactions  *Other Info* * The catch-up is pretty fast so I run "read_ledger --type domain --count" around every 20 - 30 seconds to see when it has completed. * The ledger tool was displaying the incorrect ledger count (this is a different issue) while performing a catch-up. I jumped from 12 to 6,000 transactions (more than what the pool has) and then to 9,337.  * The ledger on Node16 settled at 4,688 on Node16 and did not change. * I sent a new transaction from the CLI on a different machine and the pool stopped taking transactions.  *{color:#d04437}Error{color}* With only info level debugging this is all I captured {code} (  29) | discard | Node1 discarding message INSTANCE_CHANGE{'reason': 26, 'viewNo': 1} because Received instance change request with view no 1 which is not more than its view no 1 {code}  It appears that a view change might have been attempted while Node16 was performing a catch-up.  ></description> </Issue>
