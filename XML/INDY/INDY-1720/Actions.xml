<Action id="54606" issue="34151" author="ozheregelya" type="comment" created="2018-12-10 16:46:43.0" updateauthor="ozheregelya" updated="2018-12-10 16:46:43.0"> <body><! CDATA Similar issue was reproduced on docker pool with indy-node 1.6.725.  *Steps to Reproduce:* 1. Setup the docker pool of 6 nodes. 2. Demote and stop primary (Node1). => Node2 become primary. 3. Run load test. => One more View Change was happened during load test, new primary is Node4. 4. Promote demoted node back. => Node1 doesn't write, it can't complete View Change and stays on ViewNo 0 and sands INSTANCE_CHANGE\{'viewNo': 3, 'reason': 26} 5. Restart Node4 to initiate View Change. => View Change started, but it can't be completed for a long. Node4 selected Node3 as primary after restart for previous ViewNo 2.  *Actual Results:* Pool can't write after described steps.  *Expected Results:* Node should be in consistent state with the pool after demotion and promotion back. Primary should be in consistent state with the pool after restart.  Logs: s3://qanodelogs/indy-1720 To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1720/ /home/ev/logs/indy-1720/  ></body> </Action>
<Action id="56970" issue="34151" author="derashe" type="comment" created="2019-02-13 14:14:21.0" updateauthor="derashe" updated="2019-02-13 15:11:14.0"> <body><! CDATA *Plan of attack:*  As for now, we have mechanism, which allows newly connected nodes set same primaries, that are on other pool.  * When view_change completed, we are saving LedgerInfo, which includes seq_no of every ledger. Nodes pass this info to newly connected nodes so they could calculate replica's primaries based on this data. * In case when, addition of a new node causes addition of a new replica in pool, other nodes recalculate their primaries again despite LedgerInfo data. * That cause inconsistency of primaries of replicas that had just connected and replicas that adjust their primaries because of new replica.  As a hotfix we are suggesting to update stored LedgerInfo at the moment when we adjust replicas and selecting new primaries. In this case, we will send updated LedgerInfo to newly connected replicas, and everything will be in sync.  As a long term solution, we can try to merge functionality of storing current primaries with audit ledger.  ></body> </Action>
<Action id="57069" issue="34151" author="derashe" type="comment" created="2019-02-15 10:00:11.0" updateauthor="derashe" updated="2019-02-15 12:02:29.0"> <body><! CDATA Earlier, we used to transfer ViewChangeDone message in CurrentState. We did it to force newly connected node to make view_change with pool set at moment of this vew_change. Problem here is that we did not repeat every pool action (demote, promote) that happened after view change. So we need to fix and replace this functionality.   *Mention:* Fix will affect only nodes when they are in catchup process. In-sync nodes behaviour would not be touched.     *Feature #1.* Add new field in audit txn. This field can be : * seq_no of audit txn, which was the last in previous_view (0 in 0 view) (this option is more generic)        or * seq_no of pool ledger txn, which was the last in previous view (0 in 0 view)  Point of keeping this value is to aknowledge at which point of time view_change happened.  *Feature #2.* Write audit txn every time we finish view_change.  *We have number of options, described below.*  *Option #1. (Require feature #1, #2)*  After we finished catching up all ledgers, we will take pool state at moment of last view_change we will calculate primaries for this moment. After that, we will sequentially go through pool ledger and apply every txn to our temporary primaries. When node process last pool txn, it will apply these primaries to current pool state.  *Option #2. (Require feature #1**, #2**)*  While we are in sync, we can persist primaries in local storage (persist set of primaries, pool ledger seq_no, view_no).   In case if node had been disconnected/demoted and it need catchup. We will first catch up audit ledger. We will acknowledge current view_no. * If current view_no differs from persisted view_no, we can flush persisted primaries set, we would not need it. After that we are starting pool ledger catchup. Using postRecvTxnFromCatchup function, which will handle every txn sequentially, we can insert functionality that works as follows: ** if pool txn seq_no < seq_no from audit ledger, than do usual addition without primary selection ** if pool txn seq_no == seq_no from audit ledger, than flush current primaries, recalculate replicas count, and recalculate primaries for pool, considering audit view_no as current view_no. ** if pool txn seq_no > seq_no from audit ledger, than do usual addition with primary selection. * if current view_no == persisted view_no, than next caughtup txn must be == local persisted seq_no + 1 (to make this consistent, we need to persist primaries in executer - in moment of commitment). In this case, we can just continue do usual addition with primary selection, as pool will be consistent.  *Option #3.*  We can store current primaries in audit ledger. So when new node will connect and caughtup all ledgers, it will just apply primaries from last audit txn.  New field in audit txn would be ordered array of node's names. Because names can uniquely identify node, we can determenisticly set primary for every replica. This field will be modified every time we call select primaries (view_change, demotion, promotion).  Concerns: * We need to handle situation, when pool started view_change to view 3 (for example). But in the same time node caughtup audit ledger with last view_no 2. After that, this node will reject pp-s with 3-rd view.  ** The good thing here is that if this node get behind few checkpoints, it will start catchup, so it will get audit txns with new view. But this case need to be tested  ></body> </Action>
<Action id="58654" issue="34151" author="ashcherbakov" type="comment" body="This will be fixed by INDY-1945 and INDY-2025" created="2019-03-28 07:15:49.0" updateauthor="ashcherbakov" updated="2019-03-28 07:15:49.0"/>
<Action id="59057" issue="34151" author="vladimirwork" type="comment" created="2019-04-09 13:43:19.0" updateauthor="vladimirwork" updated="2019-04-09 13:46:25.0"> <body><! CDATA Build Info: indy-node 1.7.0~dev888  Steps to Validate: 1. Setup the docker pool of 7 nodes. 2. Demote and stop primary (Node1). 3. Run load test. 4. Promote demoted node back. 5. Restart new primary to initiate View Change. 6. Run one more load test. 7. Check all ledgers count and ViewNo at all nodes.  Actual Results: All ledgers are in sync after the second load test. ViewNo is the same at the all nodes and VC status is not in progress. Pool is in consensus.  ></body> </Action>
