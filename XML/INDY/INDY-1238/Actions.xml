<Action id="42151" issue="28848" author="ozheregelya" type="comment" created="2018-03-26 14:14:08.0" updateauthor="ozheregelya" updated="2018-03-26 14:14:08.0"> <body><! CDATA The issue is reproducing on indy-node 1.3.350. It reproduces on t2.medium and m4.large AWS instances. Command for load test running (threads count is 1 by default):Â  {code:java} for s in `seq 1 2000` ; do python3 Perf_Add_nyms.py -n 500 ; done{code} During the test load was not permanent (sometimes load test is stopped because of IS-596).  ></body> </Action>
<Action id="42248" issue="28848" author="dsurnin" type="comment" created="2018-03-28 14:06:04.0" updateauthor="dsurnin" updated="2018-03-28 14:06:04.0"> <body><! CDATA there are two main mem leaks  1 - unlimited zmq queues it was fixed with pr https://github.com/hyperledger/indy-plenum/pull/597  2 - there is a queue of stashing requests in node. this queue is also unlimited and contains requests that cannot be ordered at the moment. if node reaches some invalid state it could stash all the requests and as a result use all the available memory. we should define the policy how to restore node from this state. for reference  INDY-1250|https://jira.hyperledger.org/browse/INDY-1250    ></body> </Action>
