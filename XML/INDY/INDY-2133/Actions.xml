<Action id="60860" issue="40510" author="sergey.khoroshavin" type="comment" body=" ~lbendixsen  It looks like attached logs contain records around May 29 only." created="2019-06-11 08:49:17.0" updateauthor="sergey.khoroshavin" updated="2019-06-11 08:50:59.0"/>
<Action id="60880" issue="40510" author="lbendixsen" type="comment" body="Sorry for the mistake.   I will try again." created="2019-06-11 16:38:36.0" updateauthor="lbendixsen" updated="2019-06-11 16:38:36.0"/>
<Action id="61012" issue="40510" author="sergey.khoroshavin" type="comment" created="2019-06-14 12:38:03.0" updateauthor="sergey.khoroshavin" updated="2019-06-14 12:42:25.0"> <body><! CDATA *Preliminaries*  In order to understand following analysis one needs to know the following things: * instance change messages are sent with different reason codes * reason 43 means node found that pool didn't order anything (including freshness updates) for too long (~10 minutes) * reason 28 means node entered a view change, but failed to finish it before timeout (~7 minutes) * node increases local view no as soon as it enters a view change * instance changes with reason 43 are sent to  ** next view if node is not in a view change ** current view if node is in process of view change * instance changes with reason 28 are always sent to next view  At the time of interest BuilderNet consisted of the following 12 validator nodes: * ovvalidator * Certisign * danube * vnode1 * datum-sovrin * uvs_val2 * OgNode * SYGNET1 * xsvalidatorec2irl * fetch-ai * FoundationBuilder * makolab01  9 active nodes are required for consensus (to initiate a view change, to finish it and to order transactions)  *Analysis*  We have 8 nodes out of 9 required for consenus  {code} 2019-06-07 19:10:46,192|INFO|looper.py|Starting up indy-node 2019-06-07 19:10:47,826|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to xsvalidatorec2irl 2019-06-07 19:10:48,022|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to ovvalidator 2019-06-07 19:10:48,158|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to datum-sovrin 2019-06-07 19:10:48,185|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to Certisign 2019-06-07 19:10:48,221|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to SYGNET1 2019-06-07 19:10:48,678|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode 2019-06-07 19:15:01,862|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to fetch-ai {code}  danube connects, so we have 9 nodes and we reach consensus  {code} 2019-06-08 07:09:06,482|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode 2019-06-08 07:09:06,590|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to danube 2019-06-08 07:09:06,592|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService transitioning from PreSyncingPool to SyncingAudit 2019-06-08 07:09:25,701|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService transitioning from SyncingAudit to SyncingPool 2019-06-08 07:09:25,706|INFO|node.py|FoundationBuilder caught up to 0 txns in the last catchup 2019-06-08 07:09:25,706|INFO|replica.py|FoundationBuilder:0 set last ordered as (6, 5961) 2019-06-08 07:09:25,711|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode 2019-06-08 07:09:25,707|INFO|node.py|FoundationBuilder started participating 2019-06-08 07:09:25,707|INFO|replica.py|FoundationBuilder:0 setting primaryName for view no 6 to: datum-sovrin:0 {code}  And pool was trying to order something  {code} 2019-06-08 07:09:25,709|INFO|replica_stasher.py|FoundationBuilder:0 unstash 24 messages from future view {code}  However OgNode seem to be crashing repeatedly (for some reason), bringing pool out of and back into consensus  {code} 2019-06-08 07:09:26,554|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode ...  2019-06-08 07:12:25,210|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode {code}  Freshness updates don't happen so nodes try to enter a view change. However there are only 8 of them (FoundationBuilder, Certisign, danube, datum-sovrin, fetch-ai, ovvalidator, SYGNET1, xsvalidatorec2irl), so they cannot actually enter a view change.  {code} 2019-06-08 07:10:02,026|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai 2019-06-08 07:10:47,856|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long 2019-06-08 07:10:55,399|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from Certisign 2019-06-08 07:11:43,366|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from ovvalidator 2019-06-08 07:12:23,133|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from datum-sovrin 2019-06-08 07:13:31,377|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from xsvalidatorec2irl 2019-06-08 07:13:54,652|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1 2019-06-08 07:15:02,026|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai 2019-06-08 07:15:47,865|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long 2019-06-08 07:15:55,393|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from Certisign 2019-06-08 07:16:43,377|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from ovvalidator 2019-06-08 07:17:23,167|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from datum-sovrin 2019-06-08 07:18:31,380|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from xsvalidatorec2irl 2019-06-08 07:18:54,666|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1 2019-06-08 07:19:05,438|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from danube 2019-06-08 07:20:02,036|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai 2019-06-08 07:20:47,872|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long 2019-06-08 07:20:55,404|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from Certisign 2019-06-08 07:21:43,372|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from ovvalidator 2019-06-08 07:22:23,193|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from datum-sovrin 2019-06-08 07:23:31,384|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from xsvalidatorec2irl 2019-06-08 07:23:54,671|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1 2019-06-08 07:24:05,449|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from danube 2019-06-08 07:25:02,042|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai 2019-06-08 07:25:47,879|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long {code}  SYGNET1 manages to enter a view change (this is a bug, INDY-2145)  {code} 2019-06-08 07:20:31,528|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from SYGNET1 {code}  OgNode connects again  {code} 2019-06-08 09:49:22,947|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode {code}  OgNode broadcasts instance change, so we finally enter and try to finish a view change  {code} 2019-06-08 09:59:21,478|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from OgNode 2019-06-08 09:59:21,478|NOTIFICATION|view_changer.py|VIEW CHANGE: FoundationBuilder initiating a view change to 7 from 6 2019-06-08 09:59:21,806|INFO|view_changer.py|FoundationBuilder is sending ViewChangeDone msg to all : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':  (0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'), (1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'), (2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'), (3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'), (1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP') , 'viewNo': 7} {code}  Other nodes also entered and are trying to finish a view change. However new primary is uvs_val2, and it is offline, so despite having quorum of VIEW_CHANGE_DONE messages view change cannot be finished  {code} 2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from SYGNET1 : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from xsvalidatorec2irl : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from fetch-ai : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from ovvalidator : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from datum-sovrin : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,845|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from danube : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,859|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from OgNode : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-08 09:59:21,964|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from Certisign : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} {code}  All active nodes except SYGNET1 sent instance change indicating they think that view change is taking too long.  {code} 2019-06-08 10:06:21,488|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from xsvalidatorec2irl 2019-06-08 10:06:21,501|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 8 since View change could not complete in time 2019-06-08 10:06:21,505|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from OgNode 2019-06-08 10:06:21,506|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from fetch-ai 2019-06-08 10:06:21,543|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from datum-sovrin 2019-06-08 10:06:21,556|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from ovvalidator 2019-06-08 10:06:21,582|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from danube 2019-06-08 10:06:21,620|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from Certisign {code}  SYGNET1 is still alive, and is either on a view 6 or already moving to view 7  {code} 2019-06-08 10:08:54,856|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1 {code}  But OgNode somehow managed to start a view change to view 8 (probably due to NDY-2145). At this point we have enough nodes to initiate and finish view changes, however nodes are stuck in different views - due to yet another problem INDY-2143. This is actually very similar to what happened during December outage of Sovrin MainNet.  {code} 2019-06-08 10:09:21,504|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from OgNode 2019-06-08 10:13:21,854|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 9} from OgNode 2019-06-08 10:14:21,511|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from OgNode 2019-06-08 10:19:21,515|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from OgNode {code}  vnode1 connected, seemingly received quorum of instance changes, entered a view change, couldn't finish it because uvs_val2 is still offline....  {code} 2019-06-10 04:12:00,754|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to vnode1 2019-06-10 04:12:09,730|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from vnode1 : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo':   0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB' ,  1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q' ,  2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ' ,  3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe' ,  1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'  , 'viewNo': 7} 2019-06-10 04:19:09,443|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from vnode1 {code}  However it seems like it was able to start a view change to view 8 (just like OgNode)  {code} 2019-06-10 04:21:58,644|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from vnode1 2019-06-10 04:26:09,481|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 9} from vnode1 2019-06-10 04:26:58,659|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from vnode1 {code}  There were several restarts of vnode1 and OgNode but they didn't change anything  {code} 2019-06-10 13:35:46,109|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from vnode1 2019-06-10 13:36:45,674|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to vnode1 2019-06-10 15:57:49,370|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode 2019-06-10 15:58:04,579|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode 2019-06-10 16:01:02,002|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode 2019-06-10 16:01:30,750|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode {code}  Seems like it was time when pool restart action was issued, so everyone restarted  {code} 2019-06-10 16:07:30,003|INFO|looper.py|Starting up indy-node 2019-06-10 16:07:31,921|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to ovvalidator 2019-06-10 16:07:32,452|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to xsvalidatorec2irl 2019-06-10 16:07:33,717|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to fetch-ai 2019-06-10 16:07:35,471|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to SYGNET1 2019-06-10 16:07:35,984|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to datum-sovrin 2019-06-10 16:07:36,511|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode 2019-06-10 16:07:36,525|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to Certisign 2019-06-10 16:07:36,978|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to vnode1 2019-06-10 16:07:42,439|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to danube {code}  And it helped nodes to reach consensus and start ordering transactions (including freshness checks)  {code} 2019-06-10 16:07:30,843|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService starting catchup (is_initial=True) 2019-06-10 16:07:37,267|INFO|node.py|FoundationBuilder caught up to 0 txns in the last catchup 2019-06-10 16:07:37,267|INFO|replica.py|FoundationBuilder:0 set last ordered as (6, 5961) 2019-06-10 16:07:37,268|INFO|node.py|FoundationBuilder started participating 2019-06-10 16:07:38,436|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5962, ledger 2, state root EDHc8pEcGBq9tTy8zbEA1wFoZy3an5eMo3VJN16Rv2ec, txn root 4m1yqXFZ7U2ffM3CBqaTGM2e4S2adQQAoq4xbJ2CkAgC, audit root DpkEr9R6xXKJAwePP23ziApsdLamjBvGDAaypD7AWyWZ, requests ordered 1, discarded 0 2019-06-10 16:12:35,452|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5963, ledger 0, state root CGuyq6T4ASsqDbSJVn7sAhhybjD4rhjpxSAQkjuPo2Tx, txn root 5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB, audit root 4RQKtFKnK6UkKSh81FSps55NCBBnQnPd2JjE61sCE6DN, requests ordered 0, discarded 0 2019-06-10 16:12:35,460|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5964, ledger 1, state root 78LWbD7o46dA2C8cYT8da1kRtLwnsXwJkch5obWdqQLd, txn root 79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q, audit root Dqw3mCtY7sBXo1hF3kVzrtXNAV4SrjqQ1P1WYy1QC2A5, requests ordered 0, discarded 0 2019-06-10 16:12:35,468|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5965, ledger 1001, state root 7Mut4DkZsjAzJwxNRFWmFNN5zi3v9wj3xL28TpbaWDXG, txn root Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP, audit root CLQSA2YdcU294SNNNqDnkRxdNCFwKKypoWU8zxn7fGVe, requests ordered 0, discarded 0 2019-06-10 16:12:38,392|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5966, ledger 2, state root EDHc8pEcGBq9tTy8zbEA1wFoZy3an5eMo3VJN16Rv2ec, txn root 4m1yqXFZ7U2ffM3CBqaTGM2e4S2adQQAoq4xbJ2CkAgC, audit root 8tx43aBiMsz5LH8aEv37B1cjf12KP5GDuBpr2uWe7h91, requests ordered 0, discarded 0 2019-06-10 16:15:20,271|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5967, ledger 0, state root FkQAzxr5MHazn9PD9za19GpgHehMsF1rt55fPgf82Cov, txn root 3A5n3zSubMUb2yrt8b6Fui69cbnWTJqpGkuaprU7ZqFJ, audit root HEdseVCJsv46A9q9DtJ3wbWVVRgTxazCgWcU1HPLayRD, requests ordered 1, discarded 0 {code}   ></body> </Action>
<Action id="61013" issue="40510" author="sergey.khoroshavin" type="comment" created="2019-06-14 12:45:53.0" updateauthor="sergey.khoroshavin" updated="2019-06-14 12:45:53.0"> <body><! CDATA *Conclusion* Nodes failed to reach consensus before pool restart due to combination of two different bugs, corresponding issues are opened: * INDY-2143 - instance changes for view change timeout are sent only once, but should be sent periodically * INDY-2145 - very old instance messages are sometimes not discarded  ></body> </Action>
