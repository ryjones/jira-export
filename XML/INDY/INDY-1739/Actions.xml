<Action id="52011" issue="34277" author="spivachuk" type="comment" created="2018-10-10 13:57:23.0" updateauthor="spivachuk" updated="2018-10-10 13:57:23.0"> <body><! CDATA *Problem reason:* - {{Node.start_catchup}} method did nothing only in case the node was in ledger statuses gathering phase of catchup. In other phases of catchup this method allowed to start a new catchup.  *Changes:* - Wrote a test verifying that a catchup cannot be started if another catchup is in progress. - Fixed a bug with ability to start a catchup when another catchup is in progress. - Made a minor correction in test helpers.  *PRs:* - https://github.com/hyperledger/indy-plenum/pull/939 - https://github.com/hyperledger/indy-node/pull/969  *Version:* - indy-node 1.6.626-master - indy-plenum 1.6.558-master  *Risk factors:* - Catchup phase in scope of view change.  *Risk:* - Low  *Covered with tests:* - {{test_catchup_not_triggered_if_another_in_progress}}  *Recommendations for QA* - Please test a catchup of a node in a pool under load in AWS.  ></body> </Action>
<Action id="52104" issue="34277" author="spivachuk" type="comment" created="2018-10-11 15:00:48.0" updateauthor="spivachuk" updated="2018-10-11 15:00:48.0"> <body><! CDATA *Changes:* - Added unit tests verifying ability to start catchup in different node modes. - Reduced execution time of the integration test verifying that a catchup cannot be started if another catchup is in progress. - Made a minor correction in an existing test.  *PR:* - https://github.com/hyperledger/indy-plenum/pull/943  ></body> </Action>
<Action id="52173" issue="34277" author="zhigunenko.dsr" type="comment" created="2018-10-12 16:07:38.0" updateauthor="zhigunenko.dsr" updated="2018-10-12 16:07:38.0"> <body><! CDATA *Environment:* indy-node                        1.6.629 indy-plenum                      1.6.561  *Steps to Reproduce:* 1) Setup AWS pool with 25 nodes 2) Stop one non-primary node 3) Run load test (10txns/sec) 4) After 50k txns start node  *Actual Results:* Catching-up node cannot become equal by checkpoints. Pool lost it's consensus after  291k txns  *Expected Results:* Node catch ups others and joins to ordering process  ></body> </Action>
<Action id="52197" issue="34277" author="spivachuk" type="comment" created="2018-10-12 22:36:00.0" updateauthor="spivachuk" updated="2018-10-12 22:44:28.0"> <body><! CDATA As we can see in logs, all the nodes in the pool fell due to *out-of-memory* many times. Eventually this resulted in *loss of consensus* (after ~290K written transactions). As to *Node13 which performed catch-up*, it also fell due to *out-of-memory* many times. However, it successfully done the 1st catch-up at 11:43 (up to 62689 transactions in total) on start after being off during ~50K transactions. Then it started the 2nd catch-up because the pool went farther while the node was performing the 1st catch-up. This catch-up was interrupted by a fall due to out-of-memory. After restart Node13 successfully done the 3rd catch-up at 13:47 (up to 136670 transactions in total). Then it started the 4th catch-up because the pool went farther again but it did not complete it because of an out-of-memory fall. All the further attempts of catch-ups after restarts were interrupted by out-of-memory falls.  *So the observed issues are related to the memory leaks (see INDY-1721) and do not look related to the fix in catch-up.* It makes sense to re-run the scenario with reduced load. Please re-test the fix with load *5 txns/sec* and catch-up after *50K txns*.  ></body> </Action>
<Action id="52318" issue="34277" author="zhigunenko.dsr" type="comment" created="2018-10-16 11:42:48.0" updateauthor="zhigunenko.dsr" updated="2018-10-16 11:42:48.0"> <body><! CDATA *Environment:* indy-node                        1.6.633  *Steps to Reproduce:* 1) Setup AWS pool with 25 nodes 2) Stop one non-primary node (13th) 3) Run load test (5txns/sec) 4) After 50k txns start node  *Actual Results:* 13th node orders only 113k txns versus 120k txns on other nodes. Catchup couldn't finished even after load test stop.  *Expected Results:* Node catch ups others and joins to ordering process  *Additional Info:* ~/logs/INDY-1739-5nyms/  ></body> </Action>
<Action id="52430" issue="34277" author="spivachuk" type="comment" created="2018-10-18 17:59:06.0" updateauthor="spivachuk" updated="2018-10-18 18:01:45.0"> <body><! CDATA Node13 completed its last catch-up at 18:22. After that it received 7 generations of farther checkpoint messages (from {{\{viewNo: 2, seqNoStart: 10601, seqNoEnd: 10700\}}} to {{\{viewNo: 2, seqNoStart: 11201, seqNoEnd: 11300\}}}) from other nodes. But it did not start a next catch-up because it *had not gathered a quorum* of messages for any of these checkpoint generations. For each of these checkpoint generations *Node13* received *{color:#de350b}15{color}* messages while the quorum was *16*. At the same time, for example, *Node12* received *{color:#00875a}21{color}* messages for each of these checkpoint generations. However, Node13 reported that it was connected to all the other nodes _except for Node7_ and all these nodes reported that they were connected to Node13. Also no nodes blacklisted any others. The logs are of INFO level, so the reason, why Node13 did not receive checkpoint messages from the nodes from which Node12 received them, are unclear _(except for Node7 - Node13 was disconnected from it)_.  Please re-test the scenario one more time.  ></body> </Action>
<Action id="52455" issue="34277" author="zhigunenko.dsr" type="comment" created="2018-10-19 13:31:50.0" updateauthor="zhigunenko.dsr" updated="2018-10-19 13:31:50.0"> <body><! CDATA *Environment:* indy-node                        1.6.639         indy-plenum                      1.6.568        *Steps to Validate:* 1) Setup AWS pool with 25 nodes 2) Stop one non-primary node (22th, North America) 3) Run load test (5txns/sec) 4) After 50k txns start node  *Actual Results:* Node catch ups others and joins to ordering process at the 60к txns  ></body> </Action>
