<Action id="24525" issue="17077" author="krw910" type="comment" created="2017-05-26 20:49:38.0" updateauthor="krw910" updated="2017-05-26 20:49:38.0"> <body><! CDATA This did not work unless services were restarted. Here are my notes:  *{color:#205081}Versioning{color}* *Node* python3-stp=0.1.51 python3-ledger=0.2.28 python3-state-trie=0.1.15 python3-plenum=0.3.108 python3-sovrin-common=0.2.70 sovrin-node=0.3.116  *Client* python3-stp=0.1.51 python3-ledger=0.2.28 python3-state-trie=0.1.15 python3-plenum=0.3.108 python3-sovrin-common=0.2.70 python3-anoncreds=0.3.8 sovrin-client=0.3.111  *{color:#205081}Test Setup{color}* 6 Active Nodes 1 Active Client  *{color:#205081}Test Steps{color}* Ran the following script to setup for load test which creates 200 Trust Anchors one at a time python3 add_keys.py Steward1 000000000000000000000000Steward1  I then ran the following to quickly add 500 NYMs to the ledger python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 500 --at-once  At this point I had 712 entries on each node and verified this by doing a line count of the transactions file.  I then sent the following command to add 1800 NYMs one at a time which takes about 30 minutes. python3 load_test.py --clients-list load_test_clients.list -t NYM -c 1 -r 1800  While this was running I shut off the TCP ports in the 9,000 range on Node 6. After about 330 transactions I enabled the TCP ports in the 9,000 range. I did it this way to create a scenario where a node has fallen behind. After enabling the ports on Node 6 I disabled the ports on Node 5 so I would have two nodes to compare to see if they caught up.  *{color:#205081}Results{color}* *NODE 6* After the test completed Nodes 1-4 were all in sync. The had transaction files named 1, 1001, and 2001 with a total count of 2,512 which is correct for the commands I sent. Node 6 - Did not take any new transactions after enabling the ports for communication and did not catch up with the other nodes. I found that the service had thrown an exception after enabling the ports. "sudo systemctl status sovrin-node" {code} return self.gen.send(None) File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 672, in serviceReplicas c = self.serviceReplicaInBox(limit) File "/usr/local/lib/python3.5/dist-packages/plenum/server/node.py", line 1004, in serviceReplicaInBox msgCount += replica.serviceQueues(limit) File "/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py", line 519, in serviceQueues r = self.dequeuePrePrepares() if self.node.isParticipating else 0 File "/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py", line 1414, in dequeuePrePrepares pp, sender, _ = self.prePreparesPendingFinReqs.pop(i) IndexError: pop index out of range {code}  I then sent a single transaction to see if it would take anything new and Node 6 did nothing.  *NODE 5* I then enabled the ports back on Node 5. I watch the log on Node 3 and it was communicating with Node 5, but Node 5 would not catch up with the missing transactions. I checked the service on Node 5 and it was running fine.  I then sent a single transaction to see if Node 5 would take the new transaction and it did not.  *RESTART NODE SERVICES* I restarted the node services on Nodes 5 and 6. After the restart both nodes caught up and added a blank line entry to the transaction file before adding in what was missing so they each have one more line count than the other nodes.   ></body> </Action>
<Action id="24589" issue="17077" author="andrey.goncharov" type="comment" body=" ~krw910  I investigated the logs and it seems like the missing catch up is not the issue here. I recorded a  small demo for you with my findings|https://drive.google.com/file/d/0BxskuV-A2baheVcybDAyNGtTeEk/view?usp=sharing . Since you sent so many transactions the most probable cause is a view change bug that should be fixed by Lovesh soon." created="2017-05-29 17:29:29.0" updateauthor="andrey.goncharov" updated="2017-05-29 17:29:40.0"/>
<Action id="24614" issue="17077" author="krw910" type="comment" body=" ~andrey.goncharov  that is what I was hoping you would say. This is why I said I was blocked before. I think we need to wait until the view change issue is resolved before moving forward with this test." created="2017-05-29 22:20:47.0" updateauthor="krw910" updated="2017-05-29 22:20:47.0"/>
<Action id="24635" issue="17077" author="krw910" type="comment" body="This is blocked by tickets INDY-13 and INDY-14. Whenever I run a large number of transactions, even one at a time, I run into these two tickets. I need to be able to create a fair number of transactions and have other coming in so that I can test a node falling behind and having to catch up." created="2017-05-30 12:55:23.0" updateauthor="krw910" updated="2017-05-30 12:55:23.0"/>
<Action id="24666" issue="17077" author="krw910" type="comment" body="When given enough time the node can catch up. There are scenarios where it has issues, but the first one it captured in INDY-70. Once INDY-70 has been fixed I will retest the other scenarios where I believe we may have an issue, but we think INDY-70 may address my other concern." created="2017-05-30 15:02:04.0" updateauthor="krw910" updated="2017-05-30 15:02:04.0"/>
