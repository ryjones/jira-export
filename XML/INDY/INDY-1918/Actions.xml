<Action id="54898" issue="36232" author="mgbailey" type="comment" body="Logs from working node &quot;korea&quot; also attached." created="2018-12-17 22:29:55.0" updateauthor="mgbailey" updated="2018-12-17 22:29:55.0"/>
<Action id="54934" issue="36232" author="mgbailey" type="comment" created="2018-12-18 18:32:40.0" updateauthor="mgbailey" updated="2018-12-18 18:32:40.0"> <body><! CDATA Ok, it looks like we found the root cause. Their iptables was configured in a way that was preventing loopback, and apparently the indy-node-control service uses loopback on port 30003 to communicate with indy-node.  So we upgraded the node manually and it is back online.  The steward is redoing his iptable settings. *We still need to fix the continuous repeated retries to upgrade*, though. Another consequence of this is that now the client command to get-validator-info is not working for this node because the response message is to large  to send (the message contains upgrade attempt information). A separate ticket will be filed on this.  ></body> </Action>
<Action id="55175" issue="36232" author="derashe" type="comment" created="2018-12-29 13:42:23.0" updateauthor="derashe" updated="2018-12-29 13:42:23.0"> <body><! CDATA Result of inverstigation:  Function that handled failed callback of node_controll_tool is postConfigLedgerCatchup. This function scans ledger for last POOL_UPGRADE transaction and tries to apply it again. And so it resulted in recursive call and that was the reason of the problem.  Simplest fix that can be applied here is to prevent such a behaviour in failed_callback function.  ></body> </Action>
<Action id="55177" issue="36232" author="derashe" type="comment" body="PR: https://github.com/hyperledger/indy-node/pull/1116" created="2018-12-29 13:51:01.0" updateauthor="derashe" updated="2018-12-29 13:51:01.0"/>
<Action id="55594" issue="36232" author="derashe" type="comment" created="2019-01-14 07:15:50.0" updateauthor="derashe" updated="2019-01-15 13:01:00.0"> <body><! CDATA Rec for QA: * Run 4-nodes pool. * Choose one node and set it's iptables setting in such a way, that it prevent communication with node_control_tool (localhost:30003 by default). * After that send upgrade txn to the pool with pretty soon upgrade time (like 30 sec). (if you want to check this faster , you can set MinSepBetweenNodeUpgrades setting to low value, like 10 sec.) * After planned upgrade finish, check that every node, except chosen, upgraded to a newer version. * Get validator-info from all nodes. Check, that chosen node do not have huge upgrade log (it must have less records that other nodes). (provide validator info here if possible)  ></body> </Action>
<Action id="55671" issue="36232" author="vladimirwork" type="comment" created="2019-01-15 13:05:27.0" updateauthor="vladimirwork" updated="2019-01-15 13:05:27.0"> <body><! CDATA Build Info: indy-node 1.6.754  Steps to Validate: 1. Run 4-nodes pool. 2. Choose one node and set it's iptables setting in such a way, that it prevent communication with node_control_tool (localhost:30003 by default). 3. After that send upgrade txn to the pool with pretty soon upgrade time (like 30 sec). (if you want to check this faster , you can set MinSepBetweenNodeUpgrades setting to low value, like 10 sec.) 4. After planned upgrade finish, check that every node, except chosen, upgraded to a newer version (1.6.755). 5. Check upgrade logs and config ledger at all nodes.  Actual Results: All nodes has 8 entries in config ledger. There are 2 entries in upgrade log at the blocked node and 3 entries at the others right after upgrade. After sometime there are also 3 entries (entry about failed upgrade is added) at the blocked node. !INDY-1918.PNG|thumbnail!    ></body> </Action>
