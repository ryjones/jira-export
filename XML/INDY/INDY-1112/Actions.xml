<Action id="39789" issue="27262" author="zhigunenko.dsr" type="comment" created="2018-02-07 15:19:03.0" updateauthor="zhigunenko.dsr" updated="2018-02-07 15:19:03.0"> <body><! CDATA Case 2 Promotion of node require one more instance of primary *Steps to reproduce:* 1. Create pool with 7 nodes >> _Primary: node2 for instance 0,  node3 for instance1, node4 for instance2_ 2. Demote primary node2 >> _Primary: node5 for instance 0,  node6 for instance1 (view9)_ 3. Promote node2 _Primary: {color:red}node5 for instance 0{color},  node6 for instance1, {color:red}node5 for instance2{color} (view9)_  ></body> </Action>
<Action id="40262" issue="27262" author="sergey-shilov" type="comment" body="There is much more simple case to get the same primary for master and replica instances: just subsequent demote and promote initial primary. This is caused by current math that does not consider current primaries and relies on viewNo and instanceId only despite N (module) may be changed." created="2018-02-15 16:28:34.0" updateauthor="sergey-shilov" updated="2018-02-15 16:28:34.0"/>
<Action id="40994" issue="27262" author="sergey-shilov" type="comment" created="2018-02-28 09:45:27.0" updateauthor="sergey-shilov" updated="2018-02-28 09:45:27.0"> <body><! CDATA *Problem state / reason:*  The formula that is used for selection of primary node for master replica can not be applied for selection of primary nodes for backup replicas since N is variable.  *Changes:*  Now primaries for backup instances are chosen in a round-robin manner always starting from primary. If the next node is a primary for some instance then this node is skipped. So the first non-primary node is chosen as primary for current instance. Such approach allows to avoid election of instances of the same node as a primaries for different instances. The election procedure of the primary for master instance is not changed. Also working with node registry was changed. Now a newly added node does not add itself to owned node registry during initialisation. It waits for pool ledger catch-up instead. So now node registry is consistent with pool txns.   *Committed into:*      https://github.com/hyperledger/indy-plenum/pull/539     https://github.com/hyperledger/indy-node/pull/585     indy-node 1.2.320-master  *Risk factors:*      Nodes demotion and promotion, growing number of instances.  *Risk:*      Medium  *Recommendations for QA:*   - Check demotion and promotion of primary of master replica when number of instances is changed.  - Check demotion and promotion of random nodes.  ></body> </Action>
<Action id="41067" issue="27262" author="vladimirwork" type="comment" created="2018-03-01 10:33:30.0" updateauthor="vladimirwork" updated="2018-04-10 16:16:02.0"> <body><! CDATA Build Info: indy-node 1.3.322  Steps To Validate: 1. Force multiple view changes by primary demotion. 2. Force multiple view changes by primary restart. 3. Demote and promote nodes (primary and not) arbitrarily. 4. Check previous steps with several instances (pool of 7+ nodes) and big viewNo (20+).  Actual Results: New primary for master instance (`instId`==0) is always selected according to *`(viewNo+instId)%NumberOfNodes`* formula. All backup primaries are selected as next nodes *that are not primaries for another instance already*.  ></body> </Action>
