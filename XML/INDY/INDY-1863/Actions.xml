<Action id="59312" issue="35457" author="ashcherbakov" type="comment" body="Fixed in Build: indy-node 1.7.0.dev895Â " created="2019-04-18 07:33:04.0" updateauthor="ashcherbakov" updated="2019-04-18 07:33:04.0"/>
<Action id="59321" issue="35457" author="sergey.khoroshavin" type="comment" created="2019-04-18 11:01:07.0" updateauthor="sergey.khoroshavin" updated="2019-04-18 13:18:00.0"> <body><! CDATA *Problem reason* Actually there are two problems here: * performance - as written in description * stability - due to how current view change is implemented it is better to have as little as possible batches in flight, otherwise even small time shift between view changes on different nodes can lead to some nodes ordering more batches than others and falling out of consensus (more details in INDY-1296 and INDY-1303)  *Changes made* Added _Max3PCBatchesInFlight_ parameter, defaulted to None (meaning no limit, i.e. old behavior). When _Max3PCBatchesInFlight_ is a number then primary won't try to create new batches unless: * current number of batches in flight is less than that defined number * we need to send a new batch just after view change Also this limit won't prevent from creating as many freshness batches as needed when we already successfully entered batch creation logic.  *Version* indy-node 1.7.0.dev895   *PR* https://github.com/hyperledger/indy-plenum/pull/1163  *Covered by tests* test_max_3pc_batches_in_flight test_can_send_multiple_3pc_batches test_can_send_multiple_3pc_batches_below_limit test_cannot_send_multiple_3pc_batches_above_limit test_can_send_multiple_3pc_batches_in_next_view  *Risk* Medium  *Risk factors* Limiting number of batches in flight can lead to increased batch sizes with longer noninterruptible processing time. This can potentially lead to instabilities, although they can be addressed by INDY-1651.  *Recommendations for QA* Run load tests on AWS pool with Max3PCBatchesInFlight set to 4 on all nodes: * normal production load - to see if we are still stable * production load with forced view changes (with period set to 1800 seconds) - to see if we get any better with reduced number of batches in flight  ></body> </Action>
<Action id="59337" issue="35457" author="vladimirwork" type="comment" created="2019-04-19 11:20:56.0" updateauthor="vladimirwork" updated="2019-04-19 16:00:20.0"> <body><! CDATA Build Info: indy-node 1.7.0.dev896  Steps to Reproduce: 1. Run production load test with `Max3PCBatchesInFlight = 4` for about 1 day. 2. Stop the load to try to catch up stalled nodes not under the load. 3. Run low rate load test for a half an hour to try to catch up stalled nodes by ordering batches.  Actual Results: At least 3 nodes stalled by ViewNo (0 against 8 at the rest ones) and by all ledgers. They don't catch up and order at steps 2 and 3 too.  Logs and metrics: ev@evernymr33:logs/INDY-1863_19_04_2019_case_1_logs.tar.gz ev@evernymr33:logs/INDY-1863_19_04_2019_case_1_metrics.tar.gz  ></body> </Action>
<Action id="59364" issue="35457" author="vladimirwork" type="comment" created="2019-04-22 09:25:34.0" updateauthor="vladimirwork" updated="2019-04-22 09:25:34.0"> <body><! CDATA Build Info: indy-node 1.7.0.dev896  Steps to Reproduce: 1. Run production load test with `Max3PCBatchesInFlight = 4` and forced VCs for 8+ hours. 2. Stop the load to try to catch up stalled nodes not under the load.  Actual Results: Pool has stopped writing due to long several sequential VCs. Nodes have ViewNo from 0 to 30. Pool has written only 41k txns into domain and 99k txns into sovtoken ledger. Also there are some stacktraces in logs and journalctl (see more info in the attachments).  Logs and metrics: ev@evernymr33:logs/INDY-1863_22_04_2019_case_2_logs.tar.gz ev@evernymr33:logs/INDY-1863_22_04_2019_case_2_metrics.tar.gz  ></body> </Action>
<Action id="59366" issue="35457" author="vladimirwork" type="comment" body="All issues found were reported as INDY-2064." created="2019-04-22 11:48:25.0" updateauthor="vladimirwork" updated="2019-04-22 11:48:25.0"/>
<Action id="59408" issue="35457" author="vladimirwork" type="comment" created="2019-04-23 08:22:28.0" updateauthor="vladimirwork" updated="2019-04-23 11:44:37.0"> <body><! CDATA Build Info: indy-node 1.7.0.dev900  Steps to Reproduce: 1. Run production load test without fees with `Max3PCBatchesInFlight = 4` and forced VCs every 1800 seconds.  Actual Results: All nodes are in sync by ledger size and ViewNo but there are too few txn written into sovtoken ledger (110k vs 558k in domain ledger).  Logs and metrics: ev@evernymr33:logs/INDY-1863_23_04_2019_case_3_logs.tar.gz ev@evernymr33:logs/INDY-1863_23_04_2019_case_3_metrics.tar.gz  ></body> </Action>
<Action id="59410" issue="35457" author="vladimirwork" type="comment" created="2019-04-23 11:46:16.0" updateauthor="vladimirwork" updated="2019-04-23 11:46:16.0"> <body><! CDATA Build Info: indy-node 1.7.0.dev900  Steps to Reproduce: 1. Run load test with 40 nyms/sec with `Max3PCBatchesInFlight = 4`.  Actual Results: Pool has performed several VCs right after start of the load and has been broken after 141k txns written.  Logs and metrics: ev@evernymr33:logs/INDY-1863_23_04_2019_case_4_logs.tar.gz ev@evernymr33:logs/INDY-1863_23_04_2019_case_4_metrics.tar.gz  ></body> </Action>
<Action id="59419" issue="35457" author="vladimirwork" type="comment" created="2019-04-23 14:46:51.0" updateauthor="vladimirwork" updated="2019-04-23 14:46:51.0"> <body><! CDATA Build Info: indy-node 1.7.0.dev900  Steps to Reproduce: 1. Run load test with 40 nyms/sec with default config.  Actual Results: Pool has performed several VCs right after start of the load and has been broken after 110k txns written so it looks like we survive longer with batches in flight limitation (at least results are not worse with this limitation than without it) but we need more test runs to prove it for sure.  Logs and metrics: ev@evernymr33:logs/INDY-1863_23_04_2019_case_5_logs.tar.gz ev@evernymr33:logs/INDY-1863_23_04_2019_case_5_metrics.tar.gz  ></body> </Action>
