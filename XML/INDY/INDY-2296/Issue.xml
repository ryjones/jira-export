<Issue id="43379" key="INDY-2296" number="2296" project="10303" reporter="vladimir.demcak" creator="vladimir.demcak" type="10004" summary="Consensus with X&lt;(f+1) genesis nodes down" priority="3" status="10200" created="2019-11-19 13:12:18.0" updated="2019-12-11 13:58:19.0" votes="0" watches="2" workflowId="56487"> <environment><! CDATA indy-node: 1.12.0.dev1125 indy-node: 1.11.0    ></environment> <description><! CDATA I don't know if this is a bug but really cannot find any reason why the following happens. I cannot find it in any documentation and based on system 3*F+1 it should work. For me it looks like a bug but maybe I misunderstood something.  I have a feeling I need to do something with genesis_transaction file. I dont understand how or if it's possible to connect a client/agent to the pool with genesis file if the half of the genesis nodes are not reachable. If this is the reason It would be good to have a section or few lines in documentation for that. I saw INDY-2015 it looks similar but not sure if it is the same problem I am describing here.     *The problem:*  Assume we have 4genesis nodes + 6 new nodes and I remove 3 genesis nodes, I have 7 running nodes (1 genesis and 6 newly added nodes). Failed (F) nodes F=3 and total N=10 nodes so I assume the consensus should be met.      *How to reproduce:* # Let's start pool with pool_start.sh -  https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/pool_start.sh  and with 4 nodes. # Once the pool is running add 6 new nodes as described in  add_node| https://github.com/hyperledger/indy-node/blob/master/docs/source/add-node.md   documentation # Now we have 10 reachable nodes. !image-2019-11-19-14-05-26-244.png! # Now, let’s remove 3 genesis nodes. and wait for view changes as below. !image-2019-11-19-14-06-10-641.png! # As we can see we have F=3 failed nodes. So based on formula 3*F+1 it should be ok for having consensus (3*3+1 = 10 -> indeed we have 10 Nodes in total and 7 running)     *Problem with pool restart:*  Another thing I have observed is restarting pool. When I run pool-restart from a running node (genesis or one of the newly added nodes) like below: {code:java} pool(sandbox):wallet(xyz):did(VVV...EEE):indy> ledger pool-restart action=start {code} I get timeouts for non-genesis nodes: {code:java} Restart pool response for node Node10: +------------------------+---------------------+--------+----------+ | From                   | Request Id          | Action | Datetime | +------------------------+---------------------+--------+----------+ | TrusteeDid000000000000 | 1574257396040735100 | start  | -        | +------------------------+---------------------+--------+----------+ Restart pool response for node Node8: +------------------------+---------------------+--------+----------+ | From                   | Request Id          | Action | Datetime | +------------------------+---------------------+--------+----------+ | TrusteeDid000000000000 | 1574257396040735100 | start  | -        | +------------------------+---------------------+--------+----------+ Restart pool node NewNode1 timeout. Restart pool node NewNode5 timeout. Restart pool node NewNode4 timeout. Restart pool node Node1 timeout. Restart pool response for node Node9: +------------------------+---------------------+--------+----------+ | From                   | Request Id          | Action | Datetime | +------------------------+---------------------+--------+----------+ | TrusteeDid000000000000 | 1574257396040735100 | start  | -        | +------------------------+---------------------+--------+----------+ Restart pool node Node3 timeout. Restart pool response for node Node6: +------------------------+---------------------+--------+----------+ | From                   | Request Id          | Action | Datetime | +------------------------+---------------------+--------+----------+ | TrusteeDid000000000000 | 1574257396040735100 | start  | -        | +------------------------+---------------------+--------+----------+ Restart pool node Node2 timeout. Restart pool node NewNode6 timeout. Restart pool node Node4 timeout. Restart pool response for node Node7: +------------------------+---------------------+--------+----------+ | From                   | Request Id          | Action | Datetime | +------------------------+---------------------+--------+----------+ | TrusteeDid000000000000 | 1574257396040735100 | start  | -        | +------------------------+---------------------+--------+----------+ Restart pool node NewNode3 timeout. Restart pool node NewNode2 timeout. Restart pool node Node5 timeout. {code} I completely understand why I have timeouts for Node0-Node5 (The nodes I've stopped) but dont understand why I get timeout for running NewNodes.       ></description> </Issue>
