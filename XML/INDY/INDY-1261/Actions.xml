<Action id="42610" issue="29203" author="mgbailey" type="comment" body="Attaching additional logs provided by stewards. " created="2018-04-05 23:23:38.0" updateauthor="mgbailey" updated="2018-04-05 23:23:38.0"/>
<Action id="42633" issue="29203" author="dsurnin" type="comment" created="2018-04-06 12:54:33.0" updateauthor="dsurnin" updated="2018-04-06 12:54:33.0"> <body><! CDATA Before ev1 restart current viewNo was 3 and primary was royal_sovrin. After the restart ev1 nothing had changed - viewNo 3, primary royal_sovrin.  According to logs there are lots of nodes disconnections, i.e. last log from current master digitalbazaar  for the 2018-04-05 contains   {code:java}  2018-04-05 15:12:53,921 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from zaValidator 2018-04-05 15:13:09,936 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to zaValidator 2018-04-05 16:10:39,690 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from ev1 2018-04-05 16:11:29,565 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to ev1 2018-04-05 16:48:29,158 | INFO     | propagator.py        (179) | propagate | digitalbazaar propagating request ('J4N1K1SEB8uY2muwmecY5q', 1522946906091066) from client b'YNY!QhQzswkJE9%9uu)g(U@*AgU$Oy1{N%?27ofz' 2018-04-05 16:54:07,450 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from ev1 2018-04-05 16:54:22,446 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to ev1 2018-04-05 16:55:23,625 | INFO     | propagator.py        (179) | propagate | digitalbazaar propagating request ('J4N1K1SEB8uY2muwmecY5q', 1522947320775122) from client b'YNY!QhQzswkJE9%9uu)g(U@*AgU$Oy1{N%?27ofz' 2018-04-05 21:00:00,103 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from DustStorm 2018-04-05 21:17:05,650 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from pcValidator01 2018-04-05 21:25:39,998 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from danube 2018-04-05 21:33:52,298 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to pcValidator01 2018-04-05 21:37:33,419 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from pcValidator01 2018-04-05 21:52:41,945 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from atbsovrin 2018-04-05 21:53:20,367 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from royal_sovrin 2018-04-05 22:01:16,490 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from prosovitor digitalbazaar.log (END)  {code}  I will continue research, but for now it looks like nodes cannot reach quorum because of network issue   ></body> </Action>
<Action id="42638" issue="29203" author="mgbailey" type="comment" body=" ~dsurnin  can you tell what was the primary prior to the March 30th restart of ev1?  I suspect that this is when the consensus was lost.  I don&apos;t believe that there were network issues.  All validators showed as connected.  I am beginning to believe that a reboot of the primary node halts consensus." created="2018-04-06 15:12:01.0" updateauthor="mgbailey" updated="2018-04-06 15:12:01.0"/>
<Action id="42653" issue="29203" author="mgbailey" type="comment" created="2018-04-06 23:40:12.0" updateauthor="mgbailey" updated="2018-04-06 23:40:12.0"> <body><! CDATA Update: We brought down the service on all nodes.  Then I sent out a message to all stewards to start the service again, with ev1 being the first up.  Over the course of a few hours stewards brought up nodes.  I can see a view change in the logs, with ev1 selected as the primary.  However, I am still unable to post a transaction to the network. I am uploading updated ev1 logs here.  We are also considering an upgrade with force=True to an unchanged release to force simultaneous restart of the service.  ></body> </Action>
<Action id="42676" issue="29203" author="mgbailey" type="comment" created="2018-04-07 19:37:46.0" updateauthor="mgbailey" updated="2018-04-07 19:38:17.0"> <body><! CDATA We got up to 13 / 17 nodes connected which should have been enough for consensus, with 1 to spare.  We were unable to post transaction.  Two more nodes connected up today, danube and zaValidator, so we are now up to 15 nodes.  *And we are now able to post a transaction!*   So the new question for the ticket is _why were we unable to post a transaction with 13 nodes connected?_  One possibility is that I noticed that danube and zaValidator were among the primaries selected during the view change, even they were not up.  Could it be that these particular nodes were required for consensus?  I will post new ev1 logs for comparison, dated 20180407, now that the transaction was able to be posted:  ^ev1_log_20180407.tgz   DID of the transaction that I unsuccessfully posted last night: UCiMTm8EATcg4rEb7Pi2hX  DID of the transaction that I successfully posted today:  UCiMTm8EATcg4rEb7Pi2iX  (they vary in only 1 character)  ></body> </Action>
<Action id="42719" issue="29203" author="dsurnin" type="comment" created="2018-04-09 12:54:04.0" updateauthor="dsurnin" updated="2018-04-09 12:55:03.0"> <body><! CDATA It would be good to have logs from several more nodes - at least 3 or 4 mode nodes. According to ev1 logs 13 nodes were not enough because not all nodes responded for requests. I cannot see actual reason since I do not have all the logs. After 15 nodes connected consensus were reached  ></body> </Action>
<Action id="42724" issue="29203" author="mgbailey" type="comment" body=" ~dsurnin , which nodes do we need logs from?  I will request them." created="2018-04-09 14:17:56.0" updateauthor="mgbailey" updated="2018-04-09 14:17:56.0"/>
<Action id="42727" issue="29203" author="dsurnin" type="comment" created="2018-04-09 15:02:10.0" updateauthor="dsurnin" updated="2018-04-09 15:02:10.0"> <body><! CDATA it would be good to have logs from at least zaValidator and danube. any additional nodes you can get logs from would helpful.  also could you please run get nym req with the one you marked as unsuccessful UCiMTm8EATcg4rEb7Pi2hX and provide the output of client?  ></body> </Action>
<Action id="42745" issue="29203" author="mgbailey" type="comment" created="2018-04-09 20:25:54.0" updateauthor="mgbailey" updated="2018-04-09 20:25:54.0"> <body><! CDATA zaValidator and danube were down (the service was off) until 04/07 06:42 GMT, and 04/06 22:47 GMT, respectively, so there will be no logs in that time period.  See  https://docs.google.com/spreadsheets/d/19svUEFuEPrgEtz-K-bUTfjHh7nHFqjaXDGqH6417Xd4/edit?usp=sharing.   GET_NYM shows that the DID was posted, but further analysis of the logs shows that the posting happened 2 minutes after the client sent the message.  ></body> </Action>
<Action id="42813" issue="29203" author="dsurnin" type="comment" created="2018-04-11 09:41:35.0" updateauthor="dsurnin" updated="2018-04-11 09:41:35.0"> <body><! CDATA  ~krw910   ~mgbailey   There are only two nodes have debug logs attached, the other logs are info - a couple of strings each day.  Before ev1 restart on 2018-03-30 15:26:42,071 the viewNo is 3 and primary is royal_sovrin. According to logs several nodes tried to make view change due to master disconnected but had no quorum (Aalto, zaValidator), it could be because of some network issues on that nodes. After restart nothing had changed primary is royal_sovrin, viewNo is 3. Next view change, due to master disconnected, was around 2018-04-02 05:17:39. ev1 selected digitalbazaar as primary for viewNo 4. Several nodes, including new primary, had a view change timeout expired (no debug logs) and tried to send several INSTANCE_CHANGE msgs, but according to ev1 logs it was finished since lots of nodes send CURRENT_STATE with the same viewNo and primary name.  According to  ~mgbailey  there were 6 txns posted to pool during period  {code:java} date    time    DID verkey 2018-04-05  16:48:26,090    MbGA1UMnzv6XgRo8MFHGXp  verkey=~SbXSz233xX75CF4gQ81fXp 2018-04-05  16:55:20,774    MbGA1UMnzv6XgRo8MFHGXq  verkey=~SbXSz233xX75CF4gQ81fXp 2018-04-06  18:18:00,922    UCiMTm8EATcg4rEb7Pi2hZ  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz 2018-04-06  20:07:44,563    UCiMTm8EATcg4rEb7Pi2hY  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz 2018-04-06  23:19:43,839    UCiMTm8EATcg4rEb7Pi2hX  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz 2018-04-07  18:51:46,827    UCiMTm8EATcg4rEb7Pi2iX  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz {code}  The first 3 of them were not actually added. I cannot detect why since the nodes with debug logs received those txns as MESSAGE_RESPONSES with discard index set, but discard reason is not specified.  The last 3 of them are added. See some ouput from new cli  {code:java} pool(live):wallet(wallolo):did(V4S...e6f):indy> ledger get-nym did=UCiMTm8EATcg4rEb7Pi2hY | Sequence Number | Request ID          | Transaction time    | | 59              | 1523347539014355822 | 2018-04-06 20:07:44 |   pool(live):wallet(wallolo):did(V4S...e6f):indy> ledger get-nym did=UCiMTm8EATcg4rEb7Pi2hX | Sequence Number | Request ID          | Transaction time    | | 60              | 1523347543589883827 | 2018-04-06 23:21:55 |   pool(live):wallet(wallolo):did(V4S...e6f):indy> ledger get-nym did=UCiMTm8EATcg4rEb7Pi2iX | Sequence Number | Request ID          | Transaction time    | | 61              | 1523347563764991752 | 2018-04-07 18:51:47 | {code}  So the pool is probably working.  From my point of view most of the problems are connected with network. For tests  ~mgbailey  uses old client. This client version has internal message queue and resends all the messages to the nodes after they becomes reachable.  Primary restart does not lead to pool to reject txns. Just to view change. However during the view change pool stashing all the txns and start to process them only after view change is done. It might looks like pool is stopped since we cannot guaranty that view change will be finished in any constant time.  We have series of tests for pool restarting in plenum/tests/restart folder.   ></body> </Action>
<Action id="42826" issue="29203" author="mgbailey" type="comment" created="2018-04-11 15:13:42.0" updateauthor="mgbailey" updated="2018-04-11 15:13:42.0"> <body><! CDATA If there are network problems, we need to have them pin-pointed to particular nodes so that we can debug the nodes' networking configuration.  We need this in the logs, at least. Is there a ticket to provide logging for this issue?  We really can't just say that an outage might have been due to network issues on a production network, without better / more specifics. In addition, the network should be able to recover from a network outage without requiring a system-wide restart.  ></body> </Action>
<Action id="43045" issue="29203" author="ashcherbakov" type="comment" created="2018-04-17 08:16:12.0" updateauthor="ashcherbakov" updated="2018-04-17 08:16:12.0"> <body><! CDATA I think we need to investigate whether there are network issues on the live pool, and what is the cause.  We can create and run simple scripts (ping-pong) to test network communication. We can check firewall settings on the nodes.  ></body> </Action>
<Action id="43161" issue="29203" author="sergey-shilov" type="comment" created="2018-04-19 15:23:53.0" updateauthor="sergey-shilov" updated="2018-04-19 15:23:53.0"> <body><! CDATA Hi  ~mgbailey ,  I've implemented a simple script to measure ping time to the specified list of IPs. This is needed for ping statistics.  I've attached two files: 1. *ping_monitor.sh* - script itself 2. *live_pool_ips.list* - list of live pool nodes IPs, provided by Olga (please, re-check it).  Could I ask you to send this script and IPs list to stewards to run it from their nodes and then gather stats? Script writes result to the file named _ping_stats.out_, so that we can gather ping stats for each connection.  Usage:  _$ ping_monitor.sh ./live_pool_ips.list_  ></body> </Action>
<Action id="43162" issue="29203" author="mgbailey" type="comment" body=" ~sergey-shilov , I will see about incorporating this into the log aggregator script that I am writing, and having the results upload to a centralized server." created="2018-04-19 15:35:10.0" updateauthor="mgbailey" updated="2018-04-19 15:35:10.0"/>
<Action id="43182" issue="29203" author="mgbailey" type="comment" created="2018-04-19 22:16:30.0" updateauthor="mgbailey" updated="2018-04-19 22:16:30.0"> <body><! CDATA  ~sergey-shilov   I ran a test with this script, and found that only about 1/2 of the stewards have ICMP turned on for their nodes (or else it is blocked at firewalls).  I don't know that it will be useful.  Maybe I should reimplement it with hping3, except that this would require stewards to install this package on their nodes.  ></body> </Action>
<Action id="43196" issue="29203" author="ashcherbakov" type="comment" created="2018-04-20 07:39:33.0" updateauthor="ashcherbakov" updated="2018-04-20 07:39:33.0"> <body><! CDATA I think we need to check that TCP connections are stable (hping is an option). Also we may check firewall/iptables on the problem nodes (need to identify problem nodes from the logs).   ></body> </Action>
<Action id="43209" issue="29203" author="sergey-shilov" type="comment" created="2018-04-20 13:19:13.0" updateauthor="sergey-shilov" updated="2018-04-20 13:19:13.0"> <body><! CDATA Hi  ~mgbailey ,  I've rewritten the _ping_monitor.sh_ tool to use *hping3* instead of *ping*. I've re-attached the _ping_monitor.sh_ tool and the list of live pool IPs _live_pool_ips.list_ (note, now this list should contain lines in form IP:PORT, it is described in script's usage message).  ></body> </Action>
<Action id="43226" issue="29203" author="sergey-shilov" type="comment" created="2018-04-20 15:35:24.0" updateauthor="sergey-shilov" updated="2018-04-20 15:35:24.0"> <body><! CDATA  ~mgbailey   Also, it would be nice to ask stewards provide us their firewall (iptables etc.) settings, if it is not sensitive information for them.  ></body> </Action>
<Action id="43429" issue="29203" author="sergey-shilov" type="comment" created="2018-04-25 17:06:56.0" updateauthor="sergey-shilov" updated="2018-04-25 17:12:46.0"> <body><! CDATA  ~mgbailey   ~ashcherbakov   Well, I've done deep investigation and matching of attached logs. First of all, the only full logs we have are logs from ev1 and ServerVS, so investigation of these logs are the most important because such logs show indy-node restart/crush that lead to disconnection log line on other nodes. So it is very important to distinguish indy-node restart/crush and real network issues. Unfortunately, not all of attached logs are full, thus only investigation of ev1 and ServerVS disconnections makes sense.  Regarding ev1, about 40% of disconnects of this node in other's nodes logs are caused by indy-node crush or restart (unfortunately, we can not distinguish these events without logs of journalctl where we can see back trace):  ----------------------------------------------- 2018-04-05 16:10:39,607 | DEBUG    | looper.py            (265) | handleSignal | Signal None received, stopping looper... 2018-04-05 16:10:39,617 | INFO     | looper.py            (273) | shutdown | Looper shutting down now... 2018-04-05 16:10:39,617 | DEBUG    | motor.py             (34) | set_status | ev1 changing status from started to stopping -----------------------------------------------  Regarding ServerVS, all disconnects are caused by network issues as there are no log records related to indy-node stopping.  Also I gathered disconnecting stats for each participated node in form  ================================  <NodeName> which logs are investigated ================================  <RemoteNodeName1>: number of "disconnected from <RemoteNodeName1>" events <RemoteNodeName2>: number of "disconnected from <RemoteNodeName2>" events etc. (sorted in reverse order) ================================     So here they are:  *================================================================*  ================================  BIGAWSUSEAST1-001: ================================ zaValidator: 10 ServerVS: 7 esatus_AG: 5 DustStorm: 5 Aalto: 5 ev1: 4 Stuard: 4 danube: 3 pcValidator01: 2 icenode: 2 digitalbazaar: 2 atbsovrin: 2 royal_sovrin: 1 prosovitor: 1 ================================  OASFCU: ================================ pcValidator01: 2 ev1: 2 zaValidator: 1 royal_sovrin: 1 prosovitor: 1 icenode: 1 esatus_AG: 1 digitalbazaar: 1 danube: 1 atbsovrin: 1 Stuard: 1 ServerVS: 1 DustStorm: 1 BIGAWSUSEAST1-001: 1 Aalto: 1 ================================  ServerVS: ================================ ev1: 11 zaValidator: 10 pcValidator01: 8 icenode: 8 esatus_AG: 8 Stuard: 8 royal_sovrin: 7 prosovitor: 7 digitalbazaar: 7 DustStorm: 6 iRespond: 5 danube: 5 atbsovrin: 5 BIGAWSUSEAST1-001: 5 OASFCU: 4 Aalto: 4 ================================  Stuard: ================================ zaValidator: 9 ServerVS: 8 DustStorm: 6 iRespond: 5 ev1: 5 esatus_AG: 5 royal_sovrin: 4 atbsovrin: 4 prosovitor: 3 icenode: 3 digitalbazaar: 3 danube: 3 BIGAWSUSEAST1-001: 3 pcValidator01: 2 OASFCU: 2 Aalto: 1 ================================  atbsovrin: ================================ zaValidator: 8 Aalto: 7 ev1: 6 Stuard: 4 ServerVS: 4 DustStorm: 4 danube: 3 pcValidator01: 2 esatus_AG: 2 royal_sovrin: 1 prosovitor: 1 icenode: 1 digitalbazaar: 1 ================================  danube: ================================ zaValidator: 8 ev1: 6 ServerVS: 4 DustStorm: 4 icenode: 2 esatus_AG: 2 royal_sovrin: 1 pcValidator01: 1 iRespond: 1 Stuard: 1 ================================  digitalbazaar: ================================ zaValidator: 10 ServerVS: 6 royal_sovrin: 4 ev1: 4 esatus_AG: 4 DustStorm: 4 danube: 3 Stuard: 3 Aalto: 3 pcValidator01: 2 atbsovrin: 2 prosovitor: 1 icenode: 1 BIGAWSUSEAST1-001: 1 ================================  ev1 ================================ ServerVS: 10 zaValidator: 9 esatus_AG: 5 Stuard: 5 DustStorm: 5 icenode: 4 atbsovrin: 4 Aalto: 4 prosovitor: 3 pcValidator01: 3 danube: 3 royal_sovrin: 2 OASFCU: 2 iRespond: 1 digitalbazaar: 1 BIGAWSUSEAST1-001: 1 ================================  icenode ================================ zaValidator: 9 ServerVS: 7 royal_sovrin: 6 ev1: 6 Aalto: 6 pcValidator01: 5 prosovitor: 4 danube: 4 Stuard: 4 DustStorm: 4 esatus_AG: 3 atbsovrin: 3 digitalbazaar: 2 OASFCU: 2 iRespond: 1 BIGAWSUSEAST1-001: 1 ================================  pcValidator01 ================================ ServerVS: 7 DustStorm: 5 zaValidator: 3 icenode: 3 ev1: 3 esatus_AG: 2 Stuard: 1 Aalto: 1 ================================  zaValidator ================================ Aalto: 17 ServerVS: 14 ev1: 12 danube: 12 royal_sovrin: 11 icenode: 10 digitalbazaar: 10 Stuard: 10 DustStorm: 10 esatus_AG: 9 prosovitor: 7 atbsovrin: 7 BIGAWSUSEAST1-001: 6 OASFCU: 5 pcValidator01: 4 iRespond: 4 ================================  *================================================================*  As we can see the leaders of disconnected nodes are zaValidator and ServerVS. Logs of these nodes are one of the leaders of "carpet disconnects":  -------------------------------- 2018-04-05 18:56:56,520 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from icenode 2018-04-05 18:56:56,520 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Stuard 2018-04-05 18:56:57,151 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from pcValidator01 2018-04-05 18:56:57,675 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from ev1 -------------------------------- 2018-03-29 01:06:42,304 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from esatus_AG 2018-03-29 01:06:53,769 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from ev1 2018-03-29 01:06:55,814 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from pcValidator01 2018-03-29 01:07:01,512 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from icenode 2018-03-29 01:07:04,382 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Stuard -------------------------------- 2018-04-01 13:46:50,625 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from BIGAWSUSEAST1-001 2018-04-01 13:46:51,008 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from esatus_AG 2018-04-01 13:46:52,167 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from iRespond 2018-04-01 13:46:55,240 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from danube 2018-04-01 13:46:56,520 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Aalto 2018-04-01 13:46:56,775 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from DustStorm 2018-04-01 13:46:56,840 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from ev1 2018-04-01 13:47:02,911 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from OASFCU 2018-04-01 13:47:03,879 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from icenode 2018-04-01 13:47:04,453 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from royal_sovrin 2018-04-01 13:47:05,159 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from prosovitor 2018-04-01 13:47:08,168 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from atbsovrin 2018-04-01 13:47:08,291 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from zaValidator 2018-04-01 13:47:08,740 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Stuard 2018-04-01 13:47:09,572 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from pcValidator01 2018-04-01 13:47:09,895 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from digitalbazaar -------------------------------- zaValidator.log.2018-03-27:2018-03-28 00:05:29,241 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from icenode zaValidator.log.2018-03-27:2018-03-28 00:05:29,242 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto zaValidator.log.2018-03-27:2018-03-28 00:05:29,257 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from danube zaValidator.log.2018-03-27:2018-03-28 00:05:32,485 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to royal_sovrin zaValidator.log.2018-03-27:2018-03-28 00:05:33,299 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to icenode zaValidator.log.2018-03-27:2018-03-28 00:05:33,307 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to danube zaValidator.log.2018-03-27:2018-03-28 00:05:35,973 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from digitalbazaar zaValidator.log.2018-03-27:2018-03-28 00:06:03,390 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto zaValidator.log.2018-03-27:2018-03-28 00:06:33,677 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to digitalbazaar zaValidator.log.2018-03-27:2018-03-28 00:25:39,736 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from digitalbazaar zaValidator.log.2018-03-27:2018-03-28 00:25:48,128 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to digitalbazaar zaValidator.log.2018-03-27:2018-03-28 06:26:05,857 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto zaValidator.log.2018-03-27:2018-03-28 06:26:22,252 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto zaValidator.log.2018-03-28:2018-03-28 23:06:57,322 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from esatus_AG zaValidator.log.2018-03-28:2018-03-28 23:07:38,923 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to esatus_AG zaValidator.log.2018-03-28:2018-03-28 23:21:31,086 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from icenode zaValidator.log.2018-03-28:2018-03-28 23:21:39,441 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to icenode zaValidator.log.2018-03-28:2018-03-29 06:26:51,428 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto zaValidator.log.2018-03-28:2018-03-29 06:26:54,773 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto zaValidator.log.2018-03-30:2018-03-30 06:26:41,497 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto zaValidator.log.2018-03-30:2018-03-30 06:26:57,918 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto zaValidator.log.2018-03-30:2018-03-30 15:19:47,076 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from ev1 zaValidator.log.2018-03-30:2018-03-30 15:19:47,707 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from OASFCU zaValidator.log.2018-03-30:2018-03-30 15:19:52,652 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from icenode zaValidator.log.2018-03-30:2018-03-30 15:19:55,467 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from DustStorm zaValidator.log.2018-03-30:2018-03-30 15:19:56,190 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from ServerVS zaValidator.log.2018-03-30:2018-03-30 15:19:56,335 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from danube zaValidator.log.2018-03-30:2018-03-30 15:19:56,359 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from BIGAWSUSEAST1-001 zaValidator.log.2018-03-30:2018-03-30 15:19:56,429 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from prosovitor zaValidator.log.2018-03-30:2018-03-30 15:19:57,131 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Stuard zaValidator.log.2018-03-30:2018-03-30 15:19:57,579 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from atbsovrin zaValidator.log.2018-03-30:2018-03-30 15:20:00,734 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from digitalbazaar zaValidator.log.2018-03-30:2018-03-30 15:20:01,223 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from esatus_AG zaValidator.log.2018-03-30:2018-03-30 15:20:02,060 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto zaValidator.log.2018-03-30:2018-03-30 15:20:02,539 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from royal_sovrin zaValidator.log.2018-03-30:2018-03-30 15:20:02,541 | INFO | node.py (1106) | onConnsChanged | zaValidator lost connection to primary of master --------------------------------  Seems like zaValidator and ServerVS has often disconnects from the whole network segments, but it is just a suggestion. Anyway, I think we should take a look at these nodes.  ></body> </Action>
<Action id="43433" issue="29203" author="mgbailey" type="comment" created="2018-04-25 17:53:06.0" updateauthor="mgbailey" updated="2018-04-25 17:53:06.0"> <body><! CDATA  ~sergey-shilov , thank you for this information.  While there are disconnect events, I think this shows that the nodes are all working much of the time.  How long did the disconnect events last? Is there something specific we can ask for these two stewards to look at or to send to us?   One of these stewards is in South Africa, and the other is in Switzerland, and neither is in an AWS datacenter. I would expect that remote nodes like these might experience some network instability. Since a world-wide, diverse network is one of our design goals, and these nodes appear to be working most of the time, should we be looking at how we can more reliably recover when events like these occur?  ></body> </Action>
<Action id="43786" issue="29203" author="ashcherbakov" type="comment" created="2018-05-03 16:41:35.0" updateauthor="ashcherbakov" updated="2018-05-03 16:41:35.0"> <body><! CDATA  ~mgbailey  As disconnections are in general normal event in our distributed network, and, according to logs, we were able to successfully recover from disconnections, I think we can close this issue. Do you agree?  ></body> </Action>
<Action id="43798" issue="29203" author="mgbailey" type="comment" body=" ~ashcherbakov , agreed.  I expect that disconnections will occur, and that we recover from them effectively.  " created="2018-05-03 20:06:40.0" updateauthor="mgbailey" updated="2018-05-03 20:06:40.0"/>
