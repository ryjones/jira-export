<Action id="26339" issue="17900" author="stevetolman" type="comment" body="Assign this appropriately." created="2017-06-14 21:00:31.0" updateauthor="stevetolman" updated="2017-06-14 21:00:31.0"/>
<Action id="27502" issue="17900" author="ashcherbakov" type="comment" created="2017-06-28 14:19:21.0" updateauthor="ashcherbakov" updated="2017-06-28 14:19:21.0"> <body><! CDATA There is at least one queue which is not cleared: `replica.py: ordered`  ></body> </Action>
<Action id="27939" issue="17900" author="lovesh" type="comment" body="  ^test_node_load_consistent_time.prof  " created="2017-07-06 13:26:12.0" updateauthor="lovesh" updated="2017-07-06 13:26:12.0"/>
<Action id="27942" issue="17900" author="lovesh" type="comment" body="Some memory measurement utilities were added, a perf test utility was updated. Here is the merged PR. https://github.com/hyperledger/indy-plenum/pull/254. More changes are in https://github.com/hyperledger/indy-plenum/tree/perf-tracking" created="2017-07-06 14:22:26.0" updateauthor="lovesh" updated="2017-07-06 14:22:26.0"/>
<Action id="28707" issue="17900" author="alexander.shekhovcov" type="comment" created="2017-07-20 09:22:05.0" updateauthor="alexander.shekhovcov" updated="2017-07-20 09:46:45.0"> <body><! CDATA I've got some numbers:  +Shakedown pool #3, Node1, indy-node 0.4.48+  h3. Test #1 10 clients send 10000 requests  *Before:* PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty 03:24:47 PM   2275632   1769372     43.74    227032   1217748    418176     10.34    813452    739092        44  *Command:* python load_test.py -c 10 -r 1000  *After:* PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty 03:45:17 PM   2104388   1940616     47.98    227036   1357352    464640     11.49    976404    745548       144  *Result:* requests: 10000 kbmemused +171M (171244) kbcached  +139M (139604) kbcommit   +46M (46464)   h3. *Test #2* 10 clients send 100000 requests  *Before:* PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty 03:49:37 PM   2104080   1940924     47.98    227036   1357936    464640     11.49    977024    745520        92  *Command:* python load_test.py -c 10 -r 10000  *After:* AM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty 02:20:18 AM    491300   3553704     87.85    227456   2826240    557512     13.78   2505724    785108        92  *Result:* requests: 100000 kbmemused +1612M kbcached  +1468M kbcommit  +92M  h3. *Test #3* Restart the node  *Before:*  total        used        free      shared  buff/cache   available Mem:           3.9G        264M        427M         40M        3.2G        3.5G  *After:*  total        used        free          shared  buff/cache   available Mem:           3.9G        141M        555M         40M        3.2G        3.6G  *Result:* Restating the node frees only ~120M   *Conclusion:*  We still have a memory leak but for me it looks acceptable (for now at least). The memory leak is ~100M per 100000 requests.  *The data:* !all-mem.png|thumbnail!  !mem-commited.png|thumbnail!    sysstat|https://drive.google.com/open?id=0BwH79BR-U6L4bzdRQjdFdlNWbVE    ></body> </Action>
<Action id="28709" issue="17900" author="lovesh" type="comment" created="2017-07-20 09:32:01.0" updateauthor="lovesh" updated="2017-07-20 09:33:10.0"> <body><! CDATA  ~alexander.shekhovcov  Thanks a lot for doing this. We use pympler for benchmarking performance tests, what tool were you using?. I am not completely sure if its acceptable but it's not too scary either  ></body> </Action>
<Action id="28711" issue="17900" author="alexander.shekhovcov" type="comment" body="I used `sysstat`. It must be fixed but I am not sure that it is a couple-hour task. I think in current state pool is able to process at least 1M requests without swapping or OOM. So we can reopen this ticket later or create another one." created="2017-07-20 09:41:26.0" updateauthor="alexander.shekhovcov" updated="2017-07-20 09:42:03.0"/>
<Action id="28770" issue="17900" author="alexander.shekhovcov" type="comment" body="`sysstat` command *sar -ur 10 -o sysstat*" created="2017-07-21 08:47:57.0" updateauthor="alexander.shekhovcov" updated="2017-07-21 08:48:08.0"/>
<Action id="28811" issue="17900" author="danielhardman" type="comment" body="I think we have proved that this issue is not likely to cause disasters in the early days of the live network–but we haven&apos;t fixed the issue. Therefore, I vote that we move the ticket into the backlog and keep it open." created="2017-07-21 16:23:44.0" updateauthor="danielhardman" updated="2017-07-21 16:23:44.0"/>
<Action id="29082" issue="17900" author="mzk-vct" type="comment" created="2017-07-27 13:00:44.0" updateauthor="mzk-vct" updated="2017-07-27 13:00:44.0"> <body><! CDATA I conducted an experiment - wrote  a script|https://github.com/mzk-vct/plenum/blob/memtrack/plenum/common/tools/memtrack.py  that takes all objects in memory whose size is *over* *50.000 bytes*, integrated it in one node and run load test: *python load_test.py -c 10 -r 10000* (as  ~alexander.shekhovcov  did). It was printing results every five minutes. Here is a summary:  ^summary.csv   *In short:* 1. On the start and before approximately first two thousands of requests there were no objects of size higher than 50.000 bytes. 2. During whole runtime only 6-7 object had size over 50.000 bytes 3. Max total size of these objects was 7269904 (~7 megabytes), it was ~23.86% of all memory consumed by objects. 4. Some time after load test completed all large objects disappeared (their size decreased or they were deleted).  So, we can calculate that max total memory consumed by all objects was ~29 megabytes.  Of course there are some hidden things in internals of python that also consume memory.  This results differ from results of research of  ~alexander.shekhovcov  that shows memory leak ~100M per 100.000 requests. I think that the reason is that sysstat shows not real, but virtual memory consumption, which is usually greater.  I propose closing this ticket and opening a new one when we face significant memory leak.  ></body> </Action>
<Action id="29087" issue="17900" author="mzk-vct" type="comment" created="2017-07-27 13:39:55.0" updateauthor="mzk-vct" updated="2017-07-27 13:39:55.0"> <body><! CDATA After discussion with  ~alexander.shekhovcov  we decided that since python memory management can have some tricky logic results of the experiment above are not precise enough.  We decided to make test for exhaustion - run load test for very huge amount of requests and see whether node will fall into swap. I'll run it today and check results on the morning.     ></body> </Action>
<Action id="29135" issue="17900" author="mzk-vct" type="comment" created="2017-07-28 11:16:49.0" updateauthor="mzk-vct" updated="2017-07-28 11:33:47.0"> <body><! CDATA I run load test with *-c 20 -r 50000* parameters, 1.000.000 requests total. My script checks largest objects every 5 minutes, sysstat checks memory every 10 seconds.  I checked it after ~18 hours - only ~220.000 transactions were processed, whereas first 10.000 were processed in a first 10 minutes.  This means that throughput degrades over time.  Same thing I saw when run *test_node_load* test from *plenum/test/test_performance.py*.  So we definitely have an issue, I'm creating a ticket.     *About memory:*  Max number of objects > 50.000 bytes was 10, their totals size was ~16mb what was 37.5% of all memory consumed by all objects. But some minutes after this number dropped to ~12mb.  Sysstat records: first record, record when there were these 10 objects, and the last records (at the time I checked it) respectively: {code:java} ========= kbmemfree kbmemused %memused kbbuffers kbcached kbcommit %commit kbactive kbinact kbdirty 16:09:19 2991212 1053792 26.05 208704 564860 396148 9.79 533368 358260 64 10:26:19 209780 3835224 94.81 202952 3068116 636356 15.73 2857908 770056 1168 10:54:39 154440 3890564 96.18 203468 3126296 638652 15.79 2973924 708744 724 {code} Load test still running, I'm going to wait more to see what happen then.  ></body> </Action>
<Action id="29136" issue="17900" author="mzk-vct" type="comment" created="2017-07-28 11:21:15.0" updateauthor="mzk-vct" updated="2017-07-28 11:21:15.0"> <body><! CDATA Data file sizes:  {code} 11M    .sovrin/data/nodes/Node3/_merkleNodes 24K    .sovrin/data/nodes/Node3/config_state 10M    .sovrin/data/nodes/Node3/seq_no_db 24K    .sovrin/data/nodes/Node3/pool_state 14M    .sovrin/data/nodes/Node3/idr_cache_db 4.0K    .sovrin/data/nodes/Node3/config_transactions 20K    .sovrin/data/nodes/Node3/attr_db 11M    .sovrin/data/nodes/Node3/_merkleLeaves 472M    .sovrin/data/nodes/Node3/domain_state 8.0K    .sovrin/data/nodes/Node3/pool_transactions_sandbox 44M    .sovrin/data/nodes/Node3/transactions_sandbox 559M    .sovrin/data/nodes/Node3/ {code}  ></body> </Action>
<Action id="29291" issue="17900" author="mzk-vct" type="comment" created="2017-08-01 12:14:11.0" updateauthor="mzk-vct" updated="2017-08-01 12:14:11.0"> <body><! CDATA Made a test using  muppy|https://pythonhosted.org/Pympler/muppy.html .  *python load_test.py -c 10 -r 10000*  How memory consumption changed from the test start:   {noformat} types |   # objects |   total size ================================================ | =========== | ============ <class 'list |        8341 |      2.67 MB <class 'str |       15251 |      1.52 MB <class 'dict |        2333 |    732.38 KB <class 'float |       20990 |    491.95 KB <class 'orderedset._orderedset.entry |        3812 |    238.25 KB <class 'collections.OrderedDict |         111 |    140.34 KB <class 'int |        3553 |    102.21 KB <class 'set |         320 |     87.50 KB <class 'plenum.server.propagator.Requests |           0 |     73.12 KB <class 'sovrin_common.types.SafeRequest |        1187 |     64.91 KB <class 'sortedcontainers.sorteddict.SortedDict |           1 |     47.92 KB <class 'weakref |         426 |     33.28 KB <class 'plenum.server.models.Prepares |           0 |     23.62 KB <class 'plenum.server.models.Commits |           0 |     23.62 KB <class 'plenum.server.propagator.ReqState |         300 |     16.41 KB {noformat}   Results for all measurements with 5 minutes step:   ^muppy-10users-10000requests.7z    ></body> </Action>
<Action id="29292" issue="17900" author="mzk-vct" type="comment" created="2017-08-01 12:19:14.0" updateauthor="mzk-vct" updated="2017-08-01 12:19:14.0"> <body><! CDATA Attaching sysstat file for  1.000.000 requests|https://jira.hyperledger.org/browse/INDY-223?focusedCommentId=29135&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-29135      ^sysstat.7z    ></body> </Action>
<Action id="29293" issue="17900" author="mzk-vct" type="comment" created="2017-08-01 12:36:10.0" updateauthor="mzk-vct" updated="2017-08-01 12:36:10.0"> <body><! CDATA *Investigation summary:* # As tests show there is no significant objects growth # Most of the memory consumed then freed # However sysstat shows that file system cache contains a lot of data and it frees only when process stopped. Possible explanation of this is that leveldb maintains LSM of stored data. Also some of this memory may be consumed by zmq internals. # During this investigation significant throughput degradation was found - time required to handle one request growth, ticket created  *Recommendations:* # Investigate and fix throughput degradation because these two problems can be interrelated. # Since leak (if it really is) is not big do nothing with it until its influence become significant     ></body> </Action>
