<Action id="38384" issue="26693" author="ozheregelya" type="comment" body="The issue reproduces on docker pool on previous RC versions (1.2.49 and 1.2.50), but during previous acceptance tests this case was passed on AWS pool." created="2018-01-10 13:58:40.0" updateauthor="ozheregelya" updated="2018-01-10 13:58:40.0"/>
<Action id="38905" issue="26693" author="spivachuk" type="comment" created="2018-01-19 18:49:12.0" updateauthor="spivachuk" updated="2018-01-19 19:15:26.0"> <body><! CDATA *Problem reason:*  The node might not update {{LedgerManager.last_caught_up_3PC}} when synchronizing a ledger in case actual catch-up was not needed. {{LedgerManager.last_caught_up_3PC}} is used for updating {{last_ordered_3pc}} in the node's replicas on completion of catch-up phase. On the master's primary {{last_ordered_3pc}} is used for updating {{lastPrePrepareSeqNo}} on completion of propagate primary process. So miss of updating {{LedgerManager.last_caught_up_3PC}} on the master's primary resulted in creation of 3PC-batches with duplicated keys (with {{ppSeqNo}} starting from 1 again) and this stopped ordering of 3PC-batches.  If actual catch-up was not needed, {{LedgerManager.last_caught_up_3PC}} was updated using the optional properties {{viewNo}} and {{ppSeqNo}} of the last received {{LedgerStatus}} needed for quorum of {{LedgerStatuses}} not newer than own ledger status. Actually this update was performed only in case this last received {{LedgerStatus}} matches own ledger status (not older than it) and contained {{viewNo}} and {{ppSeqNo}}. {{LedgerStatus}} being sending by a node contains {{viewNo}} and {{ppSeqNo}} only if the node was not stopped since its master replica had ordered the batch which contained the last transaction in the ledger (because the map from transactions seq nos to 3PC-keys is stored in memory only).  In the case with stopping 3 nodes from this ticket, the pool came back to consensus by turning on {{Node2}} which was the master's primary. During catch-up phase in scope of propagate primary process {{Node2}} received the last {{LedgerStatus}} needed for quorum from {{Node3}} which had been also stopped and started previously, so this {{LedgerStatus}} did not contain {{viewNo}} and {{ppSeqNo}} and thus the master replica of {{Node2}} which was primary did not recover its {{lastPrePrepareSeqNo}} and then created batches with wrong {{ppSeqNos}} (starting from 1 again) and they were not ordered. When all the nodes in pool were simultaneously restarted, all the protocol instances were terminated and then started from scratch. Each protocol instance started 3PC-batching from the initial key ({{viewNo}}=0, {{ppSeqNo}}=1). So ordering of 3PC-batches was started again.  In the case with stopping 2 nodes from this ticket, the pool came back to consensus by turning on {{Node2}} which was the master's primary. During catch-up phase in scope of propagate primary process {{Node2}} received the last {{LedgerStatus}} needed for quorum from one of {{Node3}}, {{Node4}} or {{Node5}} (it doesn't matter from which one namely). None of these three nodes had been stopped previously, so this {{LedgerStatus}} contained {{viewNo}} and {{ppSeqNo}} and thus the master replica of {{Node2}} which was primary successfully recovered its {{lastPrePrepareSeqNo}} and then created 3PC-batches with correct {{ppSeqNos}} and they were successfully ordered.  *Changes:* - Fixed the bug with possible miss of updating {{LedgerManager.last_caught_up_3PC}} when synchronizing a ledger in case actual catch-up is not needed. Now the node considers {{viewNo}} and {{ppSeqNo}} of all the received {{LedgerStatuses}} that match own ledger. - Added a test verifying the fix specified above. - Fixed a bug in Python wrapper of libindy with trying to set a result or an exception on a cancelled future.  *PRs:* - https://github.com/hyperledger/indy-plenum/pull/505 - https://github.com/hyperledger/indy-node/pull/529 - https://github.com/hyperledger/indy-sdk/pull/482  *Version:* - indy-node 1.2.278 master - indy-plenum 1.2.225 master - python3-indy 1.3.0-dev-321  *Risk factors:* - Nothing is expected.  *Risk:* - Low  *Covered with tests:* - {{test_pool_reaches_quorum_after_f_plus_2_nodes_turned_off_and_later_on}}  *Recommendations for QA*:  To ensure that the bug has been fixed please stop / start nodes in the following order (for the pool of 5 nodes): - Stop {{Node1}}. - Stop {{Node2}}. _(Pool must lose consensus here.)_ - Stop {{Node3}}. - Start {{Node3}}. - Start {{Node2}}. _(Pool must come back to consensus here.)_ - Start {{Node1}}.  ></body> </Action>
<Action id="38951" issue="26693" author="vladimirwork" type="comment" created="2018-01-22 10:33:58.0" updateauthor="vladimirwork" updated="2018-01-22 10:33:58.0"> <body><! CDATA Build Info: indy-node 1.2.279  Steps to Reproduce: 1. Install pool of 5 nodes. 2. Stop nodes from 1st to 3rd. 3. Start nodes from *3rd to 1st* sending NYMs after each node starting to check consensus (pool reaches consensus at 2nd node starting). 4. Stop nodes from 1st to 3rd. 5. Start nodes from *1st to 3rd* sending NYMs after each node starting to check consensus (pool reaches consensus at 2nd node starting). !INDY-1081.PNG|thumbnail!  6. Stop nodes from 5th to 3rd. 7. Start nodes from 5th to 3rd sending NYMs after each node starting to check consensus. !INDY-1081_bug.PNG|thumbnail!   Actual Results: Pool reaches consensus in Step 7 at *n*, not at *n-f*. Debug logs from all nodes are in attachment.  ^logs.tar.gz    ></body> </Action>
<Action id="39881" issue="26693" author="spivachuk" type="comment" created="2018-02-08 19:50:46.0" updateauthor="spivachuk" updated="2018-02-08 20:09:17.0"> <body><! CDATA *Problem reason:* - If there are less than {{n-f}} alive nodes in a pool but the master's primary is alive then it still can create new 3PC-batches. However, these batches are not ordered by the nodes because the nodes do not gather the quorums of PREPAREs (due to there are not enough alive nodes in the pool for consesus). When the number of alive nodes in the pool becomes {{n-f}} and the primary creates some new 3PC-batch and sends the PREPREPARE, the just started nodes request the missing PREPREPAREs and PREPAREs (that were sent while they were being off). However, the just started nodes cannot gather the quorums of the missing PREPAREs because MESSAGE_REPONSEs with PREPAREs already seen by the replica are discarded. Such the criteria of discarding is incorrect for MESSAGE_REPONSEs with PREPAREs. Thus the missing batches are not ordered. Next batches are not ordered due to the previous ones have not been ordered.  *Changes:* - Fixed a bug with handling of MESSAGE_REQUEST for PREPARE: now the request is replied by a replica only if the requested PREPARE was sent by this replica earlier. - Fixed a bug with ignoring MESSAGE_RESPONSE with PREPARE in case the requested PREPARE has already been received from any replica: now the response from a replica is ignored only if the requested PREPARE has already been received from this replica. - Fixed a bug with a wrong set of 3PC-messages being requested by Replica.processPrePrepare method in case of PP_CHECK_NOT_NEXT error. - Fixed some bugs in test helpers. - Corrected the test verifying the pool consensus after stopping and starting {{f+2}} nodes including the master's primary. - Added a test verifying the pool consensus after stopping and starting {{f+2}} nodes but not the master's primary.  *PRs:* - https://github.com/hyperledger/indy-plenum/pull/523 - https://github.com/hyperledger/indy-node/pull/557  *Version:* - indy-node 1.2.299-master - indy-plenum 1.2.241-master  *Risk factors:* - The changes affect the fix from INDY-849.  *Risk:* - Low  *Covered with tests:* - {{test_quorum_after_f_plus_2_nodes_but_not_primary_turned_off_and_later_on}}  *Recommendations for QA:* - Please re-test INDY-849 since the fix from it has been affected the changes made in scope of the current ticket.  ></body> </Action>
<Action id="39895" issue="26693" author="ozheregelya" type="comment" created="2018-02-08 23:02:33.0" updateauthor="ozheregelya" updated="2018-02-08 23:02:33.0"> <body><! CDATA Environment: indy-node 1.2.299 indy-plenum 1.2.241  Steps to Validate: 1. Setup the pool of 5 nodes. 2. Stop 3 nodes in following order: 1 -> 2 -> 3, send transaction after each stopping. 3. Start 3 nodes in following order: 3 -> 2 -> 1, send transaction after each starting.  Actual Results: Pool returned to consensus after starting of 2nd node.   Following behavior was noticed during testing of this issue: transaction which was send after stopping 2nd node (which become primary after stopping of node 1) was not written. Need to discuss this case with developers.  Additional Information: INDY-849 was retested on pool of 7 nodes. Pool returned to consensus when 3 nodes was started in 2 hours after stopping. Case with stopping of 5 -> 4 -> 3 and starting 3 -> 4 -> 5 was also retested.  ></body> </Action>
<Action id="39911" issue="26693" author="ozheregelya" type="comment" created="2018-02-09 11:38:31.0" updateauthor="ozheregelya" updated="2018-02-09 11:38:31.0"> <body><! CDATA {quote}Following behavior was noticed during testing of this issue: transaction which was send after stopping 2nd node (which become primary after stopping of node 1) was not written. Need to discuss this case with developers. {quote} This is expected behavior because we can't guarantee that transaction will be written after reaching consensus if primary was stopped when transaction has been send. So, this ticker can be closed.  ></body> </Action>
