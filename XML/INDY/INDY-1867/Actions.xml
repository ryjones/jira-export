<Action id="53781" issue="35517" author="derashe" type="comment" created="2018-11-26 08:47:06.0" updateauthor="derashe" updated="2018-11-26 14:22:29.0"> <body><! CDATA Problem reason: * Pool stopped work  Findings: * Unfortunately, logs were deleted until last few hours, when nodes just tried to reconnect to each other * But using validator_info we saw that part of nodes with 30700 has unequeal commited_state_root_hash and uncommited_state_root_hash of plugin ledger. Also we've mentioned that view_change happened right in the moment when pool stopped ordering.  * After investigating plugin's code, we've understand that probably one does't have code to revert unapplied state  Details: * If plugin need to be integrated in plenum codebase, it must implement several feautres (for example feature of applying and reverting state) * Plugins can use abstractions of Ledger, PrunningState and reqHandler (like sovtoken plugin do) or they can implement such a features by themselves (like sovtokenfees do). * In second case it seems that sovtokenfees use POST_REQUEST_APPLICATION hook to apply state, but in the same time it do not have functionality to revert state in case of need. * That probably led us to state_root_hash inconsistency  Action item: * Start a ticket in plugin's code.   ></body> </Action>
<Action id="53786" issue="35517" author="zhigunenko.dsr" type="comment" created="2018-11-26 11:14:46.0" updateauthor="zhigunenko.dsr" updated="2018-11-26 11:14:46.0"> <body><! CDATA *Steps to Reproduce:* agent1, agent2 {code:java} perf_processes.py -g pool -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext "{\"payment_addrs_count\":3,\"addr_mint_limit\":10,\"mint_by\":10,\"payment_method\":\"sov\",\"plugin_lib\":\"libsovtoken.so\",\"plugin_init\":\"sovtoken_init\",\"trustees_num\":4}" -k " {\"get_nym\":{\"count\": 9}}, {\"get_schema\":{\"count\": 2}}, {\"get_attrib\":{\"count\": 2}}, {\"get_cred_def\":{\"count\": 2}}, {\"get_revoc_reg_def\":{\"count\": 1}}, {\"verify_payment\":{\"count\": 1}} " -c 450 -b 100 -l 50 --short_stat {code}  agent3 {code:java} perf_processes.py -g pool -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext "{\"payment_addrs_count\":10,\"addr_mint_limit\":1000,\"mint_by\":500,\"payment_method\":\"sov\",\"plugin_lib\":\"libsovtoken.so\",\"plugin_init\":\"sovtoken_init\",\"trustees_num\":4, \"set_fees\":{\"1\":1,\"100\":1,\"101\":1,\"102\":1,\"113\":1,\"10001\":1}}" -k " {\"nym\":{\"count\": 4}}, {\"schema\":{\"count\": 1}}, {\"attrib\":{\"count\": 3}}, {\"cred_def\":{\"count\": 1}}, {\"revoc_reg_def\":{\"count\": 1}}, {\"payment\":{\"count\": 9}} " -c 100 -b 10 -l 10 & {code}  agent4 {code:java} perf_processes.py -g pool -c 1 -n 1 -l 1 -k "demoted_node" -y one -b 100 {code}  *Actual Results:* Pool lost its consensus about view_no sudo read_ledger --type=domain --count 32700 sudo read_ledger --type=pool --count 14806 sudo read_ledger --type=sovtoken --count 78381  *Additional Info:* Logs and metrics wiil be available on ~/logs/24-25.11/ soon  ></body> </Action>
<Action id="53928" issue="35517" author="derashe" type="comment" created="2018-11-28 15:52:01.0" updateauthor="derashe" updated="2018-11-29 06:41:03.0"> <body><! CDATA During analyze of logs with load of nym+fees and node transactions, pool crashed. That happened because of slowness of ordering pool (node) transactions.  * A few nodes got behind pool in different moments, which led to OOM (because of stashing incoming requests). Somewhy, this slowness happens to happen in different time at different nodes. This led pool to consensus loss. * It was also mentioned that requests did not stashed, while behind nodes were catching up ledger.  Action items: * Retest this case, when fixes for optimizing pool txns will be done. * Define if we have problem with catchup. And if we do, then create ticket to fix that.  Upd: * After brief analisys it was revealed that we have some problems with catchup logic. That needs to be solved in scope of another ticket  ></body> </Action>
<Action id="53990" issue="35517" author="ashcherbakov" type="comment" created="2018-11-29 07:04:29.0" updateauthor="ashcherbakov" updated="2018-11-29 07:05:35.0"> <body><! CDATA So, the issues is caused by the following tickets: * INDY-1879: Don't use all pool ledger data in NODE transactions ordering - this is why NODE txns are slow which leads to View Changes and general performance degradation * INDY-1874: Stub: Stability with fees plugins - a bug in plugins leading to losing consensus after the view change * INDY-1876: A Node needs to be able to order requests received during catch-up - this is why node may go to infinite catchup and can not order * INDY-1881: Change catchup logic for correctly clearing requests queues - this is why requests are not cleared after catch-ups which may lead to OOM  ></body> </Action>
