<Action id="49730" issue="32875" author="sergey.khoroshavin" type="comment" created="2018-08-31 22:59:39.0" updateauthor="sergey.khoroshavin" updated="2018-08-31 22:59:39.0"> <body><! CDATA *Problem reason* Initially this issue was about finding some static message quotas, however after analyzing a number of metrics from load tests with different profiles it became apparent that static message quotas won't fit - when set too high they won't have desired effect, and when set too low they can kill performance in some situations. Next idea was to throttle client messages based on amount of node messages. However more metrics analysis showed that in some situations it won't help either. So, finally it was decided to try throttling client messages based on number of received, but still not ordered requests. Now it's implemented as a hard switch (receive or not) based on request queue size, but in future smarter strategies could be implemented, like smooth throttling client stack based on request queue size and smooth throttling of node stack based on time between looper runs. Code is written so that such strategies are easy implement.  *Changes* New parameters added to config: {code} ENABLE_DYNAMIC_QUOTAS = False MAX_REQUEST_QUEUE_SIZE = 1000 {code} When _ENABLE_DYNAMIC_QUOTAS_ is set to True throttling of client messages is enabled based on unordered requests queue size. _MAX_REQUEST_QUEUE_SIZE_ defines queue length threshold after which client messages are throttled.  *Versions* indy-node >= 1.6.591 indy-plenum >= 1.6.534  *PR* https://github.com/hyperledger/indy-plenum/pull/905  *Covered by tests* https://github.com/hyperledger/indy-plenum/blob/a58ad6ea644a8bbdc5a22468be68d3ff489f4444/plenum/test/node/test_quota_control.py  *Risk* Medium  *Risk factors* Performance degradation might be observed when dynamic quotas are enabled, although this is unlikely.  *Recommendations for QA* Run load test with writes of 40 NYMs per second for at least 30 minutes with enabled metrics and dynamic quota control. Observed ordering performance should be at least 20, probably 25 NYMs per second. Also metrics should show that average latency will stabilize after some time. If everything looks good it might be useful to repeat test with same parameters and _ZMQ_CLIENT_QUEUE_SIZE_ set to 10. If there is still no performance degradation it's recommended to run full-blown DDoS test (like in INDY-1388) to see if pool is stable.  ></body> </Action>
<Action id="49959" issue="32875" author="nataliadracheva" type="comment" created="2018-09-06 09:57:58.0" updateauthor="nataliadracheva" updated="2018-09-06 09:57:58.0"> <body><! CDATA *Build Info:*  Indy-node 1.6.597 Indy-plenum 1.6.538   *OS/Platform:* Ubuntu 16.04  *Component:* Indy-node  *Reason for Reopen:* - avg throughput = 18 txns/sec; - latency is not stable; - 5 nodes became unreacheable.  *Steps to Reproduce:* 1. Ran "perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym" from 4 AWS instances for 90 minutes.  *Actual Results:* 315 success nyms, 5303 nack.  *Expected Results:* 54000 new nyms records in the domain ledger.  *Additional info:* Logs and metrics are here: \\iserver\exchange\Evernym\INDY-1602\40_nyms_90_min !1602.png|thumbnail!   ></body> </Action>
<Action id="50077" issue="32875" author="nataliadracheva" type="comment" created="2018-09-07 12:46:29.0" updateauthor="nataliadracheva" updated="2018-09-07 12:47:47.0"> <body><! CDATA *Scenario 1.* *Build version:* indy-node: 1.6.598 indy-plenum: 1.6.538 *Test description:* 4 machines send 10 nym/sec per each machine for 2h 10 min *Steps to Reproduce:* 1. Run perf_processes.py from 4 AWS agent with following parameters: {code:java} python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym{code}  *Expected results:* ordering: ~20-25, stable latency *Actual results:* ordering: ~21.24, stable latency *Additional info:*  https://s3.console.aws.amazon.com/s3/buckets/qanodelogs/indy-1602/?region=us-east-1  !1602_scenario1.png|thumbnail!  ></body> </Action>
<Action id="50080" issue="32875" author="nataliadracheva" type="comment" created="2018-09-07 13:10:04.0" updateauthor="nataliadracheva" updated="2018-09-07 13:10:04.0"> <body><! CDATA *Scenario 2.* *Build version:* indy-node: 1.6.598 indy-plenum: 1.6.538 *Test description:* 4 machines send 10 nym/sec per each machine. *Preconditions:* add ZMQ_CLIENT_QUEUE_SIZE = 10 to node config. *Steps to Reproduce:* 1. Run perf_processes.py from 4 AWS agent with following parameters: {code:java} python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym {code} *Expected results:* ordering: ~20-25, stable latency *Actual results:* Load script crashed and returned command line. {code:java} Exception ignored in: <bound method Task.__del__ of <Task finished coro=<LoadClient.run_test() done, defined at perf_processes.py:1084> exception=AttributeError("'NoneType' object has no attribute 'encode'",)>> Traceback (most recent call last): File "/usr/lib/python3.5/asyncio/tasks.py", line 93, in __del__ File "/usr/lib/python3.5/asyncio/futures.py", line 215, in __del__ File "/usr/lib/python3.5/asyncio/base_events.py", line 1177, in call_exception_handler File "/usr/lib/python3.5/logging/__init__.py", line 1308, in error File "/usr/lib/python3.5/logging/__init__.py", line 1415, in _log File "/usr/lib/python3.5/logging/__init__.py", line 1425, in handle File "/usr/lib/python3.5/logging/__init__.py", line 1495, in callHandlers File "/usr/lib/python3.5/logging/__init__.py", line 855, in handle File "/usr/lib/python3.5/logging/__init__.py", line 986, in emit File "/usr/lib/python3.5/logging/__init__.py", line 908, in handleError File "/usr/lib/python3.5/traceback.py", line 100, in print_exception File "/usr/lib/python3.5/traceback.py", line 474, in __init__ File "/usr/lib/python3.5/traceback.py", line 358, in extract File "/usr/lib/python3.5/traceback.py", line 282, in line File "/usr/lib/python3.5/linecache.py", line 16, in getline File "/usr/lib/python3.5/linecache.py", line 47, in getlines File "/usr/lib/python3.5/linecache.py", line 136, in updatecache File "/usr/lib/python3.5/tokenize.py", line 458, in open File "/usr/lib/python3.5/encodings/__init__.py", line 98, in search_function File "<frozen importlib._bootstrap>", line 969, in _find_and_load File "<frozen importlib._bootstrap>", line 954, in _find_and_load_unlocked File "<frozen importlib._bootstrap>", line 887, in _find_spec TypeError: 'NoneType' object is not iterable --- Logging error --- {code}  ></body> </Action>
<Action id="50148" issue="32875" author="derashe" type="comment" body="Load tests satisfied requirements, that we&apos;ve set for this ticket. But still, we can see OOM problem. We diagnose that memory graph very correlated with handling request count, which is too big. Dynamic quotas didn&apos;t work, because there are a few replicas who cannot order txns. This should be fixed in scope of https://jira.hyperledger.org/browse/INDY-1680. This ticket should be retested with these changes." created="2018-09-10 12:18:35.0" updateauthor="derashe" updated="2018-09-10 12:20:39.0"/>
<Action id="50240" issue="32875" author="ozheregelya" type="comment" body="UPD: It should be fixed in INDY-1681, not INDY-1680." created="2018-09-11 11:22:10.0" updateauthor="ozheregelya" updated="2018-09-11 11:22:10.0"/>
<Action id="50596" issue="32875" author="nataliadracheva" type="comment" created="2018-09-18 13:24:36.0" updateauthor="nataliadracheva" updated="2018-09-18 13:24:52.0"> <body><! CDATA *Scenario 2:* *Build version:* indy-node: 1.6.603 indy-plenum: 1.6.539 *Test description:* 4 machines send 10 nym/sec per each machine. *Preconditions:* turn of dynamic quotas + set MAX_REQUEST_QUEUE_SIZE  = 1000 + add ZMQ_CLIENT_QUEUE_SIZE = 10 to node config. *Steps to Reproduce:* 1. Run perf_processes.py from 1 AWS agent with following parameters: {code:java} python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym {code} *Expected results:* ordering: ~20-25, stable latency *Actual results:* ordering: 17 nyms/sec, unstable latency (x) *Additional info:* Logs: ev@evernymr33:~/logs/1602_dq_zmq.zip  ></body> </Action>
<Action id="50826" issue="32875" author="nataliadracheva" type="comment" created="2018-09-20 09:17:50.0" updateauthor="nataliadracheva" updated="2018-09-24 09:59:38.0"> <body><! CDATA *Scenario 1:* *Build version:* *indy-node: 1.6.603* *indy-plenum: 1.6.539* *Test description:* 4 machines send 10 nym/sec per each machine. *Preconditions:* turn of dynamic quotas + set MAX_REQUEST_QUEUE_SIZE = 1000 to node config. *Steps to Reproduce:* 1. Run perf_processes.py from 4 AWS agent with following parameters:  {code:java} python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym {code}  *Expected results:* ordering: ~20-25, stable latency *Actual results:* ordering: 17 nyms/sec, stable latency  Additional info: Logs: ev@evernymr33:~/logs/indy-1602/1602_scen1_603_754.zip  ></body> </Action>
<Action id="50965" issue="32875" author="sergey.khoroshavin" type="comment" created="2018-09-21 15:36:21.0" updateauthor="sergey.khoroshavin" updated="2018-09-21 15:36:21.0"> <body><! CDATA {quote} Scenario 2: Actual results: ordering: 17 nyms/sec, unstable latency  Additional info: Logs: ev@evernymr33:~/logs/1602_dq_zmq.zip {quote} According to metrics actual throughput during steady state is a bit below 20 nyms/sec  {quote} Scenario 1: Actual results: ordering: 17 nyms/sec, stable latency  Additional info: Logs: ev@evernymr33:~/logs/indy-1602/1602_scen1_603_754.zip {quote} According to metrics actual throughput during steady state is a bit above 20 nyms/sec  Logs and metrics analysis of last two cases showed the following: - reducing ZMQ client queue size have almost no effect on performance and memory consumption - there is possible performance degradation due to dynamic quotas (without them in similar scenario we reached up to 25 nyms/sec) - request queue doesn't grow indefinitely under load above pool capacity, although it still grows much higher than required threshold - it turned out that throttling client stack is not enough - requests get to nodes through PROPAGATEs regardless of current incoming request queue size - sometimes even when request queue size is above throshold client stack doesn't seem to be throttled - there is suspicion than non-finalized requests might accumulate in input queue - memory consumption continues to grow almost linearly despite request queue length not growing anymore (this finding might be worth mentioning in INDY-1688 as well)  Plan of attack: - add more metrics to see internal queues size (like number of nonfinalized requests, requests tracked by monitor, etc) to further track memory issues - add some logs into quota controller - research possibility of throttling PROPAGATEs (in a different issue)   ></body> </Action>
<Action id="51080" issue="32875" author="sergey.khoroshavin" type="comment" created="2018-09-24 10:12:31.0" updateauthor="sergey.khoroshavin" updated="2018-09-24 10:12:31.0"> <body><! CDATA Additional testing is recommended. *Version* indy-node: 1.6.610 indy-plenum: 1.6.545 *Load* 40 NYMs per second *Scenarios* 1) run load test of 40 NYMs per second for 30 minutes against AWS pool with default config 2) run load test of 40 NYMs per second for 90 minutes from same clients against same AWS pool with _ENABLE_DYNAMIC_QUOTAS = True_  This test is needed to: * find out if enabling dynamic quotas really degrade performance * find out relations between different internal queue sizes * check whether non-finalized requests accumulate or not * check whether monitor internal request queue grow unbounded leading to memory leak   ></body> </Action>
<Action id="51110" issue="32875" author="sergey.khoroshavin" type="comment" created="2018-09-24 16:45:43.0" updateauthor="sergey.khoroshavin" updated="2018-09-24 16:45:43.0"> <body><! CDATA Final conclusions (after a few more experiments): * there is no performance degradation due to dynamic quotas or reduced ZMQ client queue size, so they can be enabled by default * there is no visible effect on memory consumption with or without these options * for some yet unknown reason node request queue is not affected by dynamic quotas (it's big, but it doesn't grow unbounded) * for some yet unknown reason amount requests in request time tracker and node request queue can be totally different * throttling client stack is not enough - PROPAGATEs should be throttled as well  ></body> </Action>
