<Action id="27872" issue="18864" author="mzk-vct" type="comment" created="2017-07-05 13:46:38.0" updateauthor="mzk-vct" updated="2017-07-05 13:46:38.0"> <body><! CDATA *Problem reason:* ZMQ has internal queues for messages, which size is limited by 1000 by default.  On high load this queues can can become overflown and new messages can be dropped.  *Changes:* Queue size can be configured now using _ZMQ_INTERNAL_QUEUE_SIZE_ parameter. Default value is 0 what means 'unlimited'.  *Committed into:* https://github.com/hyperledger/indy-stp/pull/45  *Risk factors:* Nothing is expected.  *Risk:* Low  *Covered with tests:* stp_zmq/test/test_zstack.py/test_high_load  *Recommendations for QA (optional):* 1. Test it with different number of nodes 2. Try load testing with big  amount of requests 3. Try to switch off 3pc batching (and simple batching) to increase number of messages    ></body> </Action>
<Action id="28023" issue="18864" author="mzk-vct" type="comment" created="2017-07-07 15:43:27.0" updateauthor="mzk-vct" updated="2017-07-07 15:43:27.0"> <body><! CDATA Builds:  - node 0.4.14 - client 0.4.24  ></body> </Action>
<Action id="28104" issue="18864" author="lovesh" type="comment" body="The default value of 0 would fill up all the memory of the node and crash it if one of the other node is not receiving. There is a reason why high water mark exists, don&apos;t undermine that. The ticket mentions that one possible option is dropping older messages, was it considered. I oppose the setting of default to 0, we need to be able to drop old messages." created="2017-07-10 17:16:51.0" updateauthor="lovesh" updated="2017-07-10 17:18:02.0"/>
<Action id="28223" issue="18864" author="mzk-vct" type="comment" created="2017-07-12 11:18:00.0" updateauthor="mzk-vct" updated="2017-07-12 11:18:12.0"> <body><! CDATA  ~lovesh   Yes. There is no default support of such functionality in zeromq. There is ZMQ_CONFLATE option, which can be used to simulate this, but documentation says that it does not support multipart messages.  Default values is set to 0 just in order to check how it works without such limitation. This value can be configured in any moment we see that it does not work.  That's why I added such recommendations for QA.  I suggested to move logic for dropping messages from zmq to plenum internals.  Also as far as I know  ~andkononykhin  researching possibility of reimplementing message handling in reactive way. If we can do it then it will be easier to process queue and apply our logic instead of the one zmq provides.    ></body> </Action>
<Action id="28279" issue="18864" author="krw910" type="comment" body="Blocked by INDY-406" created="2017-07-12 20:26:11.0" updateauthor="krw910" updated="2017-07-12 20:26:11.0"/>
<Action id="28603" issue="18864" author="krw910" type="comment" body="The product is good enough to go Stable. I cannot reproduce this issue and we will need to wait until the alpha pool reports back." created="2017-07-18 19:42:55.0" updateauthor="krw910" updated="2017-07-18 19:42:55.0"/>
<Action id="28606" issue="18864" author="danielhardman" type="comment" body="I agree with  ~lovesh &apos;s comment that ignoring the high water mark (allowing the FIFO queue to grow until memory is exhausted) is unwise. However, I also agree with  ~krw910  that this is good enough for the short term. Therefore, I logged a new ticket, INDY-437, to capture the additional work of improving this fix." created="2017-07-18 20:12:34.0" updateauthor="danielhardman" updated="2017-07-18 20:12:34.0"/>
