<Action id="45427" issue="30829" author="sergey.khoroshavin" type="comment" created="2018-06-01 22:27:42.0" updateauthor="sergey.khoroshavin" updated="2018-06-01 22:27:42.0"> <body><! CDATA Attached graphs with different timescale to show size of problem (Y is MESSAGE_REQUEST(PROPAGATE) sent per second, scale is about 5600). Also attached detailed graph in time area where problem was first found and filtered logs from Node14.  At the first glance it can be seen that at first node receives and discards (because already ordered) lots of propagated requests, then does some view change and catchup, and then throws a burst of MESSAGE_REQUESTSs many of which contain same reqId that was discarded previously.  ></body> </Action>
<Action id="46735" issue="30829" author="toktar" type="comment" created="2018-06-29 09:02:20.0" updateauthor="toktar" updated="2018-07-05 12:39:42.0"> <body><! CDATA *Problem reason:* * If view change happen than master replicas already ordered transaction and not all backup replicas do it, then slow replicas continue ordering after view change but can't to do it because most replicas ordereded this transaction and clean data from request list.  *Changes:* * add tests in test_different_last_ordered_before_view_change.py, * clear requests after view change for ordered txns, * change last ordered after view change to maximum of ordered on master and current value.  *PR:* *  https://github.com/hyperledger/indy-plenum/pull/781  *  https://github.com/hyperledger/indy-node/pull/808   *Version:* * indy-node 1.3.493 -master * indy-plenum 1.4.435 -master  *Risk factors:* * Problem with backup replicas, сonsequently often view changes and large logs and bad performance  *Risk:* * Low  *Covered with tests:* * * test_different_last_ordered_before_view_change.py|https://github.com/hyperledger/indy-plenum/pull/781/files#diff-7666afce4380260ea92fe195bcd62c83 * ** test_different_last_ordered_on_backup_before_view_change ** test_different_prepares_on_backup_before_view_change ** test_different_last_ordered_on_master_before_view_change *  test_api.py|https://github.com/hyperledger/indy-plenum/pull/781/files#diff-936570e7f1de2f0f56e3ea3e4c63b421  ** test_create_3pc_batch_with_empty_requests  *Recommendations for QA:*  Run load test and check load, count of view changes and log size. Check decrease of count of message requests with propagate if it possible.   ></body> </Action>
<Action id="46999" issue="30829" author="vladimirwork" type="comment" created="2018-07-06 12:12:50.0" updateauthor="vladimirwork" updated="2018-07-06 12:12:50.0"> <body><! CDATA Build Info: indy-node 1.4.493  Steps to Reproduce: 1. Run load test against 22 nodes pool of version 492 master. 2. Check time between log archives and count number of lines with PROPAGATE word. 3. Upgrade this pool to 493 master and run load test with the same parameters. 4. Check time between log archives and count number of lines with PROPAGATE word.  Actual Results: The number of view changes before and after upgrade is about the same (0) except the period right after the upgrade (it is expected because of nodes restarting). Time between log archives making is about 3-4 minutes (493) against 7-10 (492) so it looks like we have more logs after the upgrade. The number of lines with PROPAGATE word (`find /var/log/indy/sandbox/NodeXX.log.XXX.xz -exec xzcat {} \; | grep "PROPAGATE" | wc -l`) is about 60k-80k (493) against 33k-34k (492) in the same size log archive so it looks like we have more message requests with propagate after the upgrade.  Expected Results: There should be less amount of logs and propagate messages after the upgrade against the same load.  ></body> </Action>
<Action id="47043" issue="30829" author="vladimirwork" type="comment" body="Retest with forced view changes (492 -&gt; 494)." created="2018-07-06 15:06:21.0" updateauthor="vladimirwork" updated="2018-07-09 10:36:59.0"/>
<Action id="47140" issue="30829" author="vladimirwork" type="comment" created="2018-07-10 10:35:02.0" updateauthor="vladimirwork" updated="2018-07-10 10:35:02.0"> <body><! CDATA Build Info: indy-node 1.4.495  Steps to Validate: 0. Set forced view change frequency every 1800 seconds in indy_config.py. 1. Run load test against 22 nodes pool of version 492 master. 2. Check time between log archives and count number of lines with "MESSAGE_REQUEST.*PROPAGATE". 3. Upgrade this pool to 495 master and run load test with the same parameters. 4. Check time between log archives and count number of lines with "MESSAGE_REQUEST.*PROPAGATE".  Actual Results:  Before the upgrade: {noformat} ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.45*.xz -exec xzcat {} \; | grep "MESSAGE_REQUEST.*PROPAGATE" | wc -l 1539564 ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.46*.xz -exec xzcat {} \; | grep "MESSAGE_REQUEST.*PROPAGATE" | wc -l 959577 ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.47*.xz -exec xzcat {} \; | grep "MESSAGE_REQUEST.*PROPAGATE" | wc -l 795270 {noformat}  After the upgrade: {noformat} ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.49*.xz -exec xzcat {} \; | grep "MESSAGE_REQUEST.*PROPAGATE" | wc -l 17190 ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.50*.xz -exec xzcat {} \; | grep "MESSAGE_REQUEST.*PROPAGATE" | wc -l 6675 ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.51*.xz -exec xzcat {} \; | grep "MESSAGE_REQUEST.*PROPAGATE" | wc -l 35523 {noformat}  ></body> </Action>
