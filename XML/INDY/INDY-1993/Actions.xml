<Action id="58258" issue="37651" author="vladimirwork" type="comment" created="2019-03-18 15:15:44.0" updateauthor="vladimirwork" updated="2019-03-22 15:59:36.0"> <body><! CDATA PoA:  AWS QA Live Pool:  1. Production load with payments + one node big catch-up not under load. (done) (/) {noformat} ev@evernymr33:logs/1993_19_03_2019_prod_logs.tar.gz ev@evernymr33:logs/1993_19_03_2019_prod_metrics.tar.gz {noformat}  2. Production load with payments + one node big catch-up + one node small catch-up under load. (done) (x) 3rd node breaks with stacktrace (fixed in 869 master). 2, 10, 15, 20, 25 nodes (that were stopped and started during the load) don't catchup (even after service restart). 1st node was started and stopped and performs catch up successfully. {noformat} ev@evernymr33:logs/1993_20_03_2019_prod_catchup_under_load_logs.tar.gz ev@evernymr33:logs/1993_20_03_2019_prod_catchup_under_load_metrics.tar.gz {noformat}  2.2. *From 1 to 10  txn per sec* load *without* payments with pool and config txns + small catchups (1, 2, 5, 10 minutes) of a single node. (done) (!) *Node doesn't catch up if it was off for 10 minutes under 10 txns/sec (all except payments) load.* There is a constant difference from 7k to 14k between catch-up attempts.  3. Production load with payments + forced VC + pool and config txns. (done) (!) 5th node has View 6 (and not all ledgers are catched up in this node) after the load but all other nodes have View 42. {noformat} ev@evernymr33:logs/1993_20_03_2019_prod_view_changes_logs.tar.gz ev@evernymr33:logs/1993_20_03_2019_prod_view_changes_metrics.tar.gz {noformat}  3.2. Production load *without* payments + forced VC + pool and config txns. (done) (x) All nodes have different amount of txns in ledgers at the end of load and there were different ViewNo on different nodes during load. During the night the most of nodes restarted and changed the ViewNo to 0. There are also lagged nodes against 872-master. {noformat} ev@evernymr33:logs/1993_21_03_2019_10_all_view_changes_logs.tar.gz ev@evernymr33:logs/1993_21_03_2019_10_all_view_changes_metrics.tar.gz  ev@evernymr33:logs/1993_22_03_2019_view_changes_logs.tar.gzes_logs.tar.gz {noformat}  4. Production load with payments and fees + one node big catch-up + one node small catch-up under load. 5. DDOS. 6. Production load with payments and fees + forced VC + pool and config txns. 7. One, two, multiple (f changing) nodes adding (catch-up) + production load.  Docker Pool: 1. Test catch-up for all 5 ledgers (pool, config, domain, audit, token) manually and with system test (already implemented). 2. One, two, multiple (f changing) nodes adding (catch-up).  ></body> </Action>
<Action id="58259" issue="37651" author="sergey.khoroshavin" type="comment" created="2019-03-18 15:44:59.0" updateauthor="sergey.khoroshavin" updated="2019-03-18 15:44:59.0"> <body><! CDATA *Updated version* indy-node: 1.6.866-master indy-plenum: 1.6.729-master  *PR* https://github.com/hyperledger/indy-plenum/pull/1130  *Changes and reason* New catch-up logic first catches up audit ledger and only after that pool and other ledgers. This can lead to much slower catch up if node was absent for too long and lots of new nodes joined in the meantime. So this patch changes catch-up logic to include initial pre-sync of pool ledger before starting catching up audit ledger and all other ledgers (including pool again).  ></body> </Action>
<Action id="58292" issue="37651" author="sergey.khoroshavin" type="comment" created="2019-03-19 13:43:52.0" updateauthor="sergey.khoroshavin" updated="2019-03-19 14:22:15.0"> <body><! CDATA Analysis of logs from *case 1* showed that: 1. At one point nodes paused ordering and then resumed with slightly degraded performance of one of backup instances, however there were no catchups during this event and generally this is unrelated to catch-up logic 2. No node lagged behind during load, so catch-up under load apparently didn't happen 3. Node20 was manually turned off during load and turned on back later. It catched up successfully.  All in all everything looks good for now, there are no apparent regressions in catch-up logic, but more tests are needed (as per PoA).   ></body> </Action>
<Action id="58322" issue="37651" author="sergey.khoroshavin" type="comment" created="2019-03-19 22:14:16.0" updateauthor="sergey.khoroshavin" updated="2019-03-19 22:14:16.0"> <body><! CDATA *Updated version* indy-node: 1.6.869-master indy-plenum: 1.6.730-master  *PR* https://github.com/hyperledger/indy-plenum/pull/1132  *Changes and reason* Fixes crash when node receives message request for consistency proof with seqNoEnd larger than current ledger size. Problem was caused by unhandled exception raised from build_consistency_proof when replying to message requests for consistency proofs.  ></body> </Action>
<Action id="58365" issue="37651" author="sergey.khoroshavin" type="comment" created="2019-03-21 08:46:27.0" updateauthor="sergey.khoroshavin" updated="2019-03-21 13:04:57.0"> <body><! CDATA *Analysis of logs*  *Case 2* Nodes were not able to sync and continue ordering due to: 1) after long catchups (>10 minutes longs) PREPREPARES were too old to process - this is a known problem to be addressed in INDY-1983 2) due to bug in processing message requests for consistency proofs (present in load test, but fixed in 1.6.869-master) lagging nodes were crashing when other nodes started catch ups, so they were unable to finish catch up normally  *Case 3* Node 5 went out of sync during view change and apparently had corrupted ledger. Rough timeline was: * node 5 restarted, started catch up and caught up till (5, 1639), at this point pool reached (5, 1766) * node 5 started new round of catch up, caught up till (5, 1771), at this point pool reached (5, 2751) * node 5 started applying stashed 3PC messages, at the same time other nodes in pool performed quick view change to 6 and continued ordering * node 5 ordered till  (5, 2748) and also entered view change (10 minutes late than other nodes in pool) * node 5 ordered till (5, 2751) during view change, reverted 2 unordered batches and started catch up * when trying to apply catch up replies node 5 found that resulting root hashes were different from what was expected and blacklisted all other nodes * why root hashes were different is *TDB*  Also apparently some nodes were receiving large catchup requests and for some reason were unable to split reply so that message would fit into network message size limits: {code} 2019-03-19 16:59:58,855|INFO|seeder_service.py|Node6 received catchup request: CATCHUP_REQ{'ledgerId': 0, 'catchupTill': 7110, 'seqNoStart': 26, 'seqNoEnd': 7110} from b' kGFQrntZEcKJpn2QL:3TImpiGVKH9<cSx(CS6s&' 2019-03-19 16:59:59,174|WARNING|zstack.py|CONNECTION: Cannot transmit message. Error InvalidMessageExceedingSizeException('Message len 4293000 exceeded allowed limit of 131072',) {code}  *Case 3.2* Many nodes went out of sync during view changes with symptoms very similar to *case 3*. Investigation is ongoing.   ></body> </Action>
<Action id="58624" issue="37651" author="ashcherbakov" type="comment" created="2019-03-27 11:30:23.0" updateauthor="ashcherbakov" updated="2019-03-27 11:30:23.0"> <body><! CDATA *Test cases where the issues reproduced* The issues are reproduced only when we do forced view change tests and write to domain, pool and config ledgers at the same time. If we write to domain ledger only, the pool successfully survived 120 forced view changes and continues ordering.  I think the main difference here is that when we write to multiple ledgers, we created multiple PrePrepares (for each ledger) almost at the same time which increases a risk of out-of-order messages.  *Current problems:* 1) Some nodes have "phantom" audit txns that other nodes don't have. Moreover, there is no corresponding main ledger txn written to the ledger for these nodes, so it looks like it's written to audit ledger only. Moreover, there is not Ordered msg in longs corresponding to this phantom audit txn. So, it looks like it was applied, but not committed explicitly.  2) Audit ledger tried to revert txns while it's empty (LogicError). It happens at the same time, as issue 1, so it looks like it's related. 3) There are a lot of warnings `The first created batch has not been committed or reverted and yet another batch is trying to be committed` from idr_cahce (first time it's raised for Pool ledger batch). So, it looks like idr_cahce is broken.  *Important notes* 1) Issues 1 and 2 are reproduced from a very long sequence of view changes, where it's called from the PreViewChange strategy. The specific here is that view change (and hence catchup, and hence revert of unordered batches) is called from Replica, not from Node (see node's `prod`). It also leads to the fact that `forceProcessedOredered` ordered non-zero count of Ordered messages before starting a view change. 2) There were cases when we were processing a lot of 3PC messages in one run of replica's service method, and stabilized checkpoints at the same time without letting a chance to execute ordered batches.  3) A lot of PrePrepares are discarded because if incorrect time (because the node was lagging behind). But the corresponding batches were eventually ordered. It looks like the time was accepted eventually because a quorum of Prepares was gathered.   ></body> </Action>
<Action id="58626" issue="37651" author="ashcherbakov" type="comment" created="2019-03-27 11:36:53.0" updateauthor="ashcherbakov" updated="2019-03-27 11:36:53.0"> <body><! CDATA BTW one issue with audit ledger has been fixed: *Problem* If there was audit ledger catch-up (catching up more than zero txns), then multiple 3PC batches were applied (uncommitted), and then all of then needs to be reverted, then the revert of the first batch will not happen.  *Problem reason* This is because last_committed wasn't updated in audit ledger's uncommitted batch tracker (used for apply-revert-commit logic) after catchup  *PR* - fix: https://github.com/hyperledger/indy-plenum/pull/1138 - more tests: https://github.com/hyperledger/indy-plenum/pull/1136, https://github.com/hyperledger/indy-node/pull/1221    ></body> </Action>
<Action id="58639" issue="37651" author="vladimirwork" type="comment" created="2019-03-27 15:43:12.0" updateauthor="vladimirwork" updated="2019-03-28 12:24:48.0"> <body><! CDATA The last runs' results (forced VCs each 1800 seconds / domain+pool+config txns):  874-master: {noformat} ev@evernymr33:logs/1993_25_03_2019_view_changes_logs.tar.gz ev@evernymr33:logs/1993_25_03_2019_view_changes_metrics.tar.gz {noformat}  876-master: {noformat} ev@evernymr33:logs/1993_27_03_2019_view_changes_logs.tar.gz ev@evernymr33:logs/1993_27_03_2019_view_changes_metrics.tar.gz {noformat}  83-stable: {noformat} ev@evernymr33:logs/1993_28_03_2019_view_changes_logs.tar.gz ev@evernymr33:logs/1993_28_03_2019_view_changes_metrics.tar.gz {noformat}   ></body> </Action>
<Action id="58662" issue="37651" author="vladimirwork" type="comment" created="2019-03-28 10:18:52.0" updateauthor="vladimirwork" updated="2019-03-28 10:18:52.0"> <body><! CDATA Results summary: We sustain production load (domain and payment txns) for 12+ hours and catch up ledgers under load successfully if nodes are disconnected less than 10 minutes. There are no performance degradation found and audit ledger behaves as expected at this load level. All issues found during load testing with pool and config txns / forced VCs are reported as separate tickets: INDY-2032 INDY-2034 INDY-2035  ></body> </Action>
