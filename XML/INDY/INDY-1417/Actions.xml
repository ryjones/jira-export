<Action id="46525" issue="31142" author="anikitindsr" type="comment" created="2018-06-26 08:37:54.0" updateauthor="anikitindsr" updated="2018-06-26 08:37:54.0"> <body><! CDATA In the scope of this issue some experiment was spent. The steps were: * create pool of 4 nodes in docker; * patch 'scheduled' method `checkPerformance` which run every 15 seconds for node2. The main goal of this changes is to stop node's clientstack, wait some second and start it again. * restart indy-node service for node2 * connect to pool using old cli, because new cli does have any monitoring features and there is no any method for connection watching. * using netstat tool check, that previous connection was close immediately, and new connection is realy new connection with different socket  Periodically clientstack closing is not break the pool functionality and realy new socket/connection is created. As a small fix, we can add scheduled job for periodically stoping/starting clientstack with configurable timeout plus some random delay.   ></body> </Action>
<Action id="46595" issue="31142" author="sergey-shilov" type="comment" created="2018-06-27 11:39:02.0" updateauthor="sergey-shilov" updated="2018-06-27 12:01:48.0"> <body><! CDATA Since we have a connections limit (no matter whether it is controlled by external firewall for which we have a recommended setup or just by a kernel fds limit per process) we can face the situation when permanent established connections don't allow new clients to connect infinitely. To solve this problem we should implement some mechanism that disconnects connected clients so that a new clients have a chance to connect.  The main problem here is that we use ZeroMQ that fully encapsulates all work with clients connection sockets and there is no way to force ZeroMQ to close particular client connection.  During our research we figured out three possible solutions: * External firewall: configure iptables so that it drops established connection after certain time. * Close client sockets from node's code. * Periodic restart of client stack.  Let's look at each solution in details.  *1. External firewall (iptables).*  The iptables is the most standart firewall that available on the most linux distribution, that's why saying "firewall" we assume "iptables". The main advantage of using firewall is that modification of node's code is not needed. The main risk here is communication with stewards, checking configuration correctness and so on.  But reading docs showed that there is no way to achieve needed functionality using iptables, links:   https://linux.die.net/man/8/iptables   https://serverfault.com/questions/694024/can-iptables-automatically-drop-established-session-after-certain-seconds   *2. Close client sockets from node's code.*  Despite ZeroMQ encapsulates all work with clients connection sockets we can retrieve a new connection socket file descriptor from *CONNECTED* event reported by ZeroMQ monitor socket. So that we can close this file descriptor using an ordinary _close()_ system call after certain time from *CONNECTED* event.  I've done a lot of experiments with various options of closing connections and faced with some issues that seems to be very hard to avoid. The main of them is that _ZeroMQ does not expect that it's controlled sockets may be closed externally._ Internally ZeroMQ is the epoll-based network library and seems like even closing with _linger_ option does not trigger any epoll events. So this invalid descriptor continues to be kept in ZeroMQ's epoll and *DISCONNECTED* event is not triggered. This leads to ZeroMQ crash with _"Bad file descriptor"_ error during ZeroMQ finalizing. Moreover, seems like newly open socket with the same file descriptor number leads to mess in inner ZeroMQ structures associated with externally closed file descriptor as they were not finalized.  There is another method of initiating of closing connection called _"half closing"._ This is done by calling _shutdown_ system call with various options. In my experiments graceful half closing (I mean client side does it right, sends _FIN_ segment etc.) there is triggered epoll event and then triggered *DISCONNECTED* ZeroMQ event. But this approach is dangerous if client side becomes silent and does not send _FIN_ segment. In this case client connection socket fall into _FIN_WAIT1_ long state and ZeroMQ keeps associated structures that leads to issues described above.  One more concern regarding this solution: node's code and ZeroMQ work with the same client socket in different threads unsynchronized, this may lead to crashes and unpredictable behavior. So this solution can not be considered as acceptable.  There is one thing that I don't understand for now. For each client connection socket we enable TCP keepalive for dead peer detection. After several probes TCP keepalive mechanism drop client connection socket (_RST_ segment is sent to client that means lingering close) and we see *DISCONNECTED* ZeroMQ event triggered. Seems like the kernel uses some other mechanisms unreachable from user space to close sockets  and triggers some epoll event (may be EPOLLHUP). But seems like the answer on the question how the kernel does it does not solve our issues as we are working in the user space.  *3. Periodic restart of client stack.*  Periodic restart of client stack does not consider how long clients are connected, it just drops all connections, and of course this is the main disadvantage. But the main advantage of such approach is safety. In this case we do not do any things that are unexpected by ZeroMQ. There is some sockets overhead during client stack stop/start as all clients sockets for which was data transmission fall into _TIME_WAIT_ state for about one minute before final dropping, but this is not critical, we just should consider this.  There are two options of scheduling of stack restart event: by timeout and by number of connections. I prefer a combination of them. Say if we restart stack every 10 minutes unconditionally we will drop all 10 connected clients while it is not necessary. From the other hand if we restart just on connection limit (we can track it using *CONNECTED/DISCONNECTED* events) then we may fall into infinite stack restart because all ZeroMQ-based clients try to re-establish connection immediately, So if we have a limit 1000 then we can observe something like this: * have 1001 client connected * stop client stack and drop 1001 connected clients * start stack and immediately have 1001 re-established clients connection * stop client stack * etc.  So we should restart client stack after some timeout even if limit is reached. Of course we assume that our recommended firewall setting for connections limit are applied.  *CONCLUSION*  Since we still use ZeroMQ we can consider client stack restart as the most appropriate solution. It is not hard to implement and meets our requirements. One thing here is that stack restart should be tested under load as connections closing may occur in various data transmission and client/server exchange phases.  ></body> </Action>
<Action id="46596" issue="31142" author="sergey-shilov" type="comment" body="Created  INDY-1431|https://jira.hyperledger.org/browse/INDY-1431  for implementation." created="2018-06-27 11:57:02.0" updateauthor="sergey-shilov" updated="2018-06-27 11:57:02.0"/>
<Action id="46935" issue="31142" author="sergey-shilov" type="comment" created="2018-07-05 08:15:56.0" updateauthor="sergey-shilov" updated="2018-07-05 08:15:56.0"> <body><! CDATA Some info about service downtime during client stack restart.  All TCP stack clean up is done on the operating system level asynchronously relating to our application level. All open client connections sockets are fallen into TIME_WAIT state, we should consider that in our file descriptors limits calculation logic, but it does not affect service downtime. As ZeroMQ uses SO_REUSEADDR for listener socket we can re-bind client listener immediately.  Also we close listener socket with linger option, so listener socket kernel structure should be freed immediately. So ZeroMQ client stack restart itself takes at most milliseconds regardless load.  There are two other things that really affects service downtime: 1) May be it would be nice to add some sleep between client stack stop and start, in our tests we added sleep 0.2 seconds, but seems like it is not necessary as I checked that stop/start works well even without that sleep. 2) The more important option is to add some sleep AFTER client stack restart. Rationale for this is the fact that we disconnect all connected clients while we may have prepared replies for some clients. Each client is identified by connection identity and we store connection identities to be able to match replies and clients. After client stack restart we still try to send replies to clients by their identities. All IndySDK-based (and thus ZeroMQ-based) clients try to re-establish connection immediately after disconnect from the server side with the same identities. So the client will receive reply from the server if this client re-establish TCP connection before sending reply by the server even in case of client stack restart. To increase the probability of re-established TCP connection we can add some sleep (about 3-5 seconds, may be configurable) after client stack restart. But anyway not received reply by client is not a critical situation. In case of read or write requests it case re-send request and get desired result. But it would be much better to decrease probability of requests re-sending by clients.   So according to 1) and 2) overall service downtime is about 3-5 seconds (may be decreased to 1-2 seconds by configuration for real network) and most of this downtime is a sleep after client stack restart to allow disconnected clients to reconnect and receive their replies.  ></body> </Action>
<Action id="46942" issue="31142" author="ashcherbakov" type="comment" created="2018-07-05 11:16:59.0" updateauthor="ashcherbakov" updated="2018-07-05 11:16:59.0"> <body><! CDATA Implementation of the proposed option will be done in the scope of NDY-1431.     ></body> </Action>
<Action id="46961" issue="31142" author="sergey-shilov" type="comment" created="2018-07-05 16:07:02.0" updateauthor="sergey-shilov" updated="2018-07-05 16:07:02.0"> <body><! CDATA I've got to fully understanding why we can not close ZeroMQ sockets from node's code (option 2 in my research report, see https://jira.hyperledger.org/browse/INDY-1417?focusedCommentId=46595&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-46595).  In my research I was wondered why TCP keep-alive does not lead to ZeroMQ crash with "Bad file descriptor" error in epoll during finalization.  The answer was found during testing of sample ZeroMQ-based python server, sample epoll-based server in C, sniffing traffic, reading of ZeroMQ sources and (heh-heh) Linux kernel sources related to TCP and keep-alive. By the way, ZeroMQ is epoll-based library.  First of all, closing of socket leads to silent removing it from epoll (if there are no any socket duplicates, it is not our case, ZeroMQ does not creates any duplicates). This happens when close is called for socket from node's code. Such closing does not lead to ZeroMQ immediate crash, but ZeroMQ still thinks that socket is alive and keeps associated application-level handle structure. When it is time to finalize ZeroMQ library finalizes each active handle and removes associated file descriptor from epoll. If a socket was closed from node's code then attempt to remove corresponding file descriptor from epoll fails with "Bad file descriptor" error and ZeroMQ crashes.  So that, the problems of closing of sockets from node's code are: 1) memory leak as ZeroMQ keeps handles of closed sockets; 2) ZeroMQ can not be normally finalized due to "Bad file descriptor" error; 3) ZeroMQ works in separate thread, manipulation with sockets in other threads is not a good idea.  But how TCP keep-alive works? In fact, TCP keep-alive does not close socket. As I supposed in research report, the kernel uses its own mechanisms to generate EPOLLHUP, EPOLLERR and EPOLLIN events for socket. ZeroMQ wakes up from epoll_wait, finds out that events on socket, closes it and finalizes corresponding handle. Thats why nothing is broken when TCP keep-alive is enabled.  But we are working in user space and we can not generate such socket events even having its file descriptor. That's why client stack restart is still the most appropriate and safe solution since we use ZeroMQ. But of course in should be tested very aggressively.  ></body> </Action>
