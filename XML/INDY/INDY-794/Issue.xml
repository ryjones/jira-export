<Issue id="20512" key="INDY-794" number="794" project="10303" reporter="andkononykhin" creator="andkononykhin" type="10002" summary="Primary selection possibly failure cases" priority="3" resolution="10000" status="10001" created="2017-08-29 15:13:26.0" updated="2019-03-29 20:32:55.0" resolutiondate="2019-03-29 20:32:55.0" votes="0" watches="3" workflowId="20516"> <description><! CDATA During work on INDY-463 new more possible failure cases were found. They haven't been reproduced yet but at least they should be checked (seems some of them could be checked only in scope of integration auto-tests in indy-plenum) and then fixed if necessary.  Cases: # demote one node when primary is NodeX - make several view changes so the primary is NodeX again - promote demoted node - check that it accepts current primary (_expect failure cause currently nodes discards previous primary without checking difference between view numbers_) # the same as previous but restart instead of re-promotion # do view change(s) - demote one node - restart remaining pool - do some txns - promote previously demoted node - check how it does catch-up: _it has viewNo than restarted remaining pool but less txns in ledger_ # the same as previous but restart instead of re-promotion # do 4 view changes for 4-nodes pool - add new validator - do one more view change #5: _expect primary of the view change #5 will be discarded and one more view change #6 happens cause according to formula primary(viewNo=4, totalNodes=4) = primary(viewNo=5, totalNode=5)_ # disconnect node from 4-nodes pool - do several view changes until next primary for backup instance points to disconnected node - check what happens: _it seems we will get non-functional backup instance for long cause according to current code in plenum that nodes don't ensure VIEW_CHANGE_DONE from next backup instance primary and won't receive primary disconnected event as it will has already happened and it is monitored for master instance only_  ></description> </Issue>
