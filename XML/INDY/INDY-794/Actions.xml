<Action id="30334" issue="20512" author="krw910" type="comment" body=" ~ozheregelya  Please test these scenarios and log any issues you find." created="2017-08-29 20:59:42.0" updateauthor="krw910" updated="2017-08-29 20:59:42.0"/>
<Action id="39790" issue="20512" author="zhigunenko.dsr" type="comment" created="2018-02-07 15:20:38.0" updateauthor="zhigunenko.dsr" updated="2018-02-09 15:05:06.0"> <body><! CDATA h6. Case #1 demote one node when primary is NodeX - make several view changes so the primary is NodeX again - promote demoted node - check that it accepts current primary (_expect failure cause currently nodes discards previous primary without checking difference between view numbers_) *Steps to reproduce:* 1. Sequentially demote and promote NODE for each of 7 node in pool 2. After a full cycle of changing the primary node, send a few nyms *Actual results:* * Accident 1:  If node's promotion requires to increase number of backup of primary node, new backup instance duplicate current primary node|https://jira.hyperledger.org/browse/INDY-1112  - reason was found  * Accident 2:  if send demote command twice, demoted node looks like connected again {code} _indy@sandbox> send NODE dest=Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8 data={"alias":"Node6", "services":  }_ Sending node request for node DID Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8 by V4SGRU86Z58d6TV7PBUe6f (request id: 1518010476342686) indyd63811 updated its pool parameters: f 2, totalNodes 7,minNodesToConnect 3, quorums {'view_change': Quorum(5), 'timestamp': Quorum(3), 'election': Quorum(5), 'consistency_proof': Quorum(3), 'bls_signatures': Quorum(5), 'ledger_status': Quorum(4), 'view_change_done': Quorum(5), 'propagate_primary': Quorum(3), 'commit': Quorum(5), 'same_consistency_proof': Quorum(3), 'observer_data': Quorum(3), 'propagate': Quorum(3), 'f': 2, 'reply': Quorum(3), 'checkpoint': Quorum(4), 'prepare': Quorum(4)} Node request completed Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8 {code} {color:red} CONNECTION: 7s7HxzCLzaoikXpJW8eMAmWtTA3XJmEbTaN3J44vaD2X looking for Node6C at 10.0.0.7:9712 CONNECTION: 7s7HxzCLzaoikXpJW8eMAmWtTA3XJmEbTaN3J44vaD2X now connected to Node6C CATCH-UP: 7s7HxzCLzaoikXpJW8eMAmWtTA3XJmEbTaN3J44vaD2X completed catching up ledger 0, caught up 0 in total {color}  h6. Case #2 the same as previous but restart instead of re-promotion *Steps to reproduce:* 1. Sequentially stop and start indy-node service on each of 7 node in pool 2. After a full cycle of changing the primary node, send a few nyms *Actual results:* {color:green}NYMs successfully sent{color}  h6. Case #3 do view change(s) - demote one node - restart remaining pool - do some txns - promote previously demoted node - check how it does catch-up: _it has viewNo than restarted remaining pool but less txns in ledger_ *Step to reproduce:* 1) Demote one node 2) make some NYM/GET_NYM 3) Simultaneously restart services on all nodes (exclude demoted node) 4) make some NYM/GET_NYM 5) Promote node *Actual results:* {color:green}CATCH-UP is successful{color} 6) Demote one node 7) make some NYM/GET_NYM 8) Simultaneously restart services on all nodes (include demoted node) 9) make some NYM/GET_NYM 10) Promote node *Actual results:* {color:green}CATCH-UP is successful{color}  h6. Case #4 the same as previous but restart instead of re-promotion *Step to reproduce:* 1) Demote one node 2) make some NYM/GET_NYM 3) Simultaneously restart services on all nodes (exclude demoted node) 4) make some NYM/GET_NYM 5) Promote node *Actual results:* {color:green}CATCH-UP is successful{color}  h6. Case #5 do 4 view changes for 4-nodes pool - add new validator - do one more view change #5: _expect primary of the view change #5 will be discarded and one more view change #6 happens cause according to formula primary(viewNo=4, totalNodes=4) = primary(viewNo=5, totalNode=5)_ *Step to reproduce:* 1. Stop and start indy-node service on node1 - node4 sequentially 2. Add node5 to pool 3. Stop service in current primary node1 *Actual results:* {color:green}viewNo=5 discarded, viewNo=6 is successful{color}  *Step to reproduce:* 4. Restart service on node1 5. demote node5 6. Stop and start indy-node service on node2 - node4 sequentially *Actual results:* {color:green}Primary changed to node1 with correct viewNO{color}  h6. Case #6 disconnect node from 4-nodes pool - do several view changes until next primary for backup instance points to disconnected node - check what happens: _it seems we will get non-functional backup instance for long cause according to current code in plenum that nodes don't ensure VIEW_CHANGE_DONE from next backup instance primary and won't receive primary disconnected event as it will has already happened and it is monitored for master instance only_ {color:red}Trouble with "view change" has been{color}  found|https://jira.hyperledger.org/browse/INDY-1151  TO TEST: "view change" through load script  ></body> </Action>
<Action id="40411" issue="20512" author="zhigunenko.dsr" type="comment" created="2018-02-20 13:22:05.0" updateauthor="zhigunenko.dsr" updated="2018-02-27 14:02:04.0"> <body><! CDATA *Environment:* indy-anoncreds 1.0.32 indy-node 1.3.311 indy-plenum 1.2.250 libindy 1.3.1~399 libindy-crypto 0.2.0 python3-indy-crypto 0.2.0 Docker, 7 nodes  h6.CASE #1 *Steps to reproduce:* # Demote node1 # Sequentially stop and start _indy-node_ for each of 2..7 node in pool # After a full cycle of changing the primary node, promote node1  *Actual results:* {color:green}Promoted node have got actual viewChangeNo{color}  h6.CASE #2 *Steps to reproduce:* # Stop _indy_node_ on node1 # Sequentially stop and start _indy-node_ for each of 2..7 node in pool # After a full cycle of changing the primary node, start _indy_node_ on node1  *Actual results:* {color:green}Node1 have got actual viewChangeNo{color}  h6.CASE #3 *Step to reproduce:* # make some viewChanges # demote node1 with current viewNo 7 # Simultaneously stop indy-node on all nodes (exclude demoted node) # Simultaneously start indy-node on all nodes (exclude demoted node) # Promote node1  *Actual results:* {color:red}node1 cannot link with any viewChange from pool, until it reach #8 (more than current viewChange on node1). {color} Allocated into INDY-1199 {code} 2018-02-20 11:23:40,877 | DEBUG    | node.py              (1476) | sendToViewChanger | Node1 sending message to view changer: (INSTANCE_CHANGE{'viewNo': 2, 'reason': 28}, 'Node7') {code}   ></body> </Action>
