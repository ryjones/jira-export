<Action id="31662" issue="21337" author="mgbailey" type="comment" body=" ^icenode 3_log.txt  contains the logs from the steward that we attempted to onboard onto the live network.  It shows a similar problem when attempting to sync the domain ledger.  The logs say it is attempting to sync beginning at entry #17.  Entries 1-16 are contained in the genesis file, which were verified to be correct." created="2017-10-06 04:01:16.0" updateauthor="mgbailey" updated="2017-10-06 04:01:16.0"/>
<Action id="31666" issue="21337" author="ashcherbakov" type="comment" created="2017-10-06 09:06:35.0" updateauthor="ashcherbakov" updated="2017-10-06 09:06:35.0"> <body><! CDATA I think I got the issue: - old domain genesis txns contained NULLS (which were saved in domain ledger for first 10 nodes) - new domain genesis txns contain only non-Null values, and the Node11 has different ledger because of this.  We will think how to fix this better and provide a fix.  ></body> </Action>
<Action id="31668" issue="21337" author="ashcherbakov" type="comment" created="2017-10-06 09:30:55.0" updateauthor="ashcherbakov" updated="2017-10-06 09:30:55.0"> <body><! CDATA We're going to provide a migration for this (to get rid of NULL values in existing ledgers). The question is how to deliver it the best way: * Option1: create a new RC with just a hot fix for this migration * Option2: include this migration into the next RC   If this is a critical issue, then probably Option1 is better (we may have a delay with Option 2 since we have lots of quite risky features in the next RC).  regardless of Option, it needs to be tested properly.  ></body> </Action>
<Action id="31962" issue="21337" author="ashcherbakov" type="comment" created="2017-10-10 07:28:51.0" updateauthor="ashcherbakov" updated="2017-10-10 16:08:06.0"> <body><! CDATA New RC: 1.1.40 Migration script: https://github.com/hyperledger/indy-node/blob/stable/data/migrations/deb/helper_1_1_37_to_1_1_38.py  ></body> </Action>
<Action id="31975" issue="21337" author="ashcherbakov" type="comment" created="2017-10-10 15:59:04.0" updateauthor="ashcherbakov" updated="2017-10-10 16:09:05.0"> <body><! CDATA Changes after migration is applied: 1) All domain ledgers will be the same (no null values there) 2) If user's config (/home/sovrin/.sovrin/sovrin_config.py) contains {code}domainTransactionsFile = 'transactions_live'{code} (this is the case for live pool), then this line will be renamed to {code}domainTransactionsFile = 'domain_transactions_live'{code} It's needed because domain_ prefix was added to domain ledger files, so it's better to have all names equal on all Nodes. We're planning to deprecate modification of domainTransactionsFile in config (there will be network_name parameter instead to define a network (live, test., etc.)). 3) New Nodes should also have `domainTransactionsFile = 'domain_transactions_live'` in their config (otherwise they will not be able to see genesis file).  ></body> </Action>
<Action id="32005" issue="21337" author="vladimirwork" type="comment" created="2017-10-11 18:07:54.0" updateauthor="vladimirwork" updated="2017-10-11 18:07:54.0"> <body><! CDATA Steps to Reproduce:  1. Install 1.0.28 pool of 4 nodes and send some NYMs. 2. Upgrade it to 1.1.37 with force=True (and the whole pool at the same time) and send some NYMs. 3. Upgrade it to 1.1.40 with force=False and send some NYMs. 4. Add 5th (1.1.40) node and send some NYMs. 5. Add 6th (1.1.40) node and send some NYMs.  Actual results: When we add 5th node (1.1.40) to 1.1.40 pool (4 nodes) the 5th node catches up successfully, but doesn't participate in consensus (so other NYMs write in initial 4 nodes only) When we add 6th node (1.1.40) to that pool (5 nodes) the 6th node catches up succesfully, but pool falls in broken state (other NYMs don't write in any of 6 nodes). See debug logs for additional info.  ^_node1.txt    ^_node2.txt    ^_node3.txt    ^_node4.txt    ^_node5.txt    ^_node6.txt    Expected Results: Pool should work normally after both nodes adding.  *Workaround: Restart all 6 nodes in the pool.*  ></body> </Action>
<Action id="32050" issue="21337" author="vladimirwork" type="comment" created="2017-10-12 16:59:06.0" updateauthor="vladimirwork" updated="2017-10-13 09:38:05.0"> <body><! CDATA Steps to Reproduce - Case 2:  1. Install 1.0.28 pool of 4 nodes and send some NYMs. 2. Upgrade it to 1.1.37 with force=True (and the whole pool at the same time) and send some NYMs. 3. Add 5th (1.1.37, node-control-tool wasn't stopped before upgrade) node and send some NYMs. 4. Upgrade the whole pool (5 nodes) to 1.1.40 with force=True and send some NYMs.  Actual Results: Upgrade of 5th node is failed. 5th node is rolled back to 1.0.28 due to migration script's applying failure. See attachments for more info.  ^journal.txt   !migration_failure.PNG|thumbnail!   ></body> </Action>
<Action id="32104" issue="21337" author="vladimirwork" type="comment" created="2017-10-13 14:01:46.0" updateauthor="vladimirwork" updated="2017-10-17 12:25:14.0"> <body><! CDATA There are results of pool upgrade scenarios (1.1.40 version):  Scenario 1.1: install 1.0.28 pool of 4 nodes -> upgrade the pool with force=True to 1.1.37 -> add 1.1.37 5th node with genesis ledger -> upgrade the pool with force=False to 1.1.40 -> add 1.1.40 6th node  FAILED: 5th node is not upgraded and not catched up, 6th node's adding breaks the pool   Scenario 1.2: install 1.0.28 pool of 4 nodes -> upgrade the pool with force=True to 1.1.37 -> add 1.1.37 5th node with genesis ledger -> upgrade the pool with force=True to 1.1.40 -> add 1.1.40 6th node  PASSED: 5th node is upgraded and catched up, 6th node is catched up, pool works   Scenario 2: install 1.0.28 pool of 4 nodes -> upgrade the pool with force=True to 1.1.37 -> add 1.1.37 5th node with genesis ledger -> -> upgrade the pool with force=True to 1.1.40 and with 5th node turned off after the upgrade txn is scheduled -> add 1.1.40 6th node  UNCLEAR: 5th node is not upgraded but catched up, 6th node is catched up, pool works   Scenario 3: install 1.0.28 pool of 4 nodes -> upgrade the pool with force=True to 1.1.37 -> add 1.1.37 5th node with genesis ledger -> shut down the pool and manually upgrade each node to 1.1.41 -> add 1.1.41 6th node  UNCLEAR: 5th node is catched up after the manual upgrade, 6th node is catched up, pool works, but initial 4 nodes downgrade back to 1.1.37 (INDY-869)   Scenario 4: install 1.0.28 pool of 4 nodes -> upgrade the pool with force=True to 1.1.37 -> add 1.1.37 5th node with copied ledger -> upgrade the pool with force=False to 1.1.40 -> add 1.1.40 6th node  FAILED: 5th node is upgraded and catched up, there are duplicated entries in ledger after the 1.1.40 pool upgrade, 6th node is catched up after adding but is not in consensus with other 5 nodes   ></body> </Action>
<Action id="32105" issue="21337" author="ashcherbakov" type="comment" body="The problem mentioned in https://jira.hyperledger.org/browse/INDY-895?focusedCommentId=32050&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-32050 is fixed in RC 1.1.41" created="2017-10-13 14:29:36.0" updateauthor="ashcherbakov" updated="2017-10-13 14:30:59.0"/>
<Action id="32501" issue="21337" author="vladimirwork" type="comment" body="New found issue is reported as INDY-908." created="2017-10-16 09:43:30.0" updateauthor="vladimirwork" updated="2017-10-16 09:43:30.0"/>
<Action id="32558" issue="21337" author="vladimirwork" type="comment" created="2017-10-17 12:36:51.0" updateauthor="vladimirwork" updated="2017-10-17 12:36:51.0"> <body><! CDATA Build Info: indy-node 1.1.41  Steps to Validate: 1. Install 1.0.28 pool of 4 nodes. 2. Upgrade the pool with force=True to 1.1.37. 3. Add 1.1.37 5th node with genesis ledger. 4. Upgrade the pool with force=True to 1.1.40. 5. Add 1.1.40 6th node.  Actual Results: 5th node is upgraded and catched up, 6th node is catched up and reached consensus, pool works normally.  Addititonal Info: More info about all tests run is in https://jira.hyperledger.org/browse/INDY-895?focusedCommentId=32104&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-32104. All related issues found during this ticket confirmation testing are reported in INDY-869, INDY-908, INDY-909.  ></body> </Action>
<Action id="32706" issue="21337" author="vladimirwork" type="comment" created="2017-10-19 14:36:59.0" updateauthor="vladimirwork" updated="2017-10-19 14:37:50.0"> <body><! CDATA Build Info: indy-node 1.1.43  Steps to Validate - Case 2: 1. Install 1.1.37 pool of 4 nodes. 2. Add schema to ledger. 3. Add claim definition using schema from Step 2. 4. Upgrade the pool (with force=True or force=False) to 1.1.43.  Actual Results: Migration script 1.1.37->1.1.38 is applied succesfully. All ledger entries are converted normally during the upgrade.  Additional Info: There was an issue with the migration of schema and claim def ledger's entries in 1.1.42:  {quote} Oct 19 09:21:01 005c42ac09ef env 68 : Traceback (most recent call last): Oct 19 09:21:01 005c42ac09ef env 68 :   File "/usr/local/lib/python3.5/dist-packages/data/migrations/deb/helper_1_1_37_to_1_1_38.py", line 202, in <module> Oct 19 09:21:01 005c42ac09ef env 68 :     migrate_all() Oct 19 09:21:01 005c42ac09ef env 68 :   File "/usr/local/lib/python3.5/dist-packages/data/migrations/deb/helper_1_1_37_to_1_1_38.py", line 195, in migrate_all Oct 19 09:21:01 005c42ac09ef env 68 :     migrate_domain_ledger_for_node(node_data_dir) Oct 19 09:21:01 005c42ac09ef env 68 :   File "/usr/local/lib/python3.5/dist-packages/data/migrations/deb/helper_1_1_37_to_1_1_38.py", line 137, in migrate_domain_ledger_for_node Oct 19 09:21:01 005c42ac09ef env 68 :     new_name) Oct 19 09:21:01 005c42ac09ef env 68 :   File "/usr/local/lib/python3.5/dist-packages/data/migrations/deb/helper_1_1_37_to_1_1_38.py", line 49, in __migrate_ledger Oct 19 09:21:01 005c42ac09ef env 68 :     txn DATA  = json.loads(txn DATA ) Oct 19 09:21:01 005c42ac09ef env 68 :   File "/usr/lib/python3.5/json/__init__.py", line 312, in loads Oct 19 09:21:01 005c42ac09ef env 68 :     s.__class__.__name__)) Oct 19 09:21:01 005c42ac09ef env 68 : TypeError: the JSON object must be str, not 'OrderedDict' Oct 19 09:21:01 005c42ac09ef su 1183 : pam_unix(su:session): session closed for user sovrin Oct 19 09:21:01 005c42ac09ef env 68 : 2017-10-19 09:21:01,675 | ERROR    | 1_1_37_to_1_1_38.py  (28) | <module> | Migration failed: script returned 1 Oct 19 09:21:01 005c42ac09ef env 68 : Traceback (most recent call last): Oct 19 09:21:01 005c42ac09ef env 68 :   File "/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_1_37_to_1_1_38.py", line 29, in <module> Oct 19 09:21:01 005c42ac09ef env 68 :     raise Exception(msg) Oct 19 09:21:01 005c42ac09ef env 68 : Exception: Migration failed: script returned 1 {quote}  ></body> </Action>
<Action id="33016" issue="21337" author="danielhardman" type="comment" body="So if we had an issue with 1.1.42, what should be done about it? Are we ignoring it because nobody will have 1.1.42 deployed? Or does it need some attention before we close the ticket?" created="2017-10-23 20:30:36.0" updateauthor="danielhardman" updated="2017-10-23 20:30:36.0"/>
<Action id="33100" issue="21337" author="vladimirwork" type="comment" body="This issue with 1.1.42 doesn&apos;t need any additional attention because our release candidate to stable is 1.1.43. This information was added just for explanation why the additional test case was used." created="2017-10-24 07:15:27.0" updateauthor="vladimirwork" updated="2017-10-24 07:15:27.0"/>
