<Action id="65804" issue="43475" author="sergey.khoroshavin" type="comment" created="2019-11-27 12:54:45.0" updateauthor="sergey.khoroshavin" updated="2019-11-27 13:42:37.0"> <body><! CDATA Thanks for raising this.   Indy node has config for IP address it binds to (located at /etc/indy/indy.env), which could be different from public IP address announced to other nodes through NODE transaction in pool ledger.  If my understanding is correct you want to be able to route traffic from a single public IP to some kubernetes pod running indy node which has arbitrary internal IP in some subnet, and that pod can be recreated (which would change its IP) at any time. In this case you have several options: * just bind node to 0.0.0.0 - it will bind to all available interfaces and IP addresses (simplest, but probably not secure enough in some environments) * when provisioning pod run some initial configuration script which writes correct internal IP to /etc/indy/indy.env * modify indy node so that an IP range can be provided in /etc/indy/indy.env - you can look at INDY-1332 for some reference where to start implementation  However, if you want node to have several public IP addresses - this is a whole different story.  ></body> </Action>
<Action id="66059" issue="43475" author="smithbk" type="comment" created="2019-12-04 15:46:59.0" updateauthor="smithbk" updated="2019-12-07 13:36:04.0"> <body><! CDATA Thanks Sergey, but let me clarify.  In our kubernetes cloud environment, our validator is always available at some fixed IP.  For example, say it is 1.2.3.4.   There is no problem there.  The problem is that when our validator makes a TCP connection to another validator, the source IP address that it uses is not 1.2.3.4.  In our case, it could be any of two IP addresses, for example: 1.2.3.5 or 1.2.3.6, depending on where the validator happens to be running at the time.  The problem is then that the other validator will block our inbound TCP connection because the source IP is not 1.2.3.4.  Therefore, I would like to add additional metadata which appears in the pool_transactions_genesis file.  For example,   {code} "node_ip": "1.2.3.4", "node_ip_outbound":  "1.2.3.5","1.2.3.6" , {code}   I'm not suggesting that the validator code needs to change to enforce this by checking the source IP address for inbound connections; however, this data can be used to configure firewalls appropriately.  Does this use case make sense now?  ></body> </Action>
<Action id="66169" issue="43475" author="sergey.khoroshavin" type="comment" created="2019-12-09 11:33:59.0" updateauthor="sergey.khoroshavin" updated="2019-12-09 11:33:59.0"> <body><! CDATA Thanks for clarifying. Probably it's my lack of production experience with kubernetes, but I'm still not sure whether I understand correctly how your case works. I'll try to describe it here, please correct me if I'm wrong: * you have a kubernetes cluster with public IPs assigned to each node * you have a pod running indy ode, probably managed by StatefulSet * you have a Service bound to some external IP, which routes all traffic to indy node pod * inside pod with Indy Node you have only one network interface bound to current kubernetes node, and all outbound traffic goes unmodified * there is no solution (like SNAT) to make traffic going from nodes appear like it goes from service external IP If this is the case - then yes, NODE transaction should be modified to include outbound node IPs. However this looks a bit hacky, that's why I wasn't considering this scenario in the beginning.   ></body> </Action>
<Action id="66210" issue="43475" author="smithbk" type="comment" created="2019-12-10 17:53:53.0" updateauthor="smithbk" updated="2019-12-10 17:53:53.0"> <body><! CDATA  ~sergey.khoroshavin  Yes, your understanding is correct.  Also, according to the IBM Cloud experts I've been talking to in order to find a solution. they do not know of any cloud offerings which offer SNAT for outbound connections, or at least not out of the box.  It is definitely not part of kubernetes.  I agree that it would be nice and cleaner if cloud solutions provided this, but this use case is quite unique since the majority of cloud apps deal only with hostnames and even fewer would be concerned with the outbound source IP address.  That said, can you point me in the right direction for where to add this metadata?  Thanks  ></body> </Action>
<Action id="66229" issue="43475" author="sergey.khoroshavin" type="comment" created="2019-12-11 16:48:49.0" updateauthor="sergey.khoroshavin" updated="2019-12-11 16:48:49.0"> <body><! CDATA  ~smithbk  Node transactions (those which contain information about nodes) are handled here: https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/request_handlers/node_handler.py . Main entry points are *static_validation* (stateless check that request is properly formed), *dynamic_validation* (stateful check that request is valid, can be based on data already written to ledger) and *update_state* (update merkelized key-value store which is used for fast secure lookups). However it looks like nothing needs to be changed in this part of code. Also we have transaction schema validation, schemas which are relevant for this task are defined here: https://github.com/hyperledger/indy-plenum/blob/master/plenum/common/messages/client_request.py   In my opinion minimal implementation would be: * update ClientNodeOperationData schema to include optional field "node_ip_outbound", and for you use cases it looks like it should be either network subnet or list of network subnets, not individual IPs * update schema tests to match updated scheme * write integration test which sends NODE transaction with "node_ip_outbound" key added and check that it is written into ledger (it could be added here https://github.com/hyperledger/indy-plenum/tree/master/plenum/test/pool_transactions, with other tests used as a somewhat reference) * update documentation so this new key becomes part of public API and other stewards can rely on it when setting up their whitelisting rules  I think it makes sense to raise WIP PR as soon as you start working on it so that: * if you accidentally break any tests you get early notification * more people become aware of this change as soon as possible and could do preliminary review  ></body> </Action>
<Action id="66230" issue="43475" author="anikitindsr" type="comment" created="2019-12-11 17:30:41.0" updateauthor="anikitindsr" updated="2019-12-11 17:30:41.0"> <body><! CDATA  ~smithbk  If I understood correctly, the situation is (for example): # NodeA has several instances, on 3 different machines, on '1.2.3.4', '1.2.3.5' and '1.2.3.6'. If something goes wrong, you can automatically switch to another instance, '1.2.3.5' for example. Address for outbound connections will keep the same, '1.2.3.4', but inbound connections will be changed to '1.2.3.5'. Does it mean that all other nodes (NodeB, NodeC, etc) should change those connections to other one, '1.2.3.5' ? # We use ZMQ now and we expect, that all of connection-like problems will be resolved by ZMQ. I mean, if we have network problem, unplugged cable or something like that, ZMQ resolves all this problem, all of messages will be delivered/resent after reconnection. As I understood, if instance '1.2.3.4' will be broken then we need to ask ZMQ to reconnect to another address, '1.2.3.5', and resend all packages to other one. # Does this instance changing affect client side too? I mean, do we need keep the same logic on the client side?  ></body> </Action>
<Action id="66233" issue="43475" author="esplinr" type="comment" created="2019-12-11 18:08:19.0" updateauthor="esplinr" updated="2019-12-11 18:15:45.0"> <body><! CDATA The suggested change appears odd because it adds an additional field to the definition of a node transaction on the pool ledger, and that field is not actually used by Indy Node. However, I think it is appropriate because the pool ledger exists to communicate node information to all the validators in the pool, and we have explicitly decided that managing the whitelist of validator nodes should be managed outside of Indy Node by the stewards firewall solution.  Discussing this with the team, it seems like the ideal solution would be to implement a type of service discovery mechanism so that validators can change their IP addresses without a network transaction, but that is a bigger set of changes than can be completed in this ticket. A service discovery mechanism could also help with dynamic node selection, reducing the size of genesis files, and the catchup mechanism for validators and clients.  ></body> </Action>
<Action id="66239" issue="43475" author="esplinr" type="comment" created="2019-12-11 23:56:20.0" updateauthor="esplinr" updated="2019-12-11 23:57:32.0"> <body><! CDATA  ~lbendixsen  says that in the last couple of months, at least three Sovrin stewards were unreachable from other validator nodes due to them using a different IP address and port number for inbound and outbound connections. These stewards say this is a common thing when hosting behind corporate firewalls. They suggest that we explicitly list an inbound and outbound IP address. They also requested that we have an automated way for Indy Node to update this information.  The list proposed by Keith probably meets the same need with more flexibility. We probably need to also allow for a list of port numbers in addition IP addresses.  ></body> </Action>
<Action id="66256" issue="43475" author="esplinr" type="comment" created="2019-12-12 23:00:53.0" updateauthor="esplinr" updated="2019-12-12 23:02:41.0"> <body><! CDATA I spoke with  ~danielhardman .  He thinks that we should have different IP addresses for ingress and egress.  However, he is very concerned about the idea of a range of IP addresses, where the validator node can dynamically select its IP address. Our consensus algorithm assumes that validator nodes will have a stable IP address. * If a node does not respond to a message because the message was sent to the wrong IP address, its reputation will suffer. This will prompt unnecessary requests for View Changes. * When a node changes its IP address, not all of the network will recognize the need to communicate on the new IP address at the same time. The transition from the old IP address to the new IP address makes the network non-deterministic and can easily cause a partition in the network that exceeds our threshold of F. * Errors caused by nodes communicating on the wrong IP address are very difficult to diagnose (as Lynn discovered recently). * Kubernetes can select any internal IP address, so long as the administrator of the validator node runs a firewall or load balancer in front of the machine to give it a stable IP address.  Daniel also pointed out that this would be a good time to make the port designation explicit instead of allowing ZMQ to use any port within a range of 100. This is a common request from administrators who want to reduce the number of open ports on their firewalls.  ></body> </Action>
<Action id="66263" issue="43475" author="esplinr" type="comment" created="2019-12-13 15:58:29.0" updateauthor="esplinr" updated="2019-12-13 15:58:29.0"> <body><! CDATA Sergey pointed out that I misunderstood part of this issue.  The inbound IP address needs to be reliable to preserve consensus. It is not as dangerous to consensus if the outbound IP address changes dynamically.  ></body> </Action>
<Action id="66277" issue="43475" author="esplinr" type="comment" created="2019-12-16 17:06:38.0" updateauthor="esplinr" updated="2019-12-17 14:41:39.0"> <body><! CDATA We discussed this on the Indy Contributors call. * Allowing a large range of IP address could become an attack vector; any of those IP addresses could be used to disrupt validation. * A small range would probably be appropriate, but that might not help with the Kubernetes use case. * Could we limit it to a 32 or 64 subnet?  ></body> </Action>
