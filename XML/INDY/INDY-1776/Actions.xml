<Action id="52725" issue="34877" author="sergey-shilov" type="comment" created="2018-10-29 07:54:59.0" updateauthor="sergey-shilov" updated="2018-10-29 07:54:59.0"> <body><! CDATA We did the test with listener recreation in case of exceeded memory usage threshold.  *Test set up*  We used ZMQ-based sample server written in Python. As an RSS threshold we specified 500MB, overall clients throughput exceeded server's throughput (40 reqs/seq vs 25 reqs/sec). The listener socket is recreated when RSS threshold exceeded, but not often than once per 60 sec.  *Results*  When the RSS threshold was reached the listener socket was recreated the first time. Memory was not released. All further test under load (which lasted about three days) the listener socket was recreated each 60 seconds due to reached threshold, memory was not released but did not grow at the same time and was pretty stable: ~500MB (RSS) and ~1GB (VSZ).  *Conclusion*  The test showed that there is no sense to recreate listener socket to solve memory problems as such strategy leads to cyclic listener restart without memory releasing. We already have the logic to restart listener stack to avoid file descriptors exhaustion, but seems like this is not the solution for memory issues.  Also I think that this test covers the first described test in Acceptance criteria options (Python test with restart on each 1000 connections), but instead of connections the threshold is set for memory usage that makes the test more reasonable.  ></body> </Action>
<Action id="52829" issue="34877" author="sergey-shilov" type="comment" created="2018-10-31 16:29:17.0" updateauthor="sergey-shilov" updated="2018-10-31 16:29:17.0"> <body><! CDATA *Test set up*  The same test was done for the equivalent implementation in C. Test set up is the same as described by the comment (the difference is that ZMQ-based sample server written in C): https://jira.hyperledger.org/browse/INDY-1776?focusedCommentId=52725&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-52725  *Results*  Each time when 500MB RSS was reached the listener socket was recreated. Each time this event occurred the RSS fault down to 180-300 MB.  *Conclusion*  Described two tests with Python-based and C-based server implementations shown very different results. Seems like the solution with restarting of listener socket based on RSS threshold would be very good for C implementation where there is no any additional memory management. But since we have Python-based implementation this solution is useless as RSS doesn't go down under threshold and we always have the "red flag" once the threshold is reached.  ></body> </Action>
<Action id="52981" issue="34877" author="sergey-shilov" type="comment" created="2018-11-02 16:11:58.0" updateauthor="sergey-shilov" updated="2018-11-06 09:02:25.0"> <body><! CDATA We ran two indy load tests: 10 NYMs per sec and 100 NYMs per sec (MAX_CONNECTED_CLIENTS_NUM = 100, without iptables).  *Results* * *10 NYMs per sec:* we did not reach connections limit and all transactions were written * *100 NYMs per sec:* connections limit was reached, the client stack was restarted several times, memory grew and the node process was killed by OOM killer. During whole test we read maximum allowed size of batches per looper iteration from node-to-node queues, so it seems like so high memory consumption is caused by very big node-to-node traffic  ></body> </Action>
<Action id="53081" issue="34877" author="sergey-shilov" type="comment" created="2018-11-06 15:03:15.0" updateauthor="sergey-shilov" updated="2018-11-06 15:03:15.0"> <body><! CDATA *Pictures for 10 NYMs/sec and 100 NYMs/sec* !10_nyms_sec_without_iptables.png|thumbnail!  !100_nyms_sec_without_iptables.png|thumbnail!   ></body> </Action>
<Action id="53084" issue="34877" author="sergey-shilov" type="comment" created="2018-11-06 15:22:22.0" updateauthor="sergey-shilov" updated="2018-11-06 15:22:22.0"> <body><! CDATA *Conclusion*  Our investigation shows that client stack restart without limitation of number of client connections by firewall (iptables) does not help to avoid OOM during DDoS scenarios like 100 NYMs/sec. High load with restricted number of clients connections using iptables will be done in scope of INDY-1770.  ></body> </Action>
