<Action id="32937" issue="20823" author="andkononykhin" type="comment" body="Created a PR:Â https://github.com/hyperledger/indy-plenum/pull/422" created="2017-10-20 16:55:29.0" updateauthor="andkononykhin" updated="2017-10-20 16:55:29.0"/>
<Action id="32975" issue="20823" author="andkononykhin" type="comment" created="2017-10-23 10:36:56.0" updateauthor="andkononykhin" updated="2017-10-23 10:36:56.0"> <body><! CDATA Problem reason: - when node is promoted after demotion other nodes haven't updated parameters in memory. Consequences: non-sync pool ledger and memory, (possible) non-sync pool parameters between different nodes (e.g. after one other node is restarted after demoted node promotion getting pool parameters directly from pool ledger)  Changes: - triggering pool parameters update not only for newly added/promoted node but also for the one which was previously demoted - logging improvements  Committed into: -  https://github.com/hyperledger/indy-plenum/pull/422   Risk factors: - Nothing is expected.  Risk: - Low  Covered with tests: - {{plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py}}  Recommendations for QA: do the following sequence of steps * start pool of 4 nodes * promote new one * demote and promote it again * restart one of the nodes from the original pool (after restart it will get pool parameters from pool ledger file) * initiate force view change at least 3 times and ensure from logs that all nodes choose the same nodes as primaries for all protocol instances: after 3 times - 5th node should be primary for backup instance, after 4 times - 5th is primary for master one  ></body> </Action>
<Action id="33159" issue="20823" author="vladimirwork" type="comment" created="2017-10-25 13:38:26.0" updateauthor="vladimirwork" updated="2017-10-25 13:38:26.0"> <body><! CDATA Build Info: indy-node 1.2.181  Steps to Validate - Case 1: 1. Start pool of 4 nodes. 2. Promote new one. 3. Demote and promote it again. 4. Restart one of the nodes from the original pool (after restart it will get pool parameters from pool ledger file). 5. Initiate force view change at least 3 times and ensure from logs that all nodes choose the same nodes as primaries for all protocol instances.  Steps to Validate - Case 2: 1. Set up the pool of 6 nodes. 2. Run load test with following parameters on two clients: python3 load_test.py -t NYM -c 20 -r 100 --at-once 3. Look at nodes logs.  Actual Results: Primaries at all nodes are the same after each view change. There are no missing view changes at the primaries. Primary node works normally after each view change. !view_changes_node_1.PNG|thumbnail!  !view_changes_node_4.PNG|thumbnail!  !view_changes_under_load.PNG|thumbnail!   ></body> </Action>
