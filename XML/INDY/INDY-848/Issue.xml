<Issue id="20823" key="INDY-848" number="848" project="10303" reporter="ozheregelya" assignee="nage" creator="ozheregelya" type="10004" summary="Primary node is broken after view change" priority="2" resolution="10000" status="10001" created="2017-09-14 12:34:47.0" updated="2019-03-29 20:34:57.0" resolutiondate="2019-03-29 20:34:57.0" votes="0" watches="3" workflowId="20828"> <environment><! CDATA Build Info:   indy-node 1.1.37   indy-anoncreds 1.1.10   indy-plenum 1.1.27   sovrin 1.1.6 OS/Platform: Ubuntu 16.04.2 LTS Setup: 6 nodes, 1 client  ></environment> <description><! CDATA The problem reproduces not stable. Steps to reproduce are unclear. Here is possible procedure: 1. Set up the pool of 6 nodes. 2. Run load test with following parameters on two clients: python3 load_test.py -t NYM -c 20 -r 100 --at-once 3. Look at nodes logs.  Actual Results: Node1 (which was primary) missed several view changes, it also doesn't processing any transactions. Following information is shown in logs of Node1: {code:java} 2017-09-14 11:19:08,826 | INFO | node.py (1967) | _start_view_change_if_possible | VIEW CHANGE: Node1 starting view change for 31 after 2 view change indications from other nodes 2017-09-14 11:19:08,827 | INFO | node.py (2159) | startViewChange | VIEW CHANGE: Node1 changed to view 31, will start catchup now 2017-09-14 11:19:08,830 | ERROR | primary_selector.py ( 167) | _verify_primary | PRIMARY SELECTION: Node1 expected next primary to be Node2, but majority declared Node3 instead for view 31 2017-09-14 11:19:08,831 | ERROR | primary_selector.py ( 167) | _verify_primary | PRIMARY SELECTION: Node1 expected next primary to be Node2, but majority declared Node3 instead for view 31 2017-09-14 11:19:08,831 | ERROR | primary_selector.py ( 167) | _verify_primary | PRIMARY SELECTION: Node1 expected next primary to be Node2, but majority declared Node3 instead for view 31 2017-09-14 11:19:08,849 | INFO | node.py (1489) | preLedgerCatchUp | Node1 reverted 0 batches before starting catch up for ledger 0 2017-09-14 11:19:08,857 | ERROR | primary_selector.py ( 167) | _verify_primary | PRIMARY SELECTION: Node1 expected next primary to be Node2, but majority declared Node3 instead for view 31 2017-09-14 11:19:08,875 | INFO | node.py (1489) | preLedgerCatchUp | Node1 reverted 0 batches before starting catch up for ledger 2 2017-09-14 11:19:08,877 | INFO | upgrader.py ( 150) | should_notify_about_upgrade_result | Node's 'Node1' last upgrade txn is None 2017-09-14 11:19:08,877 | INFO | ledger_manager.py ( 848) | catchupCompleted | CATCH-UP: Node1 completed catching up ledger 2, caught up 0 in total 2017-09-14 11:19:08,877 | INFO | ledger_manager.py ( 848) | catchupCompleted | CATCH-UP: Node1 completed catching up ledger 0, caught up 1 in total 2017-09-14 11:19:08,899 | INFO | node.py (1489) | preLedgerCatchUp | Node1 reverted 0 batches before starting catch up for ledger 1 2017-09-14 11:19:08,926 | INFO | ledger_manager.py ( 848) | catchupCompleted | CATCH-UP: Node1 completed catching up ledger 1, caught up 3 in total 2017-09-14 11:19:08,927 | INFO | node.py (1525) | allLedgersCaughtUp | CATCH-UP: Node1 caught up till (31, 4) 2017-09-14 11:19:08,927 | INFO | node.py (1537) | allLedgersCaughtUp | CATCH-UP: Node1 does not need any more catchups 2017-09-14 11:19:08,928 | ERROR | primary_selector.py ( 167) | _verify_primary | PRIMARY SELECTION: Node1 expected next primary to be Node2, but majority declared Node3 instead for view 31 2017-09-14 11:20:08,831 | INFO | node.py (1019) | _check_view_change_completed | view change to view 31 is not completed in time, starting view change for view 32 2017-09-14 11:20:08,831 | INFO | node.py (1021) | _check_view_change_completed | VIEW CHANGE: Node1 initiating a view change to 32 from 31 2017-09-14 11:20:08,831 | INFO | node.py (2048) | sendInstanceChange | VIEW CHANGE: Node1 sending an instance change with view_no 32 since View change could not complete in time 2017-09-14 11:23:46,913 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388226898996) from client I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6 2017-09-14 11:23:47,916 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388227905354) from client b'I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6' 2017-09-14 11:23:48,921 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388228909014) from client I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6 2017-09-14 11:23:49,925 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388229912259) from client I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6 2017-09-14 11:23:50,929 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388230917988) from client b'I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6' 2017-09-14 11:23:51,940 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388231921568) from client I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6 2017-09-14 11:23:52,939 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388232925218) from client I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6 2017-09-14 11:23:53,931 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388233929200) from client b'I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6' 2017-09-14 11:23:54,951 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388234934584) from client I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6 2017-09-14 11:23:55,948 | INFO | propagator.py ( 148) | propagate | Node1 propagating request ('Th7MpTaRZVRYnPiabds81Y', 1505388235938861) from client b'I}w?$na9^IYnNXu-w!$nejVFV@w*.KQ#*M%-LZh6' 2017-09-14 11:23:55,954 | WARNING | replica.py ( 722) | readyFor3PC | Node1:0 is getting requests but still does not have a primary so the replica will not process the request until a primary is chosen 2017-09-14 11:23:55,954 | WARNING | replica.py ( 722) | readyFor3PC | Node1:1 is getting requests but still does not have a primary so the replica will not process the request until a primary is chosen{code} Expected Results: Old primary should work without problems.  Additional Information: Several nodes (including Node1) were restarted in the end of load tests.  ></description> </Issue>
