<Action id="27456" issue="18682" author="stevetolman" type="comment" body="Please time box this by spending an hour to review the logs and study the code to see if you can figure out how this is happening. After an hour, report your findings here and assign this ticket to Kelly." created="2017-06-27 21:01:58.0" updateauthor="stevetolman" updated="2017-06-27 21:01:58.0"/>
<Action id="27525" issue="18682" author="mzk-vct" type="comment" created="2017-06-28 16:16:50.0" updateauthor="mzk-vct" updated="2017-06-28 16:27:44.0"> <body><! CDATA  ~stevetolman   ~krw910  I run add_keys multiple times and also send requests manually, using CLI, but failed to reproduce it on latest builds. Since there is one hour limitation, I'm stopping investigation.   ></body> </Action>
<Action id="27527" issue="18682" author="mzk-vct" type="comment" body="By the way, seed for *EK6VWvsoNSVwD3SaVGw68N5sA7aRbepTAwRcNMKQXdYc* is *000000000000000000000000Steward1*, which in the first on the *load_test_clients.list* list, so it could mean that this error happened on the start." created="2017-06-28 16:20:58.0" updateauthor="mzk-vct" updated="2017-06-28 16:21:11.0"/>
<Action id="27532" issue="18682" author="mzk-vct" type="comment" created="2017-06-28 16:27:12.0" updateauthor="mzk-vct" updated="2017-06-28 16:27:12.0"> <body><! CDATA There would be great to know whether there were not only *REQACK*s, but also *REPLY*s from asobiC. The reason can be in fact that client tries to resend request if node does not replies after timeout.  Another reason can be in a way stp or zeromq manages message delivery.   ></body> </Action>
<Action id="27533" issue="18682" author="mzk-vct" type="comment" body=" ~krw910  I wrote my thoughts about possible reason, but they are only guesses, because problem was not reproduced" created="2017-06-28 16:28:46.0" updateauthor="mzk-vct" updated="2017-06-28 16:28:46.0"/>
<Action id="27618" issue="18682" author="krw910" type="comment" body="I have not reproduced this either. If it happens again during load testing of INDY-13 I will capture the logs and stop tests until someone can take a look at the issue." created="2017-06-29 14:59:35.0" updateauthor="krw910" updated="2017-06-29 14:59:35.0"/>
<Action id="27661" issue="18682" author="danielhardman" type="comment" body="Given that we can&apos;t dup this right now, I think we should bookmark the issue and wait to see if it surfaces again." created="2017-06-29 20:42:57.0" updateauthor="danielhardman" updated="2017-06-29 20:42:57.0"/>
<Action id="28122" issue="18682" author="mgbailey" type="comment" body="I am seeing this behavior in more places.  While doing tests with the traffic generator over the weekend, this problem manifested in two nodes in the alpha network (OASFCU and probably DustStorm).  These are ESX nodes where we have seen disconnect / connect patterns like we have seen in metis.  In addition, we have now also seen this in an AWS node in the ESN network, masakmasak.  In all these cases, it appears that there is a disconnect/reconnect event, after which transactions are no longer written to the ledger on that node.  I am attaching masakmasak logs to this ticket. At 18:52:40 the sovrin process seems to restart on its own volition.  From that point on, no further transactions are written to the ledger on that node.  ^masakmasak.log2.tgz  While the mysterious process restart is bod, the failure to catchup and synchronize with the pool is worse.   ~danielhardman ,  ~stevetolman , we need to reopen this.  It is happening too often, and will bite us." created="2017-07-11 01:36:26.0" updateauthor="mgbailey" updated="2017-07-11 01:37:08.0"/>
<Action id="28605" issue="18682" author="krw910" type="comment" body="This appears to be working just fine. We will need to keep an eye on it with the alpha pool." created="2017-07-18 19:48:19.0" updateauthor="krw910" updated="2017-07-18 19:48:19.0"/>
<Action id="28608" issue="18682" author="danielhardman" type="comment" body="I don&apos;t think I can mark this &quot;Done&quot;, since we never duplicated it. I am going to put it back in the backlog; if we encounter it again soon, we can begin working on it again; otherwise, we will eventually stumble across it and decide it&apos;s no longer an issue." created="2017-07-18 20:16:50.0" updateauthor="danielhardman" updated="2017-07-18 20:16:50.0"/>
<Action id="29392" issue="18682" author="ashcherbakov" type="comment" body=" ~danielhardman   ~nage  What should we do with this task?" created="2017-08-03 09:57:54.0" updateauthor="ashcherbakov" updated="2017-08-03 09:57:54.0"/>
