<Action id="42570" issue="29177" author="vladimirwork" type="comment" created="2018-04-05 14:30:05.0" updateauthor="vladimirwork" updated="2018-04-05 14:30:05.0"> <body><! CDATA  ~dsurnin  https://drive.google.com/drive/u/1/folders/1nCSqWRMI-TtfcGMSf6fZIRZzJmXLKXSy  Debug logs from 1, 11, 24 nodes.  ></body> </Action>
<Action id="42853" issue="29177" author="vladimirwork" type="comment" body="Now we have the same issue at 2000 clients (4 machines x 500 clients x 10 NYMs each) on 1.3.365 master." created="2018-04-12 11:35:05.0" updateauthor="vladimirwork" updated="2018-04-12 11:35:05.0"/>
<Action id="42898" issue="29177" author="sergey.khoroshavin" type="comment" body="I&apos;ve investigated logs from last session (2000 clients on 1.3.365 master) and found that nodes started having incorrect state trie. Also, before pool stopped working min/avg/max time to order (for now gathered only from 25th node) was ~500/600/700 seconds. Unfortunately due to rotation logs were not captured from the very start, we are going to restart load test from scratch in order to fix this." created="2018-04-13 08:47:56.0" updateauthor="sergey.khoroshavin" updated="2018-04-13 08:47:56.0"/>
<Action id="43206" issue="29177" author="sergey.khoroshavin" type="comment" created="2018-04-20 12:48:44.0" updateauthor="sergey.khoroshavin" updated="2018-04-20 12:48:44.0"> <body><! CDATA Incorrect state trie case was reproduced again, now with logs from the very start. Further investigation showed that all incorrect state trie events happened during view change, and after disabling view change test pool (created from scratch) was able to write (from 2000 clients) 10k+ 20k + 20k + 50k + 20k = 120k transactions without performance degradation.  Also after thoroughly looking at one of incorrect state trie events it was found that: 1) all nodes managed to get prepare quorum for (32, 14) before view change started 2) some nodes managed to get commit quorum for (32, 14) before finishing catchup, and some not 3) nodes that didn't order (32, 14) received some consistency proofs containing (32, 14) and apparently this is the reason they couldn't finish catchup 4) now nodes that didn't finish catchup started ordering new transaction (33, 1) based on (32, 13), and nodes that did finish catchup stopped ordering transactions at all  Now I'm trying to figure out what can be done to avoid this situation.  ></body> </Action>
<Action id="43430" issue="29177" author="sergey.khoroshavin" type="comment" created="2018-04-25 17:34:20.0" updateauthor="sergey.khoroshavin" updated="2018-04-25 17:34:20.0"> <body><! CDATA Full logs for incorrect state trie case: https://drive.google.com/file/d/1QW7VG2PIbArwVhcmHmc1qNCKIGa91Llp  Would be great if  ~lovesh  could take a look at them.  ></body> </Action>
<Action id="43971" issue="29177" author="ashcherbakov" type="comment" created="2018-05-07 07:10:23.0" updateauthor="ashcherbakov" updated="2018-05-07 07:10:23.0"> <body><! CDATA The found cause of the issue is that view-change was started in a different time (a lag about 40 seconds). The nodes that started view change earlier were not able to get COMMITs for last prepared certificate because of View Change timeout (60 seconds, or 5 catch-ups with 0 txns caught-up). Some nodes (that started later) were able to get COMMITs for last prepared certificate, so ended view change with a different state.  Increasing timeout didn't help.  ></body> </Action>
<Action id="44034" issue="29177" author="lovesh" type="comment" created="2018-05-08 07:23:41.0" updateauthor="lovesh" updated="2018-05-08 07:23:41.0"> <body><! CDATA  ~sergey.khoroshavin   bq. some nodes managed to get commit quorum for (32, 14) before finishing catchup, and some not How many?   ></body> </Action>
<Action id="44060" issue="29177" author="sergey.khoroshavin" type="comment" created="2018-05-08 14:34:15.0" updateauthor="sergey.khoroshavin" updated="2018-05-08 14:34:15.0"> <body><! CDATA  ~lovesh  {quote}How many?{quote} 6:  09:09:15 Node6 09:09:21 Node21 09:09:27 Node15 09:09:31 Node17 09:11:14 Node12 09:15:43 Node20   ></body> </Action>
<Action id="44063" issue="29177" author="ashcherbakov" type="comment" created="2018-05-08 14:44:40.0" updateauthor="ashcherbakov" updated="2018-05-08 14:44:40.0"> <body><! CDATA Our current thinking about ViewChange problems: 1) The current approach has the assumption that >f nodes will eventually commit their prepared certificates. However, it's possible only if there is no big crashes.  2) Another problem with the current protocol is that it relies on some timestamp, that is eventually the prepared certificates will be committed or caught up. However, this is not very deterministic.   Also there are probably some issues with implementation of the current View Change approach: 1) Whether we give enough possibility for prepared certificates to be ordered (gather COMMITs) before/between rounds of catch-up that we are doing. In particular, we start catch-up immediately, and it may revert some state (needed for committing prepared certificates).  2) Whether we have correct timeouts 3) Whether we should separate New Primary Selection Timeout (we need it since we use round robin for primary selection, and the new Primary can be unavailable), and All-Nodes-In-Sync timeout, that is the timeout we use for making sure that all nodes eventually in sync for the new view (prepared certificates committed, catch-up finished, etc.)    ></body> </Action>
<Action id="44308" issue="29177" author="lovesh" type="comment" body=" ~ashcherbakov   ~sergey.khoroshavin  Our current approach has been to assume that eventually every node will do catchup and get sufficient messages to get to its last prepared certificate. We should have a {{ViewChangeStart}} to complement {{ViewChangeDone}}. {{ViewChangeStart}} should indicate the state at which the node is starting its view change (got &gt;2f InstanceChange) messages. Apart from other info {{ViewChangeStart}} should contain the last prepared batch no ({{last_prepared_certificate_in_view}})" created="2018-05-14 06:06:25.0" updateauthor="lovesh" updated="2018-05-14 06:07:23.0"/>
<Action id="44309" issue="29177" author="ashcherbakov" type="comment" created="2018-05-14 06:25:18.0" updateauthor="ashcherbakov" updated="2018-05-14 06:25:18.0"> <body><! CDATA How ViewChangeStart should be propagated? As we don't have signatures over Node messages, we need to have quorums for them and can not trust just one such a message. What is the purpose of ViewChangeStart? How other nodes should process it? I feel like introducing this kind of messages we will end up in something very similar to PBFT.  Although I think we can make the current approach working in most of the positive cases by changing timeouts, I'm still in favour of PBFT approach as eventual option.  ></body> </Action>
<Action id="44310" issue="29177" author="lovesh" type="comment" created="2018-05-14 06:35:06.0" updateauthor="lovesh" updated="2018-05-14 06:35:06.0"> <body><! CDATA {{ViewChangeStart}} is broadcasted by each node to the pool on starting view change. {{ViewChangeStart}} makes each node aware of the exact state of other nodes before they started view change (stopped processing new requests).    ></body> </Action>
<Action id="44311" issue="29177" author="sergey.khoroshavin" type="comment" body=" ~lovesh   ~ashcherbakov  Well, if we introduce ViewChangeStart how can we be sure that some malicious nodes are not lying, or not sending different ViewChangeStart to different nodes? Do we need ViewChangeStartAck like in PBFT?" created="2018-05-14 07:16:15.0" updateauthor="sergey.khoroshavin" updated="2018-05-14 07:16:15.0"/>
<Action id="44537" issue="29177" author="ashcherbakov" type="comment" body="I suggest we analyse the logs we have in a try to increase the timeouts and after this close the ticket as further work on tests and fixes will be continued in the scope of INDY-1341, INDY-1303, INDY-1304." created="2018-05-17 09:49:24.0" updateauthor="ashcherbakov" updated="2018-05-17 09:49:24.0"/>
<Action id="44667" issue="29177" author="sergey-shilov" type="comment" created="2018-05-18 16:45:54.0" updateauthor="sergey-shilov" updated="2018-05-18 16:45:54.0"> <body><! CDATA We've done testing with increased view change time out (30 minutes), analysed logs and got the following result:   - the view change finished by view change time out spent;   - after finished view change the pool continued ordering transactions.  But despite continuation of ordering transaction such a long view change time out is not a good solution, because the probability of waiting till view change time out during view changed process is very high. The reason for it is unaligned _last_prepared_certificate_ (unlike PBFT)_._ Before starting of the view change process each node calculates it's own _last_prepared_certificate_ that corresponds to the last prepare message that has a quorum, so it's highly likely that these certificates are different for different nodes. If more than _f_ but less then _N-f_ nodes have higher prepared seqno than others then they can not complete a catch up that is sub process of view change. This leads to waiting till the view change time out. For example, in our test we observed the following situation:  ======================================================================  2018-04-28 14:49:45.537000 |  Node11:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:58.897000 |  Node12:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:49.527000 |  Node15:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:52.869000 |  Node16:0 setting last prepared for master to (0, 148) 2018-04-28 14:49:48.189000 |  Node18:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:57.800000 |  Node2:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:48.270000 |  Node23:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:53.251000 |  Node24:0 setting last prepared for master to (0, 148) 2018-04-28 14:49:53.727000 |  Node25:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:48.534000 |  Node3:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:51.814000 |  Node7:0 setting last prepared for master to (0, 149) 2018-04-28 14:49:47.264000 |  Node8:0 setting last prepared for master to (0, 149)     2018-04-28 14:49:44.947000 |  Node1:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:46.604000 |  Node10:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:50.750000 |  Node13:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:58.539000 |  Node14:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:51.600000 |  Node17:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:47.240000 |  Node19:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:50.815000 |  Node20:0 setting last prepared for master to (0, 151) 2018-04-28 14:49:55.239000 |  Node21:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:53.077000 |  Node22:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:51.266000 |  Node4:0 setting last prepared for master to (0, 150) 2018-04-28 14:49:57.592000 |  Node5:0 setting last prepared for master to (0, 151) 2018-04-28 14:49:51.674000 |  Node6:0 setting last prepared for master to (0, 150) 2018-04-28 14:50:16.465000 |  Node9:0 setting last prepared for master to (0, 150)  ======================================================================  As a result the nodes from the first group were waiting for _VIEW_CHANGE_DONE_ messages from the nodes from the second group until view change time out have spent.  So the risks of long view change time out are the following: * high memory usage as we stash all incoming messages during the catch up process that may lead to Out Of Memory; * unavailable pool for a long time equal to view change time out * potential kind of attack to keep the pool infinitely unavailable  So one of the core problems of current view change protocol is unaligned _last_prepared_certificate_ that leads to incomplete catch up during the view change process. It's not very critical as pool nodes complete the view change process by time out and continue ordering, but it adds risks described above.  One of the possible solution is to increase view change time out up to 5 minutes (it's approximate, we will clarify it in further related tickets) as current 1 minute is too low.  There is another problem with inconsistent state trie that is not reproduced with long time out, we'll continue trying to reproduce it in scope of ticket  INDY-1350|https://jira.hyperledger.org/browse/INDY-1350 . I think that this ticket may be closed.  ></body> </Action>
