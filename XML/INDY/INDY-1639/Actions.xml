<Action id="49529" issue="33135" author="sergey.khoroshavin" type="comment" created="2018-08-29 08:03:53.0" updateauthor="sergey.khoroshavin" updated="2018-08-29 08:03:53.0"> <body><! CDATA *Problem reason:* Spikes during request ordering on different instances can cause false positive view changes.  *Changes:* New strategy is implemented that takes into account only difference between number of transactions ordered on master and backup instances.  The following parameters were added to config: {code} ACC_MONITOR_ENABLED = False ACC_MONITOR_TXN_DELTA_K = 100 ACC_MONITOR_TIMEOUT = 300 ACC_MONITOR_INPUT_RATE_REACTION_HALF_TIME = 300 {code}  New strategy is used only when _ACC_MONITOR_ENABLED_ is set to True. In this case if number of txns ordered by any instance is more than ordered by master by more than _ACC_MONITOR_TXN_DELTA_K_ * input request rate per second then monitor will enter alerted state. If monitor is alerted for more than _ACC_MONITOR_TIMEOUT_ seconds it will fire master degradation event. Input request rate is averaged using moving average with reaction half time of _ACC_MONITOR_INPUT_RATE_REACTION_HALF_TIME_  *PR:* https://github.com/hyperledger/indy-plenum/pull/891  *Version:* indy-plenum: >= 1.6.525-master indy-node: >=  1.6.581-master  *Risk:* None when strategy is disabled Medium when strategy is enabled  *Risk factors:* As this strategy is new there could be yet unknown conditions leading to false positive or missed view changes.  *Covered with tests:* https://github.com/skhoroshavin/indy-plenum/blob/37ec03327555c236394643963cda705b54615049/plenum/test/monitoring/test_acc_monitor_strategy.py https://github.com/skhoroshavin/indy-plenum/blob/37ec03327555c236394643963cda705b54615049/plenum/test/monitoring/test_moving_average.py  *Recommendations for QA:* Set _ACC_MONITOR_ENABLED_ to True, run different load tests - there should be now view changes. Run load test which makes pool work at almost maximum performance (for example, write 20 NYMs per second) and slow down master primary node (using traffic shaping to delay packets or using stress tool to eat CPU) - view change should happen in 5-10 minutes.  ></body> </Action>
<Action id="49711" issue="33135" author="vladimirwork" type="comment" created="2018-08-31 15:14:08.0" updateauthor="vladimirwork" updated="2018-08-31 15:15:21.0"> <body><! CDATA Build Info: indy-node 1.6.586  Steps to Reproduce: 1. Install pool of 25 nodes and force 10 VCs (by primary shutdown/demote) to set viewNo to 10. 2. Set `ACC_MONITOR_ENABLED = True`. 3. Run load test to write 20 NYMs/sec. 4. Stop load test from previous step. 5. Run load test to write 28 NYMs/sec. 6. Stop load test from previous step. 7. Restart all nodes to reset view from 10 to 0 (there are no VCs forced by load at this moment). 8. Run load test to write 20 NYMs/sec. 9. Check viewNo at all nodes after 30-40 minutes.  Actual Results: 10 nodes are at view 1. 15 nodes are at view 0. Pool doesn't write any txns due to `Transaction has been rejected: Client request is discarded since view change is in progress` *for more than 1 hour*.  Expected Results: 1. It's unclear is this VC was false-positive or not - if it is - we should avoid false-positive VCs using this strategy. 2. VC should be completed in any case successfully and at all nodes.  ></body> </Action>
<Action id="49718" issue="33135" author="vladimirwork" type="comment" body="All logs and validator dumps are in /home/evernym/logs/1639.tar.gz." created="2018-08-31 15:57:08.0" updateauthor="vladimirwork" updated="2018-08-31 15:57:08.0"/>
<Action id="49788" issue="33135" author="derashe" type="comment" created="2018-09-04 07:59:41.0" updateauthor="derashe" updated="2018-09-04 09:19:48.0"> <body><! CDATA Problem reason:  * During the testing, after we restarted pool, it was not able to write txns and we had viewchange to 1 view_no.  Research: * Viewchange was caused by slow primary after restart * Primary was slow because of: ** processing and stashing new requests (stashing because of empty view_no) over 50k reqs ** generating consistency_proof for nodes that was behind befor restart  Conclusion: * That was not false positive view_change, and if we wait more, pool will continue ordering, so we need to retest this case * We need to research behaviour of node restart under heavy load and with behind nodes as a separate ticket (https://jira.hyperledger.org/browse/INDY-1675)     ></body> </Action>
<Action id="50091" issue="33135" author="vladimirwork" type="comment" created="2018-09-07 15:39:14.0" updateauthor="vladimirwork" updated="2018-09-07 15:39:14.0"> <body><! CDATA Build Info: indy-node 1.6.599  Steps to Reproduce: 1. Run load test to provide ~20 txns written at pool side with ACC_MONITOR_ENABLE = True. 2. Stop primaries for 1st, 2nd, 3rd, 4th backup instances consecutively. 3. Start primaries for 1st, 2nd, 3rd, 4th backup instances consecutively. 4. Check throughput and latency using metrics.  Actual Results: We have 22 nodes at view 5 and 3 nodes at view 0. *Pool has stopped ordering txns after all VCs*. The first VC performed after Step 3 because of 'reason': 25 (PRIMARY_DEGRADED) and all the next because of 'reason': 28 (INSTANCE_CHANGE_TIMEOUT) and we have the next metrics dynamics (1 - switching off 2,3,4,5 nodes; 2 - switching on 2,3,4,5 nodes; 3 - instance change from 0 to 1 view due to 25 reason):  !INDY-1639_TIMELINE.PNG|thumbnail!   Expected Results: VC due to master degraded reason looks strange with new strategy enabled. Pool should continue working after any amount of VCs.  ></body> </Action>
<Action id="50147" issue="33135" author="vladimirwork" type="comment" body="All logs and validator dumps are in /home/evernym/logs/1639newissue.tar.gz." created="2018-09-10 10:58:36.0" updateauthor="vladimirwork" updated="2018-09-10 10:58:36.0"/>
<Action id="50337" issue="33135" author="toktar" type="comment" created="2018-09-12 09:56:07.0" updateauthor="toktar" updated="2018-09-12 09:56:07.0"> <body><! CDATA First View Change was expected because master instance ordering slower than backup instance 7. Size of butches on the master was smaller and with near speeds of ordering batch speed of transaction ordering was slower. Moreover, in this time happened a stabilization of checkpoint on the master. 2-5 view changes were  expected too because  in tests were stopped  primary nodes for 1st, 2nd, 3rd, 4th backup instances and these nodes can't be a primary for master instance.  Problem with out of memory because of untreated requests queue will fix in tasks from epic Switch off replicas.  ></body> </Action>
