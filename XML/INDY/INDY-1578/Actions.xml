<Action id="48748" issue="32752" author="ckochenower" type="comment" body="nscapture archives for each of the 10 nodes are available at https://drive.google.com/drive/folders/1-I_qJbtywjkuESQc7Mj-zHIqS_Vb1jgU?usp=sharing" created="2018-08-11 00:31:34.0" updateauthor="ckochenower" updated="2018-08-11 00:31:34.0"/>
<Action id="48900" issue="32752" author="ckochenower" type="comment" created="2018-08-15 16:55:55.0" updateauthor="ckochenower" updated="2018-08-15 16:55:55.0"> <body><! CDATA A similar scenario exercised by the "shrink pool" Chaos experiment resulted in the same results. Attaching logs and/or nscapture archives from both experiments will clutter this issue.  The "shrink pool" experiment can reproduce the results documented  here|https://jira.hyperledger.org/browse/INDY-1560?focusedCommentId=48898&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48898 .   Please request logs and/or nscapture archives produced by the "shrink pool" experiment if you feel they would provide better/additional insight.  ></body> </Action>
<Action id="48908" issue="32752" author="ckochenower" type="comment" created="2018-08-15 23:10:27.0" updateauthor="ckochenower" updated="2018-08-15 23:11:41.0"> <body><! CDATA When a pool's f_value changes (and thus the Count_of_replica changes, because it is f+1), perhaps the node that was promoted and restarted doesn't immediately see the correct number of nodes, because the pool ledger is _eventually consistent_ and needs slightly more time to commit the change to the pool ledger?  I added a 10 second sleep between setting "services" equal to "VALIDATOR" and restarting the node and the node no longer needs a second restart to get into sync.  I will try reducing the sleep until I find an approximate minimum time to wait.  ></body> </Action>
<Action id="51184" issue="32752" author="ckochenower" type="comment" body="This is still a problem after upgrading to the following versions:" created="2018-09-25 23:45:28.0" updateauthor="ckochenower" updated="2018-09-25 23:45:28.0"/>
<Action id="51185" issue="32752" author="ckochenower" type="comment" created="2018-09-25 23:45:28.0" updateauthor="ckochenower" updated="2018-09-25 23:45:58.0"> <body><! CDATA This is still a problem after upgrading to the following versions:  {code} ubuntu@kellyseoul10:~$ apt list --installed | grep indy  WARNING: apt does not have a stable CLI interface. Use with caution in scripts.  indy-anoncreds/xenial,now 1.0.32 amd64  installed  indy-node/xenial,now 1.6.603 amd64  installed,upgradable to: 1.6.613  indy-plenum/xenial,now 1.6.539 amd64  installed,upgradable to: 1.6.545  libindy-crypto/xenial,now 0.4.3 amd64  installed  python3-indy-crypto/xenial,now 0.4.3 amd64  installed   ubuntu@kellyseoul10:~$ pip3 list installed | grep indy indy-anoncreds (1.0.32) indy-crypto (0.4.1) indy-node (1.6.603) indy-plenum (1.6.539) indy-plenum-dev (1.4) {code}  ></body> </Action>
<Action id="51273" issue="32752" author="keichiri" type="comment" created="2018-09-27 14:27:58.0" updateauthor="keichiri" updated="2018-09-27 14:27:58.0"> <body><! CDATA After long investigation, this problem was confirmed, and two different bugs contribute to it: 1. one of the nodes ends up with an incorrect replica count 2. one of the nodes has a replica with incorrect primaries  The bugs are in the plenum implementation  I confirmed the bug by writing the test to represent the mentioned scenario as accurately as possible.  ~Derashe  knew where the issues might lay, so he confirmed it in code  Two separate tickets are created: 1. https://jira.hyperledger.org/browse/INDY-1719 2.Â https://jira.hyperledger.org/browse/INDY-1720  ></body> </Action>
