<Action id="42575" issue="29178" author="vladimirwork" type="comment" created="2018-04-05 16:14:45.0" updateauthor="vladimirwork" updated="2018-04-05 16:14:45.0"> <body><! CDATA  ~dsurnin  https://drive.google.com/drive/u/1/folders/1zyAVAMOPI0A334mH1qfF6qKixG-5uj3b  Debug logs from 1, 11, 24 nodes.  ></body> </Action>
<Action id="43349" issue="29178" author="ashcherbakov" type="comment" created="2018-04-24 12:59:28.0" updateauthor="ashcherbakov" updated="2018-04-24 12:59:28.0"> <body><! CDATA  ~sergey.khoroshavin   ~VladimirWork  What load scripts were run against the pool? Is it a script simulating high load (many clients), or scripts with minimal activity?  ></body> </Action>
<Action id="43435" issue="29178" author="vladimirwork" type="comment" body=" ~ashcherbakov  There were several high load cases with 500..2000 simultaneous writing/reading clients against this pool before it was broken." created="2018-04-25 18:45:59.0" updateauthor="vladimirwork" updated="2018-04-25 18:47:43.0"/>
<Action id="43598" issue="29178" author="zhigunenko.dsr" type="comment" created="2018-04-28 10:41:48.0" updateauthor="zhigunenko.dsr" updated="2018-05-03 06:23:26.0"> <body><! CDATA  ~ashcherbakov  During performance testing with _indy-node 1.3.364_ and _indy-plenum 1.2.310_  24 of 25 nodes stopped to write after 298k txns under *5 clients * 1txns / sec* load  ~dsurnin  has all available logs + journalctl from whole pool  {code} #unsafe=set( 'disable_view_change' ) MAX_CATCHUPS_DONE_DURING_VIEW_CHANGE = 5000 MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE = 1800 {code}   ></body> </Action>
<Action id="44326" issue="29178" author="ashcherbakov" type="comment" body="We need to make sure if the issue is caused by View Change. If so, it will be fixed together with INDY-1341 and INDY-1350." created="2018-05-14 11:52:03.0" updateauthor="ashcherbakov" updated="2018-05-25 08:55:44.0"/>
<Action id="45488" issue="29178" author="derashe" type="comment" created="2018-06-04 16:17:19.0" updateauthor="derashe" updated="2018-06-06 08:40:54.0"> <body><! CDATA Problem reason: * Pool, that described in description of this ticket, stopped write transactions.  Inverstigation: * During the inverstigation of logs 2018-04-19 - 2018-04-20, we found that pool had some troubles, but was able to write txns in ledger. * Founded troubles is about breaking consensus in some nodes because of incorrect state tree. This was caused by incorrect process of applying stashed batches after catchup. * In this particular case, error appeared from outdated code of order_3pc_key method in plenum's replica class. This method used to call apply_stashed_reqs from plenum's node class. So, after ending of catchup last request used to apply two times. Up-to-date plenum don't have this bug. * Another noticed bug that probably could cause incorrect state is about incorrect stashing of checkpoints. That problem was resolved in INDY-1329 * Similar errors were found in INDY-1350 and INDY-1315 * apply_stashed_reqs mechanism will be fixed in scope of INDY-1328  Recommendations for QA * Try to run updated pool after completing of INDY-1328  ></body> </Action>
