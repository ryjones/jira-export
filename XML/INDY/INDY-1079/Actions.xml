<Action id="38327" issue="26635" author="mgbailey" type="comment" body="Update: The steward for the node with 18 transactions in its config ledger shut down his node.  The error messages are no longer being written to the logs of the other nodes, but transactions are still not being posted." created="2018-01-09 05:17:45.0" updateauthor="mgbailey" updated="2018-01-09 05:17:45.0"/>
<Action id="38342" issue="26635" author="mgbailey" type="comment" body=" ^transactions.tgz  contains pool, domain, and config transactions for the mapleleaf node." created="2018-01-09 13:28:32.0" updateauthor="mgbailey" updated="2018-01-09 13:28:32.0"/>
<Action id="38452" issue="26635" author="spivachuk" type="comment" created="2018-01-11 16:47:31.0" updateauthor="spivachuk" updated="2018-01-11 17:03:51.0"> <body><! CDATA As we can see for now in the attached logs, 3PC-batches stopped to be ordered by the master protocol instance and so transactions stopped to be committed into ledgers on January 4 at 17:50 (for about half an hour period) because replicas in the master protocol instance received a PREPREPARE message with a wrong state trie root hash from the master's primary. The state trie root hash of a received PREPREPARE message is verified for the master protocol instance only, so only the master protocol instance stopped to order transactions. This PREPREPARE message was sent by {{virginia:0}} just after some next view change from the series of view changes caused by sequential nodes restarts performed during the pool upgrade procedure. This PREPREPARE message most likely contained the client request {{('NVji6YsYLTK6ozNDzBHyWB', 1514923761877253)}}. We are not able to state this for sure because INFO-logs do not contain the corresponding information, but we can guess this because we see that the backup protocol instances ordered 3PC-batches with this client request at the same time. Also we see that this client request appeared for the second time - it had been already ordered earlier, two days ago. This might be the reason why the state trie root hash of the PREPREPARE message was wrong.  As to repeated processing of client requests or their PROPAGATEs, earlier we already saw such cases in INDY-959, INDY-1045 and we added a check of the request presence in {{seqNoDB}} on processing of the belated / repeated PROPAGATE message in scope of INDY-959.  ></body> </Action>
<Action id="38459" issue="26635" author="spivachuk" type="comment" created="2018-01-11 18:38:44.0" updateauthor="spivachuk" updated="2018-01-11 18:38:44.0"> <body><! CDATA *Problem reason:* - Please see the previous comment.  *Problem state:* - The issue with repeated processing of client requests or their PROPAGATEs was fixed in scope of INDY-959.  ></body> </Action>
<Action id="39124" issue="26635" author="ozheregelya" type="comment" created="2018-01-25 17:51:08.0" updateauthor="ozheregelya" updated="2018-01-25 17:51:08.0"> <body><! CDATA As it was discussed with developers on stand-up meeting, it's enough to run load test to verify this issue.  *Environment:* indy-node 1.2.279 AWS QA live pool (25 nodes)  *Steps to Validate:* 1. Setup the pool. 2. Run load test several times.  *Actual Results:* Pool successfully wrote 25000 transactions.  ></body> </Action>
