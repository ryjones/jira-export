<Action id="42400" issue="29038" author="krw910" type="comment" body=" ~gudkov  We need someone to take a look at this first thing on Monday morning." created="2018-03-30 20:32:51.0" updateauthor="krw910" updated="2018-03-30 20:32:51.0"/>
<Action id="42437" issue="29038" author="mgbailey" type="comment" body="Attaching the pool ledger  ^pool_ledger.tgz " created="2018-04-02 14:27:43.0" updateauthor="mgbailey" updated="2018-04-02 14:27:43.0"/>
<Action id="42439" issue="29038" author="dsurnin" type="comment" created="2018-04-02 15:19:45.0" updateauthor="dsurnin" updated="2018-04-02 15:20:54.0"> <body><! CDATA  ~mgbailey  According to log we have the following:  * What caused the view change?  Primary of master protocol instance disconnected  * Why were we unable to post transactions?  It looks like transactions failed validation with msg like "InvalidClientRequest('LKpJgj5zdXLwge3udVJXDD can have one and only one SCHEMA with name TranscriptSchema and version 1.0',)"  * Following the restart, were all 10 of the nodes participating in consensus? The nodes that did not restart are NewtonD, RFCU, and ibm  No, 3 nodes that did not restart are not in consensus because we cannot view change to the lower view no  INDY-1199|https://jira.hyperledger.org/browse/INDY-1199    ></body> </Action>
<Action id="42440" issue="29038" author="dsurnin" type="comment" created="2018-04-02 15:21:49.0" updateauthor="dsurnin" updated="2018-04-02 15:21:49.0"> <body><! CDATA  ~mgbailey  Do we need more research on this?  ></body> </Action>
<Action id="42443" issue="29038" author="mgbailey" type="comment" created="2018-04-02 16:29:32.0" updateauthor="mgbailey" updated="2018-04-02 16:29:32.0"> <body><! CDATA Why did a primary disconnect cause this problem?  Is there a ticket being filed to prevent this from happening in the future?   ~dsurnin  Ideally I want one of 2 outcomes from this ticket: either this is a known problem with a ticket already in progress to fix it, or a new issue identified and a ticket created to fix it.  ></body> </Action>
<Action id="42462" issue="29038" author="dsurnin" type="comment" created="2018-04-03 13:01:13.0" updateauthor="dsurnin" updated="2018-04-03 13:04:07.0"> <body><! CDATA Around 02:42 primary (australia) node restarted. The other nodes started view change because primary was disconnected. austarlia started and finished view changed together with 3 more nodes. After that all other nodes restarted, including primary. New view changed started because primary disconnected but was not finished. There are two issues that happened together 1 - Nodes cannot switch to lower viewNo (similar to  INDY-1199|https://jira.hyperledger.org/browse/INDY-1199 ) - not fixed 2 - Pool has issues to restore consensus in case of more than n-f nodes restarted but the other not (similar to  INDY-1197|https://jira.hyperledger.org/browse/INDY-1197 ) - fixed  The reason of restarts is unclear. Probably journalctl logs for that time could help  ></body> </Action>
<Action id="42468" issue="29038" author="mgbailey" type="comment" created="2018-04-03 15:19:37.0" updateauthor="mgbailey" updated="2018-04-03 15:36:10.0"> <body><! CDATA  ~dsurnin  At 02:41 Puppet, which controls our nodes, did a package upgrade on all nodes which required a node reboot.  If INDY-1199 and INDY-1197 had been fixed, would the pool have come back up cleanly? Would the external nodes, which did not reboot, have returned to consensus as well?     ></body> </Action>
<Action id="42501" issue="29038" author="dsurnin" type="comment" created="2018-04-04 06:57:57.0" updateauthor="dsurnin" updated="2018-04-04 06:57:57.0"> <body><! CDATA  ~mgbailey  I believe that chances to restore cleanly with INDY-1199 and INDY-1197 being fixed are much higher, but it requires extensive testing and monitoring  ></body> </Action>
<Action id="42579" issue="29038" author="mgbailey" type="comment" created="2018-04-05 16:42:16.0" updateauthor="mgbailey" updated="2018-04-05 16:42:16.0"> <body><! CDATA  ~dsurnin  We needed to reboot the nodes on the STN again today for a software upgrade. This time I did it manually instead of via puppet, and I watched closely.  Once again, we were unable to post transactions following the reboot. I was able to restore functionality by restarting the indy-node service on my 7 nodes.  As an experiment, I rebooted just the primary node.  Even though I did not see view change transactions in the logs, I was unable to post transactions after this.  ~krw910  *It would appear that a reboot of the primary node kills consensus of the network.*  ></body> </Action>
<Action id="42795" issue="29038" author="krw910" type="comment" created="2018-04-10 19:43:05.0" updateauthor="krw910" updated="2018-04-10 19:43:05.0"> <body><! CDATA  ~dsurnin  I am not seeing any comments that state we reproduced the issue with rebooting the physical machine that is the primary (only that machine) and have it cause the pool to fail.  Was this tried with the current Stable release? If we know the reason can someone post it as a comment? I don't believe the cause has anything to do with INDY-1199 or INDY-1197.    ~VladimirWork  I do not want this ticket closed until we have an answer to this issue {color:#d04437}*not a guess or a feeling*{color} that it is fixed.   ~mgbailey  FYI  ></body> </Action>
<Action id="42810" issue="29038" author="vladimirwork" type="comment" body=" ~zhigunenko.dsr  Please add all results and findings of this case testing against our persistent pool in this ticket." created="2018-04-11 07:59:34.0" updateauthor="vladimirwork" updated="2018-04-11 07:59:34.0"/>
<Action id="42814" issue="29038" author="dsurnin" type="comment" created="2018-04-11 10:36:19.0" updateauthor="dsurnin" updated="2018-04-11 10:36:19.0"> <body><! CDATA  ~krw910   ~mgbailey   INDY-1197 - is about the situation when small amount of nodes will not send current state to the newly connected nodes if total number of nodes is not enough for consensus - because of absence of insufficient current state msgs node will not be able to finish view change. INDY-1199 - initially this issue was found with demotion/promotions tests but the issue could be reproduced with restart also. If current view change number is greater than 0 and lots of nodes restarted (enough for consensus), then restarted nodes will choose viewNo 0, but the other nodes will change view to lower number and will not be able to participate in consensus.  In our case primary restarted, view change started, several nodes changed view, other nodes restarted and restored in the pool one by one. As a result restarted nodes cannot change view to the bigger viewNo (because of INDY-1197) and nodes that were not restarted cannot change to lower viewNo (INDY-1199)  Primary restart does not lead to pool to reject txns. Just to view change. However during the view change pool stashing all the txns and start to process them only after view change is done. It might looks like pool is stopped since we cannot guaranty that view change will be finished in any constant time.  We have series of tests for pool restarting in plenum/tests/restart folder.   ></body> </Action>
<Action id="42817" issue="29038" author="zhigunenko.dsr" type="comment" created="2018-04-11 12:15:37.0" updateauthor="zhigunenko.dsr" updated="2018-04-11 12:15:37.0"> <body><! CDATA *Environment:* AWS Persistent Pool (18 nodes) indy-anoncreds                   1.0.11 indy-node                        1.3.55 indy-plenum                      1.2.34 libindy-crypto                   0.1.6-10 python3-indy-crypto              0.2.0  *Steps to reproduce:* 1. open CLI on non-primary node  2. on primary node execute _sudo apt-get update && sudo apt-get upgrade_ and then _sudo reboot_ 3. during updating/rebooting send NYMs from non-primary node *Actual results:* Ex-primary node cannot catch up new txns. Rest of nodes are consider ex-primary as unreachable *Expected results:* Ex-primary successfully catch up and write new txns.  ></body> </Action>
<Action id="42821" issue="29038" author="ozheregelya" type="comment" body="Logs from QA Persistent pool:  ^live_system_upgrade_problem.7z Unfortunately, only INFO level." created="2018-04-11 12:46:48.0" updateauthor="ozheregelya" updated="2018-04-11 12:46:48.0"/>
<Action id="42822" issue="29038" author="krw910" type="comment" created="2018-04-11 13:27:02.0" updateauthor="krw910" updated="2018-04-11 13:27:02.0"> <body><! CDATA  ~dsurnin  I agree with your assessment of the tickets you mentioned and how they would affect that scenario. See my other comments to the testers.   ~VladimirWork   ~zhigunenko.dsr   ~ozheregelya   My concern about this ticket is  ~mgbailey  can just reboot the physical machine that is the primary and have the pool stop reaching consensus. Nothing else is happening outside of just rebooting the primary. No manual upgrade and no rebooting of other nodes to cause this.  The rebooting of the other nodes in the logs is to get the pool functioning again. If Mike reboots 7 of the nodes he controls in the STN is is enough to get the  pool functioning again. He does not need to restart the entire pool.   Due to what Mike is telling me it does not seem to match the other tickets mentioned and it does not sound like we have reproduced the issue, because in the steps mentioned in Nikita's comments it talks about updating/rebooting the primary. Mike is not updating the primary he is just rebooting the machine.  ></body> </Action>
<Action id="42824" issue="29038" author="krw910" type="comment" created="2018-04-11 14:30:58.0" updateauthor="krw910" updated="2018-04-11 14:30:58.0"> <body><! CDATA  ~dsurnin   ~zhigunenko.dsr   ~VladimirWork   Since the team cannot reproduce the exact issue  ~mgbailey  said he can set 7 of the STN nodes logging to debug and reproduce the issue. Once he does he will get the STN logs to development to debug. Please keep this ticket open until we can resolve it with Mike.  ></body> </Action>
<Action id="42829" issue="29038" author="mgbailey" type="comment" created="2018-04-11 16:15:05.0" updateauthor="mgbailey" updated="2018-04-11 16:15:05.0"> <body><! CDATA  ~krw910   ~dsurnin  I ran a new experiment on the STN. Here are the steps: * at 15:16, send NYM dest=Bb7stpoCv7S2GDxT1FQa1 verkey=~Snj2hv6VAAfHbBFYv4i4Tj ** Result: successfully *posted* to the STN * at 15:19, reboot 'australia', the current primary ** No view change seen on other nodes.  On 'australia', the same primaries are selected. * at 15:43, send NYM dest=Bb7stpoCv7S2GDxT1FQa2 verkey=~Snj2hv6VAAfHbBFYv4i4Tj ** Result: *not posted* to the STN  Attaching new logs for my 7 nodes, all with TRACE level logs. ^sgp-stn-p001_log_20180411.tgz    ></body> </Action>
<Action id="42830" issue="29038" author="mgbailey" type="comment" created="2018-04-11 16:31:34.0" updateauthor="mgbailey" updated="2018-04-11 16:32:50.0"> <body><! CDATA I have done some additional tests.  I am not adding additional logs, since I believe that there will be no additional data in them. * restart the service on the pool ** consensus restored * on the primary: sudo systemctl restart indy-node ** result: consensus lost ** view change occurred only on the primary node (this may be key) * restart the service on the pool ** consensus restored * on a different node: sudo systemctl restart indy-node ** result: consensus maintained  ></body> </Action>
<Action id="42838" issue="29038" author="ozheregelya" type="comment" created="2018-04-11 18:07:28.0" updateauthor="ozheregelya" updated="2018-04-11 18:20:01.0"> <body><! CDATA *Environment:* indy-node 1.3.364 (master) AWS QALive pool (20 nodes) with 260,490 txns in ledger.  *Steps to Reproduce:* 1. Make sure that all nodes are in consensus and that all nodes have the same primary. 2. Reboot instance with primary node (Node10).  *Actual Results:* ViewChange was not happened on one node (Node15). For the rest nodes primary was changed (to Node11). Pool is still in consensus.  After restart of problematic node (Node15) primary on this node was changed (to Node11), problematic node successfully completed catch-up and continued to work properly.  *Logs:*  https://drive.google.com/file/d/1CBZYr2pMs1dapRqVbKSvn9RgVIb5quas/view?usp=sharing  *Journalctl:*  https://drive.google.com/file/d/1_Nd4Exqe2Bhh1_OHh0l5c0y56obsGZMb/view?usp=sharing   *Additional Information:* Similar case was tried by  ~zhigunenko.dsr  and  ~VladimirWork  on another pools with *stable* version. ViewChange was successfully completed on all of the nodes in case of simple reboot.  ></body> </Action>
<Action id="42860" issue="29038" author="zhigunenko.dsr" type="comment" created="2018-04-12 12:51:54.0" updateauthor="zhigunenko.dsr" updated="2018-04-12 12:51:54.0"> <body><! CDATA *Environment:* indy-node 1.3.364 (master) AWS QALive pool (20 nodes)  *Steps to Reproduce:* 1. Make sure that all nodes are in consensus and that all nodes have the same primary. 2. execute on primary node (Node11) this "_execute sudo apt-get update && sudo apt-get upgrade && sudo reboot_" *Actual results:* Ex-primary node cannot catch up new txns. Rest of nodes are consider ex-primary as unreachable *Expected results:* Ex-primary successfully catch up and write new txns.   Logs|https://drive.google.com/open?id=1YxjUlWcSxpXylq1Mu09dIY9kcyqJ5WV3  Thu Apr 12 07:31:20 2018 -  approximate reboot time   ></body> </Action>
<Action id="42900" issue="29038" author="dsurnin" type="comment" created="2018-04-13 10:29:45.0" updateauthor="dsurnin" updated="2018-04-13 10:29:45.0"> <body><! CDATA  ~mgbailey   ~krw910  For the moment I was able to check logs from Mike. It is propagate primary case but for primary node, i.e. primary node was disconnected, some nodes started to send instance change msgs but consensus was not reached until primary was rebooted and started initialization. It looks like node was not able to correctly restore the last ordered ppseqno and started to use 0 while the rest of pool use 55, as a result all backup replicas were able to order txn while master replica was not able to reach consensus - all the nodes dropped packets with the same viewNo and ppseqno less than last ordered ppseqno. Recently we fixed propagate primary case for non-primary node, it was discovered during the fixing of  INDY-1018|https://jira.hyperledger.org/browse/INDY-1018  . The case with propagate primary for primary node is difficult to reproduce with the default settings in a good and stable network environment.  Our QA even use restart primary as a simplest way to initiate view change.  I will continue to analyze the other logs and test cases attached to ticket.  ></body> </Action>
<Action id="43418" issue="29038" author="dsurnin" type="comment" created="2018-04-25 14:24:18.0" updateauthor="dsurnin" updated="2018-04-25 14:24:18.0"> <body><! CDATA The issue reproduces by  ~mgbailey  is fixed in PR https://github.com/hyperledger/indy-plenum/pull/641  Reason is the processing current state msgs issue during the propagate primary process in case of primary node - msgs were ignored.  Recommendations for QA one can try to reproduce the issue by changing config parameter ToleratePrimaryDisconnection to some big value, i.e. 100 or more on all the nodes in the pool.  ></body> </Action>
<Action id="43419" issue="29038" author="dsurnin" type="comment" body="For the other mentioned issues separated ticket is created -  INDY-1295|https://jira.hyperledger.org/browse/INDY-1295 " created="2018-04-25 14:25:24.0" updateauthor="dsurnin" updated="2018-04-25 14:25:54.0"/>
<Action id="43730" issue="29038" author="ozheregelya" type="comment" created="2018-05-02 16:08:50.0" updateauthor="ozheregelya" updated="2018-05-02 16:08:50.0"> <body><! CDATA *Environment:* indy-node=1.3.396 AWS pool of 4 nodes  *Steps to Validate:* 1. Set up the pool with ToleratePrimaryDisconnection = 100, send several transactions. 2. Reboot the primary node (ViewNo 0). => Pool works right after reboot, ViewNo is still 0. 3. Restart the primary. => View Change was happened as usual, ViewNo is changed to 1. 4. Reboot new primary. => Pool works right after reboot, ViewNo is still 1.  *Actual Results:* Pool works without any issues in case of reboot/restart primary node.  ></body> </Action>
