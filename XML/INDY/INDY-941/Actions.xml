<Action id="33655" issue="23234" author="krw910" type="comment" body=" ~ashcherbakov  We need someone on this issue right away." created="2017-10-31 22:06:48.0" updateauthor="krw910" updated="2017-10-31 22:06:48.0"/>
<Action id="33656" issue="23234" author="mgbailey" type="comment" body="This is the agent code used:  ^intrepid.py .  " created="2017-10-31 22:17:49.0" updateauthor="mgbailey" updated="2017-10-31 22:17:49.0"/>
<Action id="33660" issue="23234" author="mgbailey" type="comment" body="I was able to reproduce this issue with an out-of-the-box faber agent" created="2017-11-01 00:29:40.0" updateauthor="mgbailey" updated="2017-11-01 00:29:40.0"/>
<Action id="34008" issue="23234" author="ashcherbakov" type="comment" created="2017-11-01 16:13:09.0" updateauthor="ashcherbakov" updated="2017-11-01 16:13:09.0"> <body><! CDATA  ~mgbailey   ~krw910  I think the problem is the following: - Once started, Agent performs some bootstrapping scripts including sending of SCHEMA txns.  - There is a timeout for this sending (30 seconds) - Sending of a txn can not be done until catch-up is finished  - It turned out in this particular case that it takes more than 30 seconds to catch-up (there is a timeout in catch-up logic as well which checks whether we got sufficient number of CatchupReplies from Nodes, and, if not, ask for missing; this timeout may be close to 30 seconds as well, so we may strat a second round of catch-up in 30 seconds only, when timeout for sending of SCHEMA is reached). - Although we will probably need to have a look at catch-up logic and see whether it's possible to reduce this timeout and speed up catch-up, I propose the following hotfix you can apply for Agents:  *Hotfix* Wait until client is connected to Nodes and catch-up is finished before sending initial SCHEMA txns. Find out `run_agent.py` script (in `indy_client/agent/run_agent.py`), and modify `def runAgent(agent, looper=None, bootstrap=None):` as follows: {code} def runAgent(agent, looper=None, bootstrap=None): assert agent  def is_connected(agent): client = agent.client from plenum.common.startable import Mode if client.mode != Mode.discovered: raise Exception("Client hasn't finished catch-up with Pool Ledger yet") if not client.hasSufficientConnections: raise Exception("Client doesn't have sufficient number of connections to send write requests")  async def wait_until_connected(agent): from stp_core.loop.eventually import eventually await eventually(is_connected, agent, timeout=120, retryWait=2)  def do_run(looper): agent.loop = looper.loop looper.add(agent) logger.info("Running {} now (port: {})".format(agent.name, agent.port)) if bootstrap: looper.run(wait_until_connected(agent)) looper.run(runBootstrap(bootstrap))  if looper: do_run(looper) else: with Looper(debug=getConfig().LOOPER_DEBUG, loop=agent.loop) as looper: do_run(looper) looper.run() {code}   ></body> </Action>
<Action id="34100" issue="23234" author="mgbailey" type="comment" created="2017-11-01 18:02:41.0" updateauthor="mgbailey" updated="2017-11-01 18:02:41.0"> <body><! CDATA  ~ashcherbakov , thanks for this hotfix, it works.  I tested it by leaving the singapore node down.  After starting the agent, it gets transaction updates from all nodes except for singapore.  It then waits for quite a while, and then it looks like it gets the update that was assigned to singapore from a different node instead. Then the agent is able to come up. It is good that I will be able to move forward with the client now.  Will it ever time out, or is it in an infinite loop now?   ~krw910 , we will want to make sure that this, or something like it, is merged into the codebase.  ></body> </Action>
<Action id="34227" issue="23234" author="ashcherbakov" type="comment" created="2017-11-02 08:19:57.0" updateauthor="ashcherbakov" updated="2017-11-02 08:19:57.0"> <body><! CDATA  ~krw910   ~mgbailey  I will merge the hotfix I provided into the codebase. As I know, there is no timeout for catch-up, so it will try forever.  ></body> </Action>
<Action id="34412" issue="23234" author="ashcherbakov" type="comment" created="2017-11-07 13:24:56.0" updateauthor="ashcherbakov" updated="2017-11-09 12:44:49.0"> <body><! CDATA Changes: - applied the hotfix above to the code; - fixed client's class to have proper can_send methods (which wait for pool ledger catch-up finishing), and added tests for this  PR: - https://github.com/hyperledger/indy-plenum/pull/440 - https://github.com/hyperledger/indy-node/pull/435  Build: - 1.2.204  ></body> </Action>
<Action id="34533" issue="23234" author="vladimirwork" type="comment" created="2017-11-10 10:29:01.0" updateauthor="vladimirwork" updated="2017-11-10 10:29:01.0"> <body><! CDATA Build Info: indy-node 1.2.205  Steps to Reproduce: 1. Install pool of 7 nodes. 2. Shut down the 7th node. 3. Send role and verkey to Faber's DID. 4. Start the Faber agent on the 2nd node. 5. Wait 2+ minutes.  Actual Results: {quote} 2017-11-10 09:55:02,302 | INFO     | run_agent.py         (79) | do_run | Running Faber College now (port: 5555) 2017-11-10 09:57:02,398 | ERROR    | eventually.py        (182) | eventually | is_connected failed; not trying any more because 120 seconds have passed; args were (<indy_client.agent.walleted_agent.WalletedAgent object at 0x7f717354a7b8>,) 2017-11-10 09:57:02,398 | ERROR    | looper.py            (249) | wrapper | Error while running coroutine wait_until_connected: NotConnectedToNetwork("Client hasn't finished catch-up with Pool Ledger yet or doesn't have sufficient number of connections",) 2017-11-10 09:57:02,399 | INFO     | looper.py            (272) | shutdown | Looper shutting down now... 2017-11-10 09:57:02,405 | INFO     | walleted_agent.py    (129) | _saveWallet | Active wallet "Faber College" saved (/home/indy/.indy-cli/wallets/agents/faber-college/faber college.wallet) 2017-11-10 09:57:02,407 | INFO     | walleted_agent.py    (129) | _saveWallet | Active wallet "issuer" saved (/home/indy/.indy-cli/wallets/agents/faber-college/issuer/issuer.wallet) 2017-11-10 09:57:02,408 | INFO     | zstack.py            (327) | stop | stack HVVEPZzbaHdLK6RRxgJLcLR6mh6Zvn1ePTUD1AZxdEG9 closing its listener 2017-11-10 09:57:02,408 | DEBUG    | authenticator.py     (37) | stop | Stopping ZAP at b'inproc://zeromq.zap.1' 2017-11-10 09:57:02,408 | INFO     | zstack.py            (331) | stop | stack HVVEPZzbaHdLK6RRxgJLcLR6mh6Zvn1ePTUD1AZxdEG9 stopped 2017-11-10 09:57:02,408 | INFO     | zstack.py            (327) | stop | stack FaberCollege closing its listener 2017-11-10 09:57:02,409 | DEBUG    | authenticator.py     (37) | stop | Stopping ZAP at b'inproc://zeromq.zap.2' 2017-11-10 09:57:02,409 | INFO     | zstack.py            (331) | stop | stack FaberCollege stopped 2017-11-10 09:57:02,409 | INFO     | looper.py            (279) | shutdown | Looper shut down in 0.010 seconds. 2017-11-10 09:57:02,409 | ERROR    | runnable_agent.py    (50) | run_agent |   ------------------------------------------------------------------ERROR------------------------------------------------------------------ Agent startup failed:  cause : Client hasn't finished catch-up with Pool Ledger yet or doesn't have sufficient number of connections  ------------------------------------------------------------------ERROR------------------------------------------------------------------  sys:1: RuntimeWarning: coroutine 'bootstrap_faber' was never awaited {quote}  Expected Results: Agent should catch up normally.  ></body> </Action>
<Action id="34548" issue="23234" author="ashcherbakov" type="comment" created="2017-11-10 15:16:13.0" updateauthor="ashcherbakov" updated="2017-11-10 16:24:34.0"> <body><! CDATA The cause of the problem is file-folder refactoring issues. Fixed in https://github.com/hyperledger/indy-node/pull/444  Now faber, acme and thrift are run against a correct folder (taking into account Networks). Agents are connected to sandbox by default. --network parameter is also added which can help in connecting to another network.  ></body> </Action>
<Action id="34610" issue="23234" author="vladimirwork" type="comment" created="2017-11-14 11:42:34.0" updateauthor="vladimirwork" updated="2017-11-14 11:42:34.0"> <body><! CDATA Build Info: indy-node 1.2.209  Steps to Validate: 1. Set up pool and shut down some of the nodes. 2. Run agent.  Actual Results: Agent catches up and works successfully. Parameter --network works correctly. !faber@live.PNG|thumbnail!   ></body> </Action>
