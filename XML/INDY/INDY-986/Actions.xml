<Action id="37098" issue="24624" author="andkononykhin" type="comment" created="2017-12-14 15:09:52.0" updateauthor="andkononykhin" updated="2017-12-14 15:09:52.0"> <body><! CDATA I was able to reproduce the case. The reason: * node's process reached the limit for number of open files because it held so many (about 1000) simultaneous tcp connections. * Soft limit for number of opened files was 1024. Each new more tcp connection was silently dropped. (I believe it is somewhere inside zmq library). * But node crashed during the file opening (from the level of our plenum code) because of 'Too many open files' exception  It was reproduced inside docker with nofile limit set to 100 and using simple "telnet" to emulate tcp connections.  Seems it's a quite serious issue and needs some discussion which was initiated vie email.  As a fast fix I'm going to increase soft/hard limits from default 1024/4096 to 16K/64 for node service: it will allow to perform load testing with higher number of clients and seems it is more appropriate values for node's needs (I failed to find any requirements for that).  Also new task INDY-1037 was created to review our usage of zmq because I failed to quickly find a zmq's configuration parameter to limit number of peers connections: there is a  ZMQ_MAX_SOCKETS|http://api.zeromq.org/3-2:zmq-ctx-set#toc4  parameter that we already use but it's not what we need here.     ></body> </Action>
<Action id="37401" issue="24624" author="ashcherbakov" type="comment" created="2017-12-15 08:30:31.0" updateauthor="ashcherbakov" updated="2017-12-15 08:30:31.0"> <body><! CDATA There is also INDY-570 about limiting number of client connections.   ></body> </Action>
<Action id="37514" issue="24624" author="andkononykhin" type="comment" created="2017-12-15 15:26:54.0" updateauthor="andkononykhin" updated="2017-12-15 15:30:01.0"> <body><! CDATA Problem reason: - tcp connections are not limited by the app but there is an OS limit for open files that lead to exception when node exhausts the limit because of too many clients are connected - mentioned OS limit (nofile) by default is 1024/4096 (soft/hard) which seems too small in any case  Changes: - increased OS limit for node service to 16K/64K - further improvements are expected in scope of INDY-570 / INDY-1037 tasks  Committed into: -  https://github.com/hyperledger/indy-node/pull/492  -  https://github.com/hyperledger/indy-node/pull/494   Risk factors: - Nothing is expected.  Risk: - Low  Covered with tests: - no  Recommendations for QA: version - indy-node 1.2.242 (master) * the issue is easily reproduced by opening 1000 connections to some node (either by using load test as in description or any kind of tool that can create tcp connection, e.g. telnet) * I think it should be checked for two cases: ## check upgrade ### install sovrin with indy-node <= 1.2.242 and ensure that service is running ### set *DUMP_VALIDATOR_INFO_PERIOD_SEC* to *10* in */usr/local/lib/python3.5/dist-packages/plenum/server/validator_info_tool.py* so the node will dump validator info more frequently (info each 10secs) and will fail faster ### restart indy-node service ### initiate 1000 connection as mentioned upper. You can check how mane connections using *lsof -p <PID> | grep TCP* under indy user (*apt-get install* *lsof* if needed) ### in node's logs (and journalctl) you should see exception the same as in the task description (wait for 10 secs)on clean machine ### do *apt-get install indy-node indy-plenum* and ensure that indy-node is >= 1.2.242 ### do node restart. ### you should see warning like: *Warning: indy-node.service changed on disk. Run 'systemctl daemon-reload' to reload units.* so do the reload ### run restart node command again ### create 1000 connections again - node should work for that time without any errors or crash ### check more connections number: node should work up to 16K (16536) connections ## on clean machine ### install sovrin with indy-node ### do steps 10-11 from previous check: the same results are expected  ></body> </Action>
<Action id="37684" issue="24624" author="vladimirwork" type="comment" created="2017-12-20 15:58:19.0" updateauthor="vladimirwork" updated="2017-12-20 15:58:19.0"> <body><! CDATA Build Info: indy-node 1.2.243  Steps to Validate: 1. Run performance tests with GET_NYM txns on upgraded to 1.2.242+ version (250 threads x 4..6 clients). 2. Run performance tests with GET_NYM txns on installed 1.2.242+ version (250 threads x 4..6 clients).  Actual Results: Pool works normally with at least 1500 simultaneous clients doing GET_NYM reads. Results with more simultaneous clients are unclear because of spontaneous threads' denials on AWS clients and issues with libindy that throws errors due to big number of requests run but there is no `OSError:  Errno 24  Too many open files` error found in nodes' logs or journalctl on any node. Test case with 16k simultaneous connections can be checked on larger AWS pool if needed in scope of new ticket.   ></body> </Action>
