<Action id="46925" issue="31171" author="sergey.khoroshavin" type="comment" created="2018-07-04 17:57:19.0" updateauthor="sergey.khoroshavin" updated="2018-07-05 11:24:31.0"> <body><! CDATA Logs investigation showed that: 1) there were multiple node restarts due to unhandled exceptions (which are now fixed in plenum starting from *1.4.429*,  PR|https://github.com/hyperledger/indy-plenum/pull/785 ) 2) these restarts triggered multiple edge cases (will be described as a separate comment) 3) some nodes stopped ordering simply because of insufficient disk space 4) no ledger was corrupted, which means if all nodes were restarted and given some time to catch up they would end up in same state finally. Also if another view change happened lagging nodes would be able to catch up given that no more restarts happened   ></body> </Action>
<Action id="46937" issue="31171" author="sergey.khoroshavin" type="comment" created="2018-07-05 10:50:17.0" updateauthor="sergey.khoroshavin" updated="2018-07-05 10:59:27.0"> <body><! CDATA What really happened during load test:  19:21  Node1 ran out of disk space and restarted, triggering view change to view 1. All other nodes were able to continue ordering  20:24  Majority of nodes (including master primary Node2) restarted due to unhandled exception. By that time they were able to order 2916 batches in view 1. Nodes 6, 21, 24 and 25 did not restart. From now pool was split into two groups: - restarted majority - entered view 0, quickly found Node1 non responsive, changed view to 1 and started ordering requests from ppSeqNo 1 - intact minority - remained in view 1, found master primary disconnected, tried to trigger view change but failed (they are minority), then found lots of 3pc messages from view 1 that seemed like already ordered but had incorrect digests - which is no surprise since these are new messages from restarted majority  21:23 Restarted majority reached ppSeqNo 2917, which triggered incorrect state trie reports from intact minority, which is also expected. No ledger was corrupted at this point.  22:03 Node2 (as well as Node4) ran out of space and restarted, triggering view change to view 2. Intact minority also entered view 2 and started catchup, which required quite a lot of time since they missed more than 4000 batches. Node6 was able to finish this catchup and continued ordering requests.  22:39 Another restart due to unhandled exception happened on all ordering nodes. It did not happen on Nodes 21, 24 and 25, which were still trying to catchup. After that: - restarted majority (now including Node6) entered view 0, found Node1 dead, changed view to 1, found Node2 dead as well, changed view to 2 and continued ordering new requests - intact minority remained in view 2, tried to change view to 3 due to primary restart but failed, then continued catchup, but since incoming checkpoints shifted low watermarks back to 0 catchup and ordering logic was broken again. These nodes could be restored by either restarting them, restarting whole pool or entering view 3.    There were also a couple of suspicious symptoms: - intact minority were stashing lots of messages (>10000) when they were in view 1, and it seemed like all of them were discarded - when first catchup after view change finished it correctly set low watermark to ~1000 (since it did catchup not only for messages from previous view, but for current view as well), and then this watermark was reset back to 0 as well. This triggered discarding of lots of PREPREPAREs followed by requesting them again through MESSAGE_REQUESTS, and then another catchup  (when CHECKPOINT with much higher watermark was received) which discarded these messages again  ></body> </Action>
<Action id="46945" issue="31171" author="sergey.khoroshavin" type="comment" body="Validation will be done in scope of  INDY-1343|https://jira.hyperledger.org/browse/INDY-1343  and  INDY-1425|https://jira.hyperledger.org/browse/INDY-1425 " created="2018-07-05 11:26:06.0" updateauthor="sergey.khoroshavin" updated="2018-07-05 11:26:06.0"/>
