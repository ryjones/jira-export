<Action id="49997" issue="33549" author="ozheregelya" type="comment" created="2018-09-06 13:19:11.0" updateauthor="ozheregelya" updated="2018-09-06 13:19:11.0"> <body><! CDATA *Test 1:* Steps to Reproduce: 1. Setup the pool of 25 nodes. 2. Run load test with 10 writes/sec and ~1000 reads/sec. {code:java}  2 machines X  python3.5 perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -c 50 -l 0.1 -y freeflow -k " {\"nym\": {\"count\": 4}}, {\"schema\":{\"count\": 1}}, {\"attrib\":{\"count\": 3}}, {\"cred_def\":{\"count\": 1}}, {\"payment\":{\"payment_method\":\"sov\", \"plugin_lib\": \"libsovtoken.so\", \"plugin_init_func\":\"sovtoken_init\"}} "  8 machines X   python3.5 perf_processes.py -m t -n 1 -c 1125 -l 0.1 -g ~/ext_transactions_genesis -k " {\"get_nym\": {\"count\": 9}}, {\"get_attrib\":{\"count\": 2}}, {\"get_schema\":{\"count\": 2}}, {\"get_cred_def\":{\"count\": 2}} "{code} => Reading was failed after several thousands of requests because of problem in load script. => Pool was failed with OOM several times and was broken after the first one. 172347 txns were written on all of the nodes. 3. Restart the pool. => Pool works after restart. 4. Run the same test once more. => Pool was failed with OOM several times and was broken after the first one. ~335414 txns were written.  Actual Results: Similar test with only NYMs {code:java}  2 machines X  python3.5 perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -c 50 -l 0.1 -y freeflow{code} showed that in case of load with all txns pool fails earlier than with only NYMs.  Logs and metrics: s3://qanodelogs/indy-1670-normal-load-2/NodeXX/ To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1670-normal-load-2/ /home/ev/logs/indy-1670-normal-load-2/  ></body> </Action>
<Action id="50064" issue="33549" author="ashcherbakov" type="comment" created="2018-09-07 09:05:36.0" updateauthor="ashcherbakov" updated="2018-09-07 09:05:36.0"> <body><! CDATA There are quite interesting results in INDY-1592, where all incoming requests were intentionally rejected with NACKs (that is during static validation, so that they didn't go to 3PC and consensus). There is no memory leak there at all, and memory consumption is only about 1.6 GB. It means that there is probably no memory leak because of ZMQ or metrics or logs.  Most probably memory leaks are caused by intensive ordering.  ></body> </Action>
<Action id="50078" issue="33549" author="ozheregelya" type="comment" created="2018-09-07 12:54:27.0" updateauthor="ozheregelya" updated="2018-09-10 13:10:01.0"> <body><! CDATA *Test 2:* NYMs only (400K txns written) Strange memory usage during normal load (10 writes/sec, 1000 reads/sec) with only NYMs requests:  !1111.png|thumbnail! At some time memory started growing up faster than before.  Logs and metrics: s3://qanodelogs/indy-1688-normal-load-nyms-strange-memory-usage/NodeXX/ To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1688-normal-load-nyms-strange-memory-usage/ /home/ev/logs/indy-1688-normal-load-nyms-strange-memory-usage/  ></body> </Action>
<Action id="50150" issue="33549" author="ozheregelya" type="comment" created="2018-09-10 13:25:26.0" updateauthor="ozheregelya" updated="2018-09-10 13:25:50.0"> <body><! CDATA *Test 3:* domain txns only, no revocations, no payments. 1638329 txns written. Pool was broken after 45 hours. !Ext-07-09-18-mix-no-pay.png|thumbnail!  At some time memory started growing up faster than before, same as in case with NYMs.  Logs and metrics: s3://qanodelogs/indy-1688-Ext-07-09-18-mix-no-pay/NodeXX/ To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1688-Ext-07-09-18-mix-no-pay/ /home/ev/logs/indy-1688-Ext-07-09-18-mix-no-pay/  ></body> </Action>
<Action id="50526" issue="33549" author="ozheregelya" type="comment" created="2018-09-17 11:08:21.0" updateauthor="ozheregelya" updated="2018-09-17 11:08:21.0"> <body><! CDATA Test 4: domain txns only, no revocations, no payments (same as Test 3, but with new metrics). 2156723 txns were written. Pool was broken after 60 hours. !Live-14-09-18-mix-no-pay.png|thumbnail! Logs and metrics: s3://qanodelogs/indy-1688-Live-14-09-18-mix-no-pay/NodeXX/ To get logs, run following command on log processor machine:  aws s3 cp --recursive s3://qanodelogs/indy-1688-Live-14-09-18-mix-no-pay/ /home/ev/logs/indy-1688-Live-14-09-18-mix-no-pay/  ></body> </Action>
<Action id="50540" issue="33549" author="derashe" type="comment" created="2018-09-17 13:52:52.0" updateauthor="derashe" updated="2018-09-17 13:52:52.0"> <body><! CDATA *Results of testing:*   The above tests showed us similar result: Pool stably write txns until some moment, when memory start growing rapidly and crush the node and the pool. That seems to happen on every node. After researches, we understand, that memory grow correlate with grow of stored requests. We've assumed, that they are not deleting because some backup instance stopping ordering. That is seems to be true, because in every test, we've seen stable work of all backups and right before "memory explosion", one or few replicas stopped ordering.  *Ticket continuation:*  So, we need to implement such a logic, that will detect if replica degraded and turn it off.  Replica deletion mechanism will be implemented in https://jira.hyperledger.org/browse/INDY-1681.  Detect mechanism will be implemented and tested in scope of this ticket, cause it will be part of investigation.  ></body> </Action>
<Action id="50828" issue="33549" author="derashe" type="comment" created="2018-09-20 09:42:49.0" updateauthor="derashe" updated="2018-09-25 06:24:25.0"> <body><! CDATA After discussions with team, we've designed few ways to fix this problem: * turn instance off * make view_change for every instance separately * deleting of old stashed requests  As for now, we've decided to use first option (which already implemented) to resolve one of possible causes of request stashing - backup primary network disconnecting https://jira.hyperledger.org/browse/INDY-1681. That will work for simplest case . For case when primary degraded we need to implement complex detection logic. That will be done in scope of another ticket  ></body> </Action>
<Action id="51084" issue="33549" author="derashe" type="comment" body="Also, we&apos;ve found another reason why requests could stash so much. For now, we clear requests queue only when achieving stable checkpoint. That means that we clear queue when we get at least 100 batches (or even more). 1 batch can contain up to 1000 txns. So, in the wost case we can get more than 100*1000 txns stashed. To resolve that, we can change node behaviour to clear txn from queue right after it was ordered on every replica." created="2018-09-24 11:10:41.0" updateauthor="derashe" updated="2018-09-24 11:10:41.0"/>
<Action id="51127" issue="33549" author="ashcherbakov" type="comment" created="2018-09-24 22:55:58.0" updateauthor="ashcherbakov" updated="2018-09-24 22:55:58.0"> <body><! CDATA  ~Derashe   In Test4 we can see that requests queue doesn't grow (1000 requests all the time), but we still see OutOfMemory.  ></body> </Action>
<Action id="51144" issue="33549" author="derashe" type="comment" created="2018-09-25 06:17:43.0" updateauthor="derashe" updated="2018-09-25 06:53:23.0"> <body><! CDATA  ~ashcherbakov   In that case, most probably, memory timeline shifted to the left. That was a known bug and it was fixed in  https://github.com/hyperledger/indy-node/pull/944   ></body> </Action>
<Action id="51195" issue="33549" author="derashe" type="comment" created="2018-09-26 09:59:09.0" updateauthor="derashe" updated="2018-09-26 10:01:53.0"> <body><! CDATA We diagnose, that if we have load under 20 tps, we are processing every txn in time and we have no memory explosion. But when we raising load up to 30 and more, we can't process txns in time and we start collecting them in our internal queues. This cause memory explosion and inexplicable node behaviour, which can be seen on image below  !metrics.png|thumbnail!  There are: * According to node's logic, requests queue must be higher than moniotor queue. Because requests queue cleaning only with stable checkpoint and monitor queue cleaning smoothly. * Monitor's queue seems to add new requests and delete ordered once smoothly at the beginning ~(14:42 - 14:47) but then monitor stopped deleting them and grow equally with requests queue.  * Latency not stable, while we are ordering requests  ></body> </Action>
<Action id="51220" issue="33549" author="derashe" type="comment" created="2018-09-26 16:31:43.0" updateauthor="derashe" updated="2018-09-26 16:33:47.0"> <body><! CDATA Upward problem was fixed in  https://github.com/hyperledger/indy-plenum/pull/924.   But still we have OOM problem:  !screenshot-2.png|thumbnail!  We can try to debug this problem is such a way: after 30 minutes load test of 40 nym/s stop load for some time (10-20 minutes) and after that resume load for 30 minutes.  Purpose here is to look at memory consumption after stopping load: * if memory will stay on the same level. That means, that we need to search for some stashing collections in node's code * if memory will fall down to adequate values, that means that we need to find safe way to block memory explosion (i.e. block processing of a new requests) in some moment. Also need more ways to optimise node's work (such as https://jira.hyperledger.org/browse/INDY-1649)  ></body> </Action>
<Action id="51223" issue="33549" author="ashcherbakov" type="comment" created="2018-09-26 17:07:23.0" updateauthor="ashcherbakov" updated="2018-09-26 17:07:23.0"> <body><! CDATA In this test we can see that the expected load was 40 txns per sec, while we order about 25-30. So, the rest is probably stashed, and ZMQ queues are growing. That nay explain the memory growth, and is probably expected. What we can also test, is to restrict ZMQ internal queues to some small value, and see if this will help.  ></body> </Action>
<Action id="51334" issue="33549" author="derashe" type="comment" created="2018-09-28 07:20:37.0" updateauthor="derashe" updated="2018-09-28 07:20:37.0"> <body><! CDATA After testing described case we've get such a graph:  !image-2018-09-28-10-17-09-638.png|thumbnail!  We can see that memory stayed on the same level after stopping load. That means that problem is about some collection in code that start grow rapidly, when we get unordered requests.  ></body> </Action>
<Action id="51349" issue="33549" author="derashe" type="comment" created="2018-09-28 12:52:14.0" updateauthor="derashe" updated="2018-09-28 12:52:14.0"> <body><! CDATA According to acceptance criteria, final result are: * There are two discovered cases of memory leak: backup primary disconnecting/degraded and unordered requests storing * Unordered requests significantly affect memory * In case of ordering everything in time, we can see no "memory explosion" * Txn type does not affect memory consumption directly, but more complex txns lower our throughput, so we get more unordered txns (with the same load) and stashing unordered requests faster. * Bug to discover and fix internal collection stashing is created https://jira.hyperledger.org/browse/INDY-1723  ></body> </Action>
