<Action id="57673" issue="35426" author="esplinr" type="comment" body="The behavior of LibIndy 1.8.1 is to exponentially increase the number of nodes that are being queried. The first read-request goes to one node, and if it fails then the follow-up request goes to two nodes, then four, etc. We start with one node in order to minimize load on the ledger." created="2019-02-27 21:58:46.0" updateauthor="esplinr" updated="2019-02-27 21:58:46.0"/>
<Action id="58568" issue="35426" author="vladimirwork" type="comment" created="2019-03-26 14:05:44.0" updateauthor="vladimirwork" updated="2019-03-27 06:33:29.0"> <body><! CDATA Build Info: libindy 1.8.2~64 rc  Actual Results: It looks like we do SendOneRequest twice to the same Node: {noformat} 019-03-26 16:37:20,365 	src/services/pool/networker.rs:62 | sending new request 2019-03-26 16:37:20,365 	src/services/pool/networker.rs:251 | is_active >> time worked: Duration { secs: 0, nanos: 779407831 } 2019-03-26 16:37:20,365 	src/services/pool/networker.rs:253 | is_active << true 2019-03-26 16:37:20,365 	src/services/pool/networker.rs:70 | existing connection unavailable 2019-03-26 16:37:20,365 	src/services/pool/networker.rs:86 | send request in new conn 2019-03-26 16:37:20,365 	src/services/pool/networker.rs:183 | PoolConnection::new: from nodes  RemoteNode { name: "Node3", public_key:  79, 5, 67, 174, 174, 29, 149, 231, 155, 21, 224, 85, 146, 120, 150, 148, 59, 245, 158, 37, 83, 26, 154, 135, 77, 138, 28, 147, 175, 47, 27, 119 , zaddr: "tcp://10.0.0.4:9706", is_blacklisted: false }, RemoteNode { name: "Node1", public_key:  245, 162, 146, 125, 78, 184, 226, 60, 221, 1, 103, 194, 231, 134, 97, 57, 147, 89, 13, 171, 80, 230, 214, 139, 195, 130, 29, 243, 184, 195, 79, 31 , zaddr: "tcp://10.0.0.2:9702", is_blacklisted: false }, RemoteNode { name: "Node5", public_key:  41, 187, 234, 133, 43, 242, 111, 244, 252, 245, 236, 183, 36, 145, 244, 169, 101, 126, 186, 216, 224, 244, 179, 97, 70, 247, 99, 222, 247, 163, 79, 16 , zaddr: "tcp://10.0.0.6:9710", is_blacklisted: false }, RemoteNode { name: "Node4", public_key:  196, 24, 141, 31, 105, 237, 27, 65, 201, 62, 59, 55, 193, 242, 181, 53, 42, 157, 34, 231, 131, 64, 176, 60, 30, 210, 123, 110, 169, 191, 133, 26 , zaddr: "tcp://10.0.0.5:9708", is_blacklisted: false }, RemoteNode { name: "Node7", public_key:  239, 13, 240, 165, 23, 18, 154, 242, 21, 175, 105, 55, 135, 9, 50, 194, 194, 110, 72, 21, 144, 164, 196, 129, 242, 196, 102, 198, 229, 214, 155, 87 , zaddr: "tcp://10.0.0.8:9714", is_blacklisted: false }, RemoteNode { name: "Node6", public_key:  169, 63, 90, 144, 217, 229, 243, 51, 122, 163, 28, 93, 107, 87, 138, 190, 137, 26, 243, 155, 117, 182, 28, 1, 13, 244, 231, 86, 194, 217, 112, 108 , zaddr: "tcp://10.0.0.7:9712", is_blacklisted: false }, RemoteNode { name: "Node2", public_key:  221, 7, 213, 25, 70, 73, 199, 226, 112, 254, 71, 75, 167, 252, 229, 6, 9, 164, 175, 172, 99, 45, 152, 229, 52, 56, 128, 121, 246, 160, 155, 14 , zaddr: "tcp://10.0.0.3:9704", is_blacklisted: false }  2019-03-26 16:37:20,365 	src/services/pool/networker.rs:258 | send_request >> pe: Some(SendOneRequest("{\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"operation\":{\"dest\":\"V4SGRU86Z58d6TV7PBUe6f\",\"type\":\"105\"},\"protocolVersion\":2,\"reqId\":1553607440353089855,\"signature\":\"3iZGeSZNVKFVPxuZHEVGbaPEpij3AGc3ADFpVyKBHbjV1fuFA2GUZSNM7Lrzk4c7XRRJxscqt4N5y5V7xZjWRSUT\"}", "1553607440353089855", 20)) 2019-03-26 16:37:20,366 	src/services/pool/networker.rs:322 | _send_msg_to_one_node >> idx 0, req_id 1553607440353089855, req {"identifier":"V4SGRU86Z58d6TV7PBUe6f","operation":{"dest":"V4SGRU86Z58d6TV7PBUe6f","type":"105"},"protocolVersion":2,"reqId":1553607440353089855,"signature":"3iZGeSZNVKFVPxuZHEVGbaPEpij3AGc3ADFpVyKBHbjV1fuFA2GUZSNM7Lrzk4c7XRRJxscqt4N5y5V7xZjWRSUT"} 2019-03-26 16:37:20,366 	src/services/pool/networker.rs:334 | _get_socket: open new socket for node 0 2019-03-26 16:37:20,366 	src/services/pool/networker.rs:328 | _send_msg_to_one_node << 2019-03-26 16:37:20,366 	src/services/pool/networker.rs:288 | send_request << 2019-03-26 16:37:20,367 	src/services/pool/networker.rs:77 | send request in existing conn 2019-03-26 16:37:20,367 	src/services/pool/networker.rs:258 | send_request >> pe: Some(SendOneRequest("{\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"operation\":{\"dest\":\"V4SGRU86Z58d6TV7PBUe6f\",\"type\":\"105\"},\"protocolVersion\":2,\"reqId\":1553607440353089855,\"signature\":\"3iZGeSZNVKFVPxuZHEVGbaPEpij3AGc3ADFpVyKBHbjV1fuFA2GUZSNM7Lrzk4c7XRRJxscqt4N5y5V7xZjWRSUT\"}", "1553607440353089855", 20)) 2019-03-26 16:37:20,367 	src/services/pool/networker.rs:322 | _send_msg_to_one_node >> idx 0, req_id 1553607440353089855, req {"identifier":"V4SGRU86Z58d6TV7PBUe6f","operation":{"dest":"V4SGRU86Z58d6TV7PBUe6f","type":"105"},"protocolVersion":2,"reqId":1553607440353089855,"signature":"3iZGeSZNVKFVPxuZHEVGbaPEpij3AGc3ADFpVyKBHbjV1fuFA2GUZSNM7Lrzk4c7XRRJxscqt4N5y5V7xZjWRSUT"} 2019-03-26 16:37:20,367 	src/services/pool/networker.rs:328 | _send_msg_to_one_node << 2019-03-26 16:37:20,367 	src/services/pool/networker.rs:288 | send_request << 2019-03-26 16:37:20,386 	src/services/pool/pool.rs:550 | received pool event: Some(NodeReply("{\"reqId\":1553607440353089855,\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"op\":\"REQACK\"}", "Node2")) 2019-03-26 16:37:20,387 	src/services/pool/pool.rs:397 | received reply from node "Node2": "{\"reqId\":1553607440353089855,\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"op\":\"REQACK\"}" 2019-03-26 16:37:20,390 	src/services/pool/pool.rs:550 | received pool event: Some(NodeReply("{\"result\":{\"type\":\"105\",\"state_proof\":{\"root_hash\":\"EvptKRBjtxbuz9FW3C6iQx5vt6EoJdd62ijVi6NKSceK\",\"multi_signature\":{\"signature\":\"R6HsqghVUzYcf9kyhAagf4TGgg52RkGu5UznvHLUe4wmACNEmyiujz8FwNt91pXbEDtRcX9KBJnMEYn4ts3mE6tonLS6FaaDCHYFTSayv4ehViFfFB79gSFhWsQ4K9xddcitYTexrAGWZ8gumAFGm7Dp9RRY5cv3kXJvaoHvaBuyxB\",\"value\":{\"pool_state_root_hash\":\"AyA4Yuk4GkME75JwzJBVCiax61hzhJjVpzHVYC1Gw5sh\",\"state_root_hash\":\"EvptKRBjtxbuz9FW3C6iQx5vt6EoJdd62ijVi6NKSceK\",\"timestamp\":1553607279,\"txn_root_hash\":\"Evvf4EwVXngnGbfgWmR2zYYjF2R1pq1e7ztXjZhfw1mS\",\"ledger_id\":1},\"participants\": \"Node6\",\"Node4\",\"Node3\",\"Node1\",\"Node7\" },\"proof_nodes\":\"+QMq+IGgIL3tL9rLCaNhaLueITfpAXOXuPKY3xpts7IFO2f\\/3IK4XvhcuFp7ImlkZW50aWZpZXIiOm51bGwsInJvbGUiOiIwIiwic2VxTm8iOjEsInR4blRpbWUiOm51bGwsInZlcmtleSI6In5Db1JFUjYzRFZZbldadEs4dUF6TmJ4In34sYCAgICgadHWs8iJ2mDFvlOq8dMok9No7v6chcAGOJZBZFLopcyg8Pafq\\/M+EyCbhK1frtr8huL+S5Mul2JTtIt8zDDFDqyAgKD2II1jfI3jgDcq2Gjho2tgZ\\/WllZg6\\/Uqm7\\/Mx0qW6kKCVea2xZlEF1WQlMBskbxjhlJXGtb58Pwug0ZUSTwjqpKBUT8SJZ2SRZsel1hR3W6igTWoM7NLQA6BW+6C+ERTHRoCAgICAgPkB8aBNV3TK\\/Xl+V5dN+OvVAb526MLsnnX+s+URFDQBmgBU\\/qB+TreFBYNTU7gs3cvtO5xSOwXjpc8+ePMDOqmWvkuF\\/6CjIqfxjX0uplflYRor4SOh2xVl9yVNvK6dSK4cmxg3pKCamro+9EH\\/vrNION29c750BMkCYdpXzasRUbZmgA75MKBk4Y7Qk\\/CVTOYO3NPLFU2ku\\/6RCUFqZvEsl+isz69WdqCvAh2B3TiFbx674DQZy\\/DF6Alki7FwS8tN8HKl7LEOjKDsb7g9\\/KXsLKRfsdCEDOtgj76LhuhhsuKHZCiSW4lfmaBae72f+ASD6vaGVq0h5q+lp8qCRwqaW6eJWyMKLnvTE6DcSopVzBN7Uw+83ixA1wmKNAkUKDR9LJDx47\\/ZDXAqqICgE51qiSZpF8xf9k1wCs+vcqZ8rUXoqNrzfG\\/x59OxCc+g7Z2rWOtEcBK0RfRHkSv9oGOKeszJrY+iJk8dEmdYOKWg5pMyVdol04vYx9blJyXBzMwuT2mY+ridNjzocWnwe7KgSPFxE7GIAdYVFQHY6GY9m5Fi6UqwOZuEC3Vx+Yrja9GgG6BzHSqYPnFTV5lWowbE46w9sZ4UcCA1LewmFX9SOaKgrRePjX+otLhtN1zoq2L2aEFbQIQcHjTeXK4AvY7Shb2A\"},\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"data\":\"{\\\"dest\\\":\\\"V4SGRU86Z58d6TV7PBUe6f\\\",\\\"identifier\\\":null,\\\"role\\\":\\\"0\\\",\\\"seqNo\\\":1,\\\"txnTime\\\":null,\\\"verkey\\\":\\\"~CoRER63DVYnWZtK8uAzNbx\\\"}\",\"seqNo\":1,\"reqId\":1553607440353089855,\"dest\":\"V4SGRU86Z58d6TV7PBUe6f\",\"txnTime\":null},\"op\":\"REPLY\"}", "Node2")) 2019-03-26 16:37:20,390 	src/services/pool/pool.rs:397 | received reply from node "Node2": "{\"result\":{\"type\":\"105\",\"state_proof\":{\"root_hash\":\"EvptKRBjtxbuz9FW3C6iQx5vt6EoJdd62ijVi6NKSceK\",\"multi_signature\":{\"signature\":\"R6HsqghVUzYcf9kyhAagf4TGgg52RkGu5UznvHLUe4wmACNEmyiujz8FwNt91pXbEDtRcX9KBJnMEYn4ts3mE6tonLS6FaaDCHYFTSayv4ehViFfFB79gSFhWsQ4K9xddcitYTexrAGWZ8gumAFGm7Dp9RRY5cv3kXJvaoHvaBuyxB\",\"value\":{\"pool_state_root_hash\":\"AyA4Yuk4GkME75JwzJBVCiax61hzhJjVpzHVYC1Gw5sh\",\"state_root_hash\":\"EvptKRBjtxbuz9FW3C6iQx5vt6EoJdd62ijVi6NKSceK\",\"timestamp\":1553607279,\"txn_root_hash\":\"Evvf4EwVXngnGbfgWmR2zYYjF2R1pq1e7ztXjZhfw1mS\",\"ledger_id\":1},\"participants\": \"Node6\",\"Node4\",\"Node3\",\"Node1\",\"Node7\" },\"proof_nodes\":\"+QMq+IGgIL3tL9rLCaNhaLueITfpAXOXuPKY3xpts7IFO2f\\/3IK4XvhcuFp7ImlkZW50aWZpZXIiOm51bGwsInJvbGUiOiIwIiwic2VxTm8iOjEsInR4blRpbWUiOm51bGwsInZlcmtleSI6In5Db1JFUjYzRFZZbldadEs4dUF6TmJ4In34sYCAgICgadHWs8iJ2mDFvlOq8dMok9No7v6chcAGOJZBZFLopcyg8Pafq\\/M+EyCbhK1frtr8huL+S5Mul2JTtIt8zDDFDqyAgKD2II1jfI3jgDcq2Gjho2tgZ\\/WllZg6\\/Uqm7\\/Mx0qW6kKCVea2xZlEF1WQlMBskbxjhlJXGtb58Pwug0ZUSTwjqpKBUT8SJZ2SRZsel1hR3W6igTWoM7NLQA6BW+6C+ERTHRoCAgICAgPkB8aBNV3TK\\/Xl+V5dN+OvVAb526MLsnnX+s+URFDQBmgBU\\/qB+TreFBYNTU7gs3cvtO5xSOwXjpc8+ePMDOqmWvkuF\\/6CjIqfxjX0uplflYRor4SOh2xVl9yVNvK6dSK4cmxg3pKCamro+9EH\\/vrNION29c750BMkCYdpXzasRUbZmgA75MKBk4Y7Qk\\/CVTOYO3NPLFU2ku\\/6RCUFqZvEsl+isz69WdqCvAh2B3TiFbx674DQZy\\/DF6Alki7FwS8tN8HKl7LEOjKDsb7g9\\/KXsLKRfsdCEDOtgj76LhuhhsuKHZCiSW4lfmaBae72f+ASD6vaGVq0h5q+lp8qCRwqaW6eJWyMKLnvTE6DcSopVzBN7Uw+83ixA1wmKNAkUKDR9LJDx47\\/ZDXAqqICgE51qiSZpF8xf9k1wCs+vcqZ8rUXoqNrzfG\\/x59OxCc+g7Z2rWOtEcBK0RfRHkSv9oGOKeszJrY+iJk8dEmdYOKWg5pMyVdol04vYx9blJyXBzMwuT2mY+ridNjzocWnwe7KgSPFxE7GIAdYVFQHY6GY9m5Fi6UqwOZuEC3Vx+Yrja9GgG6BzHSqYPnFTV5lWowbE46w9sZ4UcCA1LewmFX9SOaKgrRePjX+otLhtN1zoq2L2aEFbQIQcHjTeXK4AvY7Shb2A\"},\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"data\":\"{\\\"dest\\\":\\\"V4SGRU86Z58d6TV7PBUe6f\\\",\\\"identifier\\\":null,\\\"role\\\":\\\"0\\\",\\\"seqNo\\\":1,\\\"txnTime\\\":null,\\\"verkey\\\":\\\"~CoRER63DVYnWZtK8uAzNbx\\\"}\",\"seqNo\":1,\"reqId\":1553607440353089855,\"dest\":\"V4SGRU86Z58d6TV7PBUe6f\",\"txnTime\":null},\"op\":\"REPLY\"}" {noformat}  Expected Results: We should send requests to 2 different nodes.  ></body> </Action>
<Action id="58611" issue="35426" author="vladimirwork" type="comment" created="2019-03-27 06:33:01.0" updateauthor="vladimirwork" updated="2019-03-27 06:33:01.0"> <body><! CDATA Build Info: libindy 1.8.1~1043 master  Actual Results: There are 2 requests sent to 2 different nodes. {noformat} 2019-03-27 09:29:23,220 	src/services/pool/networker.rs:86 | send request in new conn 2019-03-27 09:29:23,220 	src/services/pool/networker.rs:183 | PoolConnection::new: from nodes  RemoteNode { name: "Node5", public_key:  41, 187, 234, 133, 43, 242, 111, 244, 252, 245, 236, 183, 36, 145, 244, 169, 101, 126, 186, 216, 224, 244, 179, 97, 70, 247, 99, 222, 247, 163, 79, 16 , zaddr: "tcp://10.0.0.6:9710", is_blacklisted: false }, RemoteNode { name: "Node7", public_key:  239, 13, 240, 165, 23, 18, 154, 242, 21, 175, 105, 55, 135, 9, 50, 194, 194, 110, 72, 21, 144, 164, 196, 129, 242, 196, 102, 198, 229, 214, 155, 87 , zaddr: "tcp://10.0.0.8:9714", is_blacklisted: false }, RemoteNode { name: "Node1", public_key:  245, 162, 146, 125, 78, 184, 226, 60, 221, 1, 103, 194, 231, 134, 97, 57, 147, 89, 13, 171, 80, 230, 214, 139, 195, 130, 29, 243, 184, 195, 79, 31 , zaddr: "tcp://10.0.0.2:9702", is_blacklisted: false }, RemoteNode { name: "Node3", public_key:  79, 5, 67, 174, 174, 29, 149, 231, 155, 21, 224, 85, 146, 120, 150, 148, 59, 245, 158, 37, 83, 26, 154, 135, 77, 138, 28, 147, 175, 47, 27, 119 , zaddr: "tcp://10.0.0.4:9706", is_blacklisted: false }, RemoteNode { name: "Node2", public_key:  221, 7, 213, 25, 70, 73, 199, 226, 112, 254, 71, 75, 167, 252, 229, 6, 9, 164, 175, 172, 99, 45, 152, 229, 52, 56, 128, 121, 246, 160, 155, 14 , zaddr: "tcp://10.0.0.3:9704", is_blacklisted: false }, RemoteNode { name: "Node6", public_key:  169, 63, 90, 144, 217, 229, 243, 51, 122, 163, 28, 93, 107, 87, 138, 190, 137, 26, 243, 155, 117, 182, 28, 1, 13, 244, 231, 86, 194, 217, 112, 108 , zaddr: "tcp://10.0.0.7:9712", is_blacklisted: false }, RemoteNode { name: "Node4", public_key:  196, 24, 141, 31, 105, 237, 27, 65, 201, 62, 59, 55, 193, 242, 181, 53, 42, 157, 34, 231, 131, 64, 176, 60, 30, 210, 123, 110, 169, 191, 133, 26 , zaddr: "tcp://10.0.0.5:9708", is_blacklisted: false }  2019-03-27 09:29:23,221 	src/services/pool/networker.rs:258 | send_request >> pe: Some(SendOneRequest("{\"identifier\":\"V4SGRU86Z58d6TV7PBUe6f\",\"operation\":{\"dest\":\"V4SGRU86Z58d6TV7PBUe6f\",\"type\":\"105\"},\"protocolVersion\":2,\"reqId\":1553668163213511759,\"signature\":\"4JATFeLU2AcSegG67v9jqtvyLh5vdZKFE4d8Z3o51aWUBbcBE9t3VDCfS34YP2BTezb87V981iWZmdSsZnxY9s6K\"}", "1553668163213511759", 20)) 2019-03-27 09:29:23,221 	src/services/pool/networker.rs:322 | _send_msg_to_one_node >> idx 0, req_id 1553668163213511759, req {"identifier":"V4SGRU86Z58d6TV7PBUe6f","operation":{"dest":"V4SGRU86Z58d6TV7PBUe6f","type":"105"},"protocolVersion":2,"reqId":1553668163213511759,"signature":"4JATFeLU2AcSegG67v9jqtvyLh5vdZKFE4d8Z3o51aWUBbcBE9t3VDCfS34YP2BTezb87V981iWZmdSsZnxY9s6K"} 2019-03-27 09:29:23,221 	src/services/pool/networker.rs:334 | _get_socket: open new socket for node 0 2019-03-27 09:29:23,221 	src/services/pool/networker.rs:328 | _send_msg_to_one_node << 2019-03-27 09:29:23,221 	src/services/pool/networker.rs:288 | send_request << 2019-03-27 09:29:23,221 	src/services/pool/networker.rs:77 | send request in existing conn 2019-03-27 09:29:23,221 	src/services/pool/networker.rs:258 | send_request >> pe: Some(Resend("1553668163213511759", 20)) 2019-03-27 09:29:23,222 	src/services/pool/networker.rs:322 | _send_msg_to_one_node >> idx 1, req_id 1553668163213511759, req {"identifier":"V4SGRU86Z58d6TV7PBUe6f","operation":{"dest":"V4SGRU86Z58d6TV7PBUe6f","type":"105"},"protocolVersion":2,"reqId":1553668163213511759,"signature":"4JATFeLU2AcSegG67v9jqtvyLh5vdZKFE4d8Z3o51aWUBbcBE9t3VDCfS34YP2BTezb87V981iWZmdSsZnxY9s6K"} 2019-03-27 09:29:23,222 	src/services/pool/networker.rs:334 | _get_socket: open new socket for node 1 2019-03-27 09:29:23,222 	src/services/pool/networker.rs:328 | _send_msg_to_one_node << 2019-03-27 09:29:23,222 	src/services/pool/networker.rs:288 | send_request << {noformat}   ></body> </Action>
<Action id="62680" issue="35426" author="esplinr" type="comment" body="Because the timeout is configurable, we will waive the requirement of making a configuration option for the number of nodes to be queried." created="2019-08-07 13:15:09.0" updateauthor="esplinr" updated="2019-08-07 13:15:09.0"/>
<Action id="62685" issue="35426" author="mgbailey" type="comment" body=" ~esplinr  I thought I heard in yesterday&apos;s meeting that the number of nodes queried is configurable. Is this incorrect? It is fixed at 2? Where is the configuration for the timeout documented, since the default of 20 seconds is completely ridiculous?" created="2019-08-07 14:04:40.0" updateauthor="mgbailey" updated="2019-08-07 14:04:40.0"/>
<Action id="69358" issue="35426" author="esplinr" type="comment" created="2020-05-29 23:57:25.0" updateauthor="esplinr" updated="2020-05-29 23:59:13.0"> <body><! CDATA The number of nodes and the timeout are configurable. See "timeout" and "number_read_nodes" here:   https://github.com/hyperledger/indy-sdk/blob/master/docs/configuration.md   ></body> </Action>
