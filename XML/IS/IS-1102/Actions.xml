<Action id="54015" issue="35716" author="slafranca" type="comment" body="Libvcx log can be found in:  https://drive.google.com/drive/u/0/folders/16z7PJftCejKR57nnGbIXLK7vP4G4CBqW" created="2018-11-29 15:57:40.0" updateauthor="slafranca" updated="2018-11-29 15:57:40.0"/>
<Action id="54485" issue="35716" author="kithat" type="comment" created="2018-12-07 10:24:03.0" updateauthor="kithat" updated="2018-12-07 10:29:21.0"> <body><! CDATA  Looked through the logs in the archive. That's what I found out: {code:java} 11778 WARN|2018-11-28T23:26:23.224+0000 | src/utils/libindy/error_codes.rs:24 | indy-sdk error code: 307  11779 WARN|2018-11-28T23:26:23.224+0000 | src/api/credential_def.rs:86 | vcx_credential_def_create_cb(command_handle: 0, rc: 1035-Unknown libindy error, credentialdef_handle: 0), source_id: "" 11780 INFO|2018-11-28T23:26:23.224+0000 | src/api/vcx.rs:216 | vcx_error_message(error_code: 1035) {code} 307 stands for ledger timeout. Seems like you got timed out for a big request. Could you please retry it ones again and if you have the same result send logs from node? That what is needed for investigation. And we need your indy-node version.  ></body> </Action>
<Action id="54576" issue="35716" author="kithat" type="comment" body="Reproduced your case in clean libindy environment. Seems like the request was too big for node and it sent the reject without reqid. So it is sounds more like an issue for indy-node than indy-sdk.  ~ashcherbakov  what will you say?" created="2018-12-10 13:12:30.0" updateauthor="kithat" updated="2018-12-10 13:12:30.0"/>
<Action id="54645" issue="35716" author="ashcherbakov" type="comment" created="2018-12-11 07:59:30.0" updateauthor="ashcherbakov" updated="2018-12-11 07:59:30.0"> <body><! CDATA Let me proivide some context here: 1) There is a validation on the Ledger for a maximum size of a client request (transaction) 2) We do not parse JSON for txns not passing this validation, so can not get request id in a REJECT to the client. But libiundy uses reqId to map rejects to requests, so libindy can not understand that reject was sent for this particualr txn, and it leads to timeout. 3) The biggest txn is CLAIM_DEF since it contains a public key which is big. 4) So, according to the current max txn size settings on the ledger, CLAIM_DEF for a SCHEMA with ~150 attributes pass validation. 5) SCHEMA txns are not so big, and we can creates SCHEMAs for thousands of attruibutes. However, the corresponding CLAIM_DEFs will not pass the validation, and it ay be surprising, especially if SCHEMA is sent by another organization.  *Possible Options on how to fix it*: *O1: Fix in Ledger:* Get reqId from requests not passing max size validation, and send Rejects with reqId Pros: - correct error handling on libindy size Cons: - we will have to somehow parse huge txns to get reqId. If the serialization format will be binary (MsgPack for example), it may be a problem, if we don't want to deserialize the whole huge msg, - If we got such a REJECT for CLAIM_DEF, it means that a new SCHEMA needs to be created with less atrributes there. If SCHEMA was created by aniother organization it can be a problem.  *O2: Fix in libindy* Do the same checks for max size as Node does before sending a txn Pros: - correct error handling on libindy size - no need to parse huge txn on the Ledger side Cons: - The same problem with huge CLAIM_DEFs for already posted SCHEMAs  *O3: add validation for max number of attributes in SCHEMA* In order to get rid of the Cons mentioned above, it may be better to not allow posting SCHEMAs for which we will have huge CLAIM_DEFS for sure. It may also require changing max txn size.   We think that O3 is the options we should go with. We just need to understand what is a reasonable number of attributes.  ></body> </Action>
<Action id="54860" issue="35716" author="kithat" type="comment" body="We have chosen to set a limit on number of attributes in schema on indy-node side so we can count this as done." created="2018-12-17 09:41:30.0" updateauthor="kithat" updated="2018-12-17 09:41:30.0"/>
