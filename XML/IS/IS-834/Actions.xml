<Action id="47785" issue="32126" author="sergey.minaev" type="comment" created="2018-07-25 10:07:08.0" updateauthor="sergey.minaev" updated="2018-07-25 10:07:08.0"> <body><! CDATA Here is a summary for IS-601 IS-603 and this one.  Changes: - open pool call now consumes config parameter with follow items - - timeout  - default timeout for network request - - extended_timeout - second timeout, used in 2 cases: if for some request libindy recieve ACK from node it will wait additional extended_timeout seconds. And receiving ledger status before catch-up wait extended_timeout from start - - preordered_nodes - array of nodes aliases which will be used for priority sending requests  Build version with all 3 ticket fixes: 1.5.0 master 646  ></body> </Action>
<Action id="47786" issue="32126" author="sergey.minaev" type="comment" body="PR with fix https://github.com/hyperledger/indy-sdk/pull/978" created="2018-07-25 10:11:02.0" updateauthor="sergey.minaev" updated="2018-07-25 10:11:02.0"/>
<Action id="47834" issue="32126" author="ozheregelya" type="comment" created="2018-07-25 22:18:45.0" updateauthor="ozheregelya" updated="2018-07-25 22:18:45.0"> <body><! CDATA First round of load testing was performed with changed timeout.  As the intermediate results it can be said that load test works better with new values of timeouts. For instance, in case of libindy 1.5.0~644 several hundreds txns were failed with timeout error (but actually were written), in case of libindy 1.5.0~648 no txns were failed with timeout. Version of libindy was the only difference in these tests. For details see  Load and Performance|https://docs.google.com/spreadsheets/d/1DTjDsLSysFBiKU-9z4-IzunJk4wEy44hE_PGZYxnN_8/edit#gid=1813415708  spreadsheet (yellow section "_indy-node_ _1.5.67 RC | libindy 1.5.0~610 vs libindy 1.5.0~640+ (after connection changes)_").  ></body> </Action>
<Action id="48270" issue="32126" author="ozheregelya" type="comment" created="2018-08-01 21:41:16.0" updateauthor="ozheregelya" updated="2018-08-01 21:41:16.0"> <body><! CDATA Environment: libindy 1.6.1~659 indy-node 1.5.534  Case 1: Steps to Validate: 1. Setup the pool. 2. Start the load test with custom pool config (-p "\{\"timeout\": 50, \"extended_timeout\": 150}"). 3. Stop more than F nodes.  Actual Results: Timeout error returns after $(extended_timeout) seconds.  Case 2: Steps to Validate: 1. Setup the pool. 2. Start the load test with custom pool config (-p "\{\"timeout\": 50, \"extended_timeout\": 150}"). 3. During test running start traffic_shaping script on all nodes.  Actual Results: Timeout error returns after $(timeout) seconds.  Case 3: Steps to Validate: 1. Setup the pool. 2. Start the load test with custom pool config (-p "\{\"preordered_nodes\":  \"Node1\", \"Node6\" }"). 3. Stop the first node from preordered_nodes list. => Requests are send to the second node from the list after timeout from the first one. 4. Stop the second node from preordered_nodes list. => Requests are send to the random node if all nodes in list were not answered during timeout.  Actual Results: Preordered_nodes list works.  Additional Information: Ticket for default values: IS-859.  ></body> </Action>
