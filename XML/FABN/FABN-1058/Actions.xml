<Action id="54779" issue="36046" author="suryalnvs" type="comment" body="Tried with the fix  https://gerrit.hyperledger.org/r/28082  but still be able to reproduce the issue." created="2018-12-13 15:44:15.0" updateauthor="suryalnvs" updated="2018-12-13 15:44:15.0"/>
<Action id="54782" issue="36046" author="harrisob@us.ibm.com" type="comment" created="2018-12-13 20:46:48.0" updateauthor="harrisob@us.ibm.com" updated="2018-12-13 20:46:48.0"> <body><! CDATA Even thought the peer is back on line, it is not able to be found by gRPC. One theory is that gRPC still has the peer at the old IP address and the peer has reconnected with a new address.  The handler will attempt to have gRPC close everything out after an error so that if it has to reconnect later it will be forced to find the peer's new address.  ></body> </Action>
<Action id="54914" issue="36046" author="denyeart" type="comment" created="2018-12-18 11:07:56.0" updateauthor="denyeart" updated="2018-12-18 11:07:56.0"> <body><! CDATA A few fixes to 'reset' gRPC have been attempted:  https://gerrit.hyperledger.org/r/#/q/FABN-1058   But still the client seems to be trying to connect to the restarted peer container at its 'old' IP address rather than the 'new' IP address. Perhaps there is caching of DNS name to IP address mapping somewhere deep in gRPC layer, that is not getting reset on subsequent calls to the peer.  ></body> </Action>
<Action id="55074" issue="36046" author="scottz" type="comment" created="2018-12-20 19:38:28.0" updateauthor="scottz" updated="2018-12-20 19:38:28.0"> <body><! CDATA Note:  This issue deals with peers. Refer also to FABN-1083 (problems reconnecting to orderers).  We have one idea to enhance SDK which might help in most scenarios when at least one other peer from the original layout is still available. Consider: as long as the recovered peer reconnects to the other peers via BOOTSTRAP, then they would know its new ip address. We should confirm in peer logs that that is happening at least. Recall: the SDK knows a list of peers in the current layout (which contained the restarted peer, and probably others), so it could contact any of them to request a new layout if the configured "discovery peer" is non-responsive. Then as long as _all_ the peers don't restart and change ip addresses within a 5 minute discover-cache interval, the SDK could find them again (say within 10 minutes). But this enhancement would not be a perfect fix, so we would still require a way for sdk to flush its cache to force it to retrieve new addresses from dns server. It sounds like Bret tried to do that already but it is not working as expected.  Some similar issues were fixed in grpc 1.10 (https://github.com/grpc/grpc/issues/14097), and more in 1.16 and 1.17.  ~suryalnvs  tested with 1.14 (current fabric-client supported version) and latest 1.17, but still seeing same problem.  ></body> </Action>
<Action id="55079" issue="36046" author="scottz" type="comment" created="2018-12-20 20:50:19.0" updateauthor="scottz" updated="2018-12-20 20:50:19.0"> <body><! CDATA Higher level question -  ~harrisob@us.ibm.com  have you confirmed whether the newer 1.4 sdk code is working? We are running system test, but I imagine you might have UT for this. When the discovery layout gets older than the discovery-cache-life expiry age, have you confirmed that the sdk code actually requests a new layout correctly? (If that is NOT happening, then of course the layout in the sdk would never be updated. If that IS happening correctly, then we can continue to investigate grpc cacheing and dns cacheing and forcing grpc to request a new ip address from dns.)  Question 2: Does sdk flush the layouts when it requests a new layout? Why? Why don't we just continue to use same layout until we _receive_ a new layout? That way, our traffic would continue ok in the case where the "discovery peer" restarted but none others did.  ></body> </Action>
<Action id="55081" issue="36046" author="scottz" type="comment" body="When  ~dongming  executed https://jira.hyperledger.org/browse/FAB-10191 in v1.4 in early December, it passed in a single vLaunch environment using docker. He said the IP addresses of the peers did not change, so there was not problem with traffic. That would make it difficult to reproduce the problem on single desktop, but maybe  ~harrisob@us.ibm.com  can find a way to use docker differently." created="2018-12-20 21:19:03.0" updateauthor="scottz" updated="2018-12-20 21:19:03.0"/>
<Action id="55165" issue="36046" author="harrisob@us.ibm.com" type="comment" created="2018-12-28 23:54:28.0" updateauthor="harrisob@us.ibm.com" updated="2018-12-28 23:54:28.0"> <body><! CDATA Tested by starting and stopping the peer in a single docker-compose running on my desktop without any issues.  The node application was running natively on the system and the fabric network was running within the virtual environment. Testing included stopping the discovery peer and seeing the no discovery results error and then starting the peer and seeing everything run.     ></body> </Action>
<Action id="55166" issue="36046" author="harrisob@us.ibm.com" type="comment" body="We can not use old layouts as they may contain peers that should no longer see the proposal." created="2018-12-28 23:59:40.0" updateauthor="harrisob@us.ibm.com" updated="2018-12-28 23:59:40.0"/>
<Action id="55255" issue="36046" author="scottz" type="comment" body="The discovery-cache-life expiry is merely a trigger to request a new layout &quot;because it has been awhile&quot;. It is not an indicator that the current layout is bad, so there is no reason you should stop using it when you request a new layout. Stop using it only when you RECEIVE a new layout." created="2019-01-03 15:10:04.0" updateauthor="scottz" updated="2019-01-03 15:10:04.0"/>
<Action id="55260" issue="36046" author="harrisob@us.ibm.com" type="comment" body="Sure we can do that,  ~mastersingh24 ,  ~denyeart  please say that it is OK to use discovery results that are old and not tell the caller that we were unable to get fresh discovery results at the time the caller requested the discover results to be refreshed." created="2019-01-03 16:57:17.0" updateauthor="harrisob@us.ibm.com" updated="2019-01-03 16:57:17.0"/>
<Action id="55314" issue="36046" author="dongming" type="comment" created="2019-01-04 19:05:44.0" updateauthor="dongming" updated="2019-01-04 19:34:56.0"> <body><! CDATA A suggestion to improve the discovery mechanism:  Once the very first init discovery is completed, SDK has the information of the entire network.  If one peer is not available at the time of init discovery later, SDK can choose a peer from the existing layout to execute discovery.  By this, we can have a robustic discovery method.     Note that Surya has used both IP address and name address for test and observed the same result.  ></body> </Action>
<Action id="55343" issue="36046" author="denyeart" type="comment" created="2019-01-05 16:40:45.0" updateauthor="denyeart" updated="2019-01-05 16:40:45.0"> <body><! CDATA My opinion is that client should NOT arbitrarily pick a peer for subsequent service discovery calls based on results of prior service discovery calls.  The client may want to target specific peers that are more trusted as the target for service discovery, for example their own org peers, rather than using an arbitrary peer from the network.  Therefore my proposal (for v1.4.1, 2.0.0) is that it should be possible to configure a list of peers that SDK should utilize for service discovery.  Currently the SDK user can only configure a single peer for service discovery use.  What do you think  ~mastersingh24   ~yacovm   ~odowdaibm ?  Note, we still need to understand the root cause of why SDK client can't connect to a restarted peer.  My proposal is orthogonal and would only be an incremental improvement for the current problem (would only help if a subset of the configured peers were restarted).  ></body> </Action>
<Action id="55344" issue="36046" author="yacovm" type="comment" created="2019-01-05 17:13:15.0" updateauthor="yacovm" updated="2019-01-05 17:13:15.0"> <body><! CDATA {quote}Currently the SDK user can only configure a single peer for service discovery use.{quote}  Really? Why is that?  Are you sure that's really the case? I don't know much about SDK but you have multiple peers  here|https://github.com/hyperledger/fabric-samples/blob/release-1.3/balance-transfer/artifacts/network-config.yaml ... can't you add a discovery role to all of them?  Another thing I don't understand is: {quote}5. Take down one of the peer (peer1-org1) from org1, it is expected to switch to second peer for the transactions and it switched after like 90 or 100 seconds {quote} Why does it take 90 seconds? you have a cached endorsement descriptor from which you compute all combinations of peers... you should select a combination, and try to send proposals. One of the peers is down? all right, then select the next combination, and so on and so forth, until you run out of combinations... Why does it take 90 seconds?     {quote}The discovery-cache-life expiry is merely a trigger to request a new layout "because it has been awhile". It is not an indicator that the current layout is bad, so there is no reason you should stop using it when you request a new layout. Stop using it only when you RECEIVE a new layout.{quote} I don't understand what triggers requesting a new layout though....  is it just time? If so then Scott is right - if you can't get a new layout from some reason, continue using the old one - It's better than nothing.  {quote}Sure we can do that, Gari Singh, David Enyeart please say that it is OK to use discovery results that are old and not tell the caller that we were unable to get fresh discovery results at the time the caller requested the discover results to be refreshed.{quote} By the way, during the time frame of 1.2,  we added a special event|https://jira.hyperledger.org/browse/FAB-9544  for a chaincode upgrade.  I opened a  JIRA|https://jira.hyperledger.org/browse/FABN-841  for that... seems like only Rick did that, and I think this can be a nice addition to the SDK lifecycle.  {quote}Note, we still need to understand the root cause of why SDK client can't connect to a restarted peer.{quote} If we can get our hands on a traffic recording file via *tcpdump* , we can open it and analyze the traffic and see what was going on. Also, logs from the peer that is used as a discovery server via *docker logs <container> | grep discovery | grep -v gossip* would be nice, to see what the peer gets and what it replies with.   ></body> </Action>
<Action id="55380" issue="36046" author="harrisob@us.ibm.com" type="comment" body="Applications are free to reinitialize the channel and provide a new peer at anytime, especially if the applications notices an issue with the endorsements based on discovery. I will update the discovery tutorial to highlight this. The application developers are also able to write their own endorsement handlers if they do not like the basic discovery handler included. With NodeSDK 1.4.1 we will support &quot;targets&quot; along with &quot;target&quot; when initializing a channel. Note that the &quot;target&quot; parameter was part of the initialize API prior to the additions of discovery providing the trusted peer to be queried. This does cross a dangerous boundary of the SDK deciding good and bad calls. I will need some guidance on what is a failed call to the peer to have it then try the next peer on the list. " created="2019-01-07 18:33:52.0" updateauthor="harrisob@us.ibm.com" updated="2019-01-07 18:33:52.0"/>
<Action id="62413" issue="36046" author="harrisob@us.ibm.com" type="comment" body="Not able to reproduce" created="2019-07-30 17:56:13.0" updateauthor="harrisob@us.ibm.com" updated="2019-07-30 17:56:13.0"/>
<Action id="62469" issue="36046" author="scottz" type="comment" created="2019-07-31 15:47:43.0" updateauthor="scottz" updated="2019-07-31 15:47:43.0"> <body><! CDATA Leaning towards agreeing with closure, based on this summary of relevant points. However, there are still a couple things that could improve recovery and I request opinions on whether to pursue those.  # We verified that with appropriate settings in our k8s network, the client is able to reconnect to peer. To be clear: the initial difficulties we were having was because the peers came back up after restart with new IP addresses. Using LoadBalancer instead of NodePort setting helped by essentially putting a static IP in front of each peer which could be used for the address. This of course helps avoid the other recovery related problems we have been discussing because we could reach that original target peer. However, it does not solve the situation when that peer is unreachable for an extended period or even removed permanently. Fortunately, using a list of targets, rather than single peer target for discovery, helps alleviate the problem when that target peer is unreachable. # Because of the previous point, this one is not as critical - although I still feel it is an issue that could use some attention. In the case where all target peers are unreachable, the admins would need to recognize the situation and reinitiate discoveries for each with different peers. I don't agree with the stated reason provided _"We can not use old layouts as they may contain peers that should no longer see the proposal."_ why my request to have SDK send discovery requests to other "known" peers (instead of just the listed target peers) was rejected. The list of acceptable peers would not change dynamically and magically. If some peers should no longer see the proposals, then that would be because the admins reinstantiated the chaincode with different orgs/peers, at which time they would need to remove those and select other acceptable peers for discovery. # It was indicated that SDK does try a subsequent layout after a 90 secs. I don't like how long this takes, but that is not the focus of this bug and thus should be discussed separately if people still feel this could be investigated and improved. _I imagine this may be based on message timeout and/or connection timeouts, so it may be reasonable to expect up to something like 30 secs, but that is just a thought._ Does anyone else feel this should be further investigated? # That discovery-cache-life timer can be used to automatically request a new layout, default every 5 minutes. As Bret suggested, clients can also request a new layout anytime if they put in code to recognize patterns such as successive transactions getting rejected or not endorsed. #  Finally, this one earlier question was not answered, i.e., if you can't get a new layout for some reason (whether the request was ad-hoc or due to that 5-minute timer expiry triggering a new request), could we continue using the old one? Is it better than doing nothing and rejecting all transactions? I feel it is, and that we can expect the list of peers in the layout is still valid until an instantiate/update is made to alter them. Hoping  ~denyeart  or  ~mastersingh24  can weigh in on whether it is OK for  ~harrisob@us.ibm.com  to have the the SDK use discovery results that are old and not tell the caller that we were unable to get fresh discovery results at the time the caller requested the discovery results to be refreshed.  ></body> </Action>
