<Action id="40926" issue="28040" author="benoit.razet" type="comment" body="I noticed something that might be important. In the tests, it is always validator 1 and 3 that struggle to catch up or reach consensus. Whereas validators 0, 2 and 4 seem to be okay. I noticed that in the toml file configuring the validators the former are using the *serial scheduler*, whereas the latter are using the *parallel scheduler*. Therefore, there may be a bug in the serial scheduler." created="2018-02-27 14:17:03.0" updateauthor="benoit.razet" updated="2018-02-27 14:17:03.0"/>
<Action id="40933" issue="28040" author="benoit.razet" type="comment" body="Setting up all the validators with the parallel scheduler does not trigger the bug, therefore the serial scheduler might have some issues" created="2018-02-27 15:11:48.0" updateauthor="benoit.razet" updated="2018-02-27 15:11:48.0"/>
<Action id="41001" issue="28040" author="benoit.razet" type="comment" body="Setting up all the validators with the *serial* scheduler triggers the bug (validators forking, not reaching consensus). It&apos;s not only that they fork but also that they start working extremely slowly (processing transaction very slowly, if not even frozen)." created="2018-02-28 13:15:01.0" updateauthor="benoit.razet" updated="2018-02-28 13:15:01.0"/>
<Action id="41029" issue="28040" author="benoit.razet" type="comment" created="2018-02-28 19:35:50.0" updateauthor="benoit.razet" updated="2018-02-28 19:35:50.0"> <body><! CDATA created a new branch narrowing down a bug in the serial scheduler:   https://github.com/benoitrazet/sawtooth-core/tree/bug-all-serial-scheduler   The validators painfully try to validate 25 transactions.  ></body> </Action>
<Action id="41032" issue="28040" author="benoit.razet" type="comment" created="2018-02-28 19:49:03.0" updateauthor="benoit.razet" updated="2018-02-28 19:49:03.0"> <body><! CDATA The undesired behavior of validators is observable under low volume of transactions. The test does 1txn every 3 seconds.  When the rate of transactions is the highest tolerable by the rest-api, the validators seem to stay in sync, at least for around 5 mins.  ></body> </Action>
<Action id="41152" issue="28040" author="benoit.razet" type="comment" created="2018-03-02 16:10:34.0" updateauthor="benoit.razet" updated="2018-03-02 16:14:27.0"> <body><! CDATA To see the bug with the serial scheduler, simply use the test_poet_liveness and decrease the volume of transactions from 1txn/sec to 0.2txn/sec.  I did this on smallbank_workload and deactivate the intkey_workload, and the forking/consensus appears  !Screen Shot 2018-03-02 at 11.09.26 AM.png|width=883,height=63! This clearly shows the validators using serial scheduler (val1 and val3) getting out of sync with the validators using the parallel scheduler.  Remark: I had to modify the rust program doing the smallbank-workload because it seems the program would not accept less that 1txn/sec workload.  ></body> </Action>
<Action id="42288" issue="28040" author="boydjohnson" type="comment" created="2018-03-28 21:22:25.0" updateauthor="boydjohnson" updated="2018-03-28 21:22:25.0"> <body><! CDATA I put logging in `validator.journal.block_validator` on lines 231 and 267. The first logging statement happens right before adding the first batch of a block that is being validated to a scheduler. The second logging statement happens after getting the batch results from the scheduler.  Validator 1,3 have serial scheduler and 0,2,4 have parallel scheduler  Average times in milliseconds:      Four largest times in milliseconds:  {{validator-0_1: 54  validator-0_1: 215,148,143,140}}  {{validator-1_1: 93  validator-1_1: 857,846,655,364}} {{validator-2_1: 49  validator-2_1: 232,144,111,110}} {{validator-3_1: 72  validator-3_1: 670,667,478,472}} {{validator-4_1: 49  validator-4_1: 184,157,146,116}}  The large times for the two schedulers with the serial scheduler are remarkable.     ></body> </Action>
<Action id="60212" issue="28040" author="benoit.razet" type="comment" body=" ~pschwarz  Thanks for revisiting this bug report. The problem was in the serial scheduler. If I remember correctly, the serial scheduler was not really maintained for several months. It would be interesting to check the status of the bug using Transact." created="2019-05-21 15:53:43.0" updateauthor="benoit.razet" updated="2019-05-21 15:53:43.0"/>
<Action id="60847" issue="28040" author="pschwarz" type="comment" body="I will close this issue, with the work-around to use the parallel scheduler." created="2019-06-10 19:28:50.0" updateauthor="pschwarz" updated="2019-06-10 19:28:50.0"/>
