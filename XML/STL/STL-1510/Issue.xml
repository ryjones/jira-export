<Issue id="38888" key="STL-1510" number="1510" project="10001" reporter="tkuhrt" assignee="rberg2" creator="tkuhrt" type="10004" summary="Sawtooth Validator Reconnection Issue" priority="2" status="10100" created="2019-03-29 19:47:40.0" updated="2019-07-17 11:15:30.0" votes="1" watches="2" workflowId="50540"> <description><! CDATA Reporting on behalf of someone else:  When I restart the validator node, after 4-5 such re-starts it stops writing to its ledger as well as stops broadcasting transactions to the other validators in the network. I have therefore created a set up on Minikube that you might be able to run, in case it would help in locating the root cause.  This is what I have done: # Taken Sawtooth’s sample file (i.e. sawtooth-kubernetes-default-poet.yaml) and retrofitted my changes for persisting the blockfiles and associated files required for validator resilience. I have made persistent the directories that were recommended by intel # Created a separate file for creating Kubernetes persistent volume claims in sawtooth-kubernetes-default-volume.yaml so that I can use the same with either my custom script or with the default sawtooth script  This will enable you to test the pod restart scenario with the Sawtooth provided transaction processors and try recreating the problem.   The steps to be followed are: # Install Minikube # Run the command kubectl apply -f sawtooth-kubernetes-default-volume.yaml to create the volume claims # Then run kubectl get pv and kubectl get pvc to confirm that the persistent volume and it’ claims are created correctly # Run kubectl apply -f sawtooth-kubernetes-default-poet.yaml # Check if pods starting with name sawtooth-0, sawtooth-3 and sawtooth-4 are running # Check for the services ports using command kubectl get services # Create a record using intkey tp by connecting to the url of the pod/service whose name starts with sawtooth-0.. # Check whether the record is present on all 3 nodes by using intkey list turn by turn with each service # Delete the pod whose name starts with sawtooth-0.. Using the command kubectl delete po sawtooth-0….. # Run kubectl get pods till you see the pod starting with sawtooth-0 having restarted # Repeat steps 7 to 10 a few times and the problem will surface post 4-5 attempts  ></description> </Issue>
