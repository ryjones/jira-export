<Issue id="28676" key="STL-1121" number="1121" project="10001" reporter="jaredcorduan" assignee="drozd" creator="jaredcorduan" type="10004" summary="validator degrades into more and more 429s" priority="3" status="10100" created="2018-03-20 13:07:38.0" updated="2019-05-21 14:34:19.0" votes="0" watches="2" workflowId="28679"> <description><! CDATA When we send intkey transactions evenly spaced out at 10 txns/sec, over time the validators 429 more and more transactions and need to be restarted.  In more detail:  We have four machines in AWS running sawtooth validators, rest APIs, and a few transaction processors (tp_settings, intkey, block_info, validator registry).  Each of these processes is run in a separate docker container, all of them built from a recent master (sha 9ccff708ef5a20e236756089722825f0026934bd) using the sawtooth-core `bin/build_all -l python installed`.  We are running software poet (with the same configuration settings as the sawtooth-core poet-liveness test), and the parallel scheduler.  On one of the nodes, we are continually sending an intkey `incr` transaction with the key `foo` and then sleeping 0.1 seconds.  Everything seems fine until about one hour in, when we start seeing 429s  become more regular.  At about 5 hours it becomes rare to *not* get a 429 (maybe one success every 20 seconds).  If we stop firing these intkey transactions, wait a few hours, then try again, the validator is still immediately 429-ing most everything.  If we then restart the validator (just the one whose rest api we are targeting with intkey), everything is fine again for another couple hours, etc.  We have run this experiment at least 5 times over multiple builds from the latest master.  ></description> </Issue>
