<Issue id="36965" key="FAB-13779" number="13779" project="10002" reporter="scottz" creator="scottz" type="10004" summary="orderersystemchannel changed leader for no apparent reason after a follower is restarted" priority="4" resolution="10000" status="6" created="2019-01-20 19:18:10.0" updated="2019-02-02 17:17:52.0" resolutiondate="2019-01-31 06:29:53.0" votes="0" watches="4" workflowId="48532"> <description><! CDATA OSN3 logs show that the orderersystemchannel changed leader for no apparent reason, not long after OSN2 had been restarted.  For the full logs from the orderers, refer to attachments on FAB-13604: * snapshot-OSN1-FAB-13604-scenario2.log * snapshot-OSN2new-FAB-13604-scenario2.log * snapshot-OSN3-FAB-13604-scenario2.log    h2. Steps to Reproduce:  Set up and run testcase scenario2 as defined in detail in FAB-13604. With 3 raft orderers in the orderersystemchannel, restart a follower orderer. When it recovers, the leadership changes. h2. Observed:  For the orderersystemchannel: * OSN3 is leader of orderersystemchannel * OSN2 was brought down * OSN2 was brought up * According to logs in OSN1 and OSN3, orderersystemchannel changes leadership from OSN3 to OSN2. It appears that OSN2 seizes leadership when it comes up!  The OSN3 logs date back to 01-15 and show the leader of orderersystemchannel is OSN3.  The OSN1 and OSN3 orderers logs show they think leadership changed from 3 to 0 to 2. {quote}$ grep "leader changed" snapshot-*scenario2* | grep orderersystemchannel | grep OSN 13   snapshot-OSN1-FAB-13604-scenario2.log:2019-01-16 15:32:55.794 UTC  orderer.consensus.etcdraft  serveRaft -> INFO a29c8 Raft leader changed: 3 -> 0 \{"channel": "orderersystemchannel", "node": 1}  snapshot-OSN1-FAB-13604-scenario2.log:2019-01-16 15:32:55.798 UTC  orderer.consensus.etcdraft  serveRaft -> INFO a29cc Raft leader changed: 0 -> 2 \{"channel": "orderersystemchannel", "node": 1}  snapshot-OSN3-FAB-13604-scenario2.log:2019-01-15 21:24:09.924 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 31e Raft leader changed: 0 -> 3 \{"channel": "orderersystemchannel", "node": 3}  snapshot-OSN3-FAB-13604-scenario2.log:2019-01-16 15:32:55.788 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 4f563 Raft leader changed: 3 -> 0 \{"channel": "orderersystemchannel", "node": 3}  snapshot-OSN3-FAB-13604-scenario2.log:2019-01-16 15:32:55.794 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 4f57f Raft leader changed: 0 -> 2 \{"channel": "orderersystemchannel", "node": 3} {quote} The OSN2new logs, which document the time after OSN2 restarts, show it believes that OSN2 (itself) is the leader: {quote}2019-01-16 15:32:55.796 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 4f1^  0m Raft leader changed: 0 -> 2 \{"channel": "orderersystemchannel", "node": 2} {quote}    Remember OSN2 was just a follower (in orderersystemchannel) when we stopped it. When OSN2 comes up, the others seem to elect it as leader even before we see any "leader changed" log from OSN2 about orderersystemchannel. This is NOT expected. h2.   h2. Expected:  The follower OSN2 should recover and rejoin the others as a follower in orderersystemchannel.    h2. _For comparison, here are some notes about the application channel during the same test:_  During this test, we had one application channel established in addition to the orderersystemchannel. There were exactly 3 orderers. This is not directly related to the observations with the orderersystemchannel, but provided here for reference to hopefully clarify things: * testorgschannel1 had orderer2 as the leader. The test objective of FAB-13604 scenario2 was to restart the leader (OSN2) of the application channel. * OSN1 is ingress orderer for the traffic arriving on channel1. * When we stopped OSN2: then OSN1/OSN3 elected OSN3 as the new leader of testorgschannel1. And OSN2 recognized it when it came back up. All this looked correct, and there were no spontaneous leader changes, with regards to the application channel testorgschannel1. * testorgschannel1 elected OSN3 as its new leader approximately 2m44s before the orderersystemchannel autonomously decided to change from OSN3 to a new leader.  {quote}        $ grep "leader changed" snapshot-*scenario2* | grep -v orderersystemchannel | grep -v 01-15           snapshot-OSN1-FAB-13604-scenario2.log:2019-01-16 15:30:11.190 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 8ba3b Raft leader changed: 2 -> 0 \{"channel": "testorgschannel1", "node": 1}          snapshot-OSN1-FAB-13604-scenario2.log:2019-01-16 15:30:11.193 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 8ba3d Raft leader changed: 0 -> 3 \{"channel": "testorgschannel1", "node": 1}          snapshot-OSN2new-FAB-13604-scenario2.log:2019-01-16 15:32:57.878 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 507 Raft leader changed: 0 -> 3 \{"channel": "testorgschannel1", "node": 2}          snapshot-OSN3-FAB-13604-scenario2.log:2019-01-16 15:30:11.183 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 42d48 Raft leader changed: 2 -> 0 \{"channel": "testorgschannel1", "node": 3}          snapshot-OSN3-FAB-13604-scenario2.log:2019-01-16 15:30:11.189 UTC  orderer.consensus.etcdraft  serveRaft -> INFO 42d4e Raft leader changed: 0 -> 3 \{"channel": "testorgschannel1", "node": 3} {quote} One other observation: We do see OSN2 rejoining testorgschannel1 as it recognizes OSN3 as the leader of that application channel. This is as expected. In fact, in this case, it joins as a follower and does NOT reclaim leadership of testorgschannel1.        ></description> </Issue>
