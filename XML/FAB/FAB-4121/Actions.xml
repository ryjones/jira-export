<Action id="24187" issue="17035" author="kchristidis" type="comment" created="2017-05-23 16:55:06.0" updateauthor="kchristidis" updated="2017-05-23 17:01:03.0"> <body><! CDATA  ~ptippett ,  ~mlishok ,  ~jyellick : Would appreciate your thoughts. (Jason I wrote the multichain manager - orderer interaction out of memory. If there are small inaccuracies I'm OK with it, but if I got something really wrong correct me.)  I'm still in the process of tweaking all the knobs that the sarama library gives me for the retry logic (in setting up the producer/consumer), so I've got another day or so of work ahead of me. But once I'm done with that, I'll need to implement whatever decision we reach here next. So let's try to resolve this sooner rather than later.  ></body> </Action>
<Action id="24240" issue="17035" author="jyellick" type="comment" created="2017-05-24 14:02:45.0" updateauthor="jyellick" updated="2017-05-24 14:02:45.0"> <body><! CDATA > Jason I wrote the multichain manager - orderer interaction out of memory. If there are small inaccuracies I'm OK with it, but if I got something really wrong correct me.  This all looks largely correct to me.    > Should there be a stop condition at which point we give up? And if that's the case, how does this ripple through (feed back to) the multichain manager?  I would say at startup, failure to connect to Kafka should be fatal.  IE, if we never had a connection, there's a good chance we will never get one, and I'd say crashing is the right response.  Assuming that we have previously connected to the Kafka cluster, I would assume we keep trying? Perhaps there could be a configurable timeout for how long we retry, but I think my vote would be to make that infinite, at least to start.  I'd also suspect that at some point we'll want some sort of monitoring/service interface, where we can get the current connection information etc., but that is out of scope for this discussion.  ></body> </Action>
<Action id="24520" issue="17035" author="ptippett" type="comment" created="2017-05-26 20:25:37.0" updateauthor="ptippett" updated="2017-05-26 20:25:37.0"> <body><! CDATA What's the implications of retrying forever and how quickly would you retry?  My concern would be filling up the logs with constant retry requests and the retries themselves causing stress on the system.  I expect they are fairly lightweight although I'm guessing..but having several (or 100s) of networks doing this constantly could be a problem.  The other problem I see with infinite retries is if the orderer crashes, then peers, load balancers, etc would know that the orderer was dead and could try another one.  If it's sitting there running with the port open and retrying, then it's harder for them to figure this out.  Finally, we must have some level of retry even on startup.  The reason I say this is a lot of the problems we've seen is the orderer tries to start up and connect consumers and producers to kafka before it's fully up and running.  It comes up a few minutes later, but it's too late.  I think the best solution would be 2 configurable settings.  One that is a retry interval to specify how long between retries.  And the other is the max number of retries before giving up and stopping.  Using the both of these together allows someone to also control the complete retry time (duration * number of retries). This could be used in all cases, whether we've never had a connection or had a previous one.  If that's not possible, then 1 setting to control the number of retries would be fine, although I'd want to make sure the retry interval wasn't "as fast as possible" but at minimum 1 second but maybe more like 30 seconds or 1 minute.  My third choice would be infinite, with maybe a 30 second or 1 minute interval, but I don't like this given the fact that it'd be hard for others to know the orderer was dead.  I should note that, like TCP/IP, you could do a varying interval where you slowly slow down the interval the more times it fails...but that's going a bit too far.  ></body> </Action>
<Action id="24524" issue="17035" author="kchristidis" type="comment" created="2017-05-26 20:41:47.0" updateauthor="kchristidis" updated="2017-05-31 04:24:53.0"> <body><! CDATA Thank you both for the feedback.  > Finally, we must have some level of retry even on startup.  The reason I say this is a lot of the problems we've seen is the orderer tries to start up and connect consumers and producers to kafka before it's fully up and running.  It comes up a few minutes later, but it's too late.  Right, that's a good point.  So I'm tending towards what you referred to as the "best solution" as well. Basically repurpose these two settings  https://github.com/hyperledger/fabric/blob/master/sampleconfig/orderer.yaml#L132...L136  so that they apply when a producer or consumer is set up, and when we attempt to produce or consumer messages (those are two different things). Might be a bit too coarse of a switch for all of these functions, but I really want to avoid offering 40 knobs and just exposing all of Kafka's settings.  Will work on this and update this JIRA if I bump into any issues.  ></body> </Action>
<Action id="24527" issue="17035" author="ptippett" type="comment" body="Thanks! That sounds good.  " created="2017-05-26 21:02:54.0" updateauthor="ptippett" updated="2017-05-26 21:02:54.0"/>
<Action id="24720" issue="17035" author="kchristidis" type="comment" created="2017-05-31 04:27:01.0" updateauthor="kchristidis" updated="2017-05-31 04:27:01.0"> <body><! CDATA If duration * number of retries elapse, does the orderer panic thus stopping its updates (writes/reads) on all chains, or does it stop working just on the chain where the issue occurred? (Which results in the zombie state that we have right now.)  I'm guessing the former?  ></body> </Action>
<Action id="24760" issue="17035" author="ptippett" type="comment" created="2017-05-31 12:57:48.0" updateauthor="ptippett" updated="2017-05-31 12:57:48.0"> <body><! CDATA Good question.  Thinking out loud a bit, if it's only the one orderer/kafka, then it's better to panic so the peers can switch to another one.  The risk here is if all orderers have a problem with only one channel, we end up losing all channels vs just the one(s) with the issue.     This brings me to if I found this issue on a running orderer/kafka, what would I do?  The first thing I'd do is likely restart the orderer (after gathering logs and looking for obvious issues).  So maybe the ideal approach would be the orderer restarts some # of times, then panics.  I expect that's something you can't do that in the orderer code itself..that would of course need to be handled by something external.  I expect this is automation that could be put in supervisord or could be handled by docker itself or an external process.     So yeah, I agree with you.  I think the best bet in this case is for the orderer to panic and throw a decent error message.  Then either supervisord, docker, or some automation could be set to restart the orderer container some # of times before giving up.  I think it's better to do this and make it clear to peers/sdk, etc that the orderer is dead vs having one orderer unable to fully function when there are a few others which may be working fine.        ></body> </Action>
<Action id="24787" issue="17035" author="mastersingh24" type="comment" created="2017-05-31 15:51:22.0" updateauthor="mastersingh24" updated="2017-05-31 15:51:22.0"> <body><! CDATA Good discussion and some good points raised above.  Here's my thoughts:  1) Is an OSN is unable to send/fetch from a partition, I think that we should always return https://github.com/hyperledger/fabric/blob/master/protos/common/common.proto#L35 (i.e. if we know we can't communicate with the partition, then don't send anything back in response to a Deliver requests even if it's in the file ledger).  Of course a clear message should be logged as well so that monitoring tools can pick up on it.  2) Retries:    There are two strategies that I'm familiar with for configuring retries: - backoff strategy - something like https://github.com/grpc/grpc/blob/e1d8d4dc1bdf54273b7ed9d4a495789430cbdf6c/doc/connection-backoff.md - MQ has 2 settings - short retry interval and long retry interval.  The short retry is used to try to quickly (re)establish communication - so you might do something like retry every 10 seconds for 10 min.  After 10 min, the long retry settings take effect - something like retry every 10 min for 12 hours (just example but you get the idea).  We used this with DataPower and MQ in my previous life.  I'd also again make sure there's a clear, easily parseable log message here as well.  I don't think that we should panic if we can't connect to Kafka - that's why I made point 1) above.  My take is that typically if one OSN cannot connect to a partition or any broker, then it's more than likely the other OSNs will have the same issue.  This would also allow OSNs to service other channels if the issue was solely with a single partition.  I guess we also need to think about how the deliver service client in the peer should respond to this as well - but at least using the 503 error code might be a hint.  ></body> </Action>
<Action id="24824" issue="17035" author="kchristidis" type="comment" created="2017-05-31 17:53:57.0" updateauthor="kchristidis" updated="2017-05-31 18:00:35.0"> <body><! CDATA > My take is that typically if one OSN cannot connect to a partition or any broker, then it's more than likely the other OSNs will have the same issue. This would also allow OSNs to service other channels if the issue was solely with a single partition.  Good point.  So, the less aggressive approach via a 503 response on the deliver side makes sense. It will require an extension of the `ConsenterSupport` interface with a `Status` method. (The `deliverServer` will then tap into it to see what's up with the orderer.)  The short/long retry interval is intuitive and – unless folks disagree – will serve our purposes nicely.  No matter which approach we end up with, a few things are certain: # The `Retry` options provided already will need to be slightly expanded/repurposed. # The  Kafka client-related timing settings|https://godoc.org/github.com/Shopify/sarama#Config  that we were hiding so far (`DialTimeout`, `ReadTimeout`, `WriteTimeout`, `Metadata.Retry.Max`, `Metadata.Retry.Backoff`, `Producer.Retry.Max`, `Producer.Retry.Backoff`, `Consumer.Retry.Backoff`) will need to be exposed in `orderer.yaml` but in a way that doesn't break what's out there already. We'll leave those settings commented out, and if a user wishes to modify them all they need to do is uncomment that line. Otherwise the orderer will run with the defaults, as it does now. # If we go the 503 route, we're looking at small code changes that are nonetheless on the hot path and affect what the user sees.  I believe these changes warrant the "review-needed" label.  ></body> </Action>
<Action id="24853" issue="17035" author="ptippett" type="comment" created="2017-05-31 20:22:42.0" updateauthor="ptippett" updated="2017-05-31 20:22:42.0"> <body><! CDATA yeah short and long retry are fine.  This setting would be used for both problems publishing messages as well as connecting to a broker, correct? I want to make sure we're also covering the case that the orderer starts and kafka hasn't come up yet so isn't ready.  As for the error message, my concern is the orderer needs to return something that the peer can pick up on to know it should probably try another orderer.  If the client returns a 503 and the peer can pick that up and choose to try another orderer, I think that's fine as well.        ></body> </Action>
<Action id="24854" issue="17035" author="kchristidis" type="comment" body=" ~ptippett : These settings will cover the connection part (the creation of the producer/consumer). The publishing part will be covered by tweaking the `Producer.Retry.Max` and `Producer.Retry.Backoff` settings as you see fit. These are currently hidden, but I plan to expose them as described in Step #2 in my previous comment." created="2017-05-31 20:43:17.0" updateauthor="kchristidis" updated="2017-05-31 20:43:17.0"/>
<Action id="25271" issue="17035" author="scottz" type="comment" created="2017-06-04 18:24:32.0" updateauthor="scottz" updated="2017-06-04 18:24:32.0"> <body><! CDATA I see different situations here that must be considered. * An orderer can fail to set up (or can drop) a connection to kafkabroker. (connection problem, network issue) * An orderer has set up connection to KB but fails to communicate on one partition/channel. (kafkabroker internal problem - partition was not set up correctly, or it WAS working but now one or more partitions got stuck or corrupted).  The two levels of retry timers make sense in many situations, especially for the first bullet. I would expect this to be combined with rollover to the next KB in the list. Retry once or twice on same KB, then the next retry is sent to the next KB on the list.  The second bullet is more complicated. If orderer can communicate with KB, AND if other channels are not stuck, then the orderer could try a different KB (but only until it goes through all of them on its list once - at which point it knows the channel/partition is stuck because at least one other channel is fine on all the KBs). In that situation, all the orderers would be cycling through the KB list in the same manner, regardless of which Kafkabroker was initially used by each orderer. Stopping the "move on to the next kafkabroker" logic after one cycle would stop disrupting the use of the other channels.  In what situations could an orderer automatically reboot itself? Or reboot a particular kafkabroker? Or a zookeeper?     ></body> </Action>
<Action id="25289" issue="17035" author="kchristidis" type="comment" created="2017-06-04 21:34:15.0" updateauthor="kchristidis" updated="2017-06-04 21:34:15.0"> <body><! CDATA  ~scottz :  > The two levels of retry timers make sense in many situations, especially for the first bullet. I would expect this to be combined with rollover to the next KB in the list. Retry once or twice on same KB, then the next retry is sent to the next KB on the list.  The OSN switches right away between all listed brokers until it finds an available broker. If none are available after the first pass, it pauses for Metadata.Retry.Backoff and repeats the process for a total of Metadata.Retry.Max times.  > The second bullet is more complicated. If orderer can communicate with KB, AND if other channels are not stuck, then the orderer could try a different KB (but only until it goes through all of them on its list once - at which point it knows the channel/partition is stuck because at least one other channel is fine on all the KBs). In that situation, all the orderers would be cycling through the KB list in the same manner, regardless of which Kafkabroker was initially used by each orderer. Stopping the "move on to the next kafkabroker" logic after one cycle would stop disrupting the use of the other channels.  Not sure I follow. This bit in particular gives me pause:  > the orderer could try a different KB (but only until it goes through all of them on its list once - at which point it knows the channel/partition is stuck because at least one other channel is fine on all the KBs)  The OSN always reaches out _to the leader of the channel_. Even if ZK reports that channel foo is replicated by brokers 1, 2, and 3, and say brokers 1 and 2 are the ISR set at the time of the query, as long as 1 is reported as the leader, the OSN will attempt to connect 1. You seem to suggest that if 1 is not reachable, the OSN should reach out to 2. That will not happen unless ZK points to 2 as the leader.  > In what situations could an orderer automatically reboot itself? Or reboot a particular kafkabroker? Or a zookeeper?  I assume that is service-dependent, i.e. it's up to whomever runs the ordering service to say that "if the orderer cannot establish a connection to more than X channels we should reboot".     ></body> </Action>
<Action id="25339" issue="17035" author="kchristidis" type="comment" created="2017-06-05 14:13:21.0" updateauthor="kchristidis" updated="2017-06-05 14:13:21.0"> <body><! CDATA We got the 5 votes needed for the extending the `ConsenterSupport` interface with a `Status` method from FAB-4155.  I believe the only outstanding issue is whether we are OK with exposing Kafka-related timing settings (FAB-4357). The changes are non-breaking and necessary, but since they affect what the user sees, consensus is needed.  ></body> </Action>
<Action id="25370" issue="17035" author="binhn" type="comment" created="2017-06-05 15:22:30.0" updateauthor="binhn" updated="2017-06-05 15:30:28.0"> <body><! CDATA  ~kchristidis  Maybe we should replace this one with FAB-4357 on the voting board?  Im assuming the retry logic (as discussed here) either has been implemented or be part of FAB-4155.  ></body> </Action>
<Action id="25375" issue="17035" author="kchristidis" type="comment" body=" ~binhn : You&apos;re right. Done." created="2017-06-05 15:28:30.0" updateauthor="kchristidis" updated="2017-06-05 15:28:30.0"/>
<Action id="25377" issue="17035" author="kchristidis" type="comment" created="2017-06-05 15:32:37.0" updateauthor="kchristidis" updated="2017-06-05 15:32:37.0"> <body><! CDATA > Im assuming the retry logic (as discussed here) either has been implemented or be part of FAB-4155.  I'd say: https://jira.hyperledger.org/browse/FAB-4136 (see the "Relates to" section of this JIRA here)  ></body> </Action>
<Action id="25467" issue="17035" author="kchristidis" type="comment" created="2017-06-06 05:42:12.0" updateauthor="kchristidis" updated="2017-06-06 05:42:12.0"> <body><! CDATA This issue has served its purpose and given me the direction necessary for fixing FAB-4136.  Closing this, thanks all for the feedback.  ></body> </Action>
