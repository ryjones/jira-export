<Issue id="13504" key="FAB-1171" number="1171" project="10002" reporter="bcbrock" assignee="sanchezl" creator="bcbrock" type="10001" summary="Cut blocks based on size, not based on the number of TX" priority="3" resolution="10000" status="6" created="2016-11-21 16:25:20.0" updated="2018-07-20 18:54:01.0" resolutiondate="2017-01-17 09:33:23.0" votes="1" watches="4" workflowId="36740"> <description><! CDATA I have been doing some benchmarking of the Kafka orderer, and as part of this I did a rough parameter sweep to find the best number of transactions-per-block under various conditions. I was surprised to find that the best way to cut blocks is based on the size of the block (in bytes), not based on the number of transactions in the block. This result held in 2 different system configurations, with varying numbers of simultaneous clients and different blob sizes. The attachments show that the highest throughput for System 1 is always obtained with a block size around 400KB. For System 2, a block size of about 800KB is best. (Note - I did not go above 800KB for System 2 because the Kafka configuration has a limit of 1MB messages).  Based on these results I would suggest that all orderers implement a block-size target in addition to or in lieu of the current transactions-per-block target.  One nice thing about a size target vs. the current number-of-transactions target is that it allows a system administrator to tune throughput of an ordering service without having to run the actual blockchain application.  Note: This issue is related to https://jira.hyperledger.org/browse/FAB-904. Since the Kafka orderer _requires_ a block size limit, this limit should at least be documented as a way to optimize throughput as well.  ></description> </Issue>
