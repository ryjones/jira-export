<Action id="60936" issue="38586" author="guoger" type="comment" created="2019-06-13 04:22:25.0" updateauthor="guoger" updated="2019-06-13 04:22:25.0"> <body><! CDATA quote etcd implementation notes: bq. This approach introduces a problem when removing a member from a two-member cluster: If one of the members dies before the other one receives the commit of the confchange entry, then the member cannot be removed any more since the cluster cannot make progress. For this reason it is highly recommended to use three or more nodes in every cluster.  ></body> </Action>
<Action id="62145" issue="38586" author="guoger" type="comment" created="2019-07-26 06:39:09.0" updateauthor="guoger" updated="2019-07-26 06:39:09.0"> <body><! CDATA I see this being moved to active iteration in ipm, however i'm not sure if this is worth tackling, given the amount of code change it entails vs the rarity of problem.  IMHO, as long as we suggest running at least 3 nodes in production, we are safe  wdyt  ~yacovm   ~jyellick   ></body> </Action>
<Action id="62148" issue="38586" author="yacovm" type="comment" body="Wait, I thought that you changed the etcdraft chain to transfer leadership if the leader is removed? Why doesn&apos;t that solve our problem?" created="2019-07-26 08:12:03.0" updateauthor="yacovm" updated="2019-07-26 08:12:03.0"/>
<Action id="62149" issue="38586" author="guoger" type="comment" body=" ~yacovm  that doesn&apos;t 100% prevent this from happening, as transfer may fail - i.e. network partition" created="2019-07-26 08:18:03.0" updateauthor="guoger" updated="2019-07-26 08:18:03.0"/>
<Action id="62156" issue="38586" author="yacovm" type="comment" body="If there is network partition then you can&apos;t agree on anything anyway so aren&apos;t we safe?" created="2019-07-26 09:50:02.0" updateauthor="yacovm" updated="2019-07-26 09:50:02.0"/>
<Action id="62157" issue="38586" author="guoger" type="comment" created="2019-07-26 10:03:08.0" updateauthor="guoger" updated="2019-07-26 10:03:08.0"> <body><! CDATA consider the flow: 1. config block to remove current leader is agreed 2. leader disseminates conf change (1st round in Raft protocol) 3. follower acks conf change 4. *network partition occurs* 5. leader sends commit to follower (2nd round in Raft protocol) - *Fail* 6. leader apply conf change locally - *Success* 7. leader attempts leader transfer *Fail*  If the leader node reboots at any point after 1, it will not start Raft instance, as it does not find itself among consenter set. So the remaining node is stuck.   ~yacovm   ></body> </Action>
<Action id="62158" issue="38586" author="yacovm" type="comment" body="I thought that leadership transfer should take before actually handling the leader removal, and in any case the leader will never propose a block that removes itself, but the new leader will propose it...Â  isn&apos;t that the case?" created="2019-07-26 10:34:33.0" updateauthor="yacovm" updated="2019-07-26 10:34:33.0"/>
