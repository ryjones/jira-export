<Action id="24048" issue="16863" author="kchristidis" type="comment" body="(Same comment applies: https://jira.hyperledger.org/browse/FAB-4012?focusedCommentId=24047&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-24047)" created="2017-05-18 21:14:17.0" updateauthor="kchristidis" updated="2017-05-18 21:14:17.0"/>
<Action id="24087" issue="16863" author="nishi" type="comment" created="2017-05-19 15:34:33.0" updateauthor="nishi" updated="2017-05-19 15:34:33.0"> <body><! CDATA After updating docker-compose-base.yaml to include {code:java}   - KAFKA_MIN_INSYNC_REPLICAS=2{code}    Reran the test with few changes to send a tx: we need to be running a minimum of 2 kafka brokers.     So now we have:  1. Bring up the network. (1 orderer, 3 kafka brokers, 3 zoo-keepers, 2 org with 2 peer each with TLS) 2. Send a invoke transaction 3. Query yields successful results as expected 4. Now identify leader of kafka cluster, from logs in our scenario it was broker1 i.e. kafka0 (kafka0:1, kafka1:2, kafka2: 3) 5. Pause all kafka brokers in the order below  1. docker pause kafka0 2. docker pause kafka1 3. docker pause kafka2 (leader stopped at end) 6. Wait 30 seconds 7. docker unpause kafka0 + kafka1(non-leaders) 8. Issue a transaction  Expected Result:  New leader to be elected (which appears is happening from kafka logs) Transaction to complete successfully     Actual Result: Transaction fails  "error:  Orderer.js : sendBroadcast - reject with SERVICE_UNAVAILABLE not ok 10 Failed to send transaction and get notifications within the timeout"     This time when I restarted order, orderer DID NOT PANIC or CRASH but instead, I see {code:java} 2017-05-19 15:28:49.764 UTC  orderer/common/broadcast  Handle -> INFO 272^  0m Consenter instructed us to shut down{code}    Please find attached orderer logs. Lowering the priority here.  ></body> </Action>
<Action id="24095" issue="16863" author="nishi" type="comment" created="2017-05-19 18:48:13.0" updateauthor="nishi" updated="2017-05-19 18:56:46.0"> <body><! CDATA This is what we observed after few more tests (this time with docker stop - so that any state would be transferred graciously)  In step 7 above, when earlier leader in this case (kafka2) was brought back up, along with another non-leader.  Orderer was able to process new transaction.  Please find attached logs (updatedLogs-April19th-2-45pm.tar.gz)     We successfully tested happy paths (where previous leader was always brought back so that other kafka broker would be in sync). Here our goal is to check if orderer recognizes leadership transfer which does not seem to be happening.     ></body> </Action>
<Action id="24125" issue="16863" author="kchristidis" type="comment" body="Unassigning myself on purpose as I&apos;m only working on one issue at time. Will re-assign when I focus on this." created="2017-05-21 23:24:00.0" updateauthor="kchristidis" updated="2017-05-21 23:24:00.0"/>
<Action id="25947" issue="16863" author="kchristidis" type="comment" created="2017-06-10 10:18:11.0" updateauthor="kchristidis" updated="2017-06-10 10:18:11.0"> <body><! CDATA That is a valid scenario and not a bug.  Consider the example below with the typical setup described in `bddtests/dc-*.yml`.  Before we pause any brokers, we query the channel:   {{(behave_venv) ubuntu@hyperledger-devenv:472d8bd:/opt/gopath/src/github.com/hyperledger/fabric/bddtests/tmp/kafka_2.11-0.9.0.1$ bin/kafka-topics.sh --zookeeper localhost:33503 --describe --topic com.acme.blockchain.jdoe.Channel1}} {{Topic:com.acme.blockchain.jdoe.Channel1 PartitionCount:1 ReplicationFactor:3 Configs:}} {{ Topic: com.acme.blockchain.jdoe.Channel1 Partition: 0 Leader: 2 Replicas: 2,3,0 Isr: 2,3,0}}     We see here that broker 2 is the leader for Channel1, and brokers 2,3,0 are in its ISR. (Basically all of the brokers that replicate this partition.)  Now we pause all of the brokers as you did (we don't even have to wait and do the leader last). Then we unpause just brokers 0 and 1 (so the former leader remains paused).  Here's what we get if we query the channel now:   {{(behave_venv) ubuntu@hyperledger-devenv:472d8bd:/opt/gopath/src/github.com/hyperledger/fabric/bddtests/tmp/kafka_2.11-0.9.0.1$ bin/kafka-topics.sh --zookeeper localhost:33503 --describe --topic com.acme.blockchain.jdoe.Channel1}} {{Topic:com.acme.blockchain.jdoe.Channel1 PartitionCount:1 ReplicationFactor:3 Configs:}} {{ Topic: com.acme.blockchain.jdoe.Channel1 Partition: 0 Leader: 0 Replicas: 2,3,0 Isr: 0}}  So, we have a new leader, broker 0. However! Check the channel's ISR. It's only got broker 0 in it, and its size is less than the minimum we had asked for when setting up the cluster (min.insync.replicas = 2). (How did we end up here? Because we paused the other two brokers in the channel's ISR.)  As a result, if we attempt to read (Deliver) from this channel it'll work, but if we attempt to write to it (Broadcast), the request will fail with a SERVICE_UNAVAILABLE response.  So the behavior that you see is totally normal.  Closing this, but feel free to ask questions or reopen if you think I missed something.  ></body> </Action>
<Action id="26342" issue="16863" author="latitiah" type="comment" body=" ~kchristidis  Your explanation makes sense here, but I think that there is still an issue. (It may be that this would be more of an improvement than a bug, though) I do believe that we should set up the fabric kafka brokers such that even if they come back up &quot;out of order&quot; that the cluster should be functional." created="2017-06-14 21:15:11.0" updateauthor="latitiah" updated="2017-06-14 21:15:11.0"/>
<Action id="26343" issue="16863" author="kchristidis" type="comment" created="2017-06-14 21:22:51.0" updateauthor="kchristidis" updated="2017-06-14 21:22:51.0"> <body><! CDATA This will not happen. This is basically a call to rewrite the way Apache Kafka works.  We should just add more brokers to prevent such issues from happening.  ></body> </Action>
<Action id="26427" issue="16863" author="latitiah" type="comment" created="2017-06-15 16:34:48.0" updateauthor="latitiah" updated="2017-06-15 16:35:57.0"> <body><! CDATA Per a RocketChat discussion with  ~kchristidis , when this scenario occurs the current workaround would be to add a new kafka broker to the cluster. Maybe in the future (given the necessary resources and time) a new kafka engine could be written to allow for this sort of failure scenario.   ~nickgaski : This workaround should be documented as a solution to this issue.  ></body> </Action>
<Action id="26432" issue="16863" author="kchristidis" type="comment" created="2017-06-15 17:05:10.0" updateauthor="kchristidis" updated="2017-06-15 17:05:10.0"> <body><! CDATA The only relevant directive here is: {quote}Given the maximum number of faults (i.e. crashed and/or non-responsive brokers) we wish to tolerate --  Have enough brokers up, and configure your default.replication.factor and minimum.insync.replicas so that you can satisfy the minimum.insync.replicas requirement. {quote} I wouldn't label this as a workaround, but as _the only deployment strategy_.  ></body> </Action>
<Action id="26434" issue="16863" author="kchristidis" type="comment" created="2017-06-15 17:08:07.0" updateauthor="kchristidis" updated="2017-06-15 17:08:07.0"> <body><! CDATA This issue should be closed.  Latitia, feel free to add this comment in FAB-3384.  ></body> </Action>
<Action id="26636" issue="16863" author="scottz" type="comment" created="2017-06-16 23:23:48.0" updateauthor="scottz" updated="2017-06-16 23:23:48.0"> <body><! CDATA Before closing this, we need to have a discussion, to make sure we understand the behavior in various scenarios, using the recommended 4 KBs, min_isr=2, replicas=3. And to make sure our product is actually behaving in a useful manner.  If we stop 3, stop 2, stop 1, and then restart 2 or 3, it should work (min_isr=2, and since k0 stays running entire time). Agree?  If we stop 3, stop 2, stop 1, stop 0, then is there any way to get it back functioning? How about start 0, start 1, start 2, start 3?  How about Start 1, start 2, start 3? Either one of those should work, based on my understanding of kafka. Agree?  What is all this talk about problems when a kafka "leader" is stopped? The network should continue to provide service (if after a brief respite) (even allowing for the orderers to reconnect to KBs if necessary) (and we can discuss the requirement for the orderer-consumer-clients and orderer-broadcast-clients to reestablish grpc connections) when only one kafkabroker is stopped, regardless of which one it is. If my understandings are incorrect, then please can you point to the Kafka literature that describes about the kafka broker config parameters functional behavior being OVERRIDDEN if a kafkabroker leader encounters trouble.  ></body> </Action>
<Action id="26639" issue="16863" author="kchristidis" type="comment" created="2017-06-17 00:15:07.0" updateauthor="kchristidis" updated="2017-06-17 00:15:07.0"> <body><! CDATA I assume that in your comment above you refer to a 4-broker setup, since you refer to brokers 0 to 3. This is also the setup that has been available since May 16th in our repo (FAB-3387), the one that I was talking about in my response above (and the only setup we should be exploring anyway). {quote}If we stop 3, stop 2, stop 1, and then restart 2 or 3, it should work (min_isr=2, and since k0 stays running entire time). Agree? {quote} Correct. {quote}If we stop 3, stop 2, stop 1, stop 0, then is there any way to get it back functioning? How about start 0, start 1, start 2, start 3?  How about Start 1, start 2, start 3? Either one of those should work, based on my understanding of kafka. Agree? {quote} Correct.  When Latitia talks about bringing brokers back "out of order", she doesn't mean bring _all of them_ back out of order. This is different to what you write in the quote above. Latitia suggests that you can bring just |min.isr| brokers up in any order and the channel should work. I clarified this in a private conversation in Rocket.Chat with Latitia. And as I wrote above, this won't happen and this is not the way Kafka works. This is exactly what my detailed example addresses BTW – you have |min.isr| brokers up but they don't belong to the replication factor set. {quote}What is all this talk about problems when a kafka "leader" is stopped? {quote} I didn't say there would be a problem. The original bug report – posted in a piecewise manner and where the claim of killing the leader (which I assume is a reference is to the channel leader) could not be trusted (as we discussed with Nishi a couple of days ago, I think you were actually targeting the Kafka controller) – didn't leave me much to work with. This is why I gave a detailed example above, noting my setup. I clearly state that "we have a new leader, broker 0", so again, no problems as far as I can tell. {quote}The network should continue to provide service (if after a brief respite) (even allowing for the orderers to reconnect to KBs if necessary) (and we can discuss the requirement for the orderer-consumer-clients and orderer-broadcast-clients to reestablish grpc connections) when only one kafkabroker is stopped, regardless of which one it is.{quote} That is indeed the case.     ></body> </Action>
