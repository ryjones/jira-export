<Action id="63235" issue="41881" author="rameshthoomu" type="comment" created="2019-08-23 17:31:47.0" updateauthor="rameshthoomu" updated="2019-08-23 17:34:42.0"> <body><! CDATA Executed this test case in IKS k8s environment. Below are the network details  1. 5 worker nodes on K8s cluster. Each worknode has 16cores*32GB memory 2. See the pod resources here https://github.com/rameshthoomu/fabric-test/blob/master/tools/PTE/CITest/k8s_testsuite/networkSpecFiles/kafka_couchdb_tls.yaml#L95-L131 (Most of the pods has 2CPU and memory 4gb resource allocation)  peer0 of org1 stopped after 10 mins of the start of the test and then restarted after 8 hrs.   Observation: I see none of the peers stopped/crashed after the default timeout of "aliveExpirationTimeout" and then the restarted peer is synced and verified the total no.of blocks.  Issue: I see total of 10334 blocks were created during 6hrs invoke test but the restarted peer took almost *35 mins *get the latest block (10334). Anyone help me in understanding why it took so long to syncup. Is this a known issue?  if yes, do I need to modify any parameters to improvise this?  ~denyeart   ></body> </Action>
<Action id="63239" issue="41881" author="denyeart" type="comment" created="2019-08-23 18:35:22.0" updateauthor="denyeart" updated="2019-08-23 18:35:22.0"> <body><! CDATA  ~rameshthoomu , given 6 hours worth of high volume blocks, 35 minutes to catch up to 10334 blocks is within normal timeframe given peer with CouchDB.  One interesting thing to try would be to disable gossip state transfer and peer-to-peer block sharing. We know this improves memory footprint of peers that are catching up. It may improve catchup speed as well.  You can try the following settings:  CORE_PEER_GOSSIP_USELEADERELECTION = false  CORE_PEER_GOSSIP_ORGLEADER = true  CORE_PEER_GOSSIP_STATE_ENABLED = false  ></body> </Action>
