<Issue id="37022" key="FAB-13810" number="13810" project="10002" reporter="scottz" assignee="suryalnvs" creator="scottz" type="10101" summary="Long run for memory analysis: no crashes, multiple TX per block on 1 thread per 1 channel (constant mode)" priority="3" resolution="10000" status="6" created="2019-01-21 19:37:40.0" updated="2020-01-09 23:52:49.0" resolutiondate="2019-07-23 20:59:59.0" votes="0" watches="1" workflowId="48591"> <description><! CDATA Execute testcase FAB-13641 but run it for a 3 days. No crashes, concurrent invocations on 1 thread per 1 channel (constant mode), each thread sends 10 transactions per second, using one thread per peer and one thread per orderer.  And monitor the prometheus metrics, such as memory consumption.   Check with design team for any particular orderer system metrics for consensus or raft or others to monitor - although we realize that feature code might not be ready yet for verification.     Design team provided these instructions to enable the golang profiler before start test (using ORDERER_GENERAL_PROFILE):       https://github.com/hyperledger/fabric/blob/master/sampleconfig/orderer.yaml#L135-L136      For suspected memory leak, they suggest to collect during/after the test:       To see how many connections the orderer has          netstat -anp | grep orderer      Connect to orderer and dump the stacktrace goroutines into the output, to check for goroutine leak:          kill -SIGUSR1 1      An enumeration of the open file descriptors (the <pid> is the process ID)          cd /proc/<pid>; ls -lr /<pid>/fd | wc -l       Collect the logs too.  ></description> </Issue>
