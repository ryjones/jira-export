<Issue id="45470" key="FAB-18025" number="18025" project="10002" reporter="ryjones" assignee="denyeart" creator="dhuseby" type="10004" summary="potential DDoS of Orderer via config" priority="2" status="10300" created="2020-06-25 03:08:49.0" updated="2021-09-30 18:15:01.0" votes="0" watches="3" workflowId="59370" security="10000" archived="N"> <description><! CDATA *How to reproduce:* The attachment is the attack payload (about 1.7G after unzip), use the following command line:          *curl -X PUT " http://localhost:8443/logspec "  -d @payload.txt*  http://localhost:8443|http://localhost:8443/  is orderer's operations server. Wait for a while after the command runs. The memory usage of orderer will increase dramatically (One of my tests shows about 10GB more VIRT memory is occupied).  *Explanation:* The problem is in " _func (l *LoggerLevels) ActivateSpec(spec string) " in  loggerlevels.go|https://github.com/hyperledger/fabric/blob/master/common/flogging/loggerlevels.go  ._ This function is called whenever above http link ( http://localhost:8443/logspec ) is accessed using PUT method. And it will parse the spec string (which is stored in payload.txt and uploaded using above command), and will store the information in a mapping specs.   |// The logging specification has the following form:| //  <logger> ,<logger>... = <level> : <logger> ,<logger>... = <level>...    The ActivateSpec() has no filter or test to check if the input spec is of appropriate length. So what if logger is a *very long* string? In payload.txt, I constructed a spec with a number of very long loggers. Each logger was about 55K chars and the spec included about 30000 loggers. Essentially the spec just looks like              \{"spec": "VERYLONGLOGGER1, VERYLONGLOGGER2, VERYLONGLOGGER3,......, VERYLONGLOGGERN=debug"} ActivateSpec() function will put these very large strings in the mapping, which will lead to a very large amount of memory occupied.   *More tests:* Due to low-level http timeout, the payload *cannot be too big*. I have ever constructed a 4GB spec but sending it always failed and could never reach ActivateSpec(). So there may be a upper limit for the amount of memory occupied. If you run the above command several times, the memory usage may get even bigger. I guess it maybe is related to golang h1. Garbage Collector Garbage Collector but I did not dig that deep. You can test it by using a script to run the command several times or even to run parallelly 2 or 3 processes (more processes will get wrong result also because of http timeout). During the tests, I also noticed that while the memory usage increased the CPU usage was also very high. If the attack runs continuously, the *performance of orderer* may also experience negtive impact. h1. Garbage Collector   *Threat level*: Default config will have operations server only listening on localhost 8443 port, which makes the attack feasible only on local machine. If operations server is listening on port that is accessible from outside, it is a security problem. Anyone can try crash the orderer process using above attack (unless orderers are all running on machines with memory much more than 32GB).   *Test environment*: docker images:  fabric-orderer   amd64-2.2.0-snapshot-55308c7c4 docker version: 19.03.9. go version: 1.14.2 OS: Linux debian buster 4.19.0-6-amd64, 32GB memory, 5GB swap.  ></description> </Issue>
