<Action id="35597" issue="24540" author="clayton sims" type="comment" body="This comes from CLS.    ~kchristidis   or  ~sanchezl   could one of you triage this.  " created="2017-11-28 21:41:18.0" updateauthor="clayton sims" updated="2017-11-28 21:41:18.0"/>
<Action id="35617" issue="24540" author="kchristidis" type="comment" body="Does not seem to be orderer related. /cc  ~denyeart " created="2017-11-29 10:41:04.0" updateauthor="kchristidis" updated="2017-11-29 10:41:04.0"/>
<Action id="35619" issue="24540" author="denyeart" type="comment" created="2017-11-29 12:09:34.0" updateauthor="denyeart" updated="2017-11-29 12:09:34.0"> <body><! CDATA  ~rhegde  In order to analyze, you would need to look at the statedb_savepoint documents in couchdb to understand current 'height' of state db, identify which documents (keys) in CouchDB are out of sync, and then look for those keys in peer debug on the various peers to understand the differences. With the existing recovery logic, we would not expect state dbs of the same 'height' to have different key documents, since the key documents for a given block get committed to disk prior to the savepoint being written.  Please identify the keys in question and point to the lines in the trace related to those keys, in the two peers that are not in sync.  ></body> </Action>
<Action id="35653" issue="24540" author="rhegde" type="comment" created="2017-11-29 20:37:42.0" updateauthor="rhegde" updated="2017-11-29 20:41:08.0"> <body><! CDATA *Response 1*  I have used the couch-db version field to figure the block number that went missing # Block 576 which translates in creation of 1 JSON Document # Block 635 which translates in creation of 2 JSON Document and multiple updates to existing JSON documents.   *Response 2*  Please refer to the log added - *peer.8ae99f2c3e9c.log* indicating Peer also went down during commit processing of Block 576.     *Response 3*   clsnet@dusd1devrhap065 ~ $ curl -H "Content-Type: application/json" -X GET  http://dusd1devrhap063.dev.local:41005/node1-node2/statedb_savepoint  \{"_id":"statedb_savepoint","_rev":"644-fdff898d3f70d6facb04ad04736a8287","BlockNum":644,"TxNum":1,"UpdateSeq":"2050-g1AAAAE2eJzLYWBg4MhgTmHgzcvPy09JdcjLz8gvLskBCjMlMiTJ____PytxCw4FSQpAMskerGYpLjUOIDXxYDWbcalJAKmpB6tZjkNNHguQZGgAUkBl87MSd-NVtwCibn9W4ga86g5A1N3PStyIV90DiLr_WUkMDMzGWQC2pmf8"}   clsnet@dusd1devrhap065 ~ $ curl -H "Content-Type: application/json" -X GET  http://dusd1devrhap063.dev.local:41005/node1-node2  \{"db_name":"node1-node2","update_seq":"2051-g1AAAAE2eJzLYWBg4MhgTmHgzcvPy09JdcjLz8gvLskBCjMlMiTJ____PytxCw4FSQpAMskerGYpLjUOIDXxYDWbcalJAKmpB6tZjkNNHguQZGgAUkBl87MSd-NVtwCibn9W4ga86g5A1N3PStyIV90DiLr_WUkMDMwmWQC2qGf9","sizes":\{"file":23070179,"external":1183642,"active":2125584},"purge_seq":0,"other":\{"data_size":1183642},"doc_del_count":0,"doc_count":1109,"disk_size":23070179,"disk_format_version":6,"data_size":2125584,"compact_running":false,"instance_start_time":"0"}   clsnet@dusd1devrhap065 ~ $ curl -H "Content-Type: application/json" -X GET  http://dusd1devrhap063.dev.local:41006/node1-node2  \{"db_name":"node1-node2","update_seq":"2518-g1AAAAE2eJzLYWBg4MhgTmHgzcvPy09JdcjLz8gvLskBCjMlMiTJ____PyvxPQ4FSQpAMskerOYeLjUOIDXxYDVfcKlJAKmpB6u5hUNNHguQZGgAUkBl87MSP-FVtwCibn9W4ju86g5A1N0nZN4DiLr_WUkMDMyZWQC5l2nQ","sizes":\{"file":25273827,"external":1190561,"active":3046761},"purge_seq":0,"other":\{"data_size":1190561},"doc_del_count":0,"doc_count":1113,"disk_size":25273827,"disk_format_version":6,"data_size":3046761,"compact_running":false,"instance_start_time":"0"}   clsnet@dusd1devrhap065 ~ $ curl -H "Content-Type: application/json" -X GET  http://dusd1devrhap063.dev.local:41006/node1-node2/statedb_savepoint  \{"_id":"statedb_savepoint","_rev":"645-60eae372ea44f412bf5627284cc560df","BlockNum":644,"TxNum":1,"UpdateSeq":"2517-g1AAAAE2eJzLYWBg4MhgTmHgzcvPy09JdcjLz8gvLskBCjMlMiTJ____PyvxPQ4FSQpAMskerOYeLjUOIDXxYDVfcKlJAKmpB6u5hUNNHguQZGgAUkBl87MSP-FVtwCibn9W4ju86g5A1N0nZN4DiLr_WUkMDMwZWQC5lWnP"}  ></body> </Action>
<Action id="35656" issue="24540" author="denyeart" type="comment" created="2017-11-29 21:47:44.0" updateauthor="denyeart" updated="2017-11-29 21:47:44.0"> <body><! CDATA  ~rhegde  Great find.  In order to troubleshoot we will need the peer log at debug level at the time of crash and upon the next start.  In the peer start you will see two INFO messages like this:  2017-11-29 21:28:47.892 UTC  ledgermgmt  OpenLedger -> INFO 1c5 Opening ledger with id = myc 2017-11-29 21:28:47.909 UTC  ledgermgmt  OpenLedger -> INFO 1e5 Opened ledger with id = myc  Between these two messages, if debug is enabled you will see recovery logic that checks the savepoint and if the savepoint doesn't match block height, it should re-apply the valid transactions from the blocks that are missing in state db.  You would see this in peer debug messages, right before the "Opened ledger" message.  In our trials, we are not able to reproduce the lost data issue (it always gets recovered correctly), therefore we need to read your debug logs.  Are you able to reproduce with debug level?  Please post the debug log, as well as block number and key that are missing.  ></body> </Action>
<Action id="35694" issue="24540" author="rhegde" type="comment" created="2017-11-30 22:48:54.0" updateauthor="rhegde" updated="2017-11-30 22:48:54.0"> <body><! CDATA  Update    ~denyeart  after some retries - we are again able to reproduce this issue by putting peer in debug mode. I will share all the logs, state db snapshot soon.  ></body> </Action>
<Action id="35933" issue="24540" author="rhegde" type="comment" created="2017-12-06 15:16:31.0" updateauthor="rhegde" updated="2017-12-06 15:20:34.0"> <body><! CDATA  ~denyeart   *Log Attachments*  All logs are uploaded for reference @  http://dts.cls-group.com/  (filename: FAB7156-NonDeterministicTransactionProcessing-5Dec2017.zip) and the credential to access is available with  ~Clayton Sims  or  ~altharp . Please check for logs.  Following is the folder structuring used for capturing the logs, bar2xxxx and ibm2xxxx are the 2 organization node folder.  bar2xxxx node's couchdb container was continuously shutdown in this logs capture.  bar2XXXX * docker.ps.XXX (contains the snapshot of docker ps at that point of time) * f4dc769a87b9.log (where f4dc769a87b9 is the container ID and can be mapped from docker.ps) * statedb_savepoint.json * all_docs.log (all documents from the couch db)  Same logs naming convention is used for the other node ibm2XXXX  *Summary of the mismatch* # Block 848, can be checked as this is missing json documents in bar2xxx and present in ibm2xxxx. # Please note - we have done multiple runs on same channel (peer-2-peer channel) which is bar2XXX-ibm2XXX.     Let me know for questions.  ></body> </Action>
<Action id="36003" issue="24540" author="denyeart" type="comment" created="2017-12-08 00:05:01.0" updateauthor="denyeart" updated="2017-12-08 00:05:01.0"> <body><! CDATA  ~rhegde  I have analyzed the debug for block 848. No data was applied to bar2ldn1 for three keys because their transactions were marked as invalid. They were marked invalid on this node because the associated keys were not found in state db at validation time, but the read set indicated they should have already existed in state db. I can only assume the prior transaction(s) state db write for these keys got missed. Unfortunately the prior transactions for these keys are not included in the provided debug logs so we cannot be sure of root cause. Do you have debug logs for the prior blocks/transactions that included these keys? You can find the keys below as well as the version mismatch and invalidation messages.  You could also do a fresh run and provide debug logs from genesis block until the problem occurs. {noformat}  36m2017-11-30 22:24:06.830 CET  statevalidator  validateKVRead -> DEBU d227 0m Version mismatch for key  p2pcls:CLS002_GC0C5608G100127D . Committed version =  %!s(*version.Height=<nil>) , Version in readSet  block_num:806 tx_num:2    33m2017-11-30 22:24:06.830 CET  statevalidator  ValidateAndPrepareBatch -> WARN d228 0m Block  848  Transaction index  0  TxId  1a16218e7fbfcbdbfe9c4e92a3d22cf566123327f72d5a5ffda7d4dd6b35a514  marked as invalid by state validator. Reason code  11  ...  36m2017-11-30 22:24:06.834 CET  statevalidator  validateKVRead -> DEBU d23b 0m Version mismatch for key  p2pcls:CLS002_GC0C5608G100127E . Committed version =  %!s(*version.Height=<nil>) , Version in readSet  block_num:806 tx_num:3    33m2017-11-30 22:24:06.834 CET  statevalidator  ValidateAndPrepareBatch -> WARN d23c 0m Block  848  Transaction index  1  TxId  35979ad62c0bbc58297e48baf796c17d93c66acdd94dc651866be26076337494  marked as invalid by state validator. Reason code  11  ...  36m2017-11-30 22:24:06.838 CET  statevalidator  validateKVRead -> DEBU d24f 0m Version mismatch for key  p2pcls:CLS002_GC0C5608G100127F . Committed version =  %!s(*version.Height=<nil>) , Version in readSet  block_num:807    33m2017-11-30 22:24:06.838 CET  statevalidator  ValidateAndPrepareBatch -> WARN d250 0m Block  848  Transaction index  2  TxId  ef1a47db16d9b1c9da3d70831a1d79e2e33afe729b2909898008858509c13c6b  marked as invalid by state validator. Reason code  11  {noformat} There is also a CouchDB setting that may be worthwhile to try to see if it fixes the problem, change delayed_commits = true to delayed_commits = false in CouchDB local.ini (change in both /opt/couchdb/etc and /opt/couchdb/etc/local.d directories to ensure the change is effective). This will ensure CouchDB flushes changes to disk even if the peer flushing/savepoint/recovery logic is not working correctly. This would help pinpoint the problem. For more details on the config see:  http://docs.couchdb.org/en/2.0.0/config/intro.html   ></body> </Action>
<Action id="36019" issue="24540" author="manish-sethi" type="comment" created="2017-12-08 15:50:06.0" updateauthor="manish-sethi" updated="2017-12-08 15:50:06.0"> <body><! CDATA I browsed through the validation and commit path of the code in order to get some hint for this problem. I bumped into a bug in the code that seems a a potential reason for this problem.  In this line (https://github.com/hyperledger/fabric/blob/release/core/ledger/kvledger/txmgmt/validator/statebasedval/state_based_validator.go#L200), an error is being ignored which would result in marking the transaction invalid.  If this version of the code is being used in the above environment, then in my opinion the best option would be to try with a fix for this.  ></body> </Action>
<Action id="36032" issue="24540" author="rhegde" type="comment" created="2017-12-08 17:52:24.0" updateauthor="rhegde" updated="2017-12-08 17:52:24.0"> <body><! CDATA  ~manish-sethi   As discussed, we have uploaded another archive, FAB7156-NonDeterministicTransactionProcessing-8Dec2017.zip which represents for comment added (https://jira.hyperledger.org/browse/FAB-7156?focusedCommentId=35653&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-35653). Please check if this represents the same root-cause. Thanks.  Archive: please note this snapshot was taken little earlier - however the document lost count difference is the same: * Diff-file1-file2*.txt contains the missing documents * file1.json - list of all json documents from node1 * file2.json - list of all json documents from node2 * bar2xxx, ibm2xxx - contains the peer log at INFO level  ></body> </Action>
<Action id="36051" issue="24540" author="manish-sethi" type="comment" created="2017-12-08 23:25:52.0" updateauthor="manish-sethi" updated="2017-12-08 23:26:04.0"> <body><! CDATA Submitted the bug fix for the release branch. It's already fixed in master branch.  https://gerrit.hyperledger.org/r/#/c/16059/  ></body> </Action>
<Action id="36055" issue="24540" author="baohua" type="comment" created="2017-12-09 01:18:39.0" updateauthor="baohua" updated="2017-12-09 01:18:39.0"> <body><! CDATA  ~manish-sethi , thanks for the fix.  While in this case, if for only some peers, there is error, what could happen in the network?  There will be still inconsistency in the stateDB among peers?  Is there any way to recovery?  Thanks!  ></body> </Action>
<Action id="36056" issue="24540" author="manish-sethi" type="comment" body=" ~baohua  - with the fix, the peer that encounters an error, does not commit the block and propagates the error upward path. Which means that the block can again be tried for commit. There won&apos;t be a data inconsistency but the peer may be well behind others till the error is fixed." created="2017-12-09 02:04:44.0" updateauthor="manish-sethi" updated="2017-12-09 02:04:44.0"/>
<Action id="36058" issue="24540" author="baohua" type="comment" created="2017-12-09 09:04:25.0" updateauthor="baohua" updated="2017-12-09 09:04:25.0"> <body><! CDATA  ~manish-sethi , got the idea.  And how many times for retry?  If the error continue happens, will the peer stop committing, or ignore the block?  I think all these problems need to be addressed?  ></body> </Action>
<Action id="36063" issue="24540" author="mastersingh24" type="comment" body="https://gerrit.hyperledger.org/r/16059" created="2017-12-09 10:31:02.0" updateauthor="mastersingh24" updated="2017-12-09 10:31:02.0"/>
<Action id="36067" issue="24540" author="denyeart" type="comment" created="2017-12-09 13:38:43.0" updateauthor="denyeart" updated="2017-12-09 13:38:43.0"> <body><! CDATA  ~baohua  There are already configurable retries between peer and couchdb.  Here is a more complete explanation I added to gerrit:  So at the couchdb layer, any 404 not found error will get turned into nil versionedValue, which matches the read set nil when the key has never been created by prior tran in state db. So that means any errors coming from couchdb layer (after retries are exhausted) is a true unexpected error which should cause block processing to halt. And the error here gets propagated all the way up to gossip state.go when it calls to commitBlock(), which will panic the peer:  logger.Panicf("Cannot commit block to the ledger due to %s", err).  Then once the couchdb system problem is corrected (e.g. restart couchdb), then when you bring the peer back up it will process the block successfully.  Now, gossip's block processing may also want to add a retry before panic at that layer.  What do you think  ~yacovm ?  ></body> </Action>
<Action id="36071" issue="24540" author="yacovm" type="comment" created="2017-12-09 17:13:39.0" updateauthor="yacovm" updated="2017-12-09 17:14:20.0"> <body><! CDATA {quote}Now, gossip's block processing may also want to add a retry before panic at that layer.  What do you think  ~yacovm ? {quote}    I would disagree.  The retry logic should be different for different types of DB providers, i.e in couchDB it would make sense to have a long sleep between retries, and multiple retries, but in an embedded DB that just writes to the disk like levelDB/rocksDB it wouldn't make any sense to retry for a long period of time.   For this reason - it doesn't make sense to put that in gossip.     I would also add and say that if such a retry logic is implemented (in a lower layer than gossip) then the panic should be moved there.   ~C0rWin  wdyt?  ></body> </Action>
<Action id="36072" issue="24540" author="c0rwin" type="comment" body="I do not think that gossip should handle re-tries logic in case of DB failures and inability of ledger to commit the block and yes this is sounds reasonable that panic would be moved into lower layers rather than gossip." created="2017-12-09 22:28:12.0" updateauthor="c0rwin" updated="2017-12-09 22:28:12.0"/>
<Action id="36074" issue="24540" author="denyeart" type="comment" body=" ~yacovm   ~C0rWin    Right - ledger layer should and does handle retry and error logic based on the specific state db.  Other non-database errors such as during block validation or block unmarshalling also can return an unexpected error up to gossip layer, triggering peer panic since block processing cannot continue. I was curious if anybody thought some of these types of errors may be recoverable upon retry rather than panic.  ~baohua  raised the question of retries here, but let&apos;s discuss in rocket.chat since it is not directly related to this JIRA item." created="2017-12-10 06:05:27.0" updateauthor="denyeart" updated="2017-12-10 06:05:27.0"/>
<Action id="36076" issue="24540" author="c0rwin" type="comment" body=" ~denyeart  if failure is due to inability to unmarshal structure or due to validation, there is very unlikely that after re-try it will succeed. On contrary if the failure is due some disk or temporary database dis-connectivity re-trying makes a lot of sense to me. Therefore if you saying that DB and disk related failure has been handled by ledger with respect to giving it another try, I do not see what else could be done. wdyt  ~yacovm ?  " created="2017-12-10 07:43:20.0" updateauthor="c0rwin" updated="2017-12-10 07:43:20.0"/>
<Action id="36077" issue="24540" author="yacovm" type="comment" body="Agreed" created="2017-12-10 08:50:42.0" updateauthor="yacovm" updated="2017-12-10 08:50:42.0"/>
