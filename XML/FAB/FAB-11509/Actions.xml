<Action id="48530" issue="32652" author="chandergovindarajan" type="comment" created="2018-08-08 06:26:21.0" updateauthor="chandergovindarajan" updated="2018-08-08 06:26:21.0"> <body><! CDATA The docker containers are on 2 completely different networks and will not be able to reach each other.  You have 2 options: 1. Use host network with your docker containers and expose ports as needed. Use the public ip:port (not the internal 127.0.0.0/8 loopback ip) on both sides to refer to each other. 2. Use a docker overlay network (using swarm) to be able to refer to containers using their names (like peer1.org1.example.com) from one VM to another. See of eg: https://docs.docker.com/network/overlay/. This (https://medium.com/@wahabjawed/hyperledger-fabric-on-multiple-hosts-a33b08ef24f) is a blog on running Fabric containers on multiple hosts using this approach.   ></body> </Action>
<Action id="48546" issue="32652" author="aspring" type="comment" created="2018-08-08 09:43:53.0" updateauthor="aspring" updated="2018-08-08 09:43:53.0"> <body><! CDATA  ~ChanderGovindarajan  thanks for your informative response.  A thought was if Option 1 would be a quicker solution I'll take router first but if that's not then I'll go with Option 2.  For option 1, here's my VM1's ifconfig result:  br-8bed47f5baa1: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500         inet 172.18.0.1  netmask 255.255.0.0  broadcast 0.0.0.0         ether 02:42:a0:43:de:fe  txqueuelen 0  (Ethernet)         RX packets 0  bytes 0 (0.0 B)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 0  bytes 0 (0.0 B)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  br-c1cd2f49d8a5: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500         inet 172.22.0.1  netmask 255.255.0.0  broadcast 0.0.0.0         ether 02:42:11:95:a9:10  txqueuelen 0  (Ethernet)         RX packets 0  bytes 0 (0.0 B)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 0  bytes 0 (0.0 B)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500         inet 172.17.0.1  netmask 255.255.0.0  broadcast 0.0.0.0         ether 02:42:89:50:3f:90  txqueuelen 0  (Ethernet)         RX packets 0  bytes 0 (0.0 B)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 0  bytes 0 (0.0 B)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  enp0s3: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500         inet 10.0.2.15  netmask 255.255.255.0  broadcast 10.0.2.255         inet6 fe80::14f0:968b:a497:7151  prefixlen 64  scopeid 0x20<link>         ether 08:00:27:68:c1:41  txqueuelen 1000  (Ethernet)         RX packets 1049  bytes 1075641 (1.0 MB)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 448  bytes 64941 (64.9 KB)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536         inet 127.0.0.1  netmask 255.0.0.0         inet6 ::1  prefixlen 128  scopeid 0x10<host>         loop  txqueuelen 1000  (Local Loopback)         RX packets 299  bytes 20187 (20.1 KB)  Here's my VM2's ifconfig result:  br-39c1d48adfe6: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500         inet 172.18.0.1  netmask 255.255.0.0  broadcast 0.0.0.0         ether 02:42:8e:32:2c:4a  txqueuelen 0  (Ethernet)         RX packets 0  bytes 0 (0.0 B)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 0  bytes 0 (0.0 B)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  docker0: flags=4099<UP,BROADCAST,MULTICAST>  mtu 1500         inet 172.17.0.1  netmask 255.255.0.0  broadcast 0.0.0.0         ether 02:42:07:a4:89:11  txqueuelen 0  (Ethernet)         RX packets 0  bytes 0 (0.0 B)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 0  bytes 0 (0.0 B)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  enp0s3: flags=4163<UP,BROADCAST,RUNNING,MULTICAST>  mtu 1500         inet 10.0.2.15  netmask 255.255.255.0  broadcast 10.0.2.255         inet6 fe80::ee58:744f:192c:8120  prefixlen 64  scopeid 0x20<link>         ether 08:00:27:19:cb:f0  txqueuelen 1000  (Ethernet)         RX packets 1025  bytes 1074143 (1.0 MB)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 433  bytes 63730 (63.7 KB)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  lo: flags=73<UP,LOOPBACK,RUNNING>  mtu 65536         inet 127.0.0.1  netmask 255.0.0.0         inet6 ::1  prefixlen 128  scopeid 0x10<host>         loop  txqueuelen 1000  (Local Loopback)         RX packets 214  bytes 15990 (15.9 KB)         RX errors 0  dropped 0  overruns 0  frame 0         TX packets 214  bytes 15990 (15.9 KB)         TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0  But none of them indicate my actually public IP address, which is 73.237.253.1xx  So, could you elaborate on how to go about with Option 1?  Much appreciated.        ></body> </Action>
<Action id="48551" issue="32652" author="chandergovindarajan" type="comment" created="2018-08-08 10:55:29.0" updateauthor="chandergovindarajan" updated="2018-08-08 10:55:29.0"> <body><! CDATA You would need to ensure that the 2 VMs are able to communicate with each other. Follow corresponding tutorials for your hypervisor (for eg for VirtualBox, https://superuser.com/questions/626342/ssh-from-one-virtual-machine-to-another-in-virtualbox).  Please do close this issue as it is not really a bug in Fabric.  In any case, I recommend moving this discussion to RocketChat (https://chat.hyperledger.org/).   ></body> </Action>
<Action id="48590" issue="32652" author="aspring" type="comment" created="2018-08-09 01:25:09.0" updateauthor="aspring" updated="2018-08-09 01:25:09.0"> <body><! CDATA  ~ChanderGovindarajan  "You would need to ensure that the 2 VMs are able to communicate with each other. ", that's exactly what I intended to achieve.  While I appreciate the info, however, none of the suggestions posted in 2013 solved the IP differentiation of two VMs created via virtualbox v 5.2.14.  Sure, I'll close this ticket as soon as we make some progress on this peer to peer connection problem.  ></body> </Action>
