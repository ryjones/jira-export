<Issue id="42055" key="FAB-16435" number="16435" project="10002" reporter="denyeart" assignee="denyeart" creator="denyeart" type="10001" summary="Improve default configuration for gossip" priority="3" resolution="10000" status="6" created="2019-08-27 04:06:21.0" updated="2020-07-08 14:52:14.0" resolutiondate="2020-07-01 12:21:08.0" votes="0" watches="5" workflowId="55099"> <description><! CDATA *Overview*  Some Fabric networks have seen issues with excessive gossip communications, memory usage, or grpc max message size.  We have an opportunity to improve some of the default peer settings, for example I propose: {code:java} CORE_PEER_GOSSIP_USELEADERELECTION = false  CORE_PEER_GOSSIP_ORGLEADER = true  CORE_PEER_GOSSIP_MAXBLOCKCOUNTTOSTORE = 10  CORE_PEER_GOSSIP_STATE_ENABLED = false CORE_PEER_GOSSIP_STATE_BLOCKBUFFERSIZE = 20   CORE_PEER_GOSSIP_STATE_BATCHSIZE = 10 {code} *More details*  We have found that unless there are a large number of peers in an organization, it is more efficient to have peers pull blocks directly from orderer nodes, by using this configuration: {code:java} CORE_PEER_GOSSIP_USELEADERELECTION = false  CORE_PEER_GOSSIP_ORGLEADER = true  CORE_PEER_GOSSIP_STATE_ENABLED = false {code} If there are a large number of channels or large block sizes, reduce the size of these configurations from default of 100 to improve memory usage (the latter setting is not needed if CORE_PEER_GOSSIP_STATE_ENABLED is false): {code:java} CORE_PEER_GOSSIP_MAXBLOCKCOUNTTOSTORE = 10  CORE_PEER_GOSSIP_STATE_BLOCKBUFFERSIZE = 20   {code} And if the block sizes are so large that you risk hitting grpc 100MB limit, reduce this setting (again, not needed if CORE_PEER_GOSSIP_STATE_ENABLED is false): {code:java} CORE_PEER_GOSSIP_STATE_BATCHSIZE = 10 {code} The CORE_PEER_GOSSIP_STATE_* settings were added in v1.4.1 and above.  These are proposed tactical improvements, while longer term improvements are identified in FAB-15317.  ></description> </Issue>
