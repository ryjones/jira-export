<Issue id="36090" key="FAB-13247" number="13247" project="10002" reporter="tock" assignee="tock" creator="tock" type="10003" summary="Recover from orderer failure in the middle of migration" priority="3" resolution="10001" status="6" created="2018-12-12 07:55:57.0" updated="2019-04-04 07:42:03.0" resolutiondate="2019-04-04 07:38:58.0" votes="0" watches="1" workflowId="47605"> <description><! CDATA *(Work in progress)*  This task addresses one main concern - an OSN that crashes before or during migration, and recovers during or after migration.  When this happens, there are two things that need to be done:  1. Restore the migration status of every chain  * While initializing the registrar, the last config block of every channel is retrieved. This is the time to inspect the ConsensusType.MigrationStatus & ConsensusType.MigrationContext of the chain, and restore the chain.MigrationStatus to its value before the crash. * It is necessary to change the start order of the chains, and start them only after the chain.MigrationStatus of all of them had been restored.  2. Carefully consume messages from Kafka in order to respect the order of migration commands * After the chains are started, they begin consuming messages from Kafka. Each chain consumes from a different topic. Messages on a single topic are ordered, but messages between topics may be consumed at arbitrary order, subject to thread scheduling. * Thus, although in the "live" set of OSN's messages where consumed in the order prescribed by the protocol ( see here| https://docs.google.com/document/d/1g_M8KO9cdbYMLBsscHd6ffv-X_ZRLeHvDcIEoV7YXUE/edit?usp=sharing ),  a recovering node may consume they out of order. For example, on OSN-1 to OSN-J commands execute in order:  START (on sys-channel) ->                                            >->CONTEXT (on chan-A) ->                                             >->CONTEXT (on chan-B) ->                                                                                        >-> COMMIT(on sys-channel)  (green path)     START (on sys-channel) ->                                            >->CONTEXT (on chan-A) ->  >->ABORT (on chan-A) ->                                                                                              >->ABORT (on chan-B) ->                                                                                                                                         >-> ABORT(on sys-channel)  (abort path)    * However, OSN-K wakes up and consumes from the channels (system/A/B) in arbitrary order, and may encounter a CONTEXT before the system channels consumes and commits the START. If the node drops the CONTEXT, a fork will be formed. This must be prevented. * Note that an OSN that recovers in the middle or after migration presents the same problems as a "slow" OSN in which the threads are slow to consume from the topics.  *   The key to handling these scenarios is the following observation: when a node consumes from Kafka, it can be sure that a correct node allowed the message into Kafka and respected the order exemplified above. Thus, when a go-routine encounters a CONTEXT for example, it can be sure that the system channel go-routine already encountered a START, or will do so in the near future. It can therefore wait for this condition to materialize. In summary: * A CONTEXT on a standard channel waits for a START on the system channel * A COMMIT on the system channel waits for all standard channels to be in CONTEXT * An ABORT on the system channel waits for all standard channels to be in ABORT     Failure scenarios: * Recover from failure in the middle of migration before commit, single node, recover migration Status during init:  # after START, some standard channel in CONTEXT.  # after START, some standard channel in CONTEXT. some in ABORT # after ABORT, system channel in ABORT, all standard channels in ABORT   # during COMMIT: bootstrap block replaced, block not written to ledger     Misc. Tasks: * refactor the migration package type names and structure to respect review comments in green path #2 CR * expose the DetectMigration(lastBlock) from chainsupport.go on the ConsenterSupport interface and reuse it in etcdraft/chain.go * decide if to panic or error on unmarshal failures after consuming transactions from Kafka     ></description> </Issue>
