<Action id="37861" issue="25000" author="amandahla" type="comment" body="What would be better here? A parameter in &quot;peer chain package&quot; so we can set this on the CDS? Or &quot;peer chain instantiate&quot; passing a parameter to indicate Kubernets usage?" created="2017-12-29 17:55:21.0" updateauthor="amandahla" updated="2017-12-29 19:06:18.0"/>
<Action id="37871" issue="25000" author="chawlanikhil24" type="comment" body="I would suggest a parameter in &quot;peer chaincode instantiate&quot; , and if the parameter is not passed then by default the chaincode instantiation execute in Docker. " created="2017-12-31 07:55:09.0" updateauthor="chawlanikhil24" updated="2017-12-31 07:55:09.0"/>
<Action id="37873" issue="25000" author="amandahla" type="comment" created="2018-01-02 11:24:44.0" updateauthor="amandahla" updated="2018-01-02 11:24:44.0"> <body><! CDATA Great.  I started some work here:   https://github.com/hyperledger/fabric/pull/92   I don't know why didn't appear here at Jira.  ></body> </Action>
<Action id="37874" issue="25000" author="yacovm" type="comment" created="2018-01-02 11:48:20.0" updateauthor="yacovm" updated="2018-01-02 11:48:20.0"> <body><! CDATA Hey  ~amandahla   Pull requests are being done via gerrit, not github. The github is used only for mirroring gerrit. Just in case it is useful for anyone I will link here a  tutorial|https://hyperledger-fabric.readthedocs.io/en/latest/submit_cr.html .   ~chawlanikhil24  ,  ~amandahla  - I personally think that adding a new field to the chaincode package isn't the right way to go, and instead it should be defined via local configuration (core.yaml, or the environment variable equivalent config).  Let me elaborate:  # If you extend the chaincode package to support k8s , since most likely the peer would need credentials to connect to k8s , that would mean the credentials would be stored in the chaincode package. IMO credentials should be avoided to be passed over the network as much as possible. # Another reason for not doing it, is that we store the hash of the chaincode package inside the ledger, and this is public knowledge to anyone in the channel - meaning that if you put credentials inside the chaincode package then you incentivize channel members to try and guess the credentials, since they know the input to the hash except from the credentials. # This leads to another problem - the peer ensures the hash of the chaincode package in the ledger is equal to the hash of the chaincode package in its file system. If you have 2 organizations that use 2 different deployments of Kubernetes that means that they can never do that, since only 1 definition in the ledger is possible, however you have 2 inputs (different credentials per k8s deployment), and as we know - a hash function should be collision resistant... # The chaincode, is in a way - an extension of the peer. The peer controls the chaincode lifecycle, and not the administrator.  Therefore I suggest it should be configured locally in the peer's core.yaml config.  Others might also want to chime in:   ~C0rWin   ~mastersingh24  ,  ~greg.haskins  ,  ~muralisr  ,   ></body> </Action>
<Action id="37875" issue="25000" author="amandahla" type="comment" created="2018-01-02 12:24:24.0" updateauthor="amandahla" updated="2018-01-02 12:24:24.0"> <body><! CDATA Thanks  ~yacovm  !  Sorry, I'll try to remove the PR.  I agree with your suggestion. What do you think about using inClusterConfig system, by default, to connect to kubernetes API?  Like Ingress-nginx does:  https://github.com/kubernetes/ingress-nginx/blob/dcf4a4595eb558d9b8ed3eb48db08ad5c9d82a34/cmd/nginx/main.go#L211  ></body> </Action>
<Action id="37876" issue="25000" author="christopherferris" type="comment" body="I like the direction this is headed. Agree with  ~yacovm  that it should be part of the core.yaml config. I suspect that there may be use cases where there may be multiple chaincode deployment options in play in a network that leverages different platform for deployment of the peer nodes (e.g. some may choose k8s and others docker swarm or something else)." created="2018-01-02 12:34:32.0" updateauthor="christopherferris" updated="2018-01-02 12:34:32.0"/>
<Action id="37877" issue="25000" author="christopherferris" type="comment" body="Note that I think this should be a post 1.1 feature, and adapted the affectsVersion and fixVersion accordingly. We really need to lock down 1.1 function for now." created="2018-01-02 12:37:47.0" updateauthor="christopherferris" updated="2018-01-02 12:37:47.0"/>
<Action id="37878" issue="25000" author="yacovm" type="comment" created="2018-01-02 12:44:09.0" updateauthor="yacovm" updated="2018-01-02 12:45:43.0"> <body><! CDATA Obviously it would be a post v1.1 feature :)    ~amandahla  I don't know much about kubernetes but I'd guess that having core.yaml just point to the kubeConfig path would make sense...  We don't want to have core.yaml baloon to an even more bigger size ;)   ></body> </Action>
<Action id="37880" issue="25000" author="amandahla" type="comment" body=" ~yacovm  my team just notice another thing here... Kubernetes needs the image available in a Docker hub. So...if we want to use Kubernetes, we need the Docker hub configuration also (url and certificate path). Can we put this on core.yaml? (sorry, bigger and bigger ;) )" created="2018-01-02 12:51:44.0" updateauthor="amandahla" updated="2018-01-02 12:51:44.0"/>
<Action id="37881" issue="25000" author="yacovm" type="comment" created="2018-01-02 13:03:15.0" updateauthor="yacovm" updated="2018-01-02 13:04:03.0"> <body><! CDATA My  guess would be that docker hub exposes some API for docker and Kubernetes calls this too, and if you want to host a private instance of docker hub you can still do that  {quote}  Can we put this on core.yaml? {quote}  Can't this be part of the same k8s-only config file?  ></body> </Action>
<Action id="37883" issue="25000" author="amandahla" type="comment" created="2018-01-02 13:40:49.0" updateauthor="amandahla" updated="2018-01-02 13:40:49.0"> <body><! CDATA I'm not sure.  The workflow would be something like this:  1) peer chaincode install -> create the image in the docker registry  2) peer chaincode instantiate -> create the pod at kubernetes instead of container at docker     ></body> </Action>
<Action id="37884" issue="25000" author="yacovm" type="comment" created="2018-01-02 14:01:18.0" updateauthor="yacovm" updated="2018-01-02 14:14:27.0"> <body><! CDATA So assuming the upload process doesn't need to store any unique identifier returned from the API call,  I think that install should: # Place the chaincode inside the peer's file system # Upload it to docker-hub (or the equivalent in the organization)   Therefore it would make sense to store both the dockerhub credentials and the k8s credentials inside the peer's configuration, be it in the same file, or separate file... I guess we can have 2 lines in the core.yaml - 1 to point to the k8s file and the other to docker hub file  Now, the instantiate actually has nothing to do with whether the chaincode is launched or not. In fabric, chaincodes are launched by demand and the launch is orthogonal to the channel. For example- if you have a chaincode that is instantiated in 2 different channels and the peer joined both of these channels, then it would only launch once, not twice, though it has been instantiated twice.   Also, chaincode is also launched at invoke (since it's by demand). The instantiate process is actually 2 different things which are entangled into 1: # Create an association between the chaincode package to the channel (hash, version, VSCC, ESCC, etc.) # Invoke the Init() method of the chaincode   The chaincode is launched in instantiate because of the 2nd reason but it may also not launch if it's already running because a previous instantiate/invoke in a context of a different channel.    ></body> </Action>
<Action id="37888" issue="25000" author="amandahla" type="comment" created="2018-01-02 17:20:06.0" updateauthor="amandahla" updated="2018-01-02 17:22:39.0"> <body><! CDATA If I get it right, we need: * check if user selected kubernetes instead of docker * if positive, the chaincode install process will publish the image using the repository (docker hub for example) configuration at core.yaml * add a kubernetes controller as exists for docker (fabric/core/container/)  If this controller works fine, the chaincode instantiate and/or launch process doesn't need any changes.  ></body> </Action>
<Action id="37891" issue="25000" author="amandahla" type="comment" body="It might be a little off topic but...Do you think that using  Cello|https://github.com/hyperledger/cello  would be a faster solution for that?" created="2018-01-02 17:45:24.0" updateauthor="amandahla" updated="2018-01-02 17:45:24.0"/>
<Action id="37893" issue="25000" author="yacovm" type="comment" created="2018-01-02 17:56:34.0" updateauthor="yacovm" updated="2018-01-02 17:56:34.0"> <body><! CDATA We need to check if the peer has been configured to use K8s instead of docker. correct about the 2nd and 3rd bullets.  {quote}If this controller works fine, the chaincode instantiate and/or launch process doesn't need any changes.{quote} Right, that would be my assumption.  ></body> </Action>
<Action id="37894" issue="25000" author="yacovm" type="comment" created="2018-01-02 17:57:09.0" updateauthor="yacovm" updated="2018-01-02 17:57:09.0"> <body><! CDATA {quote}It might be a little off topic but...Do you think that using Cello would be a faster solution for that?{quote}   Cello is a project for deploying fabric, it has nothing to do with the fabric core.  ></body> </Action>
<Action id="37925" issue="25000" author="amandahla" type="comment" created="2018-01-03 16:47:31.0" updateauthor="amandahla" updated="2018-01-03 16:47:31.0"> <body><! CDATA We noticed something...If we create the container at the chaincode install, we would be stilll using the /var/run/docker.sock. The security problem persists.  We need a way to not use this and use only kubernetes.  Has this been discussed before?  References:   https://raesene.github.io/blog/2016/03/06/The-Dangers-Of-Docker.sock/   https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container.html  ></body> </Action>
<Action id="37926" issue="25000" author="yacovm" type="comment" body="I think you can configure the peer to use an HTTP(S) endpoint...  https://github.com/hyperledger/fabric/blob/release/sampleconfig/core.yaml#L269-L271" created="2018-01-03 16:51:40.0" updateauthor="yacovm" updated="2018-01-03 16:53:35.0"/>
<Action id="37937" issue="25000" author="christopherferris" type="comment" body=" ~yacovm , I&apos;m unclear on why we need the peer to build the image. I was thinking that could be independent and that we would have a local  DTR|https://docs.docker.com/datacenter/dtr/2.4/guides/  for the network to which network participants would upload images. The peer would then simply ask Docker (or K8s) to spin up the image. Installing chaincode would just be a ledger transaction then (the reference to the chaincode image would be stored in the blockchain)." created="2018-01-03 18:25:54.0" updateauthor="christopherferris" updated="2018-01-03 18:25:54.0"/>
<Action id="37938" issue="25000" author="amandahla" type="comment" created="2018-01-03 18:30:12.0" updateauthor="amandahla" updated="2018-01-03 18:32:33.0"> <body><! CDATA According to our security team, using HTTPS is even worse... :(  So, another idea: 1) check if user selected kubernetes instead of docker 2) if positive, the chaincode install process will create a pod using the already published image at chaincode.builder and chaincode.golang.runtime using the repository (docker hub for example) configuration at core.yaml 2.1) we made the customizations via Kubernetes API 2.2) This pod will have two containers: builder and runtime. Both access the same volume. If build is ok, the executable will be available for the runtime container. The builder will be a init container.  Reference:  https://kubernetes.io/docs/concepts/workloads/pods/init-containers/   ></body> </Action>
<Action id="37940" issue="25000" author="yacovm" type="comment" created="2018-01-03 18:43:06.0" updateauthor="yacovm" updated="2018-01-03 18:43:06.0"> <body><! CDATA  ~ChristopherFerris  , I'm not against the peer not building the image and just having a reference to where its hosted. Though I think we should not require users to install another dependency (DTR) if they want to just run fabric on "easy mode".  {quote}Installing chaincode would just be a ledger transaction then (the reference to the chaincode image would be stored in the blockchain).{quote} The current lifecycle is that the you put data in the ledger at instantiate, not installation.  I'm also not sure why we would want to store a reference for the chaincode image on the blockchain. What if multiple orgs use different DTRs? You can only store a single reference, which one do you pick?    ~amandahla  : {quote}According to our security team, usings HTTPS is even worse... {quote} Can you ask them for some reasoning and security concerns? If there is a security concern I think it is interesting and you should raise it.  {quote} 1) check if user selected kubernetes instead of docker 2) if positive, the chaincode install process will create a pod using the already published image at chaincode.builder and chaincode.golang.runtime using the repository (docker hub for example) configuration at core.yaml 2.1) we made the customizations via Kubernetes API 2.2) This pod will have two containers: builder and runtime. Both access the same volume. If build is ok, the executable will be available for the runtime container.{quote} I'm missing why we should have 2 containers? Do you mean just for the build process? I guess we can delete the old one once the "runtime" container is built...   ></body> </Action>
<Action id="37953" issue="25000" author="amandahla" type="comment" created="2018-01-03 19:23:42.0" updateauthor="amandahla" updated="2018-01-03 19:23:42.0"> <body><! CDATA With this feature of InitContainer, the builder container will be deleted after he completes his task. :)  I'm just afraid of what you said about "the peer ensures the hash of the chaincode package in the ledger is equal to the hash of the chaincode package in its file system." I didn't find where this happen and if we need to review this.  ></body> </Action>
<Action id="37972" issue="25000" author="christopherferris" type="comment" created="2018-01-03 21:01:18.0" updateauthor="christopherferris" updated="2018-01-03 21:01:18.0"> <body><! CDATA  ~yacovm  DTR would only be needed for actual deployments. A dev could simply use their local Docker registry. Hence, I don't think it complicates things more than they already are.  As to the point about using different DTRs. I believe that you can use it just as regular registry and cache images from a central source. So, we could have yacovchris/ourchaincode that we each cache in our respective DTRs from some shared central registry (DockerHub or a shared DTR).  ></body> </Action>
<Action id="38101" issue="25000" author="amandahla" type="comment" body=" ~ChristopherFerris  Following your idea, should the operator  upload the images and then set the names at core.yaml in chaincode.builder and chaincode.golang.runtime (for example) ?" created="2018-01-04 20:01:43.0" updateauthor="amandahla" updated="2018-01-04 20:01:43.0"/>
<Action id="38230" issue="25000" author="amandahla" type="comment" created="2018-01-05 17:09:24.0" updateauthor="amandahla" updated="2018-01-05 17:09:24.0"> <body><! CDATA Just to share: I'm trying to do something at github before send to gerrit.  https://github.com/estaleiro/fabric/tree/issue-7406  ></body> </Action>
<Action id="38383" issue="25000" author="john.d.sheehan" type="comment" body=" ~amandahla , would it be better to use k8 jobs instead of pods for building and running chaincode?" created="2018-01-10 11:34:34.0" updateauthor="john.d.sheehan" updated="2018-01-10 11:34:34.0"/>
<Action id="38424" issue="25000" author="amandahla" type="comment" created="2018-01-11 12:08:50.0" updateauthor="amandahla" updated="2018-01-11 12:09:39.0"> <body><! CDATA  ~john.d.sheehan  Actually I'm not sure...What would be the advantages? I'm afraid to add too much complexity.  I found a example at this project to have an idea.   Pachyderm| https://github.com/pachyderm/pachyderm/blob/805e63e561a9eab4a9c52216f228f0f421714f3b/src/server/pps/server/api_server.go#L169    ></body> </Action>
<Action id="38626" issue="25000" author="amandahla" type="comment" created="2018-01-15 13:09:45.0" updateauthor="amandahla" updated="2018-01-15 13:10:08.0"> <body><! CDATA I'm working with the ideia of deployments. Instead of create a builder container, get the binary and create a runtime container that will receive this file, we'll create a deployment with two containers that will share a volume. First I'll assume that the base images are already deployed in a docker registry.  What do you think?  Working here https://github.com/estaleiro/fabric/tree/issue-7406  ></body> </Action>
<Action id="38632" issue="25000" author="john.d.sheehan" type="comment" created="2018-01-15 14:22:06.0" updateauthor="john.d.sheehan" updated="2018-01-15 14:22:06.0"> <body><! CDATA Hi  ~amandahla , the reason I suggested jobs over pods is, with pods you will always need at least one running, but jobs; start, run to completion, then exit (or retry if failed to complete).  So the peer would start a job which would * build the cc * create a docker image * upload that image to docker registry Once uploaded the job would be complete  The peer would then use this image in another job (or pod)  ></body> </Action>
<Action id="40026" issue="25000" author="ublubu" type="comment" body="Hi! I&apos;m interested in helping out with this feature. What&apos;s the current state and how are you running the code?" created="2018-02-12 16:11:21.0" updateauthor="ublubu" updated="2018-02-12 16:11:21.0"/>
<Action id="40152" issue="25000" author="amandahla" type="comment" created="2018-02-14 16:17:20.0" updateauthor="amandahla" updated="2018-02-14 16:17:20.0"> <body><! CDATA Hi, great to hear that you wanna help!  Our idea: - Create a deployment with two containers: one for build and other for run the chaincode. They will share a volume so when the build container finish, the run container will have the executable.  Right now, I'm trying to think in a way put the code available to the build container. In some peer directory mapped as a volume, for example.  Another thing it's to make the chaincode comunicate with the peer.  I was working at here:  https://github.com/estaleiro/fabric/commit/61e519bfdf53ebec8bcb9bf2c7aaf7867177a012   But currently I'm doing local changes because the team warned me that they use version 1.0.4.  I've made changes to set this configuration variables: {code:java} CORE_PEER_USEKUBERNETES=true CORE_PEER_KUBERNETES_NAMESPACE=default CORE_PEER_KUBERNETES_KUBECONFIG=/home/user/.kube/config{code} Then, just to check if the peer create the chaincode using kubernetes, I have fixed on the code a deployment plan. This is it: {code:none}  deployment := &appsv1beta1.Deployment{ 		ObjectMeta: metav1.ObjectMeta{ 			Name: deploymentID, 		}, 		Spec: appsv1beta1.DeploymentSpec{ 			Replicas: int32Ptr(1), 			Template: apiv1.PodTemplateSpec{ 				ObjectMeta: metav1.ObjectMeta{ 					Labels: map string string{ 						"app": "fabric", 						"org.hyperledger.fabric.base.version":         "0.3.2", 						"org.hyperledger.fabric.chaincode.id.name":    "mycc", 						"org.hyperledger.fabric.chaincode.id.version": "1.0", 						"org.hyperledger.fabric.chaincode.type":       "GOLANG", 						"org.hyperledger.fabric.version":              "1.0.4", 					}, 				}, 				Spec: apiv1.PodSpec{ 					Containers:   apiv1.Container{ 						{ 							Name:    "fabric-chaincode-mycc-container", 							Image:   "myhub.com/fabric-chaincode-mycc:1.0", 							Command:   string{"chaincode", "-peer.address=peer0:7051"}, 							Env:   apiv1.EnvVar{ 								{Name: "CORE_CHAINCODE_ID_NAME", Value: "mycc:1.0"}, 								{Name: "CORE_PEER_TLS_ENABLED", Value: "true"}, 								{Name: "CORE_CHAINCODE_LOGGING_LEVEL", Value: "info"}, 								{Name: "CORE_CHAINCODE_LOGGING_SHIM", Value: "warning"}, 								{Name: "CORE_CHAINCCORE_CHAINCODE_LOGGING_FORMATODE_ID_NAME", Value: "%{color}%{time:2006-01-02 15:04:05.000 MST}  %{module}  %{shortfunc} -> %{level:.4s} %{id:03x}%{color:reset} %{message}"}, 								{Name: "PATH", Value: "/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin"}, 								{Name: "CORE_CHAINCODE_BUILDLEVEL", Value: "1.0.4"}, 								{Name: "CORE_PEER_TLS_ROOTCERT_FILE", Value: "/etc/hyperledger/fabric/peer.crt"}, 							}, 						}, 					}, 				}, 			}, 		}, 	} {code} I couldn't make the unit tests work so I compile using: {code:java} make configtxgen configtxlator cryptogen peer orderer make peer-docker{code} Then I installed minikube and test using this Dockerfile: {code:none} version: '2' networks: bridge:  services: orderer: container_name: orderer image: hyperledger/fabric-orderer environment: - ORDERER_GENERAL_GENESISPROFILE=SampleDevModeSolo - ORDERER_GENERAL_LEDGERTYPE=ram - ORDERER_GENERAL_BATCHTIMEOUT=10s - ORDERER_GENERAL_BATCHSIZE_MAXMESSAGECOUNT=10 - ORDERER_GENERAL_MAXWINDOWSIZE=1000 - ORDERER_GENERAL_ORDERERTYPE=solo - ORDERER_GENERAL_LISTENADDRESS=0.0.0.0 - ORDERER_RAMLEDGER_HISTORY_SIZE=100 - ORDERER_GENERAL_LOCALMSPDIR=/etc/hyperledger/fabric/msp/sampleconfig working_dir: /opt/gopath/src/github.com/hyperledger/fabric/orderer command: orderer volumes: - ../sampleconfig/msp/:/etc/hyperledger/fabric/msp/sampleconfig/ ports: - 7050:7050 networks: - bridge  peer0: container_name: peer0 image: hyperledger/fabric-peer environment: - CORE_PEER_ADDRESSAUTODETECT=true - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_PEER_NETWORKID=peer0 - CORE_NEXT=true - CORE_PEER_ENDORSER_ENABLED=true - CORE_PEER_ID=peer0 - CORE_PEER_PROFILE_ENABLED=true - CORE_PEER_COMMITTER_LEDGER_ORDERER=orderer:7050 - CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/msp/sampleconfig - CORE_PEER_USEKUBERNETES=true - CORE_PEER_KUBERNETES_NAMESPACE=default - CORE_PEER_KUBERNETES_KUBECONFIG=/home/user/.kube/config working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start links: - orderer:orderer volumes: - /var/run/:/host/var/run/ - /home/user/.kube/config:/home/user/.kube/config - /home/user/.minikube/:/home/user/.minikube/ #in the "- <HOST>:/opt/gopath/src/github.com/hyperledger/fabric/examples/" mapping below, the HOST part #should be modified to the path on the host. This will work as is in the Vagrant environment - /opt/gopath/src/github.com/hyperledger/fabric/examples/:/opt/gopath/src/github.com/hyperledger/fabric/examples/ - ../sampleconfig/msp/:/etc/hyperledger/fabric/msp/sampleconfig/ depends_on: - orderer networks: - bridge  peer1: container_name: peer1 image: hyperledger/fabric-peer environment: - CORE_PEER_ADDRESSAUTODETECT=true - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_PEER_NETWORKID=peer1 - CORE_NEXT=true - CORE_PEER_ENDORSER_ENABLED=true - CORE_PEER_ID=peer1 - CORE_PEER_PROFILE_ENABLED=true - CORE_PEER_COMMITTER_LEDGER_ORDERER=orderer:7050 - CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/msp/sampleconfig - CORE_PEER_USEKUBERNETES=true - CORE_PEER_KUBERNETES_NAMESPACE=default - CORE_PEER_KUBERNETES_KUBECONFIG=/home/user/.kube/config working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: peer node start links: - orderer:orderer volumes: - /var/run/:/host/var/run/ - /opt/gopath/src/github.com/hyperledger/fabric/examples/:/opt/gopath/src/github.com/hyperledger/fabric/examples/ - ../sampleconfig/msp/:/etc/hyperledger/fabric/msp/sampleconfig/ depends_on: - orderer networks: - bridge  cli: container_name: cli image: hyperledger/fabric-tools tty: true environment: - GOPATH=/opt/gopath - CORE_PEER_ADDRESSAUTODETECT=true - CORE_VM_ENDPOINT=unix:///host/var/run/docker.sock - CORE_NEXT=true - CORE_PEER_ID=cli - CORE_PEER_ENDORSER_ENABLED=true - CORE_PEER_COMMITTER_LEDGER_ORDERER=orderer:7050 - CORE_PEER_ADDRESS=peer0:7051 - CORE_PEER_MSPCONFIGPATH=/etc/hyperledger/fabric/msp/sampleconfig working_dir: /opt/gopath/src/github.com/hyperledger/fabric/peer command: /bin/sh links: - orderer:orderer - peer0:peer0 volumes: - /var/run/:/host/var/run/ - /home/user/Documentos/ESTALEIRO/hyperledger-fabric-git/src/github.com/hyperledger/fabric/examples/:/opt/gopath/src/github.com/hyperledger/fabric/examples/ - ../sampleconfig/msp/:/etc/hyperledger/fabric/msp/sampleconfig/ depends_on: - orderer - peer0 - peer1 networks: - bridge {code}  ></body> </Action>
<Action id="40164" issue="25000" author="amandahla" type="comment" created="2018-02-14 16:55:56.0" updateauthor="amandahla" updated="2018-02-14 16:55:56.0"> <body><! CDATA Update: I have uploaded the branch that I'm working with version 1.0.4   https://github.com/estaleiro/fabric/tree/issue-7406-1.0.4 | https://github.com/estaleiro/fabric/tree/issue-7406-1.0.4   ></body> </Action>
<Action id="40271" issue="25000" author="amandahla" type="comment" created="2018-02-15 19:01:53.0" updateauthor="amandahla" updated="2018-02-15 19:01:53.0"> <body><! CDATA Hi, another update: I needed to change the idea because of the "platform" concept.  1) Create a pod with one container using image in "chaincode.builder" 2) Extract chaincode code to "/chaincode/input" (kubectl cp?  Issue open|https://github.com/kubernetes/client-go/issues/369  ) 3) Build 4) If ok, create tar file with "/chaincode/output" 5) A targz file will have a Dockerfile, a cert (if TLS is set) and the binpackage.tar 6) Create a deployment with 1 container using args/envs received // to think: read dockerfile in targz received to get LABELs and ENV CORE_PEER_TLS_ROOTCERT_FILE 7) Upload and extract targz in /usr/local/bin 8) Command received in envs must be executed  ></body> </Action>
<Action id="40334" issue="25000" author="ublubu" type="comment" created="2018-02-18 03:08:49.0" updateauthor="ublubu" updated="2018-02-18 03:09:35.0"> <body><! CDATA Thanks for all the info! I'm just starting to look through it.  I wonder if it might be useful to split the chaincode management into its own set of services. That way, the core of Fabric doesn't have to depend on the Kubernetes client libraries. It might also make it easier to support other orchestration frameworks in the future.      I also think the right packaging for ready-to-run chaincode is a Docker image. If that's the way to go, then the `build` step should create a Docker image with the chaincode and upload it to an image registry. The `run` step is then just a Pod (or similar) using the image in the registry.  IIUC, that's similar to what the plain-Docker version does today. It builds an image for the local registry and runs that image.  ></body> </Action>
<Action id="40336" issue="25000" author="ublubu" type="comment" created="2018-02-18 04:47:14.0" updateauthor="ublubu" updated="2018-02-18 04:47:14.0"> <body><! CDATA Another option that I'll try first is to use a Docker-in-Docker (dind) sidecar container to build and run chaincode. The Peer pod will have two containers, a sidecar running Docker-in-Docker and a main container running the `peer` executable. Instead of talking to host Docker, the `peer` will talk to the Docker running in the sidecar.     Here's some info about how dind works with Kubernetes:  https://applatix.com/case-docker-docker-kubernetes-part-2/    TL;DR: If the `peer` container runs chaincode using a `dind` sidecar, the chaincode container will be in the same network namespace as the `peer` container. The chaincode can then access the peer on `localhost` (and vice versa).     Additionally: * Chaincode image doesn't need to be uploaded to a remote registry. (If chaincode runs as a Pod, k8s needs to pull the image from a registry) * If the `peer` pod dies, the chaincode containers (and `dind` sidecar) are cleaned up.  ></body> </Action>
<Action id="40356" issue="25000" author="greg.haskins" type="comment" created="2018-02-18 17:40:08.0" updateauthor="greg.haskins" updated="2018-02-18 17:40:08.0"> <body><! CDATA Hi All,  I havent had a chance to catch up on this thread, but a quick comment as one of the maintainers of the chaincode platform abstraction:  I had been contemplating solving the k8s support using the DIND sidecar approach myself, so I support this direction.  ></body> </Action>
<Action id="40358" issue="25000" author="ublubu" type="comment" created="2018-02-18 21:55:29.0" updateauthor="ublubu" updated="2018-02-18 21:55:29.0"> <body><! CDATA Here are the relevant fields of a Pod template I've used for a peer: (omitted a bunch of things for conciseness) {code:java} spec: containers: - command: - peer - node - start env: - name: CORE_VM_ENDPOINT value: http://localhost:2375 - name: CORE_PEER_TLS_ENABLED value: "false" image: someregistry/fabric-peer name: peer - image: docker:dind name: dind securityContext: privileged: true volumeMounts: - mountPath: /var/lib/docker mountPropagation: "" name: docker-graph-storage volumes: - emptyDir: {} name: docker-graph-storage {code}  It seems to work for me.     ></body> </Action>
<Action id="40381" issue="25000" author="amandahla" type="comment" created="2018-02-19 13:35:05.0" updateauthor="amandahla" updated="2018-02-19 13:35:05.0"> <body><! CDATA I'm not familiar with the DIND approach. I'll study that. :)      ~greg.haskins  so are you already working in a way to create chaincode in kubernetes?  ></body> </Action>
<Action id="40385" issue="25000" author="greg.haskins" type="comment" created="2018-02-19 14:46:58.0" updateauthor="greg.haskins" updated="2018-02-19 14:46:58.0"> <body><! CDATA  ~amandahla  The only work I have to date is captured here:      https://gerrit.hyperledger.org/r/c/12159/      Note that this is fully functional, however, and I can run Fabric on k8s today.  The DIND conversation is merely an optimization.  ></body> </Action>
<Action id="54463" issue="25000" author="mtessler" type="comment" created="2018-12-06 21:41:49.0" updateauthor="mtessler" updated="2018-12-06 21:41:49.0"> <body><! CDATA Is this ongoing?  It looks like development on this feature has stopped.  I'd like to be able to replace the call to docker.sock with different REST API calls to use say Kubernetes or Azure container as a service, or Amazon container as a service.  It would be nice if there was a way to have a pluggable way to instantiate the chaincode container.  ></body> </Action>
<Action id="54521" issue="25000" author="denyeart" type="comment" body=" ~mastersingh24   ~sykesm  Will this be part of the upcoming chaincode refactoring?" created="2018-12-08 16:45:32.0" updateauthor="denyeart" updated="2018-12-08 16:45:32.0"/>
<Action id="55814" issue="25000" author="nyet" type="comment" body="Removing DinD is not an optimization, it is a hard and fast requirement for production k8s deployment. DinD might be an ok hack for CI or PoC, but it does not belong in production.." created="2019-01-18 18:25:42.0" updateauthor="nyet" updated="2019-01-18 18:25:42.0"/>
<Action id="59561" issue="25000" author="sebcasanova" type="comment" created="2019-04-29 08:30:39.0" updateauthor="sebcasanova" updated="2019-04-29 08:30:39.0"> <body><! CDATA Hello,  As said previously, DinD is not an option with K8S use and Production ready environment.  Did some progress made on this topic ?  Thank very much     ></body> </Action>
<Action id="61383" issue="25000" author="jgdomine" type="comment" created="2019-07-01 13:08:48.0" updateauthor="jgdomine" updated="2019-07-01 13:08:48.0"> <body><! CDATA Hi  Is there anything new on this topic? Because we used Dind as a workaround on our K8S cluster and we have eviction problems of the peer because the docker daemon uses more and more space after some days of running (while the network is idle, by that I mean there are no new transactions).  So Dind is definitely not suitable for production purposes.  Thank you for your feedback  ></body> </Action>
<Action id="62656" issue="25000" author="denyeart" type="comment" body="Tracked in FAB-13582 now." created="2019-08-06 16:42:18.0" updateauthor="denyeart" updated="2019-08-06 16:42:18.0"/>
