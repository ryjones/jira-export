<Action id="67664" issue="44139" author="btl5037" type="comment" created="2020-01-26 00:59:21.0" updateauthor="btl5037" updated="2020-01-26 01:08:41.0"> <body><! CDATA Is this during the instantiation process you are referring to? The Node gRPC module has to be compiled from source during the `npm install` process, a quick test showed it increases my containers memory usage by 400MB during the gRPC compilation process. It's even more while running DinD.  This is a very common complaint on Kubernetes and often requires users to run the instantiation process twice or increase the resources on their DinD and Peer containers to protect against resource exhaustion, where it normally fails the first time, but since it doesn't need to start the module installation process from scratch the second time it generally succeeds.  ></body> </Action>
<Action id="67668" issue="44139" author="denyeart" type="comment" body="Yes, the memory goes up upon instantiation on v1.4, and upon install on v2.0 (since chaincode now builds at install time).  On v2.0 I&apos;ve seen peer memory go up about 200MB upon node js chaincode install. So perhaps v2.0 is not quite as bad as v1.4?" created="2020-01-27 10:35:33.0" updateauthor="denyeart" updated="2020-01-27 10:35:33.0"/>
<Action id="67756" issue="44139" author="btl5037" type="comment" created="2020-01-31 02:08:53.0" updateauthor="btl5037" updated="2020-01-31 02:09:32.0"> <body><! CDATA Comments from discussions over the last few days:  During the build process, there is a large memory cost to the peer while it holds the Docker build context in memory while its passed to Docker for image creation. In pre-2.0 releases there is no mitigation for this. Starting in 2.0 you can build chaincode images out of band and use the cc_launcher to launch the prebuilt image, or any of the infinite ways you can make use of external builders and launchers to run chaincode. While it would be nice to solve this problem, it only occurs during chaincode instantiation time and then the large resource consumption is freed.  The more interesting problem, and it wasn't on the radar when we were running tests, as we didn't start monitoring peer memory usage until we had already used the peer to instantiate chaincode, is that a peer uses ~50MB of memory prior to chaincode instantiation, while the chaincode build process uses ~600MB of peer memory during the process (for fabcar node chaincode), it never returns to its pre-build state and settles around ~300MB of memory and remains in that state forever. We should investigate what is holding the lock on that additional ~200MB of memory.  I added a pprof server to the peer code and am going to see if I can look at the memory profile to see if anything stands out during that process.  ></body> </Action>
<Action id="68300" issue="44139" author="JIRAUSER19932" type="comment" body="Hi there , is there any update on this ? " created="2020-03-12 19:24:03.0" updateauthor="JIRAUSER19932" updated="2020-03-12 19:24:03.0"/>
<Action id="68551" issue="44139" author="btl5037" type="comment" created="2020-03-24 18:39:00.0" updateauthor="btl5037" updated="2020-03-24 19:03:38.0"> <body><! CDATA We can't address the high usage during chaincode build. The memory is being consumed by the Docker build context, and the context is replicated several times. Mitigating it in 1.4 really doesn't make sense as its a major architectural change. In 2.0 you can easily mitigate the problem by making use of the external builders.  In regards to the Peer not returning all of the memory it consumed even after the chaincode build process, we should enable the peer profile server and use it to diagnose what process is retaining that memory:  https://github.com/hyperledger/fabric/blob/master/sampleconfig/core.yaml#L355-L359   ></body> </Action>
<Action id="68552" issue="44139" author="denyeart" type="comment" created="2020-03-24 19:28:37.0" updateauthor="denyeart" updated="2020-03-24 19:28:37.0"> <body><! CDATA Wenjian is going to take the next step.  Wenjian, here's some notes I took the last time I used the peer's pprof to troubleshoot a memory issue...  To enable pprof, set environment variables on peer: `CORE_PEER_PROFILE_ENABLED=true CORE_PEER_PROFILE_LISTENADDRESS=0.0.0.0:6060` To collect pprof heap snapshot: http://localhost:6060/debug/pprof/heap?debug=1 To collect pprof go routine snapshot: http://localhost:6060/debug/pprof/goroutine?debug=1  curl -m 30 -X GET http://localhost:6060/debug/pprof/heap?debug=1 curl -m 30 -X GET http://localhost:6060/debug/pprof/goroutine?debug=1  The heap and go routine snapshots can be compared before and after the reproduction.  To use a pprof file: http://localhost:6060/debug/pprof/heap go tool pprof -top <heapfile>  Good pprof articles:  https://software.intel.com/en-us/blogs/2014/05/10/debugging-performance-issues-in-go-programs https://jvns.ca/blog/2017/09/24/profiling-go-with-pprof/  ></body> </Action>
<Action id="68689" issue="44139" author="wenjian" type="comment" created="2020-04-01 02:09:59.0" updateauthor="wenjian" updated="2020-04-03 12:33:14.0"> <body><! CDATA After investigation and checking memory heap profiles (fabric v2.1), I found that the peer already freed up most of the increased memory. See the attached images: * profile_1_before_ccdeploy.png: 22M, network started and a channel was created and joined * profile_2_rightafter_ccdeploy.png: 228M, right after fabcar deployment, significant memory increase was observed. * profile_3_after_ccdeploy_gc.png: 37M (unused memory were gc'ed). Additional 15M memory were related to leveldb usage.  *However, `docker stats` still shows big memory usage* for each peer container even after the peers have been idle. {quote}CONTAINER ID  NAME                                 CPU % MEM USAGE / LIMIT   MEM %  NET I/O              BLOCK I/O           PIDS  49355d73c301 peer0.org1.example.com 1.23%   251.6MiB / 3.853GiB   6.38%   1.16MB / 1.12MB 11.1MB / 1.33MB  11 bb092d833e5b peer1.org1.example.com 1.15%   249.4MiB / 3.853GiB   6.32%   1.11MB / 1.11MB 356kB / 1.33MB  11 a879b2fdfa1b peer0.org2.example.com 1.07%   251.4MiB / 3.853GiB   6.37%   1.13MB / 1.16MB 3.39MB / 1.33MB  10 dd4821b0a02a peer1.org2.example.com 1.17%   249.4MiB / 3.853GiB   6.32%   1.1MB / 1.04MB 22.3MB / 2.15MB  11 {quote} Apparently there is a difference between `docker stats` and go pprof. The reason is that `docker stats` includes caches that is not released by the container, but the peer process (top) does not include caches. Restart the docker container will release caches. Still investigate if there is a command to release caches in a docker container.    https://stackoverflow.com/questions/49159036/docker-does-not-free-memory-after-creating-and-deleting-files-with-php    https://stackoverflow.com/questions/48686599/docker-containers-memory-consumption-never-decreases-or-does-it   Note: above observation is consistent with my earlier test that started peer by its raw Peer binary without using docker container. In that test,  peer memory usage was dropped after cc deployment. The test demonstrated that peer didn't retain memory. It is docker container that retains memory in cache.     ></body> </Action>
<Action id="68708" issue="44139" author="wenjian" type="comment" created="2020-04-02 15:34:11.0" updateauthor="wenjian" updated="2020-04-02 15:34:11.0"> <body><! CDATA Update:  As mentioned in a previous comment, the most memory increase was caused by `GenerateDockerBuild` during chaincode deployment. After cc is deployed, peer has already released the related memory (see the attached pprof images). However,  `docker stats` still hows much high memory usage because it includes both peer process memory and the cache used by the docker container. How and when to release the container cache is out of peer's control.   I checked `docker stats` again after the peers were idle more than a day, the memory usage was actually decreased on all peer containers, but not in a consistent way (e.g., peer0.org2 dropped by 130+MiB, but peer1.org2 dropped less than 33MiB) {quote}CONTAINER ID  NAME                                 CPU %    MEM USAGE / LIMIT   MEM %       NET I/O              BLOCK I/O           PIDS  49355d73c301 peer0.org1.example.com 2.30%    173.7MiB / 3.853GiB 4.40%           568MB / 585MB 24MB / 1.33MB 12 bb092d833e5b peer1.org1.example.com 2.12%    187.4MiB / 3.853GiB 4.75%            566MB / 570MB 6.07MB / 1.33MB 12 a879b2fdfa1b peer0.org2.example.com 2.11%       117.1MiB / 3.853GiB 2.97%              566MB / 569MB 6.05MB / 1.33MB 12 dd4821b0a02a peer1.org2.example.com 2.05%    222MiB / 3.853GiB 5.63%               567MB / 564MB 26MB / 2.15MB 12 {quote}  ></body> </Action>
<Action id="68709" issue="44139" author="wenjian" type="comment" created="2020-04-02 15:56:12.0" updateauthor="wenjian" updated="2020-04-02 15:56:12.0"> <body><! CDATA Good pprof doc on heap dump and analysis:    https://www.freecodecamp.org/news/how-i-investigated-memory-leaks-in-go-using-pprof-on-a-large-codebase-4bec4325e192/   How to enable pprof on peer and collect a heap dump or goroutines dump: refer to Dave's comment (https://jira.hyperledger.org/browse/FAB-17444?focusedCommentId=68552&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-68552) . You can collect heap dump files at different times, e.g., before and after chaincode deployment.  To analyze the heap dump,  1. run `go tool pprof heap.out` (if you have a heapdump file) or `go tool pprof http://localhost:6060/debug/pprof/heap?debug=1`, which will generate a heapdump gz file for you  2. now it enters the pprof interactive mode, type `help` to see all commands and options. Some helpful commands are: * top or topN: output top entries * png, gif, ...: there are multiple commands generating a graph view, very helpful to pinpoint where the memory is in use. You have to install Graphvi * list <regexp>: output annotated source for functions matching regexp (e.g., list goleveldb)  ></body> </Action>
<Action id="68718" issue="44139" author="btl5037" type="comment" created="2020-04-03 01:59:36.0" updateauthor="btl5037" updated="2020-04-03 01:59:36.0"> <body><! CDATA Looking at Wenjian's analysis, and reading the articles and doc Wenjian linked about Docker memory management, as well as some of my own research, I'd venture to say we can close this issue.  Wenjian's original test used the raw Peer binary on her local machine without Docker, and she was unable to replicate the problem. But while testing with Docker we note that the Peer's memory usage remains high, even after the chaincode build process ends, I think this shows that her research is correct, it's Docker's memory management system that is retaining the memory in cache, not the Peer process itself.  I would recommend closing this as, "working as designed"   ~denyeart   ~wenjian   ></body> </Action>
<Action id="68723" issue="44139" author="wenjian" type="comment" body=" ~btl5037  Agree to close this ticket as &quot;working as designed&quot;" created="2020-04-03 12:40:02.0" updateauthor="wenjian" updated="2020-04-03 12:40:02.0"/>
