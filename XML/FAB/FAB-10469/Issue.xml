<Issue id="30752" key="FAB-10469" number="10469" project="10002" reporter="divyank" assignee="yacovm" creator="divyank" type="10004" summary="Deadlock in gossip connection store" priority="3" resolution="10000" status="6" created="2018-05-30 16:21:55.0" updated="2018-07-20 14:16:52.0" resolutiondate="2018-06-08 14:52:50.0" votes="0" watches="2" workflowId="42370"> <description><! CDATA After running our load tests with many concurrent requests for some time, we noticed some transactions failing with error: {code} Description: Failed disseminating 2 out of 2 private RWSets{code} and others timing out.  Upon profiling the peers, we found that two of them had deadlocks - thousands of go routines were waiting to acquire the gossip connection store lock: {code:java} goroutine profile: total 11350 8965 @ 0x9d699c 0x9d6a8e 0x9e7fb4 0x9e7bd9 0xa1cc09 0x10bb4b2 0x10b5cbf 0x10bf224 0xa05f01 #	0x9e7bd8	sync.runtime_Semacquire+0x38							/opt/go/src/runtime/sema.go:56 #	0xa1cc08	sync.(*RWMutex).RLock+0x48							/opt/go/src/sync/rwmutex.go:50 #	0x10bb4b1	github.com/hyperledger/fabric/gossip/comm.(*connectionStore).getConnection+0x71	/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/conn.go:57 #	0x10b5cbe	github.com/hyperledger/fabric/gossip/comm.(*commImpl).sendToEndpoint+0x1de	/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/comm_impl.go:229 #	0x10bf223	github.com/hyperledger/fabric/gossip/comm.(*commImpl).Send.func1+0x43		/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/comm_impl.go:216 {code} which was held by another routine waiting to acquire a connection lock: {code:java} 1 @ 0x9d699c 0x9d6a8e 0x9e7fb4 0x9e7ccd 0xa1bcee 0xa1cced 0x10bc73b 0x10bbd4b 0x10b76a9 0x10e156d 0x109faae 0x109f3d4 0xa05f01 #	0x9e7ccc	sync.runtime_SemacquireMutex+0x3c									/opt/go/src/runtime/sema.go:71 #	0xa1bced	sync.(*Mutex).Lock+0xed											/opt/go/src/sync/mutex.go:134 #	0xa1ccec	sync.(*RWMutex).Lock+0x2c										/opt/go/src/sync/rwmutex.go:93 #	0x10bc73a	github.com/hyperledger/fabric/gossip/comm.(*connection).close+0x8a					/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/conn.go:235 #	0x10bbd4a	github.com/hyperledger/fabric/gossip/comm.(*connectionStore).closeConn+0xca				/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/conn.go:137 #	0x10b76a8	github.com/hyperledger/fabric/gossip/comm.(*commImpl).CloseConn+0xc8					/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/comm_impl.go:349 #	0x10e156c	github.com/hyperledger/fabric/gossip/gossip.(*discoveryAdapter).CloseConn+0x7c				/opt/gopath/src/github.com/hyperledger/fabric/gossip/gossip/gossip_impl.go:956 #	0x109faad	github.com/hyperledger/fabric/gossip/discovery.(*gossipDiscoveryImpl).expireDeadMembers+0x64d		/opt/gopath/src/github.com/hyperledger/fabric/gossip/discovery/discovery_impl.go:701 #	0x109f3d3	github.com/hyperledger/fabric/gossip/discovery.(*gossipDiscoveryImpl).periodicalCheckAlive+0x223	/opt/gopath/src/github.com/hyperledger/fabric/gossip/discovery/discovery_impl.go:666 {code} the connection lock was held by another routine blocking on a send (probably to the other deadlocked peer): {code:java} 1 @ 0x9d699c 0x9d6a8e 0x9ac25b 0x9abfe3 0x10bca6a 0x10b5e9f 0x10bf923 0x10bed21 0xa05f01 #	0x10bca69	github.com/hyperledger/fabric/gossip/comm.(*connection).send+0x269		/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/conn.go:275 #	0x10b5e9e	github.com/hyperledger/fabric/gossip/comm.(*commImpl).sendToEndpoint+0x3be	/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/comm_impl.go:235 #	0x10bf922	github.com/hyperledger/fabric/gossip/comm.(*commImpl).SendWithAck.func1+0x42	/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/comm_impl.go:517 #	0x10bed20	github.com/hyperledger/fabric/gossip/comm.(*ackSendOperation).send.func1+0x40	/opt/gopath/src/github.com/hyperledger/fabric/gossip/comm/ack.go:39 {code} I am attaching go routine profiles for the two deadlocked peers.  *Environment information:*  Fabric v1.1.0 with cherry-picks for private data.  Four orgs with two peers each, three channels (one with all orgs, two with two orgs each)  The tests were run on a local docker-compose environment.        ></description> </Issue>
