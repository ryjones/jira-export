<Action id="31658" issue="21293" author="adnanchoudhury" type="comment" created="2017-10-05 22:04:35.0" updateauthor="adnanchoudhury" updated="2017-10-06 17:45:40.0"> <body><! CDATA 1. Plan is to use LTE to get the TPS (transaction per second) numbers for various scenarios for the 1.1 and for the 1.0 and compare them against each other. 2. To automate, very minimal change is needed, if any. 3. Should take 1 week to complete the test run and create reports with comparison.  ></body> </Action>
<Action id="33369" issue="21293" author="adnanchoudhury" type="comment" body="Attaching the LTE couchDB performance results in the sheet." created="2017-10-30 15:40:23.0" updateauthor="adnanchoudhury" updated="2017-10-30 15:40:23.0"/>
<Action id="33370" issue="21293" author="adnanchoudhury" type="comment" body=" ~denyeart " created="2017-10-30 15:45:40.0" updateauthor="adnanchoudhury" updated="2017-10-30 15:45:40.0"/>
<Action id="34426" issue="21293" author="denyeart" type="comment" created="2017-11-07 16:26:21.0" updateauthor="denyeart" updated="2017-11-07 16:26:21.0"> <body><! CDATA  ~AdnanChoudhury  A few observations...  The primary CouchDB performance benefit in 1.1 is the ability to bulk update in CouchDB.  In your tests you have standardized on block BatchSize of 50.  This will limit the ability to do large batches against CouchDB. Let's say a specific test is going at 1000 tps.  With a BatchSize of 50 you are cutting 20 blocks per second, and consequently doing 20 bulk updates into CouchDB per second.  It will be much more efficient if you have a BatchSize of 1000, cut 1 block per second, and do one large bulk update into CouchDB.  1 block per second will be a pretty typical production configuration.  Therefore I'd suggest switch your standard BatchSize to 1000.  This will provide much higher performance numbers that are more aligned with production configurations.   Also, you have standardized on NumKeysInEachTx of 4.  You will get almost 4x performance improvement if you standardize on NumKeysInEachTx of 1.  The primary bottleneck in 1.1 with CouchDB will be the Gets in simulation, since these cannot be executed in bulk. Going from 4 Gets per simulation to 1 Get per simulation will have a huge impact on the performance tests.  I agree we want a spectrum of conditions in the performance tests, I'd just encourage you to test more of the permutations that we expect to perform better (and which are in fact typical in production scenarios).  Finally, in the LTE tests the peer is simulating every single transaction.  In reality each peer will simulate a subset of transactions.  Given that simulation is the bottleneck, especially on CouchDB, a peer that is able to achieve ~500tps in your tests may actually yield a network that is able to achieve ~2500tps.  I've encouraged Scott and and Tong Li to execute more performance tests where peers simulate a subset of transactions (beyond the scope of LTE I understand).  ></body> </Action>
<Action id="67147" issue="21293" author="sykesm" type="comment" body="Stale" created="2020-01-22 22:09:08.0" updateauthor="sykesm" updated="2020-01-22 22:09:08.0"/>
