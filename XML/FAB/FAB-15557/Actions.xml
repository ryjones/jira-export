<Action id="60590" issue="40164" author="yacovm" type="comment" body=" ~C0rWin   ~naaman  " created="2019-06-03 12:58:05.0" updateauthor="yacovm" updated="2019-06-03 12:58:05.0"/>
<Action id="60594" issue="40164" author="c0rwin" type="comment" body=" ~jooskim1  can you provide logs of isolated peer for example? " created="2019-06-03 13:26:26.0" updateauthor="c0rwin" updated="2019-06-03 13:26:26.0"/>
<Action id="60625" issue="40164" author="jooskim1" type="comment" created="2019-06-04 01:26:35.0" updateauthor="jooskim1" updated="2019-06-04 01:26:35.0"> <body><! CDATA  ~C0rWin   Below is logs that can be found in one of the isolated peer, OrgA.PEER4. Detail is that  1. 2019-05-22 08:22:38.948 : OrgA.PEER4 starts losing its peers in the same channel,  ChannelA. On this time, it loses OrgA.PEER3. 2. 2019-05-22 08:22:43.947 : OrgA.PEER4 loses OrgA.PEER1 and OrgB.PEER1 at the same time. 3. 2019-05-22 08:22:48.947 : OrgA.PEER4 loses OrgA.PEER2 and finally OrgA.PEER4's current membership view is empty. 4. After losing all members on the same channel, OrgA.PEER4 dose not recover its membership view and from this moment this isolated peer is not able to fetch private data via gossip protocol.  -------- log starts: . . .  34m2019-05-22 08:22:34.966 UTC  comm.grpc.server  1 -> INFO 2ffb 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44246 grpc.code=OK grpc.call_duration=1.682729ms  34m2019-05-22 08:22:38.948 UTC  gossip.channel  reportMembershipChanges -> INFO 2ffc 0m Membership view has changed. peers went offline:    OrgA.PEER3.IP:7051   , current view:    OrgA.PEER2.IP:7051   OrgA.PEER1.IP:7051   OrgB.PEER1.IP7051     34m2019-05-22 08:22:39.016 UTC  comm.grpc.server  1 -> INFO 2ffd 0m unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-05-22T08:22:41.016Z grpc.peer_address=54.178.153.239:1420 grpc.code=OK grpc.call_duration=117.72Âµs  34m2019-05-22 08:22:40.040 UTC  endorser  callChaincode -> INFO 2ffe 0m    9f9d4ead  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:40.041 UTC  endorser  callChaincode -> INFO 2fff 0m    9f9d4ead  Exit chaincode: name:"qscc"  (1ms)  34m2019-05-22 08:22:40.041 UTC  comm.grpc.server  1 -> INFO 3000 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44260 grpc.code=OK grpc.call_duration=1.671653ms  34m2019-05-22 08:22:40.049 UTC  endorser  callChaincode -> INFO 3001 0m    080e648d  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:40.049 UTC  endorser  callChaincode -> INFO 3002 0m    080e648d  Exit chaincode: name:"qscc"  (1ms)  34m2019-05-22 08:22:40.050 UTC  comm.grpc.server  1 -> INFO 3003 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44264 grpc.code=OK grpc.call_duration=1.725098ms  34m2019-05-22 08:22:43.947 UTC  gossip.channel  reportMembershipChanges -> INFO 3004 0m Membership view has changed. peers went offline:    OrgA.PEER1.IP:7051   OrgB.PEER1.IP:7051    , current view:    OrgA.PEER2.IP:7051    34m2019-05-22 08:22:45.122 UTC  endorser  callChaincode -> INFO 3005 0m    96bb57db  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:45.123 UTC  endorser  callChaincode -> INFO 3006 0m    96bb57db  Exit chaincode: name:"qscc"  (1ms)  34m2019-05-22 08:22:45.123 UTC  comm.grpc.server  1 -> INFO 3007 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44268 grpc.code=OK grpc.call_duration=1.814307ms  34m2019-05-22 08:22:45.126 UTC  endorser  callChaincode -> INFO 3008 0m    3f3e0be4  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:45.127 UTC  endorser  callChaincode -> INFO 3009 0m    3f3e0be4  Exit chaincode: name:"qscc"  (1ms)  34m2019-05-22 08:22:45.127 UTC  comm.grpc.server  1 -> INFO 300a 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44272 grpc.code=OK grpc.call_duration=1.713582ms  34m2019-05-22 08:22:46.148 UTC  comm.grpc.server  1 -> INFO 300b 0m unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-05-22T08:22:48.147Z grpc.peer_address=OrgB.PEER1.IP:16116 grpc.code=OK grpc.call_duration=114.67Âµs  34m2019-05-22 08:22:46.631 UTC  comm.grpc.server  1 -> INFO 300c 0m unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-05-22T08:22:48.631Z grpc.peer_address=OrgA.PEER1.IP:34024 grpc.code=OK grpc.call_duration=114.277Âµs  34m2019-05-22 08:22:48.947 UTC  gossip.channel  reportMembershipChanges -> INFO 300d 0m Membership view has changed. peers went offline:    OrgA.PEER2.IP:7051   , current view:      34m2019-05-22 08:22:50.197 UTC  endorser  callChaincode -> INFO 300e 0m    1ee56d80  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:50.198 UTC  endorser  callChaincode -> INFO 300f 0m    1ee56d80  Exit chaincode: name:"qscc"  (1ms)  34m2019-05-22 08:22:50.199 UTC  comm.grpc.server  1 -> INFO 3010 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44276 grpc.code=OK grpc.call_duration=1.851186ms  34m2019-05-22 08:22:50.201 UTC  endorser  callChaincode -> INFO 3011 0m    fe475aa6  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:50.202 UTC  endorser  callChaincode -> INFO 3012 0m    fe475aa6  Exit chaincode: name:"qscc"  (1ms)  34m2019-05-22 08:22:50.202 UTC  comm.grpc.server  1 -> INFO 3013 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.peer_address=172.18.0.1:44280 grpc.code=OK grpc.call_duration=1.736121ms  34m2019-05-22 08:22:51.417 UTC  endorser  callChaincode -> INFO 3015 0m  payprotocol  43b72e44  Entry chaincode: name:"cscc"   34m2019-05-22 08:22:51.417 UTC  endorser  callChaincode -> INFO 3014 0m  payprotocol  6351eef6  Entry chaincode: name:"cscc"   34m2019-05-22 08:22:51.418 UTC  endorser  callChaincode -> INFO 3016 0m  payprotocol  43b72e44  Exit chaincode: name:"cscc"  (1ms)  34m2019-05-22 08:22:51.418 UTC  endorser  callChaincode -> INFO 3017 0m  payprotocol  6351eef6  Exit chaincode: name:"cscc"  (1ms)  34m2019-05-22 08:22:51.419 UTC  comm.grpc.server  1 -> INFO 3018 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.request_deadline=2019-05-22T08:25:51.409Z grpc.peer_address=OrgB.PEER1.IP:54538 grpc.code=OK grpc.call_duration=2.756749ms  34m2019-05-22 08:22:51.419 UTC  comm.grpc.server  1 -> INFO 3019 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.request_deadline=2019-05-22T08:25:51.334Z grpc.peer_address=OrgB.PEER1.IP:54538 grpc.code=OK grpc.call_duration=2.761568ms  34m2019-05-22 08:22:51.419 UTC  endorser  callChaincode -> INFO 301a 0m  payprotocol  c10956a2  Entry chaincode: name:"cscc"   34m2019-05-22 08:22:51.420 UTC  endorser  callChaincode -> INFO 301b 0m  payprotocol  c10956a2  Exit chaincode: name:"cscc"  (1ms)  34m2019-05-22 08:22:51.421 UTC  comm.grpc.server  1 -> INFO 301c 0m unary call completed grpc.service=protos.Endorser grpc.method=ProcessProposal grpc.request_deadline=2019-05-22T08:25:51.376Z grpc.peer_address=OrgB.PEER1.IP:54538 grpc.code=OK grpc.call_duration=2.500416ms  34m2019-05-22 08:22:51.682 UTC  comm.grpc.server  1 -> INFO 301d 0m unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-05-22T08:22:53.682Z grpc.peer_address=OrgA.PEER3.IP:47778 grpc.code=OK grpc.call_duration=133.361Âµs  34m2019-05-22 08:22:55.275 UTC  endorser  callChaincode -> INFO 301e 0m    61f421ef  Entry chaincode: name:"qscc"   34m2019-05-22 08:22:55.276 UTC  endorser  callChaincode -> INFO 301f 0m    61f421ef  Exit chaincode: name:"qscc"  (1ms) . . . -------- log ends:  ></body> </Action>
<Action id="60854" issue="40164" author="jooskim1" type="comment" created="2019-06-11 03:24:56.0" updateauthor="jooskim1" updated="2019-06-11 06:11:03.0"> <body><! CDATA  ~C0rWin   ~ronenschafferibm   Â   Hello,Â   I have one more thing that I should tell you guys to solve this serious problem.  We replicated the exact same fabric network environment on AWS, and for this one, we didn't install any chaincodes because we doubted the chaincode(which is using private data) was doing something weird.  We have monitored this replicated test network, and even after 5 days, the network has no problem and no peer is being isolated anymore. Thus we kinda concluded that this chaincode with private data is doing something wrong.  On the production environment, we are still having this isolated peer problem almost every 24 hours. In detail, in OrgA, a peer which hasÂ a chaincode using private data has *no more private data* to spread out to other peers on a channel, peers are still isolated-losing membership views and not recovering the membership view even if other peers on the same channel still alive.Â   Â   Thanks  Â   P.S.Â   I just monitored that replicated fabric network on AWS which has no installed chaincodes also occurs isolated peer problem.Â Peer A in OrgA is isolated.  Â   below is the logs on Peer A in OrgAÂ   ------logs startÂ   . . . 2019-06-11T05:17:42.442589801Z 2019-06-11 05:17:42.442 UTC  gossip.discovery  getDeadMembers -> WARN 63e Haven't heard from  183 175 99 112 64 35 226 232 249 63 241 33 171 245 61 142 219 12 72 2 135 213 176 175 27 37 179 107 163 7 23 160  for 25.866546422s 2019-06-11T05:17:42.442661527Z 2019-06-11 05:17:42.442 UTC  gossip.discovery  expireDeadMembers -> WARN 63f Entering  b7af63704023e2e8f93ff121abf53d8edb0c480287d5b0af1b25b36ba30717a0  2019-06-11T05:17:42.442670397Z 2019-06-11 05:17:42.442 UTC  gossip.discovery  expireDeadMembers -> WARN 640 Closing connection to Endpoint: OrgA.PEER3.IP:7051, InternalEndpoint: OrgA.PEER3.IP:7051, PKI-ID: b7af63704023e2e8f93ff121abf53d8edb0c480287d5b0af1b25b36ba30717a0, Metadata: 2019-06-11T05:17:46.307183798Z 2019-06-11 05:17:46.306 UTC  comm.grpc.server  1 -> INFO 641 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:17:48.306Z grpc.peer_address=OrgA.PEER3.IP:42816 grpc.code=OK grpc.call_duration=108.151Âµs 2019-06-11T05:17:46.886130757Z 2019-06-11 05:17:46.885 UTC  gossip.channel  reportMembershipChanges -> INFO 642 Membership view has changed. peers went offline:   OrgA.PEER3.IP:7051   , current view:   OrgA.PEER4.IP:7051   OrgB.PEER1.IP:7051    OrgB.PEER2.IP:7051    OrgA.PEER2.IP:7051   . . . 2019-06-11T05:24:15.094256937Z 2019-06-11 05:24:15.094 UTC  comm.grpc.server  1 -> INFO 68f unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:17.093Z grpc.peer_address=OrgB.PEER2.IP:49350 grpc.code=OK grpc.call_duration=146.546Âµs 2019-06-11T05:24:21.230229172Z 2019-06-11 05:24:21.229 UTC  comm.grpc.server  1 -> INFO 690 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:23.229Z grpc.peer_address=OrgB.PEER1.IP:55944 grpc.code=OK grpc.call_duration=105.335Âµs 2019-06-11T05:24:22.114872751Z 2019-06-11 05:24:22.114 UTC  comm.grpc.server  1 -> INFO 691 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:24.114Z grpc.peer_address=OrgA.PEER2.IP:41660 grpc.code=OK grpc.call_duration=103.522Âµs 2019-06-11T05:24:26.384157957Z 2019-06-11 05:24:26.383 UTC  comm.grpc.server  1 -> INFO 692 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:28.383Z grpc.peer_address=OrgA.PEER3.IP:42982 grpc.code=OK grpc.call_duration=109.119Âµs 2019-06-11T05:24:29.042570595Z 2019-06-11 05:24:29.042 UTC  comm.grpc.server  1 -> INFO 693 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:31.042Z grpc.peer_address=OrgA.PEER4.IP:48248 grpc.code=OK grpc.call_duration=108.177Âµs 2019-06-11T05:24:40.099220355Z 2019-06-11 05:24:40.098 UTC  comm.grpc.server  1 -> INFO 694 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:42.098Z grpc.peer_address=OrgB.PEER2.IP:49364 grpc.code=OK grpc.call_duration=116.193Âµs 2019-06-11T05:24:46.234846386Z 2019-06-11 05:24:46.234 UTC  comm.grpc.server  1 -> INFO 695 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:48.234Z grpc.peer_address=OrgB.PEER1.IP:55956 grpc.code=OK grpc.call_duration=108.18Âµs 2019-06-11T05:24:47.119590315Z 2019-06-11 05:24:47.119 UTC  comm.grpc.server  1 -> INFO 696 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:49.118Z grpc.peer_address=OrgA.PEER2.IP:41672 grpc.code=OK grpc.call_duration=108.734Âµs 2019-06-11T05:24:51.388774695Z 2019-06-11 05:24:51.388 UTC  comm.grpc.server  1 -> INFO 697 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:53.388Z grpc.peer_address=OrgA.PEER3.IP:42992 grpc.code=OK grpc.call_duration=103.616Âµs 2019-06-11T05:24:54.047398027Z 2019-06-11 05:24:54.047 UTC  comm.grpc.server  1 -> INFO 698 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:24:56.046Z grpc.peer_address=OrgA.PEER4.IP:48258 grpc.code=OK grpc.call_duration=127.284Âµs 2019-06-11T05:25:05.104696892Z 2019-06-11 05:25:05.104 UTC  comm.grpc.server  1 -> INFO 699 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:25:07.103Z grpc.peer_address=OrgB.PEER2.IP:49378 grpc.code=OK grpc.call_duration=103.686Âµs 2019-06-11T05:25:41.886164452Z 2019-06-11 05:25:41.885 UTC  gossip.channel  reportMembershipChanges -> INFO 69a Membership view has changed. peers went offline:   OrgB.PEER1.IP:7051    , current view:   OrgA.PEER4.IP:7051   OrgB.PEER2.IP:7051    OrgA.PEER2.IP:7051   2019-06-11T05:25:46.886065707Z 2019-06-11 05:25:46.885 UTC  gossip.channel  reportMembershipChanges -> INFO 69b Membership view has changed. peers went offline:   OrgA.PEER4.IP:7051   OrgB.PEER2.IP:7051    OrgA.PEER2.IP:7051   , current view:    2019-06-11T05:39:42.689927913Z 2019-06-11 05:39:42.687 UTC  comm.grpc.server  1 -> INFO 69c unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:39:44.687Z grpc.peer_address=OrgB.PEER2.IP:49914 grpc.code=OK grpc.call_duration=121.113Âµs 2019-06-11T05:39:42.824577254Z 2019-06-11 05:39:42.824 UTC  comm.grpc.server  1 -> INFO 69d unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-06-11T05:39:44.824Z grpc.peer_address=OrgB.PEER2.IP:49922 grpc.code=OK grpc.call_duration=106.693Âµs  ----- logs end  Â   ></body> </Action>
<Action id="60910" issue="40164" author="ronenschafferibm" type="comment" created="2019-06-12 11:58:46.0" updateauthor="ronenschafferibm" updated="2019-06-12 11:58:46.0"> <body><! CDATA  ~jooskim1   Thanks for the detailed explanation. What is the simplest way to recreate the problem on my local machine?  ></body> </Action>
<Action id="60931" issue="40164" author="jooskim1" type="comment" created="2019-06-13 01:50:19.0" updateauthor="jooskim1" updated="2019-06-13 05:49:16.0"> <body><! CDATA  ~ronenschafferibm   Â In summary, one channel, 2 organizations and all peers should be hosted on *separate machines*.Â Thus, I doubt if you can replicate the issue on the local environment.(in order to reproduce the issue on my end, I had to use AWS EC2 instances to host each peers on separate instances. *we used AWS EC2 only to replicate the issue, but the actual production environment is not using AWS. We are hosting each peers on the external data center so it shouldn't be the specific network platform issue). After you have the channel and peers set up, then one of peers in Org B has to be "docker stop" and "docker restart" and leave it for like 24 hours. then you will notice one or multiple peers in Org A being isolated(left out from current membership view) within 24 hours.  To explain more detailed setting we've got :  Â   As I explained above, your network needs 2 Organizations, A and B(call eachÂ  OrgA and OrgB).  In OrgA, the number of peers are 4 and each peers docker container is located in *physically separateÂ  machines*. The all host machines are Centos 7 and we already checkout every firewall and portsÂ  reuseÂ settings. For communication with OrgB, all peers in OrgA set to anchor peers.Â   In OrgB, the number of peers is 2 and like OrgA, peers are inÂ *physically separatedÂ  machines*.Â   Â  # After setupÂ 4 peers in OrgA and 2 peers in OrgB, also each peers should be in *physically separatedÂ  machines*. It means, each peers knows each other only by public IP address.Â  # Create a channel(we can call it channelA), and all peers join the channelA. # After setup all peers in OrgA and OrgB, check logs about membership view and all addresses of other peers are in the membership view of each peers except a peer's own self. # In OrgB, stop and start the peer container. Not remove container, just stop and start or restart command also fine. ## Peers in OrgA, You can see each peers' membership view changed.Â  # Leave the network for few hours, maybe more than 24 hours, a random peer in OrgA would start losing peers its membership view and finally, it cannot recover it and isolated on the network.  Additionally, below is my team's all testing settings. # We've tried both static gossip leader and also dynamic gossip leader. But it didn't affect anything. # All peers in OrgA are set to anchor peers. # we have tired both CORE_PEER_AUTOADDRESSDETECT=true and alsoÂ  CORE_PEER_AUTOADDRESSDETECT=false.Â But it didn't affect anything.  Before the isolating issue occurs on the network, from peers in OrgA start changing its membership view to online and offline for peers in OrgBÂ *repeatably,* even peers in OrgB are online (I also do ping test and telnet test).  Â   ></body> </Action>
<Action id="61328" issue="40164" author="jooskim1" type="comment" created="2019-06-28 04:48:41.0" updateauthor="jooskim1" updated="2019-06-28 04:48:41.0"> <body><! CDATA @ ~ronenschafferibm Â   My team found out another information for you to solve this issue.  When a peer is getting isolated, the peer container has tons of 'ESTABLISHED' status tcp connections with other peers in the other organization - in this case, peers inÂ OrgB.  My team uses following command, which gives the peer container's netstat.  *sudo nsenter -t \{docker container pid} -n netstat*  My team guesses the reason that the peer is getting isolated is the lack of usable sockets. From netstat in the container, because of repeated on and off membership changes as I reported on the above log, there is no socket in the container to connect with the other peers.  ----- nsenter -t \{OrgA.PEER4.IP} -n netstat start  .  .  .  tcp 0 0 peer4:36452 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:47710 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:39592 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:38330 OrgB.PEER2.IP:7051 ESTABLISHED tcp 0 0 peer4:34190 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:59208 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:42690 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:43232 OrgB.PEER2.IP:7051 ESTABLISHED tcp 0 0 peer4:41352 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 1 peer4:42572 OrgB.PEER1.IP:7051 SYN_SENT tcp 0 0 peer4:44140 OrgB.PEER2.IP:7051 ESTABLISHED tcp 0 0 peer4:41916 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:34304 172.18.0.2:couchdb TIME_WAIT tcp 0 0 peer4:38200 OrgB.PEER2.IP:7051 ESTABLISHED tcp 0 0 peer4:35602 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:34925 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:39412Â OrgB.PEER2.IP:7051 ESTABLISHED tcp 0 0 peer4:41018 OrgB.PEER1.IP:7051 ESTABLISHED tcp 0 0 peer4:38912 OrgB.PEER2.IP:7051 ESTABLISHED  .  .  .  ----- nsenter -t \{docker.container.pid} -n netstat end-  Â   Â   ></body> </Action>
<Action id="61409" issue="40164" author="ronenschafferibm" type="comment" created="2019-07-02 08:44:49.0" updateauthor="ronenschafferibm" updated="2019-07-02 08:44:49.0"> <body><! CDATA  ~jooskim1  That's a very interesting finding. I'd like to ask for more details the may help us: 1. How many connections are established to each peer? 2. Does the number of connections to each peer change over time? 3. How do things look at the destination peers (i.e. OrgB.PEER1 and OrgB.PEER2)? Do they also show the same connections?  Anyway, I found a bug (FAB-15840) that might be related to the issue  ></body> </Action>
<Action id="61477" issue="40164" author="vampire203" type="comment" created="2019-07-05 09:14:26.0" updateauthor="vampire203" updated="2019-07-07 14:23:37.0"> <body><! CDATA  ~ronenschafferibm , I work with Jason. Here are the answers to your question:  1. Sometimes, the number of connection is greater on one peer or distributed equally on each peers. The thing is that when the peer is restarted, the number of connection resets and then increases gradually and the peer which got restarted oldest has the most number of connection.Â   On our production environment here are the # connections currently: OrgA Peer1 : 7811 (docker container has been up for 5 days) OrgA Peer2 : 3250 (has been up for 2 days) OrgA Peer3 : 3390 (has been up for 2 days) OrgA Peer4 : 1458 (has been up for 26 hours)  2. The number of connection keep changes(increases/decreases) in short term, but in long term, it gradually increases.  3. Same thing happens on the destination peers as well. The connection gradually increases on the both peers.  ></body> </Action>
<Action id="61566" issue="40164" author="ronenschafferibm" type="comment" created="2019-07-10 08:37:38.0" updateauthor="ronenschafferibm" updated="2019-07-10 08:37:38.0"> <body><! CDATA  ~vampire203 , Thanks for the additional info. I didn't manage to recreate this problem in my environments. Can you please supply a full log file of a peer with multiple connections issue? Also, please set the logging level of the gossip module to debug. That can be done by setting the following environment variable in the peer's container:  {code} FABRIC_LOGGING_SPEC="gossip=DEBUG" {code}   https://hyperledger-fabric.readthedocs.io/en/release-1.4/logging-control.html#logging-specification   ></body> </Action>
<Action id="62025" issue="40164" author="jooskim1" type="comment" created="2019-07-23 03:33:01.0" updateauthor="jooskim1" updated="2019-07-23 03:33:01.0"> <body><! CDATA  ~ronenschafferibm   Hi, I work with Sam, and I hope this additional information helps you to solve this issue.  The attachment is a gossip debug log you requested before.   ^gossip_on_off_debug.log   ></body> </Action>
<Action id="62566" issue="40164" author="yacovm" type="comment" created="2019-08-03 17:19:46.0" updateauthor="yacovm" updated="2019-08-03 17:19:46.0"> <body><! CDATA Let's take a look at the log and see what's going on:  {code} 2019-07-22 10:02:34.945 UTC  gossip.comm  authenticateRemotePeer -> DEBU 5ad Authenticated 192.168.80.1:55336 2019-07-22 10:02:34.945 UTC  gossip.comm  GossipStream -> DEBU 5ae Servicing 192.168.80.1:55336 {code}  192.168.80.1 opened a gRPC stream to us and we can now receive point to point messages from it. As a side note, the certificate of 192.168.0.1, (his friends call it "exchange-peer-1"), says it belongs to a client OU: Subject: OU=client, CN=exchange-peer-1 It should belong to a peer OU... but let's put it aside.  {code} 2019-07-22 10:02:34.958 UTC  gossip.gossip  handleMessage -> DEBU 5af Entering, 192.168.80.1:55336 a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297 sent us GossipMessage: tag:EMPTY mem_req:<self_information:<payload:"\030\001*G\n6\n\022192.168.0.165:7053\032 \243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227\022\r\010\300\363\274\327\364\231\354\331\025\020\247\007" signature:"0E\002!\000\243\020\235\307*\342, \227\3115\260'=\360d\322\363\2632\234\310\337.h;,\242\333\2652\247\002 \032\330`\262\232\2368o\315T\030\007m\242\035\014\276\276\244\252\225Jm5\237+ ER<,\367" > > , Envelope: 158 bytes, Signature: 0 bytes 2019-07-22 10:02:34.958 UTC  gossip.gossip  handleMessage -> DEBU 5b0 Exiting 2019-07-22 10:02:34.958 UTC  gossip.discovery  handleMsgFromComm -> DEBU 5b1 Got message: GossipMessage: tag:EMPTY mem_req:<self_information:<payload:"\030\001*G\n6\n\022192.168.0.165:7053\032 \243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227\022\r\010\300\363\274\327\364\231\354\331\025\020\247\007" signature:"0E\002!\000\243\020\235\307*\342, \227\3115\260'=\360d\322\363\2632\234\310\337.h;,\242\333\2652\247\002 \032\330`\262\232\2368o\315T\030\007m\242\035\014\276\276\244\252\225Jm5\237+ ER<,\367" > > , Envelope: 158 bytes, Signature: 0 bytes 2019-07-22 10:02:34.958 UTC  gossip.gossip  ValidateAliveMsg -> DEBU 5b2 Fetched identity of  163 90 35 161 139 49 16 126 112 65 179 16 150 115 204 165 217 47 222 104 162 195 206 25 104 22 111 52 208 163 194 151  from identity store {code}  The peer exchange-peer-1 sent us a membership request, and in the message it says about its own IP address:  mem_req:<self_information:<payload:"\030\001*G\n6\n\022192.168.0.165:7053\032  This means that exchange-peer-1 tells us that its hostname is 192.168.0.165, but we got it from 192.168.80.1.  Is it behind a NAT or some proxy?   Another question would be - are you using TLS? If yes, then can you please show me the TLS server certificate of exchange-peer-1 ?  Moving on:  {code} 2019-07-22 10:02:34.959 UTC  gossip.discovery  handleAliveMessage -> DEBU 5b3 Entering GossipMessage: tag:EMPTY alive_msg:<membership:<endpoint:"192.168.0.165:7053" pki_id:"\243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227" > timestamp:<inc_num:1563787899989604800 seq_num:935 > > , Envelope: 75 bytes, Signature: 71 bytes 2019-07-22 10:02:34.959 UTC  gossip.discovery  resurrectMember -> DEBU 5b4 Entering, AliveMessage: GossipMessage: tag:EMPTY alive_msg:<membership:<endpoint:"192.168.0.165:7053" pki_id:"\243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227" > timestamp:<inc_num:1563787899989604800 seq_num:935 > > , Envelope: 75 bytes, Signature: 71 bytes t: {1563787899989604800 935 {}    0} 2019-07-22 10:02:34.959 UTC  gossip.discovery  resurrectMember -> DEBU 5b5 Exiting 2019-07-22 10:02:34.959 UTC  gossip.discovery  handleAliveMessage -> DEBU 5b6 Exiting {code}  Our membership module (gossip.discovery) decided that it saw a new message with the highest timestamp it has seen so far, so it means that peer came back alive, so it resurrected it and moved it from the dead peer list to the alive peer list.  {code} 2019-07-22 10:02:34.960 UTC  gossip.discovery  sendMemResponse -> DEBU 5b7 Entering endpoint:"192.168.0.165:7053" pki_id:"\243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227" 2019-07-22 10:02:34.960 UTC  gossip.comm  Send -> DEBU 5b8 Entering, sending GossipMessage: Channel: , nonce: 0, tag: EMPTY MembershipResponse with Alive: 3, Dead: 0, Envelope: 465 bytes, Signature: 0 bytes to  1 peers 2019-07-22 10:02:34.961 UTC  gossip.discovery  sendMemResponse -> DEBU 5b9 Exiting, replying with alive:<payload:"\030\001*H\n7\n\023kiesnet-peer-2:7051\032 \353\366\246QC\001_\376\330Y\361\261\276\003\0219\270\302\306\005\216\334)\232\260\007\304\035W\256\251*\022\r\010\340\311\220\215\247\231\354\331\025\020\323\006" signature:"0E\002!\000\223\213\327\310C \335\331\365n\345\274c{\307\221\341\017/6~\020\033i0\255Q\340\034@\005\337\002 !V\273E\006T\260\340\333\003\201\027\236\316Z\211Z\363\227\324\306g\326\025\003\200\022\000~C\016\030" > alive:<payload:"\030\001*G\n6\n\022192.168.0.165:7053\032 \243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227\022\r\010\300\363\274\327\364\231\354\331\025\020\247\007" signature:"0E\002!\000\243\020\235\307*\342, \227\3115\260'=\360d\322\363\2632\234\310\337.h;,\242\333\2652\247\002 \032\330`\262\232\2368o\315T\030\007m\242\035\014\276\276\244\252\225Jm5\237+ ER<,\367" > alive:<payload:"\030\001*H\n7\n\023kiesnet-peer-1:7051\032 \313\005\206\255\026\"F\n\203F\275R\350\211:\202\213\n\232^\222\331zQ\372\t\r\361\366\260\342-\022\r\010\314\334\323\263\352\230\354\331\025\020\327\006" signature:"0D\002 s\257\312\0315\017\020\t\310\032\000\3474\235\341\320\273\356\362\214\245\323w\300\233\250\212\224\254\371\212\026\002 -W~(\230-\3212'\001\177\320\r\250A\375\326k8t`\344\322\020qz\300\342\202\327\336\260" > 2019-07-22 10:02:34.961 UTC  gossip.comm  sendToEndpoint -> DEBU 5ba Entering, Sending to 192.168.0.165:7053 , msg: GossipMessage: Channel: , nonce: 0, tag: EMPTY MembershipResponse with Alive: 3, Dead: 0, Envelope: 465 bytes, Signature: 0 bytes 2019-07-22 10:02:34.962 UTC  gossip.comm  sendToEndpoint -> DEBU 5bb Exiting 2019-07-22 10:02:34.962 UTC  gossip.discovery  handleMsgFromComm -> DEBU 5bc Exiting {code}  After receiving the membership request, it sends back a membership response to the remote peer.  {code} 2019-07-22 10:02:35.813 UTC  gossip.channel  reportMembershipChanges -> INFO 5bd Membership view has changed. peers went online:    192.168.0.165:7053    , current view:    192.168.0.165:7053    kiesnet-peer-2:7051   {code}  After a second, the membership tracker wakes up and reports it noticed a membership change. The peer we're looking at its logs is kiesnet-peer-1, and it says: "Hey, I know about kiesnet-peer-2 and also about 192.168.0.165:7053".  {code} 2019-07-22 10:02:36.281 UTC  comm.grpc.server  1 -> INFO 5c5 unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-07-22T10:02:38.28Z grpc.peer_address=192.168.80.1:55338 grpc.code=OK grpc.call_duration=125.3Âµs {code} exchange-peer-1 sent us a ping. I'd like to know why it did it... can you please give me logs of both exchange-peer-1 and this peer so I can cross reference?   {code} 2019-07-22 10:02:36.293 UTC  gossip.comm  authenticateRemotePeer -> DEBU 5c6 Sending GossipMessage: tag:EMPTY conn:<pki_id:"\313\005\206\255\026\"F\n\203F\275R\350\211:\202\213\n\232^\222\331zQ\372\t\r\361\366\260\342-" identity:"\n\007KIESNET\022\313\006-----BEGIN CERTIFICATE-----\nMIICQjCCAeigAwIBAgIUeFZ/JsDUhfmfL3PGq6Ad8FbnXuIwCgYIKoZIzj0EAwIw\nNDELMAkGA1UEBhMCS1IxEDAOBgNVBAoTB0tpZXNuZXQxEzARBgNVBAMTCmtpZXNu\nZXQtY2EwIBcNMTkwNzE3MDIwNTAwWhgPMjEwOTA2MjMwODMxMDBaMCoxDzANBgNV\nBAsTBmNsaWVudDEXMBUGA1UEAxMOa2llc25ldC1wZWVyLTEwWTATBgcqhkjOPQIB\nBggqhkjOPQMBBwNCAAQH+2mYIXRsl6bCkx6fSFe0sETR6JLLTm/FHnjhX/b8aa4J\nJUuvz0YoEcMf/Knf9AK+1LdPi6rOxOHO73xDSfAjo4HfMIHcMA4GA1UdDwEB/wQE\nAwIHgDAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBRUxk8iKION/jtEFVWssFw9t7Bk\nMDAfBgNVHSMEGDAWgBRyRkl7ClHuj0ByMS5xNV7kGsOe+jAZBgNVHREEEjAQgg5r\naWVzbmV0LXBlZXItMTBhBggqAwQFBgcIAQRVeyJhdHRycyI6eyJoZi5BZmZpbGlh\ndGlvbiI6IiIsImhmLkVucm9sbG1lbnRJRCI6ImtpZXNuZXQtcGVlci0xIiwiaGYu\nVHlwZSI6ImNsaWVudCJ9fTAKBggqhkjOPQQDAgNIADBFAiEA0M2Nwu/IaqiEFmuw\nzxQIu8G4MhIzt5MELJmLPa/aUxUCICDgZ3aBbVLXxo2EYkb4MtHjilcZpFPxelPP\nI0yaXM1R\n-----END CERTIFICATE-----\n" > , Envelope: 897 bytes, Signature: 71 bytes to 192.168.80.1:55338 2019-07-22 10:02:36.295 UTC  gossip.comm  authenticateRemotePeer -> DEBU 5c7 Received pki_id:"\243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227" identity:"\n\010EXCHANGE\022\317\006-----BEGIN CERTIFICATE-----\nMIICRTCCAeugAwIBAgIUfWAKZuKwPJ+marrkQlKY2eKmXHIwCgYIKoZIzj0EAwIw\nNDELMAkGA1UEBhMCS1IxEDAOBgNVBAoTB0tpZXNuZXQxEzARBgNVBAMTCmtpZXNu\nZXQtY2EwIBcNMTkwNzIyMDYzMjAwWhgPMjEwOTA2MjMwODMxMDBaMCsxDzANBgNV\nBAsTBmNsaWVudDEYMBYGA1UEAxMPZXhjaGFuZ2UtcGVlci0xMFkwEwYHKoZIzj0C\nAQYIKoZIzj0DAQcDQgAEgMljLTq4FV8jTDjhvDHOFFVa22AaztyPJYlso2BaUstg\n/KEiD3Ge8DSpCVK00zS9iEhtUzsyRupF3TvtdffVFaOB4TCB3jAOBgNVHQ8BAf8E\nBAMCB4AwDAYDVR0TAQH/BAIwADAdBgNVHQ4EFgQU2W2uujfsWn1cg4IyTDZsBsHQ\n7IMwHwYDVR0jBBgwFoAUckZJewpR7o9AcjEucTVe5BrDnvowGgYDVR0RBBMwEYIP\nZXhjaGFuZ2UtcGVlci0xMGIGCCoDBAUGBwgBBFZ7ImF0dHJzIjp7ImhmLkFmZmls\naWF0aW9uIjoiIiwiaGYuRW5yb2xsbWVudElEIjoiZXhjaGFuZ2UtcGVlci0xIiwi\naGYuVHlwZSI6ImNsaWVudCJ9fTAKBggqhkjOPQQDAgNIADBFAiEA+THz11pCuELy\nyk3SORnwB7MsERCstBQtj19HVgzJ8JYCIGDgyRTVCbzXBbQjIg43B40HuUV+0EIE\n7BCxKxveGzSX\n-----END CERTIFICATE-----\n"  from 192.168.80.1:55338 2019-07-22 10:02:36.297 UTC  gossip.comm  authenticateRemotePeer -> DEBU 5c8 Authenticated 192.168.80.1:55338 2019-07-22 10:02:36.297 UTC  gossip.comm  GossipStream -> DEBU 5c9 Servicing 192.168.80.1:55338 2019-07-22 10:02:36.297 UTC  gossip.comm  serviceConnection -> DEBU 5ca Closing reading from stream 2019-07-22 10:02:36.297 UTC  gossip.comm  func2 -> DEBU 5cb Client 192.168.80.1:55336  disconnected 2019-07-22 10:02:36.298 UTC  gossip.comm  writeToStream -> DEBU 5cc Closing writing to stream 2019-07-22 10:02:36.298 UTC  comm.grpc.server  1 -> INFO 5cd streaming call completed grpc.service=gossip.Gossip grpc.method=GossipStream grpc.peer_address=192.168.80.1:55336 grpc.code=OK grpc.call_duration=1.3582301s 2019-07-22 10:02:36.299 UTC  gossip.comm  readFromStream -> DEBU 5ce a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297 canceling read because closing 2019-07-22 10:02:36.301 UTC  gossip.comm  func2 -> DEBU 5cf Client 192.168.80.1:55338  disconnected 2019-07-22 10:02:36.301 UTC  comm.grpc.server  1 -> INFO 5d0 streaming call completed grpc.service=gossip.Gossip grpc.method=GossipStream grpc.peer_address=192.168.80.1:55338 grpc.code=OK grpc.call_duration=7.8589ms 2019-07-22 10:02:36.772 UTC  gossip.discovery  periodicalReconnectToDead -> DEBU 5d1 Sleeping 25s {code}  The peer connected to us again, and we probably closed the already existing connection we had. I'm not sure though - why it happened. Seeing the logs on the other peer will help.  Next is a very strange scenario:  {code} 2019-07-22 10:02:36.814 UTC  gossip.comm  createConnection -> DEBU 5d4 Entering 192.168.0.165:7053 a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297 {code}  We're creating a connection to that peer, however: {code} 2019-07-22 10:02:37.900 UTC  gossip.comm  authenticateRemotePeer -> DEBU 625 Sending GossipMessage: tag:EMPTY conn:<pki_id:"\313\005\206\255\026\"F\n\203F\275R\350\211:\202\213\n\232^\222\331zQ\372\t\r\361\366\260\342-" identity:"\n\007KIESNET\022\313\006-----BEGIN CERTIFICATE-----\nMIICQjCCAeigAwIBAgIUeFZ/JsDUhfmfL3PGq6Ad8FbnXuIwCgYIKoZIzj0EAwIw\nNDELMAkGA1UEBhMCS1IxEDAOBgNVBAoTB0tpZXNuZXQxEzARBgNVBAMTCmtpZXNu\nZXQtY2EwIBcNMTkwNzE3MDIwNTAwWhgPMjEwOTA2MjMwODMxMDBaMCoxDzANBgNV\nBAsTBmNsaWVudDEXMBUGA1UEAxMOa2llc25ldC1wZWVyLTEwWTATBgcqhkjOPQIB\nBggqhkjOPQMBBwNCAAQH+2mYIXRsl6bCkx6fSFe0sETR6JLLTm/FHnjhX/b8aa4J\nJUuvz0YoEcMf/Knf9AK+1LdPi6rOxOHO73xDSfAjo4HfMIHcMA4GA1UdDwEB/wQE\nAwIHgDAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBRUxk8iKION/jtEFVWssFw9t7Bk\nMDAfBgNVHSMEGDAWgBRyRkl7ClHuj0ByMS5xNV7kGsOe+jAZBgNVHREEEjAQgg5r\naWVzbmV0LXBlZXItMTBhBggqAwQFBgcIAQRVeyJhdHRycyI6eyJoZi5BZmZpbGlh\ndGlvbiI6IiIsImhmLkVucm9sbG1lbnRJRCI6ImtpZXNuZXQtcGVlci0xIiwiaGYu\nVHlwZSI6ImNsaWVudCJ9fTAKBggqhkjOPQQDAgNIADBFAiEA0M2Nwu/IaqiEFmuw\nzxQIu8G4MhIzt5MELJmLPa/aUxUCICDgZ3aBbVLXxo2EYkb4MtHjilcZpFPxelPP\nI0yaXM1R\n-----END CERTIFICATE-----\n" > , Envelope: 897 bytes, Signature: 71 bytes to 192.168.80.1:55342 2019-07-22 10:02:37.901 UTC  gossip.comm  authenticateRemotePeer -> DEBU 626 Received pki_id:"\243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227" identity:"\n\010EXCHANGE\022\317\006-----BEGIN CERTIFICATE-----\nMIICRTCCAeugAwIBAgIUfWAKZuKwPJ+marrkQlKY2eKmXHIwCgYIKoZIzj0EAwIw\nNDELMAkGA1UEBhMCS1IxEDAOBgNVBAoTB0tpZXNuZXQxEzARBgNVBAMTCmtpZXNu\nZXQtY2EwIBcNMTkwNzIyMDYzMjAwWhgPMjEwOTA2MjMwODMxMDBaMCsxDzANBgNV\nBAsTBmNsaWVudDEYMBYGA1UEAxMPZXhjaGFuZ2UtcGVlci0xMFkwEwYHKoZIzj0C\nAQYIKoZIzj0DAQcDQgAEgMljLTq4FV8jTDjhvDHOFFVa22AaztyPJYlso2BaUstg\n/KEiD3Ge8DSpCVK00zS9iEhtUzsyRupF3TvtdffVFaOB4TCB3jAOBgNVHQ8BAf8E\nBAMCB4AwDAYDVR0TAQH/BAIwADAdBgNVHQ4EFgQU2W2uujfsWn1cg4IyTDZsBsHQ\n7IMwHwYDVR0jBBgwFoAUckZJewpR7o9AcjEucTVe5BrDnvowGgYDVR0RBBMwEYIP\nZXhjaGFuZ2UtcGVlci0xMGIGCCoDBAUGBwgBBFZ7ImF0dHJzIjp7ImhmLkFmZmls\naWF0aW9uIjoiIiwiaGYuRW5yb2xsbWVudElEIjoiZXhjaGFuZ2UtcGVlci0xIiwi\naGYuVHlwZSI6ImNsaWVudCJ9fTAKBggqhkjOPQQDAgNIADBFAiEA+THz11pCuELy\nyk3SORnwB7MsERCstBQtj19HVgzJ8JYCIGDgyRTVCbzXBbQjIg43B40HuUV+0EIE\n7BCxKxveGzSX\n-----END CERTIFICATE-----\n"  from 192.168.80.1:55342 2019-07-22 10:02:37.903 UTC  gossip.comm  authenticateRemotePeer -> DEBU 627 Authenticated 192.168.80.1:55342 {code}  The authentication begins only an entire second after. This implies that it took an entire second to issue a Ping RPC to the peer...  The next thing that happens, is that the remote peer also connects to us:  {code} 2019-07-22 10:02:38.326 UTC  comm.grpc.server  1 -> INFO 62b unary call completed grpc.service=gossip.Gossip grpc.method=Ping grpc.request_deadline=2019-07-22T10:02:40.326Z grpc.peer_address=192.168.80.1:55344 grpc.code=OK grpc.call_duration=124.1Âµs 2019-07-22 10:02:38.333 UTC  gossip.comm  authenticateRemotePeer -> DEBU 62c Sending GossipMessage: tag:EMPTY conn:<pki_id:"\313\005\206\255\026\"F\n\203F\275R\350\211:\202\213\n\232^\222\331zQ\372\t\r\361\366\260\342-" identity:"\n\007KIESNET\022\313\006-----BEGIN CERTIFICATE-----\nMIICQjCCAeigAwIBAgIUeFZ/JsDUhfmfL3PGq6Ad8FbnXuIwCgYIKoZIzj0EAwIw\nNDELMAkGA1UEBhMCS1IxEDAOBgNVBAoTB0tpZXNuZXQxEzARBgNVBAMTCmtpZXNu\nZXQtY2EwIBcNMTkwNzE3MDIwNTAwWhgPMjEwOTA2MjMwODMxMDBaMCoxDzANBgNV\nBAsTBmNsaWVudDEXMBUGA1UEAxMOa2llc25ldC1wZWVyLTEwWTATBgcqhkjOPQIB\nBggqhkjOPQMBBwNCAAQH+2mYIXRsl6bCkx6fSFe0sETR6JLLTm/FHnjhX/b8aa4J\nJUuvz0YoEcMf/Knf9AK+1LdPi6rOxOHO73xDSfAjo4HfMIHcMA4GA1UdDwEB/wQE\nAwIHgDAMBgNVHRMBAf8EAjAAMB0GA1UdDgQWBBRUxk8iKION/jtEFVWssFw9t7Bk\nMDAfBgNVHSMEGDAWgBRyRkl7ClHuj0ByMS5xNV7kGsOe+jAZBgNVHREEEjAQgg5r\naWVzbmV0LXBlZXItMTBhBggqAwQFBgcIAQRVeyJhdHRycyI6eyJoZi5BZmZpbGlh\ndGlvbiI6IiIsImhmLkVucm9sbG1lbnRJRCI6ImtpZXNuZXQtcGVlci0xIiwiaGYu\nVHlwZSI6ImNsaWVudCJ9fTAKBggqhkjOPQQDAgNIADBFAiEA0M2Nwu/IaqiEFmuw\nzxQIu8G4MhIzt5MELJmLPa/aUxUCICDgZ3aBbVLXxo2EYkb4MtHjilcZpFPxelPP\nI0yaXM1R\n-----END CERTIFICATE-----\n" > , Envelope: 897 bytes, Signature: 70 bytes to 192.168.80.1:55344 2019-07-22 10:02:38.333 UTC  gossip.comm  authenticateRemotePeer -> DEBU 62d Received pki_id:"\243Z#\241\2131\020~pA\263\020\226s\314\245\331/\336h\242\303\316\031h\026o4\320\243\302\227" identity:"\n\010EXCHANGE\022\317\006-----BEGIN CERTIFICATE-----\nMIICRTCCAeugAwIBAgIUfWAKZuKwPJ+marrkQlKY2eKmXHIwCgYIKoZIzj0EAwIw\nNDELMAkGA1UEBhMCS1IxEDAOBgNVBAoTB0tpZXNuZXQxEzARBgNVBAMTCmtpZXNu\nZXQtY2EwIBcNMTkwNzIyMDYzMjAwWhgPMjEwOTA2MjMwODMxMDBaMCsxDzANBgNV\nBAsTBmNsaWVudDEYMBYGA1UEAxMPZXhjaGFuZ2UtcGVlci0xMFkwEwYHKoZIzj0C\nAQYIKoZIzj0DAQcDQgAEgMljLTq4FV8jTDjhvDHOFFVa22AaztyPJYlso2BaUstg\n/KEiD3Ge8DSpCVK00zS9iEhtUzsyRupF3TvtdffVFaOB4TCB3jAOBgNVHQ8BAf8E\nBAMCB4AwDAYDVR0TAQH/BAIwADAdBgNVHQ4EFgQU2W2uujfsWn1cg4IyTDZsBsHQ\n7IMwHwYDVR0jBBgwFoAUckZJewpR7o9AcjEucTVe5BrDnvowGgYDVR0RBBMwEYIP\nZXhjaGFuZ2UtcGVlci0xMGIGCCoDBAUGBwgBBFZ7ImF0dHJzIjp7ImhmLkFmZmls\naWF0aW9uIjoiIiwiaGYuRW5yb2xsbWVudElEIjoiZXhjaGFuZ2UtcGVlci0xIiwi\naGYuVHlwZSI6ImNsaWVudCJ9fTAKBggqhkjOPQQDAgNIADBFAiEA+THz11pCuELy\nyk3SORnwB7MsERCstBQtj19HVgzJ8JYCIGDgyRTVCbzXBbQjIg43B40HuUV+0EIE\n7BCxKxveGzSX\n-----END CERTIFICATE-----\n"  from 192.168.80.1:55344 2019-07-22 10:02:38.334 UTC  gossip.comm  authenticateRemotePeer -> DEBU 62e Authenticated 192.168.80.1:55344 2019-07-22 10:02:38.334 UTC  gossip.comm  GossipStream -> DEBU 62f Servicing 192.168.80.1:55344 2019-07-22 10:02:38.334 UTC  gossip.comm  serviceConnection -> DEBU 630 Closing reading from stream 2019-07-22 10:02:38.334 UTC  gossip.comm  func2 -> DEBU 631 Client 192.168.80.1:55342  disconnected 2019-07-22 10:02:38.334 UTC  gossip.comm  writeToStream -> DEBU 632 Closing writing to stream 2019-07-22 10:02:38.334 UTC  comm.grpc.server  1 -> INFO 633 streaming call completed grpc.service=gossip.Gossip grpc.method=GossipStream grpc.peer_address=192.168.80.1:55342 grpc.code=OK grpc.call_duration=434.2311ms 2019-07-22 10:02:38.334 UTC  gossip.comm  serviceConnection -> DEBU 634 Closing reading from stream 2019-07-22 10:02:38.334 UTC  gossip.comm  func2 -> DEBU 635 Client 192.168.80.1:55344  disconnected {code}  Since we already had a connection to it, we close our current connection, and the current goroutines that service that connection, pick up the closing: {code} 2019-07-22 10:02:38.334 UTC  gossip.comm  readFromStream -> DEBU 636 a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297 canceling read because closing 2019-07-22 10:02:38.334 UTC  comm.grpc.server  1 -> INFO 637 streaming call completed grpc.service=gossip.Gossip grpc.method=GossipStream grpc.peer_address=192.168.80.1:55344 grpc.code=OK grpc.call_duration=1.8578ms 2019-07-22 10:02:38.335 UTC  gossip.comm  writeToStream -> DEBU 638 Closing writing to stream 2019-07-22 10:02:38.335 UTC  gossip.comm  readFromStream -> DEBU 639 a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297 canceling read because closing {code}  It seemed we couldn't obtain a connection because the deadline has exceeded:  {code} 2019-07-22 10:02:39.816 UTC  gossip.comm  createConnection -> DEBU 63a Exiting 2019-07-22 10:02:39.817 UTC  gossip.comm  sendToEndpoint -> WARN 63b Failed obtaining connection for 192.168.0.165:7053, PKIid:a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297 reason: context deadline exceeded 2019-07-22 10:02:39.817 UTC  gossip.comm  sendToEndpoint -> DEBU 63c Exiting {code}  If such a thing happens, we immediately mark the peer as offline and move it to the dead peer list:  {code} 2019-07-22 10:02:39.818 UTC  gossip.discovery  expireDeadMembers -> WARN 63d Entering  a35a23a18b31107e7041b3109673cca5d92fde68a2c3ce1968166f34d0a3c297  {code}  Next, the membership tracker wakes up again and reports this:  {code} 2019-07-22 10:02:40.813 UTC  gossip.channel  reportMembershipChanges -> INFO 642 Membership view has changed. peers went offline:    192.168.0.165:7053    , current view:    kiesnet-peer-2:7051   {code}  Is it possible to see logs of the 2 peers (this peer and the exachange peer) to see what's going on?   ></body> </Action>
<Action id="62567" issue="40164" author="yacovm" type="comment" body="Another question would be - you said you have 2 peers in org B... where is the second peer of orgB ? Is it running? Is it possible that both peers have the same certificate or something? (pardon the question, I highly doubt that and only asking to make sure)" created="2019-08-03 17:26:37.0" updateauthor="yacovm" updated="2019-08-03 17:26:37.0"/>
<Action id="62614" issue="40164" author="jooskim1" type="comment" created="2019-08-06 07:25:46.0" updateauthor="jooskim1" updated="2019-08-06 07:25:46.0"> <body><! CDATA Below is answers for your questions. 1. Is it behind a NAT or some proxy? --> My test environment is not under NAT or proxy.  2. Another question would be - are you using TLS? If yes, then can you please show me the TLS server certificate of exchange-peer-1 ? --> All blockchain network is not using TLS certifications.  I give you logs as attachments.   ^org1.peer1.log Â    ^org1.peer2.log    ^org2.peer1.log Â    ^org2.peer2.log   Â   Â   ></body> </Action>
<Action id="62615" issue="40164" author="yacovm" type="comment" created="2019-08-06 08:04:06.0" updateauthor="yacovm" updated="2019-08-06 08:04:06.0"> <body><! CDATA {quote}--> All blockchain network is not using TLS certifications. {quote} Â   If you're not using TLS then how do we see TLS certificates in the log?  ></body> </Action>
<Action id="62616" issue="40164" author="jooskim1" type="comment" body="Hi Yacov, thanks for the quick response. Where do you see the &quot;TLS certificates&quot; in the log? As i already mentioned earlier, i am reconfirming that we are not using any TLS on both the first and second orgs." created="2019-08-06 08:12:31.0" updateauthor="jooskim1" updated="2019-08-06 08:12:31.0"/>
<Action id="62618" issue="40164" author="yacovm" type="comment" created="2019-08-06 08:31:46.0" updateauthor="yacovm" updated="2019-08-06 08:31:46.0"> <body><! CDATA Oh, my bad - I thought that we also had these certificates prints in the gRPC logs.  Â   I'll look over your logs and analyze them when i have the time :)  ></body> </Action>
<Action id="63247" issue="40164" author="yacovm" type="comment" created="2019-08-24 17:09:33.0" updateauthor="yacovm" updated="2019-08-24 17:09:33.0"> <body><! CDATA  ~jooskim1  - there are more than 4 peers in this story... we have the kiesnet peers (1,2) and *192.168.0.169:7051* and *192.168.0.169:8051*. At the same time we have some *192.168.0.169:7053* which none of the 4 logs show.   Â   the logs of *org1.peer2* end at *05:31:45* and those of *org2.peer2* start at *05:28:38* and end at *05:33:00*   Those of *17642_org1.peer1.log* start at *04:46:36* and end at *05:31:50* ...   Â   Also, seems like only the 2nd organization has debug logs enabled.  Â   Is it possible to get all the logs of all peers, in debug level, and when they are started at the same time?  Â   Â   ></body> </Action>
<Action id="63503" issue="40164" author="jooskim1" type="comment" created="2019-09-03 07:56:56.0" updateauthor="jooskim1" updated="2019-09-03 07:57:48.0"> <body><! CDATA when we tried to reproduce the issue to provide you the log files again, we realize that the steps to reproduce the issue was totally wrong. Here's the new steps to reproduce the issue:  Below is my environment on AWS (centos7) 1. Prepare a network with org1 and org2 * org1 - 2 peer instances 1 solo orderer instances. 2 peers are anchor of org1. bootstrapped each other. * org2 - 1 peer instance. no anchor. 2. Create a channel and join peers on org1 to the channel. * Also update the channel for anchor peers information. 3. Create peer container on org2 and join the channel. 4. Down the peer container on org2(completely remove the peer, not just stopping the container).  5. Enroll the peer msp again in org2. (that means peer on org2 is using different MSP this time) 6. Create peer container on org2 and join the channel again.  so, org2-peer1 has joined the channel twice with different MSPs each time. And it seems peers on org1 are looking for all the pkids even though the first pkid on org2 must expire at some point. as a result, org2-peer1 keep goes online/offline(from org1 peers) and peers in org1 gets isolated after certain period of time.  I attached logs from my environment with DEBUG and INFO log level.Â  ^logs.zip   I also clean up duplicated log files I uploaded before.  ></body> </Action>
<Action id="63504" issue="40164" author="yacovm" type="comment" created="2019-09-03 08:19:03.0" updateauthor="yacovm" updated="2019-09-03 08:19:03.0"> <body><! CDATA what do you mean by: " Enroll the peer msp again in org2. (that means peer on org2 is using different MSP this time)" ?  You create a peer in org2 and join it to the channel, then bring it down, and you then bring it up again but with a different certificate but the MSP is still the org2 MSP, no?  ></body> </Action>
<Action id="63508" issue="40164" author="jooskim1" type="comment" created="2019-09-03 10:08:29.0" updateauthor="jooskim1" updated="2019-09-03 10:08:29.0"> <body><! CDATA You are exactly right. sorry for the confusion. I meant âMSP directoryâ or certificate.  Until today, we thought only the disconnection / reconnection on org2 peer caused the issue, but we were wrong. It was the different certificate on the same IP address.  So, if a peer in org2 is up again on the same IP address with different certificate, there are more than one PKIid on the same IP address, and that is the moment where the membership view online/offline of the org2 peer happening repeatedly on peers of org1. For all the PKIids, the anchor peers repeatedly/infinitely try to reconnect the grpc to the membership members, and it keep consumes the local ports which causes the peer isolation problem.   Thus, we think it may resolve the issue if the old PKIids are not being searched if the new PKIid(new certificate) is up on the same IP address.  Here's the log files again :  ^logs.zip   Â   ></body> </Action>
<Action id="63509" issue="40164" author="yacovm" type="comment" created="2019-09-03 10:23:06.0" updateauthor="yacovm" updated="2019-09-03 10:23:06.0"> <body><! CDATA Why are you enrolling the peer again, and not re-using the same certificate ? Because the certificate is on the peer itself and you have no access to it?  and why are you using solo orderer? you should use Raft instead, even for a single node  ></body> </Action>
<Action id="63510" issue="40164" author="jooskim1" type="comment" created="2019-09-03 10:35:33.0" updateauthor="jooskim1" updated="2019-09-03 10:35:33.0"> <body><! CDATA  ~yacovm ,Â   Because we didnât make the backup of the original certificate when we were brining down the peer on org2, so we had to re-enroll from fabric-ca and get the new certificate to bring the peer up again. Solo or Raft do not matter. We tried both and it causes the same issue.  ></body> </Action>
<Action id="64029" issue="40164" author="jyellick" type="comment" body="Is this still an issue? Or has this been resolved?" created="2019-09-24 14:10:46.0" updateauthor="jyellick" updated="2019-09-24 14:10:46.0"/>
<Action id="66377" issue="40164" author="gaurang" type="comment" created="2019-12-23 12:38:44.0" updateauthor="gaurang" updated="2019-12-23 12:38:44.0"> <body><! CDATA We too got the same issue while joining new org to the existing network.Â   Â   ></body> </Action>
<Action id="67955" issue="40164" author="mplisov" type="comment" created="2020-02-14 14:23:46.0" updateauthor="mplisov" updated="2020-02-14 14:23:46.0"> <body><! CDATA Can we have problems with gossip or discovery when two peers share the same IP address but different external endpoint fqdn (peer1.org1.example.com:7051 andÂ peer1.org2.example.com:7061 both same IP)?  Â   ></body> </Action>
<Action id="67957" issue="40164" author="yacovm" type="comment" created="2020-02-14 14:31:21.0" updateauthor="yacovm" updated="2020-02-14 14:31:21.0"> <body><! CDATA If they are different organizations then if they have the same internal IP address it's fine because they can never reach each other with the internal IP address.  Â   Additionally, every other peer is either in one of these 2 orgs, or in some third org, so it can only see and know about at most one of the internal IP addresses (or neither)  ></body> </Action>
