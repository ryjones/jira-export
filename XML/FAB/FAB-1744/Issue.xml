<Issue id="14173" key="FAB-1744" number="1744" project="10002" reporter="ryokawajp" creator="ryokawajp" type="10004" summary="Primary node does not come back to normal operation after a fail-stop, a restart and a synchronization" priority="3" resolution="10000" status="6" created="2017-01-19 02:41:25.0" updated="2018-07-20 14:11:16.0" resolutiondate="2017-04-07 20:06:03.0" votes="0" watches="3" workflowId="37101"> <environment><! CDATA - Fabric v0.6.0-preview - Security and privacy enabled - Ubuntu 16.04 - Docker  - REST API is used for sending transactions. - docker-compose.yaml to be attached soon. - K=2 - logmultiplier=2 - PBFT consensus - PBFT batch size = 1   ></environment> <description><! CDATA h2. Summary We were testing the checkpoint and the state transfer features of Hyperledger Fabric. During the test, we found a case that a failure peer does not accept new transactions even after recovery from the failure when the peer is a primary node.  h2. Related issues -  FAB-707  disconnected Peer can't recover from lost connection, then start sending view change requests - Hyperledger JIRA -- https://jira.hyperledger.org/browse/FAB-707 -- Looks like similar but the test cases are different. In FAB-707, a non-primary peer was stopped and restarted, while in this JIRA item, the primary node was stopped and restarted.  h2. Parameters - 4 nodes (vp0, vp1, vp2, vp3). The initial primary node is vp0. - chaincode_example02 - Fabric v0.6.0-preview - Security and privacy enabled - Ubuntu 14.04 (in Virtual Box and Vagrant) - Docker  1.12.1 - REST API is used for sending transactions. - docker-compose.yml (see the attached file) - K=2 - logmultiplier=2 - PBFT consensus - PBFT batch size = 1  h2. Test case - invoke 2 transactions to vp0.  - kill vp0 - invoke 3 transactions to vp1 - restart vp0 - invoke N transactions to vp1.  h2. Result - Initial sync occurs at : block height=11 - The sync (state transfer) occurs periodically after the initial sync. Period of sync = 8 blocks (or 8 Tx). -- Even after the sync, vp0 did not add a new block to its blockchain when it receives a new transactions. Other three peers (vp1, vp2, vp3) worked correctly. -- This periodic behavior does not occur when we kill vp2 (a non-primary node) instead of vp0.  h2. Interpretation of the result - It seems that it takes a time to start the Initial synchronization because in PBFT, Fabric executes the state transfer only when the sequence number exceeds the high water mark, which is order of K * logmultiplier. I suppose this is a designed behavior. - However, vp0's block height does not grow at all, while the block heights of the rest of the peers grow as they accept the additional transactions. And another state transfer occurs after 8 transactions later. This occurs periodically in the number of transactions. This behavior is different from what is expected. - When vp0 is killed, a view change occurs from view 0 to view 1. However, according to the log below, vp0 still thinks that he is still in view 0. Is this the reason why vp0 does not accept the new transactions, since the later transactions are sent to view 1?  Warning messages (example of K=2, logmultiplier=2) {noformat} vp0_1        | 01:27:42.250  consensus/pbft  recvCommit -> WARN 0bc Replica 0 ignoring commit for view=1/seqNo=8: not in-wv, in view 0, high water mark 2 vp0_1        | 01:27:42.304  consensus/pbft  recvCheckpoint -> WARN 0c5 Checkpoint sequence number outside watermarks: seqNo 8, low-mark 2 vp0_1        | 01:27:42.317  consensus/pbft  weakCheckpointSetOutOfRange -> WARN 0cd Replica 0 is out of date, f+1 nodes agree checkpoint with seqNo 8 exists but our high water mark is 6 vp0_1        | 01:27:42.318  consensus/pbft  sendPrePrepare -> WARN 0d5 Primary 0 not sending pre-prepare for batch aXZf/Zfi7CDLU5n5dANOpYIX81Oy0LuTZ2mX/Xly48d5yYaULtX+ndFIVKweQ/xej17JXRwkTIV0RiT1j+xKeA== - out of sequence numbers vp0_1        | 01:27:47.356  consensus/pbft  sendPrePrepare -> WARN 10d Primary 0 not sending pre-prepare for batch Tq+y1NuMKGw8eJjimsdRecr4RseJ//xE00lkjK+qvg2yK9giQwqCmPnlgCdBBVn/uzxwFsvYn/sHeLcL60O6Dw== - out of sequence numbers vp0_1        | 01:27:47.357  consensus/pbft  recvPrePrepare -> WARN 129 Pre-prepare from other than primary: got 1, should be 0 vp0_1        | 01:27:51.361  consensus/pbft  recvViewChange -> WARN 181 Replica 0 already has a view change message for view 1 from replica 0 {noformat}  Following is a part of situation1.log filtered by a command: `grep "Height" situation1.log`. Synchronization is highlighted in a bold font. {panel} Height of vp0 : 1 Height of vp1 : 1 Height of vp2 : 1 Height of vp3 : 1 Height of vp0 : 3 Height of vp1 : 3 Height of vp2 : 3 Height of vp3 : 3 Height of vp0 : 4 Height of vp1 : 4 Height of vp2 : 4 Height of vp3 : 4 Height of vp0 : Height of vp1 : 5 Height of vp2 : 5 Height of vp3 : 5 Height of vp0 : Height of vp1 : 6 Height of vp2 : 6 Height of vp3 : 6 Height of vp0 : Height of vp1 : 7 Height of vp2 : 7 Height of vp3 : 7 Height of vp0 : 4 Height of vp1 : 8 Height of vp2 : 8 Height of vp3 : 8 Height of vp0 : 4 Height of vp1 : 9 Height of vp2 : 9 Height of vp3 : 9 Height of vp0 : 4 Height of vp1 : 10 Height of vp2 : 10 Height of vp3 : 10 *Height of vp0 : 11* *Height of vp1 : 11* *Height of vp2 : 11* *Height of vp3 : 11* Height of vp0 : 11 Height of vp1 : 12 Height of vp2 : 12 Height of vp3 : 12 Height of vp0 : 11 Height of vp1 : 13 Height of vp2 : 13 Height of vp3 : 13 Height of vp0 : 11 Height of vp1 : 14 Height of vp2 : 14 Height of vp3 : 14 Height of vp0 : 11 Height of vp1 : 15 Height of vp2 : 15 Height of vp3 : 15 Height of vp0 : 11 Height of vp1 : 16 Height of vp2 : 16 Height of vp3 : 16 Height of vp0 : 11 Height of vp1 : 17 Height of vp2 : 17 Height of vp3 : 17 Height of vp0 : 11 Height of vp1 : 18 Height of vp2 : 18 Height of vp3 : 18 *Height of vp0 : 19* *Height of vp1 : 19* *Height of vp2 : 19* *Height of vp3 : 19* Height of vp0 : 19 Height of vp1 : 20 Height of vp2 : 20 Height of vp3 : 20 Height of vp0 : 19 Height of vp1 : 21 Height of vp2 : 21 Height of vp3 : 21 {panel}   ></description> </Issue>
