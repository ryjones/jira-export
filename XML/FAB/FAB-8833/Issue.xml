<Issue id="28464" key="FAB-8833" number="8833" project="10002" reporter="denyeart" assignee="yacovm" creator="denyeart" type="10002" summary="TEST PLAN: Service Discovery" priority="3" resolution="10000" status="6" created="2018-03-13 01:59:19.0" updated="2018-07-20 14:16:00.0" resolutiondate="2018-07-02 20:52:30.0" votes="0" watches="6" workflowId="41517"> <description><! CDATA Before closing this testplan task, coverage must be reviewed and approved (via upvote or comment) by the epic feature owner and the test leads identified below.  +*Location / Description of*+ docs, APIs, config vars, system configuration (fabric, SDKs, fabric-samples) :        +*Unit Test (UT) leader*+: Yacov  ~Automated unit tests are merged in repo, as go tests or otherwise part of build/verify jenkins jobs.~ ~Code coverage tracked in Jenkins:  https://jenkins.hyperledger.org/view/Daily/job/fabric-unit-test-coverage-daily-master-x86_64/ ~  The latest code coverage report (after the last CR will be merged) for the packages that were introduced is the following: {code:java} 16:16:31 unit-tests_1  | ok      github.com/hyperledger/fabric/common/graph    0.579s    coverage: 100.0% of statements 16:16:57 unit-tests_1  | ok      github.com/hyperledger/fabric/common/policies/inquire    0.135s    coverage: 99.1% of statements 16:17:07 unit-tests_1  | ok      github.com/hyperledger/fabric/core/cclifecycle    0.063s    coverage: 99.4% of statements 16:19:13 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery    0.027s    coverage: 100.0% of statements 16:19:18 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery/client    3.412s    coverage: 92.0% of statements 16:19:18 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery/endorsement    0.075s    coverage: 98.2% of statements 16:19:18 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery/support/acl    0.067s    coverage: 100.0% of statements 16:19:18 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery/support/chaincode    0.062s    coverage: 100.0% of statements 16:19:18 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery/support/config    0.443s    coverage: 87.7% of statements 16:19:19 unit-tests_1  | ok      github.com/hyperledger/fabric/discovery/support/gossip    0.048s    coverage: 100.0% of statements{code} A non-weighted (without taking into account lines of code per package) average of all packages above is *97.5%*  The breakdown for functionality per package is, roughly speaking: * *github.com/hyperledger/fabric/common/graph* - contains analysis of endorsement policy trees in raw form of graphs. * *github.com/hyperledger/fabric/common/policies/inquire* - contains principal set combination analysis, intersections and merging of principal sets. * *github.com/hyperledger/fabric/core/cclifecycle* - updates the gossip layer for chaincode installation/instantiation, and also serves as a retriever for chaincode endorsement policies and collection configuration. * *github.com/hyperledger/fabric/discovery* - contains the service (request processing and dispatch, access control checks) and the authentication cache for the service * *github.com/hyperledger/fabric/discovery/client* - contains a client implementation used by the go-SDK and also for the (work in progress) discovery CLI * *github.com/hyperledger/fabric/discovery/support/acl* - links the access control checks to the MSP * *github.com/hyperledger/fabric/discovery/support/chaincode* - links the cclifecycle to the discovery service * *github.com/hyperledger/fabric/discovery/support/config* - contains processing of config blocks to discovery proto representation * *github.com/hyperledger/fabric/discovery/support/gossip* - links gossip with the discovery service. The *87%* for the *discovery/support/config* is because the code parses a config block, which has a complex structure.  +*Function Test (FVT) leader*+: Yacov  A. Extended Unit Test - ---FAB-10172--- # A unit test that spawns an instance of a discovery service, with partially mocked support: The endorsement policy analysis, and chaincode lifecycle object would be using production implementations.  The gossip layer would be mocked, as the functionality for that hasn't changed since v1.0  The ledger layer would be mocked, but the data returned from it would be production implementations (i.e, production representation of endorsement policies, collection configuration). The config block retrieval would be mocked, but the data returned from it would be a production configuration block. The MSP would be mocked, but the data returned from it would be real MSP identities (which are, basically just x509 certificates decorated with strings of MSP IDs...) The test would spawn a production implementation of a client, with TLS and it would: ** Have the client query the discovery service for all possible queries, and the output of the endorser query (the most complex one) would be inspected. ** The endorsement policy would be something complex like an AND of ORs and ORs of ANDs, as well as trivial ones as ANDs and ORs. ** Issue a config update that would revoke subsequent access of the client to the service. ** The client is expected to try to query again, and it would be rejected.  B.  E2E testcases scenario ( FAB-9524 ):  Create a network with a single channel and 3 organizations - Org1, Org2, Org3 with 2 peers per organization \{p0, p1}, and a chaincode with endorsement policy of "3 choose 2" #  ## Use the discovery CLI (To be implemented in FAB-10110) to query all discovery queries, and expect: ### Peers to be in the membership view, config query should also work, but endorsement queries would return that the chaincode cannot be found since it's not instantiated. ## Install the chaincode on p0.Org1 and instantiate it. ## Use the discovery CLI to query all discovery queries, and expect: ### The membership query would now have a chaincode installed on p0.Org1 ### The endorser query would return that the endorsement policy cannot be satisfied ## Install and instantiate the chaincode on every p0 but not on p1 for Org2 and Org3. ## Use the discovery CLI to query all discovery queries, and verify the results: ### The membership query should contain now that all peers p0's have the chaincode but not the p1's. ### The endorser query should convey 3 different layouts (A, B) , (A, C) and (B, C) and with 1 peer each. ## Install another chaincode and instantiate it on all peers. Its endorsement policy would be "3 choose 3" (1 signature from every org). ## Use the discovery CLI to query a chaincode to chaincode endorsement query and verify the results: ### There should be only 1 layout with all organizations in it. ## upgrade the first chaincode and add a collection definition with a collection policy of (A, C). ## Use the discovery CLI to query a collection query for the collection, in the context of the first chaincode, and verify the results: ### There should be only a single layout corresponding to (A, C). ## Issue a config update and change the channel reader policy to Org1 OR Org2 (in other words - remove Org3 from the channel readers) ### Use the discovery CLI to query a peer query and ensure that "access denied" is returned.     C.  Other functional testing required: No other tests needed     +*System Verification Test (SVT) leader*+:  ~dongming  V1.2 System Test Plan (Epic) FAB-???? Covers stress, load, performance, full system integration, resiliency, recover, etc.  Considering this feature, what scenarios are of particular interest and concern: # Scenario Description X # Scenario Description Y # Scenario Description Z     *Yacov:* I guess it can be useful to measure the performance of service discovery queries, particularly if they are repeated one after the other with the same query. The reason is that the service has an access control cache that caches previous access control checks to reduce the CPU load on the peer and leverage mutual TLS authentication. So, if you do a query that its payload is Foo twice or thrice, the 2nd and 3rd times should be processed faster than the first one - especially if the chaincode is installed on the peer itself, because then the results would be fetched from yet another in-memory cache and it doesn't go all the way to the stateDB.  Ideally, since the client already connects to some peer for events - you can reuse the same TLS connection and use it for service discovery queries, and this means you can do a discovery query in which no side (no client, nor server) had done any signature signing or any signature verification to authorize the request.     ></description> </Issue>
