<Action id="69063" issue="44790" author="yacovm" type="comment" created="2020-04-26 08:57:04.0" updateauthor="yacovm" updated="2020-04-26 08:57:04.0"> <body><! CDATA {quote} Since these are non-blocking sends, we wouldn't expect the go routines to stick around for long ... so why run sendToEndpoint in a go routine? Doesn't seem like it would be needed or desired... {quote}  The reason is that gossip's connection creation process couples gRPC connection creation and application level authentication together atomically. When you want to send a message from a higher layer of gossip to a remote endpoint, you also authenticate that peer and this involves in: (1) Establishing gRPC connection in a blocking manner (WithBlock) (2) Sending an authentication message and receiving an authentication message to that peer  Before these two things finish, you are blocked.  In retrospect, perhaps what should have been considered is first creating the connection object and its corresponding buffers, and then making it asynchronously connect via delaying the connection creation and authentication, to the serviceConnection(). However this creates a problem... we don't really know what is the PKI-ID of the remote peer we connect to because it might have changed its PKI-ID (if its certificate is changed), so if we put it inside the mapping, and connect to it asynchronously from within the connection handling itself, - we will end up with a goroutine and a connection leak :(   Anyway, let me try to explain what's going on in the existing code:  {code} 	cs.Lock() 	destinationLock, hasConnected := cs.destinationLocks string(pkiID)  	if !hasConnected { 		destinationLock = &sync.Mutex{} 		cs.destinationLocks string(pkiID)  = destinationLock 	} 	cs.Unlock() {code} We need to ensure that only a single goroutine would connect to the endpoint we are trying to connect to. The reason is that gossip allows only a single connection for any remote peer, and if we'll try to connect concurrently then we'll end up tripping ourselves because the remote peer will drop all attempts but the last one. Additionally, we want to allow connection attempts to different endpoints concurrently, so we use a lock per destination.  {code} 	destinationLock.Lock() 	cs.RLock() 	conn, exists := cs.pki2Conn string(pkiID)  	if exists { 		cs.RUnlock() 		destinationLock.Unlock() 		return conn, nil 	} 	cs.RUnlock() {code} We check maybe during the connection to the endpoint, some other goroutine has already connected to it. {code} 	createdConnection, err := cs.connFactory.createConnection(endpoint, pkiID) 	if err == nil { 		cs.logger.Debugf("Created new connection to %s, %s", endpoint, pkiID) 	}  	destinationLock.Unlock() {code} Else, we connect to the destination while holding the lock.  This is probably where the goroutine herd is piling up and I think we should consider either using a Semaphore instead of a lock, and attempting to grab it for an upper limit of time and then giving up.  Anyway, let's continue: {code} 	cs.Lock() 	delete(cs.destinationLocks, string(pkiID)) 	defer cs.Unlock() {code} Once we reached this point, we (probably) have a connection in our hands that we established ourselves.  No point in keeping the destination lock in memory any more, as when we exit this method we will have a connection in the mapping and the destination lock won't be needed anymore at least until the connection would be deleted, so we delete the destination lock and we hold the lock for the rest of the execution of the method.  {code} 	// check again, maybe someone connected to us during the connection creation? 	conn, exists = cs.pki2Conn string(pkiID)   	if exists { 		if createdConnection != nil { 			createdConnection.close() 		} 		return conn, nil 	}  	// no one connected to us AND we failed connecting! 	if err != nil { 		return nil, err 	} {code}  Recall that gossip ensures a single bi-directional connection between each 2 peers, and while we were connecting to the remote peer, the same peer might have connected to us. That remote peer, while connecting to us - doesn't do it under the destination lock since it is used only for outgoing connections, so as a result the connection might have been registered in the connection mapping (cs.pki2Conn), so we check if it is the case and if so, we discard our connection and close it, returning the connection from the remote peer to us. Of course, it could be that we failed connecting earlier in our attempt, so we just return error as seen above.  {code}  	// At this point we know the latest PKI-ID of the remote peer. 	// Perhaps we are trying to connect to a peer that changed its PKI-ID. 	// Ensure that we are not already connected to that PKI-ID, 	// and if so - close the old connection in order to keep the latest one. 	// We keep the latest one because the remote peer is going to close 	// our old connection anyway once it sensed this connection handshake. 	if conn, exists := cs.pki2Conn string(createdConnection.pkiID) ; exists { 		conn.close() 	}  	// at this point in the code, we created a connection to a remote peer 	conn = createdConnection 	cs.pki2Conn string(createdConnection.pkiID)  = conn {code} The above code handles the corner case I mentioned above where we connected to a peer we thought its PKI-ID is X but we discovered it has changed and it's Y now.  We need to detect if that remote peer has already connected to us, but under its new name. Otherwise we violate the single connection invariant. {code}  	go conn.serviceConnection() {code} Finally, we spawn the goroutine that services the connection.   ></body> </Action>
<Action id="69064" issue="44790" author="yacovm" type="comment" created="2020-04-26 09:05:24.0" updateauthor="yacovm" updated="2020-04-26 09:05:24.0"> <body><! CDATA  ~sykesm  what do you think about the following solution?  Â  {code:java} diff --git a/gossip/comm/conn.go b/gossip/comm/conn.go index 0e375b3db..1213bcb44 100644 --- a/gossip/comm/conn.go +++ b/gossip/comm/conn.go @@ -9,6 +9,7 @@ package comm import ( 	"context" 	"sync" +	"time"  	proto "github.com/hyperledger/fabric-protos-go/gossip" 	"github.com/hyperledger/fabric/gossip/common" @@ -34,13 +35,13 @@ type connFactory interface {  type connectionStore struct { 	config           ConnConfig -	logger           util.Logger            // logger -	isClosing        bool                   // whether this connection store is shutting down -	shutdownOnce     sync.Once              // ensure shutdown is only called once per connectionStore -	connFactory      connFactory            // creates a connection to remote peer -	sync.RWMutex                            // synchronize access to shared variables -	pki2Conn         map string *connection // mapping between pkiID to connections -	destinationLocks map string *sync.Mutex //mapping between pkiIDs and locks, +	logger           util.Logger              // logger +	isClosing        bool                     // whether this connection store is shutting down +	shutdownOnce     sync.Once                // ensure shutdown is only called once per connectionStore +	connFactory      connFactory              // creates a connection to remote peer +	sync.RWMutex                              // synchronize access to shared variables +	pki2Conn         map string *connection   // mapping between pkiID to connections +	destinationLocks map string chan struct{} //mapping between pkiIDs and locks, 	// used to prevent concurrent connection establishment to the same remote endpoint }  @@ -49,7 +50,7 @@ func newConnStore(connFactory connFactory, logger util.Logger, config ConnConfig 		connFactory:      connFactory, 		isClosing:        false, 		pki2Conn:         make(map string *connection), -		destinationLocks: make(map string *sync.Mutex), +		destinationLocks: make(map string chan struct{}), 		logger:           logger, 		config:           config, 	} @@ -70,18 +71,28 @@ func (cs *connectionStore) getConnection(peer *RemotePeer) (*connection, error) 	cs.Lock() 	destinationLock, hasConnected := cs.destinationLocks string(pkiID)  	if !hasConnected { -		destinationLock = &sync.Mutex{} +		destinationLock = make(chan struct{}, 1) 		cs.destinationLocks string(pkiID)  = destinationLock 	} 	cs.Unlock()  -	destinationLock.Lock() +	destinationLockTimeout := time.NewTimer(time.Second) +	var timedOut bool +	select { +	case destinationLock <- struct{}{}: +	case destinationLockTimeout.C: +		timedOut = true +	} + +	if timedOut { +		return nil, errors.Errorf("timed out waiting for connection establishment") +	}  	cs.RLock() 	conn, exists := cs.pki2Conn string(pkiID)  	if exists { 		cs.RUnlock() -		destinationLock.Unlock() +		<-destinationLock 		return conn, nil 	} 	cs.RUnlock() @@ -91,7 +102,7 @@ func (cs *connectionStore) getConnection(peer *RemotePeer) (*connection, error) 		cs.logger.Debugf("Created new connection to %s, %s", endpoint, pkiID) 	}  -	destinationLock.Unlock() +	<-destinationLock  	cs.RLock() 	isClosing = cs.isClosing  {code}  ></body> </Action>
