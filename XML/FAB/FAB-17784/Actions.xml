<Action id="69058" issue="44911" author="denyeart" type="comment" created="2020-04-26 04:52:51.0" updateauthor="denyeart" updated="2020-05-11 22:22:52.0"> <body><! CDATA Looks like this legacy chaincode lifecycle was intentional, see comment in:  https://jira.hyperledger.org/browse/FAB-16866?focusedCommentId=65988&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-65988  In my opinion, the behavior with legacy chaincode that allows a 'partial' install (chaincode actually installed but error returned and chaincode image not built) is wrong. I realize now that it was done to have some consistency with v1.x, but I would have preferred to completely fail the install to be consistent with new lifecycle and release note the behavior change, rather than leave the chaincode in an unexpected state on the peer. Alternatively if the behavior could be documented including remediation steps, that would be acceptable.  ></body> </Action>
<Action id="69207" issue="44911" author="denyeart" type="comment" body="The second issue relating to new lifecycle should probably be split into a separate issue. For now I included it with the first to provide the recreation context." created="2020-05-11 22:14:56.0" updateauthor="denyeart" updated="2020-05-11 22:15:40.0"/>
<Action id="69209" issue="44911" author="jyellick" type="comment" created="2020-05-12 04:33:18.0" updateauthor="jyellick" updated="2020-05-12 04:33:18.0"> <body><! CDATA As  ~denyeart  mentions in his reply.  The behavior of a 'partial install' for legacy chaincodes is working as designed.  I agree that it's odd, and broken behavior, but behavior that some applications may depend on.  This is not a hypothetical concern, as it is quite common to find users in the field which have retry logic built in around chaincode instantiation to address build timeouts.  Arguably, to be more consistent, we could have returned success on legacy chaincode install, even if the build failed as was the old behavior, but returning failure and leaving the package was judged to be a reasonable balance between sanity and compatibility.  With respect to the second issue, this is also "working as designed", though perhaps less obviously so.  The assumption made by the new lifecycle, is that if the package is exactly byte-for-byte the same, and a build has already been attempted, there's no need to try to rebuild, we already have the build result.  This is desirable in the event that a chaincode is on the filesystem, but has no built version available.  If the initial build fails, we remember the build failed, rather than entering an endless resource consuming cycle of build, fail, build, fail, etc.  Build results are cached by package ID, which changes any time the package contents change.  Because the package is identically the same, the peer still expects the same result will occur and returns the cached error.  In this particular case, the chaincode bytes were removed from the filesystem, so there's no risk of the build/fail cycle occurring, so, the behavior isn't advantageous here.  So, to address the second part of this bug we could:  1) Always retry failed builds -- stop caching failed build results. 2) Explicitly remove failed build results from the cache when that failure is a part of install. 3) Accept this odd behavior as a bit of a corner case and document that either the peer must be restarted to clear the cache, or some small package modification must be made (say, changing the label slightly).  My expectation would be that usually, when a user's chaincode does not build, they update the chaincode package to address the build failure, in which case, this is a non-issue.  If the user changes the chaincode environment (specifying a different ccenv tag for instance in core.yaml), this also requires a restart, which makes the point moot.  The only time this is a problem is when we have issue with resources having transient unavailability, like the missing tag specified here.   ~sykesm  probably has an opinion here as well.  ></body> </Action>
<Action id="69214" issue="44911" author="denyeart" type="comment" created="2020-05-12 13:38:47.0" updateauthor="denyeart" updated="2020-05-12 13:38:47.0"> <body><! CDATA For the first issue, early adopters of v2.0 have already complained about the unexpected results, so I do think some action is required. We intentionally did not make v2.0 a LTS release so that we could address early adopter feedback like this, and mention any changes in v2.2 release notes.  For the second issue, I would vote for Jason's option #2.  If we go with option #3, the error message should also be clarified to indicate that the cached error results are being returned, and that a server restart would be required for a fresh build of the chaincode.  ></body> </Action>
<Action id="69215" issue="44911" author="varadatibm" type="comment" created="2020-05-12 13:47:52.0" updateauthor="varadatibm" updated="2020-05-12 13:49:06.0"> <body><! CDATA  ~jyellick  I don't think we want to drop the non-working cc on the peer... there is no way to know that it is bad until you go to instantiate...   Also on the timeout scenario, we never had an issue with the install cc before in 1.4.x... obviously with the new format (V2), there is potential timeouts and retries... and also even if they had retries I suspect people would stop after X number of attempts.  I vote that we change the behavior to not install bad cc on the peer.  ></body> </Action>
<Action id="69217" issue="44911" author="sykesm" type="comment" created="2020-05-12 14:26:17.0" updateauthor="sykesm" updated="2020-05-12 14:26:17.0"> <body><! CDATA {quote}Explicitly remove failed build results from the cache when that failure is a part of install.{quote}  I think this is probably the correct move - with a minor qualification. If someone installs chaincode that doesn't build, there's no reason to believe that problem isn't permanent so I believe caching the failed result is appropriate.  That said, if someone pokes the peer and says "install this package" and the registry indicates the package build has failed, we should remove the failure and re-run the build process assuming some external issue was resolved.  Basically, in the normal case, there's no need to beat our head against wall trying to build the same package over and over and over again but we should assume that someone explicitly doing an install probably knows something we don't.  {quote}My expectation would be that usually, when a user's chaincode does not build, they update the chaincode package to address the build failure, in which case, this is a non-issue.{quote}  Yes, this is the reason why we don't keep retrying a failed build.  ></body> </Action>
<Action id="69220" issue="44911" author="jyellick" type="comment" created="2020-05-12 20:44:10.0" updateauthor="jyellick" updated="2020-05-12 20:44:10.0"> <body><! CDATA So, it seems like we agree on (2) at least.  We should add special handling that, in the new lifecycle in event of install, if the build has already failed, we should explicitly remove the cached failure and try again.  With respect to (1), I'm not sure we have a solution?  To Varad's point, in v1.x, we did not have timeout issues on install, we had them on instantiate.  v2.0 moves the location of those timeouts.  The current behavior is that, like in v1.x, a failed build has no impact on whether the chaincode installs.  (Note, in order to support the external builders in v2.0, we had to push the build process to install, so as to make the chaincode indices work properly.).  We can either a) ignore a failed build, like in v1.4.x, b) emulate the new lifecycle and attempt to remove the chaincode package from the filesystem (though I think this is dangerous and racy because of the way legacy chaincode works), or c) leave things the way they are.  Thoughts?  ></body> </Action>
<Action id="69247" issue="44911" author="wlahti" type="comment" created="2020-05-15 19:11:47.0" updateauthor="wlahti" updated="2020-05-15 19:11:47.0"> <body><! CDATA Given the convoluted and far-reaching footprint of the legacy lifecycle code, the current agreement that came from a conversation between  ~jyellick ,  ~denyeart ,  ~varadatibm , and myself is that the only safe thing we can do in terms of this issue is update the error message (only for legacy lifecycle-installed chaincodes) to (new portion of message in red):  Error: install failed with status: 500 - {color:#FF0000}chaincode installed to peer but {color}could not build chaincode: docker build failed: docker image build failed: docker build failed: Failed to pull hyperledger/fabric-ccenv:latest2: API error (404): manifest for hyperledger/fabric-ccenv:latest2 not found: manifest unknown: manifest unknown  ></body> </Action>
<Action id="69248" issue="44911" author="wlahti" type="comment" body="I will split the new lifecycle change to ignore cached failed builds when a user retries a failed install, assuming they must know something about why the build might succeed on retry. " created="2020-05-15 19:13:17.0" updateauthor="wlahti" updated="2020-05-15 19:13:17.0"/>
<Action id="69249" issue="44911" author="wlahti" type="comment" created="2020-05-15 20:18:37.0" updateauthor="wlahti" updated="2020-05-16 15:25:18.0"> <body><! CDATA For the legacy lifecycle (LSCC) error message:   master:  https://github.com/hyperledger/fabric/pull/1276   release-2.1:  https://github.com/hyperledger/fabric/pull/1281   ></body> </Action>
