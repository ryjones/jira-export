<Action id="35998" issue="24838" author="sanchezl" type="comment" created="2017-12-07 17:49:36.0" updateauthor="sanchezl" updated="2017-12-07 18:08:37.0"> <body><! CDATA  ~rhegde , please provide: # the output of {{docker inspect}} for the containers involved # an overview of the networking configuration (e.g. docker networks or host networking? # I see that you can ping kafka from orderer container (please confirm that is what I'm seeing in the logs), can you try accessing the kafka ports themselves, e.g.: {{$ cat < /dev/tcp/kafka01.clsorder.sitnf.clsnet/9092}}  (success if you don't get Connection refused message).  I suspect the docker networking (specifically its iptables config) has somehow gotten into a bad state. You can use commands like {{route}}, and  {{iptables --list}}  on the host machines to debug (provide this output also if you can). If there is a problem in this area, re-starting the docker daemons usually cleans the networking configuration up.  ></body> </Action>
<Action id="36001" issue="24838" author="rhegde" type="comment" created="2017-12-07 22:18:46.0" updateauthor="rhegde" updated="2017-12-07 22:18:46.0"> <body><! CDATA # I will get back with the docker inspect for OSN containers + network. This will take some time as this is a restricted/controlled environment at our place. # This is Docker Swarm network. OSN is hosted on a single host, where as there are other peer nodes distributed on 2 different host systems. # Right - Kafka containers are accessible, this is proved with the ping snapshot. And at the same time there is Producer and Parent Consumer connection getting established however it is the Partition Consumer channel that doesn't go through. We also ran a netstat on orderer container to see multiple connections established on port 9092 by orderer with kafkas. We will provide a snapshot of netstat too.  The only exception to the above is the public channel which is allowed to create Partition Consumer channel connection.  We even tried restarting docker-daemon of the OSN hosting node only (before filing JIRA), this didn't help.  ></body> </Action>
<Action id="36016" issue="24838" author="sanchezl" type="comment" created="2017-12-08 15:19:34.0" updateauthor="sanchezl" updated="2017-12-08 15:19:34.0"> <body><! CDATA The Kafka logs show a number of attempts to access a partition from the wrong broker: {noformat}  2017-12-06 13:46:53,026  ERROR  ReplicaFetcherThread-0-2 , Error for partition  cls2obo,0  to broker 2:org.apache.kafka.common.errors.UnknownTopicOrPartitionException: This server does not host this topic-partition. (kafka.server.ReplicaFetcherThread){noformat} {noformat}  2017-12-06 13:51:32,674  ERROR  ReplicaFetcherThread-0-3 , Error for partition  testchainid,0  to broker 3:org.apache.kafka.common.errors.NotLeaderForPartitionException: This server is not the leader for that topic-partition. (kafka.server.ReplicaFetcherThread){noformat} This would explain the {{Consumer}} (a.k.a. parent) connection succeeding, but not being able to create a {{PartitionConsumer}}.  This type of error would indicate that the kafka metadata is out of sync. So you will need to figure out if this is a result of problems with: * Kafka-kafka communication * Kafta-Zookeeper communication * Zookeeper-Zookeeper communication.  Another area to look into is the persistence in your docker swarm network, for example if one of the zookeeper services was re-stared, did the replacement service pick up the same volume mapped to $ZOO_DATA_DIR and possibly $ZOO_DATA_LOG_DIR ? A problem here could explain the initial occurrence of the problem ("out-of-nowhere").   ></body> </Action>
<Action id="36087" issue="24838" author="rhegde" type="comment" created="2017-12-10 23:24:12.0" updateauthor="rhegde" updated="2017-12-10 23:24:12.0"> <body><! CDATA We are getting this problem on another dev-environment now, this started with following message from orderer. All 3 orderers went down with below panic message  *Snippet from Orderer (s)*  2017-12-09 01:21:46.304 UTC  orderer/kafka  startThread -> CRIT 94d9  channel: testchainid  Cannot set up channel consumer = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition. panic:  channel: testchainid  Cannot set up channel consumer = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.  goroutine 65  running : panic(0xa7f820, 0xc4221037b0)         /opt/go/src/runtime/panic.go:500 +0x1a1 github.com/hyperledger/fabric/vendor/github.com/op/go-logging.(*Logger).Panicf(0xc4201c6600, 0xbc08c3, 0x31, 0xc421e7f1e0, 0x2, 0x2)         /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/op/go-logging/logger.go:194 +0x127 github.com/hyperledger/fabric/orderer/kafka.startThread(0xc42156d7a0)         /opt/gopath/src/github.com/hyperledger/fabric/orderer/kafka/chain.go:173 +0xafb created by github.com/hyperledger/fabric/orderer/kafka.(*chainImpl).Start         /opt/gopath/src/github.com/hyperledger/fabric/orderer/kafka/chain.go:94 +0x3f     *All 4 Kafka Logs* (they have same print during the same time period - doesn't show any problem)   2017-12-09 01:31:56,841  INFO  Group Metadata Manager on Broker 3 : Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.GroupMetadataManager)  2017-12-09 01:41:56,841  INFO  Group Metadata Manager on Broker 3 : Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.GroupMetadataManager)  2017-12-09 01:51:56,841  INFO  Group Metadata Manager on Broker 3 : Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.GroupMetadataManager)     *Log after Orderer Restart*  2017-12-10 23:19:26.628 UTC  orderer/kafka  try -> DEBU 981  channel: testchainid  Connecting to the Kafka cluster  ></body> </Action>
<Action id="36400" issue="24838" author="sanchezl" type="comment" created="2017-12-11 19:52:25.0" updateauthor="sanchezl" updated="2017-12-11 19:52:25.0"> <body><! CDATA  ~rhegde , this message seems to be a separate issue: {code:java} panic:  channel: testchainid  Cannot set up channel consumer = kafka server: The requested offset is outside the range of offsets maintained by the server for the given topic/partition.{code} For this check the logs for a previous message of the form: {code:java} " channel: %s  Starting chain with last persisted offset %d and last recorded block %d"{code}  ></body> </Action>
<Action id="36405" issue="24838" author="sanchezl" type="comment" created="2017-12-11 20:07:59.0" updateauthor="sanchezl" updated="2017-12-11 20:07:59.0"> <body><! CDATA After spending some time with the CLS team on a remote debug session, the main issue appears to be that the nodes in the Kafka/Zookeeper cluster do not seem to agree on the state of the various topics.  The error messages I pointed out in a previous comment come from the Kafka replication thread. The broker is trying to contact the leader for the partitions, in order to bring itself into sync, but the broker that it believes is the leader does not believe it is the leader (or even contain some of the partitions at all!). I'm not sure how this disconnect between the brokers and their meta-data occurred, but it has not self corrected after numerous restarts. The orderer will not function until this is resolved.  The brokers and zookeeper nodes are being run on a Docker swarm network using overlay networks. I witnessed some inconsistencies in the ip addresses of the zookeeper nodes when trying to access them from the Kafka nodes: The zookeeper nodes had ip address _xxx.xxx.xxx._5, _xxx.xxx.xxx._7, and _xxx.xxx.xxx._8, but the zookeeper hostnames did not resolve to any of those addresses. I found this disconcerting, but I don't know if this is expected when using docker swarm mode and overlay networks.  I don't think there is anything else we can do from a Fabric/Orderer perspective until the Kafka cluster is fixed. Fixing the Kafka cluster might require the assistance of Docker (for swarm support) and/or Confluence (for Kafka support).  ></body> </Action>
<Action id="37784" issue="24838" author="rhegde" type="comment" created="2017-12-25 22:41:27.0" updateauthor="rhegde" updated="2017-12-25 22:41:27.0"> <body><! CDATA Further investigation -  Kafka setup is run by not disabling time based pruning, causing the log segment to prune. This has caused the orderer node to have a offset which is already pruned by Kafka and result in a  continuous attempt to connect to the consumer channel (kafka cluster) using short/long interval setting and eventually give-up with the above error.  Upon discussion on RC with  ~jyellick , this seems not repairable using a normal operational procedure means.  Kafka time based pruning needs to be disabled by setting KAFKA_LOG_RETENTION_MS=-1.  ></body> </Action>
<Action id="37797" issue="24838" author="kchristidis" type="comment" created="2017-12-26 15:05:41.0" updateauthor="kchristidis" updated="2017-12-26 15:05:41.0"> <body><! CDATA {quote}Kafka time based pruning needs to be disabled by setting KAFKA_LOG_RETENTION_MS=-1. {quote}  This is _precisely_ what our documentation dictates, no?  https://hyperledger-fabric.readthedocs.io/en/latest/kafka.html#steps  Step 6, substep 5:  {quote}log.retention.ms = -1. Until the ordering service adds support for pruning of the Kafka logs, you should disable time-based retention and prevent segments from expiring. (Size-based retention —see log.retention.bytes— is disabled by default in Kafka at the time of this writing, so there’s no need to set it explicitly.) {quote}  ></body> </Action>
<Action id="37798" issue="24838" author="rhegde" type="comment" created="2017-12-26 15:29:20.0" updateauthor="rhegde" updated="2017-12-26 15:29:20.0"> <body><! CDATA True - this is part of documentation.   I would suggest until pruning is supported and to avoid any other users ending up in the same situation, can we make this variable as part of Kafka container environment variable setup for e2e-example which is  the starting reference for any fabric user (https://github.com/hyperledger/fabric/blob/release-1.1/examples/e2e_cli/base/docker-compose-base.yaml#L17).  ></body> </Action>
