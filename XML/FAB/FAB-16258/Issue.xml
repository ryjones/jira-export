<Issue id="41703" key="FAB-16258" number="16258" project="10002" reporter="scottz" creator="scottz" type="10101" summary="TC: Chaos testing: restart all components peers and raft, single and mult hits, mult channels, concurrency" priority="2" resolution="10001" status="6" created="2019-08-08 15:36:33.0" updated="2021-01-10 17:33:46.0" resolutiondate="2021-01-10 17:33:46.0" votes="0" watches="6" workflowId="54737"> <description><! CDATA This testtask was created as a clone of FAB-14420, which was the v1.4.2 testtask. The implementation in v2.0 is expected to be different than the steps below which were used in 1.4.2. In v2.0, we plan to use the new k8s operator/test framework to build functions to do these restart tests.  chaos: restarts peers/orderers, singletons and multiples. Expect some lost TXs. Without servdisc, expect more TX lost.  At end, must confirm all peers/orderers same height.  It would be good to have a variety of traffic: samplecc, privatedata, some using service discovery. Overlay this restarts script with some of the other traffic testcases.  This will be similar to 10191, 12834, 12835, etc. h2. Setup * Create a fabric network with 3 channels, 3 raft orderers, 2 orgs with 2 peer per org. * Set up the k8s cluster: set servicetype = LoadBalancer to allow bouncing the orderers. * Use our usual settings for batchsize = 500 , batchtimeout = 2s, and SnapshotInterval = 100 MB * Use fabric-test/tools/PTE/scripts/gen_cfgInputs.sh to create traffic generation files. * Do NOT use failover mode in PTE. We want the same clients to continue sending to the same orderers as initially selected. This means there will be multiple failures, but it also means that we will still be sending transactions to ALL the orderers at the end of the test after they have all recovered.  h2. Input # * Send transactions using multiple threads, with PTE settings for TargetPeers=OrgAnchor (to use one peer in every org), and TargetOrderers=RoundRobin (to use all the orderers, assuming there are at least that many total threads). # * To balance load among the peers as well as among the orderers, choose a number of threads to be Lowest Common Multiple of #orgs X #orderers, for each channel. In our case that means choose 6 per channel (this means 3 for peer1org1 and 3 for peer1org2, for each channel), 9 total per peer, and 6 per orderer. # * Use PTE in Constant Mode (to send the next transaction as soon as the TX response ACK for the previous one was received), to provide multiple transactions in flight simultaneously, for every thread. # * Start sending transactions for two hours. # * After a few minutes, start the chaos-kube tool to restart a random orderer every 2 minutes, for 90 minutes. # * Feb 27: modify: restart random orderer every 2 minutes, 2 orderers every 6 minutes, 3 orderers every 12 mins.  h2. Expectations * PTE tool does not resend transactions upon receiving NACK, nor when TXs are lost. Therefore there will be transaction failures (unreceived) in the pteReport counters. There will also be failures for the NACKs when sending transactions to an orderer that is stopped, and when sending to an orderer while the leader is stopped while the others are trying to elect a new leader. * Since the recovery of a single orderer is expected to take less than 20 seconds, we should never see a situation where consensus quorum is halted, because we are waiting 2 minutes before stopping the next one. * At the end of the test, all the orderers should be up, functioning well, processing transactions and blocks, and end with the same block height in all the channels. (We could look at the metrics to possibly help with this validation, or else connect a consumer to each orderer to retrieve the last block on each channel.)  Â   ></description> </Issue>
