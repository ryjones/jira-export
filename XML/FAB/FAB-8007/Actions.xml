<Action id="39484" issue="27302" author="denyeart" type="comment" created="2018-02-01 12:23:14.0" updateauthor="denyeart" updated="2018-02-01 12:23:14.0"> <body><! CDATA  ~yacovm  I agree controlling the size is fairly urgent.  Are you suggesting we do both - reference identities AND compress over every K blocks? What is the expected savings if we do both vs each one individually?  Is the client certificate really required twice?  Could we do the identity referencing also for the orderer certificate from the block signature?  ></body> </Action>
<Action id="39486" issue="27302" author="yacovm" type="comment" created="2018-02-01 12:33:22.0" updateauthor="yacovm" updated="2018-02-01 12:46:10.0"> <body><! CDATA I am suggesting to do both, yes. {quote} What is the expected savings if we do both vs each one individually?{quote} I can check if you want. My intuition says that if as K grows, the compression rate is improved, but we don't want to increase K too much because if peers do state transfer between themselves, or just any API call that fetches the entire block from the disk, this would mean to un-compress lots of data only for fetching a single block. Perhaps, what we should do is not compress every K blocks but rather every D cumulative bytes over a sequence of blocks.  I argue that doing referencing alone gives an advantage that compressing every K blocks in the ledger doesn't give - it reduces the size of the transactions and blocks before they end up in the peers, and thus reduce the bytes sent over the wire, which increases the effective bandwidth of an OSN.  {quote}Is the client certificate really required twice?{quote} Well, that's how the code is written... the *TransactionAction.Header* field, is the *SignatureHeader* of the original proposal header and it is checked in validation in the peer in *validateEndorserTransaction*. I'd advise that we don't change this, to be backward compatible with old peer versions.  {quote}Could we do the identity referencing also for the orderer certificate from the block signature?{quote} I guess we could.   ></body> </Action>
<Action id="39490" issue="27302" author="denyeart" type="comment" created="2018-02-01 13:27:49.0" updateauthor="denyeart" updated="2018-02-01 13:27:49.0"> <body><! CDATA I agree that identity referencing would be the most valuable, as you say that would give instant benefit over the wire and on disk.  Concerning the compression, note that there are already K blocks per block storage file, so we could compress at the block storage file level, and implicitly get compression over K blocks.  Currently block storage file is hardcoded to be 64MB:   https://github.com/hyperledger/fabric/blob/release/common/ledger/blkstorage/fsblkstorage/config.go#L26    https://github.com/hyperledger/fabric/blob/release/core/ledger/ledgerconfig/ledger_config.go#L64   We could punt on the compression in Fabric and recommend hosting services do disk compression, giving us compression over the K blocks in each block storage file for free.     ></body> </Action>
<Action id="39491" issue="27302" author="yacovm" type="comment" created="2018-02-01 13:35:39.0" updateauthor="yacovm" updated="2018-02-01 13:41:17.0"> <body><! CDATA 64 MB sounds to me like a bit too much to be a compression unit. I haven't delved into low-level compression APIs of golang standard library, but unless there is a way of extracting a single block from that 64MB compressed blob into the memory, that means to fetch a single 100K block from the disk, we need to expand 64MB in memory just for that.   {quote}We could punt on the compression in Fabric and recommend hosting services do disk compression{quote} While that's doable, I would recommend we also implement compression ourselves, because the average user doesn't have such capabilities. We could make it toggle-able at the peer level, and if the storage service provider has compression, then the peer admin can just disable the compression in the peer itself.  wdyt?  ></body> </Action>
<Action id="39496" issue="27302" author="denyeart" type="comment" created="2018-02-01 13:44:58.0" updateauthor="denyeart" updated="2018-02-01 13:44:58.0"> <body><! CDATA We probably should make the 64 MB size configurable.  I'm just saying that there is an opportunity to sync up file size and compression threshold to keep things simpler. If we move forward with this we could set default block storage file size with compression optimization in mind, given a representative read/write workload.  Let's get some more insight from service providers such as  ~ptippett  .  ></body> </Action>
<Action id="39601" issue="27302" author="mastersingh24" type="comment" created="2018-02-02 18:07:25.0" updateauthor="mastersingh24" updated="2018-02-02 18:07:25.0"> <body><! CDATA I like the store and reference idea and seems like it should work well. As for compression, it could be a background task.  There are also filesystems that do compression  ></body> </Action>
<Action id="39651" issue="27302" author="angelo.decaro" type="comment" created="2018-02-05 09:25:22.0" updateauthor="angelo.decaro" updated="2018-02-05 09:39:50.0"> <body><! CDATA  ~yacovm , this is very interesting. The modification to the MSP would be minimal. Actually, one can modify just cachedMSP to do this additional work. We can introduce a generic interface to allow cachedMSP to get the serialised identity given the hash of it and the work is done :) Anyway, I would be glad to give support :)  ></body> </Action>
<Action id="39652" issue="27302" author="yacovm" type="comment" created="2018-02-05 09:32:31.0" updateauthor="yacovm" updated="2018-02-05 09:39:41.0"> <body><! CDATA I don't think this should be driven via the MSP cache. What if the cache is empty when the peer boots? also - the MSP cache is per channel, it has redundancy as channel number grows.   If anything we should have a cache at the ledger/DB level and not use the cache for the MSP for this. I will soon also post a JIRA where I propose to have a cache layer in the peer so we won't fetch values for keys over the wire like in couchDB.   ></body> </Action>
<Action id="39813" issue="27302" author="jyellick" type="comment" created="2018-02-07 19:45:39.0" updateauthor="jyellick" updated="2018-02-07 19:45:39.0"> <body><! CDATA I think I am not fully grasping the proposal, despite reading the description a few times.  I'll try to summarize my understanding:  Whenever the peer sees a full certificate in the validation path, it automatically creates a record for that cert in a 'certificate reference table' or similar in the state database.  Because validation must execute deterministically on peers, we can conclude that at validation time, all peers have the same 'certificate reference table', so all peers will come to agreement as to whether a certificate's short form *idRef* is resolvable to an actual identity.  Is this summary accurate?  The endorsement phase can replace the embedded certificates, including its own in the endorsement, with an *idRef* assuming that the certificate is found in the statedb.  Would you include this reference into the statedb as a part of the read set?  > This would reduce the size of a single identity to 32 bytes, and consequently - the total average size of a 1-endorsement transaction to less than 1KB.  Based on the above statement, I assume that you do _not_ intend that the identity in the outermost signature header be replaced with an *idRef*, leaving you with ~700 bytes for the submitter's identity, 32 bytes for the one embedded inside the tx, and 32 bytes for the endorser's identity.  The reason why I note this, is from an orderer perspective, there is of course no statedb, and we would not want to introduce it as a dependency.  From an orderer perspective, so long as this outermost identity is left intact, optimizations inside the endorser-tx are welcomed.  The orderer similarly cannot assume that its identity is known to the peers, so it would always need to include its identity in the signature of each block.  My primary concern about this scheme overall is that replacing identities with hashes adds another layer of opacity to the system.  In order to allow clients to fully understand transactions, would you propose adding some new service which allows a client to map hashes back to identities?  At a broader level, I suspect that for 'serious deployment', putting the block storage onto a filesystem which supports compression is going to be a good idea regardless.  I'd be very curious to see what the performance of such a configuration would be.  I would also be curious to see some performance numbers which back up the notion that we are IO bound or CPU bound.  If we are not IO bound  then the filesystem compression seems like it would provide 90+% of the value by making filesystem usage lower, would work with Kafka brokers too, all while adding zero additional complexity to fabric.  Also, have we investigated simply using compression on the HTTP2 body? Since we are using gRPC, this also seems like a very low work, potentially high reward option.  ></body> </Action>
<Action id="39818" issue="27302" author="yacovm" type="comment" created="2018-02-07 20:32:27.0" updateauthor="yacovm" updated="2018-02-08 08:48:29.0"> <body><! CDATA {quote}Whenever the peer sees a full certificate in the validation path, it automatically creates a record for that cert in a 'certificate reference table' or similar in the state database. Because validation must execute deterministically on peers, we can conclude that at validation time, all peers have the same 'certificate reference table', so all peers will come to agreement as to whether a certificate's short form idRef is resolvable to an actual identity. Is this summary accurate?{quote} Yes  {quote}The endorsement phase can replace the embedded certificates, including its own in the endorsement, with an idRef assuming that the certificate is found in the statedb. Would you include this reference into the statedb as a part of the read set?{quote} No, there is no change in the read set or anything of the sort. The peer just checks the stateDB if it sees a reference instead of the full identity. If the peer sees a full identity, and sees there is no reference in the stateDB, it adds one. {quote} Based on the above statement, I assume that you do not intend that the identity in the outermost signature header be replaced with an idRef, leaving you with ~700 bytes for the submitter's identity, 32 bytes for the one embedded inside the tx, and 32 bytes for the endorser's identity.{quote} No, I intend to do that exactly. I guess you're concerned for the orderer, right?  So, I think the client can just attach the preimage of the idRef to a new field in the envelope, which is not the signature and not the payload that is signed, and the orderer can verify using a hash that it matches, and use that "attachable" identity to verify the signature. {quote} The reason why I note this, is from an orderer perspective, there is of course no statedb, and we would not want to introduce it as a dependency. From an orderer perspective, so long as this outermost identity is left intact, optimizations inside the endorser-tx are welcomed. The orderer similarly cannot assume that its identity is known to the peers, so it would always need to include its identity in the signature of each block.{quote} Perhaps I should read the entire comment instead of replying per block, but it's too late now. See above.  {quote}My primary concern about this scheme overall is that replacing identities with hashes adds another layer of opacity to the system. In order to allow clients to fully understand transactions, would you propose adding some new service which allows a client to map hashes back to identities?{quote}  Why would clients understand transactions? What do you mean by that? Which identities does a client need to understand?  {quote}I would also be curious to see some performance numbers which back up the notion that we are IO bound or CPU bound. If we are not IO bound then the filesystem compression seems like it would provide 90+% of the value by making filesystem usage lower, would work with Kafka brokers too, all while adding zero additional complexity to fabric.{quote} I don't really understand what you mean here. An orderer node has a limited bandwidth. That bandwidth, dictates how many peers as a function of transaction throughput that orderer can deliver, no? If you deliver blocks at a rate of X Mbps to 10 peers, then you deliver 10X Mbps of data. It has nothing to do with how fast these 10 peers process these blocks, because the "free factor" here is the amount of peers that pull from that orderer node. If you decrease the data you send, you increase the amount of peers it can talk to without reducing its throughput.  {quote}Also, have we investigated simply using compression on the HTTP2 body? Since we are using gRPC, this also seems like a very low work, potentially high reward option.{quote} Doesn't compression before encryption harm security? I'm not an expert on this, but my intuition says it does, because the amount of data that is encrypted is smaller, and also - when you compress stuff, you create a bias in the length of the payload and this is in a way- more information to an attacker, no? Perhaps  ~elli-androulaki   or  ~ales  or  ~angelo.decaro  can chime in here?  Now, putting security aside- from experiments I made, when you compress a block that has only 1 transaction, it compresses to only half the size, but if you just put the hashes - a transaction is compressed to a third of the size, without even counting the identity of the orderer. It might be that in a high throughput environment, the cumulative repetitions of the transactions makes compression cost-effective as the hashing, but if the throughput is low - it is not the case.   {quote} At a broader level, I suspect that for 'serious deployment', putting the block storage onto a filesystem which supports compression is going to be a good idea regardless. I'd be very curious to see what the performance of such a configuration would be.{quote}  Well, but: # You can do this regardless of the solution of the identity referencing, no? Won't that be a cumulative effect? Or, do you think that the compression would end up with the same space capacity regardless if we reference identities or not? # What about the users that don't have this capability? I've seen users complain on rocket-chat that their disk space runs out very, very fast.  Since this is not a project that is aimed only for high-end customers like financial institutions and enterprises, I think it makes sense to try and reduce the technical requirements of deploying Fabric with success. I think it makes sense this feature will be configurable by the clients and peers, and if we have a serious deployment with a lot of disk space and / or effective compression at the file system level, they could just turn this off, no?     ></body> </Action>
<Action id="39838" issue="27302" author="luomin" type="comment" body="No changes were made above. It is a misoperation. Sorry" created="2018-02-08 07:10:30.0" updateauthor="luomin" updated="2018-02-08 07:10:30.0"/>
<Action id="39846" issue="27302" author="elli-androulaki" type="comment" created="2018-02-08 12:32:17.0" updateauthor="elli-androulaki" updated="2018-02-08 12:32:17.0"> <body><! CDATA Hi, so i agree that in principal one could do this sort of identity deduplication (also a from of compression) at the ledger side, starting from the transaction submission stage.   If you know that your identity has been used in some other transaction in the past, one could provide a reference to it in its transactions to follow, forcing the fabric entities that play the role of "verifier" to retrieve the full identity from the ledger prior to starting the validation. Of course, the devil is in the details :)  Regarding the point that extra logic would need to be added to the orderers (besides the committing peers), one could say that at a first stage, client-certificates are attached in whole, and this identity-by-reference inclusion in transactions applies for endorser identities. Given endorser identities are more than the creator single identity, we may still benefit, no? In the future, one could leverage threshold signatures on the endorsement side, to make the size of endorsedAction even smaller.   Exception is the utxo type of transactions that are likely to include only client signatures...   Another point is that when privacy preserving client or endorser identities are in place, such a deduplication is not possible to be applied.     -----   > Doesn't compression before encryption harm security? I'm not an expert on this, but my intuition says it does, because the amount of data that is encrypted is smaller, and also - when you compress stuff, you create a bias in the length of the payload and this is in a way- more information to an attacker, no?  I guess it depends on the setting, i.e., who you consider to be your attacker (neighboring peer, orderer) and what are its powers (e.g., can it see the packages exchanged across peers?), what is the type of messages you are to encrypt. If the attacker is given a ciphertext that is assumed to be the encryption of compressed version of one of two messages whose compressed version exhibits different sizes, then of course the attacker can gain some advantage into distinguishing the encrypted message. But if the compression ratios of are the same for all equal size exchanged messages, then performing compression at the single message side would not  leak more than the amount of information exchanged among parties (that is anyway leaked, no?).   On the other hand if compression takes place considering the respective ledger (i.e., deduplication at the identity level) and the adversary eavesdrops a communication among two parties from a different channel, it may infer that the two parties exchanged a transaction that includes only one certificate, increasing the chances (from the adversarial perspective) that the transaction refers to identities that have already appeared in the respective channel.        ></body> </Action>
<Action id="39847" issue="27302" author="yacovm" type="comment" created="2018-02-08 12:46:04.0" updateauthor="yacovm" updated="2018-02-08 12:46:04.0"> <body><! CDATA  ~elli-androulaki , the attacker I had in mind in the story is not a member of the blockchain, but someone that can route all the traffic through itself via ip layer hijacking, and it is not supposed to see the encrypted TLS traffic.    ></body> </Action>
<Action id="39864" issue="27302" author="jyellick" type="comment" created="2018-02-08 15:47:04.0" updateauthor="jyellick" updated="2018-02-08 15:47:04.0"> <body><! CDATA {quote}No, I intend to do that exactly. I guess you're concerned for the orderer, right?  So, I think the client can just attach the preimage of the idRef to a new field that is not part of the envelope, and the orderer can verify using a hash that it matches, and use that "attachable" identity to verify the signature. {quote} If the transaction still comes with the identity attached as well as the hash, it seems like that is defeating the purpose? From an ordering perspective, we will have to order both of these (so that other nodes can validate the transaction), so this would, at least for the orderer add complexity without any performance value. Of course having the rest of the payload be smaller would help. And of course the blocks would be smaller in size which could help the Deliver service somewhat.  This still raises questions to me. What about a client wishing to anonymously DoS the chain? They could send unlimited transactions, leaving only their certificate hash in the blockchain. So long as they never send a successful transaction, there will be no obvious way to map the transaction back to a user. {quote}I don't really understand what you mean here. An orderer node has a limited bandwidth. That bandwidth, dictates how many peers as a function of transaction throughput that orderer can deliver, no? If you deliver blocks at a rate of X Mbps to 10 peers, then you deliver 10X Mbps of data. It has nothing to do with how fast these 10 peers process these blocks, because the "free factor" here is the amount of peers that pull from that orderer node. If you decrease the data you send, you increase the amount of peers it can talk to without reducing its throughput. {quote} Yes,the orderer has a finite amount of available bandwidth. However, bandwidth is not the only finite resource. CPU is also a finite resource, as is memory, as is disk. If orderers were _only_ doing Deliver, then I agree, it is going to be IO bound, but orderers also do Broadcast, which limits the rate at which transactions may pass through the system, and is largely CPU bound.  If the orderer cannot validate transactions (CPU bound) fast enough to produce enough blocks to saturate the downlink of the Deliver service (Network Bound). Then reducing the transaction size will not increase throughput. Similarly, if the peer cannot validate blocks fast enough (CPU bound) that it pulls blocks from the orderer to saturate the downlink, then reducing transaction size will similarly not reduce throughput.  Yes, the orderers and peers should scale horizontally, but before we start optimizing the transaction size on the network as a bottleneck, I'd like to see that it is indeed the transaction size that is the bottleneck. Anecdotally, I have heard complaints about peers lagging far behind the orderer in terms of block height, because they cannot validate transactions as fast as the orderer puts them in the blockchain. Hopefully the parallel validation in v1.1 will alleviate that, but I'm still wary of making a structural change to our messages to optimize for something which has not been demonstrated to be a real world problem. {quote}Doesn't compression before encryption harm security? I'm not an expert on this, but my intuition says it does, because the amount of data that is encrypted is smaller, and also - when you compress stuff, you create a bias in the length of the payload and this is in a way- more information to an attacker, no? Perhaps Alessandro Sorniotti or Angelo De Caro can chime in here? {quote} I agree, a security expert is needed here, but I wonder whether it's leaking any information which isn't already there. The transaction length is already visible, and is strongly correlated with number of signers. {quote}Now, putting security aside- from experiments I made, when you compress a block that has only 1 transaction, it compresses to only half the size, but if you just put the hashes - a transaction is compressed to a third of the size, without even counting the identity of the orderer. It might be that in a high throughput environment, the cumulative repetitions of the transactions makes compression cost-effective as the hashing, but if the throughput is low - it is not the case. {quote} I agree, clearly, the hash is going to be the superior solution from a bandwidth perspective. But we need to agree what problem we are trying to solve. Do things take up too much room on disk? Is our throughput too low because the network is clogged? The hash clearly solves many or all of these problems, but if it is solving ones which do not actually exist, and we could solve the ones which do with no development effort, then all the better. {quote}Well, but: 1. You can do this regardless of the solution of the identity referencing, no? Won't that be a cumulative effect? Or, do you think that the compression would end up with the same space capacity regardless if we reference identities or not? 2. What about the users that don't have this capability? I've seen users complain on rocket-chat that their disk space runs out very, very fast. Since this is not a project that is aimed only for high-end customers like financial institutions and enterprises, I think it makes sense to try and reduce the technical requirements of deploying Fabric with success. {quote} # I would expect that the affect would be muted after the referencing, though obviously, the only way to be sure would be to test. # This is what I was looking for. The issue as written discusses the "what" but not so much the "why". What would you think of creating default checkpoint/pruning rules for these users? (Even with compression, users will still face disk exhaustion issues).  ></body> </Action>
<Action id="39865" issue="27302" author="yacovm" type="comment" created="2018-02-08 16:22:48.0" updateauthor="yacovm" updated="2018-02-08 16:29:13.0"> <body><! CDATA {quote}If the transaction still comes with the identity attached as well as the hash, it seems like that is defeating the purpose? From an ordering perspective, we will have to order both of these (so that other nodes can validate the transaction),{quote}  So, the orderer would not put it into the block, but would send it to other nodes.  I'm talking about the bandwidth downstream - from the orderers to the peers.   {quote}This still raises questions to me. What about a client wishing to anonymously DoS the chain? They could send unlimited transactions, leaving only their certificate hash in the blockchain. So long as they never send a successful transaction, there will be no obvious way to map the transaction back to a user.{quote} If the orderer includes the transaction in the block, it means it came attached with a valid identity of the client, no? Then it's not anonymous.    {quote}Yes,the orderer has a finite amount of available bandwidth. However, bandwidth is not the only finite resource. CPU is also a finite resource, as is memory, as is disk. If orderers were only doing Deliver, then I agree, it is going to be IO bound, but orderers also do Broadcast, which limits the rate at which transactions may pass through the system, and is largely CPU bound. If the orderer cannot validate transactions (CPU bound) fast enough to produce enough blocks to saturate the downlink of the Deliver service (Network Bound). Then reducing the transaction size will not increase throughput. Similarly, if the peer cannot validate blocks fast enough (CPU bound) that it pulls blocks from the orderer to saturate the downlink, then reducing transaction size will similarly not reduce throughput. {quote} Right, but assuming peers are somewhat uniform in their processing power, and the transaction rate that goes into broadcast is constant - as the peer number scales, the bandwidth becomes the bottleneck for deliver, simply because all you do is add more peers.   {quote}Anecdotally, I have heard complaints about peers lagging far behind the orderer in terms of block height, because they cannot validate transactions as fast as the orderer puts them in the blockchain.{quote} That's just because no one runs a deployment with more than a couple of peers.   {quote} I'm still wary of making a structural change to our messages to optimize for something which has not been demonstrated to be a real world problem.{quote} {quote}The hash clearly solves many or all of these problems, but if it is solving ones which do not actually exist, and we could solve the ones which do with no development effort, then all the better.{quote} This makes sense - if this isn't a cost-effective change, then we shouldn't do it.  {quote}What would you think of creating default checkpoint/pruning rules for these users? (Even with compression, users will still face disk exhaustion issues).{quote} Right, I agree that if we have auto-pruning then disk space isn't a problem.    To sum up, I guess it might be indeed a little too early to introduce such a change in fabric, since most deployments are tiny and will probably be in the future as well. So, I'm removing the version label for now.   ></body> </Action>
<Action id="39868" issue="27302" author="jyellick" type="comment" created="2018-02-08 17:30:34.0" updateauthor="jyellick" updated="2018-02-08 17:30:34.0"> <body><! CDATA {quote} If the orderer includes the transaction in the block, it means it came attached with a valid identity of the client, no? Then it's not anonymous. {quote}  The scenario I am thinking is this:  # CA issues identity "Foo", along with a million others.  # Owner of "Foo" deliberately does _not_ ever successfully transact. # Owner of "Foo" creates a tx with the creator as a hash, and the attached identity so that the orderer can validate, the payload data is garbage # Orderer verifies that the hash in the header corresponds to the attached "Foo" identity, and allows the tx through # Orderer strips out the attached identity, leaving only the hash (because otherwise, we are not actually reducing the message size) # Peer receives a tx which has only a hashed identity (which it has never observed), and a garbage data payload  So, yes, the orderer knows the identity and that the identity is valid, but it doesn't know the tx is garbage.  The peer knows the tx is garbage, but it does not know the associated identity.  How can the peers figure out which identity is doing this? The only evidence on the chain is an unresolvable hash of identity.  I assume the peer guys would have to go ask the ordering admins to somehow capture the tx with the attached identity before it has been stripped out, or we would have to allow blacklisting of identities by hash, or maintain a map at the CA from hash to cert.  It's certainly solvable, but it was meant only as an illustration of some of the additional complexity that could cascade in.  ></body> </Action>
<Action id="39873" issue="27302" author="yacovm" type="comment" created="2018-02-08 18:37:46.0" updateauthor="yacovm" updated="2018-02-08 18:42:50.0"> <body><! CDATA I think any client can send a transaction anonymously by simply getting his hands on a transaction of some other user via calling deliver on the orderer, and then sending that transaction over and over again, and it will be included in the block, no? The orderer doesn't check the txID and doesn't do any time checks.   {quote} I assume the peer guys would have to go ask the ordering admins to somehow capture the tx with the attached identity before it has been stripped out, or we would have to allow blacklisting of identities by hash, or maintain a map at the CA from hash to cert. It's certainly solvable, but it was meant only as an illustration of some of the additional complexity that could cascade in.{quote}  This might be resolved when we have CRLS for TLS.   ></body> </Action>
<Action id="60361" issue="27302" author="samodergec" type="comment" created="2019-05-24 14:32:25.0" updateauthor="samodergec" updated="2019-05-24 14:32:25.0"> <body><! CDATA Hey  ~yacovm   ~mastersingh24   ~denyeart   ~jyellick  , We are currently looking into use case where endorsement policy will contains about 10 organisations. Consequently that means each transaction will contains x.509 certificate with is approximately 0.7K Kbytes in size, therefore results in overhead of 7Kbytes per single transaction. Given the estimated load of thousands of transactions implies significant burden in terms of space. Current absence of ledger truncate capabilities makes this problem even more acute. The raised concern in this JIRA is legit and we wondering what is the current status and whenever any progress has been done so far? We would like to collaborate on this topic and see what could be done to decrease overhead of keep x.509 certs on disk.   ></body> </Action>
<Action id="71320" issue="27302" author="nkalichynskyi" type="comment" created="2021-01-28 13:03:34.0" updateauthor="nkalichynskyi" updated="2021-01-28 13:03:34.0"> <body><! CDATA Hi,   ~yacovm ,  ~mastersingh24 ,  ~denyeart ,  ~jyellick , was there any progress on this, maybe another ticket created or something? Big transaction size is still an issue.  ></body> </Action>
<Action id="71321" issue="27302" author="yacovm" type="comment" body="No" created="2021-01-28 13:58:54.0" updateauthor="yacovm" updated="2021-01-28 13:58:54.0"/>
