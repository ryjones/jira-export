<Action id="21254" issue="15142" author="scottz" type="comment" created="2017-03-01 20:38:47.0" updateauthor="scottz" updated="2017-03-01 20:38:47.0"> <body><! CDATA These logs appeared after the last kafka-broker was stopped. I expected the Service_Unavailable errors. But then after they are restarted, we see http errors (which were NOT seen when stopping the kafka-brokers). The connection between my Producer clients and the orderers should never have been broken when the kafka-brokers were restarted (which should break only the connection between orderer and kafka-broker, right?).  Broadcast error on TX 5270 (the first error for Producer-o2-c0 after 5269 ACKs); 2017-03-01 19:33:56.417952166 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE Broadcast error on TX 5461 (the first error for Producer-o1-c0 after 5460 ACKs); 2017-03-01 19:33:56.41872936 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE Broadcast error on TX 5935 (the first error for Producer-o0-c0 after 5934 ACKs); 2017-03-01 19:33:56.421273538 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE All the kafka brokers are restarted 0 1 2, 2017-03-01 19:34:16.303906105 +0000 UTC 2017/03/01 19:34:20 transport: http2Client.notifyError got notified that the client transport was broken EOF. 2017/03/01 19:34:20 transport: http2Client.notifyError got notified that the client transport was broken EOF. 2017/03/01 19:34:20 transport: http2Client.notifyError got notified that the client transport was broken EOF. 2017/03/01 19:34:20 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:34750->127.0.0.1:5005: read: connection reset by peer. 2017/03/01 19:34:20 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:34754->127.0.0.1:5005: read: connection reset by peer. 2017/03/01 19:34:20 transport: http2Client.notifyError got notified that the client transport was broken read tcp 127.0.0.1:34756->127.0.0.1:5005: read: connection reset by peer.   ></body> </Action>
<Action id="25305" issue="15142" author="kchristidis" type="comment" created="2017-06-05 04:16:12.0" updateauthor="kchristidis" updated="2017-06-05 04:16:12.0"> <body><! CDATA When reproducing this, please make sure that the "verbose" flag for Kafka logs is set to true:  https://github.com/hyperledger/fabric/blob/master/bddtests/dc-orderer-base.yml#L17   Please also attach the logs from every ordering service node used in this run.  ></body> </Action>
<Action id="25565" issue="15142" author="weeds" type="comment" body="assigned to scott until information is obtained for Kostas- Please reassign at appropriate time. Thanks!" created="2017-06-06 19:30:26.0" updateauthor="weeds" updated="2017-06-06 19:30:26.0"/>
<Action id="25585" issue="15142" author="scottz" type="comment" created="2017-06-06 22:33:39.0" updateauthor="scottz" updated="2017-06-06 22:33:39.0"> <body><! CDATA This problem was reproduced today, using 4 kafka brokers, min_isr=2.  ORD97 passes when we stop 3 of 4 KBs (and we closed FAB-2604), but still fails when docker stop all (4) of the KBs: stop 0, 1, 2, 3, sleeep 20, start 3, 2, 1, 0  Surya is rerunning, and collecting all the logs.  ></body> </Action>
<Action id="25666" issue="15142" author="kchristidis" type="comment" body="The logs are not collected at DEBUG level. Please repeat and re-attach." created="2017-06-07 16:23:56.0" updateauthor="kchristidis" updated="2017-06-07 16:23:56.0"/>
<Action id="25672" issue="15142" author="kchristidis" type="comment" created="2017-06-07 16:46:54.0" updateauthor="kchristidis" updated="2017-06-07 16:46:54.0"> <body><! CDATA BTW, this is how I'm reproducing your test and it's passing:  https://gerrit.hyperledger.org/r/#/c/10285/   But perhaps I'm missing something, let's see.  ></body> </Action>
<Action id="25757" issue="15142" author="scottz" type="comment" created="2017-06-08 15:49:49.0" updateauthor="scottz" updated="2017-06-08 15:49:49.0"> <body><! CDATA Observation: When we see the first failure after taking down the kafkabrokers, it is 503 Service Unavailable. As the broadcastclient continues to broadcast transactions, we see EOF err responses.  Observation: When we connected a NEW broadcast client after the kafkabrokers recovered, it was able to successfully send transactions which were delivered in blocks successfully. So it looks like the problem is not "orderer service does not resume", but rather the problem is "grpc connection between broadcast clients and orderer gets shut down when fewer than min_isr kafkabrokers are running and orderer service becomes unavailable"  We are wondering if the EOF we are seeing (for all but the first failed broadcast transaction) is being returned from our side of the grpc connection ClientStream being used by our broadcast client; we suspect the orderer shuts down the grpc connection from its side when the 503 error first occurred. Our remaining concern is WHY would it do that? Sure, we could enhance the broadcast client to reattempt another grpc connection automatically - but I don't see why that should be necessary.     ></body> </Action>
<Action id="25770" issue="15142" author="jyellick" type="comment" created="2017-06-08 16:34:34.0" updateauthor="jyellick" updated="2017-06-08 16:34:34.0"> <body><! CDATA > We are wondering if the EOF we are seeing (for all but the first failed broadcast transaction) is being returned from our side of the grpc connection ClientStream being used by our broadcast client;  Yes, I believe this is the case  > we suspect the orderer shuts down the grpc connection from its side when the 503 error first occurred.   Correct, the orderer closes the connection after reporting any error via {{Broadcast}} (or {{Deliver}})  > Our remaining concern is WHY would it do that?  This decision actually has some significant history attached to it, this was not always the behavior.  In brief, there was a particular bug around returning a rate limiting HTTP 429 in an earlier incarnation of the Broadcast API (this status no longer is returned).  This led to scenarios where some errors were treated fatally, and other scenarios are not.  This made error processing difficult for the client, and it was concluded that simpler is better.  Clients must handle reconnect on server disconnect, so why not let this common code handle all errors, rather than require the client to implement custom error dependent logic.  ></body> </Action>
<Action id="25780" issue="15142" author="kchristidis" type="comment" body="Scott: Given the comments above, are we good to close?" created="2017-06-08 18:25:31.0" updateauthor="kchristidis" updated="2017-06-08 18:25:31.0"/>
<Action id="25785" issue="15142" author="scottz" type="comment" created="2017-06-08 19:29:27.0" updateauthor="scottz" updated="2017-06-08 19:29:27.0"> <body><! CDATA  ~kchristidis  , your test is passing because a new grpc connection is created for every broadcast. Ours creates a single broadcast connection, and sends all the transactions using that same one.   ~jyellick  It is understandable that an error within the communication path between broadcast client and orderer could result in tearing down the grpc connection, requiring broadcast client to create one again. However, this 503 is an indication of the orderer recognizing a downstream problem, between the orderer and kafkabrokers. The orderer is embedding a 503 return code within a good message that signifies a failure to complete the request, not a connection error. Your decision creates a situation where we could see a huge flurry of activity in a network with many broadcast clients, all trying simultaneously to reestablish grpc connections to the orderers - when the problem is really within the orderer service (connections between orderers and kafkabrokers or kafkabrokers stopped). We may need to test and revisit this decision in future.  Note the grpc connections between orderer and deliver-clients never drop in our tests, and they remain stable the entire time.   ></body> </Action>
<Action id="25787" issue="15142" author="kchristidis" type="comment" created="2017-06-08 19:51:11.0" updateauthor="kchristidis" updated="2017-06-08 19:51:11.0"> <body><! CDATA > We may need to test and revisit this decision in future.  That is fine.  > Note the grpc connections between orderer and deliver-clients never drop in our tests, and they remain stable the entire time.   Understood. I believe this is what's causing this: https://jira.hyperledger.org/browse/FAB-4172?focusedCommentId=25758&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-25758  Going back to what this JIRA is all about, what remains for this issue to be closed? I may well be missing something.  ></body> </Action>
<Action id="25864" issue="15142" author="scottz" type="comment" created="2017-06-09 14:35:28.0" updateauthor="scottz" updated="2017-06-09 14:42:03.0"> <body><! CDATA I agree to allow code to remain as is, but would like to repurpose this bug to be used for a documentation enhancement to add a note about the connection management. That is, should the client wish to retry or send new transactions to the orderer after a failure, it is the broadcast client's responsibility to recreate the grpc connections (via the SDK API) because the fabric orderer service tears down the grpc connection for ANY type of error - even if the error is internal to the orderer service rather than a problem with the grpc connection itself. I struggled to find an ideal place where we could add this in the fabric or fabric-sdk-node doc pages. My suggestions: * create an SDK Implementer's Guide * create a TroubleShooting Guide which contains a list of errors, and possible suggested resolutions *  http://hyperledger-fabric.readthedocs.io/en/latest/FAQ/architecture_FAQ.html#application-side-programming-model         ></body> </Action>
<Action id="25865" issue="15142" author="kchristidis" type="comment" body=" ~nickgaski : See Scott&apos;s comment above – could we add that note somewhere?" created="2017-06-09 14:38:54.0" updateauthor="kchristidis" updated="2017-06-09 14:38:54.0"/>
<Action id="66560" issue="15142" author="joe-alewine" type="comment" body="With Kafka deprecated, this can probably be closed." created="2020-01-09 16:54:17.0" updateauthor="joe-alewine" updated="2020-01-09 16:54:17.0"/>
