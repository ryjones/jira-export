<Action id="26192" issue="17842" author="kchristidis" type="comment" created="2017-06-13 19:39:41.0" updateauthor="kchristidis" updated="2017-06-13 20:43:49.0"> <body><! CDATA You have set the sarama/Kafka logs to true (ORDERER_KAFKA_VERBOSE=true) and you have operations in the background running over all brokers every couple of hundred of milliseconds repeatedly:   https://github.com/hyperledger/fabric/blob/master/sampleconfig/orderer.yaml#L181    https://github.com/hyperledger/fabric/blob/master/sampleconfig/orderer.yaml#L187      What you see is normal. If you want to silence the logs, set the verbosity of the Kafka logs to FALSE.  ></body> </Action>
<Action id="26260" issue="17842" author="xixuejia" type="comment" created="2017-06-14 11:45:43.0" updateauthor="xixuejia" updated="2017-06-14 11:45:43.0"> <body><! CDATA Hi  ~kchristidis , thanks for comment. The flooding log is at WARN level and there were more than 100 WARNING messages per second. Apparently, it's not caused by the retry mechanism. I apologize that I didn't check peer logs. Actually, in peer log, there were also WARNING log flooding below {code} 2017-06-14 03:42:28.633 UTC  blocksProvider  DeliverBlocks -> WARN 1803  mychannel  Got error &{SERVICE_UNAVAILABLE}{code} I looked into the peer code here  https://github.com/hyperledger/fabric/blob/master/core/deliverservice/blocksprovider/blocksprovider.go#L133-L136  I believe that's the root cause of log flooding. When ordering service becomes SERVICE_UNAVAILABLE, peer disconnects with orderer nodes and continues the loop, reconnects with orderer nodes and still receives SERVICE_UNAVAILABLE... infinite loop without any retry mechanism here  ></body> </Action>
<Action id="26281" issue="17842" author="kchristidis" type="comment" created="2017-06-14 15:20:25.0" updateauthor="kchristidis" updated="2017-06-14 15:28:25.0"> <body><! CDATA  ~C0rWin : As far as I can see, the connection attempt happens using a backoff algorithm:   https://github.com/hyperledger/fabric/blob/a069559d342ce2d5ac15fcf9a5d4e15290cbcb7b/core/deliverservice/client.go#L67   Any idea what brings about this issue?  ></body> </Action>
<Action id="26284" issue="17842" author="kchristidis" type="comment" body="Xixue, we&apos;ll also need those logs – at least a decent piece of them if they&apos;re too big." created="2017-06-14 15:31:28.0" updateauthor="kchristidis" updated="2017-06-14 15:31:28.0"/>
<Action id="26286" issue="17842" author="xixuejia" type="comment" created="2017-06-14 15:41:27.0" updateauthor="xixuejia" updated="2017-06-14 15:41:27.0"> <body><! CDATA Hi  ~kchristidis , they are repeated single line WARNING messages. For peers {code}2017-06-14 03:42:28.633 UTC  blocksProvider  DeliverBlocks -> WARN 1803  mychannel  Got error &{SERVICE_UNAVAILABLE}{code} for orderer nodes {code}2017-06-13 18:01:18.783 UTC  orderer/common/deliver  Handle -> WARN 53f25 Rejecting deliver request because of consenter error{code}  ></body> </Action>
<Action id="26290" issue="17842" author="xixuejia" type="comment" created="2017-06-14 15:46:26.0" updateauthor="xixuejia" updated="2017-06-14 15:46:26.0"> <body><! CDATA I think the reason why backoff algorithm does not take effect is that err is nil while SERVICE_UNAVAILABLE Status is just a part of *resp* in L126 https://github.com/hyperledger/fabric/blob/master/core/deliverservice/client.go#L115-L126  ></body> </Action>
<Action id="26299" issue="17842" author="yacovm" type="comment" body="https://gerrit.hyperledger.org/r/#/c/10471/" created="2017-06-14 16:53:57.0" updateauthor="yacovm" updated="2017-06-14 16:53:57.0"/>
<Action id="26305" issue="17842" author="xixuejia" type="comment" body="Thanks  ~yacovm ! The retry mechanism added in that changeset should also fix this issue." created="2017-06-14 17:35:33.0" updateauthor="xixuejia" updated="2017-06-14 17:35:33.0"/>
<Action id="26321" issue="17842" author="yacovm" type="comment" body="Merged. can you check?" created="2017-06-14 19:04:19.0" updateauthor="yacovm" updated="2017-06-14 19:04:19.0"/>
<Action id="26326" issue="17842" author="xixuejia" type="comment" created="2017-06-14 20:06:16.0" updateauthor="xixuejia" updated="2017-06-14 20:06:16.0"> <body><! CDATA Hi  ~yacovm , I retried with the latest patchset. It did work at the very start, WARNING logs stop flooding even if we got SERVICE_UNAVAILABLE. But after a couple of minutes, currDelay overflows and becomes negative and finally it becomes 0 which bypasses the retry mechanism just added    https://github.com/hyperledger/fabric/blob/master/core/deliverservice/blocksprovider/blocksprovider.go#L156  {code:java} 2017-06-14 19:59:59.185 UTC  blocksProvider  DeliverBlocks -> WARN 108  mychannel  Got error &{SERVICE_UNAVAILABLE} 2017-06-14 20:00:09.186 UTC  blocksProvider  DeliverBlocks -> ERRO 109 maxDelay: 10000000000.000000 2017-06-14 20:00:09.186 UTC  blocksProvider  DeliverBlocks -> ERRO 10a currDelay: 4611686018427387904.000000 2017-06-14 20:00:09.186 UTC  blocksProvider  DeliverBlocks -> ERRO 10b statusCounter: 55 2017-06-14 20:00:09.190 UTC  blocksProvider  DeliverBlocks -> WARN 10c  mychannel  Got error &{SERVICE_UNAVAILABLE} 2017-06-14 20:00:09.190 UTC  blocksProvider  DeliverBlocks -> ERRO 10d maxDelay: 10000000000.000000 2017-06-14 20:00:09.190 UTC  blocksProvider  DeliverBlocks -> ERRO 10e currDelay: -9223372036854775808.000000 2017-06-14 20:00:09.190 UTC  blocksProvider  DeliverBlocks -> ERRO 10f statusCounter: 56 2017-06-14 20:00:09.195 UTC  blocksProvider  DeliverBlocks -> WARN 110  mychannel  Got error &{SERVICE_UNAVAILABLE} 2017-06-14 20:00:09.195 UTC  blocksProvider  DeliverBlocks -> ERRO 111 maxDelay: 10000000000.000000 2017-06-14 20:00:09.195 UTC  blocksProvider  DeliverBlocks -> ERRO 112 currDelay: 0.000000 2017-06-14 20:00:09.195 UTC  blocksProvider  DeliverBlocks -> ERRO 113 statusCounter: 57 2017-06-14 20:00:09.200 UTC  blocksProvider  DeliverBlocks -> WARN 114  mychannel  Got error &{SERVICE_UNAVAILABLE} 2017-06-14 20:00:09.200 UTC  blocksProvider  DeliverBlocks -> ERRO 115 maxDelay: 10000000000.000000 2017-06-14 20:00:09.200 UTC  blocksProvider  DeliverBlocks -> ERRO 116 currDelay: 0.000000 2017-06-14 20:00:09.201 UTC  blocksProvider  DeliverBlocks -> ERRO 117 statusCounter: 58 2017-06-14 20:00:09.205 UTC  blocksProvider  DeliverBlocks -> WARN 118  mychannel  Got error &{SERVICE_UNAVAILABLE}{code}  ></body> </Action>
<Action id="26338" issue="17842" author="yacovm" type="comment" created="2017-06-14 20:57:28.0" updateauthor="yacovm" updated="2017-06-14 21:06:49.0"> <body><! CDATA Thanks for testing it for enough time and not declaring it to be fixed after a short while.  If you could cast it to an unsigned int or something like that and try again, that'd be great.  Edit - actually, it won't help. I guess maybe check that it's negative or some condition, up to you.     Thanks again.  ></body> </Action>
<Action id="26346" issue="17842" author="xixuejia" type="comment" created="2017-06-14 21:37:30.0" updateauthor="xixuejia" updated="2017-06-14 21:37:30.0"> <body><! CDATA Verified locally and this should fix the overflow issue  https://gerrit.hyperledger.org/r/#/c/10601/  ></body> </Action>
