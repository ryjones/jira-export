<Action id="65885" issue="41662" author="guoger" type="comment" body="Reopen if resurface" created="2019-12-02 09:46:03.0" updateauthor="guoger" updated="2019-12-02 09:46:03.0"/>
<Action id="66709" issue="41662" author="manish-sethi" type="comment" created="2020-01-15 18:53:02.0" updateauthor="manish-sethi" updated="2020-01-15 18:53:02.0"> <body><! CDATA  ^logs.txt.zip   Observed again. Logs attached. Reopening this Jira  ></body> </Action>
<Action id="68111" issue="41662" author="sykesm" type="comment" created="2020-02-26 12:25:54.0" updateauthor="sykesm" updated="2020-02-26 12:25:54.0"> <body><! CDATA Hit this again - locally this time.   ~guoger  - are you looking into this? I'd like to at least see something go in that does not rely on the 20m timeout to detect the issue.  ></body> </Action>
<Action id="68208" issue="41662" author="sykesm" type="comment" created="2020-03-03 16:58:22.0" updateauthor="sykesm" updated="2020-03-03 16:58:22.0"> <body><! CDATA Hit it again: https://dev.azure.com/Hyperledger/Fabric/_build/results?buildId=9132&view=logs&jobId=a11cdf60-0d11-5130-c801-1b4953009464&j=a11cdf60-0d11-5130-c801-1b4953009464&t=849b73ad-cbaa-5065-7214-c0ac3d42920d  With the timeout mitigation, it times out faster. Woohoo!  {code} goroutine 1839  running : github.com/hyperledger/fabric/orderer/common/cluster_test.(*deliverServer).Deliver(0xc0001b6680, 0x1361d20, 0xc000476b50, 0x0, 0x0) 	/home/vsts/work/1/go/src/github.com/hyperledger/fabric/orderer/common/cluster/deliver_test.go:191 +0x691 github.com/hyperledger/fabric/vendor/github.com/hyperledger/fabric-protos-go/orderer._AtomicBroadcast_Deliver_Handler(0x11a79c0, 0xc0001b6680, 0x135ed20, 0xc000289140, 0x2688610, 0xc0019ac600) 	/home/vsts/work/1/go/src/github.com/hyperledger/fabric/vendor/github.com/hyperledger/fabric-protos-go/orderer/ab.pb.go:674 +0xce github.com/hyperledger/fabric/vendor/google.golang.org/grpc.(*Server).processStreamingRPC(0xc0000dcdc0, 0x13637c0, 0xc000478600, 0xc0019ac600, 0xc0002078c0, 0x1be7060, 0x0, 0x0, 0x0) 	/home/vsts/work/1/go/src/github.com/hyperledger/fabric/vendor/google.golang.org/grpc/server.go:1199 +0x1522 github.com/hyperledger/fabric/vendor/google.golang.org/grpc.(*Server).handleStream(0xc0000dcdc0, 0x13637c0, 0xc000478600, 0xc0019ac600, 0x0) 	/home/vsts/work/1/go/src/github.com/hyperledger/fabric/vendor/google.golang.org/grpc/server.go:1279 +0x12d8 github.com/hyperledger/fabric/vendor/google.golang.org/grpc.(*Server).serveStreams.func1.1(0xc00003f2b0, 0xc0000dcdc0, 0x13637c0, 0xc000478600, 0xc0019ac600) 	/home/vsts/work/1/go/src/github.com/hyperledger/fabric/vendor/google.golang.org/grpc/server.go:710 +0xc9 created by github.com/hyperledger/fabric/vendor/google.golang.org/grpc.(*Server).serveStreams.func1 	/home/vsts/work/1/go/src/github.com/hyperledger/fabric/vendor/google.golang.org/grpc/server.go:708 +0xb9 FAIL	github.com/hyperledger/fabric/orderer/common/cluster	84.005s {code}  ></body> </Action>
<Action id="68283" issue="41662" author="dereckluo" type="comment" body="hit again, 3/11/2020" created="2020-03-11 15:16:09.0" updateauthor="dereckluo" updated="2020-03-11 15:16:09.0"/>
<Action id="68296" issue="41662" author="dereckluo" type="comment" body="  ^3-12.txt  hit again, logs attached." created="2020-03-12 17:22:55.0" updateauthor="dereckluo" updated="2020-03-12 17:22:55.0"/>
<Action id="68463" issue="41662" author="dereckluo" type="comment" body="hit again, 3/20/2020  ^3-20.txt  " created="2020-03-20 14:42:20.0" updateauthor="dereckluo" updated="2020-03-20 14:42:20.0"/>
<Action id="68769" issue="41662" author="wlahti" type="comment" body="Hit again today in CI. Looks like we have plenty of logs for it. :)" created="2020-04-08 12:49:12.0" updateauthor="wlahti" updated="2020-04-08 12:49:12.0"/>
<Action id="69099" issue="41662" author="wenjian" type="comment" body="Hi again, 04/28/2020." created="2020-04-28 14:08:45.0" updateauthor="wenjian" updated="2020-04-28 14:08:45.0"/>
<Action id="69368" issue="41662" author="wenjian" type="comment" body="Hit again, 06/01/2020,Â  https://dev.azure.com/Hyperledger/0227bd2c-c4f6-478f-be00-ee519f115180/_apis/build/builds/15114/logs/73 " created="2020-06-01 18:46:32.0" updateauthor="wenjian" updated="2020-06-01 18:46:32.0"/>
<Action id="69604" issue="41662" author="denyeart" type="comment" body="Hit again https://dev.azure.com/Hyperledger/Fabric/_build/results?buildId=16827&amp;view=logs&amp;jobId=a11cdf60-0d11-5130-c801-1b4953009464&amp;j=a11cdf60-0d11-5130-c801-1b4953009464&amp;t=849b73ad-cbaa-5065-7214-c0ac3d42920d" created="2020-07-01 15:21:31.0" updateauthor="denyeart" updated="2020-07-01 15:21:31.0"/>
<Action id="69605" issue="41662" author="denyeart" type="comment" body=" ~guoger  Is a fix in progress?" created="2020-07-01 15:22:29.0" updateauthor="denyeart" updated="2020-07-01 15:22:29.0"/>
<Action id="69667" issue="41662" author="wenjian" type="comment" body="Hit again, 07/10/2020,Â Â  https://dev.azure.com/Hyperledger/0227bd2c-c4f6-478f-be00-ee519f115180/_apis/build/builds/17481/logs/60 " created="2020-07-10 12:27:29.0" updateauthor="wenjian" updated="2020-07-10 12:27:29.0"/>
<Action id="69884" issue="41662" author="tock" type="comment" created="2020-08-10 05:35:08.0" updateauthor="tock" updated="2020-08-10 05:38:05.0"> <body><! CDATA Hit again 10 Aug. 2020:  https://dev.azure.com/Hyperledger/Fabric/_build/results?buildId=19166&view=logs&j=6b58850f-3858-5a05-33e2-5e41cbf03c4e&t=bddec1cf-ba37-5883-9c3e-fd1e8608f9a1&l=3592   Â    https://dev.azure.com/Hyperledger/0227bd2c-c4f6-478f-be00-ee519f115180/_apis/build/builds/19166/logs/49   ></body> </Action>
<Action id="69890" issue="41662" author="tock" type="comment" body="Looking at the logs, one of the things I see is the message &quot;oops, something went wrong&quot;. This implies that the offending test case is `TestBlockPullerFailures`. This is a table test so it is hard to tell which test case is failing... however it also says &quot;Failed verifying received blocks: bad signature&quot; which means the failing test case isÂ &quot;failure at verifying pulled block&quot;. Lets add some additional logs around that test, ans increase its level to &quot;debug&quot; for the next time it fails." created="2020-08-10 08:12:03.0" updateauthor="tock" updated="2020-08-10 08:12:03.0"/>
<Action id="70402" issue="41662" author="wlahti" type="comment" body="Hit this again today in CI and attached the logs. I&apos;ll try to take a look to see if the recent log changes help narrow things down." created="2020-09-29 15:11:37.0" updateauthor="wlahti" updated="2020-09-29 15:11:37.0"/>
<Action id="70787" issue="41662" author="sykesm" type="comment" created="2020-11-23 16:01:13.0" updateauthor="sykesm" updated="2020-11-23 16:01:13.0"> <body><! CDATA Looks like we hit it again on a PR from a member of the community.   https://dev.azure.com/Hyperledger/Fabric/_build/results?buildId=25863&view=logs&jobId=6b58850f-3858-5a05-33e2-5e41cbf03c4e&j=6b58850f-3858-5a05-33e2-5e41cbf03c4e&t=bddec1cf-ba37-5883-9c3e-fd1e8608f9a1    ^PR2156.log   ></body> </Action>
<Action id="70795" issue="41662" author="guoger" type="comment" created="2020-11-24 05:22:49.0" updateauthor="guoger" updated="2020-11-24 05:52:56.0"> <body><! CDATA it looks like this particular (latest) flake happens at test {{TestBlockPullerBadBlocks}}, within following case {code} 		{ 			name:           "bad type", 			corruptBlock:   statusType, 			expectedErrMsg: "faulty node, received: status:INTERNAL_SERVER_ERROR", 		}, {code}  when block puller makes *second* attempt to fetch last block seq, it got following error repeatedly every 10s, and eventually panics: {code} 2020-11-22T19:29:24.4075999Z  31m2020-11-22 19:28:34.611 UTC  test  fetchLastBlockSeq -> ERRO 31a 0m Failed receiving the latest block from 127.0.0.1:46881: didn't receive a response within 10s 2020-11-22T19:29:24.4076983Z  33m2020-11-22 19:28:34.612 UTC  test  func1 -> WARN 31b 0m Received error of type 'didn't receive a response within 10s' from 127.0.0.1:46881 2020-11-22T19:29:24.4077946Z  33m2020-11-22 19:28:34.612 UTC  test  connectToSomeEndpoint -> WARN 31c 0m Could not connect to any endpoint of  {"CAs":null,"Endpoint":"127.0.0.1:46881"}  {code}  somehow, {{deliverServer}} did not see the expected Seek enqueued  ></body> </Action>
<Action id="70796" issue="41662" author="guoger" type="comment" body=" https://github.com/hyperledger/fabric/pull/2157 Â adding more logs" created="2020-11-24 06:13:26.0" updateauthor="guoger" updated="2020-11-24 06:13:26.0"/>
<Action id="70915" issue="41662" author="guoger" type="comment" body="https://github.com/hyperledger/fabric/pull/2196 gracefully fail test instead of panic" created="2020-12-06 14:55:48.0" updateauthor="guoger" updated="2020-12-06 14:55:48.0"/>
<Action id="71045" issue="41662" author="jyellick" type="comment" body="Looks like we still see a panic because the test has already exited? Adding log." created="2021-01-05 19:35:05.0" updateauthor="jyellick" updated="2021-01-05 19:35:05.0"/>
<Action id="71059" issue="41662" author="guoger" type="comment" created="2021-01-06 15:07:11.0" updateauthor="guoger" updated="2021-01-06 15:07:11.0"> <body><! CDATA  ~jyellick  thx for the log. I'm not sure this particular failure is the same as the ones before, but thankfully it now reports which exact test case failed.  Even though it's still mysterious to me how mock deliverServer receives extraneous {{seek}}, created a  PR|https://github.com/hyperledger/fabric/pull/2241  to gracefully handle potential deadlock  ></body> </Action>
