<Issue id="41740" key="FAB-16281" number="16281" project="10002" reporter="baohua" creator="baohua" type="10001" summary="Performance degradation with large ledger due to growing txid database" priority="3" status="10000" created="2019-08-12 15:37:23.0" updated="2020-04-30 18:04:42.0" votes="0" watches="5" workflowId="54775"> <description><! CDATA When doing the duplicate transaction checking for each transaction, it will query whether the transaction id exists in the indexDB.  With large ledger, there will be lots of existing transactions information in the indexDB, and the performance will go down. Since this is per-transaction behavior, it will affect the performance nonNegligible.  This issue will become worse with accumulating the transactions over time.     There may be several workarounds:  1) Enhance indexDB (which uses levelDB now) by using better DB in terms of large-scale.  2) Design specific data structure to enhance this duplication checking.  3) Re-design the existing transaction id generation mechanism, e.g., add timestamp to allow storing with separate smaller DB size.     Till now, i feel the flexible solution is to encode the time information into the transaction id, e.g., timestamp, block height, epoch etc. To keep compatibility with the existing one, need to include the new information as optional. If there's no such information, then follow legacy mechanism to check the entire indexDB.  ></description> </Issue>
