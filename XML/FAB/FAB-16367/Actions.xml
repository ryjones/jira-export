<Action id="63140" issue="41934" author="sykesm" type="comment" created="2019-08-21 15:13:48.0" updateauthor="sykesm" updated="2019-08-21 15:14:08.0"> <body><! CDATA Excellent comment in the shim:  {code} 		//better not be nil 		var nextStateMsg *pb.ChaincodeMessage  		defer func() { 			handler.triggerNextState(nextStateMsg, errc) 		}() {code}   1  https://github.com/hyperledger/fabric/blob/6f64c22f644da6226665972b6590ab0cf3b2d399/core/chaincode/shim/handler.go#L243-L248  ></body> </Action>
<Action id="63141" issue="41934" author="sykesm" type="comment" created="2019-08-21 15:22:38.0" updateauthor="sykesm" updated="2019-08-21 15:22:38.0"> <body><! CDATA kvledger terminated the peer with a panic and the recover function in the shim dereferenced a nil during recovery.  The shim code no longer exists in master. The ledger panic looks appropriate.  ></body> </Action>
<Action id="63165" issue="41934" author="abel23" type="comment" body="But terminating the peer doesn&apos;t look like a solution. Wouldn&apos;t that cause denial of service if the peer goes down." created="2019-08-22 04:17:17.0" updateauthor="abel23" updated="2019-08-22 04:17:17.0"/>
<Action id="63993" issue="41934" author="jyellick" type="comment" body="This looks like an underlying couchdb error caused the peer to crash because it could not perform the necessary channel join operations.  The recourse seems correct.  I see no denial of service here, as only an admin can trigger the join channel path, and if couch is unavailable, then unavoidably, there will be a denial of service." created="2019-09-23 20:32:21.0" updateauthor="jyellick" updated="2019-09-23 20:32:21.0"/>
<Action id="64015" issue="41934" author="abel23" type="comment" body=" ~jyellick  These issues reported by me occurs only in docker swarm mode" created="2019-09-24 08:23:46.0" updateauthor="abel23" updated="2019-09-24 08:23:46.0"/>
<Action id="64084" issue="41934" author="jyellick" type="comment" created="2019-09-24 17:08:15.0" updateauthor="jyellick" updated="2019-09-24 17:08:15.0"> <body><! CDATA  ~abel23  yes, I realize you only observe this when using docker swarm.  However, it seems to me as if the peer is working as expected – when it encounters an irrecoverable couchdb error, it panics and exits, rather than continue operating and risk corruption.  So, I do not see this as a Fabric bug.  As this only appears in docker swarm, it seems likely to me that this is some swarm configuration issue or other incompatibility.  As much as we would like to provide support for all configurations, if the error is due to integration with another toolset (be it docker swarm, kubernetes, etc.) such that we cannot easily reproduce the problem, then there is very little we can do.  ></body> </Action>
