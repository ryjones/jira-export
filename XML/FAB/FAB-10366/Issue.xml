<Issue id="30605" key="FAB-10366" number="10366" project="10002" reporter="manish-sethi" assignee="c0rwin" creator="manish-sethi" type="10004" summary="A new peer joining the network unaware of the purged pvt data" priority="3" resolution="10000" status="6" created="2018-05-24 16:41:57.0" updated="2018-07-20 14:16:49.0" resolutiondate="2018-05-31 18:30:41.0" votes="0" watches="5" workflowId="42326"> <description><! CDATA *Scenario*: When a new peer joins the network, the peer receives the blocks from genesis block. Similar is the situation when a peer is re-started after a long halt and the peer starts receiving the block where it left the last time. In summary, the situation is such that the other peers in the network are well advanced from the peer who is trying to catch up with the network  *Problem*: In the above scenario, when the catching up peer receives the block, it also tries to pull the private data (if any, for which the peer is eligible per the last configuration). However, since we allow automatic purge of the pvt data via BTL (Block-To-Live) policy, all the peers in the network are likely to have purged the data. This causes the catching up peer to keep hitting timeouts for each block.  *Solutions*: There are a couple of possible solutions that we can attempt to solve  *Solution 1*: This solution requires the catching up peer to be constantly aware of the latest height (say via orderer). Before attempting to pull the pvt data, the peer consults the BTL (locally available most recent collection config) and evaluates whether the pvt data is already (logically) expired. If yes, then do not even try to pull this pvt data. However, one of the assumptions for this solution to work is that the changes in the BTL are applicable only on the new data written after the BTL - This assumption hold true for Fabric-1.2, as we don't even allow change in BTL for now.  *Solution 2*: This solution requires a significant change in the gossip design. However, the main benefits of this solution is that this allows a high degree of flexibilities in the configurations going forward. For instance, this does not make any assumptions about the future BTL configurations and hence will allow changing the BTLs retrospectively. Further, the benefits also extends to other configuration such as collection membership. For instance, in fabric 1.2, to overcome the problem of catching up peer due to change in membership, the sending peer evaluates the point in time configuration. This implementation rules out a possibility where a business case demands that the peer should respect the latest collection configuration for distribution as opposed to the historic one.  At the very high level, in this solution, the catching up peer attempts to get the private data from other peer, but the other peer sends a proper refusal code as opposed to simply refusing. To take an example, assume that a catching up peer is committing block#5 and hence trying retrieve the pvt data associated with this block. However, the refusal code that this peer gets may look like <data purged, atBlock#50> and <not authorised, atBlock#100> etc. Here, the first refusal code depicts that the sending peer is aware of the more recent configurations and has purged the data as of block#50. Similarly, the second refusal code depicts that the sending peer is aware of removal of the asking peer at block#100. However, the challenge is posed by the fact that the catching up peer cannot blindly trust the refusal code sent by the sending peer. Instead, the peer needs to record/index these responses by the block number (#50 and #100 in the above example) and when the peer reaches that height, it needs to validate these responses first hand by consulting the locally available configurations. In other words, since the catching up peer does not have access to the future configurations, it simply trusts the sending peer refusal code and proceeds but re-evaluates the codes (potentially, in a background thread) when the peer itself learns the fact in the future.  ></description> </Issue>
