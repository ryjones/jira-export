<Action id="45078" issue="30605" author="denyeart" type="comment" created="2018-05-24 16:53:52.0" updateauthor="denyeart" updated="2018-05-24 16:53:52.0"> <body><! CDATA In my opinion this is a must-fix problem for v1.2, and it seems Solution 1 is the more feasible solution in v1.2 timeframe.  I believe the height of other peers is already known via gossip, so the catching up peer should not attempt to pull from peers whose height is so far advanced that the private data would be expected to be purged based on BlockToLive of the collection.  ></body> </Action>
<Action id="45089" issue="30605" author="yacovm" type="comment" created="2018-05-24 19:08:59.0" updateauthor="yacovm" updated="2018-05-24 19:08:59.0"> <body><! CDATA {quote}This solution requires the catching up peer to be constantly aware of the latest height (say via orderer). {quote} "constantly aware" - and "say via orderer" is easier said than done... ;)  It's not possible to be constantly aware of a value that constantly changes, on a server that is far away from you, and that server itself, might not even be the latest one, right? If you add a new ordering node, it needs to catch up via pulling messages from kafka, and the node might be down and then brought back again and is catching up from kafka, etc. Do you suggest the peer polls the orderer every once in a while or something? I don't think that's such a good idea, or is easily done.  I also don't understand why you even need to know the latest height. The solution seems much simpler to me - when you look for peers that might have the data, you also take in account their ledger height. If the BTL policy indicates that the no longer should have the data according to their published height - you skip them.    {quote}This solution requires a significant change in the gossip design {quote} The gossip private data module has nothing to do with gossip itself...     {quote}In other words, since the catching up peer does not have access to the future configurations, it simply trusts the sending peer refusal code and proceeds but re-evaluates the codes (potentially, in a background thread) when the peer itself learns the fact in the future  the peer needs to record/index these responses by the block number (#50 and #100 in the above example) and when the peer reaches that height, it needs to validate these responses first hand by consulting the locally available configurations. {quote}    so if the peer lied and you missed the private data, you go back to where you were, and then you pull the private data once again, right? This awfully sound like reconciliation to me. :)     {quote}In my opinion this is a must-fix problem for v1.2 {quote} I disagree. without state reconciliation, we don't have a full solution any way you look at it. Can't we just not purge private data until v1.3 ? It's only a few months... Plus -  we should be shutting down the critical code paths these days - not extending them.    {quote}I believe the height of other peers is already known via gossip, so the catching up peer should not attempt to pull from peers whose height is so far advanced that the private data would be expected to be purged based on BlockToLive of the collection. {quote}    That's exactly the solution I expected to see in the JIRA.  ></body> </Action>
<Action id="45091" issue="30605" author="denyeart" type="comment" created="2018-05-24 19:28:17.0" updateauthor="denyeart" updated="2018-05-24 19:28:17.0"> <body><! CDATA Ok, sounds like  ~yacovm  is agreeable to Solution 1, but use the peer's height you are trying to pull from to determine whether to make the pull request or not.  Seems the remaining question is whether the solution can be contained with high quality in v1.2 timeframe.  ></body> </Action>
<Action id="45092" issue="30605" author="c0rwin" type="comment" created="2018-05-24 19:48:33.0" updateauthor="c0rwin" updated="2018-05-24 19:48:33.0"> <body><! CDATA {quote} Solution 1: This solution requires the catching up peer to be constantly aware of the latest height (say via orderer). Before attempting to pull the pvt data, the peer consults the BTL (locally available most recent collection config) and evaluates whether the pvt data is already (logically) expired. If yes, then do not even try to pull this pvt data. However, one of the assumptions for this solution to work is that the changes in the BTL are applicable only on the new data written after the BTL - This assumption hold true for Fabric-1.2, as we don't even allow change in BTL for now. {quote}  In my opinion *Solution 1* is over complicated and neglects existence of leader election as the leader node maintains connectivity with the ordering service for scalability reasons, so going with *Solution 1* we might end up with all peers going to ordering service to check for most updated ledger height, which IMO will defeat whole point of having scalable dissemination infrastructure.  {quote} Solution 2: This solution requires a significant change in the gossip design. However, the main benefits of this solution is that this allows a high degree of flexibilities in the configurations going forward. For instance, this does not make any assumptions about the future BTL configurations and hence will allow changing the BTLs retrospectively. Further, the benefits also extends to other configuration such as collection membership. For instance, in fabric 1.2, to overcome the problem of catching up peer due to change in membership, the sending peer evaluates the point in time configuration. This implementation rules out a possibility where a business case demands that the peer should respect the latest collection configuration for distribution as opposed to the historic one. {quote}  With respect to *Solution 2*, it has a lot in common and reminds me same issues we have to face while solving data reconciliation.  {quote} In my opinion this is a must-fix problem for v1.2 {quote}  I am failing to see why this is a must fix for v1.2 as we do not have reconciliation for v1.2, this is mostly an optimization problem. At the moment since we allowing peer to continue in absence of private data, it won't get stack, moreover I'd expect BTL to be configured to something which will keep private data for long enough period of time. Hence peer has to be offline for significant amount of time to miss peaces of private data which is purged.  {quote} Ok, sounds like Yacov Manevich is agreeable to Solution 1, but use the peer's height you are trying to pull from to determine whether to make the pull request or not. {quote}  Some variation of *Solution 1* sounds to me as a right approach  {quote} Seems the remaining question is whether the solution can be contained with high quality in v1.2 timeframe. {quote}  *However*, I think that this should be properly designed as part of data reconciliation process, rather than trying to provide something half backed within very limited time frame.  ></body> </Action>
<Action id="45099" issue="30605" author="c0rwin" type="comment" created="2018-05-24 22:15:21.0" updateauthor="c0rwin" updated="2018-05-24 22:15:21.0"> <body><! CDATA Per discussion w/  ~denyeart   ~manish-sethi  and  ~yacovm , we decided to address the problem in the following way:  While peer which detects is has missing private data will select peers to pull missing peaces from, it will filter out peers which for sure already purged data that data based on BTL and know ledger height, thus optimizing the retrieval time.   suggested fix is: https://gerrit.hyperledger.org/r/22319  ></body> </Action>
<Action id="45105" issue="30605" author="mastersingh24" type="comment" created="2018-05-24 23:14:10.0" updateauthor="mastersingh24" updated="2018-05-24 23:14:10.0"> <body><! CDATA The suggested fix above seems ok, but I'm really not sure why we should be adding more code when we are supposed to be done here.  While the current code might not be optimal for late joining peers, it's an edge case (IMHO) for where people using this will be in terms of deployment.  ></body> </Action>
<Action id="45108" issue="30605" author="denyeart" type="comment" body=" ~mastersingh24  Without a fix here, it will not be possible for ANY late joining peer to EVER catch up, as the BlockToLive policy would ensure that the peer tries pull private data for EVERY prior transaction until retries are exhausted.  This is not an edge case or optimization, but a fix for the main case for late joining peers." created="2018-05-25 00:07:15.0" updateauthor="denyeart" updated="2018-05-25 00:07:15.0"/>
<Action id="45207" issue="30605" author="c0rwin" type="comment" body="https://gerrit.hyperledger.org/r/22319" created="2018-05-28 20:57:45.0" updateauthor="c0rwin" updated="2018-05-28 20:57:45.0"/>
