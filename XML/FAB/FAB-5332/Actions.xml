<Action id="30637" issue="19286" author="muralisr" type="comment" body=" ~yacovm  can you add a comment what context this came out of (I vaguely remember there was a different context that instigated this JIRA) ?" created="2017-09-07 18:38:54.0" updateauthor="muralisr" updated="2017-09-07 18:38:54.0"/>
<Action id="30644" issue="19286" author="yacovm" type="comment" created="2017-09-07 19:33:03.0" updateauthor="yacovm" updated="2017-09-07 19:33:03.0"> <body><! CDATA Basically HSBN had a problem where the writes were failing and the deliver service was simply pulling more and more blocks indefinitely.  That's what I remember,  ~C0rWin  can correct me if I'm wrong.  After  this|https://gerrit.hyperledger.org/r/#/c/12495/  fix went in, however- the operation would block starting from a certain block gap between the ledger and the deliver service. so.. in a way, the first bullet is fulfilled. The second bullet - no so much.   We need to think more about how and whether to do this.  ~C0rWin  opinions? What shall be done with this JIRA?  ></body> </Action>
<Action id="33257" issue="19286" author="jeffgarratt" type="comment" body=" ~yacovm   ~C0rWin  Any updates to this one?" created="2017-10-26 19:23:26.0" updateauthor="jeffgarratt" updated="2017-10-26 19:23:26.0"/>
<Action id="33652" issue="19286" author="c0rwin" type="comment" created="2017-10-31 20:59:37.0" updateauthor="c0rwin" updated="2017-10-31 20:59:37.0"> <body><! CDATA Currently if ledger got stuck, for example due to the fact that file system is full, the commit of the block will fail and peer will result with panic. While, I understand the intent behind this ticket and even if we do delivery service and leader election to stop, the only way to resume will be to restart the peer, which for such situation seems reasonable.   If we going on adding functionality to stop delivery service and leader election, we need to think how we would alert or let operator to know that peer has to be resumed.   As a first step I'd suggest adding exponential backoff strategy to retry committing new blocks in case of failure.  ></body> </Action>
<Action id="33659" issue="19286" author="yacovm" type="comment" created="2017-10-31 22:39:11.0" updateauthor="yacovm" updated="2017-10-31 22:44:24.0"> <body><! CDATA Here is an idea:   As well known, the state module has a goroutine which repeatedly does something that can be roughly described as : {code} case <-s.payloads.Ready()    // wait until the next block in order is available s.payloads.Pop()   // pop that block commit block {code}   The payload buffer (s.payloads) is fed blocks by the goroutine of the deliver service via addPayload in blocking mode, and the relevant part of the code is: {code} 	for blockingMode && s.payloads.Size() > defMaxBlockDistance*2 { 		time.Sleep(enqueueRetryInterval) 	}  return s.payloads.Push(payload) {code}  As we know, if the payload buffer is "too full", the goroutine that calls addPayload in blocking mode - spins in the for loop above doing sleep until the size of the payload buffer is reduced enough. So, essentially we  have a struct (the one in state.go) that 2 goroutines have access to it concurrently: * The goroutine which commits the blocks into the ledger * The goroutine which passes the blocks to the 1st goroutine via the payload buffer, and if the file system is full - it will be either: ** stuck in receiving a block from the payload buffer ** stuck doing an endless sleep waiting for the payload buffer to get smaller in size.  I think that for v1.1 what we can do is: * Have an atomic variable that is: ** Set to the current time right before commitBlock() is called ** Set to 0 right after commitBlock() is called * Have the code in addPayload check in the for loop that sleeps, whether the difference between the current time and the last time commitBlock() was called isn't too high. If it is - it means that commitBlock() takes too much time and it is stuck, so return a special error that the blocks provider (in the deliver service) would check and if it would indicate the ledger is stuck, then we "return" (this is the code path that triggers the leader election to give up on leadership)   What do you think  ~C0rWin  ?    ></body> </Action>
