<Action id="50502" issue="33806" author="kchristidis" type="comment" body="Thanks for the write-up. This one&apos;s tricky. For the {{cluster}} bootstrap method are we basically instructing the new OSN to use the info contained in the provided genesis block, but overwrite the consenter set info contained therein, with that of {{netboot.yml}}?" created="2018-09-16 12:10:47.0" updateauthor="kchristidis" updated="2018-09-16 12:10:47.0"/>
<Action id="50503" issue="33806" author="yacovm" type="comment" created="2018-09-16 12:51:05.0" updateauthor="yacovm" updated="2018-09-16 20:03:20.0"> <body><! CDATA {quote}For the {{cluster}} bootstrap method are we basically instructing the new OSN to use the info contained in the provided genesis block, but overwrite the consenter set info contained therein, with that of {{netboot.yml}}? {quote} Yep - all cluster members, regardless of what channel they are in - have the same system channel. After we fetch the system channel, we fetch the channels themselves, selectively - from the various nodes in the consenter set in the YAML file, and afterwards we resume normal operation.   That said - we need somehow to prevent the OSN from repeating this operation again in case it crashes. # One way of doing it, would be to say that once an OSN has completed its catching up with the cluster, the admin would change its bootstrap method to file. # Another way, more tricky - but is easier in terms of administration would be, to have the newly joined OSN: ## At startup - check if it has only a genesis block in its system chain. If yes - then wipe out the other chains and go to step (2.2) (below). else, it has more than the genesis block in its system chain, and in that case - it just acts like an OSN that has been booted with a *file* bootstrap method. ## Write the system channel blocks to a temporary file, instead of appending to the system chain. ## Create the channel ledgers and pull them as described in this JIRA ## Once all chains are fetched up to the last configuration block, we write the system chain from the temporary file into the actual system chain. Now, if the OSN has crashed during this operation, when it boots - it will fetch everything from scratch. The advantage of this, is that it's a "fire and forget" method - the administrator doesn't need to do anything after the initial bootstrapping, and after a successful fetching of all chains and writing of the system chain, the administrator doesn't need to change the configuration of the OSN to be with a *file* method. The only corner case is that if the OSN crashes at the last stage where it appends the system channel to its ledger, then we're left with an OSN that is stuck. I think that this corner case can be just documented and be said that administrators need to make sure that if the OSN crashed at that stage - then they need to intervene, and in practice - the chances of this happening are very slim, as the data is already available locally.  ></body> </Action>
<Action id="50506" issue="33806" author="kchristidis" type="comment" created="2018-09-16 19:32:09.0" updateauthor="kchristidis" updated="2018-09-16 19:32:09.0"> <body><! CDATA Got it. I get the proposal. Let's let it simmer for a few days, in case we can come up with a simpler approach. If not, what you describe here should work.     {quote}3. Create the channel system chains {quote} PS: I take it you mean the channel chains here, right?  ></body> </Action>
<Action id="50509" issue="33806" author="yacovm" type="comment" body="Yes" created="2018-09-16 20:02:26.0" updateauthor="yacovm" updated="2018-09-16 20:02:26.0"/>
<Action id="50511" issue="33806" author="yacovm" type="comment" body="Another aspect this writeup doesn&apos;t cover, is how do you add an existing node to an existing channel. I would say that long term we should support this, but i don&apos;t think it&apos;s an interesting use case for v1.4 release, while adding a new node is crucial from obvious reasons." created="2018-09-16 21:53:06.0" updateauthor="yacovm" updated="2018-09-16 21:53:06.0"/>
<Action id="50569" issue="33806" author="jyellick" type="comment" created="2018-09-17 19:06:36.0" updateauthor="jyellick" updated="2018-09-17 19:06:36.0"> <body><! CDATA A few thoughts: {quote}Currently, in the kafka orderer type, adding a new OSN (ordering service node) is straight forward - just configure it to point to the right kafka cluster, and it'll pull transactions from kafka, and cut the blocks as needed. {quote} I would point that in actually Kafka suffers from this very issue, in that if the Kafka broker set sufficiently diverges from the initial config (ie, no broker address from the original set is still available), then adding a new OSN is generally no longer possible. It would be nice for us to tackle this issue as well. {quote}and also - it needs to know in which channels it is a member of, assuming we dont want all OSNs to contain all chains. {quote} Certainly, up until now, we have enforced that all orderers must handle all channels. However, since each channel has its own config, from an ordering perspective, picking only a subset of channels to follow does seem easy. I think we need to take a look at the SDK/user side though too. Do we still allow submission to any channel through any orderer? If not, how does the client know which orderer set to use? Is this maybe something from service discovery then? {quote}Approaches 1 and 2 both seem to me like having 1 very big downside: Pulling all the channels is simply inefficient and very, very slow.  If a chain has accumulated a large amount of data, pulling the blocks would take a very long time, and during that time - the OSN is not serviceable.  I believe that it is a reasonable assumption that:  Most data a block transactions is PEM encoded certificates Most data in transactions is text, or hashed text And therefore - if we take an entire tarball of a ledger of an OSN, and then compress it with gzip - it will be significantly cheaper to download out of band, than to send the blocks 1 by 1 via the Deliver API. {quote} I'm a little skeptical that approaches (1) and (2) should be discounted because they are going to be 'very, very slow.' Certainly it could be a problem, but even on a relatively thin pipe, say 10MB/s, 100 GB of blockchain data could be synced in 3 hours or so. In a LAN environment with a Gbit connection, 100 GB transfers in 15 minutes.  Also, for the service deployments, I would not expect the operator to have access to the filesystem of hosts.  The 'secure containers' etc. prevent or limit the network admins from executing arbitrary logic like shell commands, and the idea that an operator would have a copy of all user data in order to bootstrap a new orderer sounds unappealing.  {quote}the first 2 approaches involve a considerable amount of logic and testing, and this endangers the release of the Raft OSN feature. {quote} Do we see the ability to change Raft membership and add new nodes as a must have to release Raft? Seems like if there is a risk of containment we could start with a static set of Raft consenters, and add the reconfiguration later. I'd note that I've yet to see anyone dynamically add an OSN with Kafa (for anything other than test purposes). Most deployments I've seen they project the needed number of orderers at bootstrap and simply leave them static. {quote}The layout of the netboot.yaml file will have the following properties: {quote} I'm rather reluctant to introduce yet another config file with yet another expression of the data we already encode in other places. Particularly, it seems like there's a 100% overlap with the information here and the latest config block of the orderer system channel. Why not simply use this, then we don't need to introduce any new flags to the config. If the block is block 0, then we proceed as usual. If it is a later block, then we use it purely for connection info.  ></body> </Action>
<Action id="50572" issue="33806" author="yacovm" type="comment" created="2018-09-17 20:25:25.0" updateauthor="yacovm" updated="2018-09-17 20:25:25.0"> <body><! CDATA Thanks much for the reply, Jason!  {quote} If the block is block 0, then we proceed as usual. If it is a later block, then we use it purely for connection info.{quote}  That's a great idea! We first pull the chains, and pull the system chain last until it reaches the sequence of the bootstrap config block in the file system, which is used to bootstrap the orderer. If we crash in the middle, we know we should start over because the ledger of the system chain hasn't reached the bootstrap block.  {quote}Do we see the ability to change Raft membership and add new nodes as a must have to release Raft? Seems like if there is a risk of containment we could start with a static set of Raft consenters, and add the reconfiguration later{quote}  I believe that decoupling the orderer from the kafka cluster would pave the way to use cases where not all orderers would serve all channels, and organizations would want further segregation of data for confidentiality and for sharding, and they'll demand this sooner or later. You're right that if this endangers the entire release we are better of not including this, however - I think it's too early to say this is a risk of containment.   {quote}I'd note that I've yet to see anyone dynamically add an OSN with Kafa (for anything other than test purposes). Most deployments I've seen they project the needed number of orderers at bootstrap and simply leave them static.{quote} Of course, because setting up a 3-tiered orderer (orderer front-ends connecting to Kafka which is also connected to a ZK cluster) system is so complex and tiresome that people don't expect it to be reconfigurable. However - that needs to change, and we are changing it (again - unless it won't be contained).   {quote} I'm a little skeptical that approaches (1) and (2) should be discounted because they are going to be 'very, very slow.' Certainly it could be a problem, but even on a relatively thin pipe, say 10MB/s, 100 GB of blockchain data could be synced in 3 hours or so. In a LAN environment with a Gbit connection, 100 GB transfers in 15 minutes. Also, for the service deployments, I would not expect the operator to have access to the filesystem of hosts. The 'secure containers' etc. prevent or limit the network admins from executing arbitrary logic like shell commands, and the idea that an operator would have a copy of all user data in order to bootstrap a new orderer sounds unappealing.{quote} Agreed.    {quote}I would point that in actually Kafka suffers from this very issue, in that if the Kafka broker set sufficiently diverges from the initial config (ie, no broker address from the original set is still available), then adding a new OSN is generally no longer possible. It would be nice for us to tackle this issue as well.{quote} This is actually an interesting idea. We might be able to reuse most of the code for this JIRA for the kafka OSN.  {quote}I think we need to take a look at the SDK/user side though too. Do we still allow submission to any channel through any orderer? If not, how does the client know which orderer set to use? Is this maybe something from service discovery then? {quote} If an SDK is written and configured properly, it consumes the discovery service from some peer, and not the connection profile, or some external registry service. Since v1.2 it is possible to  query for the orderers of the channel|https://hyperledger-fabric.readthedocs.io/en/latest/discovery-cli.html  (see "configuration query" section).   ></body> </Action>
<Action id="52303" issue="33806" author="guoger" type="comment" body="Do we encounter similar problem while rebooting a fairly lagged OSN, where certs on other OSNs were rotated?" created="2018-10-16 05:44:49.0" updateauthor="guoger" updated="2018-10-16 05:44:49.0"/>
<Action id="52307" issue="33806" author="yacovm" type="comment" body="The block puller I&apos;m implementing doesn&apos;t use TLS pinning and instead uses standard Fabric auth, so as long as CA certs aren&apos;t rotated and the blocks that are committed update the raft chain object we should be fine" created="2018-10-16 07:12:15.0" updateauthor="yacovm" updated="2018-10-16 07:12:15.0"/>
