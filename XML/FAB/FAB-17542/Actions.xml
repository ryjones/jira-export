<Action id="68122" issue="44423" author="caod" type="comment" created="2020-02-26 20:06:38.0" updateauthor="caod" updated="2020-02-26 20:06:38.0"> <body><! CDATA From out initial investigation of one instance of this flake in CI (see attached logs), we found that in this particular failure, the {{ TestReplicateChainsGreenPath| https://github.com/hyperledger/fabric/blob/master/orderer/common/cluster/replication_test.go#L413  }} is the test that surfaces the 20m timeout. The goroutines imply another 20m timeout from {{deliver_test}}Â around reading from {{ ds.seekAssertions| https://github.com/hyperledger/fabric/blob/master/orderer/common/cluster/deliver_test.go#L184  }} that is likely a side effect of the ReplicateChains timeout.Â We've decided to add a 1min timeout around the {{r.ReplicateChains()}} call in the {{replication_test}} as well as around the deliver server'sÂ {{seekAssertions}} channel to ensure any other tests that might use that path can also timeout properly if {{seekAssertions}} somehow never receives. Regarding the root cause, when comparing the failing logs to a successful test run, the test hangs when attempting to probe for whether to pull from Channel B: {code:java} 2020-02-25T21:12:32.4491922Z  34m2020-02-25 20:52:54.002 UTC  test  channelsToPull -> INFO 3b9 0m Probing whether I should pull channel B 2020-02-25T21:12:32.4492905Z  34m2020-02-25 20:52:54.004 UTC  test  fetchLastBlockSeq -> INFO 3ba 0m 127.0.0.1:36683 is at block sequence of 30 2020-02-25T21:12:32.4493923Z  34m2020-02-25 20:52:54.005 UTC  test  HeightsByEndpoints -> INFO 3bb 0m Returning the heights of OSNs mapped by endpoints map 127.0.0.1:36683:31  2020-02-25T21:12:32.4494984Z  34m2020-02-25 20:52:54.010 UTC  test  fetchLastBlockSeq -> INFO 3bc 0m 127.0.0.1:36683 is at block sequence of 30 2020-02-25T21:12:32.4496061Z  34m2020-02-25 20:52:54.011 UTC  test  connectToSomeEndpoint -> INFO 3bd 0m Connected to 127.0.0.1:36683 with last block seq of 30 2020-02-25T21:12:32.4497019Z  34m2020-02-25 20:52:54.011 UTC  test  obtainStream -> INFO 3be 0m Sending request for block  30  to 127.0.0.1:36683 2020-02-25T21:12:32.4497926Z  34m2020-02-25 20:52:54.013 UTC  test  pullBlocks -> INFO 3bf 0m Got block  30  of size 0 KB from 127.0.0.1:36683 2020-02-25T21:12:32.4498896Z  31m2020-02-25 20:52:54.016 UTC  test  fetchLastBlockSeq -> ERRO 3c0 0m Failed receiving the latest block from 127.0.0.1:36683: EOF 2020-02-25T21:12:32.4499919Z  33m2020-02-25 20:52:54.017 UTC  test  func1 -> WARN 3c1 0m Received error of type 'EOF' from {"CAs":null,"Endpoint":"127.0.0.1:36683"} 2020-02-25T21:12:32.4501115Z  33m2020-02-25 20:52:54.017 UTC  test  connectToSomeEndpoint -> WARN 3c2 0m Could not connect to any endpoint of  {"CAs":null,"Endpoint":"127.0.0.1:36683"}  {code} The aboveÂ {{Failed receiving the latest block from 127.0.0.1:36683: EOF}} implies that we were unable to fetch the configuration block  21  which would determine if channel B could be pulled from. In a successful test we would expect the following lines when probing B (notice we successfully pulled the configuration block  21  indicating we do not belong in Channel B):  {code:java} 2020-02-26 15:04:44.743 EST  test  channelsToPull -> INFO 01e Probing whether I should pull channel B 2020-02-26 15:04:44.744 EST  test  fetchLastBlockSeq -> INFO 01f 127.0.0.1:57953 is at block sequence of 30 2020-02-26 15:04:44.744 EST  test  HeightsByEndpoints -> INFO 020 Returning the heights of OSNs mapped by endpoints map 127.0.0.1:57953:31  2020-02-26 15:04:44.745 EST  test  fetchLastBlockSeq -> INFO 021 127.0.0.1:57953 is at block sequence of 30 2020-02-26 15:04:44.745 EST  test  connectToSomeEndpoint -> INFO 022 Connected to 127.0.0.1:57953 with last block seq of 30 2020-02-26 15:04:44.745 EST  test  obtainStream -> INFO 023 Sending request for block  30  to 127.0.0.1:57953 2020-02-26 15:04:44.745 EST  test  pullBlocks -> INFO 024 Got block  30  of size 0 KB from 127.0.0.1:57953 2020-02-26 15:04:44.746 EST  test  fetchLastBlockSeq -> INFO 025 127.0.0.1:57953 is at block sequence of 30 2020-02-26 15:04:44.746 EST  test  connectToSomeEndpoint -> INFO 026 Connected to 127.0.0.1:57953 with last block seq of 30 2020-02-26 15:04:44.746 EST  test  obtainStream -> INFO 027 Sending request for block  21  to 127.0.0.1:57953 2020-02-26 15:04:44.746 EST  test  pullBlocks -> INFO 028 Got block  21  of size 0 KB from 127.0.0.1:57953 2020-02-26 15:04:44.746 EST  test  channelsToPull -> INFO 029 I do not belong to channel B or am forbidden pulling it (not in the channel), skipping chain retrieval {code} The 1min timeout should fail faster when we encounter this bug again but it would be nice to do more investigation into the root cause for why we could not receive the configuration block.  ></body> </Action>
<Action id="68123" issue="44423" author="caod" type="comment" body="On investigating other errors related to timing out in this test, it doesn&apos;t seem to be limited to the ReplicationTest but all seem to timeout on the seekAssertions. We&apos;re going to remove the timeout for ReplicateChains and instead just leave the one on seekAssertions as that should cover every case that seems to be timing out when seekAssertions is not receiving." created="2020-02-26 20:31:17.0" updateauthor="caod" updated="2020-02-26 20:31:17.0"/>
<Action id="68124" issue="44423" author="tsharris" type="comment" body=" https://github.com/hyperledger/fabric/pull/749 " created="2020-02-26 20:39:44.0" updateauthor="tsharris" updated="2020-02-26 20:39:44.0"/>
<Action id="68125" issue="44423" author="caod" type="comment" body="I thought about it some more and I think I have a theory regarding the root cause. This is just a hunch but I&apos;m thinking the deliver server&apos;s  Deliver| https://github.com/hyperledger/fabric/blob/master/orderer/common/cluster/deliver_test.go#L171-L198  Â is being called more times than expected (whether due to some retry logic from any failure that may happen in that path). Each time Deliver is called it pulls a seekAssertion off the channel (we only put a set number of seekAssertions on the channel during setup, and that&apos;s assuming nothing in the test flakes out normally). Each failure that causes a retry or a new unexpected call to Deliver is thus pulling another seekAssertion off the channel until eventually it&apos;s empty and thus hangs waiting for another seekAssertion (but there won&apos;t be any more since we only put an expected number in the channel). If that&apos;s truly what&apos;s causing the hanging then I&apos;d propose we should look into a way of maintaining the number of seekAssertions remaining whenever a Deliver needs to be called again." created="2020-02-26 20:48:42.0" updateauthor="caod" updated="2020-02-26 20:50:20.0"/>
<Action id="68127" issue="44423" author="yacovm" type="comment" created="2020-02-26 22:43:14.0" updateauthor="yacovm" updated="2020-02-26 22:43:14.0"> <body><! CDATA {quote}I'd propose we should look into a way of maintaining the number of seekAssertions remaining whenever a Deliver needs to be called again. {quote} Â   The seek assertions are there to ensure the block pulling logic is behaving as expected.Â  Since the test dictates what kind of block to send according to the seek sent from the production code, then won't it mask (in theory) possible problems that may occur in production, where the responses to the seeks are not controlled by the test?  Â   If you ask me, these "Received error of type EOF" are probably due to the server side returning nil, hence tearing down the gRPC stream.  This happens because of putting nil in the channel as seen below: {code:java} // Orderer returns its last block is 30. // This is needed to get the latest height by comparing among all orderers. osn.enqueueResponse(30)  // First we poll for the block sequence we got previously again, from some orderer. osn.addExpectProbeAssert() osn.enqueueResponse(30)  // And afterwards pull the block from the first orderer. osn.addExpectPullAssert(30) osn.blockResponses <- &orderer.DeliverResponse{ Type: &orderer.DeliverResponse_Block{Block: block30WithConfigBlockOf21}, } // And the last config block is pulled via reconnecting to the orderer. osn.blockResponses <- nil {code} In conjunction to the deliver blocks method which returns nil in case a nil is in the channel: {code:java} func (ds *deliverServer) deliverBlocks(stream orderer.AtomicBroadcast_DeliverServer) error { for { blockChan := ds.blocks() response := <-blockChan // A nil response is a signal from the test to close the stream. // This is needed to avoid reading from the block buffer, hence // consuming by accident a block that is tabled to be pulled // later in the test. if response == nil { return nil } if err := stream.Send(response); err != nil { return err } } } {code} Now, that was on purpose because it's a hacky way of making the gRPC test server to close the stream after delivering the block.  I think that it is best to mock the gRPC service here (it is done in all kinds of places in the Fabric code base, when needed, but not here) and see if it solves the flakes for this test.  ></body> </Action>
<Action id="68139" issue="44423" author="sykesm" type="comment" created="2020-02-27 14:43:42.0" updateauthor="sykesm" updated="2020-02-27 14:43:42.0"> <body><! CDATA Mitigation was added in https://github.com/hyperledger/fabric/pull/749. The change simply prevents the blocking channel receive to hang out for more than a minute before killing the tests.  This change is *not* a fix; merely a mitigation to prevent 20 minute hangs during test in CI.  ></body> </Action>
