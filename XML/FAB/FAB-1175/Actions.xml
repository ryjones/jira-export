<Action id="20287" issue="13508" author="nishi" type="comment" created="2017-01-06 17:48:18.0" updateauthor="nishi" updated="2017-01-06 20:32:33.0"> <body><! CDATA It was observed that with 1000databases inserting 1000records in each DB, test crashed eventually with OutOfMemory error.  More Details on memory error can be found at: https://ibm.ent.box.com/notes/108909449546  Hence ran following benchmark tests with 1->500 databases w.r.t. leveldb performance.   1->10000 writes/reads were performed during these tests. Each write/read is 1000bytes in length. Writes were done using batch and applying these batches on db with a savepoint.   Please find attached 3 files:  1. mdb_mtx_leveldb_benchmark_test.go (go benchmark test program)  2. "levelDB_testOutput_500DB_1_To_1000tx_Each" has results on: Creation 1-500 databases Write 1-1000 transactions in each of these 500 databases (writes/reads happen after 500DBs are created successfully) Read 1-1000 transactions from each of these 500 databases.  3. "leveldb_1DB_1_To_10000TX_benchmark_Output" has results on: 1 DB Write 1-10000 transactions from single DB Read 1-10000 transactions from single DB     ></body> </Action>
<Action id="20367" issue="13508" author="denyeart" type="comment" body="Due to the overhead per database on leveldb, it was decided to refactor the code to have one database store the data for all the chains." created="2017-01-16 11:23:52.0" updateauthor="denyeart" updated="2017-01-16 11:23:52.0"/>
