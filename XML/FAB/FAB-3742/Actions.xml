<Action id="24573" issue="16493" author="yacovm" type="comment" created="2017-05-29 05:42:24.0" updateauthor="yacovm" updated="2017-05-29 05:42:24.0"> <body><! CDATA This certainly a "needs-review" one but I think we should include this. * From what I remember (I reviewed the CC cache change set at that time) - If we add good enough unit tests that test the life cycle in the presence of expiration, we can be sure it doesn't beak anything * Peers are supposed to run indefinitely and I don't think we should let a module consume an unlimited amount of memory.   ></body> </Action>
<Action id="24601" issue="16493" author="christopherferris" type="comment" body="Wouldn&apos;t shooting in the head periodically solve the problem? Yes, it would be great if the peer ran indefinitely, but that seems an unrealistic goal. We should design it such that it can sustain a brief outage (e.g. can be quickly restarted)." created="2017-05-29 18:17:20.0" updateauthor="christopherferris" updated="2017-05-29 18:17:20.0"/>
<Action id="24602" issue="16493" author="christopherferris" type="comment" body="Also, I am not convinced this is a bug, per se as much as a poor design;-)" created="2017-05-29 18:17:56.0" updateauthor="christopherferris" updated="2017-05-29 18:17:56.0"/>
<Action id="24624" issue="16493" author="yacovm" type="comment" created="2017-05-30 07:38:22.0" updateauthor="yacovm" updated="2017-05-31 10:37:45.0"> <body><! CDATA {quote}Wouldn't shooting in the head periodically solve the problem? Yes, it would be great if the peer ran indefinitely, but that seems an unrealistic goal. We should design it such that it can sustain a brief outage (e.g. can be quickly restarted).{quote}  We could stop the merging of the fix that prevents none authorized stop of the peer via gRPC and call that a feature that recycles this cache.  Now, jokes aside - I think that adding code that periodically restarts isn't a good idea from a couple of reasons: * If the peer is a leader peer and is restarted, it may cause a view divergence in the rest of the peers and that would cause them to react and start electing a new leader. Eventually the system would stabilize but this is redundant churn and would just look "weird" from a macro view of operations. * If we restart in the middle of a transaction simulation the gRPC call of the client would fail and that is a client impact. * If the code that periodically restarts the peer is put inside the peer and shoots itself in the head - then we need additional code that monitors the peer and restarts it. This is complex. * If the code that periodically restarts the peer is only outside, it can't do so in times when a transaction simulation isn't in progress.   From what I've seen in the code (IIUC, please correct me if I'm wrong) , the chaincode deployment spec is part of this object that is cached and this can be a considerable amount in some use cases. I think that we should insert simple logic that if the cache size is greater than some threshold, it empties the cache. Such an addition is easily tested and is confined to one file and I think is better than shooting the node in the head.  Another idea is to have a list that when some item is either added or retrieved from the list, it is put to the top of the list, and if the list reaches a threshold - we remove the last item from the list, keeping an invariant that the list is smaller than a certain size.  This is less poor design but more like  lenient code review |https://gerrit.hyperledger.org/r/#/c/8313/11/core/common/ccprovider/ccinfocache.go@37  and I think that it should be classified as a bug because it actually may crash the peer with outofmemory.  ></body> </Action>
<Action id="24727" issue="16493" author="denyeart" type="comment" body="How large is the cache per chaincode instantiated/upgraded?  It seems the fix could wait until a subsequent release, unless you convince me that the cache is of such significant size that there is real risk of memory issues in 1.0 timeframe." created="2017-05-31 05:06:36.0" updateauthor="denyeart" updated="2017-05-31 05:06:36.0"/>
<Action id="24732" issue="16493" author="yacovm" type="comment" created="2017-05-31 07:25:42.0" updateauthor="yacovm" updated="2017-05-31 07:44:57.0"> <body><! CDATA {quote}How large is the cache per chaincode instantiated/upgraded? It seems the fix could wait until a subsequent release, unless you convince me that the cache is of such significant size that there is real risk of memory issues in 1.0 timeframe. {quote} Dave, that's a good question.  I did the following experiment: I added some lines to print the size of the content of the object that is cached (I omitted the length of the hashes since it's ~ 32 bytes each hash): {code:java} func (c *ccInfoCacheImpl) GetChaincode(ccname string, ccversion string) (CCPackage, error) { 	// c.cache is guaranteed to be non-nil  	key := ccname + "/" + ccversion  	c.RLock() 	ccpack, in := c.cache key  	c.RUnlock()  	if !in { 		var err error  		// the chaincode data is not in the cache 		// try to look it up from the file system 		ccpack, err = c.ccfs.GetChaincode(ccname, ccversion) 		if err != nil || ccpack == nil { 			return nil, fmt.Errorf("cannot retrieve package for chaincode %s/%s, error %s", ccname, ccversion, err) 		}  		// we have a non-nil CCPackage, put it in the cache 		c.Lock() 		c.cache key  = ccpack 		c.Unlock()  		cp := ccpack.(*CDSPackage) 		b, _ := proto.Marshal(cp.depSpec) 		size := len(cp.id) + len(cp.buf) + len(cp.datab) + len(b) 		fmt.Println("<<<<<", size) fmt.Println("<<<", len(cp.id), len(cp.buf), len(cp.datab), len(b)) 	}  	return ccpack, nil } {code} Then I deployed the code to my test env and looked at the logs of the peers and it was printed: {quote}<<<<< 21671112  <<< 32 10835525 68 10835525 {quote} The size of the chaincode *.go* file that I deployed is: *2.2K*  but it seems that somehow the size of the object that is stored is *20MB*.  ></body> </Action>
<Action id="24749" issue="16493" author="denyeart" type="comment" body="Thanks  ~yacovm  at 20MB per, I do see cause for concern.  Next question - if you have one chaincode instantiated on 100 channels, does the cache grow 1x or 100x?" created="2017-05-31 11:48:20.0" updateauthor="denyeart" updated="2017-05-31 11:48:20.0"/>
<Action id="24751" issue="16493" author="yacovm" type="comment" created="2017-05-31 12:04:43.0" updateauthor="yacovm" updated="2017-05-31 19:53:50.0"> <body><! CDATA The cache is for chaincodes, and is orthogonal for channels. Also verified this by adding another channel and doing the same from scratch - saw only print such as above so I conclude the 2nd Get was a cache hit and not a miss.  But I'd argue that 20MB per CC is too much. If the peer has 10 different CCs it means 200 MB  Perhaps I have something weird in my env? could you try to reproduce this locally?   ~muralisr  how does this get to 20MB? I thought the CDS only has the .go code which is 2.2KB. Edit - OK, so it's actually the GOPATH itself and not just the CC.  ></body> </Action>
<Action id="24782" issue="16493" author="binhn" type="comment" created="2017-05-31 15:37:30.0" updateauthor="binhn" updated="2017-05-31 15:37:30.0"> <body><! CDATA -1  I don't think we need to worry about the cc info cache at this point – too small to be concerned, and it can be improved post 1.0 without causing any changes else where.   ~yacovm  the 20m is the cc image, not the cache. FAB-2493 is an attempt to reduce the size of the total cc code.  ></body> </Action>
<Action id="24796" issue="16493" author="yacovm" type="comment" created="2017-05-31 16:21:03.0" updateauthor="yacovm" updated="2017-05-31 17:01:42.0"> <body><! CDATA I see. Well I did a small experiment - I installed the same CC under 50 names (a bash loop from 1 to 50) and then instantiated all CCs under the same channel. My test env has system-related monitoring (monitors CPU, memory, IO, etc.) How would you explain the following graph? !Capture.PNG|thumbnail!   ></body> </Action>
<Action id="24798" issue="16493" author="yacovm" type="comment" created="2017-05-31 16:29:38.0" updateauthor="yacovm" updated="2017-05-31 16:38:49.0"> <body><! CDATA These are the other peers, for reference - I didn't install the CC on them, and I killed all the containers on  peer0 afterwards  !Capture2.PNG|thumbnail!   The chaincode's init method, btw, is: {code} func (t *BenchmarkChaincode) Init(stub shim.ChaincodeStubInterface) pb.Response { return shim.Success(nil) } {code}  So I don't think there should be anything interesting in the blocks created for the channel.  ></body> </Action>
<Action id="24801" issue="16493" author="yacovm" type="comment" created="2017-05-31 16:52:49.0" updateauthor="yacovm" updated="2017-05-31 17:23:04.0"> <body><! CDATA Now, just to be sure- next thing I did is disable the cache in *peer/node/start.go* : {quote}	 // enable the cache of chaincode info //ccprovider.EnableCCInfoCache() {quote}  and did the whole thing again, and now the memory graph doesn't show an increase: !Capture3.PNG|thumbnail!   (ignore the start, I had a compilation error at first so had to redeploy...)  I think this clearly proves that there is something not right going on. Can't we just either put an LRU eviction from the cache or (the simplest solution) - disable it with commenting out the line above?   ~binhn  are you saying that FAB-2493 would fix this issue?  ></body> </Action>
<Action id="24840" issue="16493" author="christopherferris" type="comment" body="+1 to disabling for now." created="2017-05-31 18:42:13.0" updateauthor="christopherferris" updated="2017-05-31 18:42:13.0"/>
<Action id="24855" issue="16493" author="binhn" type="comment" created="2017-05-31 20:54:13.0" updateauthor="binhn" updated="2017-05-31 20:58:15.0"> <body><! CDATA  ~yacovm  It looks like we cache the code.   ~ales  Why would we need the code? We should nil it out from the cached CDS.  In any case, we shouldn't be concerned about this until  FAB-2493 is in to see what the size be. If only a few mbytes, I would post 1.0  ></body> </Action>
<Action id="24857" issue="16493" author="c0rwin" type="comment" body="It seems pretty dangerous and IMO should be disabled voting +1." created="2017-05-31 21:08:17.0" updateauthor="c0rwin" updated="2017-05-31 21:08:17.0"/>
<Action id="24930" issue="16493" author="ales" type="comment" created="2017-06-01 14:00:34.0" updateauthor="ales" updated="2017-06-01 14:00:34.0"> <body><! CDATA Disabling the cache is not a good idea IMHO because that would mean that *for every* proposal we would need to do a trip to the fs, Granted, in some cases we might benefit from cache hits from the fs page cache, but that's a gamble that I wouldn't be willing to make.  Nilling out the code field would be an option if we used it only from the endorser to check instantiation policies, but we ended up using the cache from every code path (and not only from the endorse path that doesn't require accessing the code field) and so that's also not viable.  We could put a cap on the number of cached items and implement some purging policies (simplest of which would be random purging which would require a handful of lines of code to be changed and probabilistically should still converge to caching what is useful).  On a side note: if memory kept by cc instances that have been upgraded is an issue, why aren't we concerned about leaking whole containers that we don't stop and kill when we upgrade?  ></body> </Action>
<Action id="24934" issue="16493" author="c0rwin" type="comment" created="2017-06-01 14:09:48.0" updateauthor="c0rwin" updated="2017-06-01 14:09:48.0"> <body><! CDATA Well, we need to compare performance with and without a cache to be able to state something more educative rather than speculating on intuition. I think that potential infinite memory consumption is a way more dangerous than having peer reading from disk on each proposal. Having stable system is far more important at this point.  And yes, I'd agree that generally have limit capacity cache would be a perfect option, but speaking of release and time constraints I still thinking that disabling cache will be better for 1.0 release, unless we would like to take a risk and do add constraints to cache size.  ></body> </Action>
<Action id="24939" issue="16493" author="ales" type="comment" created="2017-06-01 14:20:58.0" updateauthor="ales" updated="2017-06-01 14:20:58.0"> <body><! CDATA I'm not 100% sure we need benchmarks to establish that a trip to the kernel's fs layer (and potentially to disk) is slower than a hit in a memory cache.  That said, I understand the concern. How big of a risk do you think that my proposal of a max cache size and random evictions carries?  ></body> </Action>
<Action id="24942" issue="16493" author="c0rwin" type="comment" created="2017-06-01 14:29:47.0" updateauthor="c0rwin" updated="2017-06-01 14:29:47.0"> <body><! CDATA Reading from disk is definitely could slow down the overall throughput, while two main questions to ask:  1. How significantly it will slow down? 2. Whenever reading from disk is a bottleneck at current point in time?    ></body> </Action>
<Action id="24943" issue="16493" author="ales" type="comment" body="I like the idea of LRU better of course and could implement that as well. Though I&apos;m concerned about the fact that the code changes would be larger (random eviction is going to be much smaller)." created="2017-06-01 14:30:53.0" updateauthor="ales" updated="2017-06-01 14:30:53.0"/>
<Action id="24944" issue="16493" author="ales" type="comment" body=" ~C0rWin  I don&apos;t object disabling the cache, it&apos;s a one line change. What I&apos;m saying is that if we can introduce a max cache size and benefit from this cache without introducing too much risk, I&apos;d like use to at least discuss the option before dismissing it altogether." created="2017-06-01 14:34:14.0" updateauthor="ales" updated="2017-06-01 14:34:14.0"/>
<Action id="24945" issue="16493" author="c0rwin" type="comment" body="Having random eviction you will loose all benefit of the cache, not sure how this less intrusive. IMO no big different between having LRU and random eviction. As a starter probably we can simply limit the cache size and start to retain items once limit exceeds the limit, from the bottom/top of the list. " created="2017-06-01 14:38:32.0" updateauthor="c0rwin" updated="2017-06-01 14:38:32.0"/>
<Action id="24946" issue="16493" author="yacovm" type="comment" created="2017-06-01 14:40:46.0" updateauthor="yacovm" updated="2017-06-01 14:43:18.0"> <body><! CDATA Once  ~greg.haskins 's change set gets in: * Not all CDSs would be created equal so some will be big, some will be small, and we would need to look at the total size of the cache and not the item number in it. * I think that if we don't want to implement LRU and instead we would do random evictions, we would end up needing to evict entries until the cache is small enough, since, again- not all CDSs would be created equal. Therefore it would make sense to have an LRU because it's easy to just evict the tail of a list until the cache is small enough, then to pick a random element from a map, until the cache is small enough (because picking a random item from a map requires iterating over it...) * I think it's worth to do the check I did again after it gets merged (can anyone notify me on this please in case I miss it?)  ></body> </Action>
<Action id="24947" issue="16493" author="ales" type="comment" body="good point, but that&apos;s no longer a small change (though it&apos;s not clear to me what change-size is acceptable at this stage)." created="2017-06-01 14:43:16.0" updateauthor="ales" updated="2017-06-01 14:43:16.0"/>
<Action id="24959" issue="16493" author="ales" type="comment" created="2017-06-01 15:54:50.0" updateauthor="ales" updated="2017-06-01 15:54:50.0"> <body><! CDATA so, just to wrap up, here's the options on the table: # disable the cache; PROS: simplest change; CONS: one trip to the fs for every single proposal; # implement max number of entries with random eviction; PROS we benefit from the cache, simple change, size of cache can't grow infinitely; CONS: eviction policy isn't optimal # implement max cache size (in bytes) with LRU; PROS: we benefit from the cache, size of cache is predictable; CONS: more complex change  Thoughts?  ></body> </Action>
<Action id="24963" issue="16493" author="denyeart" type="comment" body="At this stage, I would vote for option #2, and keep it very simple for now, e.g. hardcode max size of 100 chaincodes in cache, and upon the 101st evict another.  We don&apos;t need to consider size per entry for the first impl, just keep it extremely simple.  Then in next release make it LRU." created="2017-06-01 16:15:44.0" updateauthor="denyeart" updated="2017-06-01 16:15:44.0"/>
<Action id="25039" issue="16493" author="muralisr" type="comment" body=" ~ales  isn&apos;t there an option 4 ? Currently we only need the InstantiationPolicy. Instead of caching the entire package (with the CDS) why don&apos;t we just cache just ChaincodeData (contains InstantiationPolicy and a few sundry fields)" created="2017-06-01 23:02:03.0" updateauthor="muralisr" updated="2017-06-01 23:02:03.0"/>
<Action id="25059" issue="16493" author="binhn" type="comment" created="2017-06-02 02:29:38.0" updateauthor="binhn" updated="2017-06-02 02:29:38.0"> <body><! CDATA  ~ales   I removed the chaincode source from the cache by insert 1 line into https://github.com/hyperledger/fabric/blob/master/core/common/ccprovider/ccinfocache.go#L94  {{ depSpec.CodePackage = nil}}  and successfully ran `make unit-test` and `e2e_cli`      ~yacovm   Could you give that a try and see how the memory consumption effected.     ></body> </Action>
<Action id="25076" issue="16493" author="yacovm" type="comment" created="2017-06-02 10:02:24.0" updateauthor="yacovm" updated="2017-06-02 10:03:10.0"> <body><! CDATA The cache is populated in *GetChaincode* too, not only in *PutChaincode*. When I put {code} depSpec.CodePackage = nil {code}  It still climbs.   When I also do: {code} 		cp.buf = nil 		cp.depSpec.CodePackage = nil {code} (These are the big byte slices) - it doesn't work.... complains at instantiate that: {quote} Error starting container: Failed to generate platform-specific docker build: Error returned from build: 1 "can't load package: package github.com/hyperledger/fabric/examples/chaincode/go/metrics: cannot find package "github.com/hyperledger/fabric/examples/chaincode/go/metrics" in any of: /opt/go/src/github.com/hyperledger/fabric/examples/chaincode/go/metrics{quote}  I deleted these 2 lines in *GetChaincode* and verified that it works again (but memory still climbs)  ></body> </Action>
<Action id="25081" issue="16493" author="muralisr" type="comment" created="2017-06-02 11:50:28.0" updateauthor="muralisr" updated="2017-06-02 11:50:28.0"> <body><! CDATA  ~binhn  if the cache was used just to check for the one intended purpose, we could nil out other data. But the func call replaces the old one so all calls go through the cache.   My suggestion above was basically create a separate cache call which will cache only ChaincodeData and used that only for this purpose.   ></body> </Action>
<Action id="25116" issue="16493" author="yacovm" type="comment" created="2017-06-02 14:44:50.0" updateauthor="yacovm" updated="2017-06-02 14:45:18.0"> <body><! CDATA  ~binhn  said in the scrum:  {quote} @muralisr i am still not clear on where the changes should be as @aso mentioned that the cache is also used in validation and commit paths if we only use the cache on endorsement, then i agreed with your assessment that it is only here during instantiation and if only instantiation, why would we bother caching? instantiation doesn't happen often {quote}  He then continued and proposed to disable the cache.  I think we should start voting on disabling the cache (I'm for doing it)  ~C0rWin   ~binhn   ~ChristopherFerris   ~muralisr  and others...    ></body> </Action>
<Action id="25127" issue="16493" author="muralisr" type="comment" body=" ~binhn  et all... let me put out a  WIP  CR so you can assess the options. And if I find I was wrong, I&apos;ll report that too :-)" created="2017-06-02 15:26:42.0" updateauthor="muralisr" updated="2017-06-02 15:26:42.0"/>
<Action id="25146" issue="16493" author="yacovm" type="comment" created="2017-06-02 17:22:14.0" updateauthor="yacovm" updated="2017-06-02 17:24:29.0"> <body><! CDATA ok, I have good news! I pulled latest master after  ~greg.haskins 's change set has been merged and the numbers are great:  This is the size of the contents of the object cached {quote}<<< 32 1187 68 1187 <<<<< 2474 {quote}  While before it was: {quote} <<<<< 21671112 <<< 32 10835525 68 10835525 {quote}  And this is the graph now: !Capture6.PNG|thumbnail!   The memory before the sink is when the peer CLI was compiled with the old version and the memory after the sink is much much lower as you see, where the peer CLI was compiled with the latest master   ></body> </Action>
<Action id="25147" issue="16493" author="binhn" type="comment" created="2017-06-02 17:30:26.0" updateauthor="binhn" updated="2017-06-02 17:30:26.0"> <body><! CDATA  ~yacovm   Thanks for great news.  With this info, I would vote to defer enhancing the cache to post 1.0  ></body> </Action>
<Action id="25148" issue="16493" author="muralisr" type="comment" body=" ~yacovm  it really depends upon the chaincode and its dependencies. Previously we were including the world. But now we are including just the chaincode and its exact dependencies." created="2017-06-02 17:31:10.0" updateauthor="muralisr" updated="2017-06-02 17:31:10.0"/>
<Action id="25239" issue="16493" author="muralisr" type="comment" created="2017-06-03 21:57:25.0" updateauthor="muralisr" updated="2017-06-03 21:57:25.0"> <body><! CDATA Please checkout https://gerrit.hyperledger.org/r/#/c/10121/ for the "minimal" cache discussed in scrum and implements "option 4" mentioned a few comments back.  {code:java} Alessandro Sorniotti isn't there an option 4 ? Currently we only need the InstantiationPolicy. Instead of caching the entire package (with the CDS) why don't we just cache just ChaincodeData (contains InstantiationPolicy and a few sundry fields) {code}  This completely eliminates chaincode code from the cache.  ></body> </Action>
<Action id="25255" issue="16493" author="muralisr" type="comment" created="2017-06-04 12:44:02.0" updateauthor="muralisr" updated="2017-06-04 12:44:02.0"> <body><! CDATA   ~ales  ~binhn   ~yacovm  appreciate if you can take a look...    ~yacovm  perhaps you can run your tests and generate numbers with this change ?  ></body> </Action>
<Action id="25256" issue="16493" author="yacovm" type="comment" created="2017-06-04 12:56:46.0" updateauthor="yacovm" updated="2017-06-04 13:00:18.0"> <body><! CDATA   {quote} ~yacovm  perhaps you can run your tests and generate numbers with this change ? {quote}   # The cache is already very small now thanks to Greg's change set. I think that refactoring the code just to decrease the cache size even more is not a cost-effective move in terms of risk-reward. We could use your change set post v1.0, however. And there we need to implement cache eviction. # I'm using the environment now for performance evaluation of gossip. When I did the tests that provided the graphs it was a pain to switch the code as it used different git remotes, diverged branches, etc... You can do what I did with the fmt.Print's and install 50 chaincodes and instantiate them. I don't think that graphs is a must for this scenario.  ></body> </Action>
<Action id="25258" issue="16493" author="muralisr" type="comment" created="2017-06-04 14:33:40.0" updateauthor="muralisr" updated="2017-06-04 14:33:40.0"> <body><! CDATA  ~yacovm  The cache depends upon the chaincode size. For anything other than toy chaincodes it could be significant. The changeset eliminates the size of the chaincode as a variable. I think its worth taking a look and then decide if its we want to include this for 1.0.    ></body> </Action>
<Action id="25341" issue="16493" author="muralisr" type="comment" created="2017-06-05 14:19:49.0" updateauthor="muralisr" updated="2017-06-05 14:46:30.0"> <body><! CDATA The CR https://gerrit.hyperledger.org/r/#/c/10121/  is totally internal with no API or visible user changes. This can be investigated post 1.0 given other items with higher priority.  We don't have to do anything for this JIRA but I can go either way.... it has some benefits, just risk vs rewards at this point. Given myself -2 on the gerritt just in case.  ></body> </Action>
<Action id="25356" issue="16493" author="angelo.decaro" type="comment" created="2017-06-05 14:57:39.0" updateauthor="angelo.decaro" updated="2017-06-05 14:57:39.0"> <body><! CDATA +1 for https://gerrit.hyperledger.org/r/#/c/10121/  The changes are easy to follow and it reduces the amount of information stored in the cache.  ></body> </Action>
<Action id="25436" issue="16493" author="christopherferris" type="comment" body="This CR doesn&apos;t address running out of space. Just pushes off the date when we hit the problem, unless I am missing something." created="2017-06-05 20:55:09.0" updateauthor="christopherferris" updated="2017-06-05 20:55:09.0"/>
<Action id="25456" issue="16493" author="muralisr" type="comment" created="2017-06-05 23:41:41.0" updateauthor="muralisr" updated="2017-06-05 23:41:41.0"> <body><! CDATA  ~ChristopherFerris  No, you aren't missing anything.  The JIRA should be divided into two tasks (1) its caching too much (which is how it was noticed in the first place and (2) it doesn't have a policy for purging cache.   The CR I put out just addresses (1) and takes chaincode size out of the equation. We still need (2).  If it makes it easier to manage,we could just create a "fabric is caching CC Package when all it needs its CC policy" bug and associated https://gerrit.hyperledger.org/r/#/c/10121/  with it.  ></body> </Action>
<Action id="25520" issue="16493" author="binhn" type="comment" created="2017-06-06 15:27:59.0" updateauthor="binhn" updated="2017-06-06 15:27:59.0"> <body><! CDATA  ~muralisr  {quote}The CR I put out just addresses (1) and takes chaincode size out of the equation. We still need (2). {quote} Let's create an improvement JIRA for that, and move this item to post 1.0  ></body> </Action>
<Action id="25752" issue="16493" author="muralisr" type="comment" body=" ~binhn  created improvement JIRA https://jira.hyperledger.org/browse/FAB-4473." created="2017-06-08 15:14:32.0" updateauthor="muralisr" updated="2017-06-08 15:14:32.0"/>
<Action id="29598" issue="16493" author="elli-androulaki" type="comment" body="So, what do we do about this item now? Should it be post v1.1? Not sure it is related to fabric-crypto actually :)" created="2017-08-09 13:22:20.0" updateauthor="elli-androulaki" updated="2017-08-09 13:22:20.0"/>
<Action id="29621" issue="16493" author="muralisr" type="comment" body=" ~elli-androulaki   the cache size is much smaller than chaincode now that we are storing just the &quot;ChaincodeData&quot; part (counted 68 bytes for chaincode_example02). Given that, my vote would be to close this." created="2017-08-09 16:40:35.0" updateauthor="muralisr" updated="2017-08-09 16:40:35.0"/>
<Action id="50333" issue="16493" author="elli-androulaki" type="comment" body="hi, any updates on this one? :)  For now, the item is being marked as &quot;Future&quot; but just object if you think it needs to be moved to 1.4 or be closed." created="2018-09-12 09:44:10.0" updateauthor="elli-androulaki" updated="2018-09-12 09:45:12.0"/>
<Action id="51152" issue="16493" author="elli-androulaki" type="comment" created="2018-09-25 12:30:15.0" updateauthor="elli-androulaki" updated="2018-09-25 12:30:15.0"> <body><! CDATA Closing as no-longer relevant;  ~ales  said this cache no longer exists.     ></body> </Action>
<Action id="51153" issue="16493" author="elli-androulaki" type="comment" body="Not relevant any more." created="2018-09-25 12:30:50.0" updateauthor="elli-androulaki" updated="2018-09-25 12:30:50.0"/>
