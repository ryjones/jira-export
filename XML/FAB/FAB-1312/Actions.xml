<Action id="20077" issue="13719" author="nishi" type="comment" created="2016-12-13 15:27:52.0" updateauthor="nishi" updated="2016-12-13 15:48:47.0"> <body><! CDATA After increasing timeout on test "go test -timeout 240m" all 1000 records were inserted successfully into each of the 1000 databases.  Upon subsequent reading of these 1000 records from each database test ended abruptly with "signal killed" message  from web on "signal killed" : "If the user or sysadmin did not kill the program the kernel may have. The kernel would only kill a process under exceptional circumstances such as extreme resource starvation (think mem+swap exhaustion)."  On running - dmesg | grep -E -i -B100 'killed process'  15679.524422  Out of memory: Kill process 5195 (lockbasedtxmgmt) score 942 or sacrifice child  15679.524641  Killed process 5195 (lockbasedtxmgmt) total-vm:5570356kB, anon-rss:3815248kB, file-rss:1104kB  After increasing vagrant base memory inside virtual box from 4096MB to 8192MB at the time of vagrant startup, test ran for a little longer time and processed/read few extra records but eventually crashed without completing processing/reading all 1000 records from 1000 databases.    ></body> </Action>
<Action id="20080" issue="13719" author="nishi" type="comment" created="2016-12-13 16:58:45.0" updateauthor="nishi" updated="2016-12-13 16:58:45.0"> <body><! CDATA updated /etc/sysctl.conf to include  vm.overcommit_memory=1 vm.overcommit_ratio=50  Resulting in out of memory errors. Looks like VM has to be fine tuned with right set of parameters to be able to use and swap memory efficiently  Processed so far 20 databases Processing/Reading record 0 in db db_336: fatal error: runtime: out of memory  runtime stack: runtime.throw(0x989bbb, 0x16) /opt/go/src/runtime/panic.go:566 +0x95 runtime.sysMap(0xc595290000, 0xf6d0000, 0x0, 0xc3e458) /opt/go/src/runtime/mem_linux.go:219 +0x1d0 runtime.(*mheap).sysAlloc(0xc239a0, 0xf6d0000, 0x0) /opt/go/src/runtime/malloc.go:407 +0x37a runtime.(*mheap).grow(0xc239a0, 0x7b63, 0x0) /opt/go/src/runtime/mheap.go:726 +0x62 runtime.(*mheap).allocSpanLocked(0xc239a0, 0x7b63, 0x8000) /opt/go/src/runtime/mheap.go:630 +0x4f2 runtime.(*mheap).alloc_m(0xc239a0, 0x7b63, 0x100000000, 0x7fc6bba59288) /opt/go/src/runtime/mheap.go:515 +0xe0 runtime.(*mheap).alloc.func1() /opt/go/src/runtime/mheap.go:579 +0x4b runtime.systemstack(0x7fc6bdceee18) /opt/go/src/runtime/asm_amd64.s:314 +0xab runtime.(*mheap).alloc(0xc239a0, 0x7b63, 0x100000000, 0x4235ae) /opt/go/src/runtime/mheap.go:580 +0x73 runtime.largeAlloc(0xf6c6000, 0x7fc6bdceee00, 0x454d6a) /opt/go/src/runtime/malloc.go:774 +0x93 runtime.mallocgc.func1() /opt/go/src/runtime/malloc.go:669 +0x3e runtime.systemstack(0xc42001b500) /opt/go/src/runtime/asm_amd64.s:298 +0x79 runtime.mstart() /opt/go/src/runtime/proc.go:1079  goroutine 5094  running : runtime.systemstack_switch() /opt/go/src/runtime/asm_amd64.s:252 fp=0xc517711b70 sp=0xc517711b68 runtime.mallocgc(0xf6c6000, 0x0, 0x0, 0x1) /opt/go/src/runtime/malloc.go:670 +0x903 fp=0xc517711c10 sp=0xc517711b70 runtime.growslice(0x8baa20, 0xc588d20000, 0xc5690e8, 0xc56a000, 0xc56b8b4, 0x27b8, 0x3000, 0xc540fa3200) /opt/go/src/runtime/slice.go:126 +0x24e fp=0xc517711ca0 sp=0xc517711c10 testing.(*common).log(0xc42007a180, 0xc540fa3200, 0x27b8) /opt/go/src/testing/testing.go:460 +0x15a fp=0xc517711d10 sp=0xc517711ca0 testing.(*common).Logf(0xc42007a180, 0x98febc, 0x20, 0xc540e9fbc0, 0x3, 0x3) /opt/go/src/testing/testing.go:473 +0x75 fp=0xc517711d58 sp=0xc517711d10 github.com/hyperledger/fabric/core/ledger/testutil.AssertEquals(0xbfc180, 0xc42007a180, 0x8b2a20, 0xc540ecf4e0, 0x8b2a20, 0xc540ecf500) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/testutil/test_util.go:136 +0x128 fp=0xc517711dc8 sp=0xc517711d58 github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/lockbasedtxmgmt.TestDBTxGet(0xc42007a180) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/lockbasedtxmgmt/multidb_multitx_test.go:128 +0x4dd fp=0xc517711f78 sp=0xc517711dc8 testing.tRunner(0xc42007a180, 0x9c7680) /opt/go/src/testing/testing.go:610 +0x81 fp=0xc517711fa0 sp=0xc517711f78 runtime.goexit() /opt/go/src/runtime/asm_amd64.s:2086 +0x1 fp=0xc517711fa8 sp=0xc517711fa0 created by testing.(*T).Run /opt/go/src/testing/testing.go:646 +0x2ec  goroutine 1  chan receive : testing.(*T).Run(0xc420236e40, 0x982b6d, 0xb, 0x9c7680, 0xc42014bd01) /opt/go/src/testing/testing.go:647 +0x316 testing.RunTests.func1(0xc420236e40) /opt/go/src/testing/testing.go:793 +0x6d testing.tRunner(0xc420236e40, 0xc42014ae30) /opt/go/src/testing/testing.go:610 +0x81 testing.RunTests(0x9c8310, 0xc16880, 0xe, 0xe, 0xc42023aad0) /opt/go/src/testing/testing.go:799 +0x2f5 testing.(*M).Run(0xc42014aef8, 0xc42014bef8) /opt/go/src/testing/testing.go:743 +0x85 main.main() github.com/hyperledger/fabric/core/ledger/kvledger/txmgmt/lockbasedtxmgmt/_test/_testmain.go:80 +0xc6  goroutine 17  syscall, 84 minutes, locked to thread : runtime.goexit() /opt/go/src/runtime/asm_amd64.s:2086 +0x1  goroutine 899  select, 84 minutes : github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.(*DB).tCompaction(0xc4499a6180) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db_compaction.go:804 +0x83c created by github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb.openDB /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/syndtr/goleveldb/leveldb/db.go:146 +0x73a  ></body> </Action>
<Action id="20095" issue="13719" author="nishi" type="comment" created="2016-12-14 18:54:12.0" updateauthor="nishi" updated="2016-12-14 18:54:12.0"> <body><! CDATA Reran test after modifying util/db.go (line: 79) as "dbOpts := &opt.Options{OpenFilesCacheCapacity: -1, BlockCacheCapacity: 2 * 1024 * 1024}"  All writes (1000/1000) were successful.  Reads were processed only in 61/1000 databases and then test exited with "signal killed process".  dmesg | grep -E -i -B100 'killed process'  resulted in     5792.005255  Out of memory: Kill process 2675 (lockbasedtxmgmt) score 950 or sacrifice child   5792.005553  Killed process 2675 (lockbasedtxmgmt) total-vm:9523480kB, anon-rss:7748680kB, file-rss:944kB  same as in previous run.    ></body> </Action>
<Action id="20138" issue="13719" author="nishi" type="comment" created="2016-12-15 21:46:31.0" updateauthor="nishi" updated="2016-12-15 21:46:31.0"> <body><! CDATA With default db options: dbOpts := &opt.Options{} Ran go test to generate memprofile on different sample sets 1 X 1 , 2 X 2, 100 X 100, 100 X 200, 200 X 100, 200 X 200 (databses X records) using following command: go test -c && ./commontests.test -test.memprofile=tmp.pprof Uploaded file "Report on Memory Usage from MemProfiling.ods" after consolidating memory profiling on these samples. Also uploading new multidb_multitx_test.go (updated after txmgr code refactoring)   ></body> </Action>
<Action id="21953" issue="13719" author="denyeart" type="comment" body="Recent stress tests in LevelDB did not see this issue." created="2017-04-07 18:09:33.0" updateauthor="denyeart" updated="2017-04-07 18:09:33.0"/>
