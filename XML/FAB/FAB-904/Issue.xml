<Issue id="13235" key="FAB-904" number="904" project="10002" reporter="bcbrock" assignee="nickgaski" creator="bcbrock" type="10002" summary="Kafka orderer requires message-size constraint specifications/enforcement/documentation" priority="3" resolution="10000" status="6" created="2016-10-28 20:57:13.0" updated="2018-07-20 14:10:39.0" resolutiondate="2017-03-28 19:15:15.0" votes="0" watches="4" workflowId="36582"> <description><! CDATA For better or worse, Kafka configuration includes several parameters related to maximum messages sizes. I hit this just now when trying to order 1KiB blobs in blocks of 1000 - this exceeded the default 1MB message-size limit and the orderer crashed when Kafka threw an exception.  One way or the other the Kafka orderer should reject blobs that are too big, and be coded in such a way that it will never aggregate blocks that are too big for the current configuration. It may be possible for the orderer to learn these constraints by metadata requests to Kafka. Or the orderer could take constraints from the config file and reconfigure Kafka based on the constraints (?)   The interaction of the constraints will need to be well-documeted so that clients understand their resonsibility (if any) for creating consistent Kafka/orderer configurations and avoiding issuing blobs that are too big. This will also have to go back into the SDK to gracefully handle these types of failures.   I'm labelling this a bug because the orderer "should have" known that my block was getting too big and cut the block before it got too big.  ></description> </Issue>
