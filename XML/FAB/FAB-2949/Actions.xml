<Action id="21763" issue="15529" author="yacovm" type="comment" created="2017-03-31 08:02:33.0" updateauthor="yacovm" updated="2017-03-31 10:04:27.0"> <body><! CDATA That FAB-1371 has nothing to do with disaster recovery.  *Gossip pull snapshot consolidation* talks about a mechanism in gossip that pulls entities from peers by asking for a snapshot of all entities of a type that a peer has. I guess snapshot is the word that mislead you here.  Now, Hyperledger Fabric has a 3-tier architecture.  Before we talk about disaster recovery, I think it's worth to talk a bit about HA, because the 2 are related, and for that- I guess it would be useful to outline the communication patterns in the fabric architecture: * SDK (apps) to users: I'd say this is completely up to the customer to implement * SDK (apps) to  peers: Here I think we have lots of work that can be done, as currently- the client SDK has no ability to dynamically obtain a list of peers to use as endorsers. There have been some discussion on rocket.chat and it was proposed to either leverage DNS, or to interface with a service discovery service, if such exists in the organization. * Peers to Peers: Peers communicate with each other via gossip, and as long as the anchor peers of the channel are reachable - any peer can find other peers. However, if all anchor peers become unreachable- then new peers can't  find the existing peers of the channel. It is required that peers find each other in order to disseminate blocks from the ordering service to peers. If peers are configured to use leader election- then 1 peer from the organization, for each channel (can be the same peer for all channels) is elected by the peers to connect on their behalf to the ordering service, and to pull blocks from it. If that peer becomes unreachable, the peers identify that, and start electing a new leader. * Peer to  Ordering service: The configuration blocks come with a set of endpoints of ordering service nodes. We (Haifa team) are working on a set of change sets, that enable the peer to re-connect to different ordering service nodes out of the set defined in the configuration block, should the current ordering service node become unreachable. * ordering service nodes to  ordering service nodes: This is relevant only for sBFT ( ~kchristidis  correct me if I'm wrong) * ordering service nodes to kafka: I guess  ~kchristidis  can elaborate on the HA aspects here but from what I know there is master-slave replication within the kafka servers of the same channel. * Peer/Orderer to DB : I guess Dave/Manish can shed some light on the DRP plans here, and I guess that's practically the main work in DRP (the things outlined above are basically HA) since as ordering nodes and peers fail, someone needs to bring them up. I'd say that if peers fail and new peers are brought up instead of them - as long as we don't have pruning for raw ledger blocks - these peers would be able to catch up with other peers even if they are brought to life from scratch. However- if we start pruning raw ledger, this is where manual/special intervention would be needed and it is related to DR.  ></body> </Action>
<Action id="21764" issue="15529" author="grapebaba" type="comment" body="Thanks for detail explains. Previous I only considered peer ledger related recover. I wonder if we could send a snapshot to remote persistent storage periodically like etcd so that we can remain ledger and recovery peers cluster in case all previous peers crash and cannot bring up again. " created="2017-03-31 08:19:03.0" updateauthor="grapebaba" updated="2017-03-31 08:19:03.0"/>
<Action id="21766" issue="15529" author="yacovm" type="comment" created="2017-03-31 08:49:05.0" updateauthor="yacovm" updated="2017-03-31 08:49:05.0"> <body><! CDATA I don't think that sending a snapshot periodically scales very well. Also- you don't need to replicate the state. Just the ledger, as you can simply re-run the ledger's transactions from the genesis block to reach the final state the peer needs to be. CouchDB is a network servicing DB, right? I guess that it's possible to setup a couchDB cluster, or if the organization has money- on a volume that resides on a SAN so that it's possible to bring up a peer with the old data in case of a peer crash.  ></body> </Action>
<Action id="21784" issue="15529" author="kchristidis" type="comment" created="2017-04-01 19:01:19.0" updateauthor="kchristidis" updated="2017-04-01 19:01:35.0"> <body><! CDATA ??ordering service nodes to ordering service nodes: This is relevant only for sBFT (Kostas Christidis correct me if I'm wrong)??  Correct.  ??ordering service nodes to kafka: I guess Kostas Christidis can elaborate on the HA aspects here but from what I know there is master-slave replication within the kafka servers of the same channel.??  Correct. We are leveraging Kafka's replication factor  *  for a topic to replicate all the messages that a Kafka broker orders for a particular channel. (* Technically it's a combination of the "replication factor", "minimum in-sync replicas", and "ACKS a producer is expecting" settings.)   ></body> </Action>
<Action id="21913" issue="15529" author="grapebaba" type="comment" created="2017-04-06 03:46:37.0" updateauthor="grapebaba" updated="2017-04-06 03:46:37.0"> <body><! CDATA  ~yacovm  SDK (apps) to peers: Here I think we have lots of work that can be done, as currently- the client SDK has no ability to dynamically obtain a list of peers to use as endorsers. There have been some discussion on rocket.chat and it was proposed to either leverage DNS, or to interface with a service discovery service, if such exists in the organization. Is there a JIRA ticket for this feature?  ></body> </Action>
<Action id="66993" issue="15529" author="sykesm" type="comment" body="Stale" created="2020-01-22 19:00:07.0" updateauthor="sykesm" updated="2020-01-22 19:00:07.0"/>
