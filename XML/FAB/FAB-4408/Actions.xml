<Action id="25471" issue="17547" author="kchristidis" type="comment" created="2017-06-06 07:25:22.0" updateauthor="kchristidis" updated="2017-06-06 07:25:22.0"> <body><! CDATA Code completed and UTs pass.  Experimenting with integration tests.  ></body> </Action>
<Action id="25473" issue="17547" author="kchristidis" type="comment" created="2017-06-06 08:57:29.0" updateauthor="kchristidis" updated="2017-06-06 08:57:29.0"> <body><! CDATA Here's what needs to be revised with the current approach:  When an OSN restarts, it ranges over the existing chains and starts them one by one:  https://github.com/hyperledger/fabric/blob/master/orderer/multichain/manager.go#L133   When a new chain creation request comes, the OSN starts it and moves along:  https://github.com/hyperledger/fabric/blob/master/orderer/multichain/manager.go#L191   Implicit in both of these instances is the assumption that the `Chain.Start()` call will return right away.  With our fix however, this is not necessarily the case. We might bump into a channel owned by a replica that's inaccessible — in this case the multichain manager may be "stuck" starting that channel for the next, say, 10 hours (or whatever LongTotal is set to). This will block the loading of all other channels. This is not acceptable.  I'll think of a proper fix to this tomorrow with a clear head.  The first thought that comes to mind now (make `Chain.Start()` for Kafka a no-op, and move the `retrySetupProducer`, `retryPostConnect`, and `retrySetupConsumer` functions in the `processMessagesToBlocks` goroutine that is launched by `Chain.Start()`) strikes me as hackish.  ></body> </Action>
<Action id="25721" issue="17547" author="kchristidis" type="comment" body=" https://gerrit.hyperledger.org/r/#/c/10319/ " created="2017-06-08 03:56:47.0" updateauthor="kchristidis" updated="2017-06-09 06:33:25.0"/>
