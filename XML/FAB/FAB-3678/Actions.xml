<Action id="23402" issue="16357" author="elli-androulaki" type="comment" created="2017-05-05 13:07:01.0" updateauthor="elli-androulaki" updated="2017-05-05 13:07:01.0"> <body><! CDATA  ~keithsmith ,  ~mastersingh24 , what do you think about the issue, and the solution in view of v1?  We think that the issue may be severe if we want to allow committing peers to join a channel at any point in time after a channel has been created...   ></body> </Action>
<Action id="23403" issue="16357" author="smithbk" type="comment" created="2017-05-05 13:29:35.0" updateauthor="smithbk" updated="2017-05-05 13:29:35.0"> <body><! CDATA I personally think this should be documented for v1 and fixed in v1.1. As for solutions, can't we just periodically push a config block update with the real-time which is used to evaluation expiration, so there would be no change to certs and no translation between real-time and block-time?  ></body> </Action>
<Action id="23404" issue="16357" author="elli-androulaki" type="comment" created="2017-05-05 13:46:23.0" updateauthor="elli-androulaki" updated="2017-05-05 13:46:58.0"> <body><! CDATA  ~keithsmith :  ```I personally think this should be documented for v1 and fixed in v1.1.``` So, documenting would be fine, if we are fine with the fact that peers *will not* be able to join a channel and do the block validation after at least one of the certificates (creator/endorser) have expired... But are we fine with this limitation for v1 ?  ```As for solutions, can't we just periodically push a config block update with the real-time which is used to evaluation expiration, so there would be no change to certs and no translation between real-time and block-time?``` We could, but then who would we trust to push such an update. Would it be the channel admin? If so, it may require more than one signatures  and be too complicated for a tx that needs to go in to all channels every fixed time period... If not the channel admin, then who? Every MSP? I feel that considering this at certificate issuing time (as in the proposals above) may be cleaner...  What do you think?    ></body> </Action>
<Action id="23407" issue="16357" author="yacovm" type="comment" created="2017-05-05 14:19:57.0" updateauthor="yacovm" updated="2017-05-05 14:32:14.0"> <body><! CDATA {quote}Even if we assume that certificates are renewed considerable amount of time before certificates expire, so that we do not cause state-forks, we still have the issue that a peer that now joins a channel cannot validate properly any of the endorsed transactions with expired endorser certificates.{quote}  Don't you have the same problem for manual revocations? if an endorser and a client trigger a block creation and later on, the endorser is revoked and now its in a CRL of yours - new peers would mark this Txn as invalid, and old peers have already committed this Txn, no?  Also- why can't the time of the ordering service be encoded into the blocks? When a peer gets such a block, it can calculate whether the endorser was (was not) expired at that time?  ></body> </Action>
<Action id="23409" issue="16357" author="elli-androulaki" type="comment" created="2017-05-05 14:51:32.0" updateauthor="elli-androulaki" updated="2017-05-05 14:51:32.0"> <body><! CDATA ```Don't you have the same problem for manual revocations?``` So, a committing peer would always start from a valid config block, say genesis block or intermediate config block. The idea is that this block would give it all the information it needs to validate blocks and transactions following that block in the channel. Now, for revoked users we are covered cause CRLs are included in the MSP config that is included in the config block. But, for validation of already expired certificates (since for assessing expiration we rely on real time), a peer who performs this validation much after this tx has been added to the channel (say a month later cause he joined the channel later), cannot properly assess whether a certificate is valid or not.  ```When a peer gets such a block, it can calculate whether the endorser was (was not) expired at that time?``` Hm, do you mean considering the timestamp included in the block itself? This is indeed an option! But we would still need to modify the validate function of the peer that would need to consider the block's time as valid time...   ></body> </Action>
<Action id="23410" issue="16357" author="yacovm" type="comment" created="2017-05-05 15:04:23.0" updateauthor="yacovm" updated="2017-05-05 15:10:33.0"> <body><! CDATA {quote}Hm, do you mean considering the timestamp included in the block itself? This is indeed an option! But we would still need to modify the validate function of the peer that would need to consider the block's time as valid time... {quote} It's pretty easy, I think- just adjust the *NotBefore* and *NotAfter* fields of the x509.Certificate, and restore them after validation.  {code} func (msp *bccspmsp) getValidationChain(cert *x509.Certificate) (  *x509.Certificate, error) { // The block was created a week ago, so instead of bringing this peer's clock a week in the past, 	// bring the certificate's expiration time a week in the future 	delta := time.Duration(time.Hour * 24 * 7) 	cert.NotAfter = cert.NotAfter.Add(delta) 	validationChain, err := cert.Verify(*(msp.opts)) 	if err != nil { 		return nil, fmt.Errorf("The supplied identity is not valid, Verify() returned %s", err) 	} {code} {quote} So, a committing peer would always start from a valid config block, say genesis block or intermediate config block. The idea is that this block would give it all the information it needs to validate blocks and transactions following that block in the channel. Now, for revoked users we are covered cause CRLs are included in the MSP config that is included in the config block. But, for validation of already expired certificates (since for assessing expiration we rely on real time), a peer who performs this validation much after this tx has been added to the channel (say a month later cause he joined the channel later), cannot properly assess whether a certificate is valid or not.{quote} What I mean is - if a peer *p* has signed a transaction and peer *q1* committed it, and later on - *p* is revoked via a CRL, peer *q2* that gets the block after that - will not validate the transaction signed by *p*  ></body> </Action>
<Action id="23411" issue="16357" author="elli-androulaki" type="comment" created="2017-05-05 15:10:10.0" updateauthor="elli-androulaki" updated="2017-05-05 15:10:10.0"> <body><! CDATA {quote} What I mean is - if a peer p has signed a transaction and peer q1 committed it, and later on - p is revoked via a CRL, peer q2 that gets the block after that - will not validate the transaction signed by p {quote} I am not sure about that. Actually peer *q1* should evaluate *p*'s certificate considering the MSP configuration of the config block preceding the tx under evaluation. Because at that time *p* was not included in the CRL, the tx evaluation would continue under these terms.  ></body> </Action>
<Action id="23412" issue="16357" author="yacovm" type="comment" created="2017-05-05 15:31:13.0" updateauthor="yacovm" updated="2017-05-05 15:31:13.0"> <body><! CDATA {quote}I am not sure about that. Actually peer q1 should evaluate p's certificate considering the MSP configuration of the config block preceding the tx under evaluation. Because at that time p was not included in the CRL, the tx evaluation would continue under these terms.{quote}  Yeah, my bad. if *q1* committed a Txn it means that the config blocks until then didn't have *p* in the CRLs.  ></body> </Action>
<Action id="23414" issue="16357" author="yacovm" type="comment" created="2017-05-05 16:10:02.0" updateauthor="yacovm" updated="2017-05-05 16:34:23.0"> <body><! CDATA Anyway- if you go via the timestamp from the block creation, it appears that the x509 verifyOptions support the technique:   {code} type VerifyOptions struct { 	DNSName       string 	Intermediates *CertPool 	Roots         *CertPool // if nil, the system roots are used 	CurrentTime   time.Time // if zero, the current time is used 	// KeyUsage specifies which Extended Key Usage values are acceptable. 	// An empty list means ExtKeyUsageServerAuth. Key usage is considered a 	// constraint down the chain which mirrors Windows CryptoAPI behaviour, 	// but not the spec. To accept any key usage, include ExtKeyUsageAny. 	KeyUsages   ExtKeyUsage } {code}  As for Kafka nodes (to take into account that we have different nodes that cut blocks) maybe we can use the timestamp of the last message of the block?  ></body> </Action>
<Action id="23425" issue="16357" author="ashutosh_kumar" type="comment" body="To me , it seems over-engineering. In real life scenario , certificates do not expire that often that you need to integrate certificate validity to main processing path. Cert and Key management system usually takes care of certificate expiration problem which usually happens independent of main processing." created="2017-05-05 20:21:55.0" updateauthor="ashutosh_kumar" updated="2017-05-05 20:21:55.0"/>
<Action id="23513" issue="16357" author="elli-androulaki" type="comment" created="2017-05-08 07:23:02.0" updateauthor="elli-androulaki" updated="2017-05-08 07:23:02.0"> <body><! CDATA  ~yacovm , I talked to  ~kchristidis , and unfortunately will not be easy to set a timestamp in the block now that is actually checked/respected and signed...       ~ashutosh_kumar , the issue we refer to goes beyond the actual x509 cert expiration and key-management associated to this. What we try to guarantee is the compromise of the safety (consistency) of the system assuming that an endorser/client certificate has expired  our tx validation process needs to deterministically — i.e., across all committing peers this is exectuted ---- decide on whether the tx certificates have expired .  ></body> </Action>
<Action id="23550" issue="16357" author="yacovm" type="comment" body=" ~kchristidis  -why?" created="2017-05-08 17:06:13.0" updateauthor="yacovm" updated="2017-05-08 17:22:41.0"/>
<Action id="23574" issue="16357" author="kchristidis" type="comment" created="2017-05-08 20:53:20.0" updateauthor="kchristidis" updated="2017-05-08 20:53:56.0"> <body><! CDATA So, I probably didn't explain my thinking clearly enough when discussing this with Elli earlier, my bad.  Adding the field is easy, but making sure its value is monotonically increasing is _not_. In fact, in Kafka you just don't have this guarantee, plain and simple. And in the BFT case, I'm not even sure how we would go about approaching this.  If we don't care about this property, then this goes back to being easy.  ></body> </Action>
<Action id="23576" issue="16357" author="yacovm" type="comment" body="Why would we care about the timestamp being monotonously increasing? We just need consistency across validators IIUC." created="2017-05-08 21:48:59.0" updateauthor="yacovm" updated="2017-05-08 21:48:59.0"/>
<Action id="23584" issue="16357" author="binhn" type="comment" created="2017-05-09 03:40:54.0" updateauthor="binhn" updated="2017-05-09 03:40:54.0"> <body><! CDATA  ~yacovm  We need a monotonic timestamp (or whatever we call it) so that when a cert is expired, it will forever be expired; otherwise it might be expired now and might not on the next block.  But as  ~kchristidis  pointed out, this is difficult in some cases, though it might be the best out of the options that  ~elli-androulaki  mentioned above.  I think at this point, we should seriously consider short-cuts for 1.0, and an attractive one to me is do nothing. If people want to kick a cert out of the system, use CRL; otherwise, let all go free until next release.  Thought?  In any case, I think we should add a uint64 field to the BlockHeader.  ></body> </Action>
<Action id="23586" issue="16357" author="yacovm" type="comment" created="2017-05-09 07:51:20.0" updateauthor="yacovm" updated="2017-05-09 09:48:46.0"> <body><! CDATA {quote}We need a monotonic timestamp (or whatever we call it) so that when a cert is expired, it will forever be expired; otherwise it might be expired now and might not on the next block. {quote} Understood. in that case let's consider the possible orderer types: # Solo - I guess we don't have a problem here, and it's a reasonable assumption that no one would suddenly adjust the time. # Kafka - Here I proposed we take the timestamp of the last message of the block. To preserve monotonic timestamps, I guess maybe we can make block *b_i* that is cut by orderer node *o_j*  to have the timestamp of: Max(last_msgTS(*b_i*), TS(*b_i-1*))  This way if you have a leader switch you don't go back in time. If the time difference between nodes is small (less than 1 min?) that won't be very harmful, but if the time difference is big that may impose a problem, but I think that if we document this requirement that orderer nodes need to be in sync with regard to local time, that would be OK.  What do you think  ~binhn  ? # BFT - I'm not sure.  ~marko.vukolic  what do you think?       {quote}If people want to kick a cert out of the system, use CRL; otherwise, let all go free until next release.  Thought? {quote} I think that the current state of the code requires intervention in any case because in order to ignore certificate expiration you need to make the VSCC explicitly set the time of the cert to be before the "expires" and after the "not used before" fields or otherwise the x509 library won't agree to validate the endorser's signature.    {code:java} // isValid performs validity checks on the c. func (c *Certificate) isValid(certType int, currentChain   *Certificate, opts *VerifyOptions) error {     now := opts.CurrentTime     if now.IsZero() {         now = time.Now()     }     if now.Before(c.NotBefore) || now.After(c.NotAfter) {         return CertificateInvalidError{c, Expired}     }{code}  ></body> </Action>
<Action id="23590" issue="16357" author="vukolic" type="comment" created="2017-05-09 10:30:12.0" updateauthor="vukolic" updated="2017-05-09 10:44:42.0"> <body><! CDATA In both Kafka (i.e., a CFT ordering service) and a BFT ordering service - clock asynchrony may raise performance concerns or deviation from "real time".  1) for CFT what  ~yacovm  suggests max(a,b) is ok and can be done without perfromance penalty - yet aggregates skew wrt "real-time" (referred to as OPTION 2 in the text below). If the new leader would wait for "his" time which is behind the time of old leader (in attempt to address the clock skew - OPTION 1 below) then we have performance penalty (waiting at a new leader).  2) in BFT the same approach would apply - but we need to care about Byzantine leaders. To cope with Byzantine leaders one could go like roughly like below  message batch_header \{          uint64 seq = 1;         bytes prev_hash = 2;         bytes data_hash = 3;          uint64 timestamp = 4; }  BFT orderer would have two CONST (?) params (in principle, positiveSkew/negativeSkew do not have to be const - but let's simplify to start with)  const negativeSkew = sth const positiveSkew = sth //where positiveSkew>=negativeSkew holds  At least positiveSkew needs to consider network latency as well. From our experience with XFT ( http://vukolic.com/xft-osdi2016.pdf ), on public internet, positiveSkew must be at least 1-2s  negativeSkew may be smaller and will really depend on underlying clock synchronization (if any). negativeSkew should be small to limit the ability of a Byzantine primary to delay future primaries in OPTION 1. However, small negativeSkew may impact liveness in case clocks are not syncrhonized.  Then the protocol for say SBFT could be modified roughly as follows (pardon my whiteboard go)  ----------------------------------- 1) Primary, on assembling the batch looks at its local time  localtime := time.Now()  if localtime<=s.sys.LastBatch().Header.timestamp  \{     //OPTION 1: wait here for s.sys.LastBatch().Header.timestamp-localtime     localtime := time.Now()     //alternatively OPTION 2: localtime:= max of (time.Now(), s.sys.LastBatch().Header.timestamp) - see discussion above for clock skew aggregation  }  // proceed with batch assembly with localtime as batchheader.realtime  2) Replicas on reception of pre-prepare do the following:  if batchheader.timestamp <=   s.sys.LastBatch().Header.timestamp     //leader is Byzantine proceed to view change  localtime := time.Now()  if (batchheader.timestamp+positiveSkew) < localtime \{     return; //simply do not process pre-prepare. do not view change the leader here (pretend like the message was lost) }  if (batchheader.timestamp-negativeSkew) > localtime \{     return; //simply do not process pre-prepare. do not view change the leader here (pretend like the message was lost) }  ></body> </Action>
<Action id="23591" issue="16357" author="vukolic" type="comment" created="2017-05-09 11:14:52.0" updateauthor="vukolic" updated="2017-05-09 11:14:52.0"> <body><! CDATA BTW - the code below is non-deterministic  and *must not* appear in the commit phase  we are basically trying here to sketch a solution that would say now = s.sys.LastBatch().Header.timestamp instead of now = time.Now()    // isValid performs validity checks on the c.func (c *Certificate) isValid(certType int, currentChain   *Certificate, opts *VerifyOptions) error \{     now := opts.CurrentTime     if now.IsZero() \{         now = time.Now()     }     if now.Before(c.NotBefore) || now.After(c.NotAfter) \{         return CertificateInvalidError\{c, Expired}     }  ></body> </Action>
<Action id="23606" issue="16357" author="binhn" type="comment" created="2017-05-09 15:47:04.0" updateauthor="binhn" updated="2017-05-09 15:47:04.0"> <body><! CDATA  ~vukolic   {quote}the code below is non-deterministic  and *must not* appear in the commit phase {quote} Good point. Where is that code from,  ~yacovm  ?  I grep and didn't find any hit in our code.      ~yacovm   Assuming that solo won't be used in production, then I agree. Otherwise, we would need to do similar to CFT at the failover. For CFT, your method should work, but I think we should use the block header time that is calculated as Marko's pseudo option 2 since timestamp on the transaction is the local time from client node.      ~kchristidis   Thoughts?     ></body> </Action>
<Action id="23608" issue="16357" author="yacovm" type="comment" created="2017-05-09 16:04:39.0" updateauthor="yacovm" updated="2017-05-10 08:07:39.0"> <body><! CDATA {quote}Good point. Where is that code from,  ~yacovm  ?  I grep and didn't find any hit in our code. {quote} I was trying to explain how the verification works in the x509 package, the code is from  https://golang.org/src/crypto/x509/verify.go  {quote}Assuming that solo won't be used in production, then I agree. {quote} I don't understand. How can we have a cluster of solo orderers? You assume their file system is written to shared storage or something? Otherwise if the solo orderer dies, the ledger dies with it and then no one can submit any transaction because the new orderer doesn't know the channels. {quote}Otherwise, we would need to do similar to CFT at the failover. {quote} You mean for Kafka right? {quote}but I think we should use the block header time that is calculated as Marko's pseudo option 2 since timestamp on the transaction is the local time from client node. {quote} I honestly don't understand how to translate what he wrote to what we should do in CFT with either solo or kafka.  Can you explain precisely?      ~jyellick  what's your opinion?      ~vukolic  can't we simply use the following "guidelines" for BFT orderers (maybe that's somewhat what you meant to convey too?) ? * Ask the network operators that the servers would be in sync with regard to time * If the new *batch.timestamp* in preprepare is too in the future, drop the message (like you said) * If the new *batch.timestamp* in preprepare is in the past, make all replicas set the *batch.timestamp* to be the timestamp of the previous batch (all nodes have the timestamp of the last batch unless it's the genesis block, right?) now... the primary replica that sent the preprepare might be in the past without knowing so. * So when the *prepare* message is sent from all replicas to all replicas, write in that message a flag that means the timestamp chosen by replicas is the timestamp in the past. All replicas that see this flag would set the timestamp of the current batch to the timestamp of last batch.  * In commit - send the timestamp chosen, just in case for sanity check. * The question remains how to set the timestamp of the first batch (genesis block) I *think* that you can do here exactly what you said - if the timestamp is too in the past or in the future, reject the message, otherwise use that timestamp.  ></body> </Action>
<Action id="23625" issue="16357" author="vukolic" type="comment" created="2017-05-10 08:39:32.0" updateauthor="vukolic" updated="2017-05-10 08:40:50.0"> <body><! CDATA  ~yacovm      I'd rather discuss this over code/pseudocode rather than "guidelines" which can be vague. Some examples:  > Ask the network operators that the servers would be in sync with regard to time  such expectations need to be made precise.  > If the new *batch.timestamp* in preprepare is in the past,  "past" of what? time.Now() at a replica receiving preprepare or "past" of last block time? My pseudocode above treats these two cases differently. It requires primary never to assign a timestamp to block n+1smaller than the timestamp of the block n. If the ts(block n+1) is in between ts(block n) and time.now() we need to say how big of a delay (wrt time.now() is aceptable).  > the primary replica that sent the preprepare might be in the past without knowing so  It cannot since primary needs to do, when sending pre-prepare, all the checks that a replica would do when it receives a pre-prepare. So primary must not assign the timestamp to block n+1 smaller than the timestamp of block n     > So when the *prepare* message is sent from all replicas to all replicas, write in that message a flag that means the timestamp chosen by replicas is the timestamp in the past.  Not clear to me, what you mean here :)  > In commit - send the timestamp chosen, just in case for sanity check.     The "chosen" timestamp must be fixed in a pre-prepare message and then we use PBFT style consensus to agree on that time along with the entire batch. Doing something extra in commit msg does not help.     Now to step back a bit: I think we can work out the details of this solution for BFT (along the lines of the discussion above) - at the moment we simply need to add  uint64 timestamp = 4;  to  message batch_header \{          uint64 seq = 1;         bytes prev_hash = 2;         bytes data_hash = 3;  }  in common.proto  and have simple(r) Kafka and Solo solutions in place for v1.0.0.              ></body> </Action>
<Action id="23627" issue="16357" author="mastersingh24" type="comment" created="2017-05-10 10:21:21.0" updateauthor="mastersingh24" updated="2017-05-10 10:21:21.0"> <body><! CDATA I don't feel "rushing" into a solution at this point is the way to go.  Fine to keep discussing. * In the current flow, endorsers "sign" with longterm enrollment certificates * We already support CRLs and CRL enforcement is deterministic since they are delivered via config transactions / blocks  But the system is definitely vulnerable to non-determinism in the one place it MUST have deterministic behavior.  So my proposal is that we ignore certificate expiration when it comes to verification.   I think this is the cleanest solution and is really not an exposure given we have CRL enforcement.  So I'd suggest we make sure we always pass verify in terms of expiration and we can log at WARNING level if a certificate is expired.       ></body> </Action>
<Action id="23628" issue="16357" author="vukolic" type="comment" body="That&apos;s fine as a shorter term solution as well - to have certs explicitly be expired using CRLs (only) - and essentially otherwise disregarding  the expiration time in the validation/commit phase (modulo log warning which may be non-uniform and non-deterministic across peers)" created="2017-05-10 10:38:52.0" updateauthor="vukolic" updated="2017-05-10 10:38:52.0"/>
<Action id="24221" issue="16357" author="elli-androulaki" type="comment" created="2017-05-24 08:00:11.0" updateauthor="elli-androulaki" updated="2017-05-24 08:00:11.0"> <body><! CDATA Ok, so adding two JIRA items associated to this. * The v1.0 item would require the removing of time-related checks at identity validation time.  * The post v1.0 item would relate to the design and implementation of a complete solution.   ~vukolic ,  ~mastersingh24 ,  ~keithsmith ,  ~binhn ,  ~yacovm , would you agree with this?     ></body> </Action>
<Action id="24243" issue="16357" author="binhn" type="comment" body=" ~elli-androulaki  +1" created="2017-05-24 14:53:25.0" updateauthor="binhn" updated="2017-05-24 14:53:25.0"/>
<Action id="24250" issue="16357" author="yacovm" type="comment" created="2017-05-24 16:14:54.0" updateauthor="yacovm" updated="2017-05-24 16:25:09.0"> <body><! CDATA What needs to be carefully considered here is that if a user has a ledger that is populated with blocks via an ordering service from v1.0, and then the user upgrades to v1.1 or something that has additional things in its block - would the logic implemented in post v1.0 support validation of blocks created in v1.0?  From what I know ( ~jyellick  /  ~kchristidis  please correct me if I'm wrong ) there is no version field in the data blocks that the VSCC consumes, but there is in the transaction (channel header): {code} chdr, err := utils.UnmarshalChannelHeader(payl.Header.ChannelHeader) {code} But this value is set by the client, not the orderer. So maybe we need to add some version field to the block header?   ></body> </Action>
<Action id="24300" issue="16357" author="smithbk" type="comment" created="2017-05-25 07:52:03.0" updateauthor="smithbk" updated="2017-05-25 14:13:13.0"> <body><! CDATA Do we need to remove the time-related NotBefore/After checks from ALL identity validation, or just at commit time to make it deterministic?  It seems to me that we should continue to make the time-related checks when making endorsement checks, for example.  ></body> </Action>
<Action id="24336" issue="16357" author="binhn" type="comment" body=" ~smithbk  Time checks would not make sense since we have multiple unsynchronized clocks, even not at the commit time. I think we should ignore all time checks as  ~vukolic  suggested  #above | https://jira.hyperledger.org/browse/FAB-3678?focusedCommentId=23628&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-23628 " created="2017-05-25 14:47:23.0" updateauthor="binhn" updated="2017-05-25 14:47:23.0"/>
<Action id="24446" issue="16357" author="elli-androulaki" type="comment" created="2017-05-26 13:32:52.0" updateauthor="elli-androulaki" updated="2017-05-26 13:32:52.0"> <body><! CDATA  ~yacovm , I am wondering if including a version on the MSP configuration could tackle the issue. Say MSP v1 is still not considering pure-time based expiration, and then MSP v15 (after blocks support timestamps, and MSP code is upgraded to support identity validity w.r.t. the block timestamp) does. When one tries to re-validate transactions on the ledger, it only needs to do the validation with the MSP versions include din each block...   Of course we do not have the hashes of the MSP code, neirther configuration field in MSP to denote msp code version. But that may be something to do in the future(?)  ></body> </Action>
<Action id="24459" issue="16357" author="yacovm" type="comment" created="2017-05-26 14:59:38.0" updateauthor="yacovm" updated="2017-05-26 15:01:07.0"> <body><! CDATA So, for v1.0 it was decided not to take into account the expiration dates of the certificates, and for this we need the validation code to ignore them upon block ingestion. At v1.1 (for example) we would take into account the expiration period via some solution.  The question remains- how does v1.1 peer code (which isn't written yet) knows that- given a block, it was produced by an ordering service of v1.0 and its transactions should therefore be validated by the rules of v1.0 and not by the rules of v1.1.  I like the line of thought of what you suggested because we could leverage the fact that the MSPConfig is a protobuf, and in protobuff if you add a field and then deserialize a message from a time before you added the field, it would have a zero value, so an MSP of version v1.1 can know it needs to use v1.0 logic if its config version is 0 (implicitly or explicitly set). So indeed if the MSP code of v1.1 would know the version of the ordering service (from the last config block) it can know whether to "check the time" or not. A small problem here would be - that we need first to upgrade *all the peers*, and then upgrade the ordering service + the first thing the ordering service would have to do before servicing requests is to create a config Txn that bumps the version of the MSPConfig to 1 (from 0).  The danger here is - if some peers are left not upgraded, it would cause a split in the blockchain.  ></body> </Action>
<Action id="24462" issue="16357" author="elli-androulaki" type="comment" created="2017-05-26 15:08:32.0" updateauthor="elli-androulaki" updated="2017-05-26 15:08:32.0"> <body><! CDATA Hm, I think we would need to follow the same line of thought we adopted for chaincodes and their (upgraded) instances across different chains.  That is the peer has a version of msp V that should be backward compatible with versions v<=V, and only activate V on the chain if there is a respective announcement on that chain.   Indeed peers need to be upgraded, as well as orderers (mainly orderers) to support V, but they will not use it untill a config tx suggesting the use of version V is announced to the BC.  This belongs to a family of issues related to fabric-upgrade that we should sort out earlier than later. For MSP related stuff, identity channel would definitely make things easier :)     ></body> </Action>
<Action id="24480" issue="16357" author="elli-androulaki" type="comment" created="2017-05-26 17:04:06.0" updateauthor="elli-androulaki" updated="2017-05-26 17:04:06.0"> <body><! CDATA In the interest of time, and since you brought up the version-issue,  ~yacovm  would you like to open a task for v1 to investigate it further? If not, i can volunteer to make a first pass (just let me know). You may link the new issue to this one here.  Though it is likely that we don't need to do any change in the code, but there is value in having things documented: i) we convince ourselves that this is the way to go, and (ii) we keep things well explained somewhere for others to see/approve/chip in. I will be happy to provide text for suggested solutions.  This item i will mark for post v1 to avoid confusion.  ></body> </Action>
<Action id="24512" issue="16357" author="yacovm" type="comment" created="2017-05-26 20:04:49.0" updateauthor="yacovm" updated="2017-05-29 05:32:54.0"> <body><! CDATA {quote}Indeed peers need to be upgraded, as well as orderers (mainly orderers) to support V, but they will not use it untill a config tx suggesting the use of version V is announced to the BC.{quote}  Well let's not forget that all are trying to do here is make the ordering service govern the logic that the peer executes for channel validation, nothing more.  This also begs the question - Is there a way of peers at version v1.0 to "detect" that the block was created at version v1.1 ? (Maybe - if you have a raw serialized proto message - *data0   bytes* and you deserialize to to a proto object, then serialize it to bytes and get *data1   bytes* and then compare the length of the output *len(data1*) to the length of the original serialized message *len(data0)*  ?)  I saw that you updated the description. {quote}For Kafka, the latter should be easier to be implemented, but for sBFT this may get more complex. However it is a change in the proto messages that we may need to do in the future. {quote} Why do we have to choose between the first bullet and the 2nd bullet of option (2) ? I know the consensus is supposed to be pluggable but can't we say that in any case we can fall back to the 1st bullet (once a while - announce a time incrememt) for any consensus implementation, and for kafka and solo we use the 2nd bullet (put timestamp in blocks)?   {quote}In the interest of time, and since you brought up the version-issue, Yacov Manevich would you like to open a task for v1 to investigate it further? If not, i can volunteer to make a first pass (just let me know). You may link the new issue to this one here.{quote} FAB-4203   ></body> </Action>
<Action id="25835" issue="16357" author="mastersingh24" type="comment" created="2017-06-09 11:59:42.0" updateauthor="mastersingh24" updated="2017-06-09 11:59:42.0"> <body><! CDATA I don't think we should be using certificate expiration in block validation.  We don't control how certificates are issued.  I don't have a problem with checking certificate expiration during access control - e.g. trying to invoke/query a peer , send a transaction to an orderer, etc.  But transaction validation must be 100% deterministic and we have a mechanism to revoke certificates.  ></body> </Action>
<Action id="25838" issue="16357" author="yacovm" type="comment" created="2017-06-09 12:08:37.0" updateauthor="yacovm" updated="2017-06-09 20:16:22.0"> <body><! CDATA {quote}I don't think we should be using certificate expiration in block validation.  We don't control how certificates are issued.{quote} I think it makes sense.  However - right now we don't take into account expiration at access control too, since the change was in the lowest levels of the MSP. Shall we leave it as is, and open another JIRA to be addressed post v1.0 to bring back taking into account expiration everywhere but validation?  ~elli-androulaki  ,  ~adc  ^  BTW - It makes sense to also add validation of expired certificates on the SDK side. I guess that FAB-3202 should take care of it post v1.0  ></body> </Action>
<Action id="25861" issue="16357" author="troyronda" type="comment" body=" ~mastersingh24   ~yacovm  - I think we should have an item to deal with expiration everywhere but validation." created="2017-06-09 14:24:13.0" updateauthor="troyronda" updated="2017-06-09 14:24:13.0"/>
<Action id="26736" issue="16357" author="elli-androulaki" type="comment" created="2017-06-19 13:48:27.0" updateauthor="elli-androulaki" updated="2017-06-19 13:49:03.0"> <body><! CDATA  ~mastersingh24 ,  ~troyronda ,  ~yacovm , I think there are a few non-negligibly important reasons to opt in for expiration being considered (even if post-v1 only) at tx and block validation time.  One reason is that this would considerably decrease the size of announced CRLs. CRLs would need to be populated only with the id-s of certificates that haven't expired yet, and not with the rest of them.   Another reason, as I see it is that in many companies have regulations for renewal of certificates every fixed time period. Reasons for such regulations are a lot, among others also compliance with new standards that may have been produced in the meantime. Expiration enablement of fabric credentials used at validation time would provide a clean way towards enforcing such regulations on the platform. It would anyway look weird if, e.g., a long expired certificate appears in a transaction's endorsements years after the certificate has expired, and the endorsement is accepted.   Assuming a block-wide trusted timestamp, to support expiration in the current MSP we would need to extend the MSP validateIdentity function such that it takes as argument a timestamp. This timestamp acts as the base for comparison and decision for certificate expiration.  Would you think it would be a big of an issue if we extend the interface post-v1?   For the upgrade process, one could use the same mechanism that we use for chaincode (installation/instantiation) upgrade, no? Have peers run the upgraded fabric/msp locally that is backward compatible, and only exercise the new version as soon as proper fabric "upgrade" announcement is advertised via the channels. Just a thought.   ></body> </Action>
<Action id="28593" issue="16357" author="passkit" type="comment" created="2017-07-18 15:56:35.0" updateauthor="passkit" updated="2017-07-18 16:30:21.0"> <body><! CDATA I second Elli's points and add a couple from my perspective.  I think many users would be shocked to learn that an expired certificate can be used to query and invoke in v1.0.0.  With regard to CRLs and the significance of expiry:  *Firstly* Let's deal with the practicality of regulating access solely through CRLs.  Currently there is an assumption that the revoker of a certificate will handle generation and distribution of a CRL.  I would strongly suggest that anyone who supports that assumption try to generate a CRL themselves using Fabric-CA.  You will need to manually query the database, parse the information into an openssl format, build an openssl config file, create the CRL and sign it with the CA key - this is not something that users are going to be able to perform.  Generating the CRL is only half of the story.  You then need to distribute that CRL across the network.  A large network may have many peers outside the direct control of the CA Issuing entity, creating a logistical problem that simply would not be economical or timely at scale. (Edit: I now understand that a config update would handle this, but that in itself is no small task).  Ultimately, CA's would need to build both their own CRL tools, and an MSP Management/Deployment system, which kind of defeats the whole point of having a CA tool within the Fabric ecosystem.  *Secondly* For Financial Services and other regulated industries, parties and/or practitioners need to be licensed or need to maintain certifications in order to initiate or participate in transactions.  Customer data has to be regularly vetted to be able to continue to trade.  With the expect for malpractice or other irregularities which would result in immediate suspension of privileges, most of these restrictions are typically time bound.  Issuing certificates in line with such time limits provides an efficient and low cost way of preventing unauthorised transactions - with CRLs available as an emergency measure.    For high volume subscription based services where participation privileges depend on the paying of monthly or annual dues; again CRLs are not an effective solution.  On a side note - the Fabric-CA also doesn't allow for the setting of variable expiry dates.  This should also be addressed.  *Thirdly* Certificate/hey pairs are bearer instruments (meaning if you are in possession of the files, you can assume the identity) - they are not particularly effective at validating identity.  Regular rotation of certificates may in some use cases be able to mitigate the risk of certificates falling into the wrong hands (although in other cases, long expiry/low rotation may be better).  For architecture identity (endorsers, orderers, etc.) then long expiry and CRLs are probably appropriate.  But for client/transaction identity then expiry is a much more effective method of management.  The challenge then becomes how to validate that, at the time of the transaction; the certificate used to sign or endorse was valid.  We have faced this issue before with Apple when generating signed content for Wallet that has the opportunity to be installed after the certificate has expired.  A 2 stage check of the certificate expiry vs. a signing date embedded in the signature; together with a check of the certificate against the CRL in force at the time of the signature is how this is accomplished.   ></body> </Action>
<Action id="28835" issue="16357" author="mastersingh24" type="comment" created="2017-07-22 11:09:21.0" updateauthor="mastersingh24" updated="2017-07-22 11:09:21.0"> <body><! CDATA  ~passkit  - As an aside, we will be making CRL creation / management easier with Fabric CA going forward.     Clearly we can enforce expiration as part of admission control - i.e. when a client submits a transaction for endorsement or when a client submits a transaction to the orderer.  If we wish to support short-lived certificates (e.g. transaction certificates) we'll need to add a "skew" parameter as we can't guarantee that all nodes will have the exact same time and need to account for drift (this concept is nothing new - it was used in web services with signed SAML assertions for a long time).  If we just did this, then we don't really need to do anything special or change any formats (we'd just need to ensure that we check for expiry at admission time).  Next, we'd need to assess whether or not this leaves a hole at validation time since technically it would be possible to avoid endorsement and submit transactions directly to the orderer.  This of course could be hard to pull off depending on the actual endorsement policy for a specific chaincode - so perhaps we really could get away with not enforcing expiration during the commit / validation phase.  If we did decide that we needed to do this, then adding a timestamp to endorsement proposals should solve this problem.  The client would need to add a timestamp to the endorsement proposal and again we could use a "skew" time to ensure that the client did not set the timestamp in the past to leverage an expired certificate.  Then at validation / commit time, we would use this timestamp to check for expiration.   I'm not convinced that we need to go this far, but obviously needs to deeper thought     ></body> </Action>
<Action id="28837" issue="16357" author="yacovm" type="comment" created="2017-07-22 11:59:12.0" updateauthor="yacovm" updated="2017-07-22 11:59:43.0"> <body><! CDATA {quote}Next, we'd need to assess whether or not this leaves a hole at validation time since technically it would be possible to avoid endorsement and submit transactions directly to the orderer. {quote}  Isn't it possible to add a filter to the broadcast that also checks the certificates of the endorsers are valid / not expired?  ></body> </Action>
<Action id="29488" issue="16357" author="elli-androulaki" type="comment" created="2017-08-07 11:25:22.0" updateauthor="elli-androulaki" updated="2017-08-07 11:25:22.0"> <body><! CDATA We could also go all the way to what  ~vukolic ,  ~marko.vukolic  suggested with adding a real-time indication inside a signed (by the orderers) block. Then all time-related decisions, i.e., transaction expiration (to limit the replays), msp identity expiration could leverage the time of current block as reference, along with the time listed in the transaction or identity respectively.  Though my understanding of what Marko said was that this is a complex process when orderers can be byzantine, for a crash-fault tolerant model, this can be easier(?)  Real-time integration inside the blockchain can also be useful for many applications that depend on it.  ></body> </Action>
<Action id="29636" issue="16357" author="passkit" type="comment" created="2017-08-10 03:40:58.0" updateauthor="passkit" updated="2017-08-10 03:40:58.0"> <body><! CDATA Just read through the updated description.  We have a similar problem working with Apple Wallet, in that wallet content needs to be signed and the package signature validated before the content can be ingested into the wallet.  There is a similar case that signed content bundles may be residing in emails or backups, etc. and may need to be validated long after the certificate has been signed.  To solve this, the signature contains the signing date.  When validating the transaction, the certificate expiry is checked against the signing date and crl and a decision can be made whether, at the time of the signature, that certificate was valid.  I don't claim to know the intricacies of how Fabric validates and I am most likely oversimplifying or overlooking something, but none the less, both proposals above seem a little over-engineered.  ></body> </Action>
<Action id="29638" issue="16357" author="elli-androulaki" type="comment" created="2017-08-10 08:27:50.0" updateauthor="elli-androulaki" updated="2017-08-10 08:27:50.0"> <body><! CDATA Hi  ~passkit ,  Ok, so let me see if i understand this correctly as I do not have the full context of the use-case that you are referring to.  To my understanding you have the need to validate certificates asynchronously (i.e., even after the certificate has expired), and what you do is that you include timestamps inside the signed data (generated with that certificate's secret key). In this way you can use these timestamps (that you trust) as time of reference for deciding of whether the certificate was valid at the time the signature was produced. Is this the case?  So in Fabric as you mentioned we need to be able to assess the validity of certificates of all parties -- clients and endorsers (as part of endorsement policy) – at transaction validation time in a deterministic way. However, our trust model does not allow that we trust the signer of the data to include a correct timestamp in what it signs: an expired and later compromised client may attempt to add a faulty timestamp in its signed data to have the signature accepted by the system, even if the actual certificate was expired when the signature was produced.  For clients we could have the endorsers of a chaincode actually doing the check of client validity at endorsement time (using their local time), and then have the committing peers take for granted the validity of client certificate.   However, i) orderers cannot rely on such checks cause they cannot assess the validity of endorsement policy of a chaincode/credibility of endorsers, and would need some other way to figure out clients validity, while ii) we still have the issue for validating (potentially expired & compromised) endorser certificates.  The problem is brought down to having a timestamp for each given transaction, which   - we can trust to use as reference for validating certificates included in that transaction, i.e., generated by parties that do not have motives to modify this timestamp to alter the certificate validation result   - would be the same for all entities that receive this transaction at commitment phase, so that validation of certificates included in a transaction ends up being a deterministic process.  Any suggestions are more than welcome!     ></body> </Action>
<Action id="29642" issue="16357" author="passkit" type="comment" created="2017-08-10 09:28:16.0" updateauthor="passkit" updated="2017-08-10 09:28:16.0"> <body><! CDATA Elli, yes you are correct in your understanding, you have exactly described our case.  At the root of any transaction, you have the transaction proposal. This (surely) contains an auditable and immutable point in time when it was first submitted to the network, and we enforce that this time cannot be in the past. _Note the actual time of the asset exchange recorded in the proposal, could potentially be in the past, provided the chain logic allowed it._  Any subsequent endorsements or ordering must be ahead of the proposal timestamp. All nodes in the commitment phase will therefore need to have to have a current certificate at the time of signing, as backdating an endorsement date would (with all probability) result in a date earlier than the proposal submission date.  As blocks are immutable, I am having trouble understanding why would retrospective validation would pose an issue?  What I don't understand is: {quote}However, our trust model does not allow that we trust the signer of the data to include a correct timestamp in what it signs: an expired and later compromised client may attempt to add a faulty timestamp in its signed data to have the signature accepted by the system, even if the actual certificate was expired when the signature was produced. {quote} Why, if a timestamp is signed, would you not allow it to be trusted. Under what circumstances could a later compromised client retrospectively alter a transaction already committed to the chain?  As mentioned, I do not know the intricate details of how each party determines validity, but it just strikes me as odd that it would require one of the options above to address this issue. I'm left feeling that we are uncovering some fundamental shortcomings in the original design.  ></body> </Action>
<Action id="29659" issue="16357" author="mastersingh24" type="comment" created="2017-08-10 16:02:27.0" updateauthor="mastersingh24" updated="2017-08-10 16:02:27.0"> <body><! CDATA  ~passkit   -  We basically have 3 issues to deal with:  1) Distributed trust (or lack thereof)  2) Distributed time  3) Determinism (which is problematic mainly due to 2) but potentially with 1) as well)     In thinking about this more, here's a proposal:  1) We should be able to enforce expiration as part of admission control - i.e. when a peer receives a request and when the orderer receives a request.  If we get into short-lived transaction certificates, we can also add an agreed upon "skew" time (as was common practice for signed SAML asserts in the WS-* days)     2) For endorsement responses, we could actually add a timestamp as part of the signed response.  I'd advice that we actually use some type of skew here to account for drift (e.g. round to the nearest 15 min, 30 min, hour, whatever).  The skew could add a small window where an expired cert could get through, but hopefully 1) would filter out most bad client transactions.   I think it's fair to assume / force that all peers in the network be responsible for maintaining accurate time and using a skew to round up will help given that drift from time servers is usually in minutes     So now by the time the transaction gets back to the peer for validation, you'd have the timestamp for when the transaction occurred in the *signed proposal response*   Maybe I'm missing something but seems like something like this would work.      ~elli-androulaki  thoughts?        ></body> </Action>
<Action id="29997" issue="16357" author="elli-androulaki" type="comment" body="Regardless of the way we choose to allow for expiration, identity validation needs to take as input a timestamp of reference, hence we created FAB-5778." created="2017-08-18 16:41:16.0" updateauthor="elli-androulaki" updated="2017-08-21 14:18:20.0"/>
<Action id="30044" issue="16357" author="elli-androulaki" type="comment" created="2017-08-21 14:21:34.0" updateauthor="elli-androulaki" updated="2017-08-21 14:28:32.0"> <body><! CDATA  ~mastersingh24 , per your recommendation, we created a document listing potential ways to support identity expiration in Fabric:  https://docs.google.com/document/d/1nevpNkdGTRne2rLyrnVlwY9_keLnL5DTmE_tZb5QgoM/edit?usp=sharing    ~mastersingh24 ,  ~passkit ,  ~jyellick ,  ~troyronda ,  ~yacovm  your input would be very welcome!  Adding  ~angelo.decaro ,  ~ales ,  ~mne .  ></body> </Action>
<Action id="32643" issue="16357" author="elli-androulaki" type="comment" body="Hi, I updated the document on support for  identity expiration support|https://docs.google.com/document/d/1nevpNkdGTRne2rLyrnVlwY9_keLnL5DTmE_tZb5QgoM/edit?usp=sharing  based on more recent discussions on this with  ~jyellick . Feedback on the document or in this JIRA would be very welcome!" created="2017-10-18 16:29:47.0" updateauthor="elli-androulaki" updated="2017-10-18 16:29:47.0"/>
<Action id="40603" issue="16357" author="mastersingh24" type="comment" body="Moving this to the 1.3 bucket for now" created="2018-02-22 21:21:11.0" updateauthor="mastersingh24" updated="2018-02-22 21:21:11.0"/>
<Action id="51815" issue="16357" author="vdods" type="comment" body="I&apos;m getting &quot;x509: certificate has expired or is not yet valid&quot; because of this again, even though the temp hack in mspimplvalidate.go getValidityOptsForCert should prevent this by construction (because it sets the validation options CurrentTime to one second after the NotBefore time for the cert).  Oddly, adding time.Minute also fails, but adding time.Hour works.  I&apos;ve pored over the relevant code (even the crypto/x509 and time packages), added a bunch of debug printouts of my own making the analogous cert time checks, and can&apos;t find why this shouldn&apos;t work already.  It&apos;s really perplexing." created="2018-10-06 09:41:01.0" updateauthor="vdods" updated="2018-10-06 09:41:01.0"/>
<Action id="51816" issue="16357" author="vdods" type="comment" body="I haven&apos;t been able to find a workaround besides making my own fork of fabric and hacking the hack to add time.Hour instead of time.Second.  What do people think about making this change in the codebase?" created="2018-10-06 10:43:49.0" updateauthor="vdods" updated="2018-10-06 10:43:49.0"/>
<Action id="51817" issue="16357" author="yacovm" type="comment" body="What is the expiration (NotBefore field) of the CAs in the certification chain of the leaf certificate?" created="2018-10-06 12:51:51.0" updateauthor="yacovm" updated="2018-10-06 12:57:51.0"/>
