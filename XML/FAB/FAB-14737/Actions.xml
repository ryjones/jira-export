<Action id="58360" issue="38613" author="jyellick" type="comment" body=" ~guoger  What do you think?" created="2019-03-21 03:31:12.0" updateauthor="jyellick" updated="2019-03-21 03:31:12.0"/>
<Action id="58430" issue="38613" author="guoger" type="comment" created="2019-03-22 07:27:05.0" updateauthor="guoger" updated="2019-03-22 07:27:05.0"> <body><! CDATA  ~jyellick  i think the bigger problem here is why does node 9 repeatedly starts election in that period of time...  ~dongming  do you have more info so i can debug?  the log message is from etcd/raft lib itself, we could probably name the logger passed into it, so user can choose to set different level for it.  ></body> </Action>
<Action id="58456" issue="38613" author="dongming" type="comment" created="2019-03-22 14:18:43.0" updateauthor="dongming" updated="2019-03-22 14:28:05.0"> <body><! CDATA  ~guoger  I attached a snippet of log here.  The full log is available in LogDNA-community-quality-lab, search for orderer10-ordererorg2 at timestamp `Mar 19 6:16:00pm`. This is a stress test with Log Level=INFO.  ></body> </Action>
<Action id="58822" issue="38613" author="guoger" type="comment" created="2019-04-02 03:10:27.0" updateauthor="guoger" updated="2019-04-02 03:11:25.0"> <body><! CDATA My theory is, node 9 sent {{MsgPreVote}} at reasonable interval (HeartbeatTimeout), however those messages are not processed instantaneously at recipient (bulked in either sender or receiver buffer). However, this turbulence shouldn't prevent cluster from eventually stablizing, as the test eventually passed election phase and started serving requests. (To confirm this theory, we'll need the log of orderer 9.)  Therefore, I think this can be tackled at communication layer (maybe through QoS)in the future, when we see a valid use case where dynamic network (nodes come and go) with dozens of nodes is needed.  ></body> </Action>
<Action id="58952" issue="38613" author="scottz" type="comment" body="J says this orderer is quickly processing the prevotes in the ingress Q from that other orderer. It is not a problem. And if anything it is a symptom of turbulence/delays due to other reasons." created="2019-04-04 18:14:48.0" updateauthor="scottz" updated="2019-04-04 18:14:48.0"/>
