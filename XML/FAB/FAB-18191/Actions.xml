<Action id="70074" issue="45941" author="btl5037" type="comment" body="Essentially the ask is, can we delete the contents of the directories, rather than the directories themselves, as these are canonically the mount points for implementations such as Kubernetes. While the deletion is simple, does the recreation depend on the directories not existing. Looking at some comments, I see that it may be the case that the ledger management system may use the existence of the directory to decide whether it needs to reinitialize, so the existence of the directory(even an empty one) may look like the db already exists." created="2020-08-26 23:09:15.0" updateauthor="btl5037" updated="2020-08-26 23:17:58.0"/>
<Action id="70077" issue="45941" author="manish-sethi" type="comment" body=" ~btl5037  - this should be a fairly simple fix and empty directory should not be a problem." created="2020-08-27 00:27:57.0" updateauthor="manish-sethi" updated="2020-08-27 00:27:57.0"/>
<Action id="70078" issue="45941" author="senthil1" type="comment" created="2020-08-27 03:38:36.0" updateauthor="senthil1" updated="2020-08-27 03:38:36.0"> <body><! CDATA How is it a bug?  As per the spec, the user/admin is supposed to pass only the parent filesystem directory to the peer. The folders for databases such as pvtdatastore, confighistory, purgemgr, snapshot bookkeeper, and stateleveldb are created and managed by the peer internally. Even the relative pathname is the logic of the peer ( https://github.com/hyperledger/fabric/blob/42166984577a5577adaeedb1052bbe5a71a643fe/core/ledger/kvledger/ledger_filepath.go#L1)  and never exposed to the user/admin.  While this can be fixed simply, going forward, the suggestion would be not to use anything that is not exposed via core.yaml or not present in the deployment recommendation (such as using separate volume for the CouchDB). Otherwise, it might pop up so many unnecessary problems like this one.  If there is a need for providing a separate mount point for each internal database, the correct way to doing this is by exposing all folder paths via core.yaml.  ></body> </Action>
<Action id="70079" issue="45941" author="btl5037" type="comment" body="Our official backup doc actually states you can consider backing up only a subset of the directories specified in core.yaml rootfilesystem for the purposes of reducing the overall size of the backup. In particular it actually calls out all of the directories mentioned in your link as directories, you may or may not want to backup. I would consider the implementation of mounting a volume at the location of the subset of data you want to backup to be canonical for both kubernetes and docker deployments." created="2020-08-27 04:11:12.0" updateauthor="btl5037" updated="2020-08-27 04:48:00.0"/>
<Action id="70080" issue="45941" author="senthil1" type="comment" created="2020-08-27 05:10:37.0" updateauthor="senthil1" updated="2020-08-27 05:10:37.0"> <body><! CDATA The current assumption is that the peer will create and delete directories (https://github.com/hyperledger/fabric/blob/42166984577a5577adaeedb1052bbe5a71a643fe/core/ledger/kvledger/drop_dbs.go#L15) in a given filesystem path to store/remove data whenever needed. Admin is allowed to copy all or part of data for various purposes (similar to what one can do to directories created and managed by PostgreSQL - https://www.postgresql.org/docs/12/storage-file-layout.html). # Why can't we mount a persisted volume to the parent filesystem path alone which holds all directories? # Are we creating a persisted volume for each directory in the ledgerData separately? If we are not creating a separate one for each directory, why some directories use a common persisted volume?  If it is a must to mount each directory in a separate persisted volume, as I said earlier, exposing all store path via core.yaml seems the correct way so that the peer would know that it is not allowed to create/delete the directory (but needs to do it carefully as it would affect existing deployment) – I wouldn't prefer this unless there is a strong reason/requirement.  ></body> </Action>
<Action id="70081" issue="45941" author="btl5037" type="comment" created="2020-08-27 05:34:35.0" updateauthor="btl5037" updated="2020-08-27 05:48:42.0"> <body><! CDATA The user is only backing up their `stateleveldb` directory. Just like they only back up their `/opt/couchdb/data` directory in couchdb. They are simply trying to backup their database data in both instances. What we are telling them is they need to pay for additional storage (which is a very large portion of the cost of running a fabric network) to back up all of these additional things they don't care about losing, and that they can't backup just their database on its own.   I'm looking at our production test network which has only a few 100,000 blocks and the size of those directories is not insignificant (100's GB per peer). And that is with one channel with very small data sizes, for a network with many channels and hundreds of peers, this could very easily grow to 100's of TB across the network of data that doesn't need to be backed up. That's millions of dollars a year in unnecessary cost at the current block storage rates.  So, to your point that postgres expects you to backup the root data directory, all the user is asking is to be able to backup their root data directory for leveldb as well. An apples to apple comparison would be couchdb telling its users to backup `/opt/couchdb` (which is their equivalent of our rootFSPath) which they dont do. So perhaps we do need to expose the statedb's directory in core.yaml and handle it accordingly.  ></body> </Action>
<Action id="70082" issue="45941" author="senthil1" type="comment" created="2020-08-27 06:13:07.0" updateauthor="senthil1" updated="2020-08-27 06:13:07.0"> <body><! CDATA What is the use of only backing up stateLevelDB data? For the proper recovery, we need either only blockstore data or the whole ledgerData directory. # With a block store, we can reconstruct all other data directory. # With only a block store and stateLevelDB, the peer would be started with an inconsistent/incorrect state, i.e., purge manager and collection config history DB would be empty and will not be synced to the current block store height (as per the current peer recovery mechanism).  We need to update this  https://hyperledger-fabric.readthedocs.io/en/release-2.2/upgrading_your_components.html#ledger-backup-and-restore  to be very explicit on what must be backed up.  For us, the whole _ledgerData_ directory is the same as the PostgreSQL _PGDATA_ directory. The blockstore is similar to the pg_wal directory in the PostgreSQL. Similar to how one can backup only pg_wal ( https://www.postgresql.org/docs/13/continuous-archiving.html),  we can also backup only blockstore and still recover a peer.  On the storage problem, most of the storage is occupied by the blockstore. Each certificate storage takes around 1KB. Most endorsement certificates, client certificates, orderer certificates per transaction would be duplicate with the old transaction. We can save a significant amount of storage using some deduplication technique if we need to address this problem in a correct/long term view.  ></body> </Action>
<Action id="70114" issue="45941" author="wenjian" type="comment" body="PR:  https://github.com/hyperledger/fabric/pull/1828 " created="2020-09-01 13:54:35.0" updateauthor="wenjian" updated="2020-09-01 13:54:35.0"/>
<Action id="70130" issue="45941" author="wenjian" type="comment" body="PR backported to release-2.2:  https://github.com/hyperledger/fabric/pull/1833 " created="2020-09-02 18:31:27.0" updateauthor="wenjian" updated="2020-09-02 18:31:27.0"/>
