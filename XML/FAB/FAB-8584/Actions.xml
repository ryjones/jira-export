<Action id="41039" issue="28106" author="scottz" type="comment" created="2018-02-28 21:03:03.0" updateauthor="scottz" updated="2018-02-28 22:35:54.0"> <body><! CDATA Observations: only 2 of 3 orderers panicked, about 40 secs after kafka2 went down, after seeing a few SERVICE_UNAVAILALBE logs. RAM Memory was low. Plenty of free disk space on hard drive.  We will try to reproduce with orderer log level set to DEBUG instead of ERROR.  This is the panic log in the orderer: {code:java} 2018-02-28 18:38:06.763 UTC  common/deliver  deliverBlocks -> ERRO 040  channel: testorgschannel8  Error reading from channel, cause was: SERVICE_UNAVAILABLE panic: interface conversion: ledger.QueryResult is nil, not *common.Block{code}  ></body> </Action>
<Action id="41041" issue="28106" author="clayton sims" type="comment" body=" ~denyeart   I removed &apos;must-fix&apos; tag from this per Kostas and Barry." created="2018-02-28 22:49:54.0" updateauthor="clayton sims" updated="2018-02-28 22:49:54.0"/>
<Action id="41043" issue="28106" author="suryalnvs" type="comment" created="2018-02-28 23:17:41.0" updateauthor="suryalnvs" updated="2018-02-28 23:17:41.0"> <body><! CDATA Attached multihost_instantiate_8channels.tar.gz.txt file with logs for 3 orderers, 5 kafkas, 3 zookeepers, and 1 peer with debug logs.  Environment: Total 8 machines with 16gb ram, 160 gb disk space.  5 kafkas are distributed across 5 machines,  3 zookeepers across 3 machines,  3 orderers across 3 machines,  64 peers distributed across 8 machines with 8 peers on each.  we have one orderer, one zookeeper and one kafka on wfabric001, wfabric002, wfabric003 machines. {code:java} ubuntu@wfabric001:~$ df -h Filesystem      Size  Used Avail Use% Mounted on udev            7.9G     0  7.9G   0% /dev tmpfs           1.6G   26M  1.6G   2% /run /dev/vda1       156G  5.7G  150G   4% / tmpfs           7.9G  1.1M  7.9G   1% /dev/shm tmpfs           5.0M     0  5.0M   0% /run/lock tmpfs           7.9G     0  7.9G   0% /sys/fs/cgroup tmpfs           1.6G     0  1.6G   0% /run/user/1000 ubuntu@wfabric001:~$ free -m               total        used        free      shared  buff/cache   available Mem:          16046        1331        9244          26        5471       14310 Swap:             0           0           0 {code} {code:java}  ubuntu@wfabric002:~$ df -h Filesystem      Size  Used Avail Use% Mounted on udev            7.9G     0  7.9G   0% /dev tmpfs           1.6G   26M  1.6G   2% /run /dev/vda1       156G  5.6G  150G   4% / tmpfs           7.9G  992K  7.9G   1% /dev/shm tmpfs           5.0M     0  5.0M   0% /run/lock tmpfs           7.9G     0  7.9G   0% /sys/fs/cgroup tmpfs           1.6G     0  1.6G   0% /run/user/1000 ubuntu@wfabric002:~$ free -m               total        used        free      shared  buff/cache   available Mem:          16046        1775        8871          26        5399       13868 Swap:             0           0           0  {code} {code:java}  ubuntu@wfabric003:~$ df -h Filesystem      Size  Used Avail Use% Mounted on udev            7.9G     0  7.9G   0% /dev tmpfs           1.6G   26M  1.6G   2% /run /dev/vda1       156G  5.6G  150G   4% / tmpfs           7.9G 1012K  7.9G   1% /dev/shm tmpfs           5.0M     0  5.0M   0% /run/lock tmpfs           7.9G     0  7.9G   0% /sys/fs/cgroup tmpfs           1.6G     0  1.6G   0% /run/user/1000 ubuntu@wfabric003:~$ free -m               total        used        free      shared  buff/cache   available Mem:          16046        1351        9270          26        5425       14292 Swap:             0           0           0  {code}       ></body> </Action>
<Action id="41044" issue="28106" author="scottz" type="comment" created="2018-02-28 23:23:34.0" updateauthor="scottz" updated="2018-02-28 23:23:34.0"> <body><! CDATA Surya is attaching logs. We reproduced the error, using a different scenario (instantiating a chaincode in 4 channels simultaneously). This time we are sure there was no memory problem. But, like last time, we ran several docker containers on a single host (this time, each host has something like 8 peers and an orderer and/or kafka and/or a zookeeper). So,  ~kchristidis  's theory still makes sense - about the root cause of the trouble on the zookeeper stemming from the fact the i/o contention. (Refer to https://community.hortonworks.com/articles/105059/zookeeper-health-checks.html .) And we realize that good production environments should set up ZKs on their own host; that is an existing ZK recommendation.  But we would still like to make sure the orderer handles the situation better than panicking because of a null pointer.  ></body> </Action>
<Action id="41046" issue="28106" author="denyeart" type="comment" body=" ~kchristidis  What&apos;s your opinion on the null pointer?" created="2018-02-28 23:51:42.0" updateauthor="denyeart" updated="2018-02-28 23:51:42.0"/>
<Action id="41053" issue="28106" author="kchristidis" type="comment" created="2018-03-01 02:47:22.0" updateauthor="kchristidis" updated="2018-03-01 02:55:31.0"> <body><! CDATA  ~scottz ,  ~suryalnvs : # How many channels are there total? (Scott talks about 4 channels, the log attached by Surya talks about 8 channels.) What are their names? (UPDATE: Based on what I see in the logs: {{firstchannel}}, and {{testorgschannel1-8}}?) # How many blocks have your client(s) received on each? # Please list the contents of the {{/var/hyperledger/production/orderer}} directory on each of the orderers. (Include the contents of subdirectories.)  ></body> </Action>
<Action id="41054" issue="28106" author="suryalnvs" type="comment" body="We had created 8 channels in total, but separated out the instantiation into two. We did instantiation on first 4 channels and seen the issue. The channel names are testorgschannel1, testorgschannel2 through testorgschannel8. I am not sure about the blocks." created="2018-03-01 02:53:49.0" updateauthor="suryalnvs" updated="2018-03-01 02:53:49.0"/>
<Action id="41055" issue="28106" author="kchristidis" type="comment" created="2018-03-01 03:04:34.0" updateauthor="kchristidis" updated="2018-03-01 03:06:42.0"> <body><! CDATA Surya, thanks for the follow-up.  I will need the info about the blocks when you have it handy.  Can you please also provide me with a play-by-play of every step exercised by your test?  You have given me the hardware setup above (wfabric001 to 003), but what exactly happens in the test is a bit fuzzier than it should be (note: I did read FAB-8208). Something along these lines will help: # 8 channels are created: foo, bar, baz, ... # The same chaincode is instantiated on channels foo, bar, ... # 32 threads start invoking these chaincodes at a rate of 2tps. (How are the requests spread out across channels?) # How about delivery? (etc., etc.)  ></body> </Action>
<Action id="41056" issue="28106" author="kchristidis" type="comment" body="While I&apos;m looking at this tomorrow, I&apos;d like to ask that we spend some cycles on the following - let&apos;s attempt to find the minimum possible configuration that gives this error. For example: can we reproduce with just 1 channel? Please keep the DEBUG level for Fabric logs, but feel free to turn down the verbosity for the sarama logger. This doesn&apos;t seem to be part of the problem." created="2018-03-01 03:08:52.0" updateauthor="kchristidis" updated="2018-03-01 03:08:52.0"/>
<Action id="41057" issue="28106" author="denyeart" type="comment" created="2018-03-01 04:09:47.0" updateauthor="denyeart" updated="2018-03-01 04:09:47.0"> <body><! CDATA  ~kchristidis  I think Scott's concern isn't so much that kafka/zookeeper had a problem, but how orderer handled the problem.  Specifically when the iterator exited prematurely and a nil result comes out, the nil is attempted to be converted into a block:   https://github.com/hyperledger/fabric/blob/master/common/ledger/blockledger/file/impl.go#L73   I think a nil check needs to be added there and handled gracefully.   ~manish-sethi  can speak more to the iterator contract, if that is needed.      ></body> </Action>
<Action id="41058" issue="28106" author="kchristidis" type="comment" body="Been studying the logs and the iterator code in detail after the original post and yes, you&apos;re right -- I&apos;m at the same spot." created="2018-03-01 04:27:12.0" updateauthor="kchristidis" updated="2018-03-01 04:27:12.0"/>
<Action id="41059" issue="28106" author="kchristidis" type="comment" body="Will have this fixed tomorrow." created="2018-03-01 04:28:40.0" updateauthor="kchristidis" updated="2018-03-01 04:28:40.0"/>
<Action id="41063" issue="28106" author="kchristidis" type="comment" created="2018-03-01 05:20:48.0" updateauthor="kchristidis" updated="2018-03-01 05:23:23.0"> <body><! CDATA So building on what Dave reported, here's the play-by-play of how the bug comes to be: # The Deliver handler has spawned a goroutine that waits for the next block from the ledger:  https://github.com/hyperledger/fabric/blob/v1.1.0-alpha/common/deliver/deliver.go#L304  # The Deliver handler is also listening on the local cancel channel for anything that may go wrong in the orderer-to-Kafka connection:  https://github.com/hyperledger/fabric/blob/v1.1.0-alpha/common/deliver/deliver.go#L312  # The cancel channel closes because the orderer-to-Kafka connection breaks. # The Deliver handler closes the iterator and is then about to return an error message to the invoker but... # That closing of the iterator means that the iterator returns a {{nil}} block and a {{nil}} error (as they should):  https://github.com/hyperledger/fabric/blob/v1.1.0-alpha/common/deliver/deliver.go#L77  # This {{nil, nil}} combo is not expected by the goroutine (see step 1) which tries to convert the first return value into a block, thinking that it's business as usual:  https://github.com/hyperledger/fabric/blob/v1.1.0-alpha/common/ledger/blockledger/file/impl.go#L73   On a sidenote, I suspect that the problem stems from a misunderstanding from our side of what the {{Next}} method of the {{Iterator}} interface is supposed to do. It _is_ the case that "Next blocks until there is a new block available, or returns an error if the next block is no longer retrievable" for the JSON and RAM ledgers, but this is no longer the case for the file ledger (which wraps the block ledger) because now Next can also block until Close is called. (I will leave the comment on that interface intact for now, but this is a good cue for us to remove the RAM/JSON implementations.)  Anyway, non-fancy patch pushed here:  https://gerrit.hyperledger.org/r/c/18585/   ></body> </Action>
<Action id="41103" issue="28106" author="dongming" type="comment" created="2018-03-01 19:11:57.0" updateauthor="dongming" updated="2018-03-01 19:11:57.0"> <body><! CDATA Retest with the fix,  the orderer logs show the same "SERVICE_UNAVAILBLE" message in deliverBlocks, but orderer panic does not occur when kafkas go down..  2018-03-01 17:54:18.872 UTC  common/deliver  deliverBlocks -> ERRO 2c7  channel: testorgschannel3  Error reading from channel, cause was: SERVICE_UNAVAILABLE 2018-03-01 17:54:18.862 UTC  common/deliver  deliverBlocks -> ERRO 2c6  channel: testorgschannel3  Error reading from channel, cause was: SERVICE_UNAVAILABLE     ></body> </Action>
<Action id="41118" issue="28106" author="kchristidis" type="comment" created="2018-03-01 21:28:54.0" updateauthor="kchristidis" updated="2018-03-01 21:28:54.0"> <body><! CDATA Great. That is the expected behavior. Thanks for the update.  The overall problem that you're seeing with this test is most likely due to ZK having to contend for disk access with other process. The article that Scott linked to above is a good introduction into the problem: https://community.hortonworks.com/articles/105059/zookeeper-health-checks.html  ></body> </Action>
