<Action id="45890" issue="30995" author="wlahti" type="comment" body="Are you actively trying to use the eventhub for this scenario? Is it possible to use the peer&apos;s new deliver service instead? I believe the Java SDK refers to it as the ChannelEventHub. " created="2018-06-13 16:07:35.0" updateauthor="wlahti" updated="2018-06-13 16:07:35.0"/>
<Action id="45895" issue="30995" author="aatkddny" type="comment" created="2018-06-13 17:15:17.0" updateauthor="aatkddny" updated="2018-06-13 17:15:17.0"> <body><! CDATA It does it regardless of what you are doing.   In this particular case there's no eventing at all defined in the peer or the client.     This is purely a load issue - I've tried single threaded, 10 threads and 200 threads and the first two work and the last doesn't.    ></body> </Action>
<Action id="45900" issue="30995" author="rickr" type="comment" created="2018-06-13 18:42:52.0" updateauthor="rickr" updated="2018-06-13 18:42:52.0"> <body><! CDATA Alan as we discussed already in rocket chat .. this happens even when you have no _legacy_ EventHubs created in your client side code correct ? (y/n) ?    Now *without* the legacy EventHub created does this happen if on your client side you also don't create a Peer that has an event source role defined ?  Does it make any difference ?   ></body> </Action>
<Action id="45901" issue="30995" author="aatkddny" type="comment" created="2018-06-13 18:56:26.0" updateauthor="aatkddny" updated="2018-06-13 18:56:26.0"> <body><! CDATA Rick, as I tried to say, it happens all ways.   I've tried it all four ways. Event url, no event url, source true and source false. It happens with every combination I can think of.  Every single time I get that peer log message and then the client chokes.   Frankly if I could have figured out a way to pump in 15k values into a default block setup (2s or 10tx cut) in a reasonable timeframe I'd have done that and not bothered with this jira. This all started because I was waiting 2s for each tx to complete in a giant for loop and wanted to speed it up.   ></body> </Action>
<Action id="45903" issue="30995" author="rickr" type="comment" created="2018-06-13 19:30:33.0" updateauthor="rickr" updated="2018-06-13 19:30:33.0"> <body><! CDATA Ok I just wanted to make sure it was clear what the client side (SDK) configurations with respect to eventhubs and peer's with eventing service you experienced this with.   Thanks.  ></body> </Action>
<Action id="45906" issue="30995" author="wlahti" type="comment" created="2018-06-13 20:03:46.0" updateauthor="wlahti" updated="2018-06-13 20:09:14.0"> <body><! CDATA When you set PeerRole.EVENT_SOURCE for each of your connections that's using the java sdk client, you still see the exact error message you listed?  {code:java} 15:49:44.435 UTC  eventhub_producer  Chat -> ERRO da13 error during Chat, stopping handler: rpc error: code = Canceled desc = context canceled{code} What's the nature of the network setup you're using? It is your own local setup where only you are connecting to the peer(s)?  Looking at the eventhub code, I don't think that error message should ever occur unless some client(s) are still connecting to the eventhub's "Chat" service on the peer.     FYI: I'm also following-up to see if there's support for adding a way to explicitly disable the eventhub, which I think will benefit all working on/with fabric.  ></body> </Action>
<Action id="45910" issue="30995" author="aatkddny" type="comment" created="2018-06-13 21:45:19.0" updateauthor="aatkddny" updated="2018-06-13 21:45:19.0"> <body><! CDATA I don't set  PeerRole.EVENT_SOURCE directly. This is using the newer load from network config call. That role is triggered by the eventSource in the YAML and I've tried it both ways.     ></body> </Action>
<Action id="45911" issue="30995" author="rickr" type="comment" created="2018-06-13 22:22:40.0" updateauthor="rickr" updated="2018-06-13 22:23:02.0"> <body><! CDATA Alan, can you for the scenario where you don't have any event urls in your network config add this after intializiing the channel :  if (!_channel_.getEventHubs().isEmpty()) { throw new RuntimeException("didn't expect this :" + _channel_.getEventHubs().size()) }  make sure none are being created.  Then recreate ..  appreciate it.   ></body> </Action>
<Action id="45912" issue="30995" author="wlahti" type="comment" created="2018-06-13 22:39:14.0" updateauthor="wlahti" updated="2018-06-13 22:39:14.0"> <body><! CDATA Thanks, Rick. That should be a good datapoint to help us get to the bottom of this.   I've been chatting with Alan in rocketchat and he's planning to recreate this again soon and send us the peer logs.  ~aatkddny , hopefully you can also try Rick's suggestion above to see if there's still somehow an eventhub connection created by the SDK?  Alan also mentioned one other thing that I'll note here as it raises another possible cause: "there's only one application connecting to the channel". Alan, could you also confirm if there are any other applications connecting to other channels through the same peer? The eventhub is scoped at the peer level so any blocks committed to any channel's ledger on the peer will be sent as a block event to any clients listening to the eventhub even if that client is transacting on a different channel from the one you're testing. This may be completely irrelevant to this situation but I wanted to mention it since I don't have the full context and it could definitely cause this behavior on a peer under heavy load.  ></body> </Action>
<Action id="45913" issue="30995" author="aatkddny" type="comment" created="2018-06-14 01:35:49.0" updateauthor="aatkddny" updated="2018-06-14 01:35:49.0"> <body><! CDATA  ~rickr  - it doesn't throw the runtime exception after the call to:   {{channel = client.loadChannelFromConfig(name, org.getNetworkConfig());}}  ></body> </Action>
<Action id="45914" issue="30995" author="aatkddny" type="comment" created="2018-06-14 01:49:46.0" updateauthor="aatkddny" updated="2018-06-14 01:51:00.0"> <body><! CDATA ok so i added three files. # the networkconfig file i'm using for this fabric to be sure that's not messed up. # the peer log from the peer that went wrong. unfortunately now while it's throwing the exceptions in the java code i can't find the  eventhub_producer ERRO from above. There are  eventhub_producer calls in it. # the java log. I stopped it too late and it doesn't show the start of the errors. You get the idea though.  Edit: Uck I changed configTx this afternoon. This one has  MaxMessageCount: 200 instead of the default of 10. I will regenerate and rerun if necessary, please let me know.  ></body> </Action>
<Action id="45920" issue="30995" author="rickr" type="comment" body="Fairly ceratin then this code is not using any legacy EventHubs." created="2018-06-14 12:04:32.0" updateauthor="rickr" updated="2018-06-14 12:04:32.0"/>
<Action id="45921" issue="30995" author="wlahti" type="comment" created="2018-06-14 12:10:35.0" updateauthor="wlahti" updated="2018-06-14 12:10:35.0"> <body><! CDATA In that log, I don't see any issues on the peer side. We see connections to the peer's deliver service (logged by "common/deliver"), which is the service the Java SDK's EVENT_SOURCE role refers to, and blocks being sent out. From a quick search, I notice 8 connections to the deliver service.  The only eventhub log messages are the expected ones since the eventhub design will always receive a block on its event processor queue (as noted by the poorly worded log message " eventhub_producer  Send -> DEBU c06 Event sent successfully" for which I am responsible adding when I was new to the project/golang and didn't quite understand the eventhub's (poor) design) which will simply be discarded when no clients are connected.   ></body> </Action>
<Action id="45945" issue="30995" author="aatkddny" type="comment" created="2018-06-14 19:32:45.0" updateauthor="aatkddny" updated="2018-06-14 19:32:45.0"> <body><! CDATA since the eventhub part of this now doesn't reproduce this might help with the client side issue  so just to kill this for good - the client side may be fixed with an upgrade of the grpc code to 1.12 - or at least this very similar sounding error implies that that is the case.  https://github.com/grpc/grpc-java/issues/2601      ></body> </Action>
<Action id="45976" issue="30995" author="wlahti" type="comment" body=" ~aatkddny  Would you mind updating the title, description, and component now that we&apos;ve successfully bypassed the eventhub? It sounds like this is now likely an issue with the GRPC version used by the 1.1 Java SDK. " created="2018-06-15 14:11:13.0" updateauthor="wlahti" updated="2018-06-15 14:13:20.0"/>
<Action id="45977" issue="30995" author="aatkddny" type="comment" created="2018-06-15 14:26:30.0" updateauthor="aatkddny" updated="2018-06-15 14:26:30.0"> <body><! CDATA  ~wlahti  - done.  Note for completeness, I just ran same on 1.2.0-SNAPSHOT vs a 1.1 Fabric.  Same problem occurs so the comment I added yesterday doesn't address this particular issue.  Note bolded grpc version in the exception below.     {{2018-06-15 10:20:31.868 ERROR 12721 ---  ool-7-thread-22  org.hyperledger.fabric.sdk.Channel : Sending proposal to xxx-peer1.xxx.com failed because of: gRPC failure=Status\{code=UNAVAILABLE, description=Channel shutdownNow invoked, cause=null}}}{{java.lang.Exception: io.grpc.StatusRuntimeException: UNAVAILABLE: Channel shutdownNow invoked}} {{ at org.hyperledger.fabric.sdk.Channel.sendProposalToPeers(Channel.java:2766)  fabric-sdk-java-1.2.0-SNAPSHOT.jar:na }} {{ at org.hyperledger.fabric.sdk.Channel.sendProposal(Channel.java:2672)  fabric-sdk-java-1.2.0-SNAPSHOT.jar:na }} {{ at org.hyperledger.fabric.sdk.Channel.sendTransactionProposal(Channel.java:2557)  fabric-sdk-java-1.2.0-SNAPSHOT.jar:na }} {{ at com.mo.fabric.facade.InvokeFabric.invoke(InvokeFabric.java:138)  classes/:na }} {{ at com.mo.dare.processor.Processor.write(Processor.java:191)  classes/:na }} {{ at com.mo.dare.processor.Processor.create(Processor.java:94)  classes/:na }} {{ at com.mo.dare.endpoint.AgencyEndpoint.create(AgencyEndpoint.java:37)  classes/:na }} {{ at com.mo.dare.camel.AgencyPersist.process(AgencyPersist.java:34)  classes/:na }} {{ at org.apache.camel.component.bean.BeanProcessor.process(BeanProcessor.java:103)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:77)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:541)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:198)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:541)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:198)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:109)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.MulticastProcessor.doProcessParallel(MulticastProcessor.java:841)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.MulticastProcessor.access$200(MulticastProcessor.java:85)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.MulticastProcessor$1.call(MulticastProcessor.java:328)  camel-core-2.19.3.jar:2.19.3 }} {{ at org.apache.camel.processor.MulticastProcessor$1.call(MulticastProcessor.java:314)  camel-core-2.19.3.jar:2.19.3 }} {{ at java.util.concurrent.FutureTask.run(FutureTask.java:266)  na:1.8.0_162 }} {{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)  na:1.8.0_162 }} {{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)  na:1.8.0_162 }} {{ at java.lang.Thread.run(Thread.java:748)  na:1.8.0_162 }} {{Caused by: io.grpc.StatusRuntimeException: UNAVAILABLE: Channel shutdownNow invoked}} {{ at io.grpc.Status.asRuntimeException(Status.java:526) ~{color:#ff0000}* grpc-core-1.12.0.jar:1.12.0 *{color}}} {{ at io.grpc.stub.ClientCalls$UnaryStreamToFuture.onClose(ClientCalls.java:467) ~{color:#ff0000}* grpc-stub-1.12.0.jar:1.12.0 *{color}}} {{ at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.CensusStatsModule$StatsClientInterceptor$1$1.onClose(CensusStatsModule.java:684) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.PartialForwardingClientCallListener.onClose(PartialForwardingClientCallListener.java:39) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.ForwardingClientCallListener.onClose(ForwardingClientCallListener.java:23) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.ForwardingClientCallListener$SimpleForwardingClientCallListener.onClose(ForwardingClientCallListener.java:40) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.CensusTracingModule$TracingClientInterceptor$1$1.onClose(CensusTracingModule.java:391) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.ClientCallImpl.closeObserver(ClientCallImpl.java:471) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.ClientCallImpl.access$300(ClientCallImpl.java:63) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.close(ClientCallImpl.java:553) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl.access$600(ClientCallImpl.java:474) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.ClientCallImpl$ClientStreamListenerImpl$1StreamClosed.runInContext(ClientCallImpl.java:591) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ at io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:123) ~ grpc-core-1.12.0.jar:1.12.0 }} {{ ... 3 common frames omitted}}  ></body> </Action>
<Action id="45978" issue="30995" author="wlahti" type="comment" body="Interesting. Thanks for the update. It would also be best for everyone working on the project if you put the newest log in the description above in place of the eventhub logs, just to keep it clear where the focus should be. " created="2018-06-15 14:32:05.0" updateauthor="wlahti" updated="2018-06-15 14:32:05.0"/>
<Action id="45979" issue="30995" author="aatkddny" type="comment" body="done. pls advise if you need anything else from me to remediate." created="2018-06-15 14:48:20.0" updateauthor="aatkddny" updated="2018-06-15 14:48:20.0"/>
<Action id="46102" issue="30995" author="denyeart" type="comment" body=" ~wlahti   ~rickr  Could you summarize where we are with this one? Do we expect a fix in v1.2?" created="2018-06-18 12:20:19.0" updateauthor="denyeart" updated="2018-06-18 12:20:19.0"/>
<Action id="46117" issue="30995" author="rickr" type="comment" created="2018-06-18 13:48:59.0" updateauthor="rickr" updated="2018-06-18 13:48:59.0"> <body><! CDATA The SDK has the latest grpc levels available in the 1.2.0-SNAPSHOTs There is nothing I can see that would affect the SDK underload differently. This maybe several things yet:  GRPC/HTTP2 error on Java client side,  GRPC/HTTP2 GO error on the server side.  A bug under load on the Peer side.   An interesting scenario would be if this could be tested with two or more clients (preferably on different vms but that's a stretch) with a lesser load hitting the same Peer.  ></body> </Action>
<Action id="47111" issue="30995" author="rickr" type="comment" body="I&apos;ve created a testcase that is a multithreaded version of End2end. With 20 threads running  through the scenarios of getting endorsement sending endorsements to the Orderer and then doing various queries over a 4 hour period and validating the results with not seeing any issues.   While no such test can be fully exhaustive I think this covered a large part. Need a testcase I can run locally that will reproduce. " created="2018-07-09 15:50:47.0" updateauthor="rickr" updated="2018-07-09 15:50:47.0"/>
<Action id="47174" issue="30995" author="rickr" type="comment" created="2018-07-10 15:33:36.0" updateauthor="rickr" updated="2018-07-10 15:36:19.0"> <body><! CDATA I think there's a good chance this is related to two issues FAB-11094 and/or FAB-10966 that updates dependencies for issues for a netty inifinite loop.   Since I can't reproduce please provide a localy run testase that does so and with these fixes appllied -- if it reproduce that reopen.  ></body> </Action>
