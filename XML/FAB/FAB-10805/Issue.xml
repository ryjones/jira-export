<Issue id="31281" key="FAB-10805" number="10805" project="10002" reporter="scottz" assignee="latitiah" creator="scottz" type="10004" summary="MemoryError during fabric-test behave tests, cut short the daily svt test run" priority="2" resolution="10002" status="6" created="2018-06-21 21:55:26.0" updated="2019-03-19 11:13:35.0" resolutiondate="2018-11-12 16:08:28.0" votes="0" watches="2" workflowId="35884"> <description><! CDATA All OTE and LTE tests failed on latest stable build, June 21, build 58  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-svt-x86_64/test_results_analyzer/.|https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-svt-x86_64/test_results_analyzer/   Looking more closely, we see that there are NO results posted for behave tests in the orderer or peer feature files. Some run, but a MemoryError is seen in the full console logs which is the reason that Behave is not exiting cleanly and is not reporting the test results.  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-svt-x86_64/58/consoleFull .    The very first error is  `Scenario Outline: -FAB-3857-: 1024 key/value pairs in Payloads of size 1048576 – @1.2`  The first failures are the ones that use very large payloads, and then every test after that fails (including all LTE and OTE tests):  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-svt-x86_64/test_results_analyzer/  . Search the full Console logs file for string "MemoryError" in JOB 58 =  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-svt-x86_64/58/ . *Is it possible that behave does not deallocate things between tests?* After each OTE test, OTE dumps out the "df" and "free" commands on the host, before tearing down the network. It does not indicate memory exhaustion, which is confusing.  `11:37:36 ====== $ df  11:37:36 Filesystem 1K-blocks Used Available Use% Mounted on  11:37:36 udev 8206896 0 8206896 0% /dev  11:37:36 tmpfs 1643244 34488 1608756 3% /run  11:37:36 /dev/vda1 40593708 18113028 22464296 45% /  11:37:36 tmpfs 8216208 0 8216208 0% /dev/shm  11:37:36 tmpfs 5120 0 5120 0% /run/lock  11:37:36 tmpfs 8216208 0 8216208 0% /sys/fs/cgroup  11:37:36 none 2097152 964504 1132648 46% /w  11:37:36 tmpfs 1643244 0 1643244 0% /run/user/1001  11:37:36 ====== $ free  11:37:36 total used free shared buff/cache available  11:37:36 Mem: 16432420 987760 10567004 999016 4877656 13956880  11:37:36 Swap: 0 0 0`  \{\{{{ }}}}  I was going to suggest trying some CI configuration we can do to allocate more memory, or to investigate if some things are not cleaned up between tests... Or maybe we are trying to store too many logs. But the data we have now seems to indicate that there is no problem with disk space or RAM, between the OTE tests, so we might want to collect more data at the time of (and leading up to) the failures.  It was suggested that this tutorial or something like it might help us profile what is going on, if we capture some host data DURING each behave test - hopefully upon error, or before tearing down the network, but that may be tricky because the behave tests abort abnormally; at a minimum, we should dump the memory usage data between tests.     https://www.pluralsight.com/blog/tutorials/how-to-profile-memory-usage-in-python .  ~rameshthoomu   says he can ssh into the host running a job in a sandbox to view the memory logs data, once we add something that will show us the memory usage on the host before and during each test.  ></description> </Issue>
