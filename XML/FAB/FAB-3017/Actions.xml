<Action id="21940" issue="15597" author="manish-sethi" type="comment" created="2017-04-07 11:43:56.0" updateauthor="manish-sethi" updated="2017-04-07 11:43:56.0"> <body><! CDATA Live backup of goleveldb using filesystem level backup tools is not a recommended way. Because, there won't be any guarantees that incremental live copy of the db dir will result into a consistent backup. There is an outstanding enhancement request for supporting backup in goleveldb (https://github.com/syndtr/goleveldb/issues/135). So, currently the only option to get a reliable backup of goleveldb is to stop the db and then take backup  *So, it is better to just take the live backup of /chains folder using backup tool and rebuild /ledgerProvider, statedb and historydb.*  In the case of couchdb being as a statedb, one may replicate couchdb externally and we could think of enabling this alongwith the backed up /chains. However, we need to tackle the scenario where the couchdb may be ahead of a chain (if this is behind, the peer should be able to bring it upto the level of /chain). Also, the biggest issue would be to ensure that by mistake operator does not endup mixing the /chains with wrong couchdb. May be, we could store hash of the block alongwith the save point but none of these kind of mechanisms may guarantee about the data sanity.    ></body> </Action>
<Action id="22192" issue="15597" author="manish-sethi" type="comment" created="2017-04-12 13:31:54.0" updateauthor="manish-sethi" updated="2017-04-12 13:40:12.0"> <body><! CDATA In order to re-construct whole chains from the /chains folder, this would require to write a function that reconstructs /ledgerProvider, /statedb and /historydb from /chains folder. In the function, this should first open (and close) each of the ledger present in the /chains but missing in /lederProvider. Open of a ledger would cause, to build the corresponding statedb and historydb (if enabled). Finally, it should add entry into /ledgerProvider. A crash during this process should not affect the peer state because, after the restart of the process, it would simply be able to resume (as we would add entry into the /ledgerProvider as a last step)  I think that ideally, this function should be invoked from command line without start of the peer so as to have this as a separate utility. For the first cut, we can invoke this function from within `/core/ledger/kvledger/kv_ledger_provider.go: NewProvider()` function. However, this may increase the time of the peer start for the first time (when started with imported "/chains").  In a different scenario where in a new sets of chains are allowed to be copied (in addition to existing chains), this function would be able to detect the newer chains under /chains folder and perform the above for the newer chains.  Let me know if this looks a reasonable strategy.   ></body> </Action>
<Action id="22358" issue="15597" author="denyeart" type="comment" created="2017-04-13 15:52:26.0" updateauthor="denyeart" updated="2017-04-13 15:52:26.0"> <body><! CDATA Overall I like the approach  ~manish-sethi , but we cannot add significant new function in 1.0 at this point, such as /ledgerProvider recovery.  Let's assume *offline* backup/restore only for v1.0 (and perhaps for cases where file system backup via snapshot is in place, need to check if goleveldb is ok with that). * Can support with current v1.0 code * Backup /chains and /ledgerProvider directories together. Optionally backup /index directory as well. To make this simple, /ledgerProvider should be moved into top level /chains directory (peer of /index), so that a single directory can be backed up. * Can optionally backup state db and history db.  Peer startup recovery currently works if these are missing, partially synced, or fully synced with chains. Need to test/handle scenario where state/history db is ahead of chain.  FUTURE item post-v1.0: Ability to backup only /chains/chains directory or /chains/ledgerProvider directory. * If /chains/chains was backed up, reconstruct /chains/ledgerProvider. This option provides an online backup option, as described in prior comment. * If /chains/ledgerProvider was backed up, automatically re-join all the previously joined channels, get blocks from orderering service (as normal) or peers (state transfer). This option provides an option to backup channel memberships for easy recovery, with a very small footprint.  Otherwise peer would have to manually re-join potentially 1000s of channels.  FUTURE items can be deferred from 1.0, and design can be re-visited post-1.0.  The above is initial thoughts only.    ></body> </Action>
<Action id="22720" issue="15597" author="manish-sethi" type="comment" created="2017-04-19 17:10:48.0" updateauthor="manish-sethi" updated="2017-04-19 17:11:38.0"> <body><! CDATA *Ledger backup* It is not required to backup the ledger data of a peer. This is because, even in the worst case of catastrophic failure of a peer (e.g, a disk failure) , the peer can be brought up with no ledger at all. The peer can then re-join the desired channels and as a result, the peer will automatically create a ledger for each of the channels and eventually will receive the blocks via regular block transfer mechanism from other peers in the channel.  Still, some organizations may want to be more self-reliant and hence may want to take backup of their ledger data so as to be able to recover from catastrophic failures and resume from thereon. In addition, ledger data backups may help to expedite the addition of a new peer in the channel which can be achieved by backing up the ledger data from one peer and starting the new peer with the backed up ledger data.  In order to back up the ledger data of a peer, perform the following steps.  1. Stop the peer 2. Backup the folder `<peer.fileSystemPath>`/ledgersData. `<peer.fileSystemPath>` refers to the value of the property specified in the configurations (core.yaml) - default value of this property is `/var/hyperledger/production`  In the step 2 above, some of the sub-folders under `<peer.fileSystemPath>`/ledgersData can be skipped while backing up the ledger data. These sub-folders include `stateLeveldb`, `historyLeveldb`, `chains/index`. Skipping these folders reduces the storgae demand for the backup however, at the same time, the peer recovery from the backedup data may take more time. This is cause by the fact that the information in these folders (i.e., the state db, history db, and indexes on blocks) is re-constructed when the peer starts.  In order to restore the ledger data to a fresh peer, perform the following steps.  1. Copy the backed-up folder at `<peer.fileSystemPath>`/ledgersData 2. Start the peer   ></body> </Action>
<Action id="22922" issue="15597" author="denyeart" type="comment" created="2017-04-24 02:12:42.0" updateauthor="denyeart" updated="2019-03-28 17:57:39.0"> <body><! CDATA There is not yet a good place in Operations Guide for the backup/restore doc.  I have created a follow-on Jira item for documentation: FAB-3364.  FAB-3364 has updated guidance, while this jira (FAB-3017) focuses on test.  ></body> </Action>
