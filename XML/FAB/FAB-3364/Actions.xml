<Action id="41947" issue="15957" author="joe-alewine" type="comment" created="2018-03-22 20:40:26.0" updateauthor="joe-alewine" updated="2018-03-22 20:41:11.0"> <body><! CDATA  ~denyeart   Do we still need this? We cover how to back up ledger data in our Upgrade doc.  ></body> </Action>
<Action id="45761" issue="15957" author="denyeart" type="comment" body=" ~joe-alewine   ~pandrejko  Yes, we still need this doc topic.   Many people ask about how to backup and restore a peer, so that if they need to rebuild a peer they can resume from the point of the last backup rather than process all blocks from genesis block again." created="2018-06-10 09:20:39.0" updateauthor="denyeart" updated="2018-06-10 09:20:39.0"/>
<Action id="48736" issue="15957" author="gregnotso" type="comment" body="I can see quite a few FABs on this topic but no real implementation plan. Some vendors may not want to wait and may have already implemented their own proprietary code to allow for backup and restore. Needless to say they would not share it here." created="2018-08-10 19:35:48.0" updateauthor="gregnotso" updated="2018-08-10 19:35:48.0"/>
<Action id="59277" issue="15957" author="raft 3" type="comment" created="2019-04-16 10:00:36.0" updateauthor="raft 3" updated="2019-04-16 10:00:36.0"> <body><! CDATA What does _"Stop the peer"_ means exactly?  Is killing the peer safe? What if it was in the middle of writing a block to the disk?  In particular, I especially want to know if stopping the peer with the below command is safe? {code:java} kill -STOP <peer process id>{code} After taking the backup, it can be resumed with: {code:java} kill -CONT <peer process id>{code} We are running Fabric in Kubernetes, and as opposed to Docker, Kubernetes doesn't have any mechanism to stop/resume the containers. We are considering taking the backup in between STOP and CONT but of course only if we can be sure it's a safe operation.  ></body> </Action>
<Action id="62038" issue="15957" author="denyeart" type="comment" body=" ~raft 3  Yes, peers are crash tolerant and can be stopped/killed at anytime. Upon restart there is logic to check what was written and recover/resume from there." created="2019-07-23 13:24:02.0" updateauthor="denyeart" updated="2019-07-23 13:47:07.0"/>
<Action id="62039" issue="15957" author="hakan.eryargi" type="comment" created="2019-07-23 13:38:22.0" updateauthor="hakan.eryargi" updated="2019-07-23 13:38:22.0"> <body><! CDATA  ~denyeart  thanks, good to know!  Assuming that restart logic only applies when the peer is actually restarted, I guess this doesn't apply to kill -STOP/-CONT. In particular, to my knowledge, STOP happens at OS level, application (peer) cannot catch that signal, OS just stops the application without killing it. When the peer is resumed by CONT, this wont be a restart but a resume, so restart logic wont apply.  Also not sure what happens to IO in between STOP/CONT.  Can you please confirm?   http://man7.org/linux/man-pages/man7/signal.7.html   ></body> </Action>
<Action id="62040" issue="15957" author="denyeart" type="comment" body=" ~raft 3  I can&apos;t confirm, but I would assume the OS resumes IO where it left off in which case STOP/CONT would be safe." created="2019-07-23 13:46:50.0" updateauthor="denyeart" updated="2019-07-23 13:46:50.0"/>
<Action id="62101" issue="15957" author="hakan.eryargi" type="comment" created="2019-07-24 22:15:57.0" updateauthor="hakan.eryargi" updated="2019-07-24 22:15:57.0"> <body><! CDATA  ~denyeart  this is the worst case scenario I can think of: * peer starts writing a block * we stop the peer in the middle of that, assuming peer IO is also stopped here * we take backup (latest block is not completely backed up in this case, but in half) * we resume the peer, it continues writing the block and continue normally  Later when we restore from the backup and restart the peer, latest block will not be complete.   I'm guessing, peers are crash tolerant means, they can tolerate being killed in the middle of writing a block. So, when restoring from a backup where a block is half written, they can also tolerate this.  Can you please confirm this?      ></body> </Action>
<Action id="62112" issue="15957" author="manish-sethi" type="comment" created="2019-07-25 14:11:03.0" updateauthor="manish-sethi" updated="2019-07-25 14:11:03.0"> <body><! CDATA  ~hakan.eryargi , yes your understanding is correct.  To be precise, upon start of a peer, any such partial written block is detected and removed and peer behaves as if it never received that block.  ></body> </Action>
<Action id="66070" issue="15957" author="joe-alewine" type="comment" body="Changes here: https://github.com/hyperledger/fabric/pull/299" created="2019-12-04 21:04:41.0" updateauthor="joe-alewine" updated="2019-12-04 21:04:41.0"/>
<Action id="66251" issue="15957" author="joe-alewine" type="comment" body="Done as part of upgrade. See: https://hyperledger-fabric.readthedocs.io/en/master/upgrading_your_components.html#ledger-backup-and-restore" created="2019-12-12 19:47:27.0" updateauthor="joe-alewine" updated="2019-12-12 19:47:27.0"/>
<Action id="66570" issue="15957" author="silveraid" type="comment" body=" ~manish-sethi ,  ~hakan.eryargi , I was looking for existing backup solutions and run into this ticket. I just want everyone to know that peers are not crash resistant! It happened with us twice already that when the peer crashed the LevelDB database got corrupted and from that there was no recovery. We had to zero out all data from these peers to be able to recover them." created="2020-01-09 19:54:40.0" updateauthor="silveraid" updated="2020-01-09 19:54:40.0"/>
<Action id="66766" issue="15957" author="manish-sethi" type="comment" body=" ~silveraid  - You use a remote NFS for leveldb or you observed this in the local OS filesystem?" created="2020-01-17 16:30:41.0" updateauthor="manish-sethi" updated="2020-01-17 16:30:41.0"/>
<Action id="71499" issue="15957" author="silveraid" type="comment" created="2021-03-05 20:06:01.0" updateauthor="silveraid" updated="2021-03-05 20:06:01.0"> <body><! CDATA No, I don't think so, but I don't know much about the implementation, this happened to us in IBM Blockchain Platform v1 SaaS many times, always when the peer get stopped. Even though we are not using the v1 platform anymore it seems like that you more than likely solved this issue here:  https://jira.hyperledger.org/browse/FAB-18304  Thanks,  ></body> </Action>
<Action id="71522" issue="15957" author="denyeart" type="comment" body=" ~silveraid  Yes, the root cause of leveldb manifest corruption was finally found and a fix was pushed into goleveldb, which Fabric has now vendored in." created="2021-03-12 03:03:15.0" updateauthor="denyeart" updated="2021-03-12 03:03:15.0"/>
