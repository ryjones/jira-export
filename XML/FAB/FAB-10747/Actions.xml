<Action id="46373" issue="31216" author="troyronda" type="comment" created="2018-06-21 13:36:17.0" updateauthor="troyronda" updated="2018-06-21 13:38:46.0"> <body><! CDATA  ~divyank  To clarify "The purge could be wiping out private data from transactions that were in-flight (endorsed, but not committed) from the transient store leading to the pull timeout at commit time."  Is it the case that you are seeing private data wiped out that is newer than transientstoreMaxBlockRetention; and this is causing "current" transactions to fail because the private data got wiped prematurely from Temp DB?   (and "current" transactions mean those being committed near the block interval represented by transientstoreMaxBlockRetention).  ></body> </Action>
<Action id="46378" issue="31216" author="divyank" type="comment" body="Yes, I believe that is the case. The peers come to a halt immediately after the transient data purge indicating that the private data from the “current” transactions temporarily placed in the store was prematurely deleted. " created="2018-06-21 14:23:17.0" updateauthor="divyank" updated="2018-06-21 14:23:17.0"/>
<Action id="46396" issue="31216" author="denyeart" type="comment" body="We are looking into this.  From a side channel it sounds like the problem was confirmed to be related to transientstoreMaxBlockRetension, as setting it to a high value to disable the transient store purging makes the problem go away." created="2018-06-21 17:00:04.0" updateauthor="denyeart" updated="2018-06-21 17:28:23.0"/>
<Action id="46405" issue="31216" author="bstasyszyn" type="comment" created="2018-06-21 18:51:24.0" updateauthor="bstasyszyn" updated="2018-06-21 18:51:24.0"> <body><! CDATA I reproduced this problem with a simple chaincode that reads from a private data collection and then writes to it. I invoked the chaincode in 50 concurrent threads with random keys and values. The collection policy is Org1 AND Org2.  Notes: - I set transientstoreMaxBlockRetention=10 to more readily reproduce the issue - I added a log in PurgeByHeight that prints the key of the private data that's being purged  Attached are the complete logs, but here's a summary of one Transaction:  org1peer2_1      | 2018-06-21 18:38:08.722 UTC  transientstore  PurgeByHeight -> WARN a7a ********************** Deleting private data for key  Pd12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc1765251118545fa2-2518-46a9-b1d5-26fa7223d2ff . txID  d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 , uuid  18545fa2-2518-46a9-b1d5-26fa7223d2ff , blockHeight  0  org1peer2_1      | 2018-06-21 18:38:08.722 UTC  transientstore  PurgeByHeight -> WARN a7b ********************** Deleting private data for key  Pd12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511a7d7414c-cedc-4b59-bd47-131f46adbd54 . txID  d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 , uuid  a7d7414c-cedc-4b59-bd47-131f46adbd54 , blockHeight  0  org1peer1_1      | 2018-06-21 18:38:08.919 UTC  transientstore  PurgeByHeight -> WARN 8a1 ********************** Deleting private data for key  Pd12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511e38ccfb3-88f7-426a-8f1d-d60db95cd5b2 . txID  d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 , uuid  e38ccfb3-88f7-426a-8f1d-d60db95cd5b2 , blockHeight  0  org1peer1_1      | 2018-06-21 18:38:08.919 UTC  transientstore  PurgeByHeight -> WARN 8a2 ********************** Deleting private data for key  Pd12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511ea4d5e23-1426-4dbc-a8ed-31212f87d427 . txID  d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 , uuid  ea4d5e23-1426-4dbc-a8ed-31212f87d427 , blockHeight  0  org2peer1_1      | 2018-06-21 18:38:09.465 UTC  transientstore  PurgeByHeight -> WARN 14c4 ********************** Deleting private data for key  Pd12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc176525110b39c4da-9642-40ce-899a-18203b841f4d . txID  d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 , uuid  0b39c4da-9642-40ce-899a-18203b841f4d , blockHeight  0  org2peer1_1      | 2018-06-21 18:38:09.465 UTC  transientstore  PurgeByHeight -> WARN 14c5 ********************** Deleting private data for key  Pd12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc176525118bed6d59-e830-4964-afc4-b28e363dfb77 . txID  d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 , uuid  8bed6d59-e830-4964-afc4-b28e363dfb77 , blockHeight  0  org1peer2_1      | 2018-06-21 18:38:15.565 UTC  gossip/privdata  fromTransientStore -> ERRO c11 No collection config was found for chaincode examplecc collection name examplecc txID d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 org1peer2_1      | 2018-06-21 18:38:20.429 UTC  gossip/privdata  fromTransientStore -> ERRO c17 No collection config was found for chaincode examplecc collection name examplecc txID d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 org1peer2_1      | 2018-06-21 18:38:21.567 UTC  gossip/privdata  fromTransientStore -> ERRO c21 No collection config was found for chaincode examplecc collection name examplecc txID d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511 org2peer1_1      | 2018-06-21 18:39:09.591 UTC  gossip/privdata  StoreBlock -> WARN 14dc Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511, seq: 2, namespace: examplecc, collection: coll1, hash: 75adadc0bb533591a4539a5474078b26fa5a52cc3e222128497737dc65dc1036 org1peer1_1      | 2018-06-21 18:39:10.449 UTC  gossip/privdata  StoreBlock -> WARN 9c9 Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: d12c0292796e899cf807f20b5cdcdce404e8e3529f13cb83014db7cc17652511, seq: 2, namespace: examplecc, collection: coll1, hash: 75adadc0bb533591a4539a5474078b26fa5a52cc3e222128497737dc65dc1036   ></body> </Action>
<Action id="46432" issue="31216" author="manish-sethi" type="comment" created="2018-06-22 11:15:20.0" updateauthor="manish-sethi" updated="2018-06-22 11:15:20.0"> <body><! CDATA I briefly looked at the logs attached but could not find anything unusual that would hint for a bug. This may just that the concurrency is much higher than the configuration '{color:#a31515}peer.gossip.pvtData.transientstoreMaxBlockRetention'.{color}  However, in order to further confirm,  ~bstasyszyn  can you run your test again with running peer with DEBUG logs on. That would print some more info which can be used to compute the blocks that gets committed between "adding pvt data to the transient store" and "PurgeByHeight" for a particular txid.  For PurgeByHeight, you already have added logs and during '{color:#000000}adding to store'{color}, you may see a message "{color:#a31515}Persisting private data to transient store for txid = %s{color}" in debug logs. This can been used to see how many block gets committed between these two log statements.  ></body> </Action>
<Action id="46458" issue="31216" author="divyank" type="comment" created="2018-06-22 16:32:04.0" updateauthor="divyank" updated="2018-06-22 16:32:04.0"> <body><! CDATA Hi  ~manish-sethi , thank you for looking into this. In the logs provided below we were making 50 concurrent private data transactions with `transientstoreMaxBlockRetention` set to the default value of 1000. This very much represents real world usage.  Here are the logs you requested (attaching only the relevant portions). This confirms our suspicions that the private data from an ongoing transaction was prematurely deleted: {code:java} 2018-06-22 15:44:10.953 UTC  kvledger  CommitWithPvtData -> INFO 76e95d Channel  mychannel : Committed block  2000  with 20 transaction(s) ... 2018-06-22 15:44:11.033 UTC  transientstore  PersistWithConfig -> DEBU 76e9ca Persisting private data to transient store for txid = 4e89e224b8795438f7a1eadf0fffe6ee6fa8c4ae7a6da2c4e646c0d9b0af33d7 ... 2018-06-22 15:44:11.074 UTC  kvledger  CommitWithPvtData -> DEBU 76ea0f Channel  mychannel : Committing block  2000  transactions to history database ... 2018-06-22 15:44:11.098 UTC  transientstore  PurgeByHeight -> DEBU 76eae4 Purging orphaned private data from transient store received prior to block  1000  ...  Notice the one minute delay during which all peers are stuck  2018-06-22 15:45:24.035 UTC  gossip/privdata  StoreBlock -> WARN 773e1c Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: txID: 4e89e224b8795438f7a1eadf0fffe6ee6fa8c4ae7a6da2c4e646c0d9b0af33d7, seq: 3, namespace: benchmark_cc, collection: billing, hash: 3967b9eeb0607a0e58527886751dac8614b484723c0878ff54dd7a5d4c6cbe89 2018-06-22 15:45:24.036 UTC  kvledger  CommitWithPvtData -> DEBU 773e1d Channel  mychannel : Validating state for block  2001  {code}  ></body> </Action>
<Action id="46504" issue="31216" author="denyeart" type="comment" created="2018-06-25 19:55:52.0" updateauthor="denyeart" updated="2018-06-25 19:55:52.0"> <body><! CDATA  ~divyank  I am not able to reproduce the problem during normal operations.  If I set CORE_PEER_GOSSIP_PVTDATA_TRANSIENTSTOREMAXBLOCKRETENTION=5 and send a constant load of transactions they all succeed as normal, that is, the commits are always getting private data that was executed in chaincode within the last 5 blocks, so purging data older than 5 blocks ago from transient store has no negative effect.  Now, if I dial CORE_PEER_GOSSIP_PVTDATA_TRANSIENTSTOREMAXBLOCKRETENTION all the way down to 1, then I can reproduce the problem.  That is, commits will attempt to get private data that was executed within the last couple blocks, but this data will have been purged already from transient store, and therefore the commit will retry to pull private for pullRetryThreshold (default 60 seconds) before giving up and committing the block with missing private data.  If all is good at CORE_PEER_GOSSIP_PVTDATA_TRANSIENTSTOREMAXBLOCKRETENTION=5, certainly all should be good at the default CORE_PEER_GOSSIP_PVTDATA_TRANSIENTSTOREMAXBLOCKRETENTION=1000 as that is a much more conservative purging schedule.  We will need to understand more about your scenario to understand why the private data cannot be pulled in your scenario. Are you able to provide some more detailed reproduction steps?     ></body> </Action>
<Action id="46576" issue="31216" author="divyank" type="comment" created="2018-06-27 01:42:10.0" updateauthor="divyank" updated="2018-06-27 01:42:10.0"> <body><! CDATA  ~denyeart  this is hard to reproduce in a local, non-networked environment. Here is a minimal test case that reliably reproduces the issue:  https://gerrit.hyperledger.org/r/c/23693/6/test/integration/orgs/multiple_orgs_test.go   The trick I used was to add a small  sleep|https://gerrit.hyperledger.org/r/c/23693/6/pkg/client/channel/invoke/txnhandler.go#84  between endorse and commit on the client side to increase the number of potential pending transactions during the transient store purge.  Here are the INFO logs from the test run (suggest not opening this large file in a browser):  https://jenkins.hyperledger.org/job/fabric-sdk-go-tests-verify-s390x/3027/consoleText   The "TransientStoreMaxBlockRetention" config was set to 500 and we saw the first private data timeout after the first purge at block 1000: {code:java} org1peer1_1          | 2018-06-26 22:55:55.030 UTC  gossip/privdata  StoreBlock -> WARN 501ef Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: aa33f9ed3ba9fcd013c7cb5d27439f5e3dcec2abb06243e01c9b7b1d715b4f4c, seq: 5, namespace: exampleCC, collection: billing, hash: 59a874bdb661d41ed7a10a9812dff3ca486e286bfc4ce70e7e20cd5eb56cf6d3 ... org1peer1_1          | 2018-06-26 22:55:55.044 UTC  kvledger  CommitWithPvtData -> INFO 501f0 Channel  orgchannel : Committed block  1001  with 42 transaction(s) {code} And again at block 1500, 2000, 2500, and so on... {code:java} org1peer1_1          | 2018-06-26 23:01:41.082 UTC  gossip/privdata  StoreBlock -> WARN 7708f Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: 963f15356bc1b63fadce3e1651fde33eaaf91c6cece4212b90adfe24d54d74eb, seq: 12, namespace: exampleCC, collection: billing, hash: 74e027d91e5c3f2764d4de6557b3d245de1ad5303cedcd02b98b63248d4e756d ... org1peer1_1          | 2018-06-26 23:01:41.090 UTC  kvledger  CommitWithPvtData -> INFO 77090 Channel  orgchannel : Committed block  1501  with 32 transaction(s) ... org1peer1_1          | 2018-06-26 23:08:28.632 UTC  gossip/privdata  StoreBlock -> WARN 9e225 Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: 30ab66c0136527201a38c71c97d2093c3ffb1daaf5445664a3fc325f4ede695a, seq: 10, namespace: exampleCC, collection: billing, hash: fa76af7c750987b4187d4e8f604d98955a39085246d6272e02dc5b4b6fcb7420 ... org1peer1_1          | 2018-06-26 23:08:28.649 UTC  kvledger  CommitWithPvtData -> INFO 9e226 Channel  orgchannel : Committed block  2001  with 100 transaction(s){code}     Here are the peer debug logs from another run of the test:  https://jenkins.hyperledger.org/job/fabric-sdk-go-tests-verify-s390x/3029/consoleText|https://jenkins.hyperledger.org/job/fabric-sdk-go-tests-verify-s390x/3027/consoleText  (purge setting was set to 250 for this run as CI would time out before reaching 1000 blocks)  In this run the first transient data purge happens at block 500 and we can see that the private data for transaction ID '3a024c03c89773b5d3534c3262132a78b9d0d40830076184b53dd0db5aa60007' which was added after the local block height reached 498 was prematurely deleted even though the intent was to " purge  orphaned private data from transient store received prior to block  250 " which was committed minutes before: {code:java} org1peer1_1          | 2018-06-27 00:24:21.960 UTC  kvledger  CommitWithPvtData -> INFO 15d55f Channel  orgchannel : Committed block  250  with 38 transaction(s){code} {code:java}  org1peer1_1          | 2018-06-27 00:26:55.463 UTC  kvledger  CommitWithPvtData -> INFO 237e84 Channel  orgchannel : Committed block  498  with 57 transaction(s){code} {code:java} org1peer1_1          | 2018-06-27 00:26:55.705 UTC  transientstore  PersistWithConfig -> DEBU 238041 Persisting private data to transient store for txid = 3a024c03c89773b5d3534c3262132a78b9d0d40830076184b53dd0db5aa60007{code} {code:java}  org1peer1_1          | 2018-06-27 00:26:56.745 UTC  kvledger  CommitWithPvtData -> INFO 23915e Channel  orgchannel : Committed block  500  with 100 transaction(s){code} {code:java} org1peer1_1          | 2018-06-27 00:26:56.848 UTC  transientstore  PurgeByHeight -> DEBU 239375 Purging orphaned private data from transient store received prior to block  250 {code} {code:java} org1peer1_1          | 2018-06-27 00:28:03.674 UTC  gossip/privdata  StoreBlock -> WARN 23b52e Could not fetch all missing collection private write sets from remote peers. Will commit block with missing private write sets: txID: 3a024c03c89773b5d3534c3262132a78b9d0d40830076184b53dd0db5aa60007, seq: 15, namespace: exampleCC, collection: billing, hash: a259e9d014e971496652d4da2c6322157efc06913a050a459b8d5e2d184ec632 ... org1peer1_1          | 2018-06-27 00:28:03.675 UTC  kvledger  CommitWithPvtData -> DEBU 23b52f Channel  orgchannel : Validating state for block  501  {code}    ></body> </Action>
<Action id="46636" issue="31216" author="denyeart" type="comment" body=" ~divyank  Thanks for the details.  I found the issue - you were right, all data in transient store was getting purged each cycle. I am pushing a fix." created="2018-06-27 21:20:54.0" updateauthor="denyeart" updated="2018-06-27 21:20:54.0"/>
<Action id="46637" issue="31216" author="denyeart" type="comment" created="2018-06-27 21:21:41.0" updateauthor="denyeart" updated="2018-06-28 19:48:50.0"> <body><! CDATA  https://gerrit.hyperledger.org/r/#/c/23767/  master   https://gerrit.hyperledger.org/r/#/c/23769/  release-1.2  ></body> </Action>
