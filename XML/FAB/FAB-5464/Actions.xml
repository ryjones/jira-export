<Action id="31071" issue="19482" author="shimos" type="comment" created="2017-09-21 17:25:02.0" updateauthor="shimos" updated="2017-09-21 17:25:02.0"> <body><! CDATA Some error messages are seen also in endorsement proposals and commit requests.  {noformat}  2017-09-14 22:23:39.249   INFO  invoke-chaincode - transaction proposal was good  2017-09-14 22:23:39.249   DEBUG  invoke-chaincode - Successfully sent Proposal and received ProposalResponse: Status - 200, message - "OK", metadata - "", endorsement signature: 0E^B!^@���oϵ� �^Y��JH��&�HN�6��3�S�ݗ��^B &^P=X����x4���o��$��7�^Y��N^^^X3 info:  EventHub.js : _connect - options {"grpc.ssl_target_name_override":"peer0.org1.example.com","grpc.default_authority":"peer0.org1.example.com"}  2017-09-14 22:23:39.252   DEBUG  Helper -  crypto_ecdsa_aes : ecdsa signature:  Signature { r: <BN: 2a31e714058b2bb3bb1d8c01a1b97812dcbcc5d9087b2adbe4e15c177986ec93>, s: <BN: 56fa1d3c6e03b6282079a03bcbecdbd3d0e9931b7d705d0d650c442bdde0f825>, recoveryParam: 1 }  2017-09-14 22:23:39.254   DEBUG  Helper -  crypto_ecdsa_aes : ecdsa signature:  Signature { r: <BN: 634ebad74604a1463ff6762841e9875c6b1a64a9d5b26a8d797a6046c2b2d70b>, s: <BN: 190b46d99b659e8c70fce48ce55520ea31104a3f3c5f9d0a594af71fbdaccc9b>, recoveryParam: 0 } E0914 22:23:39.255486949       9 ssl_transport_security.c:575  Could not add root certificate to ssl context. E0914 22:23:39.255547752       9 ssl_transport_security.c:1297  Cannot load server root certificates. E0914 22:23:39.255564368       9 security_connector.c:857    Handshaker factory creation failed with TSI_INTERNAL_ERROR. E0914 22:23:39.255569266       9 secure_channel_create.c:112  Failed to create secure subchannel for secure name 'peer0.org1.example.com:7053' E0914 22:23:39.255574025       9 secure_channel_create.c:143  Failed to create subchannel arguments during subchannel creation. E0914 22:23:39.258106349       9 ssl_transport_security.c:427  Corruption detected. E0914 22:23:39.258136302       9 ssl_transport_security.c:403  error:0B07C065:x509 certificate routines:X509_STORE_add_cert:cert already in hash table E0914 22:23:39.258142067       9 secure_endpoint.c:177       Decryption error: TSI_DATA_CORRUPTED error:  Orderer.js : sendBroadcast - on error: "Error: Endpoint read failed\n    at ClientDuplexStream._emitStatusIfDone (/opt/app/dev/node_modules/grpc/src/node/src/client.js:255:19)\n    at ClientDuplexStream._receiveStatus (/opt/app/dev/node_modules/grpc/src/node/src/client.js:233:8)\n    at /opt/app/dev/node_modules/grpc/src/node/src/client.js:757:12"  2017-09-14 22:23:39.258   ERROR  invoke-chaincode - Failed to send transaction and get notifications within the timeout period.  2017-09-14 22:23:39.258   ERROR  invoke-chaincode - Failed to order the transaction. Error code: undefined {noformat}  It seems it comes from grpc, so I reported to grpc community: https://github.com/grpc/grpc/issues/12642  The issue above might be related to this one.  ></body> </Action>
<Action id="32620" issue="19482" author="bmos299" type="comment" body="This bug does sound like a match with https://github.com/grpc/grpc/issues/12642.  In our having to use an older version of the node sdk to avoid this issue.  I am changing the status to high.   It probably borders on highest as concurrency breaks." created="2017-10-18 14:00:07.0" updateauthor="bmos299" updated="2017-10-18 14:00:07.0"/>
<Action id="32622" issue="19482" author="bmos299" type="comment" created="2017-10-18 14:10:58.0" updateauthor="bmos299" updated="2017-10-18 14:16:03.0"> <body><! CDATA In talking with  ~denyeart  moving to highest. Murali suspects this may not be a grpc bug, but a bug in our locking. He suggests the following lock may need to used from handler.go:       {code:java} //serialSend serializes msgs so gRPC will be happy func (handler *Handler) serialSend(msg *pb.ChaincodeMessage) error { handler.serialLock.Lock() defer handler.serialLock.Unlock() var err error if err = handler.ChatStream.Send(msg); err != nil { err = errors.WithMessage(err, fmt.Sprintf(" %s Error sending %s", shorttxid(msg.Txid), msg.Type.String())) chaincodeLogger.Errorf("%+v", err) } return err } {code}       ></body> </Action>
<Action id="32632" issue="19482" author="bmos299" type="comment" body="This seems to have occurred more consistently between the node sdk and the fabric ca. " created="2017-10-18 15:35:30.0" updateauthor="bmos299" updated="2017-10-18 15:35:30.0"/>
<Action id="32661" issue="19482" author="shimos" type="comment" created="2017-10-18 23:22:35.0" updateauthor="shimos" updated="2017-10-18 23:22:35.0"> <body><! CDATA  ~bmos299  Recently when I tried to run PTE with TLS disabled, this bug seemed to occur. So probably you're right, it might not be because of the gRPC bug (it's a TLS-related issue)...  But note that there seems to be a different bug related to the gRPC bug as reported in FAB-6266.  ></body> </Action>
<Action id="32919" issue="19482" author="lin2k" type="comment" body="same problem with java sdk. I think this is a server side problem." created="2017-10-20 06:55:12.0" updateauthor="lin2k" updated="2017-10-20 06:56:24.0"/>
<Action id="32951" issue="19482" author="denyeart" type="comment" created="2017-10-20 19:33:11.0" updateauthor="denyeart" updated="2017-10-20 19:33:11.0"> <body><! CDATA Based on latest comments, shifting to fabric-peer.    ~muralisr  can you elaborate on your theory here?  ></body> </Action>
<Action id="32963" issue="19482" author="muralisr" type="comment" created="2017-10-21 17:58:40.0" updateauthor="muralisr" updated="2017-10-21 18:00:03.0"> <body><! CDATA  ~denyeart  I was going by  {code:java} E0914 22:23:39.258106349       9 ssl_transport_security.c:427  Corruption detected. {code}  One possibility is concurrent writes on the gRPC stream without locks to serialize those writes. This can happen if there are multiple fabric channels generating blocks concurrently (that's probably NOT the case here if we are dealing with a single channel).   The other possibility is client (nodejs) is sending multiple events on the same event stream concurrently. Is this what you mean by the following  ~lin2K  ? {code:java} EventHub is shared by all the invocations, thus the number of event connection is just one. {code}  A quick check would be to wrap this snippet of code with a lock in events/producer/handler.go https://github.com/hyperledger/fabric/blob/master/events/producer/handler.go#L127 like shown in the "serialSend" example by  ~bmos299  comment.  Tagging ...   ~wlahti   If you are sending concurrent events from the client side,  ~lin2K  you would have to do a similar fix on the nodejs side as well.  ></body> </Action>
<Action id="33004" issue="19482" author="shimos" type="comment" created="2017-10-23 18:31:48.0" updateauthor="shimos" updated="2017-10-23 18:31:48.0"> <body><! CDATA  ~muralisr  I'm answering because I think below is for me..  {noformat} The other possibility is client (nodejs) is sending multiple events on the same event stream concurrently. Is this what you mean by the following Tony kong ?  EventHub is shared by all the invocations, thus the number of event connection is just one. {noformat}  Right. I invoked transactions concurrently form a single nodejs client, and register their transactions to the same EventHub. So all the block events should have come through the single event stream. (And they did for a while until this disconnection happened)  ></body> </Action>
<Action id="33248" issue="19482" author="jeffgarratt" type="comment" body="Have you taken a look at this  ~wlahti ?" created="2017-10-26 19:06:43.0" updateauthor="jeffgarratt" updated="2017-10-26 19:06:43.0"/>
<Action id="33379" issue="19482" author="wlahti" type="comment" body=" ~shimos  Can you provide the modified chaincode you used?" created="2017-10-30 17:32:49.0" updateauthor="wlahti" updated="2017-10-30 17:32:49.0"/>
<Action id="33382" issue="19482" author="shimos" type="comment" body=" ~wlahti  Sure, I have attached the chaincode.  ^example_cc.go  " created="2017-10-30 18:54:49.0" updateauthor="shimos" updated="2017-10-30 18:54:49.0"/>
<Action id="33387" issue="19482" author="wlahti" type="comment" created="2017-10-30 19:44:31.0" updateauthor="wlahti" updated="2017-10-30 19:49:05.0"> <body><! CDATA Thanks.  To clarify, you're using invoke-parallel-simplified.js in place of invoke.js, the updated example_cc.go, and then running `{{node test/integration/e2e.js}}` (or the individual tests one by one) to reproduce this? Any other necessary steps?  ></body> </Action>
<Action id="33391" issue="19482" author="shimos" type="comment" body=" ~wlahti  Yes, I rather ran tests individually ( create-channel.js, join-channel.js, ..) but the steps you mention should be sufficient." created="2017-10-30 20:34:46.0" updateauthor="shimos" updated="2017-10-30 20:34:46.0"/>
<Action id="33393" issue="19482" author="wlahti" type="comment" created="2017-10-30 21:57:13.0" updateauthor="wlahti" updated="2017-10-31 15:29:06.0"> <body><! CDATA Are you able to hit this error every time? I finally have the Node SDK tests working locally but I haven't seen this issue using the latest Fabric, Node SDK, and Fabric CA code from master. I'll try with the v1.0 code next.   ~shimos  Any chance you can log in to Rocketchat to discuss at some point? I think that'll help move things along.   ></body> </Action>
<Action id="33626" issue="19482" author="shimos" type="comment" created="2017-10-31 16:17:45.0" updateauthor="shimos" updated="2017-10-31 16:17:45.0"> <body><! CDATA  ~wlahti  Thanks for testing. When I reported this in July, it occurred almost every time. But now I run with the current master and release branch, it seems not to be happening any more as you say.  I also tested with v1.0.0, and it produced eventhub reconnection and timeout errors, so something after 1.0.0 has fixed at least my issue.  As for Rocket.chat, sure, which channel would be suitable?  ></body> </Action>
<Action id="33810" issue="19482" author="wlahti" type="comment" body=" ~shimos  Since this no longer appears to be an issue with the latest code, do you think we can go ahead and close this JIRA?" created="2017-11-01 13:48:20.0" updateauthor="wlahti" updated="2017-11-01 13:48:20.0"/>
<Action id="34000" issue="19482" author="shimos" type="comment" body=" ~wlahti  Okay for my case, but I&apos;m wondering if this might still happen because some people posted comments here reporting this in the different situation.." created="2017-11-01 15:35:06.0" updateauthor="shimos" updated="2017-11-01 15:35:06.0"/>
<Action id="34002" issue="19482" author="wlahti" type="comment" body="Ah yes, good point. They hadn&apos;t chimed in here recently so I overlooked those comments.  ~lin2K   ~bmos299 , do you notice this issue using the code from master or only in v1.0 networks?" created="2017-11-01 15:39:01.0" updateauthor="wlahti" updated="2017-11-01 15:39:01.0"/>
<Action id="34206" issue="19482" author="bmos299" type="comment" body="I have hit this problem on Fabric 1.0.3. " created="2017-11-01 19:40:08.0" updateauthor="bmos299" updated="2017-11-01 19:40:08.0"/>
<Action id="34335" issue="19482" author="jeffgarratt" type="comment" body=" ~bmos299  was this sporadic or easily repeatable under a specific load?" created="2017-11-04 17:12:00.0" updateauthor="jeffgarratt" updated="2017-11-04 17:12:00.0"/>
<Action id="35416" issue="19482" author="denyeart" type="comment" body=" ~bmos299  It the problem easily reproducible on 1.0.x?  How about on 1.0 master?" created="2017-11-27 01:38:09.0" updateauthor="denyeart" updated="2017-11-27 01:38:09.0"/>
<Action id="35952" issue="19482" author="xixuejia" type="comment" created="2017-12-06 21:33:47.0" updateauthor="xixuejia" updated="2017-12-06 21:33:47.0"> <body><! CDATA This issue should also be fixed by https://gerrit.hyperledger.org/r/#/c/15719/    Hi  ~bmos299 , would you please also try that fix? Several users have confirmed that the above patch fixed this issue  ></body> </Action>
<Action id="38295" issue="19482" author="wlahti" type="comment" body=" ~bmos299  Any update on trying the workaround mentioned above by Xi Xue Jia?" created="2018-01-08 16:12:41.0" updateauthor="wlahti" updated="2018-01-08 16:12:41.0"/>
<Action id="39169" issue="19482" author="wlahti" type="comment" body=" ~bmos299   ~dongming  Any chance to try out this workaround yet?" created="2018-01-26 14:03:11.0" updateauthor="wlahti" updated="2018-01-26 14:03:11.0"/>
<Action id="39175" issue="19482" author="dongming" type="comment" body=" ~wlahti  In my PTE test, I do not see the issue with the latest master fabric and sdk." created="2018-01-26 15:19:35.0" updateauthor="dongming" updated="2018-01-26 15:19:35.0"/>
<Action id="39382" issue="19482" author="wlahti" type="comment" body="Since  ~shimos  (the reporter of this item) and  ~dongming  have both reported positive test results, marking this item as complete. If there&apos;s anyone else I&apos;m unaware of who&apos;s still hitting this issue, please comment and re-open the item." created="2018-01-30 19:49:04.0" updateauthor="wlahti" updated="2018-01-30 19:49:04.0"/>
<Action id="43814" issue="19482" author="grapebaba" type="comment" body="we are hitting this issue on fabric 1.0.2, we already update the node sdk patch, this error still ocurr" created="2018-05-04 11:34:45.0" updateauthor="grapebaba" updated="2018-05-04 11:34:45.0"/>
<Action id="46092" issue="19482" author="robertdiebels" type="comment" created="2018-06-18 12:03:24.0" updateauthor="robertdiebels" updated="2018-06-18 12:05:15.0"> <body><! CDATA I was hitting this issue as well.  Versions: fabric-peer 1.0.x (tested on 1.0.0, 1.0.4 and 1.0.6) fabric-client (NodeJS): ^1.1.0  Downgrading fabric-client to 1.0.8 as well as ensuring I'm using grpc version 1.10.1 for NodeJS fixed the issue for me. The fabric-peer version I'm using is 1.0.6.  ></body> </Action>
