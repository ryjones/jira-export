<Issue id="32225" key="FAB-11322" number="11322" project="10002" reporter="yoheiueda" creator="yoheiueda" type="10004" summary="Reducing peer-chaincode communication with PutState calls in &quot;write back&quot;" priority="3" resolution="10002" status="6" created="2018-07-26 08:53:03.0" updated="2018-08-29 07:08:23.0" resolutiondate="2018-08-21 12:34:22.0" votes="0" watches="5" workflowId="43473"> <description><! CDATA I analyzed bottlenecks in Fabric peer and chaincode execution, and one of bottlenecks is frequent communication between peer and chanincode containers. The GRPC stream between peer and chaincode is guarded by a mutex lock, and it causes heavy lock contention when TPS is very high.  In the current implementation, every time PutState is called from chaincode, the chaincode container communicates with the peer. In other words, PutState are processed in 'write-through' mode.  However, data set by PutState calls are not actually stored into StateDB until transaction commitment. So communication for PutState calls can be delayed until the transaction simulation finishes.  We can piggyback multiple PutState calls in a ChaincodeMessage_COMPLETED message. In other words, PutState calls can be processed in "write-back" mode.  Only noticeable differences by this change is the how errors of PutState calls are notified. Most of PutState calls do not cause errors, but in some cases, PutState causes error. So timing of such error notification will be different between write-through and write-back modes.  If this difference is acceptable, we can silently implement write-back mode without changing the shim APIs.  If this difference is not acceptable, we need to add a new API such as stub.SetUpdateMode(mode) to choose write-through and write-back modes.   Later, I will post my work-in-progress code to show how it works.  ></description> </Issue>
