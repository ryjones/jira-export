<Action id="61067" issue="40629" author="senthil1" type="comment" created="2019-06-18 04:09:21.0" updateauthor="senthil1" updated="2019-06-18 04:09:21.0"> <body><! CDATA  ~manish-sethi   ~denyeart   ~wlahti   ~ChristopherFerris   We have also faced a similar problem. When the number event hub connections was higher, the throughput dropped more than 70%.  ~tparth  can add more details if needed.     ></body> </Action>
<Action id="61070" issue="40629" author="yoheiueda" type="comment" created="2019-06-18 09:09:30.0" updateauthor="yoheiueda" updated="2019-06-18 09:09:30.0"> <body><! CDATA As  ~Senthil1  pointed out, the caching approach leads to memory bloat when there exist a lot of channels.   https://gerrit.hyperledger.org/r/c/fabric/+/31944#message-ff6b4fe3_10f0d90f      Instead, we need to broadcast a deserialized block to waiters when it becomes ready.  Waiters wait at this cond variable, so I think we can implement such broadcast.   https://github.com/hyperledger/fabric/blob/7135b516461415c471a19851c371e94300a6269d/common/ledger/blkstorage/fsblkstorage/blocks_itr.go#L47      I think the performance improvement will be similar for caching and broadcast approaches, when a single channel is used.  ></body> </Action>
