<Action id="19376" issue="12923" author="kchristidis" type="comment" created="2016-10-22 04:36:07.0" updateauthor="kchristidis" updated="2016-10-22 04:36:07.0"> <body><! CDATA This (or at least the biggest part of it) should be handled during this sprint.  The most likely scenario ahead, for better or for worse, is Option 3 as presented here: https://hyperledgerproject.slack.com/archives/fabric-consensus-dev/p1476806651002202  (Option 4 will give us a headache with a lot of noise in the leader election process: https://hyperledgerproject.slack.com/archives/fabric-consensus-dev/p1476997517002456)  The plan I have in mind as I'm writing this is the following:  - Two partitions per channel. - Each orderer (shim) writes (produces) and listens (consumes) to/from both. - Partition 1 is where all the orderers (shims) forward the transactions as soon as they receive them from the clients. - Once K transactions have been posted in partition 1, or time T has elapsed since the first transaction after the last block (whichever comes first), the orderer that's marked as the leader for the channel in the etcd/ZK ensemble sends a "time-to-cut" message in partition 1, and proceeds with creating the block and posting in partition 2. - If he fails to do that within X amount of time, a new leader (shim) is elected that's charged with the same task. - This gives us a "raw ledger" of sorts in Partition 2, and this is what the shims Deliver from.  I may refine it as I find flaws during implementation, (and I anticipate I'll bump into several issues when implementing this anyway), but feel free to join in on the fun now, and point out the difficulties in the road that lies ahead.  (FWIW, I think that  ~cca88  has a point when he suggests that in the Kafka case we shouldn't be doing blocks to begin with: https://hyperledgerproject.slack.com/archives/fabric-consensus-dev/p1477035141002478)  ></body> </Action>
<Action id="19418" issue="12923" author="kchristidis" type="comment" body="Need to add Gari&apos;s suggestion here for consumer groups, and need to break it into tasks as part of this story will extend to the next sprint as well." created="2016-10-24 16:33:55.0" updateauthor="kchristidis" updated="2016-10-24 16:33:55.0"/>
<Action id="19529" issue="12923" author="bcbrock" type="comment" created="2016-10-26 22:58:02.0" updateauthor="bcbrock" updated="2016-10-26 23:20:57.0"> <body><! CDATA   ^Kafka Connect Orderer 20161026.pdf  (sorry, page 1 of this PDF is blank) This picture shows how I imagined Kafka Connect might be involved. Like your proposal, it requires storage for both an unchained and a chained ledger for each channel. It might be simpler though in that it doesn't require the shims to elect a leader. There is no "block"-chain here. Each TX is a block. It's possible that Kafka Connect may be the wrong choice here if it requires a new Kafka Connect process for each channel. Given that the shim knows about the channels, it may be simpler to simply have intermediate shim threads that create the local chains, and store them in Kafka for simplicity. The end game of this type of approach would be to propose a modification to Kafka to directly support hash-chained message streams.  ></body> </Action>
<Action id="20000" issue="12923" author="kchristidis" type="comment" created="2016-12-06 21:30:42.0" updateauthor="kchristidis" updated="2016-12-06 21:30:42.0"> <body><! CDATA Completed here: https://github.com/kchristidis/fabric/commit/f9006f4c997dbbc8ae5a8f6e1b45fbf1cb3afffa  We're working out an issue with BDD testing. I'll break it into smaller changesets and start pushing for review.  ></body> </Action>
<Action id="20001" issue="12923" author="kchristidis" type="comment" body="The design behind this is described here: https://docs.google.com/document/d/1vNMaM7XhOlu9tB_10dKnlrhy5d7b1u8lSY8a-kVjCO4/edit" created="2016-12-06 21:32:03.0" updateauthor="kchristidis" updated="2016-12-06 21:32:03.0"/>
<Action id="20061" issue="12923" author="kchristidis" type="comment" body="https://gerrit.hyperledger.org/r/3207" created="2016-12-12 05:30:26.0" updateauthor="kchristidis" updated="2016-12-12 05:30:26.0"/>
