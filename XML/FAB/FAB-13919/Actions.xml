<Action id="56272" issue="37184" author="denyeart" type="comment" created="2019-01-28 17:18:16.0" updateauthor="denyeart" updated="2019-01-28 17:18:16.0"> <body><! CDATA If you "Move the ledger folder" while peer process is up, peer still has a handle to the original file and can continue writing and committing.  This is the expected behavior on unix and is what you are seeing (unix doen't convey to the process that a file has moved). If you however make the file system inaccessible, you will see an error upon commit and peer will stop since it is unable to continue processing blocks.  For more details see unix documentation or various posting like this one:   https://stackoverflow.com/questions/2028874/what-happens-to-an-open-file-handle-on-linux-if-the-pointed-file-gets-moved-del      ></body> </Action>
<Action id="56308" issue="37184" author="mghasletwala" type="comment" created="2019-01-29 05:21:43.0" updateauthor="mghasletwala" updated="2019-01-29 05:22:51.0"> <body><! CDATA Actually "peer" only seems to be writing but actually nothing is being written. So the conclusion that  If you "Move the ledger folder" while peer process is up, peer still has a handle to the original file and can continue writing and committing  is incorrect. Blocks are not getting written on moved location. If they were then when we move back the files to original location the Block reading should be successful. But it fails. Which means that for some reason "peer" is unable to track that block writing is actually failing. The issues needs to be investigated.  In production deployment, we cannot have a situation where block file writing failures go undetected until some request to read blockfile is generated.  ></body> </Action>
<Action id="56339" issue="37184" author="denyeart" type="comment" body=" ~mghasletwala  Again, on unix when the file is moved it is a request for the OS to delete the original file as soon as all processes that have a handle to it release their handles. Any process that already has a handle to the old file will keep their handle to the old file. The process will continue writing to the old file. When the process dereferences the file handle, assuming it was the last process to have a handle on the file, the file will then be completely deleted from the OS.  This is how unix processes work with files that get removed under them. There is nothing that Fabric can fix with how unix deals with moved/deleted files. In cases where the OS returns an error writing to file, peer will indeed throw an error and stop since it is unable to commit." created="2019-01-29 14:27:23.0" updateauthor="denyeart" updated="2019-01-29 14:27:23.0"/>
<Action id="56385" issue="37184" author="manish-sethi" type="comment" created="2019-01-30 15:07:40.0" updateauthor="manish-sethi" updated="2019-01-30 15:07:40.0"> <body><! CDATA I Agree with the comments made by  ~denyeart  here.   ~mghasletwala  - Following are the links to similar questions that had previous been answered that may help you  - https://chat.hyperledger.org/channel/fabric-ledger?msg=GCDQ6arZB5biu5N5k - https://jira.hyperledger.org/browse/FAB-5551  ></body> </Action>
<Action id="56475" issue="37184" author="denyeart" type="comment" created="2019-01-31 22:10:54.0" updateauthor="denyeart" updated="2019-01-31 22:10:54.0"> <body><! CDATA  ~mghasletwala   Copying the mailing list answer here:  The Jira subject line is not true:  _>> Peer commits transaction even if storage location is not available_  As part of each block commit, fsync is indeed called to flush the block to disk and make it durable, and if there is an error writing to the file, the peer will panic since it is not able to proceed with block processing.  In your test scenario OS does not return any error because the OS allows you to move or delete a file from under a process that already has a handle to it. And the OS allows the process that has a handle to keep writing to the moved or deleted file. The same is true for any process, this is not Fabric specific behavior. Go ahead and try it on your favorite enterprise database. Or see the accepted answer in this space: https://stackoverflow.com/questions/2028874/what-happens-to-an-open-file-handle-on-linux-if-the-pointed-file-gets-moved-del  A production system should be locked down such that its files are not tampered with. There are infinite number of ways that a system admin could sabotage their own peer, deleting files underneath a running peer is just one of them. Blockchain doesn't solve the problem of protecting a peer from its own malicious or careless system admin. It solves the problem of protecting the rest of the network from any individual malicious or careless system admin.  I believe you are trying to test peer's behavior when there is an error writing to the file system. In that case, attempt a test that will actually cause an error writing to the file system, for example make the file system unavailable. You will see that peer handles errors upon block commit with this error: panic: Cannot commit block to the ledger due to Error while appending block to file: <OS ERROR MESSAGE>.  Most container orchestration software will be configured to automatically restart peer. Peer restart has recovery logic to ensure it can resume where it left off. But even the smartest peer will never be able to outsmart its own devilish system admin.  ></body> </Action>
