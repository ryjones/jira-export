<Action id="43450" issue="29710" author="scottz" type="comment" created="2018-04-26 04:24:30.0" updateauthor="scottz" updated="2018-04-26 04:24:30.0"> <body><! CDATA  https://gerrit.hyperledger.org/r/#/c/21035/      ></body> </Action>
<Action id="43470" issue="29710" author="sambhavdutt" type="comment" created="2018-04-26 13:22:29.0" updateauthor="sambhavdutt" updated="2018-04-26 13:22:29.0"> <body><! CDATA The OTE tests passed in the latest build, for the day.   *Build Logs* https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-daily-x86_64/333/artifact/gopath/src/github.com/hyperledger/fabric-test/regression/daily/ote_logs/  ></body> </Action>
<Action id="43503" issue="29710" author="scottz" type="comment" created="2018-04-26 16:42:11.0" updateauthor="scottz" updated="2018-04-26 16:50:36.0"> <body><! CDATA I could reproduce this on my laptop using Vagrant. Unsurprisingly, my machine ran out of space. I am not sure if it is same root cause, but I suspect so because the following error is the same as was seen in the daily test run. I think it might be when the orderer crashes, but not sure.  `Producer-o2-c0 failed to broadcast TX 2233 (ACK=2233 NACK=1), 2018-04-26 16:20:09.834210026 +0000 UTC m=+160.973592995, err: rpc error: code = Unavailable desc = transport is closing`  I also saw SERVICE_UNAVAILABLE error responses printed, from the orderer NACKs, because the kafkas had crashed, due to memory exhaustion.  While watching the logs on a couple orderers during the test `runote.sh -t -FAB-7078-`, I saw those orderers panic.  ```  ubuntu@hyperledger-devenv:cf7d15d:/opt/gopath/src/github.com/hyperledger/fabric$ docker logs -f orderer3.example.com 2018-04-26 15:57:59.572 UTC  localconfig  completeInitialization -> INFO 001 Kafka.Version unset, setting to 0.10.2.0 2018-04-26 16:02:35.448 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 002  channel: testchainid  Error during consumption: kafka: error while consuming testchainid/0: EOF 2018-04-26 16:02:35.883 UTC  orderer/consensus/kafka  enqueue -> ERRO 003  channel: testorgschannel1  cannot enqueue envelope because = dial tcp 172.18.0.7:9092: connect: connection refused 2018-04-26 16:02:36.196 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 004  channel: testorgschannel1  cannot post time-to-cut message = dial tcp 172.18.0.7:9092: connect: connection refused 2018-04-26 16:02:36.893 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 005  channel: testorgschannel1  Error during consumption: kafka: error while consuming testorgschannel1/0: EOF 2018-04-26 16:02:37.607 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 006  channel: testchainid  Error during consumption: kafka: error while consuming testchainid/0: dial tcp: lookup 212b79f90502 on 127.0.0.11:53: no such host 2018-04-26 16:02:38.901 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 007  channel: testorgschannel1  Error during consumption: kafka: error while consuming testorgschannel1/0: dial tcp: lookup 0518784a3760 on 127.0.0.11:53: no such host 2018-04-26 16:02:39.617 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 008  channel: testchainid  Error during consumption: kafka: error while consuming testchainid/0: dial tcp: lookup 212b79f90502 on 127.0.0.11:53: no such host 2018-04-26 16:02:40.922 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 009  channel: testorgschannel1  Error during consumption: kafka: error while consuming testorgschannel1/0: dial tcp: lookup 0518784a3760 on 127.0.0.11:53: no such host 2018-04-26 16:02:41.635 UTC  orderer/consensus/kafka  processMessagesToBlocks -> ERRO 00a  channel: testchainid  Error during consumption: kafka: error while consuming testchainid/0: dial tcp: lookup 212b79f90502 on 127.0.0.11:53: no such host 2018-04-26 16:02:43.064 UTC  orderer/commmon/multichannel  commitBlock -> CRIT 00b  channel: testorgschannel1  Could not append block: Error while appending block to file: write /var/hyperledger/production/orderer/chains/testorgschannel1/blockfile_000002: no space left on device panic:  channel: testorgschannel1  Could not append block: Error while appending block to file: write /var/hyperledger/production/orderer/chains/testorgschannel1/blockfile_000002: no space left on device  goroutine 3278  running : github.com/hyperledger/fabric/vendor/github.com/op/go-logging.(*Logger).Panicf(0xc42022fa40, 0xd82ae6, 0x28, 0xc4201e4a80, 0x2, 0x2) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/op/go-logging/logger.go:194 +0x126 github.com/hyperledger/fabric/orderer/common/multichannel.(*BlockWriter).commitBlock(0xc4202ed5c0, 0xc4209537d8, 0x3, 0x8) /opt/gopath/src/github.com/hyperledger/fabric/orderer/common/multichannel/blockwriter.go:171 +0x2f8 github.com/hyperledger/fabric/orderer/common/multichannel.(*BlockWriter).WriteBlock.func1(0xc4202ed5c0, 0xc4209537d8, 0x3, 0x8) /opt/gopath/src/github.com/hyperledger/fabric/orderer/common/multichannel/blockwriter.go:155 +0x79 created by github.com/hyperledger/fabric/orderer/common/multichannel.(*BlockWriter).WriteBlock /opt/gopath/src/github.com/hyperledger/fabric/orderer/common/multichannel/blockwriter.go:153 +0x8c```     ></body> </Action>
<Action id="44300" issue="29710" author="scottz" type="comment" body="The updates were merged. We can close this now. If problems encountered, then the error debugging info should help us get to root cause, and we can open a new issue with that new information." created="2018-05-13 20:02:02.0" updateauthor="scottz" updated="2018-05-13 20:04:23.0"/>
