<Issue id="17704" key="FAB-4506" number="4506" project="10002" reporter="kletkeman" creator="kletkeman" type="10001" summary="predicting constant txn failures under normal operation owing to flow control requirement" priority="1" resolution="10001" status="6" created="2017-06-09 18:11:09.0" updated="2020-01-22 19:16:14.0" resolutiondate="2020-01-22 19:16:14.0" votes="0" watches="3" workflowId="35212" security="10001"> <description><! CDATA The flow control requirement for v1 fabrics has some potentially nasty implications for smart contracts with an IoT component (and indeed for any contracts with multiple event sources for the same asset).  In an IoT scenario, a large asset (e.g. aircraft) could have many sensors that report periodically or all at once, for example when the aircraft has reached the gate. This burst would be sent to an asset in the contract, which could be the aircraft itself or the flight that the aircraft has just completed (and then propagated to the aircraft asset).  Each device will send a unique event through an IoT broker and those will be mapped to a contract API and payload and then sent. We cannot flow control based on the asset ID because there is no concept of a target asset in an IoT message. It might be embedded in the payload, but brokers are not contextual in that way.  So these are likely to be processed in parallel and sent to the contract simultaneously, resulting in all but one failing when the block is committed.  In v0.6, this was not an issue of course because queries looked at committed data while invokes looked at data to be committed, so multiple invokes could build on one another. But in v1, queries and invokes alike look only at committed data, which means that the many events that arrived together and are in limbo are going to automatically clash at commit time, with all but one ultimately failing.  After pondering flow control schemes, I have come to the conclusion that there is no way to prevent key clashes via flow control. We may actually be forced to create a smart flow control queue where each transaction that is sent will be monitored and checked off, with failed txns allowed up to 'n' retries before we give up.  But this has an extremely negative property of not preserving original order unless we add an extra level of complexity to only retry the oldest pending transaction once it hasÂ reported as failed or succeeded. This has the likely result of creating "stuck queues" as we wait for network interruptions or fabric glitches to work themselves out.  I think this is a very serious potential issue, and I am hoping that this has been solved in some way I just don't know about. But if not, then is the expected solution a complex flow control queue as I discuss above? Perhaps that could be built into the fabric on a channel by channel basis so that we don't all have to solve the problem?  ></description> </Issue>
