<Action id="51291" issue="34159" author="denyeart" type="comment" created="2018-09-27 18:01:27.0" updateauthor="denyeart" updated="2018-09-27 18:03:19.0"> <body><! CDATA Peer panics because it is unable to commit to state database, which is the expected behavior when state database becomes unavailable.  This may simply be an issue of stressing CouchDB beyond what the test hardware can handle.  Please attach log from couchdb side as well.  Is it possible to ramp up the test progressively so that we can see if there is a gradual degradation of response time and commit time prior to the eventual panic.  ></body> </Action>
<Action id="51294" issue="34159" author="adnanchoudhury" type="comment" created="2018-09-27 18:49:07.0" updateauthor="adnanchoudhury" updated="2018-09-27 18:49:23.0"> <body><! CDATA  ~denyeart  Agree the hardware can be an issue. Heere is the Couchlog (too big to attach here) https://drive.google.com/file/d/1nnJj0I_1ftl6nCBoD7LwTWmMd6DSxAyZ/view?usp=sharing    ></body> </Action>
<Action id="51309" issue="34159" author="adnanchoudhury" type="comment" body=" ~denyeart  ramping up test progressively in the current hardware will take a long time. Takes ~4 hours to get 2 million records in. " created="2018-09-27 21:45:22.0" updateauthor="adnanchoudhury" updated="2018-09-27 21:45:22.0"/>
<Action id="51489" issue="34159" author="chris.elder" type="comment" created="2018-10-02 09:28:58.0" updateauthor="chris.elder" updated="2018-10-02 09:28:58.0"> <body><! CDATA I reviewed the CouchDB logs.  It looks like things run normally until around timestamp 2018-09-27T01:30:31.195135Z.  Following this, GET and POST operations begin taking longer to execute.  Also, CouchDB starts to see "wait" states:   info  2018-09-27T04:10:47.989370Z nonode@nohost <0.201.0> -------- 11 clients waiting to open db shards/20000000-3fffffff/testorgschannel2_lscc.1537984595  Overall, I would say that this looks like the host is running out of resources.  Based on the wait state for shards, I would guess that it is running out of file handles.     ></body> </Action>
<Action id="51541" issue="34159" author="scottz" type="comment" created="2018-10-02 17:39:15.0" updateauthor="scottz" updated="2018-10-02 17:39:15.0"> <body><! CDATA Here are some other test ideas. We should definintely do the first, to see if the test passes on K8S.  1. Run same test on IBP K8S multihost cluster. (This test passed this way in July in v1.2). If pass, then this confirms if the panic was due to a resource issue on the vlaunch with docker containers.  2. If desired, we could continue to debug on the vlaunch machine to determine the factors influencing the likely resource contention:  2.A. Modify original test: without step 2 or 4 (without the upgrade and index build); if panics, then confirms is a resource limitation: the vlaunch machine cannot support the traffic rate of invokes and un-indexed queries with such a large DB size. Else the index rebuild process is a factor.  2.B. Modify prev test 2A (assuming 2A passed): by doing step 2 and 4 BUT WAIT for it to complete, BEFORE starting the step3 traffic; if panics, then confirms the index rebuild process temporarily uses resources, interfering with the traffic. Else the rebuilt indexes allow the queries of a 1M+ database size to be faster so the traffic rate can be handled OK.  ></body> </Action>
<Action id="66711" issue="34159" author="scottz" type="comment" body="More cpu and memory resources alleviate the problem." created="2020-01-15 20:37:54.0" updateauthor="scottz" updated="2020-01-15 20:37:54.0"/>
