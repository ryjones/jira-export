<Action id="25637" issue="17603" author="muralisr" type="comment" created="2017-06-07 12:33:40.0" updateauthor="muralisr" updated="2017-06-07 12:33:40.0"> <body><! CDATA The panic is from core/ledger/kvledger/kv_ledger.go   {code:java} logger.Debugf("Channel  %s : Committing block  %d  transactions to state database", l.ledgerID, blockNo) if err = l.txtmgmt.Commit(); err != nil { panic(fmt.Errorf(`Error during commit to txmgr:%s`, err)) } {code}  Removing fabric-peer for now.  ></body> </Action>
<Action id="25652" issue="17603" author="muralisr" type="comment" body="assigning it to me to take a first look." created="2017-06-07 15:02:40.0" updateauthor="muralisr" updated="2017-06-07 15:02:54.0"/>
<Action id="25690" issue="17603" author="denyeart" type="comment" created="2017-06-07 19:41:51.0" updateauthor="denyeart" updated="2017-06-07 19:42:22.0"> <body><! CDATA There are some tips for increasing resources here:   https://wiki.apache.org/couchdb/Performance   I typically make updates to /opt/couchdb/etc/default.ini, but I think local.ini is the better config file to document.   ~chris.elder  has been doing stress testing against CouchDB and has a list of couch and erlang config options to update.  Chris can you comment on which properties you set and how?  Assuming this defect is simply due to couchdb config, we could use this defect to update our default couchdb config (/fabric/images/couchdb/local.ini) and/or documentation.  ></body> </Action>
<Action id="25691" issue="17603" author="denyeart" type="comment" created="2017-06-07 19:50:05.0" updateauthor="denyeart" updated="2017-06-07 19:50:05.0"> <body><! CDATA The design is to perform retries against CouchDB upon errors like this.  And if we can't commit within X retries, then the block cannot be processed and there is no option but to panic the peer and have an administrator try to understand the errors.  However in the attached logs I don't see the "Retrying couchdb" message as I would expect prior to the peer panic.  We'll need to understand why no retries occurred and fix that, in addition to the underlying problem.  ></body> </Action>
<Action id="25692" issue="17603" author="muralisr" type="comment" body="Will let  ~Ratnakar  connection properties  ~denyeart   ~chris.elder ... assigning it to chris now that its likely couchdb config issue" created="2017-06-07 19:51:25.0" updateauthor="muralisr" updated="2017-06-07 19:51:25.0"/>
<Action id="25729" issue="17603" author="mastersingh24" type="comment" created="2017-06-08 12:43:41.0" updateauthor="mastersingh24" updated="2017-06-08 12:43:41.0"> <body><! CDATA It would also be helpful to know the configuration of the test machines, etc.  As  ~denyeart  mentioned, the panic behavior is expected and if you are trying to run these types of tests on standard desktop machines, you are going to run into issues for sure.  I expect you will also need to tune things like ulimit, etc on the host machines.      ~denyeart  - at this point, I would not consider this to be a "Highest" defect but up to you  ></body> </Action>
<Action id="25753" issue="17603" author="ratnakar" type="comment" created="2017-06-08 15:15:33.0" updateauthor="ratnakar" updated="2017-06-08 15:15:33.0"> <body><! CDATA  ~chris.elder  here are the system configurations: {code:java} CPU : Intel® Xeon(R) CPU E3-1505M v5 @ 2.80GHz × 8 Memory : 16 GB Disk 250 GB Kernel : 4.8.0-53-generic OS : Ubuntu 16.10 (yakkety)  ulimit : 1024{code}  ></body> </Action>
<Action id="25754" issue="17603" author="chris.elder" type="comment" created="2017-06-08 15:23:34.0" updateauthor="chris.elder" updated="2017-06-08 15:23:34.0"> <body><! CDATA The following are some recommendations for configuring CouchDB:  1. Set an environment variable for Erlang to allow more connections on CouchDB: export ERL_MAX_PORTS=10000  2. Increase ulimit for open files on CouchDB: ulimit -n 100000  3. Add the following to the local.ini in image/couchdb before running "make docker":   httpd  max_connections=10000  4. This is probably not the issue, but ephemeral ports can be extended with the following: sudo sysctl -w net.ipv4.ip_local_port_range="1024 64000"  ></body> </Action>
<Action id="25756" issue="17603" author="denyeart" type="comment" body="Per Gari&apos;s suggestion I will move this to &apos;High&apos;, but the team will continue working this as top priority." created="2017-06-08 15:49:48.0" updateauthor="denyeart" updated="2017-06-08 15:49:48.0"/>
<Action id="25782" issue="17603" author="ratnakar" type="comment" created="2017-06-08 19:17:14.0" updateauthor="ratnakar" updated="2017-06-08 19:17:14.0"> <body><! CDATA As per  ~chris.elder  suggestion changed the settings and ran the tests in three iterations  (About 2.5 hours)  However , would like to revert few of the changes and retry again update the results by tomorrow  ></body> </Action>
<Action id="25883" issue="17603" author="ratnakar" type="comment" created="2017-06-09 17:27:15.0" updateauthor="ratnakar" updated="2017-06-09 17:27:39.0"> <body><! CDATA Per  ~muralisr 's suggestion reverted all the configuration settings and ran two iterations.  Unable to reproduce the issue any more.     Docker images : *x86_64-1.0.0-beta*  ></body> </Action>
<Action id="25911" issue="17603" author="ratnakar" type="comment" created="2017-06-09 19:46:55.0" updateauthor="ratnakar" updated="2017-06-09 20:16:39.0"> <body><! CDATA It is observed that all peers crashes again, however this time it is because *disc space is occupied 100%*  _+VLaunch machine configurations+_: {panel} OS: Ubuntu 16.04.2 LTS (xenial) CPU: Intel(R) Xeon(R) CPU E5-2658 v2 @ 2.40GHz RAM: 8GB {panel} *  ## df -h* {code:java} Filesystem                           Size  Used Avail Use% Mounted on udev                                 3.9G     0  3.9G   0% /dev tmpfs                                799M   82M  717M  11% /run /dev/mapper/UB16--4--64SVR--vg-root   75G   71G     0 100% / tmpfs                                3.9G  1.2M  3.9G   1% /dev/shm tmpfs                                5.0M     0  5.0M   0% /run/lock tmpfs                                3.9G     0  3.9G   0% /sys/fs/cgroup /dev/sda1                            472M  228M  220M  51% /boot tmpfs                                799M   28K  799M   1% /run/user/124 tmpfs                                799M     0  799M   0% /run/user/0 none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/e3f0904001039e8a3f8261b563a7e19da0e1f7c117a58bd9cabb2288f13c1fe2 none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/f58967866de99aec000f927f543063dce00bb0dfb5a03701c748f76bdf86c2bb shm                                   64M     0   64M   0% /var/lib/docker/containers/45fc09f06da6367f0d069acef967ff752665f5ebc6e14549a4274f84356bc541/shm shm                                   64M     0   64M   0% /var/lib/docker/containers/258171d899ec1c4b5ae83c73bd7b630a78d81e7885b05028af10f147ee928a29/shm none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/5b69180112b967ea9c80fadc1a387cec2a192b6e1a724bfb7fca70d0c622b021 shm                                   64M     0   64M   0% /var/lib/docker/containers/040e2425c5e4133a705494630395051ba2f37e63e28c48ec6adbe2ed78023821/shm none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/32cbd6e66405dc8073d83e276b87498bce7756b384f2c1fdbeef4396c07aeaa4 none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/9b3086a496431b55328c6eb55adaedb6bd8139adad2f6a8814ff34311778ec82 shm                                   64M     0   64M   0% /var/lib/docker/containers/6129b345ee22035b4965fea1b749a6117d63ba4f8d9e6d5777b5c6b1ef54e07e/shm none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/24352086a90fb6fa9056c44a9ec956e74a936254c438a921b69096a005b19a6c shm                                   64M     0   64M   0% /var/lib/docker/containers/6f8d4ae749f055196133760103d84e3308bb7f1c6be7ff111f6630c38c532d3c/shm shm                                   64M     0   64M   0% /var/lib/docker/containers/2c31776d226be6500897c9e44e0ed84e97ee6b20489ab1209d36ca5abc963714/shm none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/432b1d671441a983ac6e15e779089d547eba51098ca52a3cf36dbb7de4613cf4 none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/e9c55f73ee2598cea445996c38dee71b634b099597b88ea6c16425d30b923d33 shm                                   64M     0   64M   0% /var/lib/docker/containers/bc191b13cdeb5d7f67a1d3f757b2f75a3dee10ae3481c2458c6e86b8801c8f14/shm none                                  75G   71G     0 100% /var/lib/docker/aufs/mnt/dcab30df339f03b99db82a33e704fcdd5bbaadec2cd3fa98bb655c8e2a1d453e shm                                   64M     0   64M   0% /var/lib/docker/containers/c6d969e52c0ed9190c4860b6ac5dca10a21679cfeb4d8773d860df3ae6826124/shm shm                                   64M     0   64M   0% /var/lib/docker/containers/5500d49ba3f5487de0b021b2ea5ce1040927f5ba9a129b2bf33725f6776c2738/shm {code}  {code:java} 2017-06-09 19:09:30.892 UTC  gossip/state  directMessage -> WARN 684e Received state transfer request for channel mychannel0 while expecting channel mychannel1 skipping request... 2017-06-09 19:09:30.894 UTC  gossip/state  handleStateResponse -> WARN 684f Payload with sequence number 2846 was received earlier 2017-06-09 19:09:30.897 UTC  gossip/state  handleStateResponse -> WARN 6850 Payload with sequence number 2847 was received earlier 2017-06-09 19:09:30.899 UTC  gossip/state  handleStateResponse -> WARN 6851 Payload with sequence number 2848 was received earlier 2017-06-09 19:09:30.900 UTC  gossip/state  handleStateResponse -> WARN 6852 Payload with sequence number 2849 was received earlier 2017-06-09 19:09:30.902 UTC  gossip/state  handleStateResponse -> WARN 6853 Payload with sequence number 2850 was received earlier 2017-06-09 19:09:30.904 UTC  gossip/state  handleStateResponse -> WARN 6854 Payload with sequence number 2851 was received earlier 2017-06-09 19:09:33.440 UTC  gossip/comm  sendToEndpoint -> WARN 6855 Failed obtaining connection for peer1.org1.example.com:7051, PKIid: 67 124 78 183 187 42 148 108 146 164 250 224 124 234 37 91 104 181 113 138 120 108 203 10 88 28 232 83 238 170 114 83  reason: context deadline exceeded 2017-06-09 19:09:33.457 UTC  gossip/discovery  expireDeadMembers -> WARN 6856 Entering   67 124 78 183 187 42 148 108 146 164 250 224 124 234 37 91 104 181 113 138 120 108 203 10 88 28 232 83 238 170 114 83   2017-06-09 19:09:33.458 UTC  gossip/discovery  expireDeadMembers -> WARN 6857 Closing connection to Endpoint: peer1.org1.example.com:7051, InternalEndpoint: peer1.org1.example.com:7051, PKI-ID:  67 124 78 183 187 42 148 108 146 164 250 224 124 234 37 91 104 181 113 138 120 108 203 10 88 28 232 83 238 170 114 83 , Metadata:    2017-06-09 19:09:33.458 UTC  gossip/discovery  expireDeadMembers -> WARN 6858 Exiting 2017-06-09 19:09:34.307 UTC  gossip/comm  sendToEndpoint -> WARN 6859 Failed obtaining connection for peer1.org2.example.com:7051, PKIid: 11 182 95 191 106 158 167 194 187 127 150 168 2 195 169 9 19 211 237 12 146 96 8 191 186 166 171 234 153 112 39 53  reason: context deadline exceeded 2017-06-09 19:09:34.307 UTC  gossip/discovery  expireDeadMembers -> WARN 685a Entering   11 182 95 191 106 158 167 194 187 127 150 168 2 195 169 9 19 211 237 12 146 96 8 191 186 166 171 234 153 112 39 53   2017-06-09 19:09:34.307 UTC  gossip/discovery  expireDeadMembers -> WARN 685b Closing connection to Endpoint: peer1.org2.example.com:7051, InternalEndpoint: , PKI-ID:  11 182 95 191 106 158 167 194 187 127 150 168 2 195 169 9 19 211 237 12 146 96 8 191 186 166 171 234 153 112 39 53 , Metadata:    2017-06-09 19:09:34.307 UTC  gossip/discovery  expireDeadMembers -> WARN 685c Exiting 2017-06-09 19:09:36.440 UTC  gossip/comm  sendToEndpoint -> WARN 685d Failed obtaining connection for peer1.org1.example.com:7051, PKIid: 67 124 78 183 187 42 148 108 146 164 250 224 124 234 37 91 104 181 113 138 120 108 203 10 88 28 232 83 238 170 114 83  reason: context deadline exceeded 2017-06-09 19:09:36.520 UTC  statecouchdb  ApplyUpdates -> ERRO 685f Error during Commit(): Put http://couchdb0:5984/mychannel1/mycc1%0069_80: dial tcp: lookup couchdb0 on 127.0.0.11:53: no such host panic: Error during commit to txmgr:Put http://couchdb0:5984/mychannel1/mycc1%0069_80: dial tcp: lookup couchdb0 on 127.0.0.11:53: no such host goroutine 617  running : panic(0xc816c0, 0xc421833380) /opt/go/src/runtime/panic.go:500 +0x1a1 github.com/hyperledger/fabric/core/ledger/kvledger.(*kvLedger).Commit(0xc42163f9c0, 0xc43ed73300, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/kv_ledger.go:229 +0x806 github.com/hyperledger/fabric/core/ledger/ledgermgmt.(*closableLedger).Commit(0xc421309f60, 0xc43ed73300, 0x0, 0x0) <autogenerated>:1 +0x54 github.com/hyperledger/fabric/core/committer.(*LedgerCommitter).Commit(0xc42162c690, 0xc43ed73300, 0x7f315043a400, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/core/committer/committer_impl.go:86 +0x14a github.com/hyperledger/fabric/gossip/state.(*GossipStateProviderImpl).commitBlock(0xc42159b360, 0xc43ed73300, 0x4, 0x4) /opt/gopath/src/github.com/hyperledger/fabric/gossip/state/state.go:621 +0x56 github.com/hyperledger/fabric/gossip/state.(*GossipStateProviderImpl).deliverPayloads(0xc42159b360) /opt/gopath/src/github.com/hyperledger/fabric/gossip/state/state.go:437 +0x478 created by github.com/hyperledger/fabric/gossip/state.NewGossipStateProvider /opt/gopath/src/github.com/hyperledger/fabric/gossip/state/state.go:219 +0x7d9   {code}    ></body> </Action>
<Action id="25917" issue="17603" author="muralisr" type="comment" body=" ~Ratnakar  wonder if this was the issue before as well.... is the couch db log still around ? it may have something interesting." created="2017-06-09 20:22:50.0" updateauthor="muralisr" updated="2017-06-09 20:22:50.0"/>
<Action id="25918" issue="17603" author="ratnakar" type="comment" created="2017-06-09 20:26:59.0" updateauthor="ratnakar" updated="2017-06-09 20:26:59.0"> <body><! CDATA  ~muralisr  due to disk space full issue I couldn't be able to capture the couchdb logs (I will try to see though)  I was able to capture 6GB of orderer logs earlier and hence I would say it is not the same issue before.     Since this issue could be due to resource limitation , reducing to medium.  ></body> </Action>
<Action id="25995" issue="17603" author="binhn" type="comment" body=" ~Ratnakar   ~chris.elder  are we at the point where we can conclude  FAB-4243  ?" created="2017-06-11 18:59:28.0" updateauthor="binhn" updated="2017-06-11 18:59:28.0"/>
<Action id="26592" issue="17603" author="manish-sethi" type="comment" created="2017-06-16 16:52:02.0" updateauthor="manish-sethi" updated="2017-06-16 16:56:12.0"> <body><! CDATA  ~Ratnakar  - The root error (`no such host` - DNS resolution error) in the panic stack seems strange in the first sight - as this appears to be too basic to fail.   {{ panic: Error during commit to txmgr:Put  http://couchdb0:5984/mychannel1/mycc1%0069_80:  dial tcp: lookup couchdb0 on 127.0.0.11:53: no such host}}   In a primitive search, I came across posts that talks about this and link the root cause to - the concurrent request to golang function for DNS resolution under file descriptor constrained environment.  There are some tests that folks have appended in these threads for reproducing this error. You may like to run those independently to verify your environment.     Related posts -  https://github.com/golang/go/issues/3575   https://github.com/AdRoll/hologram/issues/73 |https://github.com/AdRoll/hologram/issues/73, https://github.com/golang/go/issues/16345  ></body> </Action>
<Action id="26787" issue="17603" author="denyeart" type="comment" created="2017-06-19 19:42:10.0" updateauthor="denyeart" updated="2017-06-19 19:42:10.0"> <body><! CDATA  ~Ratnakar  Further study of CouchDB under stress is resulting in these other work items: FAB-4243 - Increase CouchDB max_dbs_open default value FAB-3464 - Document ulimit setting for CouchDB stress environments  FAB-4872 - Document max_dbs_open and ephemeral port settings for CouchDB stress environments   Given that these work items will cover the configuration needed to enable CouchDB in high stress environments, can we close this Bug (FAB-4442) at this point?  ></body> </Action>
<Action id="26862" issue="17603" author="denyeart" type="comment" body="Discussed with  ~Ratnakar . On his next stress trials he will use the config options mentioned above.  It *should* work with the config options in place. I will close for now, please re-open if there are additional issues." created="2017-06-20 15:48:14.0" updateauthor="denyeart" updated="2017-06-20 15:48:14.0"/>
