<Action id="56669" issue="37418" author="yacovm" type="comment" created="2019-02-04 20:46:05.0" updateauthor="yacovm" updated="2019-02-04 20:46:05.0"> <body><! CDATA  ~guoger  the  inuse_space graph|https://jira.hyperledger.org/secure/attachment/16712/orderer1st-profile.png  hints the memory leak comes from the WAL.... can you take a look?  I don't see in the logs attached that *Taking snapshot* is written... can it be that a spanshot is never taken and therefore the WAL is never compacted?   ></body> </Action>
<Action id="56677" issue="37418" author="guoger" type="comment" body=" ~yacovm  I suspect that it&apos;s not enabled. May somebody check the {{SnapshotInterval}} value in channel config?" created="2019-02-05 03:16:54.0" updateauthor="guoger" updated="2019-02-05 03:16:54.0"/>
<Action id="56703" issue="37418" author="suryalnvs" type="comment" created="2019-02-05 20:31:10.0" updateauthor="suryalnvs" updated="2019-02-05 20:48:25.0"> <body><! CDATA After adding SnapshotInterval, reran the test and observed the following:  1. Orderer3 is the leader for both orderer system channel and application channel  2. After running the traffic for 40000 transactions, memory for orderer1st and orderer2nd gradually increased whereas orderer3rd has stable memory {code:java} ibmadmin@cello-launcher:~/cello/src/agent/ansible/vars$ ./kubectl --kubeconfig=kubeconfig.regression top pod orderer1st-ordererorg  NAME                    CPU(cores)   MEMORY(bytes)  orderer1st-ordererorg   12m          460Mi ibmadmin@cello-launcher:~/cello/src/agent/ansible/vars$ ./kubectl --kubeconfig=kubeconfig.regression top pod orderer2nd-ordererorg NAME                    CPU(cores)   MEMORY(bytes) orderer2nd-ordererorg   16m          490Mi ibmadmin@cello-launcher:~/cello/src/agent/ansible/vars$ ./kubectl --kubeconfig=kubeconfig.regression top pod orderer3rd-ordererorg NAME                    CPU(cores)   MEMORY(bytes) orderer3rd-ordererorg   6m           230Mi {code} 3. Checked back again after around 5 minutes {code:java} ibmadmin@cello-launcher:~/cello/src/agent/ansible/vars$ ./kubectl --kubeconfig=kubeconfig.regression top pod orderer1st-ordererorg ; date NAME                    CPU(cores)   MEMORY(bytes) orderer1st-ordererorg   4m           602Mi Tue Feb  5 15:02:52 EST 2019 ibmadmin@cello-launcher:~/cello/src/agent/ansible/vars$ ./kubectl --kubeconfig=kubeconfig.regression top pod orderer2nd-ordererorg ; date NAME                    CPU(cores)   MEMORY(bytes) orderer2nd-ordererorg   4m           600Mi Tue Feb  5 15:03:13 EST 2019 ibmadmin@cello-launcher:~/cello/src/agent/ansible/vars$ ./kubectl --kubeconfig=kubeconfig.regression top pod orderer3rd-ordererorg ; date NAME                    CPU(cores)   MEMORY(bytes) orderer3rd-ordererorg   7m           229Mi Tue Feb  5 15:03:23 EST 2019 {code} Attached * orderer1st-logs-withSnapshotInterval.zip, orderer1st-logs-1-withSnapshotInterval.zip (extension) * orderer1st-withSnapshotinterval-afterTraffic.txt * orderer1st-withSnapshotInterval.png * confixtx.yml used   Environment variables used for orderers {code:java} - { name: "FABRIC_LOGGING_SPEC", value: "debug:orderer.common.cluster.step=error" } - { name: "ORDERER_GENERAL_PROFILE_ENABLED", value: "true" } - { name: "ORDERER_GENERAL_LISTENADDRESS", value: "0.0.0.0" } - { name: "ORDERER_FILELEDGER_LOCATION", value: "/shared/data" } - { name: "ORDERER_CONSENSUS_WALDIR", value: "/shared/data/etcdraft/wal" } - { name: "ORDERER_CONSENSUS_SNAPDIR", value: "/shared/data/etcdraft/snapshot" } - { name: "ORDERER_GENERAL_GENESISMETHOD", value: "file" } - { name: "ORDERER_GENERAL_GENESISFILE", value: "/etc/hyperledger/fabric/artifacts/keyfiles/genesis.block" } - { name: "ORDERER_GENERAL_LOCALMSPID", value: "ordererorg" } - { name: "ORDERER_GENERAL_LOCALMSPDIR", value: "/etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/msp" } - { name: "ORDERER_GENERAL_TLS_ENABLED", value: "true" } - { name: "ORDERER_GENERAL_TLS_PRIVATEKEY", value: "/etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/tls/server.key" } - { name: "ORDERER_GENERAL_CLUSTER_CLIENTPRIVATEKEY", value: "/etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/tls/server.key" } - { name: "ORDERER_GENERAL_TLS_CERTIFICATE", value: "/etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/tls/server.crt" } - { name: "ORDERER_GENERAL_CLUSTER_CLIENTCERTIFICATE", value: "/etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/tls/server.crt" } - { name: "ORDERER_GENERAL_TLS_ROOTCAS", value: " /etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/tls/ca.crt " } - { name: "ORDERER_GENERAL_CLUSTER_ROOTCAS", value: " /etc/hyperledger/fabric/artifacts/keyfiles/ordererorg/orderers/orderer1st-ordererorg.ordererorg/tls/ca.crt " } - { name: "ORDERER_KAFKA_RETRY_SHORTINTERVAL", value: "1s" } - { name: "ORDERER_KAFKA_RETRY_SHORTTOTAL", value: "30s" } - { name: "ORDERER_KAFKA_VERBOSE", value: "true" } {code}    ></body> </Action>
<Action id="56723" issue="37418" author="yacovm" type="comment" created="2019-02-06 15:29:53.0" updateauthor="yacovm" updated="2019-02-06 15:40:49.0"> <body><! CDATA Can you try to set the snapshot interval to 5 and try again, but without 100 channels (only 1 application channel), and also - get the stack trace of the goroutines at the end using the SIGUSR1 signal? (stack trace and that memory heap dump picture too)  Thanks in advance.   ></body> </Action>
<Action id="56729" issue="37418" author="yacovm" type="comment" body="and can we check it for a more prolonged period of time - like a couple of hours? " created="2019-02-06 19:19:38.0" updateauthor="yacovm" updated="2019-02-06 19:19:38.0"/>
<Action id="56736" issue="37418" author="scottz" type="comment" created="2019-02-06 23:28:02.0" updateauthor="scottz" updated="2019-02-06 23:28:02.0"> <body><! CDATA Yes,  ~kchristidis  , we will update the RaftSVT epic where the "standard" settings are described.  Yes,  ~yacovm  , we will rerun as you request. The lab Surya is using is already using only 1 channel. We will set the snapshot interval to 5, and run for 2 hours before collecting the requested data.  I would like to clarify something from  ~suryalnvs  's previous comment. The data that he collected and shared in his comment shows a memory increase in 2 of the 3 orderers during a period when NO traffic was running. Steps he did: # Send 40000 TXs # wait until test finished # wait a few (10) minutes more # execute the top command (his item 2) # wait 5 minutes # execute the top command (his item 3, which shows orderer 3 with approx same memory usage, but others increased usage)  This was surprising. And it is a different scenario than what was first captured, which showed our pictures of memory consumption increasing in all 3 orderers (albeit somewhat inconsistently) while traffic was running continuously over a 24 hour period. So it seems we may have multiple code paths that could leak memory.  Now that I've clarified that, rest assured that Surya confirms he will rerun the traffic scenario and collect the data as requested, as soon as he frees up from the urgent work his manager asked him to do today. (Apologies for the delay. I suspect it may be another day or two.)     ></body> </Action>
<Action id="56874" issue="37418" author="suryalnvs" type="comment" body=" ~yacovm  What value should be used for SnapshotInterval for rerun as it will be referring bytes with FAB-13656 change ?" created="2019-02-11 17:17:47.0" updateauthor="suryalnvs" updated="2019-02-11 17:17:47.0"/>
<Action id="56882" issue="37418" author="yacovm" type="comment" body="5MB" created="2019-02-11 19:41:05.0" updateauthor="yacovm" updated="2019-02-11 19:41:05.0"/>
<Action id="56896" issue="37418" author="suryalnvs" type="comment" created="2019-02-11 23:00:12.0" updateauthor="suryalnvs" updated="2019-02-11 23:15:04.0"> <body><! CDATA Reran the test with 5MB as snapshot interval and for 2 hours. * orderer3 is the leader on testorgschannel1 * orderer2 is the leader on orderer system channel * orderer2 is the ingress for traffic during the test on testorgschannel1  Attached the following:  1. netstat and number of file descriptors output: 02112019-orderer1st-netstat.txt, 02112019-orderer2nd-netstat.txt, 02112019-orderer3rd-netstat.txt  2. pprof out image: 02112019-orderer1st-out.png, 02112019-orderer2nd-out.png, 02112019-orderer3rd-out.png  3. pprof pb.gz file: 02112019-orderer1st-pprof.pb.gz,02112019-orderer2nd-pprof.pb.gz, 02112019-orderer3rd-pprof.pb.gz  4. output of top on all orderers for 100 minutes during the traffic: 02112019-top-command-output-on-orderers.txt  5. Memory analysis from grafana: 02112019-orderers-pod-memory, 02112019-orderers-container-memory-during-traffic-2hours  ></body> </Action>
<Action id="56923" issue="37418" author="yacovm" type="comment" body="how come the  top command|https://jira.hyperledger.org/secure/attachment/16793/02112019-top-command-output-on-orderers.txt  shows memory which is a lot less than the  pod memory|https://jira.hyperledger.org/secure/attachment/16795/02112019-orderers-pod-memory.png ? " created="2019-02-12 13:47:42.0" updateauthor="yacovm" updated="2019-02-12 13:47:42.0"/>
<Action id="56938" issue="37418" author="scottz" type="comment" created="2019-02-12 20:34:17.0" updateauthor="scottz" updated="2019-02-12 20:34:17.0"> <body><! CDATA Scratching our heads on this. Apparently, the tools do not display the same thing. Looked online for help. Here's what it seems now: * The grafana graph for the pod memory usage shows memory used including cached memory and buffers usage.  * The top command that we ran against the pods (each with only one container with one orderer) using "kubectl top pod" seems to show the memory used, but not counting any cache/buffers (which seem to be the vast majority).  During the life of the test, the overall numbers in the pod graph show an increase (this includes memory and cache/buffers). We ran a constant rate of traffic during the time period. The memory usage in the pods graphs shows a steady increase as well, especially in orderer2. And for the last eighth of the graph, the pod memory usage remained constant, when the traffic had been stopped.    The series of top commands every minute shows the increase in memory usage too. By the end of the test, the memory shown by the top commands was higher than at the beginning.  ====Mon Feb 11 15:27:17 EST 2019==== NAME CPU(cores) MEMORY(bytes)  orderer1st-ordererorg 15m 75Mi  orderer2nd-ordererorg 214m *276Mi*  orderer3rd-ordererorg 59m 74Mi  ====Mon Feb 11 17:07:48 EST 2019==== NAME CPU(cores) MEMORY(bytes)  orderer1st-ordererorg 13m 102Mi  orderer2nd-ordererorg 160m *419Mi*  orderer3rd-ordererorg 47m 112Mi  We noticed in other tests (not this one) that the memory usage levels seemed to drop off once the traffic ended. But we do not see that here; possibly we did not wait long enough; we will continue to review other tests and try to understand the behavior.  ></body> </Action>
<Action id="56940" issue="37418" author="yacovm" type="comment" created="2019-02-12 20:58:48.0" updateauthor="yacovm" updated="2019-02-12 21:07:15.0"> <body><! CDATA Well, 276MB and 429MB are not that bad. Can you prolong the test? I suggest we wait and see if it goes up to more, like 1GB, and let's see how far it can go in general, and only then provide:    * Heap dump to a png file similar to  ^02112019-orderer3rd-out.png . * Stack trace dump via *KILL -SIGUSR1* * Logs - you can just point us to a server and directory - we'll log in remotely and see, no need to upload them here.     Don't stop the traffic but just keep on sending stuff until we get to really high memory levels.     Also - what is special about orderer2 ? is that the leader?     ></body> </Action>
<Action id="56945" issue="37418" author="suryalnvs" type="comment" body="orderer2 is ingress on application channel and it is the leader for orderer system channel" created="2019-02-12 22:04:16.0" updateauthor="suryalnvs" updated="2019-02-12 22:04:16.0"/>
<Action id="56946" issue="37418" author="yacovm" type="comment" body="If it&apos;s the ingress then it somewhat makes sense, because its transaction gRPC streams (we have a stream for consensus message, and a different stream for transactions, for quality of service) are saturated with transactions." created="2019-02-12 22:16:45.0" updateauthor="yacovm" updated="2019-02-12 22:16:45.0"/>
<Action id="56986" issue="37418" author="suryalnvs" type="comment" created="2019-02-13 16:28:17.0" updateauthor="suryalnvs" updated="2019-02-13 16:28:17.0"> <body><! CDATA  ^021319-memory-orderers.txt  Top output of orderers from the latest traffic run.  In this case, orderer1 is the leader for application channel OSN and orderer2 is the ingress OSN.  ></body> </Action>
<Action id="56988" issue="37418" author="yacovm" type="comment" created="2019-02-13 16:51:53.0" updateauthor="yacovm" updated="2019-02-13 16:55:23.0"> <body><! CDATA thanks for the memory log.    # Can we also get the heap dumps in PNG form, please? The best would be to get the dumps really at the start, and the last dumps before it ran out of space. # Is it possible to reach at least 1GB on the memory consumption on the next run? I'm curious how far it can go. # We now have metrics that are pulled via prometheus, is it possible to obtain them in the next run, (metrics as a function of time)? specifically the ones in the "go" namespace, with "mem" as subsystem.  ></body> </Action>
<Action id="56989" issue="37418" author="suryalnvs" type="comment" body="Attached the heap dumps of all three orderers (only attached heap dumps from 1 hour apart for all orderers due to attachment limitation here). Can share more heap dumps if needed. Our first heap dump was from 6pm yesterday (02/12) whereas the network was launched on 02/11. The last heap dump is from 2.24am 02/13, 30min  before the out of space crash." created="2019-02-13 17:06:53.0" updateauthor="suryalnvs" updated="2019-02-13 17:06:53.0"/>
<Action id="56990" issue="37418" author="suryalnvs" type="comment" created="2019-02-13 17:14:29.0" updateauthor="suryalnvs" updated="2019-02-13 17:14:29.0"> <body><! CDATA Definitely, we are aiming to achieve for number 2 by increasing the storage volume of each orderer to 128GB from 50GB and number 3 by enabling metrics with prometheus in the new run with the following: * yacov/fabric:orderer image * logs - debug level * heap dumps - every 30 mins  * top output - every 30 mins * fabric prometheus metrics  ></body> </Action>
<Action id="56991" issue="37418" author="yacovm" type="comment" body="Let&apos;s run the latest version for the next run, no need to use my custom docker image." created="2019-02-13 17:18:42.0" updateauthor="yacovm" updated="2019-02-13 17:18:42.0"/>
<Action id="56993" issue="37418" author="sykesm" type="comment" created="2019-02-13 17:32:59.0" updateauthor="sykesm" updated="2019-02-13 17:32:59.0"> <body><! CDATA Let me expand on Yacov's requests:  We need to identify the area of memory growth over time. Any attempt to use one perspective for that is unworkable. This means, in particular, that looking at the Kubernetes memory information is insufficient. That also means looking at the pprof heap in use isn't enough.  So, what we need:  1. Every 30 minutes, please collect a pprof heap dump from the running orderers. Do this over a span of a couple of hours. Save this data as we will use it to determine what allocations are *growing* - not simply where they are allocated. This basically means something like {{curl -s http://address:port/debug/pprof/heap > heap-profile.pb}} is used to gather the data for each orderer. The thread create profile information could also be helpful...  2. Use the metrics support that was added to the orderer; we added it for a reason. In the case of memory leaks, the go.mem information gives a breakdown of the various types of memory usage. It's very possible memory growth is coming from somewhere other than the heap.  Thanks.  ></body> </Action>
<Action id="56994" issue="37418" author="sykesm" type="comment" created="2019-02-13 17:34:49.0" updateauthor="sykesm" updated="2019-02-13 17:34:49.0"> <body><! CDATA {quote}logs - debug level{quote}  Why? We're not trying to debug a logic issue. All that will do is slow things down and generate way more data than is likely going to be helpful.  ></body> </Action>
<Action id="56996" issue="37418" author="yacovm" type="comment" created="2019-02-13 17:39:35.0" updateauthor="yacovm" updated="2019-02-13 17:41:20.0"> <body><! CDATA Thanks  ~sykesm  for chiming in, good ideas.   ~suryalnvs  - can you please accommodate Matt's requests?     By the way - how many/what containers do you have in each pod? What is each pod running besides an orderer? {quote}   ./kubectl --kubeconfig=kubeconfig.regression top pod orderer1st-ordererorg {quote}    Is it possible to know the memory consumption of the orderer process only, and not the entire pod?  ></body> </Action>
<Action id="56997" issue="37418" author="sykesm" type="comment" created="2019-02-13 17:58:48.0" updateauthor="sykesm" updated="2019-02-13 17:58:48.0"> <body><! CDATA Latest data appears to show heap in use going *down* from the beginning to the end.  Example, orderer 1: Feb 12, 6:03p - 36954.42 Feb 13, 2:24a - 21459.55K  ></body> </Action>
<Action id="56999" issue="37418" author="suryalnvs" type="comment" body=" ~yacovm  one pod is having one container with orderer running in it. so a different pod for each orderer. No other processes are running along side orderer in the pod." created="2019-02-13 18:36:51.0" updateauthor="suryalnvs" updated="2019-02-13 18:36:51.0"/>
<Action id="57004" issue="37418" author="scottz" type="comment" created="2019-02-13 20:38:55.0" updateauthor="scottz" updated="2019-02-13 20:38:55.0"> <body><! CDATA After talking with Matt and to accommodate other recent requests, here is the new plan in regards to our next test to gather data for mem leak investigation: * Use latest stable images. * Each pod has one container. Each container has one orderer. * Set logs to INFO level in the peers and orderers. * Dump heap usage in the orderers - every 15 mins. * Dump threadcreate usage in the orderers - every 15 mins. * Dump "top" output from the pods - every 15 mins. * Gather fabric prometheus metrics, especially go.mem. * In a new clean network, start traffic (32 threads). * After a few minutes for things to stabilize, monitor the metrics, say every 30 minutes. * After a couple hours, let's see what we see. After that, we can decide whether to add more time or traffic to the test run.  ></body> </Action>
<Action id="57052" issue="37418" author="sykesm" type="comment" created="2019-02-14 19:50:12.0" updateauthor="sykesm" updated="2019-02-14 19:50:25.0"> <body><! CDATA Reviewed metrics from the run that was done last night. With the exception of a steady increase in the container working set of one orderer, everything is flat.  In the case of that one orderer with an increasing working set, there was no increase in heap, threads, go routines, or open file descriptors so it's not something that can be explained relative to those data points.  My understanding is that the test team is going to do a long run with request==limit==4GiB for the orderer containers to ensure things behave within the expected operational limits.  ></body> </Action>
<Action id="57054" issue="37418" author="yacovm" type="comment" created="2019-02-14 20:59:42.0" updateauthor="yacovm" updated="2019-02-14 20:59:42.0"> <body><! CDATA Thanks  ~sykesm  .  We got to see the usefulness of the 1.4 metrics work :)  ></body> </Action>
<Action id="57057" issue="37418" author="scottz" type="comment" created="2019-02-14 23:14:32.0" updateauthor="scottz" updated="2019-02-14 23:14:32.0"> <body><! CDATA Matt's comments explained our analysis, so we can agree to close this now. Thanks to everyone for their help with this.  If we see other memory problems while monitoring other tests, including a weeklong test (restricting ourselves to using lower traffic rate such as 15 tps, in order to avoid running out of disk space), we will open a new bug.  ></body> </Action>
<Action id="57058" issue="37418" author="scottz" type="comment" body="It looks like we were misled by some of the memory analysis tools; we learned quite a bit about them. It appears that there is no memory leak in the heap. (Refer also to Matt&apos;s comments.)" created="2019-02-14 23:17:27.0" updateauthor="scottz" updated="2019-02-14 23:17:27.0"/>
