<Issue id="37418" key="FAB-14054" number="14054" project="10002" reporter="scottz" creator="scottz" type="10004" summary="Memory leak in Raft OSNs" priority="3" resolution="10203" status="6" created="2019-02-04 14:22:56.0" updated="2019-02-23 21:08:54.0" resolutiondate="2019-02-14 23:17:27.0" votes="0" watches="5" workflowId="49021"> <description><! CDATA A long traffic run was started Feb 1 evening. It was unexpectedly observed that the memory utilization continually climbs. (Strangely enough, the growth is not even consistent among the orderers.)  We used images built Thursday night, so it DID include the streaming changes that were merged Thursday Jan 31.  We used prometheus to monitor the memory and disk usage. Refer to graphs attached from a 24-hour timespan, Sat am - Sun am. Disk utilization (count) Memory utilization (percent) Memory utilization (count)     Design team provided these instructions to enable the golang profiler before start test (using ORDERER_GENERAL_PROFILE):       https://github.com/hyperledger/fabric/blob/master/sampleconfig/orderer.yaml#L135-L136      For suspected memory leak, they suggest to collect during/after the test:       To see how many connections the orderer has          netstat -anp | grep orderer      Connect to orderer and dump the stacktrace goroutines into the output, to check for goroutine leak:          kill -SIGUSR1 1      An enumeration of the open file descriptors (the <pid> is the process ID)          cd /proc/<pid>; ls -lr fd | wc -l       Collect the logs too.  ></description> </Issue>
