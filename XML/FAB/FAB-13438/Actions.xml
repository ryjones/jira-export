<Action id="55149" issue="36393" author="guoger" type="comment" body="We need to enable `CheckQuorum` in etcdraft so that leader can voluntarily steps down to follower, so that disconnected orderer knows when to error out" created="2018-12-27 09:59:29.0" updateauthor="guoger" updated="2018-12-27 09:59:29.0"/>
<Action id="55150" issue="36393" author="yacovm" type="comment" body="We also want followers to return service unavailable to the peers/clients that connect to them, if they don&apos;t know who is the leader." created="2018-12-27 10:32:16.0" updateauthor="yacovm" updated="2018-12-27 10:32:16.0"/>
<Action id="55323" issue="36393" author="kchristidis" type="comment" body="Let me be the devil&apos;s advocate for a second: why should we drop the Deliver RPC if the cluster is temporarily leaderless? I can see the arguments for this going both ways?" created="2019-01-05 02:19:23.0" updateauthor="kchristidis" updated="2019-01-05 05:19:58.0"/>
<Action id="55324" issue="36393" author="yacovm" type="comment" created="2019-01-05 08:38:24.0" updateauthor="yacovm" updated="2019-01-05 08:38:24.0"> <body><! CDATA Temporary leaderless is fine, I agree. However - leader loss detection anyway comes with a delay of its own (since we don't have a callback from Raft.Node (or do we? Is there a Ready message saying we lost leadership?)), so I think that if leadership loss events distribute uniformly and we detect loss of leadership by probing the soft state once in an interval of T seconds, it means that on average - we lost leadership T/2 seconds ago, so we already have some temporal aspect here, and writing logic that does book-keeping of how much time we are leaderless isn't that cost-effective IMO.  Though, I think that having a "second chance" detection is not that complex - instead of refusing deliver when we detect leadership loss, we can refuse deliver only after we detect leadership loss in 2 consecutive probes. This ensures leadership was either lost, or was flaky (gained, lost, gained, and lost again, etc.) for a period of time that is at least T.  wdyt?  ></body> </Action>
<Action id="55392" issue="36393" author="kchristidis" type="comment" created="2019-01-07 22:41:37.0" updateauthor="kchristidis" updated="2019-01-07 22:41:37.0"> <body><! CDATA The lazy person in me says that this we should probably backlog this, i.e. for now a client may maintain a Deliver session with an OSN even if that OSN is unaware of a leader.   ~guoger ,  ~adarshsaraf123 : Your thoughts on this?  ></body> </Action>
<Action id="55479" issue="36393" author="guoger" type="comment" created="2019-01-10 07:02:13.0" updateauthor="guoger" updated="2019-01-10 07:02:13.0"> <body><! CDATA https://gerrit.hyperledger.org/r/c/28585 https://gerrit.hyperledger.org/r/c/28588 https://gerrit.hyperledger.org/r/c/28619  ></body> </Action>
<Action id="55678" issue="36393" author="guoger" type="comment" body="A side note, this makes a disconnected node non-readable/-writeable. IIRC, if a chain is disconnected from kafka, it&apos;s still readable. So in some sense, it breaks the contract there. cc  ~yacovm  ~kchristidis " created="2019-01-15 15:51:15.0" updateauthor="guoger" updated="2019-01-15 15:51:15.0"/>
<Action id="55680" issue="36393" author="yacovm" type="comment" created="2019-01-15 16:01:20.0" updateauthor="yacovm" updated="2019-01-15 16:01:20.0"> <body><! CDATA But we have code sections like this:  {code} 				switch kafkaErr.Err { 				case sarama.ErrOffsetOutOfRange: 					// the kafka consumer will auto retry for all errors except for ErrOffsetOutOfRange 					logger.Errorf(" channel: %s  Unrecoverable error during consumption: %s", chain.ChainID(), kafkaErr) close(chain.errorChan) {code}  ></body> </Action>
