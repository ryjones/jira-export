<Action id="69810" issue="40790" author="denyeart" type="comment" created="2020-07-27 03:01:21.0" updateauthor="denyeart" updated="2020-07-27 03:08:19.0"> <body><! CDATA As a refresher on how reconciler works... * In each reconciliation cycle, peer with missing private data will attempt to reconcile the last 10 blocks with missing private data (configurable with reconcileBatchSize) * It will attempt to pull from each of the available remote peers that are members of the collection, one at a time in random order ( see loop at https://github.com/hyperledger/fabric/blob/release-2.2/gossip/privdata/pull.go#L261 , note the same logic is used for regular block commit private data pulling and reconciler pulling, the only difference is that block commit attempts remote peers for pullRetryThreshold, while reconciliation attempts each remote peer once). * If none of the remote peers provides the data, reconciler will go back to sleep, with warning message "reconcile -> WARN 159 missing private data is not available on other peers" * If the missing data was successfully retrieved, the same reconciler cycle will attempt to reconcile the prior 10 blocks with missing private data. "reconcile -> INFO 177 Reconciliation cycle finished successfully. reconciled 1 private data keys from blocks range  21 - 21 "  If the reconcileSleepInterval is fairly long, having permanently missing private data is not much of a burden on the peer or network, since peer will only attempt one pull per remote peer per reconciliation cycle.  Any newer missing private data will get reconciled first, before attempting the older permanently missing private data, so the older permanently missing private data will not block progress.  The peer admin can at any time turn reconciler off with "reconciliationEnabled" configuration property.  I'm thinking the solution should be kept as simple as possible, perhaps a new peer configuration setting like "reconciliationAttempts" to go along with "reconciliationEnabled"  true.  If the peer can't reconcile private data for a given block range after N attempts, stop attempting.  If there is newer missing private data, the reconciler can attempt to retrieve the newer data N more times. This would be an in memory counter only, so that peer restart would reset the attempts so that peer can try again after restarting. This way if the missing private data is not available for a time, but then becomes available at a later time (e.g. a peer went offline for a period of time), it can still be reconciled later at the next peer restart, rather than forever lost.  This approach keeps things simple since there would be no changes to persisted data (e.g. no permanent retirement or tracking attempts across peer restarts).  ></body> </Action>
<Action id="69811" issue="40790" author="senthil1" type="comment" created="2020-07-27 10:36:11.0" updateauthor="senthil1" updated="2020-07-27 10:36:11.0"> <body><! CDATA * Instead of using new and old private data, we need to consider active and inactive data. The missing private data associated with block 10 can still be active while the current block height is 100k. * If all members are contacted successfully and no one is giving the requested data, can't we assume that the data is lost forever and remove those missing entries from the private data store? We could also move these entries to a separate store to keep track of missing active data. * If some members are unreachable and others are not giving the requested data, the retry mechanism without removing the missing entries makes sense. However, we need to have a certain limit before making them lost forever. * Assume that a peer has joined a new collection and the private data associated with the latest block N is missing forever (for some reason). The peer can never fetch any private data if we stop attempting reconciliation for block <= N.  ></body> </Action>
<Action id="69814" issue="40790" author="denyeart" type="comment" created="2020-07-27 17:53:13.0" updateauthor="denyeart" updated="2020-07-27 18:46:59.0"> <body><! CDATA _If all members are contacted successfully and no one is giving the requested data, can't we assume that the data is lost forever and remove those missing entries from the private data store?_  The issue is that a remote peer with the private data may simply be offline for a day. We don't want to automatically give up forever.  _Assume that a peer has joined a new collection and the private data associated with the latest block N is missing forever (for some reason). The peer can never fetch any private data if we stop attempting reconciliation for block <= N._  Good point. So we won't be able to keep it as simple as I hoped. Per our discussion we'll need to put the private data that can't yet be reconciled into a lower priority queue to try later, while the main reconciliation continues for the private data that hasn't been attempted yet.  ></body> </Action>
<Action id="69828" issue="40790" author="senthil1" type="comment" created="2020-07-29 15:23:14.0" updateauthor="senthil1" updated="2020-07-29 17:23:48.0"> <body><! CDATA *Background* # The reconciler periodically fetches _missingDataInfo_ associated with the most recent N blocks from the private data store. # For each _missingEntry_  blkNum, txNum, Ns, Coll , the reconciler fetches the collectionConfig and passes a map missingEntry collectionConfig to the private data puller. # The private data puller uses the collectionConfig to check which peers are members and which are not (peer signed data is verified against the membership policy defined in the collectionConfig to check the membership). Note that gossip maintains the current active peers in each channel. # The private data puller splits the active peers who are members of each collectionConfig into two sets: preferred and others. # The preferred peers are in the collection from the time the data was created. The other peers are also collection members but not at the time the data was created (low chances for them to have this missing data, probably, they are also running the reconciler). The priority is given to all preferred peers before moving to other peers. # Once peers per collectionConfig are collected, the private data puller first checks whether any member peer has already purged the missing data. Even if a single member has purged the data, the puller adds the missing data to PurgedElements and does not try to fetch it from any peers. # For each missing data, the puller assigns a peer (as per the order in the priority list) and try to fetch it. # Once the puller assigns a peer per missing data, it sends the request to peers and waits for the response. Whichever missing data did not receive a response, the puller retries with the remaining peer members until no more eligible peers or no more missing data. # If no peer in the network is a member of the collection associated with the missing data, puller skips the missing entry. # Finally, the puller returns _fetchedData_ and _purgedData_ to the reconciler. The reconciler call the ledger to store only the fetchedData.**  *Reconciler will be able to provide the following information to the ledger:* # fetched data # purged data by any single member — security issue to trust any single peer? # still missing data because no member is active — can happen due to network partitioning or members left the channel? # still missing data though all members are active — very rare. Can happen when all peers are acting as malicious.  To tag still missing data with either (3) or (4), we need to make changes to private data puller which does not look like a cakewalk.  *What the ledger can do with the above information:* # Fetched data would be used to fill the missing entries # Purged data can be moved to a separate namespace called `to-be-purged-missing-data` — no need to process them but delete it as per the purge schedule (if the block height of the peer is too behind). # If some data could not be fetched because all or some members are unreachable, we can move those data to a bucket and do exponential back-off. # If some data could not be fetched though all members are active, we can still move those data to a bucket and do exponential back-off assuming that not all peers per organization are active.  *To simplify the solution, we can make the reconciler pass only the following information without any tags:* # fetched data # purged data # still missing data  *Solution Approach* # Three namespaces at the pvtdata store – _missing,_ _tried, and to-be-purged_ (just tentative names). # The reconciler will pass both fetchedData and toBePurgedData. # The PvtDataStore will keep the list of missing data given to the reconciler so that it can find out which data are still missing. # The still-missing pvtdata will be moved to the _tried_ namespace. # The fetchedData will be used to fill the missing data and toBePrugedData will be moved to the _to-be-purged namespace_. # For still missing data, we can use a cache library that supports time to live and a callback on eviction. ## We can move these still missing data from missing namespace to _tried_. ## We can store these still missing data in the cache and set a time to live. ## When the time to live expires for an entry, the callback function would be called which will move the the entry from _tried_ to the missing namespace. ## To enable exponential back-off, we will store the entry with value as the number of retries performed so far in the same cache (with no time to live). Using this value, we will set actual time to live after the retry.  *An Alternative Solution Approach Proposed By*  ~manish-sethi   **  # The First 5 steps are still the same (some possible changes to _to-be-purged_ namespace) # For every N number of iterations of reconciliation, the pvtdata will give an opportunity to missing data entries present in the _tried_ namespace. # If the data are still missing, the priority of those entries in the _tried_ namespace will be reduced. ## In order to assign priorities, we will have multiple _tried_ namespaces each prefixed with a number. ## The lower number denotes a high priority. Hence, to lower the priority of certain missing entries, the entries will be moved from one _tried-#_ namespace to another. ## If there is no such namespace with such a priority, a new namespace will be created. ## The priority of missing entries will be lowered after every retry.  Comparison: | |*Approach 1*|*Approach 2*| |Disk Writes|Low|Low to High| |Memory Usage|Low to High|Low| |Time-Based|Yes|Approximately| |On Peer Failure|Knowledge is Lost|Knowledge is Preserved| |Dependency|Caching Library|None| |Coding Complexity|?|?|     As Approach 2 has better advantages over approach 1, we will go with approach 2.  ></body> </Action>
<Action id="69829" issue="40790" author="denyeart" type="comment" created="2020-07-29 17:32:31.0" updateauthor="denyeart" updated="2020-07-29 19:17:28.0"> <body><! CDATA I agree with Approach 2. To keep things simple, the first implementation will have a single _tried_ namespace. This will address the most critical issue of permanently missing private data from blocking other private data reconciliation.  It should also be possible to configure the frequency or altogether disable the processing of the _tried_ items.  ></body> </Action>
<Action id="70120" issue="40790" author="manish-sethi" type="comment" body="https://github.com/hyperledger/fabric/pull/1721" created="2020-09-01 17:36:11.0" updateauthor="manish-sethi" updated="2020-09-01 17:36:11.0"/>
