<Issue id="24800" key="FAB-7330" number="7330" project="10002" reporter="kchristidis" assignee="sanchezl" creator="kchristidis" type="10002" summary="Kafka/ZK nodes should be shutdown gracefully and in a rolling fashion during upgrade" priority="3" resolution="10000" status="6" created="2017-12-05 22:29:10.0" updated="2018-07-20 14:15:11.0" resolutiondate="2018-01-12 21:59:10.0" votes="0" watches="8" workflowId="40734"> <description><! CDATA *Definitions*  Assume that _*f*_ is the number of failures (broker crashes) that your Kafka cluster can tolerate.  *Problem*  If more than _*f*_ brokers are shutdown abruptly at around the same time, data that was acknowledged as received and committed by the Kafka cluster may be lost and resumption of normal operation may be hindered.  *Explanation*  Kafka by default does not flush data to disk continuously for performance reasons ( source|http://www.allprogrammingtutorials.com/tutorials/configuring-log-flush-interval-in-kafka.php ). This includes both  received messages|https://kafka.apache.org/documentation/#impl_writes  and metadata such as the high watermark (see: {{replica.high.watermark.checkpoint.interval.ms}} configuration setting). From  an older Kafka replication doc|https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication#KafkaReplication-Writes : {quote}"There is no guarantee that any replica has persisted the commit message to disks though. Given that correlated failures are relatively rare, this approach gives us a good balance between response time and durability." {quote} An abrupt, i.e. non-graceful, shutdown of a broker then constitutes a failure.  Therefore, during an upgrade, we should not be terminating the brokers abruptly — we currently do a {{docker stop}} on the Kafka containers which  translates to a SIGTERM/SIGKILL|https://docs.docker.com/engine/reference/commandline/stop/  on the Kafka process. We should also not be terminating more than _*f*_ brokers at a time.  When we do that, we may come up with a situation similar to what  ~jeffgarratt  observed in one of his test runs. I am attaching the logs of this run here for future reference.  Here's what we observe in the logs: # Before the upgrade, channel1 was assigned to replicas 2, 3, and 0. 13 messages were committed to the channel's partition. # After the upgrade, replica 2 is assigned as the leader of the channel1 partition. _Its *committed* high watermark (HW) is 0._ # Replicas 0 and 3 have a log end offset (leo) _and_ a HW of 12 for the channel1 partition. The former is documented in the logs ({{ 2017-12-04 18:25:30,534  INFO Completed load of log com.acme.blockchain.jdoe.channel1-0 with 1 log segments and log end offset 12 in 32 ms (kafka.log.Log)}}), while the latter can be inferred by the log printouts ({{ 2017-12-04 18:26:36,244  INFO Truncating log com.acme.blockchain.jdoe.channel1-0 to offset 12. (kafka.log.Log)}}) and our knowledge of the fact that a replica truncates its log to its checkpointed HW ( source|https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Replication  – see "Leader failure", bullet point 3a). # Replicas 0 and 3 now issue a {{ReplicaFetchRequest}} ( source|https://cwiki.apache.org/confluence/display/KAFKA/Kafka+replication+detailed+design+V2 ) to the channel1 leader (replica 2) in order to catch up. This request starts from offset 12. But replica 2 does not have offset 12 in its log segment. Replicas 0 and 3 treat this as a fatal error ({{ 2017-12-04 18:26:36,937  FATAL  ReplicaFetcherThread-0-2 , Exiting because log truncation is not allowed for partition com.acme.blockchain.jdoe.channel1-0, Current leader 2's latest offset 0 is less than replica 3's latest offset 12 (kafka.server.ReplicaFetcherThread)}}) and exit. # We now have a partition that is both read-only (because there are not enough live in-sync replicas) and has suffered data loss.  Here's an explanation for the above: # Note that the default values for {{log.flush.interval.messages}}, {{log.flush.interval.ms}}, and {{log.flush.scheduler.interval.ms}} practically disable an explicit fsync and instant rely on the background flush done by the OS ( source|https://community.hortonworks.com/articles/80813/kafka-best-practices-1.html ). This makes us prone to losing committed messages during an abrupt restart. Also note that the default value for {{replica.high.watermark.checkpoint.interval.ms}} is set to 5 seconds. This is the frequency with which the HW is saved out to disk. This means there is a significant chance of a not persisting the most recent HW to disk. # 13 messages were posted to the channel1 successfully before the upgrade. # Replicas 0 and 3 persisted that info to disk. (Technically, replica 3 didn't do so cleanly, but it managed to retrieve that data w/o issues upon restarting, see {{ 2017-12-04 18:25:30,511  WARN Found a corrupted index file due to requirement failed: Corrupt index found, index file (/var/hyperledger/bddtests/volumes/kafka/fe661a26d91f11e7863e02d158fa0198/kafka3/filestore/com.acme.blockchain.jdoe.channel1-0/00000000000000000000.index) has non-zero size but the last offset is 0 which is no larger than the base offset 0.}. deleting /var/hyperledger/bddtests/volumes/kafka/fe661a26d91f11e7863e02d158fa0198/kafka3/filestore/com.acme.blockchain.jdoe.channel1-0/00000000000000000000.timeindex, /var/hyperledger/bddtests/volumes/kafka/fe661a26d91f11e7863e02d158fa0198/kafka3/filestore/com.acme.blockchain.jdoe.channel1-0/00000000000000000000.index and rebuilding index... (kafka.log.Log)}} message in its log.) # Replica 2 was shut down before it could persist the high watermark or the messages to disk. # As far as the Kafka cluster is concerned, replica 2 was part of the ISR set for channel1 when we stopped its container. Given that  the number of remaining live replicas|https://github.com/jeffgarratt/fabric-prototype/blob/master/features/upgrade.feature#L420  is less than  the minimum ISR requirement for channel1|https://github.com/jeffgarratt/fabric-prototype/blob/master/dc-orderer-kafka.yml#L78 , channel1 becomes read-only when replica 2 goes down, and replica 2 remains at the ISR set. # When the Kafka/ZK cluster is upgraded and rebooted, replica 2 is the first one to complete loading its log for channel1, since it's practically empty. It is therefore the first one among the brokers in the assigned replica (AR) set of channel1 to go live. Given that it is also the preferred replica for channel1 (recall that: {{ 2017-12-04 18:26:35,750  INFO Topic creation \{"version":1,"partitions":\{"0": 2,3,0 }} (kafka.admin.AdminUtils$)}}), it is assigned as the leader of channel1 post-upgrade. This results in data loss.  *Solution*  For the purposes of demonstrating upgrade we should ignore any tweaking of flushing-related settings. Our main objective here should be to just shut down up to _*f*_ brokers at a time, and do so gracefully. The latter part is technically optional ( source|https://kafka.apache.org/documentation/#basic_ops_restarting : "Syncing the logs will happen automatically whenever the server is stopped other than by a hard kill") but is the correct practice during an upgrade — refer to the "Graceful shutdown"  section on the Kafka documentation website|https://kafka.apache.org/documentation/#basic_ops_restarting , and on the "Implementing a graceful shutdown" section of the  Apache Kafka Cookbook|https://www.safaribooksonline.com/library/view/apache-kafka-cookbook/9781785882449/ch06s04.html .  With that in mind, we should be doing the following: # Ensure that {{controlled.shutdown.enable}} is set to true. This is the default. # Execute the {{kafka-server-stop.sh}} script provided by Kafka on replica 0. # Confirm the replica has shut down. # Upgrade replica to latest Kafka binary. # Bring up replica. # Ensure that replica is part of the ISR set again. (How? We want the cardinality of the ISR set to be equal to the replication factor so we _could_ use  the kafka-utils Python package|http://kafka-utils.readthedocs.io/en/latest/kafka_check.html#checking-in-sync-replicas : {{$kafka-check --cluster-type=foo min_isr --default_min_isr 3}} where 3 is our replication factor. Is there an easier / more scriptable way of getting this data though?) # Repeat steps 2 to 6 for replicas 1 to 3.  We should be adopting a similar strategy for the ZooKeeper ensemble: # Execute the {{zkServer.sh}} script on ZK node 1: {{zkServer.sh stop}} # Confirm the node has shut down. # Upgrade node to latest ZooKeeper binary. # Bring up node. # Ensure it is part of the ensemble. (How? Perhaps via the {{stat}} command, and checking whether the mode is "leader" or "follower"? Is there an easier / more scriptable way of going at it?) # Repeat step 1 to 5 for nodes 2 and 3.  ></description> </Issue>
