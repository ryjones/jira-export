<Action id="60430" issue="40173" author="vampire203" type="comment" body="hey Nikolay, we also have the similar issue and reported on FAB-15557 It may be different to your case, but see if you can find any similarities on the problem" created="2019-05-28 11:46:31.0" updateauthor="vampire203" updated="2019-05-28 11:46:31.0"/>
<Action id="60548" issue="40173" author="nn77" type="comment" created="2019-05-31 08:40:48.0" updateauthor="nn77" updated="2019-05-31 08:40:48.0"> <body><! CDATA Hello, Sam! I will look through your case!  {color:#FF0000}Some additional information!{color}  I have mentioned that restart of offline peer pod helps. But there is one case - this pod starts working after THREE restarts.   Working peer starts working immediately after restart.     I will collect logs when problem repeats.     ></body> </Action>
<Action id="60582" issue="40173" author="nn77" type="comment" created="2019-06-03 07:51:20.0" updateauthor="nn77" updated="2019-06-03 07:51:20.0"> <body><! CDATA {noformat} *no* further _formatting_ is done here{noformat} So. When I delete failed peer pod:  It tries to start two times with following error:    {code:java} 2019-06-03 07:29:48.367 UTC  nodeCmd  serve -> INFO 001 Starting peer: Version: 1.4.0 Commit SHA: d700b43 Go version: go1.11.1 OS/Arch: linux/amd64 Chaincode: Base Image Version: 0.4.14 Base Docker Namespace: hyperledger Base Docker Label: org.hyperledger.fabric Docker Namespace: hyperledger 2019-06-03 07:29:48.368 UTC  ledgermgmt  initialize -> INFO 002 Initializing ledger mgmt 2019-06-03 07:29:48.368 UTC  kvledger  NewProvider -> INFO 003 Initializing ledger provider panic: Error opening leveldb: resource temporarily unavailable goroutine 1  running : github.com/hyperledger/fabric/common/ledger/util/leveldbhelper.(*DB).Open(0xc00005a980) /opt/gopath/src/github.com/hyperledger/fabric/common/ledger/util/leveldbhelper/leveldb_helper.go:79 +0x271 github.com/hyperledger/fabric/core/ledger/kvledger.openIDStore(0xc000196f40, 0x36, 0x1) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/kv_ledger_provider.go:267 +0x10e github.com/hyperledger/fabric/core/ledger/kvledger.NewProvider(0x10b0200, 0x1f51e20, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/kv_ledger_provider.go:60 +0x77 github.com/hyperledger/fabric/core/ledger/ledgermgmt.initialize(0xc00005a840) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:68 +0x304 github.com/hyperledger/fabric/core/ledger/ledgermgmt.Initialize.func1() /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:52 +0x2a sync.(*Once).Do(0x1f51ef8, 0xc0002b7520) /opt/go/src/sync/once.go:44 +0xb3 github.com/hyperledger/fabric/core/ledger/ledgermgmt.Initialize(0xc00005a840) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:51 +0x55 github.com/hyperledger/fabric/peer/node.serve(0x1f51e20, 0x0, 0x0, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/peer/node/start.go:172 +0x56f github.com/hyperledger/fabric/peer/node.glob..func1(0x1e3fbe0, 0x1f51e20, 0x0, 0x0, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/peer/node/start.go:119 +0x9c github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).execute(0x1e3fbe0, 0x1f51e20, 0x0, 0x0, 0x1e3fbe0, 0x1f51e20) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:762 +0x473 github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x1e40300, 0x8, 0x0, 0x1e3f4c0) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:852 +0x2fd github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).Execute(0x1e40300, 0xc0004d9f40, 0x1) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:800 +0x2b main.main() /opt/gopath/src/github.com/hyperledger/fabric/peer/main.go:53 +0x2f7 {code} Third time it starts with following logs:          {code:java} 2019-06-03 07:30:17.376 UTC  nodeCmd  serve -> INFO 001 Starting peer: Version: 1.4.0 Commit SHA: d700b43 Go version: go1.11.1 OS/Arch: linux/amd64 Chaincode: Base Image Version: 0.4.14 Base Docker Namespace: hyperledger Base Docker Label: org.hyperledger.fabric Docker Namespace: hyperledger 2019-06-03 07:30:17.378 UTC  ledgermgmt  initialize -> INFO 002 Initializing ledger mgmt 2019-06-03 07:30:17.378 UTC  kvledger  NewProvider -> INFO 003 Initializing ledger provider 2019-06-03 07:30:17.574 UTC  kvledger  NewProvider -> INFO 004 ledger provider Initialized 2019-06-03 07:30:17.735 UTC  ledgermgmt  initialize -> INFO 005 ledger mgmt initialized 2019-06-03 07:30:17.735 UTC  peer  func1 -> INFO 006 Auto-detected peer address: 10.129.1.67:7051 2019-06-03 07:30:17.735 UTC  peer  func1 -> INFO 007 Auto-detect flag is set, returning 10.129.1.67:7051 2019-06-03 07:30:17.735 UTC  peer  func1 -> INFO 008 Auto-detected peer address: 10.129.1.67:7051 2019-06-03 07:30:17.735 UTC  peer  func1 -> INFO 009 Auto-detect flag is set, returning 10.129.1.67:7051 2019-06-03 07:30:17.737 UTC  nodeCmd  computeChaincodeEndpoint -> INFO 00a Entering computeChaincodeEndpoint with peerHostname: 10.129.1.67 2019-06-03 07:30:17.737 UTC  nodeCmd  computeChaincodeEndpoint -> INFO 00b Exit with ccEndpoint: 10.129.1.67:7052 2019-06-03 07:30:17.737 UTC  nodeCmd  createChaincodeServer -> WARN 00c peer.chaincodeListenAddress is not set, using 10.129.1.67:7052 2019-06-03 07:30:17.738 UTC  sccapi  registerSysCC -> INFO 00d system chaincode lscc(github.com/hyperledger/fabric/core/scc/lscc) registered 2019-06-03 07:30:17.738 UTC  sccapi  registerSysCC -> INFO 00e system chaincode cscc(github.com/hyperledger/fabric/core/scc/cscc) registered 2019-06-03 07:30:17.738 UTC  sccapi  registerSysCC -> INFO 00f system chaincode qscc(github.com/hyperledger/fabric/core/scc/qscc) registered 2019-06-03 07:30:17.738 UTC  sccapi  registerSysCC -> INFO 010 system chaincode (+lifecycle,github.com/hyperledger/fabric/core/chaincode/lifecycle,true) disabled 2019-06-03 07:30:17.740 UTC  gossip.service  func1 -> INFO 011 Initialize gossip with endpoint peer0-bch-org1:7051 and bootstrap set  peer0-bch-org1:7051  2019-06-03 07:30:17.743 UTC  gossip.gossip  NewGossipService -> INFO 012 Creating gossip service with self membership of Endpoint: peer0-bch-org1:7051, InternalEndpoint: peer0-bch-org1:7051, PKI-ID: 52e1b00e2a43eadcb583ae4fa0a79f79b5ad597ecfe5b5a507d728bc89a619b0, Metadata: 2019-06-03 07:30:17.744 UTC  gossip.gossip  start -> INFO 013 Gossip instance peer0-bch-org1:7051 started 2019-06-03 07:30:17.745 UTC  sccapi  deploySysCC -> INFO 014 system chaincode lscc/(github.com/hyperledger/fabric/core/scc/lscc) deployed 2019-06-03 07:30:17.745 UTC  cscc  Init -> INFO 015 Init CSCC 2019-06-03 07:30:17.745 UTC  sccapi  deploySysCC -> INFO 016 system chaincode cscc/(github.com/hyperledger/fabric/core/scc/cscc) deployed 2019-06-03 07:30:17.745 UTC  qscc  Init -> INFO 017 Init QSCC 2019-06-03 07:30:17.745 UTC  sccapi  deploySysCC -> INFO 018 system chaincode qscc/(github.com/hyperledger/fabric/core/scc/qscc) deployed 2019-06-03 07:30:17.745 UTC  sccapi  deploySysCC -> INFO 019 system chaincode (+lifecycle,github.com/hyperledger/fabric/core/chaincode/lifecycle) disabled 2019-06-03 07:30:17.746 UTC  nodeCmd  serve -> INFO 01a Deployed system chaincodes 2019-06-03 07:30:17.752 UTC  peer  Initialize -> INFO 01b Loading chain my_channel 2019-06-03 07:30:17.752 UTC  ledgermgmt  OpenLedger -> INFO 01c Opening ledger with id = my_channel 2019-06-03 07:30:17.796 UTC  ledgermgmt  OpenLedger -> INFO 01d Opened ledger with id = my_channel 2019-06-03 07:30:17.858 UTC  gossip.gossip  JoinChan -> INFO 01e Joining gossip network of channel my_channel with 16 organizations 2019-06-03 07:30:17.858 UTC  gossip.gossip  learnAnchorPeers -> INFO 01f Learning about the configured anchor peers of bch2MSP for channel my_channel :  {peer0-bch-org2 7051}   ......... 2019-06-03 07:30:17.859 UTC  gossip.gossip  learnAnchorPeers -> INFO 02a Learning about the configured anchor peers of bch1MSP for channel my_channel :  {peer0-bch-org1 7051}   ........... 2019-06-03 07:30:17.885 UTC  gossip.comm  authenticateRemotePeer -> ERRO 030 Failed verifying signature from 172.30.247.38:7051 : Could not acquire policy manager for channel my_channel 2019-06-03 07:30:17.885 UTC  gossip.comm  Handshake -> WARN 032 Authentication failed: Could not acquire policy manager for channel my_channel 2019-06-03 07:30:17.885 UTC  gossip.gossip  func1 -> WARN 033 Deep probe of peer0-bch-org2:7051 failed: Could not acquire policy manager for channel my_channel github.com/hyperledger/fabric/gossip/gossip.(*gossipServiceImpl).learnAnchorPeers.func1 /opt/gopath/src/github.com/hyperledger/fabric/gossip/gossip/gossip_impl.go:249 github.com/hyperledger/fabric/gossip/discovery.(*gossipDiscoveryImpl).Connect.func1 /opt/gopath/src/github.com/hyperledger/fabric/gossip/discovery/discovery_impl.go:160 runtime.goexit /opt/go/src/runtime/asm_amd64.s:1333 2019-06-03 07:30:17.885 UTC  gossip.discovery  func1 -> WARN 034 Could not connect to Endpoint: peer0-bch-org2:7051, InternalEndpoint: peer0-bch-org2:7051, PKI-ID: <nil>, Metadata: : Could not acquire policy manager for channel my_channel   ...............   2019-06-03 07:30:17.931 UTC  gossip.state  NewGossipStateProvider -> INFO 06c Updating metadata information, current ledger sequence is at = 198, next expected block is = 199 2019-06-03 07:30:17.931 UTC  sccapi  deploySysCC -> INFO 06d system chaincode lscc/my_channel(github.com/hyperledger/fabric/core/scc/lscc) deployed 2019-06-03 07:30:17.931 UTC  cscc  Init -> INFO 06e Init CSCC 2019-06-03 07:30:17.931 UTC  sccapi  deploySysCC -> INFO 06f system chaincode cscc/my_channel(github.com/hyperledger/fabric/core/scc/cscc) deployed 2019-06-03 07:30:17.932 UTC  qscc  Init -> INFO 070 Init QSCC 2019-06-03 07:30:17.932 UTC  sccapi  deploySysCC -> INFO 071 system chaincode qscc/my_channel(github.com/hyperledger/fabric/core/scc/qscc) deployed 2019-06-03 07:30:17.932 UTC  sccapi  deploySysCC -> INFO 072 system chaincode (+lifecycle,github.com/hyperledger/fabric/core/chaincode/lifecycle) disabled 2019-06-03 07:30:17.956 UTC  discovery  NewService -> INFO 073 Created with config TLS: false, authCacheMaxSize: 1000, authCachePurgeRatio: 0.750000 2019-06-03 07:30:17.956 UTC  nodeCmd  registerDiscoveryService -> INFO 074 Discovery service activated 2019-06-03 07:30:17.957 UTC  nodeCmd  serve -> INFO 075 Starting peer with ID= name:"peer0-bch-org1"  , network ID= dev , address= 10.129.1.67:7051  2019-06-03 07:30:17.957 UTC  nodeCmd  serve -> INFO 076 Started peer with ID= name:"peer0-bch-org1"  , network ID= dev , address= 10.129.1.67:7051  2019-06-03 07:30:17.957 UTC  nodeCmd  func8 -> INFO 077 Starting profiling server with listenAddress = 0.0.0.0:6060 2019-06-03 07:30:21.803 UTC  comm.grpc.server  1 -> INFO 078 unary call completed {"grpc.start_time": "2019-06-03T07:30:21.803Z", "grpc.service": "gossip.Gossip", "grpc.method": "Ping", "grpc.request_deadline": "2019-06-03T07:30:23.803Z", "grpc.peer_address": "10.129.3.91:39768", "grpc.code": "OK", "grpc.call_duration": "74.164µs"} 2019-06-03 07:30:22.858 UTC  gossip.channel  reportMembershipChanges -> INFO 079 Membership view has changed. peers went online:   peer0-bch-org3:7051    , current view:   peer0-bch-org3:7051    2019-06-03 07:30:23.932 UTC  gossip.election  beLeader -> INFO 07a 52e1b00e2a43eadcb583ae4fa0a79f79b5ad597ecfe5b5a507d728bc89a619b0 : Becoming a leader 2019-06-03 07:30:23.932 UTC  gossip.service  func1 -> INFO 07b Elected as a leader, starting delivery service for channel my_channel ................  and all peers are online {code}       ></body> </Action>
<Action id="60591" issue="40173" author="yacovm" type="comment" body=" ~C0rWin   ~naaman  " created="2019-06-03 12:58:24.0" updateauthor="yacovm" updated="2019-06-03 12:58:24.0"/>
<Action id="60606" issue="40173" author="nyet" type="comment" created="2019-06-03 16:00:41.0" updateauthor="nyet" updated="2019-06-03 16:09:33.0"> <body><! CDATA {code:go} if dbInst.db, err = leveldb.OpenFile(dbPath, dbOpts); err != nil { panic(fmt.Sprintf("Error opening leveldb: %s", err)) } {code}  If OpenFile() is returning EAGAIN, it likely means dbPath is a non-socket that can't be used, possibly due to open file limits or resource starvation. It could be  1) your instance is too small 2) your file descriptor limits have to be increased 3) fabric has a file descriptor leak  ></body> </Action>
<Action id="60635" issue="40173" author="nn77" type="comment" created="2019-06-04 07:20:14.0" updateauthor="nn77" updated="2019-06-04 07:20:14.0"> <body><! CDATA Hello!  1) There is plenty of resources in cluster, so I can exclude starvation of resources.  2) I will try to check this(when problem repeats), because persistent data remains of NFS. And limit can be set on nfs server.     But these two suggestions do not explain why peers start working on third start. And why other services which have persistent data on same NFS work flawlessly...  ></body> </Action>
<Action id="60734" issue="40173" author="nn77" type="comment" created="2019-06-06 14:45:00.0" updateauthor="nn77" updated="2019-06-06 14:45:00.0"> <body><! CDATA update!  checked ulimits on failed pod:  cat /proc/80190/limts | grep "open files"  Max open files 1048576 1048576  ls -1 /proc/80190/fd | wc -l 3     So I guess fd limits are okay..     ></body> </Action>
<Action id="60967" issue="40173" author="naaman" type="comment" created="2019-06-13 14:42:29.0" updateauthor="naaman" updated="2019-06-13 14:42:29.0"> <body><! CDATA {color:#242729}An identical issue has been resolved by  ~mastersingh24  see {color}  {color:#242729} https://stackoverflow.com/questions/54479043/why-do-i-get-a-resource-unavailable-error-on-db-while-launching-peer-pod {color}     {color:#242729}That error typically occurs when the peer cannot get a lock on DB files. Make sure that the peers are not mounting the same shared volume.{color}  {color:#242729}Please check the volumes you mount and indicate if that solved the issue.{color}  ></body> </Action>
<Action id="60970" issue="40173" author="nn77" type="comment" created="2019-06-13 14:51:30.0" updateauthor="nn77" updated="2019-06-13 14:51:30.0"> <body><! CDATA I have separate volume for each peer.  And if I had same volumes for two peers - then one peer could not start permanently because of second peer's lock.  But problem  peers start on third time always  ></body> </Action>
<Action id="60998" issue="40173" author="naaman" type="comment" created="2019-06-14 02:21:38.0" updateauthor="naaman" updated="2019-06-14 02:21:38.0"> <body><! CDATA The reason for this is the NFS lease time (lease-time in the nfs config) which is 30 seconds by default. The lock is not released by the NFS server until the lease time expires. The logs show that the failed attempt was at 07:29:48 and the successful attempt at 07:30:17 which confirms this. It is possible to reduce the lease-time (10 sec is the minimum) but I would NOT recommend this.  We can modify the code to wait keep attempting to lock the leveldb file for 30+ seconds and only then panic. This will help when the problem is the lease time but means that in cases where there is real contention on the lock the failure will be delayed (by 30 sec or so).  ~mastersingh24  wdyt?  ></body> </Action>
<Action id="61001" issue="40173" author="nn77" type="comment" created="2019-06-14 05:28:14.0" updateauthor="nn77" updated="2019-06-14 05:28:14.0"> <body><! CDATA Well, maybe. Still hlf network consists of 14 peers. And just one or two go offline and other continue working. And they are on same NFS (every peer has own directory). Why does just one peer stop? If there is lock in levelDB why there is no error when peer was running?  Beginning of working on third start is a second part of the problem.. I think so.  ></body> </Action>
<Action id="61024" issue="40173" author="nyet" type="comment" created="2019-06-14 18:43:14.0" updateauthor="nyet" updated="2019-06-14 18:43:14.0"> <body><! CDATA Why would NFS locking (or lease time) matter if no peer shares any files with any other peer?  Unless a peer is dying while holding a lock on an NFS located file, and a subsequent newly launched replacement peer can't access the same NFS located file due the lock being stale.  Am I missing something?  BTW another reason to distrust containers. Don't just kill them. The underlying processes might not exit cleanly.  ></body> </Action>
<Action id="61046" issue="40173" author="naaman" type="comment" created="2019-06-17 07:35:04.0" updateauthor="naaman" updated="2019-06-17 07:35:04.0"> <body><! CDATA The lease time explains why the peer was only able to restart on the third attempt (the lock was not yet released in the first two attempts). It doesn't explain why the peer lost the ability to connect to all other peers and thus "went offline".   ~NN77  can you provide more detailed logs (higher log level) from an offline peer and an online peer. I want to verify that the peer is attempting to connect to the correct endpoints (DNS names are resolved to the correct address) and if so try to figure out what prevents the connection from being established.  ></body> </Action>
<Action id="61048" issue="40173" author="nn77" type="comment" created="2019-06-17 07:47:16.0" updateauthor="nn77" updated="2019-06-17 07:47:16.0"> <body><! CDATA Hello!  I have following environment variables set: {code:java} - name: CORE_LOGGING_PEER value: debug - name: CORE_CHAINCODE_LOGGING_LEVEL value: debug - name: CORE_CHAINCODE_LOGGING_SHIM value: debug - name: SHIM_LOGGING_LEVEL value: debug - name: CC_LOGGING_LEVEL value: debug{code}    and this  {code:java} - name: FABRIC_LOGGING_SPEC value: INFO{code} So I change last one to debug and  {code:java} - name: FABRIC_LOGGING_LEVEL value: DEBUG{code} right? Or shall I add anything more?    ---- I checked DNS when peer went offline, it did work as it should....     And one more: In saturday one of peers went offline, same symptoms, same logs. But it had been working for 71 days!  ></body> </Action>
<Action id="61077" issue="40173" author="naaman" type="comment" created="2019-06-18 12:21:15.0" updateauthor="naaman" updated="2019-06-18 14:08:50.0"> <body><! CDATA Yes, setting FABRIC_LOGGING_SPEC to DEBUG should be fine  If the log produced is too large then we may want to restrict debug only to the gossip component with FABRIC_LOGGING_SPEC="gossip=DEBUG"     Use FABRIC_LOGGING_SPEC not FABRIC_LOGGING_LEVEL  ></body> </Action>
<Action id="61307" issue="40173" author="nn77" type="comment" created="2019-06-27 13:45:40.0" updateauthor="nn77" updated="2019-06-27 14:41:42.0"> <body><! CDATA Hello. It happened again, with new peer that has uptime of 83 days. It has new behaviour - *It is unable to start completely*  here are logs: {code:java} 2019-06-27 13:36:34.108 UTC  viperutil  getKeysRecursively -> DEBU 001 Found map string interface{} value for peer.BCCSP 2019-06-27 13:36:34.108 UTC  viperutil  unmarshalJSON -> DEBU 002 Unmarshal JSON: value cannot be unmarshalled: invalid character 'S' looking for beginning of value 2019-06-27 13:36:34.108 UTC  viperutil  getKeysRecursively -> DEBU 003 Found real value for peer.BCCSP.Default setting to string SW 2019-06-27 13:36:34.108 UTC  viperutil  getKeysRecursively -> DEBU 004 Found map string interface{} value for peer.BCCSP.SW 2019-06-27 13:36:34.108 UTC  viperutil  unmarshalJSON -> DEBU 005 Unmarshal JSON: value cannot be unmarshalled: invalid character 'S' looking for beginning of value 2019-06-27 13:36:34.108 UTC  viperutil  getKeysRecursively -> DEBU 006 Found real value for peer.BCCSP.SW.Hash setting to string SHA2 2019-06-27 13:36:34.109 UTC  viperutil  unmarshalJSON -> DEBU 007 Unmarshal JSON: value is not a string: 256 2019-06-27 13:36:34.109 UTC  viperutil  getKeysRecursively -> DEBU 008 Found real value for peer.BCCSP.SW.Security setting to int 256 2019-06-27 13:36:34.109 UTC  viperutil  getKeysRecursively -> DEBU 009 Found map string interface{} value for peer.BCCSP.SW.FileKeyStore 2019-06-27 13:36:34.109 UTC  viperutil  unmarshalJSON -> DEBU 00a Unmarshal JSON: value cannot be unmarshalled: unexpected end of JSON input 2019-06-27 13:36:34.109 UTC  viperutil  getKeysRecursively -> DEBU 00b Found real value for peer.BCCSP.SW.FileKeyStore.KeyStore setting to string 2019-06-27 13:36:34.109 UTC  viperutil  getKeysRecursively -> DEBU 00c Found map string interface{} value for peer.BCCSP.PKCS11 2019-06-27 13:36:34.109 UTC  viperutil  unmarshalJSON -> DEBU 00d Unmarshal JSON: value is not a string: <nil> 2019-06-27 13:36:34.109 UTC  viperutil  getKeysRecursively -> DEBU 00e Found real value for peer.BCCSP.PKCS11.Pin setting to <nil> <nil> 2019-06-27 13:36:34.109 UTC  viperutil  unmarshalJSON -> DEBU 00f Unmarshal JSON: value is not a string: <nil> 2019-06-27 13:36:34.109 UTC  viperutil  getKeysRecursively -> DEBU 010 Found real value for peer.BCCSP.PKCS11.Hash setting to <nil> <nil> 2019-06-27 13:36:34.109 UTC  viperutil  unmarshalJSON -> DEBU 011 Unmarshal JSON: value is not a string: <nil> 2019-06-27 13:36:34.110 UTC  viperutil  getKeysRecursively -> DEBU 012 Found real value for peer.BCCSP.PKCS11.Security setting to <nil> <nil> 2019-06-27 13:36:34.110 UTC  viperutil  getKeysRecursively -> DEBU 013 Found map string interface{} value for peer.BCCSP.PKCS11.FileKeyStore 2019-06-27 13:36:34.110 UTC  viperutil  unmarshalJSON -> DEBU 014 Unmarshal JSON: value is not a string: <nil> 2019-06-27 13:36:34.110 UTC  viperutil  getKeysRecursively -> DEBU 015 Found real value for peer.BCCSP.PKCS11.FileKeyStore.KeyStore setting to <nil> <nil> 2019-06-27 13:36:34.110 UTC  viperutil  unmarshalJSON -> DEBU 016 Unmarshal JSON: value is not a string: <nil> 2019-06-27 13:36:34.110 UTC  viperutil  getKeysRecursively -> DEBU 017 Found real value for peer.BCCSP.PKCS11.Library setting to <nil> <nil> 2019-06-27 13:36:34.110 UTC  viperutil  unmarshalJSON -> DEBU 018 Unmarshal JSON: value is not a string: <nil> 2019-06-27 13:36:34.110 UTC  viperutil  getKeysRecursively -> DEBU 019 Found real value for peer.BCCSP.PKCS11.Label setting to <nil> <nil> 2019-06-27 13:36:34.110 UTC  viperutil  EnhancedExactUnmarshalKey -> DEBU 01a map peer.BCCSP:map Default:SW SW:map Security:256 FileKeyStore:map KeyStore:  Hash:SHA2  PKCS11:map FileKeyStore:map KeyStore:<nil>  Library:<nil> Label:<nil> Pin:<nil> Hash:<nil> Security:<nil>    2019-06-27 13:36:34.111 UTC  bccsp_sw  openKeyStore -> DEBU 01b KeyStore opened at  /etc/hyperledger/msp/keystore ...done 2019-06-27 13:36:34.111 UTC  bccsp  initBCCSP -> DEBU 01c Initialize BCCSP  SW  2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 01d Reading directory /etc/hyperledger/msp/signcerts 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 01e Inspecting file /etc/hyperledger/msp/signcerts/peer0-bch-org4-cert.pem 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 01f Reading directory /etc/hyperledger/msp/cacerts 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 020 Inspecting file /etc/hyperledger/msp/cacerts/ca.bch-org4-cert.pem 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 021 Reading directory /etc/hyperledger/msp/admincerts 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 022 Inspecting file /etc/hyperledger/msp/admincerts/admin-bch-org4-cert.pem 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 023 Reading directory /etc/hyperledger/msp/intermediatecerts 2019-06-27 13:36:34.111 UTC  msp  getMspConfig -> DEBU 024 Intermediate certs folder not found at  /etc/hyperledger/msp/intermediatecerts . Skipping.  stat /etc/hyperledger/msp/intermediatecerts: no such file or directory  2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 025 Reading directory /etc/hyperledger/msp/tlscacerts 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 026 Inspecting file /etc/hyperledger/msp/tlscacerts/tlsca-bch-org4-cert.pem 2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 027 Reading directory /etc/hyperledger/msp/tlsintermediatecerts 2019-06-27 13:36:34.111 UTC  msp  getMspConfig -> DEBU 028 TLS intermediate certs folder not found at  /etc/hyperledger/msp/tlsintermediatecerts . Skipping.  stat /etc/hyperledger/msp/tlsintermediatecerts: no such file or directory  2019-06-27 13:36:34.111 UTC  msp  getPemMaterialFromDir -> DEBU 029 Reading directory /etc/hyperledger/msp/crls 2019-06-27 13:36:34.111 UTC  msp  getMspConfig -> DEBU 02a crls folder not found at  /etc/hyperledger/msp/crls . Skipping.  stat /etc/hyperledger/msp/crls: no such file or directory  2019-06-27 13:36:34.111 UTC  msp  getMspConfig -> DEBU 02b Loading NodeOUs 2019-06-27 13:36:34.112 UTC  msp  newBccspMsp -> DEBU 02c Creating BCCSP-based MSP instance 2019-06-27 13:36:34.112 UTC  msp  New -> DEBU 02d Creating Cache-MSP instance 2019-06-27 13:36:34.112 UTC  msp  loadLocaMSP -> DEBU 02e Created new local MSP 2019-06-27 13:36:34.113 UTC  msp  Setup -> DEBU 02f Setting up MSP instance horizonMSP 2019-06-27 13:36:34.113 UTC  msp.identity  newIdentity -> DEBU 030 Creating identity instance for cert -----BEGIN CERTIFICATE----------END CERTIFICATE----- 2019-06-27 13:36:34.113 UTC  msp.identity  newIdentity -> DEBU 031 Creating identity instance for cert -----BEGIN CERTIFICATE----------END CERTIFICATE----- 2019-06-27 13:36:34.124 UTC  msp.identity  newIdentity -> DEBU 032 Creating identity instance for cert -----BEGIN CERTIFICATE----------END CERTIFICATE----- 2019-06-27 13:36:34.124 UTC  bccsp_sw  loadPrivateKey -> DEBU 033 Loading private key  54e9b5ea656322a0b326f4dec4e2912489a755ef80c5d39cf7bf48c43bf373bd  at  /etc/hyperledger/msp/keystore/54e9b5ea656322a0b326f4dec4e2912489a755ef80c5d39cf7bf48c43bf373bd_sk ... 2019-06-27 13:36:34.125 UTC  msp.identity  newIdentity -> DEBU 034 Creating identity instance for cert -----BEGIN CERTIFICATE----------END CERTIFICATE----- 2019-06-27 13:36:34.125 UTC  msp  setupSigningIdentity -> DEBU 035 Signing identity expires at 2029-03-26 12:02:00 +0000 UTC 2019-06-27 13:36:34.125 UTC  msp  Validate -> DEBU 036 MSP horizonMSP validating identity 2019-06-27 13:36:34.125 UTC  nodeCmd  serve -> INFO 037 Starting peer: Version: 1.4.0 Commit SHA: d700b43 Go version: go1.11.1 OS/Arch: linux/amd64 Chaincode: Base Image Version: 0.4.14 Base Docker Namespace: hyperledger Base Docker Label: org.hyperledger.fabric Docker Namespace: hyperledger 2019-06-27 13:36:34.125 UTC  msp  GetDefaultSigningIdentity -> DEBU 038 Obtaining default signing identity 2019-06-27 13:36:34.125 UTC  msp.identity  Sign -> DEBU 039 Sign: plaintext: 00000000000000000000000000000000...00000000000000000000000000000000 2019-06-27 13:36:34.125 UTC  msp.identity  Sign -> DEBU 03a Sign: digest: 66687AADF862BD776C8FC18B8E9F8E20089714856EE233B3902A591D0D5F2925 2019-06-27 13:36:34.126 UTC  ledgermgmt  initialize -> INFO 03b Initializing ledger mgmt 2019-06-27 13:36:34.126 UTC  kvledger  NewProvider -> INFO 03c Initializing ledger provider 2019-06-27 13:36:34.126 UTC  kvledger.util  CreateDirIfMissing -> DEBU 03d CreateDirIfMissing  /var/hyperledger/production/ledgersData/ledgerProvider/  2019-06-27 13:36:34.126 UTC  kvledger.util  logDirStatus -> DEBU 03e Before creating dir -  /var/hyperledger/production/ledgersData/ledgerProvider/  exists 2019-06-27 13:36:34.126 UTC  kvledger.util  logDirStatus -> DEBU 03f After creating dir -  /var/hyperledger/production/ledgersData/ledgerProvider/  exists panic: Error opening leveldb: resource temporarily unavailable goroutine 1  running : github.com/hyperledger/fabric/common/ledger/util/leveldbhelper.(*DB).Open(0xc000178a80) /opt/gopath/src/github.com/hyperledger/fabric/common/ledger/util/leveldbhelper/leveldb_helper.go:79 +0x271 github.com/hyperledger/fabric/core/ledger/kvledger.openIDStore(0xc00024d500, 0x36, 0x1) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/kv_ledger_provider.go:267 +0x10e github.com/hyperledger/fabric/core/ledger/kvledger.NewProvider(0x10b0200, 0x1f51e20, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/kvledger/kv_ledger_provider.go:60 +0x77 github.com/hyperledger/fabric/core/ledger/ledgermgmt.initialize(0xc0001789c0) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:68 +0x304 github.com/hyperledger/fabric/core/ledger/ledgermgmt.Initialize.func1() /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:52 +0x2a sync.(*Once).Do(0x1f51ef8, 0xc000347520) /opt/go/src/sync/once.go:44 +0xb3 github.com/hyperledger/fabric/core/ledger/ledgermgmt.Initialize(0xc0001789c0) /opt/gopath/src/github.com/hyperledger/fabric/core/ledger/ledgermgmt/ledger_mgmt.go:51 +0x55 github.com/hyperledger/fabric/peer/node.serve(0x1f51e20, 0x0, 0x0, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/peer/node/start.go:172 +0x56f github.com/hyperledger/fabric/peer/node.glob..func1(0x1e3fbe0, 0x1f51e20, 0x0, 0x0, 0x0, 0x0) /opt/gopath/src/github.com/hyperledger/fabric/peer/node/start.go:119 +0x9c github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).execute(0x1e3fbe0, 0x1f51e20, 0x0, 0x0, 0x1e3fbe0, 0x1f51e20) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:762 +0x473 github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x1e40300, 0x8, 0x0, 0x1e3f4c0) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:852 +0x2fd github.com/hyperledger/fabric/vendor/github.com/spf13/cobra.(*Command).Execute(0x1e40300, 0xc000027f40, 0x1) /opt/gopath/src/github.com/hyperledger/fabric/vendor/github.com/spf13/cobra/command.go:800 +0x2b main.main()  {code}  I looked into this folder:  /var/hyperledger/production/ledgersData/ledgerProvider/  It did not have .ldb file.      update2:  I checked some other peers and most of peers have file with name like 00002.ldb.  and 5 or so do not have it. However only one peer does not work.     update3:   I renamed folder /var/hyperledger/production/ledgersData/ledgerProvider/  for peer to recreate it. It creates new folder with some items, but it has NO .ldb file        ledgerProvider/LOG: {code:java} =============== Jun 27, 2019 (UTC) =============== 14:01:16.020104 log@legend F·NumFile S·FileSize N·Entry C·BadEntry B·BadBlock Ke·KeyError D·DroppedEntry L·Level Q·SeqNum T·TimeElapsed 14:01:16.023972 version@stat F·   S·0B   Sc·   14:01:16.025087 db@open opening 14:01:16.025405 journal@recovery F·1 14:01:16.025909 journal@recovery recovering @2 14:01:16.029692 version@stat F·   S·0B   Sc·   14:01:16.045803 db@janitor F·2 G·0 14:01:16.045839 db@open done T·20.737572ms =============== Jun 27, 2019 (UTC) =============== 14:01:46.014905 log@legend F·NumFile S·FileSize N·Entry C·BadEntry B·BadBlock Ke·KeyError D·DroppedEntry L·Level Q·SeqNum T·TimeElapsed 14:01:46.018781 version@stat F·   S·0B   Sc·   14:01:46.019670 db@open opening 14:01:46.019942 journal@recovery F·1 14:01:46.022388 journal@recovery recovering @4 14:01:46.030217 version@stat F·   S·0B   Sc·   14:01:46.046632 db@janitor F·2 G·0 14:01:46.046681 db@open done T·26.998966ms =============== Jun 27, 2019 (UTC) =============== 14:02:27.015976 log@legend F·NumFile S·FileSize N·Entry C·BadEntry B·BadBlock Ke·KeyError D·DroppedEntry L·Level Q·SeqNum T·TimeElapsed 14:02:27.019711 version@stat F·   S·0B   Sc·   14:02:27.020597 db@open opening 14:02:27.020921 journal@recovery F·1 14:02:27.021469 journal@recovery recovering @6 14:02:27.025633 version@stat F·   S·0B   Sc·   14:02:27.051927 db@janitor F·2 G·0 14:02:27.051982 db@open done T·31.367147ms  {code}  by the way, I forgot to mention - *I have OKD as cluster environment*  ></body> </Action>
<Action id="61310" issue="40173" author="nn77" type="comment" created="2019-06-27 15:33:17.0" updateauthor="nn77" updated="2019-06-27 15:33:17.0"> <body><! CDATA more updates:  I moved peer to volume on HostPath - It started working.  Any clues how can I debug problem with nfs and why it is so random?  ></body> </Action>
<Action id="61313" issue="40173" author="naaman" type="comment" created="2019-06-27 16:17:44.0" updateauthor="naaman" updated="2019-06-27 16:17:44.0"> <body><! CDATA  ~NN77  I suggest you open a new Jira for the issues related to leveldb and the associated NFS locking. I would like to keep this issue focused on the gossip related issues. As we discussed, it's likely that the NFS issues only affect startup but are not related to the reason of why the peers go "offline".  As I mentioned in our earlier discussion there could be several issues with NFS locking including lock lease time, unsuccessful cleanup that prevents relock, permissions related issues, and possibly resources related issues. In the previous logs you provided it was clearly the lease time and that's WAD. I could not tell if that was the case this time around.     ></body> </Action>
<Action id="61842" issue="40173" author="nn77" type="comment" created="2019-07-16 07:58:41.0" updateauthor="nn77" updated="2019-07-16 08:22:56.0"> <body><! CDATA Hello  ~naaman  !  Some more updates - peer which resides on node's hostpath became unavailable.  Restarted it - same symptoms, it did start on THIRD attempt.  ></body> </Action>
<Action id="61844" issue="40173" author="nn77" type="comment" created="2019-07-16 08:29:57.0" updateauthor="nn77" updated="2019-07-16 08:29:57.0"> <body><! CDATA And some interesting logs here:    {code:java} 2019-07-16 03:45:09.289 UTC  gossip.channel  reportMembershipChanges -> INFO 6cdcc Membership view has changed. peers went offline:    peer0-bchorg2:7051    peer0-bchorg3:7051    , current view:     2019-07-16 06:17:49.348 UTC  gossip.privdata  StoreBlock -> INFO 6cdcd  mychan  Received block  409  from buffer 2019-07-16 06:17:49.350 UTC  committer.txvalidator  Validate -> INFO 6cdce  mychan  Validated block  409  in 1ms 2019-07-16 06:17:49.427 UTC  kvledger  CommitWithPvtData -> INFO 6cdcf  xmychan  Committed block  409  with 1 transaction(s) in 76ms (state_validation=23ms block_commit=11ms state_commit=37ms)  {code} we stopped talking with peers, but received some blocks. How so?     ></body> </Action>
<Action id="61903" issue="40173" author="naaman" type="comment" body="The problem was resolved by FAB-15840, see more details there. What happens is that after a peer gets disconnected (for whatever reason) from the other peers for a few minutes it may not attempt to reconnect to the other peers again and thus remain isolated. At that time the peer is still functional it&apos;s just that it&apos;s gossip view will never be restored. " created="2019-07-17 11:28:17.0" updateauthor="naaman" updated="2019-07-17 11:28:17.0"/>
<Action id="61904" issue="40173" author="nn77" type="comment" created="2019-07-17 11:33:04.0" updateauthor="nn77" updated="2019-07-17 11:33:04.0"> <body><! CDATA No, peer is not fully functional. Last log which I sent yesterday (with empty gossip view and received block is just coincidence).  Usually when this situation occurs peer is unable to launch chaincodes, to be discovered by discovery and DOES NOT receive any blocks.  ></body> </Action>
<Action id="61906" issue="40173" author="naaman" type="comment" body="Right, &quot;still functional&quot; is probably not the right term. What I meant was that the peer continues to run as if it was the only peer (i.e., isolated) and will not report any errors.  " created="2019-07-17 11:43:21.0" updateauthor="naaman" updated="2019-07-17 11:43:21.0"/>
