<Action id="41762" issue="28571" author="latitiah" type="comment" created="2018-03-16 15:47:34.0" updateauthor="latitiah" updated="2018-03-16 15:49:11.0"> <body><! CDATA The reason for the false positives is the behave tests never complete execution. Behave dies with the following error: {code} 07:47:25 OSError:  Errno 12  Cannot allocate memory 07:47:26 /w/workspace/fabric-test-daily-x86_64/gopath/src/github.com/hyperledger/fabric-test/regression/daily{code} As a result, the behave summary is never executed for consumption by Jenkins.   There are 2 things that should be researched/done here.  # Why are the system resources running out of memory? # The Jenkins job should show a failure when there is no summary from the test executions.     ></body> </Action>
<Action id="41786" issue="28571" author="rameshthoomu" type="comment" body=" ~ry  we are seeing cannot allocate memory issues in x86_64 build machines.. Below is the memory details on that build nodes" created="2018-03-19 01:45:21.0" updateauthor="rameshthoomu" updated="2018-03-19 01:45:21.0"/>
<Action id="41787" issue="28571" author="rameshthoomu" type="comment" created="2018-03-19 01:45:21.0" updateauthor="rameshthoomu" updated="2018-03-19 01:49:21.0"> <body><! CDATA  ~ry  we are seeing cannot allocate memory issues in x86_64 build machines.. Below are the memory details on that build nodes. Can I get *ssh* access to one of the production or sandbox x86_64 to debug this further.  https://logs.hyperledger.org/production/vex-yul-hyp-jenkins-3/fabric-test-daily-x86_64/292/ {code:java} 09:47:15.296 ---> nproc: 09:47:15.296  2  09:47:15.296  09:47:15.306 ---> df -h: 09:47:15.307  Filesystem      Size  Used Avail Use% Mounted on 09:47:15.307 udev            3.9G     0  3.9G   0% /dev 09:47:15.307 tmpfs           799M   34M  765M   5% /run 09:47:15.307 /dev/vda1        39G   15G   25G  37% / 09:47:15.307 tmpfs           3.9G     0  3.9G   0% /dev/shm 09:47:15.307 tmpfs           5.0M     0  5.0M   0% /run/lock 09:47:15.307 tmpfs           3.9G     0  3.9G   0% /sys/fs/cgroup 09:47:15.307 none            2.0G  1.8G  218M  90% /w 09:47:15.307 tmpfs           799M     0  799M   0% /run/user/1001  09:47:15.307  09:47:15.315 ---> free -m: 09:47:15.315                total        used        free      shared  buff/cache   available 09:47:15.315 Mem:           7983         798        4392        1864        2792        5001 09:47:15.315 Swap:             0           0           0 {code}  ></body> </Action>
<Action id="41884" issue="28571" author="rameshthoomu" type="comment" created="2018-03-21 13:52:56.0" updateauthor="rameshthoomu" updated="2018-03-21 13:52:56.0"> <body><! CDATA @latitiah On the upgraded build node (16 core, 64. gigs of ram), I don't see the memory allocation error but I see the below error{{ }} {code:java} 03:08:48.624 IOError:  Errno 11  Resource temporarily unavailable{code} see the build log  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-daily-x86_64/295/consoleFull   {{Could you please check this.}}  ></body> </Action>
<Action id="42187" issue="28571" author="latitiah" type="comment" created="2018-03-26 21:28:40.0" updateauthor="latitiah" updated="2018-03-26 21:28:40.0"> <body><! CDATA As this is a CI issue concerning the CI resources and not a software issue, I'm reassigning.  Some hints on area to start searching: - What node is it trying to use that the job is not able to access? You will need to dig into this a bit more on the CI system nodes.  - Monitor to see the node usage. Per Ry (via Rocketchat), the minions were set to 4 core, 16G. You can look at the minion name to see which one is having problems when this occurs.  ></body> </Action>
<Action id="42742" issue="28571" author="scottz" type="comment" created="2018-04-09 18:27:00.0" updateauthor="scottz" updated="2018-04-09 18:27:00.0"> <body><! CDATA  ~latitiah  Is there a different way to configure and run Behave? Could we create a dummy xml file (so the default behavior would be displayed as failure and indicate the tests were started but not completed) that gets removed or overwritten after Behave tests complete?  This would not preclude the CI team investigation to find and fix the root cause.  ></body> </Action>
<Action id="43573" issue="28571" author="scottz" type="comment" body="what is the status of this issue? It has bounced around between people without any explanations. We should add comments whenever we reassign an issue, to say what we learned and what we eliminated as possible causes and what we think the next step should be done to investigate." created="2018-04-27 18:47:27.0" updateauthor="scottz" updated="2018-04-27 18:47:27.0"/>
<Action id="43591" issue="28571" author="latitiah" type="comment" body=" ~scottz  As stated in my last comment, I reassigned this issue since this is something that CI needs to investigate. I gave some areas and hints of where to focus the investigation. This is not a behave execution problem, but a jenkins script execution problem. An OSError indicates a system issue - in this case one of the nodes indicated that it ran out of memory." created="2018-04-28 02:17:48.0" updateauthor="latitiah" updated="2018-04-28 02:17:48.0"/>
<Action id="44130" issue="28571" author="rameshthoomu" type="comment" created="2018-05-09 14:24:41.0" updateauthor="rameshthoomu" updated="2018-05-09 14:26:49.0"> <body><! CDATA We are now getting the test summary from the behave tests..  https://logs.hyperledger.org/production/vex-yul-hyp-jenkins-3/fabric-test-daily-x86_64/345/   https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-daily-x86_64/test_results_analyzer/ {code:java} 04:40:41.991 0 features passed, 6 failed, 1 skipped 04:40:41.991 8 scenarios passed, 89 failed, 40 skipped 04:40:41.991 240 steps passed, 89 failed, 2798 skipped, 0 undefined 04:40:41.991 Took 124m46.588s{code} This test ran on build node  prd-ubuntu1604-x86_64-4c-16g-415|https://jenkins.hyperledger.org/computer/prd-ubuntu1604-x86_64-4c-16g-415   ></body> </Action>
<Action id="44284" issue="28571" author="rameshthoomu" type="comment" body="Not seeing this issue in the recent builds. Closing this issue for now." created="2018-05-12 15:11:17.0" updateauthor="rameshthoomu" updated="2018-05-12 15:11:17.0"/>
<Action id="52101" issue="28571" author="latitiah" type="comment" body="Reopened since we are seeing this issue again:  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-daily-behave-master-x86_64/51/consoleFull" created="2018-10-11 14:06:12.0" updateauthor="latitiah" updated="2018-10-11 14:06:12.0"/>
<Action id="52102" issue="28571" author="latitiah" type="comment" created="2018-10-11 14:27:16.0" updateauthor="latitiah" updated="2018-10-11 14:27:16.0"> <body><! CDATA The traceback for this failure indicates that the error is crashing the tool itself. As a result, it would be helpful if CI can look for this error and display the build as a failed build when this error is seen and the exit code is not 0.  The tracceback for this error looks like the following: {code}06:12:57 Traceback (most recent call last): 06:12:57   File "/usr/local/bin/behave", line 11, in <module> 06:12:57     sys.exit(main()) 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/__main__.py", line 183, in main 06:12:57     return run_behave(config) 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/__main__.py", line 127, in run_behave 06:12:57     failed = runner.run() 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/runner.py", line 804, in run 06:12:57     return self.run_with_paths() 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/runner.py", line 824, in run_with_paths 06:12:57     return self.run_model() 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/runner.py", line 626, in run_model 06:12:57     failed = feature.run(self) 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/model.py", line 321, in run 06:12:57     failed = scenario.run(runner) 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/model.py", line 1114, in run 06:12:57     failed = scenario.run(runner) 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/model.py", line 711, in run 06:12:57     if not step.run(runner): 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/model.py", line 1379, in run 06:12:57     formatter.result(self) 06:12:57   File "/usr/local/lib/python2.7/dist-packages/behave/formatter/pretty.py", line 151, in result 06:12:57     self.stream.write(indent(step.error_message.strip(), u"      ")) 06:12:57   File "/usr/lib/python2.7/codecs.py", line 370, in write 06:12:57     self.stream.write(data) 06:12:57 IOError:  Errno 11  Resource temporarily unavailable{code}  ></body> </Action>
<Action id="52108" issue="28571" author="scottz" type="comment" body="yes if all tests were not executed to completion, then the build should be marked as a failure and we should also see at least one test failure marked in the final results. For example, job 51 of the daily behave master test suite should be shown as a failure (red dot), not a pass (blue dot) and we should not see &quot;no failures&quot;.  https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-daily-behave-master-x86_64/51/.|https://jenkins.hyperledger.org/view/fabric-test/job/fabric-test-daily-behave-master-x86_64/51/   (Picture attached for reference.)" created="2018-10-11 16:42:06.0" updateauthor="scottz" updated="2018-10-11 16:42:06.0"/>
<Action id="52414" issue="28571" author="rameshthoomu" type="comment" body=" ~latitiah  The script has -e (exit on the first error) set, if the test command returns non zero, the script should fail.. May be test command is not returning non zero error code. How to check what error code the test test command is returning.-" created="2018-10-18 13:37:28.0" updateauthor="rameshthoomu" updated="2018-10-18 13:37:28.0"/>
<Action id="52691" issue="28571" author="latitiah" type="comment" body="I&apos;m not sure what script you are referring to, but  https://github.com/hyperledger/fabric-test/blob/master/regression/daily/runBehaveTestSuite.sh#L1  does not use the &quot;-e&quot; option. That said, the behave tool is crashing with a Traceback, it is definitely returning a non-zero exit code." created="2018-10-26 15:58:08.0" updateauthor="latitiah" updated="2018-10-26 15:58:08.0"/>
<Action id="52759" issue="28571" author="jtclark" type="comment" body=" ~latitiah ,  ~scottz  - All of the fabric-test-daily-behave-master jobs reference above are no longer available. Is there are more recent example of this behavior? Or, would it be better to troubleshoot this on the Sandbox?" created="2018-10-30 10:30:35.0" updateauthor="jtclark" updated="2018-10-30 10:30:35.0"/>
<Action id="53367" issue="28571" author="jtclark" type="comment" body=" ~latitiah ,  ~scottz  - Still waiting for a more recent example of this behavior. " created="2018-11-13 01:46:19.0" updateauthor="jtclark" updated="2018-11-13 01:46:19.0"/>
<Action id="53500" issue="28571" author="latitiah" type="comment" created="2018-11-15 16:43:35.0" updateauthor="latitiah" updated="2018-11-15 16:43:35.0"> <body><! CDATA You can force this by causing the behave tool to crash when executing the test. The console will show a "Traceback" error while the job will have a success.  In the fabric-test repo, go to "feature/environment.py" and add the following code to the "after_all" method (at the end of the file): {code} raise Exception {code}  The exit status should not be 0. {code} echo $? {code}   ></body> </Action>
<Action id="55299" issue="28571" author="rameshthoomu" type="comment" body=" ~jtclark  any update on this?" created="2019-01-04 15:51:23.0" updateauthor="rameshthoomu" updated="2019-01-04 15:51:23.0"/>
<Action id="55302" issue="28571" author="latitiah" type="comment" body="Yes, this still occurs." created="2019-01-04 16:01:03.0" updateauthor="latitiah" updated="2019-01-04 16:01:03.0"/>
<Action id="55308" issue="28571" author="jtclark" type="comment" body="Transitioning this bug to  ~vijaypunugubati ." created="2019-01-04 16:36:20.0" updateauthor="jtclark" updated="2019-01-04 16:36:20.0"/>
<Action id="56023" issue="28571" author="rameshthoomu" type="comment" body="assigning this to  ~latitiah ." created="2019-01-22 16:59:35.0" updateauthor="rameshthoomu" updated="2019-01-22 16:59:35.0"/>
<Action id="62088" issue="28571" author="denyeart" type="comment" body=" ~vijaypunugubati  Is this still an issue?" created="2019-07-24 15:55:42.0" updateauthor="denyeart" updated="2019-07-24 15:55:42.0"/>
<Action id="62111" issue="28571" author="scottz" type="comment" created="2019-07-25 14:08:42.0" updateauthor="scottz" updated="2019-07-25 14:08:42.0"> <body><! CDATA There are these paths: # Behave runs through all testcases, and they pass. # Behave runs through all testcases, and there are one or more failures. # Behave runs through some testcases, crashes, exits with error code, without completing all the testcases. No failures on the testcases that were completed. # Behave runs through some testcases, crashes, exits with error code, without completing all the testcases. There are failures on the testcases that were completed. # Behave is never started because of some prep failure, such as when the job could not download images.  Things look good on the display board now for 1 & 2; testcase failures are handled well, with appropriate colors. For #5, it shows NA in the display columns, so that is fine. The only scenario that has not happened within the last week, and which I cannot confirm whether jenkins handles it properly, is when the behave framework crashes and exits abnormally without completing all the testcases. So, for 3 & 4, what would jenkins show in those cases? I am not sure how we could test this without submitting some code to force behave to crash.  On the other hand, do we want to spend time on this, if we are going to deprecate Behave soon...  ></body> </Action>
<Action id="62430" issue="28571" author="scottz" type="comment" created="2019-07-30 19:52:33.0" updateauthor="scottz" updated="2019-07-30 19:52:33.0"> <body><! CDATA Behave is being deprecated, so we can close this. A few more weeks or months of false positives wont' change anything. Besides, the behave framework crashes have been fixed (as far as we know) so this this does not happen anymore so there is nothing to trigger this and to troubleshoot.  ></body> </Action>
