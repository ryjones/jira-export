<Action id="57056" issue="37689" author="scottz" type="comment" created="2019-02-14 21:57:49.0" updateauthor="scottz" updated="2019-02-14 23:05:47.0"> <body><! CDATA These logs were also seen.  2019-02-13 22:09:31.551014 W | wal: sync duration of 2.087949482s, expected less than 1s 2019-02-13 22:09:31.552566 W | wal: sync duration of 7.750080345s, expected less than 1s 2019-02-13 22:09:31.558517 W | wal: sync duration of 7.743392857s, expected less than 1s  I understand that any orderer in a channel would start a new election term if it does not receive any heartbeats from the leader of the channel. With transactions in flight, an orderer leader change can cause transactions to be dropped, and also other transactions to be NACKed.  We have some questions. Please help us document what happened: # Why was a leader election triggered at all? What caused the delays? Jay mentioned that we were waiting for disk IO to complete. Can we dig any deeper? # Why was a leader election triggered in *only one* of the channels? # How can an operator tune their network configuration, node hardware, or environment variables to prevent this from happening as much as possible?  ></body> </Action>
<Action id="57076" issue="37689" author="guoger" type="comment" created="2019-02-15 15:26:11.0" updateauthor="guoger" updated="2019-02-15 15:26:11.0"> <body><! CDATA bq. 1. Why was a leader election triggered at all? What caused the delays? Jay mentioned that we were waiting for disk IO to complete. Can we dig any deeper? on linux, it's this call: {code} syscall.Fdatasync(int(f.Fd())) {code} bq. 2. Why was a leader election triggered in only one of the channels? because channels are independent from each other, delayed file sync on leader in channelA doesn't affect leaders in other channels. bq. 3. How can an operator tune their network configuration, node hardware, or environment variables to prevent this from happening as much as possible? - perhaps find out wal sync performance by running wal  benchmark test|https://github.com/etcd-io/etcd/blob/master/wal/wal_bench_test.go  - set `ElectionTimeout` accordingly - trace syscall to spot delay  ></body> </Action>
<Action id="57089" issue="37689" author="scottz" type="comment" created="2019-02-16 02:34:29.0" updateauthor="scottz" updated="2019-02-16 02:34:29.0"> <body><! CDATA Answers make sense. However, upon further analysis, we notice that _something_ affected transactions in two (and maybe all 3) channels at the same time. It seems we should be looking for the root cause of a specific interruption causing disk write delays (and lost transactions) for all the channels. This has happened twice out of many high-volume traffic tests.  We clarified the timeline and added our detailed observations in the attachment `lost_transactions.txt`  ></body> </Action>
<Action id="57272" issue="37689" author="scottz" type="comment" body="We are rerunning the testcase again tonight, FAB-13760, with logs enabled as per direct message conversations with developers, and if the problem occurs we should have data available for review in about 16 hours." created="2019-02-18 23:25:20.0" updateauthor="scottz" updated="2019-02-18 23:25:20.0"/>
<Action id="57309" issue="37689" author="dongming" type="comment" created="2019-02-19 16:23:26.0" updateauthor="dongming" updated="2019-02-19 16:23:26.0"> <body><! CDATA Re-ran the same test twice with the latest image 5937ae4 from 2/18/19 with logging level FABRIC_LOGGING_SPEC=info:orderer.consensus.etcdraft=debug:orderer.common.cluster=debug. The issue was not reproduced though it was observed three times with the same test in the past.  However, the TPS is degraded by 1/4 (from 496 to 369) compare to the orderer image 55869bc59 from Jay in 2/12/19.  We do not know if this is caused by the debug log or something else. We will run with logging level "info" today to see if the logging level impact the TPS.    ></body> </Action>
<Action id="57392" issue="37689" author="scottz" type="comment" body="Using INFO logs instead of DEBUG logs with yesterday&apos;s images did not get the TPS back up to the rates we were seeing last week with J&apos;s private images. It looks like the additional time being consumed is reflected in increased endorsement response time as well as broadcast transaction response time. Investigation ongoing; considering network latency and the test client software and the test machine as possible causes which would impact both of those." created="2019-02-20 23:38:00.0" updateauthor="scottz" updated="2019-02-20 23:38:00.0"/>
<Action id="57493" issue="37689" author="kchristidis" type="comment" created="2019-02-23 21:02:14.0" updateauthor="kchristidis" updated="2019-02-23 21:02:14.0"> <body><! CDATA The observation on TPS going down is useful. Let's keep this in the back of our mind as/when we do throughput tests.  The suspicion behind this one is a slow disk. If WAL syncing takes more than 5 seconds the FSM is blocked, hence the leader re-election.  If we want to confirm this suspicion, we'll need to re-run and take note of all the warning-level logs that pop-up whenever WAL syncing takes more than 1 second.  ></body> </Action>
<Action id="57953" issue="37689" author="jyellick" type="comment" body="Any updates on this?" created="2019-03-07 04:37:01.0" updateauthor="jyellick" updated="2019-03-07 04:37:01.0"/>
<Action id="58052" issue="37689" author="scottz" type="comment" body="Yes, in past runs we have seen wal logs (delays over 5 secs) when leadership changes occurred (warnings which disappeared when the leadership changes disappeared when we increased our hosts IOPS per GB from 2 to 10). Therefore we agree this is the likely cause of the leadership changes (and makes sense with our analysis provided in the attachment lost-transactions.txt) - even though it is difficult to obtain comprehensive and conclusive data to show that it is the only cause, or that those warning logs occurred every time. We will hold this bug open for now, and keep an eye open during other testing." created="2019-03-11 14:42:49.0" updateauthor="scottz" updated="2019-03-11 14:42:49.0"/>
<Action id="58053" issue="37689" author="scottz" type="comment" created="2019-03-11 15:00:54.0" updateauthor="scottz" updated="2019-03-11 15:03:10.0"> <body><! CDATA Regarding suggestions from  ~guoger  : *  _perhaps find out wal sync performance by running wal benchmark test_  <--  ~dongming  Let's try this in hosts with 2 IOPS and again in hosts with 10 IOPS *  _set `ElectionTimeout` accordingly_  <-- Do you mean increasing *ElectionTick* (10) and/or *TickInterval* (500ms) to allow more than 5 secs before a follower times out waiting for heartbeats and calls for a new election? OK, we previously discussed this and think it might help the network accommodate these occasional delays. However we would prefer to look for another solution because it comes with the drawback that it would take longer for the network to recognize a true outage of the channel leader orderer. * _trace syscall to spot delay_ <-- please provide more specifics of what/when/how to do this  ></body> </Action>
<Action id="58308" issue="37689" author="dongming" type="comment" body="We tested with IOPS=10 and do not see the issue." created="2019-03-19 17:46:36.0" updateauthor="dongming" updated="2019-03-19 17:46:36.0"/>
<Action id="58309" issue="37689" author="dongming" type="comment" body="Close the issue." created="2019-03-19 17:48:51.0" updateauthor="dongming" updated="2019-03-19 17:48:51.0"/>
