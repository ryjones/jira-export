<Action id="63957" issue="42405" author="denyeart" type="comment" body=" ~manish-sethi  Can you take a look?" created="2019-09-23 03:31:27.0" updateauthor="denyeart" updated="2019-09-23 03:31:27.0"/>
<Action id="64294" issue="42405" author="scottallan" type="comment" body=" ~denyeart   ~manish-sethi  has anyone been able to verify this report yet.  It would seem to be a critical if verified." created="2019-10-03 19:47:50.0" updateauthor="scottallan" updated="2019-10-03 19:48:03.0"/>
<Action id="64399" issue="42405" author="manish-sethi" type="comment" created="2019-10-08 16:07:40.0" updateauthor="manish-sethi" updated="2019-10-08 16:07:40.0"> <body><! CDATA Yes, this seems to be a corner case bug which could hit if the key has been deleted (explicitly or because of expiry) before reconciliation for this takes place. I'll submit a CR with a fix.  Meanwhile  ~RyanMathison  can you confirm on the deletion /expiry aspect - e.g., are you using BlockToLive for this data or the key was deleted explicitly?  ></body> </Action>
<Action id="64403" issue="42405" author="manish-sethi" type="comment" created="2019-10-08 17:56:55.0" updateauthor="manish-sethi" updated="2019-10-08 20:20:50.0"> <body><! CDATA https://gerrit.hyperledger.org/r/#/c/fabric/+/33904/ - release-1.4  https://gerrit.hyperledger.org/r/#/c/fabric/+/33908/ - master    ></body> </Action>
<Action id="64404" issue="42405" author="ryanmathison" type="comment" body=" ~manish-sethi  BlockToLive was set to 0 and no keys were deleted explicitly. I think what lead to our issue is that we had an entire 4 organization network running on a single kubernetes cluster.  This lead to the network becoming unstable when it went under stress testing. From looking at the logs, this caused peers to have very long response times which prevented the private data from being passed to the peer who was restarted." created="2019-10-08 18:03:57.0" updateauthor="ryanmathison" updated="2019-10-08 18:04:42.0"/>
<Action id="64409" issue="42405" author="manish-sethi" type="comment" created="2019-10-08 20:28:43.0" updateauthor="manish-sethi" updated="2019-10-08 20:53:53.0"> <body><! CDATA  ~RyanMathison  Do you mean to say that your application never deletes keys? Then this should not have led to this panic. Because this would mean that the line you pointed out should return the existing hashed value and cannot be nil.   Can you paste the dump stack so as to verify that is happening on the loc that you pointed out?  Also, it may be helpful to attach the logs of the affected peer.  ></body> </Action>
<Action id="64410" issue="42405" author="ryanmathison" type="comment" created="2019-10-08 21:00:11.0" updateauthor="ryanmathison" updated="2019-10-08 21:00:11.0"> <body><! CDATA  ~manish-sethi  I have already attached everything that I have available.  Before the peer restarted, for the affected block it logged "_Could not fetch all missing collection private write sets from remote peers. Will commit block  41080  with missing private write sets: txID:"_  Then in  ^PeerRestartError.log we can see "_ReadDoc -> DEBU 17b  common-channel_mycc$$hcollectionorg1org4$details  Document not found (404), returning nil value instead of 404 error"_  This means it tried to read the hash of the private collection which it was missing when it did the original commit of block 41080.  This log is coming from {{{color:#172b4d}couchDoc, _, err := db.ReadDoc(key){color}}} which is exactly what I was pointing to  The next log is  "_ gossip.gossip  Stop -> INFO 17c Stopping gossip"_ which is the first step to the peer stopping. This means that the peer failure to restart is caused by the not being able to find the private details hash because checkIfPrivateWriteIsState is not handling a nil return from ReadDoc        ></body> </Action>
<Action id="64412" issue="42405" author="manish-sethi" type="comment" created="2019-10-08 22:29:43.0" updateauthor="manish-sethi" updated="2019-10-08 22:29:43.0"> <body><! CDATA {quote}This means it tried to read the hash of the private collection which it was missing when it did the original commit of block 41080 {quote} Hash of the key should never be missing (unless the key was deleted afterwards) because the hash is always present in the block that got committed. The loc you highlighted is trying to read the hash of the private key (not the actual private key that was missing) during reconciliation of the missing private key in order to verify whether the key was further updated and hence the older value should be ignored. Assuming the key was deleted, then yes, there was a bug and I have pushed a CR with a fix (see links in one of the above comments) that should fix your issue.  However, I wanted to verify with you on whether your application also deletes the keys to make sure that it's not some other hidden bug at any other place.  ></body> </Action>
<Action id="64505" issue="42405" author="ryanmathison" type="comment" created="2019-10-09 16:18:16.0" updateauthor="ryanmathison" updated="2019-10-09 16:18:16.0"> <body><! CDATA  ~manish-sethi  Thank you for all your help.   I have looked through our settings and our application and I am unable to find anywhere where we are deleting the keys.  ></body> </Action>
<Action id="64562" issue="42405" author="manish-sethi" type="comment" created="2019-10-10 16:12:21.0" updateauthor="manish-sethi" updated="2019-10-10 16:12:21.0"> <body><! CDATA  ~RyanMathison  - Not able to think any situation other than a delete that could have caused this (unless there is some hidden bug in committing state to couchdb).  As an additional verification, can you check whether you set the value to a `nil` or an empty byte array. I vaguely recall that these two values could lead to an implicit delete. Also, is it possible for you to locate the transaction envelop / block that updated this keyhash last time before restarting the crashing peer - the rwset present in this transaction envelop can be checked for the last operation performed on this keyhash.  ></body> </Action>
<Action id="65232" issue="42405" author="denyeart" type="comment" created="2019-11-01 21:07:16.0" updateauthor="denyeart" updated="2019-11-01 21:07:16.0"> <body><! CDATA I've reproduced the problem on v1.4.3 with following steps: * Start peer1 * Use chaincode to create a private key * Use chaincode to delete a private key * Stop peer1 * Start peer2 * peer2 catches up to block height and commits blocks with missing private data (can't pull private data from peer1 since it is down) * Re-start peer1 * peer2 private data reconciler now is able to pull the missing private data from peer1 * However since the committed public hash has already been deleted on peer2, the committedVersion is nil and peer2 panics with nil pointer exception in findAndRemoveStalePvtData function.  To workaround the problem on v1.4.3, bring up a new peer (or delete all data in peer's fileSystemPath), and join it to the channel. This time the peer will pull all private data while catching up to block height.  I have also confirmed that if you hit the problem on a v1.4.3 peer, you can simply upgrade to v1.4.4 binary and the problem goes away - reconciler is able to pull missing private data correctly in v1.4.4, even if the key has since been deleted.  ></body> </Action>
