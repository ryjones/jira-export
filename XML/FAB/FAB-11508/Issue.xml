<Issue id="32648" key="FAB-11508" number="11508" project="10002" reporter="kchristidis" assignee="kchristidis" creator="kchristidis" type="10003" summary="Add support for consenter metadata to channel configuration" priority="3" resolution="10000" status="6" created="2018-08-07 18:41:48.0" updated="2018-08-14 16:18:54.0" resolutiondate="2018-08-14 16:18:54.0" votes="0" watches="1" workflowId="43874"> <description><! CDATA h2. Goal  As a consensus plugin author, I need access to the consenter (replica) set configuration  This is a joint proposal with  ~jyellick .  h2. Why are we doing this?  When the Raft-based ordering service is bootstrapped, all Raft nodes (i.e. all Raft-enabled OSNs) need to know each other's addresses so that they can reach out to each other and form a cluster.  This has broader applicability; we will need this for other implementations as well, the BFT one being the first that comes to mind.  h2. Why can't we do this today?  The closest thing we have to this today is the {{Orderer.Kafka.Brokers}} key in {{configtx.yaml}}, whose value is accessible via the {{KafkaBrokers}} getter of the the {{ChannelConfig.Orderer}} interface. (Note that as a consensus plugin author, you have access to the {{consensus.ConsenterSupport}} interface, and by extension to the {{channelconfig.Orderer}} interface.)  That is, obviously, Kafka-specific.  h2. What are the suggested approaches?  We _could_ do something similar for Raft, with an {{Orderer.RaftNodes}} key, and a {{RaftNodes}} getter in the {{ChannelConfig.Orderer}} interface.  Then, for BFT — or any other implementation for that matter — we'd do the same.  The first problem with this approach is that it takes an already big interface {{ChannelConfig.Orderer}} interface, and expands it even further, as the number of supported implementations increases.  We therefore need _a more generic getter_, call it {{Nodes}}, that retrieves the value of an {{Orderer.Nodes}} key in {{configtx.yaml}}. (N.B. The field/method names provided here are not _fixed_.)  _But_ —  If we have all implementations sharing the same message definition for {{Orderer.Nodes}}, then extensibility takes a hit. Consider for instance the case where you may want to assign an initial weight to each node in the BFT replica set.  *So, this generic getter will have to _return an opaque bytes field_, whose underlying/unmarshaled type depends on the consensus type.*  Note that the Fabric codebase will _not_ add implementation-specific getters to its external-facing interfaces; it will be up to the caller to retrieve that byte slice and parse it accordingly.  Fabric will also _not_ perform any sanity checking/validation on the submitted {{Orderer.Nodes}} values. The risk here is that without validation a bad configuration block may put the system into an unrecoverable state. This stands in contrast to some checks we have elsewhere in the system (see validation of admin certs added to MSPs, or barebones sanity checking for IP addresses in Kafka brokers). At the end of the day however, this is a risk that we should live with: we cannot guard _effectively_ against bad user input anyway.  -One additional issue with the proposed approach has to do with the attack described in FAB-7559: _just_ getting the address of a consenter in our consenter set is not safe. We need to know the owning org, so we can load _just_ this org's TLS root CA when establishing the connection and avoid the BJP hijacking attack described in FAB-7559.-  -Therefore, the owning org is a field that should be present in all message definitions that correspond to {{Orderer.Node}}.-  -(This attack is actually only relevant in the BFT case, but we may as well tackle it in time for Raft to build on top of, so that we keep the number of moving pieces to a minimum when transitioning from Raft to BFT.)-  Instead of adding an {{owning_org}} field to the {{XXX_Consenter}} message definition as initially suggested, let's do TLS pinning. We identify the exact TLS certificate that a consenting node should present before they're allowed to participate in consensus. So, we replace the {{owning_org}} field with a {{tls_cert}} one.  h2. Suggested proposal  Change from  https://github.com/hyperledger/fabric/blob/release-1.1/protos/orderer/configuration.proto#L31  to: {code:java} message ConsensusType { 	string type = 1; // Possible values: kafka, raft, pbft, etc. 	bytes metadata = 2; // Opaque metadata, dependent on consensus type. }  {code}   Add a new method to  https://github.com/hyperledger/fabric/blob/cd075b71b637e9adba226b945cd41d6150303b77/common/channelconfig/api.go#L81 : {code:java} ConsensusMetadata()   byte {code}  h2. How does this relate to FAB-7559 and listing orderer endpoints (addresses) inside the orderer orgs?  Addresses (endpoints) in orderer org configuration group denote org nodes that satisfy the {{AtomicBroadcast}} interface, i.e. that respond to {{Broadcast}} and {{Deliver}} RPCs. We call these nodes *ordering nodes*. (Example consumers of these addresses: the delivery service clients.)  Addresses listed in the slice contained in the {{ConsensusType.metadata}} field denote nodes participating in the _consensus mechanism_ of Fabric. We call this nodes *consenting nodes*. (Example consumers of these addresses: Raft and PBFT nodes.)  This implies that: # An ordering node is not necessarily a consenting node, and vice-versa. # When we tackle FAB-7559, we will need to introduce settings in {{orderer.yaml}} that disable the {{AtomicBroadcast}} service and the consensus module.  ></description> </Issue>
