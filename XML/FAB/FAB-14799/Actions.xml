<Action id="58835" issue="38733" author="yacovm" type="comment" body="I think that adding it to the leader is not that intrusive, but we shouldn&apos;t piggyback this into the heart-beats. " created="2019-04-02 09:54:32.0" updateauthor="yacovm" updated="2019-04-02 09:54:32.0"/>
<Action id="58844" issue="38733" author="jyellick" type="comment" body=" ~yacovm  How would you ultimately address the need for multiple orgs to monitor Raft health without either a) requiring that the orderer operations endpoint be opened across orgs b) that the prometheus server scraping the endpoint be opened across orgs?" created="2019-04-02 13:54:40.0" updateauthor="jyellick" updated="2019-04-02 13:54:40.0"/>
<Action id="58847" issue="38733" author="yacovm" type="comment" body="they can either look at the logs, and we do log there errors and warnings, or we can add metrics for transmission errors... what do you say? " created="2019-04-02 14:04:18.0" updateauthor="yacovm" updated="2019-04-02 14:04:18.0"/>
<Action id="58862" issue="38733" author="adarshsaraf123" type="comment" created="2019-04-02 18:10:43.0" updateauthor="adarshsaraf123" updated="2019-04-02 18:10:43.0"> <body><! CDATA I might have missed something but the last time around we were discussing about this there was no reliable way of figuring out the number of active nodes. Do we plan on using the heartbeats from followers at the leader for this? I believe  ~yacovm  had some problem with this approach back then.    Also, I am curious about why just knowing about the leader is not enough from an operations standpoint? If an admin observes that the cluster is leaderless for a long time (we can do so by changing back the {{IsLeader}} metric to the earlier {{LeaderID}} metric), isn't that sufficient indication of the cluster health versus needing to know the number of active nodes?  ></body> </Action>
<Action id="58864" issue="38733" author="jyellick" type="comment" created="2019-04-02 18:44:34.0" updateauthor="jyellick" updated="2019-04-02 18:44:34.0"> <body><! CDATA If we have decided to use metrics to surface cluster health, I don't think "look at the logs" is an acceptable answer.  And, operationally it would be very confusing to say "If your node is the leader, check cluster health by looking at your metric server for 'active nodes', otherwise look at your logs".  I'm also a little unsure what metrics for transmission errors gives us?  If that's a useful metric, then it's one we might want to consider adding, but, concretely, consider the following scenario:  There are five orgs, each contributing a single node to a five node cluster.  One org, wants to do some maintenance on their node.  They know their node is not the leader, but, they have no insight into whether bringing their node down would cause loss of quorum.  Obviously this org _could_  talk with each of the other admins until discovering which node is the leader and how many nodes are in quorum, but, this seems like a bad workflow.   ~yacovm  What is your opposition to including this in the heartbeat? It should be quite small, and seems like generally useful information?  ></body> </Action>
<Action id="58867" issue="38733" author="yacovm" type="comment" created="2019-04-02 21:20:20.0" updateauthor="yacovm" updated="2019-04-02 21:55:09.0"> <body><! CDATA So, first of all - I, as an SRE - would hope that someone from another org talks to me about a maintenance when he/she brings down an OSN, because I might be doing some network maintenance that would affect my own node, and even if not - then I would see in my logs that I lost contact to that other OSN and start wondering what's up.  In addition, when you bring down a node for maintenace, even if it doesn't make you lose quorum - you reduce the amount of nodes you can lose, so in a way - the SLA is impacted here, and you need to ensure (or at least that's what makes sense to me) that the other parties are OK with it, and are not planning any special event that a loss would lead to massive revenue loss  From my experience, when a node goes down - you (as an SRE) start investigating and making phone calls why it is down - is that a network outage? a maintenance you're not aware of? etc. No one should bring down servers of a running system, *even* if the system is not an active-active, but only an active-passive replication system.  So, by doing this we are actually creating a slippery slope for admins to take unnecessary risks.   Secondly - I personally think that an organization should connect the logs to some ELK stack or something equivalent, and actually monitor errors and warnings in the logs.  Metrics - are usually used to measure how bad the system performs from a quantitative aspect, than from a qualitative aspect.    Now, if we add the active nodes count that means we need to take the existing payload, and wrap it into another protobuf message, which means we're adding an indirection of marshaling or unmarshaling for every block we send. Are we sure that's the right approach?   What if we, for instance - add another field to the ConsensusMessage we have which would be a metadata  bytes field, that would be used as a catch-all field for metadata from now onwards? we can then take this metadata and have it be EtcdRaftMetadata protobuf message.   Also - if we do that, I think we can go the extra mile and actually compute ourselves the amount of nodes that can crash without the loss of the quorum, and report that metrics instead of the alive nodes. We don't want people that are bad at integer division to make mistakes ;)   Opinions?  ></body> </Action>
<Action id="58872" issue="38733" author="guoger" type="comment" created="2019-04-03 01:35:10.0" updateauthor="guoger" updated="2019-04-03 07:32:30.0"> <body><! CDATA bq. I might have missed something but the last time around we were discussing about this there was no reliable way of figuring out the number of active nodes  ~adarshsaraf123  IIRC, last time we talked about this is when i ask Yacov whether it is possible/easy to retrieve this info directly from communication layer, and the answer was no. But this does not prevent us from getting it at upper layer - every time a message is received from a node, mark it as active with a timeout ({{ElectionTimeout}})  bq. What if we, for instance - add another field to the ConsensusMessage we have which would be a metadata bytes field, that would be used as a catch-all field for metadata from now onwards? we can then take this metadata and have it be EtcdRaftMetadata protobuf message.  ~yacovm  This is what I would advocate for if we decide to cascade extra info. However I was concerned about adding extra marshaling/unmarshaling. but we need to have a decision before official release, otherwise it wouldn't be extensible due to compatibility.  Also, if we do this, we probably can simply have a pingpong service running on every node, which is independent from consensus, and no info needs to be cascaded from leader. If a network is split and none of the partitions can form quorum, {{ActiveNodes}} can still be a meaningful indicator.  ></body> </Action>
<Action id="61210" issue="38733" author="sykesm" type="comment" created="2019-06-24 15:22:37.0" updateauthor="sykesm" updated="2019-06-24 15:22:37.0"> <body><! CDATA This story needs to be broken down a bit. We don't have clear acceptance criteria and much of the nuance of the work is not captured in the story. There are some references to what's required but they need to be more explicit.  Will follow up with  ~jyellick  after IPM.  ></body> </Action>
<Action id="61515" issue="38733" author="guoger" type="comment" body=" https://gerrit.hyperledger.org/r/c/fabric/+/32206 " created="2019-07-08 14:20:06.0" updateauthor="guoger" updated="2019-07-08 14:20:06.0"/>
