<Issue id="40165" key="FAB-15558" number="15558" project="10002" reporter="rajatsharma" creator="rajatsharma" type="10004" summary="Consenter for channel exiting" priority="2" status="10701" created="2019-05-26 19:42:36.0" updated="2019-06-12 12:19:08.0" votes="1" watches="3" workflowId="53067"> <description><! CDATA We have a Fabric v1.2 network with 4 Kafka and 3 Zookeepers. We were performing automated bulk testing on one channel. There was some issue of space so we had upgraded the servers, and had restarted the servers. After some time the whole fabric network was working fine but that channel was not working.   With the onset of this issue we started encountering these logs in orderer:  {code:java} 2019-05-16 21:55:02.537 UTC  orderer/consensus/kafka  processMessagesToBlocks -> WARN 0f1  channel: mychannel  got right time-to-cut message (for block 25641), no pending requests though; this might indicate a bug 2019-05-16 21:55:02.537 UTC  orderer/consensus/kafka  processMessagesToBlocks -> CRIT 0f2  channel: mychannel  Consenter for channel exiting {code}  After which these logs started repeating in orderer:  {code:java} 2019-05-16 21:03:42.737 UTC  common/deliver  Handle -> WARN 49e Error reading from 10.0.0.9:36274: rpc error: code = Canceled desc = context canceled 2019-05-16 21:03:52.746 UTC  common/deliver  deliverBlocks -> WARN 49f  channel: mychannel  Rejecting deliver request for <IP_ADRESS> because of consenter error {code}  And in the peer:  {code:java} 2019-05-16 21:04:52.779 UTC  blocksProvider  DeliverBlocks -> WARN 453  mychannel  Got error &{SERVICE_UNAVAILABLE}{code}  On inspecting this issue a bit more in the Kafka we found that Kafka has moved it’s offset from 0 and even cleared some logs:  {code:java} INFO Incrementing log start offset of partition mychannel-0 to 30457 in dir /tmp/kafka-logs (kafka.log.Log) INFO Cleared earliest 0 entries from epoch cache based on passed offset 30457 leaving 6 in EpochFile for partition mychannel-0 (kafka.server.epoch.LeaderEpochFileCache) INFO Updated PartitionLeaderEpoch. New: {epoch:13, offset:8621}, Current: {epoch:12, offset8618} for Partition: mychannel1-0. Cache now contains 5 entries. (kafka.server.epoch.LeaderEpochFileCache) INFO  GroupMetadataManager brokerId=1  Removed 0 expired offsets in 0 milliseconds. (kafka.coordinator.group.GroupMetadataManager) INFO Found deletable segments with base offsets  0,4883,7464,22368,25071,26892  due to log start offset 30457 breach (kafka.log.Log) INFO Scheduling log segment 0 for log mychannel-0 for deletion. (kafka.log.Log) INFO Scheduling log segment 4883 for log mychannel-0 for deletion. (kafka.log.Log) INFO Scheduling log segment 7464 for log mychannel-0 for deletion. (kafka.log.Log) INFO Scheduling log segment 22368 for log mychannel-0 for deletion. (kafka.log.Log) INFO Scheduling log segment 25071 for log mychannel-0 for deletion. (kafka.log.Log) INFO Scheduling log segment 26892 for log mychannel-0 for deletion. (kafka.log.Log) INFO Deleting segment 0 from log mychannel-0. (kafka.log.Log) INFO Deleting segment 4883 from log mychannel-0. (kafka.log.Log) INFO Deleting segment 7464 from log mychannel-0. (kafka.log.Log) INFO Deleting segment 22368 from log mychannel-0. (kafka.log.Log) INFO Deleting segment 25071 from log mychannel-0. (kafka.log.Log) INFO Deleting segment 26892 from log mychannel-0. (kafka.log.Log) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000026892.index.deleted (kafka.log.OffsetIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000022368.index.deleted (kafka.log.OffsetIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000026892.timeindex.deleted (kafka.log.TimeIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000022368.timeindex.deleted (kafka.log.TimeIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000025071.index.deleted (kafka.log.OffsetIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000000000.index.deleted (kafka.log.OffsetIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000007464.index.deleted (kafka.log.OffsetIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000004883.index.deleted (kafka.log.OffsetIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000025071.timeindex.deleted (kafka.log.TimeIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000000000.timeindex.deleted (kafka.log.TimeIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000007464.timeindex.deleted (kafka.log.TimeIndex) INFO Deleting index /tmp/kafka-logs/mychannel-0/00000000000000004883.timeindex.deleted (kafka.log.TimeIndex) {code}  We’ve still not figured out why did Kafka clear it’s log when we had set KAFKA_LOG_RETENTION_MS=-1, This was the docker-compose for kafka:  {code:java} kafka0:        image: hyperledger/fabric-kafka:amd64-0.4.13        container_name: kafka0        environment:            - KAFKA_LOG_RETENTION_MS=-1            - KAFKA_MESSAGE_MAX_BYTES=103809024            - KAFKA_REPLICA_FETCH_MAX_BYTES=103809024            - KAFKA_BROKER_ID=0            - KAFKA_ZOOKEEPER_CONNECT=zookeeper0:2181,zookeeper1:2181,zookeeper2:2181            - KAFKA_UNCLEAN_LEADER_ELECTION_ENABLE=false            - KAFKA_DEFAULT_REPLICATION_FACTOR=3            - KAFKA_MIN_INSYNC_REPLICAS=2        ports:            - 9000:9092        networks:          - byfn {code}  The thing is now in this condition if we spawn a new orderer then it looks for blocks from 0 but it is getting stuck at 5231, while old orderer is looking for 25641 which is also causing an error. And moreover when we had mentioned KAFKA_LOG_RETENTION_MS=-1, still Kafka has cleared the logs.   We had checked block information on peer then we encountered these: {code:java} {"height":25641,"currentBlockHash":"BaUHJKTuPH2ND8MYadbnumj0yxjQe8jkjnhxIW/n5Rk=","previousBlockHash":"1RDhRtlcXkdadHu7GDg/lJWzTeh4oOtUipGzHaemCos="}`{code}  We’re not able to find a way to use that channel again.  ></description> </Issue>
