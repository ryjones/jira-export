<Issue id="44829" key="FAB-17768" number="17768" project="10002" reporter="btl5037" creator="btl5037" type="10003" summary="Create AZP Pipeline Definition for Driving State Fork Tests" priority="3" resolution="10001" status="6" created="2020-04-16 06:10:03.0" updated="2021-01-10 17:38:20.0" resolutiondate="2021-01-10 17:38:20.0" votes="0" watches="1" workflowId="58748" archived="N"> <description><! CDATA Now that we have the chaincode, index definitions and JSON query driver, we can put all the pieces together and create a pipeline for driving this test.  The pipeline should perform the following stages and jobs, we will run them on static nodes so we can control the environment: * The first stage should stand up the network and generate the artifacts on Node A  *  ** Since we need to test v1.1.x Fabric, unless we can easily add support for this to the Operator tool (consult with Surya on the possibility of doing this), we will need to use the legacy Network Launcher tool for standing up the network. ** Use 10 channels and 2 peers, the rest of the topology should be irrelevant. ** Docker supports limiting CPU and Memory, use 500 millicores and 500MB memory, we want to starve the peers resources ** Both peers should be joined to all channels and use legacy lifecycle ** Install chaincode with single field indexes on both peers with 2of2 endorsement policy ** Instantiate Chaincode on all channels ** Publish the artifacts needed by other jobs to submit transactions ** Start continuously upgrading the chaincode on each channel, one after the other *** We can use the same chaincode and just increment the version number. But after each loop over each of the all 10 channels we swap what index file we are using. So we are alternating between single field indexes and double field indexes. This process continues indefinitely. If the chaincode upgrade fails check the failure message for an MVCC conflict, if the fork occurred the upgrade will fail and this error returned. *** If the fork occurs we should kill the process but leave the network running and exit the pipeline * In a separate stage on Node B ** Download the artifactsÂ  ** Run Invokes against the network submitting transactions to both peers for endorsement * In a separate stage on Node C ** Use the JSON query client to constantly query CouchDB using the public endpoint, queries should run one after another  We need to figure out way to tell Node B and Node C they can shut down gracefully when a fork occurs and clean up the artifacts on Node B and Node C. Otherwise the test should have a 30 minute timeout and should clean up all of its artifacts on all nodes at the end of the pipeline  ></description> </Issue>
