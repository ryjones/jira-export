<Action id="63441" issue="42133" author="caod" type="comment" body=" ~jyellick  any ideas? fwiw I think even in the failure case it is invoking `processMessagesToBlocks` but it&apos;s hard to see where it fails in that function (or even if it is failing) without a logger messageÂ  https://github.com/hyperledger/fabric/blob/release-1.4/orderer/consensus/kafka/chain.go#L339 " created="2019-08-30 15:32:19.0" updateauthor="caod" updated="2019-08-30 16:03:11.0"/>
<Action id="63444" issue="42133" author="caod" type="comment" body=" ~steveLiuu  can you post any other logs from further above in the startThread if there are any? Curious to see if maybe it failed to setup the topic for channel" created="2019-08-30 16:05:47.0" updateauthor="caod" updated="2019-08-30 16:05:47.0"/>
<Action id="63460" issue="42133" author="steveliuu" type="comment" created="2019-08-30 18:41:10.0" updateauthor="steveliuu" updated="2019-08-30 18:45:53.0"> <body><! CDATA Hi,  ~caod   I have uploaded the logs.  ></body> </Action>
<Action id="63466" issue="42133" author="caod" type="comment" body=" ~steveLiuu  any other logs afterwards? It seems the connection is closed to the broker in both cases so it might be unrelated as to why `processMessagesToBlock` isn&apos;t showing any logs. Rather if it&apos;s not logging anything it&apos;s likely that there is nothing in the stream of ordered messages for the kafka consumer" created="2019-08-30 19:38:01.0" updateauthor="caod" updated="2019-08-30 19:38:01.0"/>
<Action id="63477" issue="42133" author="steveliuu" type="comment" created="2019-09-01 10:13:14.0" updateauthor="steveliuu" updated="2019-09-02 02:49:28.0"> <body><! CDATA  ~caod Â There is no other logs afterward. WhyÂ the stream of ordered messages are missing sometimes?  BTW, sometimes the orderer fails at the beginning. Thus, we restart the orderer and then the orderer works fine.Â   ></body> </Action>
<Action id="63498" issue="42133" author="jyellick" type="comment" created="2019-09-03 01:14:53.0" updateauthor="jyellick" updated="2019-09-03 01:14:53.0"> <body><! CDATA The {{processMessagesToBlock}}Â seems like a red herring to me.Â Â My guess is that some command is failing early on, perhaps channel creation, and your scripts are not detecting the failure and retrying or aborting?  Kafka takes time to start up, so, for the first few seconds through first few minutes, your requests will see a response of SERVICE_UNAVAILABLE.  It sounds to me like perhaps you are sending in a channel creation request which is being rejected because Kafka is not yet up (about half the time), and everything fails from thereon out.  It might not be exactly that, but carefully walking through looking for uncaught errors would be where I would start.  Because Solo does not have any dependencies, it almost never returns SERVICE_UNAVAILABLE.  ></body> </Action>
<Action id="63499" issue="42133" author="steveliuu" type="comment" created="2019-09-03 03:32:56.0" updateauthor="steveliuu" updated="2019-09-03 04:07:19.0"> <body><! CDATA Hi,  ~jyellick   * UPDATE * The warning log below isn't showing up in every orderer error logs  After walking through the orderer log again, we found a warning log. {code:java}  33m2019-08-29 08:53:15.853 UTC  orderer.common.broadcast  ProcessMessage -> WARN 668 0m  channel: mychannel  Rejecting broadcast of config message from 10.42.0.1:58076 because of error: error applying config update to existing channel 'mychannel': error authorizing update: error validating ReadSet: proposed update requires that key  Group  /Channel/Application be at version 0, but it is currently at version 1 {code} This is the only error log which is different from the successful logs and we didn't find theÂ SERVICE_UNAVAILABLE error.Â   BTW, we upload both successful and failed orderer logs.Â :)  ></body> </Action>
<Action id="63500" issue="42133" author="jyellick" type="comment" body="Are you certain that you&apos;re clearing your persistent volumes (both orderers and brokers)? The error above looks like you&apos;re attempting to create a channel which already exists." created="2019-09-03 04:31:04.0" updateauthor="jyellick" updated="2019-09-03 04:31:04.0"/>
<Action id="63507" issue="42133" author="steveliuu" type="comment" created="2019-09-03 08:53:12.0" updateauthor="steveliuu" updated="2019-09-03 08:53:12.0"> <body><! CDATA  ~jyellick  Yes, the persistent volumes are clearing. That error didn't show up in every error logs. Maybe, that was the accidents.  We found that keeping restarting the orderer with the same settings can solve thisÂ {{processMessagesToBlockÂ }}problem. However, we still wonder what happen to the orderer.  ></body> </Action>
<Action id="63518" issue="42133" author="jyellick" type="comment" created="2019-09-03 14:10:22.0" updateauthor="jyellick" updated="2019-09-03 14:10:22.0"> <body><! CDATA To re-iterate, {{processMessagesToBlock}} is almost definitely not the problem.    Looking closely at both your success and failure logs, the channel is created successfully in both cases.  In the failure log, it appears there is an attempt to create the channel a second time, which (correctly) fails.  Are you trying to send the channel creation or other transactions to multiple orderers? In the Fabric ordering model, you need only submit to a single orderer.  ></body> </Action>
<Action id="63536" issue="42133" author="steveliuu" type="comment" created="2019-09-04 03:23:23.0" updateauthor="steveliuu" updated="2019-09-04 03:23:23.0"> <body><! CDATA  ~jyellick Â No, we use only one orderer in this scenario.  Sorry, I didn't describe the scenario of channel creation failure clearly making you confused. The channel creation failure in this scenario means that some of the channel functionalities didn't work properly. The channel is "built" but it didn't work as expected.  For example, as long asÂ {{processMessagesToBlock}} andÂ {{processConnect}} didn't appear in the log,Â {{joinChannel}}Â may work but {{waitForBlockNum}}Â will not increase. Even though we haveÂ {{instantiated}} or {{invoked}} the chaincode, the block seems not to be created. That's what I was asking.Â   The log below is also in the attachment. {code:java}  36m2019-08-29 08:25:37.841 UTC  fsblkstorage  waitForBlock -> DEBU 7ba 0m Going to wait for newer blocks. maxAvailaBlockNumber= 0 , waitForBlockNum= 1 {code} Thank you so much for your reply.  ></body> </Action>
<Action id="63538" issue="42133" author="jyellick" type="comment" created="2019-09-04 05:19:42.0" updateauthor="jyellick" updated="2019-09-04 05:19:42.0"> <body><! CDATA According to the orderer logs: The Kafka producer is setup correctly and sends a connect message, the consumer is setup correctly on both testchainid and mychannel, and receives messages on testchainid, but not on mychannel.  It's almost as if the Kafka cluster the producer is connecting to is different from the one the consumer is connecting to.  Can you check your Kafka broker logs to look for anything suspicious? Are the the partition offsets growing / are messages getting produced to the Kafka partitions? (You may inspect the topic with the channel name 'mychannel'). What happens if you restart your orderer? What happens if you send more transactions to the orderer?  The behavior is very odd, and had we not executed this exact test with Kafka networks thousands of times, I would think it is a bug in Fabric.  However, this scenario is exactly what's done in byfn, as well as our integration tests, so I suspect a deployment issue.  ></body> </Action>
<Action id="63541" issue="42133" author="steveliuu" type="comment" created="2019-09-04 10:24:16.0" updateauthor="steveliuu" updated="2019-09-04 10:24:16.0"> <body><! CDATA  ~jyellick  I have checked the kafka brokers and zookeepers log. It seems the logs are similar both in failure and success. I also uploaded the file to the attachment.Â   *What happens if you restart your orderer?* The orderer may work after several restarts.Â   *What happens if you send more transactions to the orderer?* Under the failed creation situation, only the grpc call log of channel creation appears in orderer's log. I also have tried to send repeatedlyÂ {{instantiate txs}} to the orderer. When I check theÂ peer {{chaincode list --instantiated -C mychannel}}, there is no any instantiated chaincode there. However, Installed chaincode is searchable inÂ {{peer chaincode list --install}}.  I am curious about why the same deployment and procedure to have a different result.  *Attachment*   ^20190829_fail_query2.zip    ^20190829_success_kafka.zip   ></body> </Action>
<Action id="63548" issue="42133" author="jyellick" type="comment" created="2019-09-04 17:05:50.0" updateauthor="jyellick" updated="2019-09-04 17:05:50.0"> <body><! CDATA There's nothing obvious in the logs which indicate why you'd be seeing this failure.  I'd suggest the next debugging step is to use the Kafka console producer and console consumer to see if the brokers are behaving as you would expect them to.  I would start by consuming the mychannel topic on the brokers.  If you see a bunch of binary data, including a PEM encoded cert, then the message made it to Kafka, if you do not, then the message produced by the orderer did not make it to the broker.  We can debug from there.  You can see the console consumer documentation here: https://kafka.apache.org/quickstart#quickstart_consume  ></body> </Action>
<Action id="63985" issue="42133" author="jyellick" type="comment" body="Is this still an issue? Were you able to debug using the console consumer?" created="2019-09-23 20:01:02.0" updateauthor="jyellick" updated="2019-09-23 20:01:02.0"/>
<Action id="64113" issue="42133" author="steveliuu" type="comment" created="2019-09-25 08:54:42.0" updateauthor="steveliuu" updated="2019-09-25 08:54:42.0"> <body><! CDATA  ~jyellick Â Yes, I saw the binary data when channel creation is fine. It seems that the message didn't make it to kafka when creating channel failed to finish the process. I have uploaded the log of both situations.   ^consumer-fail.zip    ^consumer-success.zip   ></body> </Action>
<Action id="64132" issue="42133" author="jyellick" type="comment" body="I have a guess as to what may be the culprit here.  Your Kafka configuration specifies {{num.partitions=4}}, typically we would specify that {{num.partitions=1}}, as we require a total order across all messages (and messages distributed across partitions have no relative order).  The code _should_ be ensuring that the messages get sent to the right partition, but, given a lot of different factors, (which partitioning scheme is being used, which API style Sarama is using under the covers), so I think it&apos;s worth trying with that Kafka parameter tuned to 1." created="2019-09-25 16:36:04.0" updateauthor="jyellick" updated="2019-09-25 16:36:04.0"/>
<Action id="64218" issue="42133" author="steveliuu" type="comment" created="2019-10-01 02:36:53.0" updateauthor="steveliuu" updated="2019-10-01 02:36:53.0"> <body><! CDATA  ~jyellick Â Hi, it seems that didn't work. The channel still not built properly. Below are the exec command and k8s YAML of the orderer.  *Attachment*   ^kafka_sts.json  {code:java} letÂ execCommandÂ =Â "execÂ kafka-server-start.shÂ /opt/kafka/config/server.propertiesÂ --overrideÂ zookeeper.connect="Â +Â zooConnectÂ +Â "Â --overrideÂ default.replication.factor=1Â --overrideÂ Â min.insync.replicas=1Â --overrideÂ broker.id=${HOSTNAME##*-}Â Â --overrideÂ listeners=PLAINTEXT://:9092Â Â Â Â Â --overrideÂ log.dir=/var/lib/kafkaÂ Â Â --overrideÂ auto.create.topics.enable=trueÂ Â Â Â --overrideÂ auto.leader.rebalance.enable=trueÂ Â Â --overrideÂ background.threads=10Â Â Â Â --overrideÂ compression.type=producerÂ Â Â --overrideÂ delete.topic.enable=falseÂ Â Â --overrideÂ leader.imbalance.check.interval.seconds=300Â Â Â --overrideÂ leader.imbalance.per.broker.percentage=10Â Â Â Â --overrideÂ log.flush.interval.messages=9223372036854775807Â Â Â Â --overrideÂ log.flush.offset.checkpoint.interval.ms=60000Â Â Â Â --overrideÂ log.flush.scheduler.interval.ms=9223372036854775807Â Â Â --overrideÂ log.retention.bytes=-1Â Â Â --overrideÂ log.retention.hours=168Â Â Â --overrideÂ log.roll.hours=168Â Â Â --overrideÂ log.roll.jitter.hours=0Â Â --overrideÂ log.segment.bytes=1073741824Â Â Â --overrideÂ log.segment.delete.delay.ms=60000Â Â --overrideÂ message.max.bytes=1000012Â Â --overrideÂ num.io.threads=8Â Â Â --overrideÂ num.network.threads=3Â Â Â --overrideÂ num.recovery.threads.per.data.dir=1Â Â Â --overrideÂ num.replica.fetchers=1Â Â --overrideÂ offset.metadata.max.bytes=4096Â Â --overrideÂ offsets.commit.required.acks=-1Â Â --overrideÂ offsets.commit.timeout.ms=5000Â Â --overrideÂ offsets.load.buffer.size=5242880Â Â Â --overrideÂ offsets.retention.check.interval.ms=600000Â Â Â --overrideÂ offsets.retention.minutes=1440Â Â Â --overrideÂ offsets.topic.compression.codec=0Â Â Â --overrideÂ offsets.topic.num.partitions=50Â Â Â --overrideÂ offsets.topic.replication.factor=3Â Â --overrideÂ offsets.topic.segment.bytes=104857600Â Â --overrideÂ queued.max.requests=500Â Â --overrideÂ quota.consumer.default=9223372036854775807Â Â --overrideÂ quota.producer.default=9223372036854775807Â Â --overrideÂ replica.fetch.min.bytes=1Â Â --overrideÂ replica.fetch.wait.max.ms=500Â Â Â --overrideÂ replica.high.watermark.checkpoint.interval.ms=5000Â Â --overrideÂ replica.lag.time.max.ms=10000Â --overrideÂ replica.socket.receive.buffer.bytes=65536Â Â --overrideÂ replica.socket.timeout.ms=30000Â Â --overrideÂ request.timeout.ms=30000Â Â --overrideÂ socket.receive.buffer.bytes=102400Â Â --overrideÂ socket.request.max.bytes=104857600Â Â --overrideÂ socket.send.buffer.bytes=102400Â --overrideÂ unclean.leader.election.enable=trueÂ --overrideÂ zookeeper.session.timeout.ms=6000Â --overrideÂ zookeeper.set.acl=falseÂ --overrideÂ broker.id.generation.enable=trueÂ --overrideÂ connections.max.idle.ms=600000Â --overrideÂ controlled.shutdown.enable=trueÂ --overrideÂ controlled.shutdown.max.retries=3Â --overrideÂ controlled.shutdown.retry.backoff.ms=5000Â --overrideÂ controller.socket.timeout.ms=30000Â Â --overrideÂ fetch.purgatory.purge.interval.requests=1000Â Â --overrideÂ group.max.session.timeout.ms=300000Â --overrideÂ group.min.session.timeout.ms=6000Â --overrideÂ inter.broker.protocol.version=2.2.0Â --overrideÂ log.cleaner.backoff.ms=15000Â --overrideÂ log.cleaner.dedupe.buffer.size=134217728Â --overrideÂ log.cleaner.delete.retention.ms=86400000Â --overrideÂ log.cleaner.enable=trueÂ --overrideÂ log.cleaner.io.buffer.load.factor=0.9Â --overrideÂ log.cleaner.io.buffer.size=524288Â --overrideÂ log.cleaner.io.max.bytes.per.second=1.7976931348623157E308Â --overrideÂ log.cleaner.min.cleanable.ratio=0.5Â --overrideÂ log.cleaner.min.compaction.lag.ms=0Â --overrideÂ log.cleaner.threads=1Â --overrideÂ log.cleanup.policy=deleteÂ --overrideÂ log.index.interval.bytes=4096Â --overrideÂ log.index.size.max.bytes=10485760Â --overrideÂ log.message.timestamp.difference.max.ms=9223372036854775807Â --overrideÂ log.message.timestamp.type=CreateTimeÂ --overrideÂ log.preallocate=falseÂ --overrideÂ log.retention.check.interval.ms=300000Â --overrideÂ max.connections.per.ip=2147483647Â --overrideÂ num.partitions=1Â --overrideÂ producer.purgatory.purge.interval.requests=1000Â --overrideÂ replica.fetch.backoff.ms=1000Â --overrideÂ replica.fetch.max.bytes=1048576Â --overrideÂ replica.fetch.response.max.bytes=10485760Â --overrideÂ reserved.broker.max.id=1000" {code} Â   ></body> </Action>
<Action id="64337" issue="42133" author="guoger" type="comment" created="2019-10-08 07:13:02.0" updateauthor="guoger" updated="2019-10-08 07:13:21.0"> <body><! CDATA I'm seeing this error in the log you provided  {code}  2019-09-25 08:20:33,176  ERROR  KafkaApi-0  Number of alive brokers '2' does not meet the required replication factor '3' for the offsets topic (configured via 'offsets.topic.replication.factor'). This error can be ignored if the cluster is starting up and not all brokers are up yet. (kafka.server.KafkaApis) {code}  can you make sure all brokers are up and running before attempting channel creation?  ></body> </Action>
