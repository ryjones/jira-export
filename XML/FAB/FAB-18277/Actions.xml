<Action id="70534" issue="46217" author="denyeart" type="comment" created="2020-10-19 16:30:37.0" updateauthor="denyeart" updated="2020-10-19 16:35:36.0"> <body><! CDATA After a discussion on CouchDB slack, it appears that there is a new api option {{?buffer_response=true}} on view queries in v3.1.1 that ensures that the response is complete before returning back a 200. If not set the default is for the response to be streaming in which case a 200 is returned even while the response is streaming back (and may eventually abort).  So I think we have two options:  1) Fabric could update to v3.1.1 and add `buffer_response=true` to queries, to ensure that we get back accurate error codes (I think we need to do this anyways for validation path - batchRetrieveDocumentMetadata)  2) Fabric could take advantage of the streaming response and start processing the response before it is done. This would speed up snapshot generation when using CouchDB, but the Fabric logic would have to handle eventual 200 errors. And we wouldn't want the streaming behavior in others paths like validation.   ~manish-sethi  what do you think?  ></body> </Action>
<Action id="70535" issue="46217" author="JIRAUSER20954" type="comment" created="2020-10-19 17:27:26.0" updateauthor="JIRAUSER20954" updated="2020-10-19 17:27:26.0"> <body><! CDATA What is the impact to memory usage with this? The release note only mentions "Â increases memory usage on the nodes but simplifies error handling for the client" without any additional details. Will it hold the entire response in memory until it's received, or some other subset?  For non-trivial payloads this could be quite large. We've seen production chaincodes with 10's MB value payloads. For a the snapshot page size of 1000 records this could be many GB of memory if the entire response is held.  ></body> </Action>
<Action id="70537" issue="46217" author="denyeart" type="comment" created="2020-10-19 18:05:19.0" updateauthor="denyeart" updated="2020-10-19 18:17:46.0"> <body><! CDATA I expect when using buffered response, the entire response would be in memory (I believe it is anyways since we don't currently process streaming responses).  Note that the page size is already configurable using internalQueryLimit:   https://github.com/hyperledger/fabric/blob/master/sampleconfig/core.yaml#L640-L644   I agree we should call this out in docs and recommend consumers reduce it if they have large states.  And I agree that large payloads adds motivation to shift to streaming processing, at least for snapshot generation.  ></body> </Action>
