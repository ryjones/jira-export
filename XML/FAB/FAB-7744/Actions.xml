<Action id="38676" issue="26821" author="yacovm" type="comment" created="2018-01-16 11:56:13.0" updateauthor="yacovm" updated="2018-01-16 11:56:13.0"> <body><! CDATA If we could have multiple gRPC streams, and multiple goroutines handling them - that should (in my opinion) increase the endorsement throughput by hundreds of percentages.  What we need, IMO is just to have the chaincode shim open several connections to the peer and the peer would register them to the same chaincode.    ></body> </Action>
<Action id="49358" issue="26821" author="yacovm" type="comment" created="2018-08-24 19:24:30.0" updateauthor="yacovm" updated="2018-08-24 19:24:30.0"> <body><! CDATA {quote}The result shows these three techniques show about 30% better performance than the original one. {quote} 30% performance when doing what? Is this a micro-benchmark ? a macro-benchmark? What is the setup and evaluation being done?    {quote}we need to establish a fixed number of multiple GRPC streams between peer and chaincode in advance {quote} Back in the old days of the chaincode, before the massive refactoring that  ~sykesm  and  ~jyellick  did - I tried to do just that, but the change was too intrusive and the chaincode code itself was in a state that was too unmaintainable so I gave up, but - I think it's definitively the way to go: * Doesn't change API between the peer and the shim * Should be centered around specific areas    {quote}*Solution 2: Opposite TLS connection* {quote} The problem with this, is that the chaincode needs to be accessible and resolvable from the peer and you need to adjust the peer's code to every environment the chaincode container would run in (for example-  what if the DNS record of the chaincode container are only updated once it is created? what if there is a delay when doing that? etc.) , while in the existing way you don't need to worry about this because you know that the chaincode can always contact the peer.   Overall as you said - this requires a big amount of code change in fabric.    {quote}*Solution 3:  Reversal GRPC connection* {quote} Doesn't this require changing the protobuf API between the chaincode and the peer?  The fact that the chaincode initiates a TLS connection doesn't mean that the peer can initiate a connection to the chaincode via that connection, or at least this would require some massive gRPC hacking, no?        ></body> </Action>
<Action id="49398" issue="26821" author="yoheiueda" type="comment" created="2018-08-27 09:29:41.0" updateauthor="yoheiueda" updated="2018-08-27 09:29:41.0"> <body><! CDATA {quote}30% performance when doing what? Is this a micro-benchmark ? a macro-benchmark? What is the setup and evaluation being done? {quote} I did the experiment with a proprietary code.  I think I need to do the same experiments with the latest refactored code and PTE. {quote}I tried to do just that, but the change was too intrusive and the chaincode code itself was in a state that was too unmaintainable so I gave up, but - I think it's definitively the way to go: * Doesn't change API between the peer and the shim * Should be centered around specific areas{quote} I understand your points.  I agree that adopting multiple GRPC streams meets these two points. {quote}Doesn't this require changing the protobuf API between the chaincode and the peer? {quote} The protobuf API will change with this approach. {quote}The fact that the chaincode initiates a TLS connection doesn't mean that the peer can initiate a connection to the chaincode via that connection, or at least this would require some massive gRPC hacking, no? {quote} We can start an GRPC connection using an existing established TLS connection via the {{grpc.WithDialer }}option.  This code spinet shows how it works:   https://github.com/yoheiueda/grpc-test/blob/master/reversal.go#L56-L60   Then, we can initiate a new GRPC stream per a invoke transaction.  I believe this change will simply chaicnode-related code. For example, if we maintain each transaction with one GRPC stream, we don't need to maintain mapping between transaction ID and transaction context, so we can also remove the mutex lock for it.  However, the amount of required code changes of this approach will be large.     I will rethink how to implement Solution 1 first with the latest refactored code, and estimate possible performance gains.  ></body> </Action>
<Action id="56712" issue="26821" author="yoheiueda" type="comment" created="2019-02-06 08:46:32.0" updateauthor="yoheiueda" updated="2019-02-06 08:46:32.0"> <body><! CDATA The phase 2 design proposed in  FAB-13582  uses the opposite TLS connection described in Solution 2 showed in this JIRA issue.  I think this lock contention issue should be addressed with  FAB-13582 . When   FAB-13582  is implemented, we will be able consider the Solution 2 approach.  ></body> </Action>
<Action id="67213" issue="26821" author="sykesm" type="comment" body="Stale" created="2020-01-22 22:23:06.0" updateauthor="sykesm" updated="2020-01-22 22:23:06.0"/>
