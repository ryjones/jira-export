<Issue id="36402" key="FAB-13442" number="13442" project="10002" reporter="yacovm" assignee="yacovm" creator="yacovm" type="10003" summary="Do not panic upon inability to pull channels " priority="3" resolution="10000" status="6" created="2018-12-25 18:32:35.0" updated="2019-04-29 09:47:47.0" resolutiondate="2019-04-29 09:47:47.0" votes="0" watches="3" workflowId="47934"> <description><! CDATA Consider the following use case:  A channel is created and then the organizations decide they don't want it anymore: * A channel is created with an improper name and is then created again with the proper name, and the first channel is never used * A channel might represent some business operation, and that business operation may cease, i.e - all organizations of that channel leave the channel.  In such a case, the operators of the raft cluster might decide to perform a series of config updates that remove all consenters from the channel. * When a running OSN is removed from a channel, its chain object ceases operation and should return always* service not available*. * When an OSN is booted up and it sees that it is not a member of a channel, it runs a special chain that always returns *service unavailable* to any request.  Now, the problem is the following:  Let's say that some new OSN instance joins the cluster. It: # Pulls the system channel to see what channels exist # For each channel, it probes all the consenters of the system channel, to retrieve the last config block of the channel in order to check whether it is among the consenters of the channel or not. ## If it gets unauthorized from all *reachable consenters* (we classify service_unavailable in the same way as an unreachable consenter while pulling blocks, because that OSN itself might not be able to reach the majority of the cluster...) it deduces that it shouldn't pull the channel. ## If it cannot reach any system channel consenter or all consenters return *service unavailable*, it has a problem, because it cannot distinguish between the following 2 scenarios: ### All consenters that service the application channel are in the middle of leader election, or it could be that the application channel is serviced by consenters that are currently offline, and the remaining consenters alive are the ones that don't service the application channel. ### All consenters of the system channel, do not service this channel at all.  A way distinguish between the 2, is to ensure that: # *All consenters* in the system channel respond (no one is unreachable) to the attempt to pull blocks from the application channel in question. # All consenters in the system channel unanimously return that they do not service the channel.   Meaning, that if at least one consenter is unreachable, and the rest return that they do not service the channel - we do not proceed to skip pulling that channel. This is problematic, since one of the reasons for onboarding a new OSN might be because some OSN is dead for good due to a fire, flood, etc.  Obviously, this is also a problem when we consider the byzantine model where a consenter may lie, thus it can always return service unavailable with any reason to an onboarded OSN.  Another option would be to ensure that a channel can never be deserted completely - if we enforce that config updates never remove the last consenter. However this is a problem too - what if the last consenter, is unavailable right now?   Therefore, I see that we don't really have a choice but to make onboarding skip channels that cannot be pulled because of service unavailable, and that could be taken care of via FAB-13180 which ensures we can onboard existing OSNs to channels, via an API call on the OSN while it is running.  Thus I see - FAB-13180 as being crucial to even cases where you bring up a completely fresh OSN, since a temporary failure of a chain needs to be addressed.   ></description> </Issue>
