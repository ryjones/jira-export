<Action id="41303" issue="28262" author="denyeart" type="comment" created="2018-03-07 15:21:35.0" updateauthor="denyeart" updated="2018-03-07 15:21:35.0"> <body><! CDATA  ~john.d.sheehan  Are you suggesting that non-anchors in your network do not demonstrate this problem?   ~yacovm  What other information would help narrow down the problem?  ></body> </Action>
<Action id="41312" issue="28262" author="yacovm" type="comment" body="give me a stack trace of all goroutines please, on both ends of the connection after half the time they die. " created="2018-03-07 15:36:22.0" updateauthor="yacovm" updated="2018-03-07 15:36:22.0"/>
<Action id="41313" issue="28262" author="yacovm" type="comment" body="you can do that by killing the peer with a SIGABRT signal" created="2018-03-07 15:36:49.0" updateauthor="yacovm" updated="2018-03-07 15:36:49.0"/>
<Action id="41315" issue="28262" author="john.d.sheehan" type="comment" created="2018-03-07 15:39:09.0" updateauthor="john.d.sheehan" updated="2018-03-07 15:39:09.0"> <body><! CDATA  ~denyeart  it appears non-anchors are not effected, currently,  : fabric@mem0-peer0Org0 0 %; sudo lsof -i -P |grep fabric |wc -l 9985  : fabric@mem0-peer1Org0 0 %; sudo lsof -i -P |grep fabric |wc -l 8  : fabric@mem1-peer0Org1 0 %; sudo lsof -i -P |grep fabric |wc -l 9995  : fabric@mem1-peer1Org1 0 %; sudo lsof -i -P |grep fabric |wc -l 7     and  : fabric@mem0-peer0Org0 0 %; free -m               total        used        free      shared  buff/cache   available Mem:           3999        1725         173          40        2100        2121 Swap:          2047           0        2047    : fabric@mem0-peer1Org0 0 %; free -m               total        used        free      shared  buff/cache   available Mem:           3999         165        1894          40        1938        3714 Swap:          2047           0        2047    : fabric@mem1-peer0Org1 0 %; free -m               total        used        free      shared  buff/cache   available Mem:           3999        1492         403          25        2103        2370 Swap:          2047           0        2047    : fabric@mem1-peer1Org1 0 %; free -m               total        used        free      shared  buff/cache   available Mem:           3999         143        1919          40        1936        3736 Swap:          2047           0        2047  ></body> </Action>
<Action id="41320" issue="28262" author="yacovm" type="comment" body="Do you have a load balancer somewhere?" created="2018-03-07 15:48:58.0" updateauthor="yacovm" updated="2018-03-07 15:48:58.0"/>
<Action id="41321" issue="28262" author="john.d.sheehan" type="comment" created="2018-03-07 15:55:01.0" updateauthor="john.d.sheehan" updated="2018-03-07 15:55:01.0"> <body><! CDATA While the stack trace is still pasting, I grabbed this earlier,  go tool pprof  --nodecount=1000 --nodefraction=0.00001  -png http://localhost:6060/debug/pprof/heap > peer0org0-3hr.png !peer0org0-3hr.png!  ></body> </Action>
<Action id="41323" issue="28262" author="yacovm" type="comment" body="i think i found the bug... will upload a change set in a few minutes" created="2018-03-07 15:58:03.0" updateauthor="yacovm" updated="2018-03-07 16:03:07.0"/>
<Action id="41324" issue="28262" author="john.d.sheehan" type="comment" body=" ~yacovm  re:loadbalancer, well the vms are in a datacenter, but they are all on the same subnet, so I don&apos;t know. I didn&apos;t add one." created="2018-03-07 15:58:39.0" updateauthor="john.d.sheehan" updated="2018-03-07 15:58:39.0"/>
<Action id="41325" issue="28262" author="john.d.sheehan" type="comment" body=" ~yacovm  you found the bug quicker than it takes to copy and paste the stack trace into a terminal :)" created="2018-03-07 16:01:39.0" updateauthor="john.d.sheehan" updated="2018-03-07 16:01:39.0"/>
<Action id="41326" issue="28262" author="yacovm" type="comment" body="I think this should fix it https://gerrit.hyperledger.org/r/#/c/18819/" created="2018-03-07 16:02:48.0" updateauthor="yacovm" updated="2018-03-07 16:02:48.0"/>
<Action id="41328" issue="28262" author="john.d.sheehan" type="comment" created="2018-03-07 16:21:33.0" updateauthor="john.d.sheehan" updated="2018-03-07 16:21:33.0"> <body><! CDATA still showing these auth failures,  createConnection to peer0.org1.example.com:7051 createConnection success to  peer0.org1.example.com:7051 createConnection to peer0.org1.example.com:7051 auth failure, remote endpoint  peer0.org1.example.com:7051 claims to be a different peer, expected  28 168 154 200 135 17 141 147 116 30 254 241 114 34 94 42 19 98 57 55 96 91 159 124 26 242 173 168 199 223 113 60  but got  199 247 88 142 244 39 77 203 186 217 71 34 65 75 216 156 48 90 254 79 57 228 122 225 87 162 62 240 251 29 12 86  Probe Probe createConnection to peer0.org1.example.com:7051 createConnection to peer0.org1.example.com:7051 auth failure, remote endpoint  peer0.org1.example.com:7051 claims to be a different peer, expected  28 168 154 200 135 17 141 147 116 30 254 241 114 34 94 42 19 98 57 55 96 91 159 124 26 242 173 168 199 223 113 60  but got  199 247 88 142 244 39 77 203 186 217 71 34 65 75 216 156 48 90 254 79 57 228 122 225 87 162 62 240 251 29 12 86  createConnection success to  peer0.org1.example.com:7051     but the open file descriptor count seems stable,  : fabric@mem0-peer0Org0 0 %; sudo lsof -i -P |grep fabric  peer      30017 fabric   26u  IPv6 411410      0t0  TCP *:7051 (LISTEN) peer      30017 fabric   27u  IPv6 411411      0t0  TCP *:7053 (LISTEN) peer      30017 fabric   28u  IPv4 411412      0t0  TCP peer0.org0.example.com:7052 (LISTEN) peer      30017 fabric   29u  IPv6 413102      0t0  TCP *:6060 (LISTEN) peer      30017 fabric   30u  IPv4 413948      0t0  TCP peer0.org0.example.com:60114->orderer0.example.com:7050 (ESTABLISHED) peer      30017 fabric   37u  IPv4 415937      0t0  TCP peer0.org0.example.com:7052->peer0.org0.example.com:52504 (ESTABLISHED) peer      30017 fabric   38u  IPv6 413991      0t0  TCP peer0.org0.example.com:7051->peer1.org0.example.com:36562 (ESTABLISHED) peer      30017 fabric   39u  IPv4 414307      0t0  TCP peer0.org0.example.com:58806->peer0.org1.example.com:7051 (ESTABLISHED)  ></body> </Action>
<Action id="41329" issue="28262" author="yacovm" type="comment" created="2018-03-07 16:49:04.0" updateauthor="yacovm" updated="2018-03-07 16:49:04.0"> <body><! CDATA yep, this was supposed to fix the file descriptor issue, not the auth failures.  Could you please tell me if the memory consumption is still increasing ?  ></body> </Action>
<Action id="41330" issue="28262" author="john.d.sheehan" type="comment" body=" ~yacovm  memory consumption seems stable. I&apos;ll let it run overnight and update then." created="2018-03-07 16:55:23.0" updateauthor="john.d.sheehan" updated="2018-03-07 16:55:23.0"/>
<Action id="41339" issue="28262" author="denyeart" type="comment" body="Merged in 1.1 and 1.2 (master).  Awaiting unrelated CI fix in 1.0.x branch." created="2018-03-07 20:54:01.0" updateauthor="denyeart" updated="2018-03-07 20:54:01.0"/>
<Action id="41351" issue="28262" author="john.d.sheehan" type="comment" created="2018-03-08 08:30:10.0" updateauthor="john.d.sheehan" updated="2018-03-08 08:30:10.0"> <body><! CDATA  ~yacovm  memory consumption seems stable,  : fabric@mem0-peer0Org0 0 %; while   1   ; do echo $(date); free -m; echo "--"; sleep 3600; done Wed Mar 7 18:02:37 UTC 2018               total        used        free      shared  buff/cache   available Mem:           3999         172        1657          40        2169        3700 Swap:          2047           0        2047 -- Wed Mar 7 19:02:37 UTC 2018               total        used        free      shared  buff/cache   available Mem:           3999         176        1652          40        2169        3696 Swap:          2047           0        2047 --  ...  Thu Mar 8 07:02:37 UTC 2018               total        used        free      shared  buff/cache   available Mem:           3999         233        1582          40        2183        3638 Swap:          2047           0        2047 -- Thu Mar 8 08:02:37 UTC 2018               total        used        free      shared  buff/cache   available Mem:           3999         238        1576          40        2184        3633 Swap:          2047           0        2047       And the other anchor peer shows something similar  ></body> </Action>
<Action id="41352" issue="28262" author="yacovm" type="comment" created="2018-03-08 08:42:36.0" updateauthor="yacovm" updated="2018-03-08 08:42:36.0"> <body><! CDATA Happy to hear that. Thanks for the catch, that bug has been there for around a year and no one noticed. This one https://gerrit.hyperledger.org/r/#/c/18823/ is still waiting to be merged to v1.0,  ~mastersingh24  - can we override the CI and merge it?   ~john.d.sheehan  - as for the "claims to be a different peer" - it's most likely a configuration issue, as this is by design. Please contact me via rocket chat and we'll solve your configuration issue.   ></body> </Action>
<Action id="41354" issue="28262" author="john.d.sheehan" type="comment" created="2018-03-08 11:42:33.0" updateauthor="john.d.sheehan" updated="2018-03-08 11:42:33.0"> <body><! CDATA The auth failure message was due to a misconfiguration of the CORE_PEER_GOSSIP_EXTERNALENDPOINT and CORE_PEER_GOSSIP_BOOTSTRAP variables by me. With patch 18823, everything is working as expected.  ></body> </Action>
