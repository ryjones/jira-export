<Issue id="28262" key="FAB-8688" number="8688" project="10002" reporter="john.d.sheehan" assignee="yacovm" creator="john.d.sheehan" type="10004" summary="fabric anchor peer consumes all memory and dies" priority="3" resolution="10000" status="6" created="2018-03-07 11:19:15.0" updated="2018-07-20 14:15:56.0" resolutiondate="2018-03-09 14:01:04.0" votes="0" watches="4" workflowId="41458"> <description><! CDATA I have the following (simplified) configuration, 5vms  vm0: orderer (solo)  vm1: peer0.org0 (anchor peer)  vm2: peer1.org0  vm3: peer0.org1 (anchor peer)  vm4: peer1.org1     After I create/fetch, join, update, install, and instantiate a chaincode, the anchor peers slowly consume all the memory on their vms. They also exhaust all the file descriptors.     10:49 AM ~/pkg-cli0     : fabric@mem0-peer0Org0 0 %; sudo lsof -i -P |grep fabric  sudo  password for fabric:  sshd       8333 fabric    3u  IPv4 305300      0t0  TCP 59.27.089f.ip4.static.sl-reverse.com:22->gbibp9ph1--blueice3n1.emea.ibm.com:50174 (ESTABLISHED) peer       8371 fabric   26u  IPv6 297874      0t0  TCP *:7051 (LISTEN) peer       8371 fabric   27u  IPv6 297875      0t0  TCP *:7053 (LISTEN) peer       8371 fabric   28u  IPv4 297876      0t0  TCP peer0.org0.example.com:7052 (LISTEN) peer       8371 fabric   29u  IPv6 293829      0t0  TCP *:6060 (LISTEN) peer       8371 fabric   30u  IPv4 300279      0t0  TCP peer0.org0.example.com:39694->orderer0.example.com:7050 (ESTABLISHED) peer       8371 fabric   32u  IPv4 303776      0t0  TCP peer0.org0.example.com:37976->peer0.org1.example.com:7051 (ESTABLISHED) peer       8371 fabric   37u  IPv4 300296      0t0  TCP peer0.org0.example.com:7052->peer0.org0.example.com:60316 (ESTABLISHED) peer       8371 fabric   38u  IPv6 306403      0t0  TCP peer0.org0.example.com:7051->peer0.org1.example.com:49664 (ESTABLISHED) peer       8371 fabric   39u  IPv6 303785      0t0  TCP peer0.org0.example.com:7051->peer0.org1.example.com:49638 (ESTABLISHED) peer       8371 fabric   40u  IPv4 301705      0t0  TCP peer0.org0.example.com:37982->peer0.org1.example.com:7051 (ESTABLISHED) peer       8371 fabric   41u  IPv4 303791      0t0  TCP peer0.org0.example.com:37984->peer0.org1.example.com:7051 (ESTABLISHED) peer       8371 fabric   42u  IPv6 303793      0t0  TCP peer0.org0.example.com:7051->peer0.org1.example.com:49646 (ESTABLISHED) peer       8371 fabric   43u  IPv6 303795      0t0  TCP peer0.org0.example.com:7051->peer0.org1.example.com:49648 (ESTABLISHED) peer       8371 fabric   44u  IPv6 303798      0t0  TCP peer0.org0.example.com:7051->peer0.org1.example.com:49650 (ESTABLISHED) peer       8371 fabric   45u  IPv6 303801      0t0  TCP pe ....     After about an hour,  : fabric@mem0-peer0Org0 0 %; sudo lsof -i -P |grep fabric |wc -l 1563     The logs from the peers are populated with the following,  createConnection to peer0.org0.example.com:7051 createConnection to peer0.org0.example.com:7051 createConnection success to  peer0.org0.example.com:7051 auth failure, remote endpoint  peer0.org0.example.com:7051 claims to be a different peer, expected  249 210 188 90 154 10 219 131 93 163 0 179 80 44 135 146 114 130 247 120 98 96 25 183 81 84 152 173 122 24 168 40  but got  158 223 71 133 65 188 150 122 216 109 203 141 72 228 30 37 164 247 117 59 199 140 85 142 236 105 225 76 205 115 251 234  createConnection to peer0.org0.example.com:7051 auth failure, remote endpoint  peer0.org0.example.com:7051 claims to be a different peer, expected  249 210 188 90 154 10 219 131 93 163 0 179 80 44 135 146 114 130 247 120 98 96 25 183 81 84 152 173 122 24 168 40  but got  158 223 71 133 65 188 150 122 216 109 203 141 72 228 30 37 164 247 117 59 199 140 85 142 236 105 225 76 205 115 251 234  createConnection to peer0.org0.example.com:7051 createConnection success to  peer0.org0.example.com:7051 createConnection to peer0.org0.example.com:7051  This comes from `fabric/gossip/comm/comm_impl.go:createConnection`     I'm not sure it is a configuration issue (firewall, /etc/hosts, core.yaml, configtx.yaml, crypto-config.yaml, orderer.yaml, etc) on my end, since both invoke and query seem to work as expected     ></description> </Issue>
