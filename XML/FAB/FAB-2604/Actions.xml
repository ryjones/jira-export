<Action id="21282" issue="15171" author="scottz" type="comment" created="2017-03-02 18:08:17.0" updateauthor="scottz" updated="2017-03-02 18:08:17.0"> <body><! CDATA Attached test output logs fail_PlusExtraTX_ORD97.txt, where you can see a block delivered at 17:54:06 containing less than 10 TXs, approx 11 seconds after the last kafka-broker was restarted (and 30 seconds after the 2nd one was restarted). The funny thing is that (refer to FAB-2575) there are no more transactions being acknowledged by the orderers after 17:52:55, when the service became unavailable. So this blocknyumber 4106 with 4 Transactions must have been sitting in the kafka-brokers when they were stopped and only got delivered after the KBs recovered.  Note this (extra/duplicate TX delivered) does not happen every time. Sometimes the orderers all stay in sync and all deliver the same number of transactions that were ACKd.  ></body> </Action>
<Action id="25293" issue="15171" author="suryalnvs" type="comment" body="I tried the above scenario multiple times using *OTE testname=ORD-97 TX=100000 Channels=1 Orderers=3 ordererType=kafka kafka-brokers=3 oteSpy=OFF producersPerCh=1* on commit *3b40efaee8584fd2662c59c0f7fc5a1a18c38f58*, Date: Fri Jun 2 03:00:52 2017 +0000 in vagrant and TLS enabled. I can see an extra transaction being delivered described as above. Also attaching the log for reference." created="2017-06-04 23:20:50.0" updateauthor="suryalnvs" updated="2017-06-04 23:21:27.0"/>
<Action id="25304" issue="15171" author="kchristidis" type="comment" created="2017-06-05 03:44:54.0" updateauthor="kchristidis" updated="2017-06-05 04:17:55.0"> <body><! CDATA Thanks for re-running this. I have the following questions: # What is your setup? Can you please attach your docker-compose.yml file? You bring down 3 brokers and the log says "All the kafka brokers are restarted" but the default configuration we should be using employs 4 brokers. Why the difference? # You send 500K transactions to the ordering service. Do you wait for a SUCCESS response before sending the next transaction? (Not implying you should, but trying to get a better sense of _how_ the test works.) # I'll also need an almost line-by-line explanation of the attached log. Which lines in the log are considered OK, and which are normal? Is "Producer-o2-c0 failed to broadcast TX 19488 (ACK=19488 NACK=1), 2017-06-04 23:07:16.629858188 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE" an expected line? I don't know. What does "Producer-o2-c0 broadcast error delay period (120) ended (ACK=19488 NACK=120), 2017-06-04 23:09:16.776974927 +0000 UTC" mean? What does "Broadcast error on last TX 166666 of Producer-o2-c0: EOF" mean? What does "Producer-o2-c0 finished sending broadcast msgs: ACKs 19488 NACK 147178 (errDelayCntr 120) Other 0 , 2017-06-04 23:09:27.425772331 +0000 UTC" mean? # Does each transaction carry a unique payload? If not, can we make it unique? And then, can we modify the test so that it returns a list of the ACK'd transactions? This way we can compare that list with the list retrieved by the orderers (via a Deliver RPC with start = oldest and stop = newest) and quickly identify the extra transaction. # Related to the above, the title refers to this extra transaction as "redelivered". How are we certain that this is the case? Do we already use a method such as the one described in step 4? If not, it may well be a transaction from the NACK set, or a Kafka-internal CONNECT or TTX transaction that gets packaged into the blocks when it shouldn't.  Given the difficulty of reproducing this, it is important that we get detailed logs from the faulty run, by setting `ORDERER_KAFKA_VERBOSE=true` as in the  preferred configuration files|https://github.com/hyperledger/fabric/blob/master/bddtests/dc-orderer-base.yml#L17  and following the suggestion in step 4. Please also attach the logs from every ordering service node used in this run.  ></body> </Action>
<Action id="25432" issue="15171" author="suryalnvs" type="comment" body=" ~kchristidis  Attached the docker-compose files, kafka3 and orderer0 logs with kafka verbose set to true." created="2017-06-05 20:46:44.0" updateauthor="suryalnvs" updated="2017-06-05 20:46:44.0"/>
<Action id="25435" issue="15171" author="kchristidis" type="comment" body=" ~suryalnvs : Thank you for this. Can you please have a look at the questions I posted above, and let me know what the answers are?" created="2017-06-05 20:53:27.0" updateauthor="kchristidis" updated="2017-06-05 20:53:27.0"/>
<Action id="25439" issue="15171" author="kchristidis" type="comment" body="Also, can you please provide the orderer logs for all OSNs?" created="2017-06-05 21:00:33.0" updateauthor="kchristidis" updated="2017-06-05 21:00:33.0"/>
<Action id="25459" issue="15171" author="scottz" type="comment" created="2017-06-06 01:41:08.0" updateauthor="scottz" updated="2017-06-06 01:41:08.0"> <body><! CDATA   1. docker-compose.yml file was attached today. We reran the test and obtained new logs using VERBOSE and 4 or 5 kafka brokers. Surya can explain exactly the details, if you would like to sit and review together.   2. Producer client sends a msg, waits for a TX Ack (i.e. not a delivery), and proceeds to next one.   3. We can get together tomorrow and explain it. Agree it can be confusing at first. When we take down an orderer to which a producer is connected, that producer will see NACKs counted thereafter until it finishes sending the planned number of transactions. (It slows down to 1 per second for a couple minutes, to give the restarted orderer or KB a chance to recover and have possible success in future transactions; otherwise it zips through the rest of its allotment too quickly.) The 503 is when kafkabrokers are not available, and also leads to a NACK, although in this case the orderer is still working fine (and knows there are not enough KBs to provide a service).   4.Yes, each transaction includes a string with the producer/orderer/channel and a timestamp. That could be a huge number of transactions to dump for each orderer. Do you really need to see ACKd transactions? I would think you want to see the Delivered blocks of transactions, and just check them for duplicates.   5. We are not certain; we are just counting delivered TXs. How could orderer deliver a NACK'd transaction? If you know what the kafka-internal transactions differ from ours, we might be able to quickly search for them too.  ></body> </Action>
<Action id="25566" issue="15171" author="suryalnvs" type="comment" created="2017-06-06 19:45:47.0" updateauthor="suryalnvs" updated="2017-06-06 19:45:47.0"> <body><! CDATA I reran the above scenario multiple times with the modifications suggested to use 4 KBs and set MIN_ISR=2. The goal is still to stop enough KBs such that the network halts for a time period, in this case we stop 3 out of the 4.  So today we were using *OTE testname=ORD-97 TX=500000 Channels=1 Orderers=3 ordererType=kafka kafka-brokers=4 oteSpy=OFF producersPerCh=1* on commit *c50e0dd1ca1ea96cb69503fc83a302c53eff96a6**,* ***Date: Tue Jun 6 13:36:08 2017 +0000* in vagrant and TLS enabled.  The test is to basically restart the 3 kafka brokers out of 4 when sending the traffic to orderers (kafka_min_insync=2). The restart of the kafka brokers is done using the command "{color:#14892c}docker stop kafka0 && sleep 20 && docker stop kafka1 && sleep 20 && docker restart kafka2 && sleep 20 && docker start kafka1 && sleep 20 && docker start kafka0{color}" among the available 4 kafka brokers and observed successful delivery of all the transactions to orderers and batching up transactions into blocks in the orderers. The test is able to pass successfully and this issue is not reproducible.       ></body> </Action>
<Action id="25569" issue="15171" author="kchristidis" type="comment" body="Thank you both for your help in figuring this out. Closing this, but if you see something fishy, feel free to reopen." created="2017-06-06 19:56:25.0" updateauthor="kchristidis" updated="2017-06-06 19:56:25.0"/>
<Action id="27162" issue="15171" author="scottz" type="comment" body="Reproduced again today using beta+ load. As suggested previously, we will proceed to try to identify the extra transactions, to confirm if they are actually extra or redelivered transactions or if they are some other thing causing counts to be incorrect." created="2017-06-21 19:31:21.0" updateauthor="scottz" updated="2017-06-21 19:31:21.0"/>
<Action id="27165" issue="15171" author="suryalnvs" type="comment" created="2017-06-21 21:32:48.0" updateauthor="suryalnvs" updated="2017-06-21 21:32:48.0"> <body><! CDATA ubuntu@hyperledger-devenv:307c903:/opt/gopath/src/github.com/hyperledger/fabric/test/tools/OTE$ *go test -run ORD97 -timeout 120m* ========== ========== OTE testname=ORD-97 TX=500000 Channels=1 Orderers=3 ordererType=kafka kafka-brokers=4 oteSpy=DEFER producersPerCh=1 OTE_MASTERSPY=<ignored>                            masterSpy=DEFER OTE_SPY_ORDERER=                                   spyOrd=0 CONFIGTX_ORDERER_BATCHSIZE_MAXMESSAGECOUNT=        batchSize=500 CONFIGTX_ORDERER_BATCHTIMEOUT=                     batchTimeout=30 CORE_LOGGING_LEVEL= CORE_LEDGER_STATE_STATEDATABASE= Launching network:  cd ../../../examples/fabric-docker-compose-svt && ./network_setup.sh -s -n 1 Results of exec command: docker ps -a stdout=CONTAINER ID        IMAGE                          COMMAND                  CREATED             STATUS                              PORTS                                              NAMES 85268eb965cf        hyperledger/fabric-testenv     "/bin/bash -c 'sle..."   47 seconds ago      Exited (0) Less than a second ago                                                      cli 2d78b9204bae        hyperledger/fabric-orderer     "orderer"                48 seconds ago      Up 47 seconds                       0.0.0.0:9050->7050/tcp                             orderer2.example.com 94d4ca1d6047        hyperledger/fabric-orderer     "orderer"                48 seconds ago      Up 47 seconds                       0.0.0.0:7050->7050/tcp                             orderer0.example.com d74c63593c10        hyperledger/fabric-orderer     "orderer"                48 seconds ago      Up 47 seconds                       0.0.0.0:8050->7050/tcp                             orderer1.example.com fea725a956b7        hyperledger/fabric-kafka       "/docker-entrypoin..."   50 seconds ago      Up 49 seconds                       9093/tcp, 0.0.0.0:9094->9092/tcp                   kafka1 5934f071f92f        hyperledger/fabric-kafka       "/docker-entrypoin..."   50 seconds ago      Up 49 seconds                       9093/tcp, 0.0.0.0:9096->9092/tcp                   kafka2 e0a02835dca1        hyperledger/fabric-kafka       "/docker-entrypoin..."   50 seconds ago      Up 49 seconds                       0.0.0.0:9092->9092/tcp, 9093/tcp                   kafka0 2835dd643611        hyperledger/fabric-kafka       "/docker-entrypoin..."   50 seconds ago      Up 48 seconds                       9093/tcp, 0.0.0.0:9098->9092/tcp                   kafka3 0523b6a2361f        hyperledger/fabric-peer        "peer node start -..."   53 seconds ago      Up 50 seconds                       0.0.0.0:9051->7051/tcp, 0.0.0.0:9053->7053/tcp     peer0.org2.example.com 19be3bde454d        hyperledger/fabric-peer        "peer node start -..."   53 seconds ago      Up 50 seconds                       0.0.0.0:8051->7051/tcp, 0.0.0.0:8053->7053/tcp     peer1.org1.example.com d2c7c705bfbf        hyperledger/fabric-peer        "peer node start -..."   53 seconds ago      Up 51 seconds                       0.0.0.0:7051->7051/tcp, 0.0.0.0:7053->7053/tcp     peer0.org1.example.com ec946d519f69        hyperledger/fabric-ca          "sh -c 'fabric-ca-..."   53 seconds ago      Up 50 seconds                       0.0.0.0:8054->7054/tcp                             ca_peerOrg2 4a16752c9542        hyperledger/fabric-peer        "peer node start -..."   53 seconds ago      Up 49 seconds                       0.0.0.0:10051->7051/tcp, 0.0.0.0:10053->7053/tcp   peer1.org2.example.com 60a70a7a0ab1        hyperledger/fabric-zookeeper   "/docker-entrypoin..."   53 seconds ago      Up 51 seconds                       2181/tcp, 2888/tcp, 3888/tcp                       zookeeper2 5b2d7e8c24f5        hyperledger/fabric-zookeeper   "/docker-entrypoin..."   53 seconds ago      Up 52 seconds                       2181/tcp, 2888/tcp, 3888/tcp                       zookeeper0 8f90cfb1eeb6        hyperledger/fabric-zookeeper   "/docker-entrypoin..."   53 seconds ago      Up 52 seconds                       2181/tcp, 2888/tcp, 3888/tcp                       zookeeper1 77b79a0a911a        hyperledger/fabric-ca          "sh -c 'fabric-ca-..."   53 seconds ago      Up 52 seconds                       0.0.0.0:7054->7054/tcp                             ca_peerOrg1  Using 1 new channelIDs, e.g. testchan321 Finished creating all CONSUMERS clients Producer-o1-c0 successfully broadcast TX 0 (ACK=1 NACK=0), 2017-06-21 21:09:57.705905229 +0000 UTC Producer-o2-c0 successfully broadcast TX 0 (ACK=1 NACK=0), 2017-06-21 21:09:57.71177524 +0000 UTC Producer-o0-c0 successfully broadcast TX 0 (ACK=1 NACK=0), 2017-06-21 21:09:57.736477814 +0000 UTC stop all the Kafka brokers, 0 1 2 3, 2017-06-21 21:10:29.658083787 +0000 UTC *docker stop kafka0 && sleep 20 && docker stop kafka1 && sleep 20 && docker start kafka1 && sleep 20 && docker start kafka0* Producer-o0-c0 failed to broadcast TX 1788 (ACK=1788 NACK=1), 2017-06-21 21:10:56.396022608 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE Producer-o1-c0 failed to broadcast TX 2419 (ACK=2419 NACK=1), 2017-06-21 21:10:56.399470933 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE Producer-o2-c0 failed to broadcast TX 2330 (ACK=2330 NACK=1), 2017-06-21 21:10:56.400266684 +0000 UTC, err: Got unexpected status: SERVICE_UNAVAILABLE All the kafka brokers are restarted, 2017-06-21 21:11:37.156081639 +0000 UTC ===== Test calling startMasterSpy on orderer  1  at  2017-06-21 21:12:34.658159242 +0000 UTC === startSpyDefer proceeding to startConsumerMaster to spy on orderer1 2017-06-21 21:12:34.675 UTC  msp  getMspConfig -> INFO 013 intermediate certs folder not found at  ../../../examples/fabric-docker-compose-svt/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/intermediatecerts . Skipping.:  stat ../../../examples/fabric-docker-compose-svt/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/intermediatecerts: no such file or directory  2017-06-21 21:12:34.676 UTC  msp  getMspConfig -> INFO 014 crls folder not found at  ../../../examples/fabric-docker-compose-svt/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/intermediatecerts . Skipping.:  stat ../../../examples/fabric-docker-compose-svt/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/crls: no such file or directory  2017-06-21 21:12:34.676 UTC  msp  getMspConfig -> INFO 015 MSP configuration file not found at  ../../../examples/fabric-docker-compose-svt/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/config.yaml :  stat ../../../examples/fabric-docker-compose-svt/crypto-config/peerOrganizations/org1.example.com/peers/peer0.org1.example.com/msp/config.yaml: no such file or directory  Producer-o1-c0 broadcast error delay period (120) ended (ACK=2419 NACK=120), 2017-06-21 21:12:56.512982144 +0000 UTC Producer-o0-c0 broadcast error delay period (120) ended (ACK=1788 NACK=120), 2017-06-21 21:12:56.513045431 +0000 UTC Producer-o2-c0 broadcast error delay period (120) ended (ACK=2330 NACK=120), 2017-06-21 21:12:56.513295731 +0000 UTC Broadcast error on last TX 166666 of Producer-o2-c0: EOF Producer-o2-c0 finished sending broadcast msgs: ACKs       2330  NACK 164336 (errDelayCntr 120)  Other 0 , 2017-06-21 21:13:14.391939496 +0000 UTC Broadcast error on last TX 166666 of Producer-o1-c0: EOF Producer-o1-c0 finished sending broadcast msgs: ACKs       2419  NACK 164247 (errDelayCntr 120)  Other 0 , 2017-06-21 21:13:14.84969967 +0000 UTC Broadcast error on last TX 166668 of Producer-o0-c0: EOF Producer-o0-c0 finished sending broadcast msgs: ACKs       1788  NACK 164880 (errDelayCntr 120)  Other 0 , 2017-06-21 21:13:14.977187382 +0000 UTC Send Duration (seconds):  202 Recovery Duration (secs):  46 waitSecs for last batch:   45 PRODUCERS   OrdererIdx  ChannelIdx ChannelID              TX Target         ACK        NACK                      0           0 mychannel0                166668        1788      164880                      1           0 mychannel0                166666        2419      164247                      2           0 mychannel0                166666        2330      164336 Print only the first 3 chans of only the first 3 ordererIdx (and the last ordererIdx if masterSpy is present), plus any others that contain failures. Totals numOrdIdx=4 numChanIdx=1 numCONSUMERS=4 CONSUMERS   OrdererIdx  ChannelIdx ChannelID                    TXs     Batches                      0           0 mychannel0                  6545          14                      1           0 mychannel0                  6545          14                      2           0 mychannel0                  6545          14                      3           0 mychannel0                  6545          14  * MasterSpy on orderer 1 Not counting genesis blks (1 per chan)        1 Total TX broadcasts Requested to Send    500000 Total TX broadcasts send success ACK       6537 Total TX broadcasts sendFailed - NACK    493463 Total Send-LOST TX (Not Ack or Nack))         0 Total Recv-LOST TX (Ack but not Recvd)       *-8*  Total deliveries received TX on each ordrr         6545    6545    6545    6545  Total deliveries received Blocks on each ordrr       14      14      14      14  TEST ORD-97 BAD! Some EXTRA TX were delivered by orderer service! RESULT=FAILED: TX Req=500000 BrdcstACK=6537 NACK=493463 DelivBlk= 14 14 14 14  DelivTX= 6545 6545 6545 6545  numChannels=1 batchSize=500 --- FAIL: Test_ORD97_100000TX_1ch_3ord_kafka_3kbs (342.92s)     ote_test.go:242: TEST ORD-97 BAD! Some EXTRA TX were delivered by orderer service! RESULT=FAILED: TX Req=500000 BrdcstACK=6537 NACK=493463 DelivBlk= 14 14 14 14  DelivTX= 6545 6545 6545 6545  numChannels=1 batchSize=500 FAIL exit status 1 FAIL    github.com/hyperledger/fabric/test/tools/OTE    342.945s     We are calculating the transactions using length of Block.Data.Data in each block. We are getting too many transactions in the above case.  ></body> </Action>
<Action id="27196" issue="15171" author="kchristidis" type="comment" body="Until this can be reproduced deterministically, and we&apos;ve got wayyyy more details on what&apos;s going on (I know you guys are working on it based on the suggestions given above), I will lower its priority and label it as Unverifiable." created="2017-06-22 14:25:13.0" updateauthor="kchristidis" updated="2017-06-22 14:25:13.0"/>
<Action id="29709" issue="15171" author="kchristidis" type="comment" body="Any updates on this one?" created="2017-08-11 13:34:49.0" updateauthor="kchristidis" updated="2017-08-11 13:34:49.0"/>
