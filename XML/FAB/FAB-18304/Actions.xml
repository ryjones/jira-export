<Action id="70591" issue="46264" author="yacovm" type="comment" created="2020-10-30 15:02:35.0" updateauthor="yacovm" updated="2020-10-30 15:03:23.0"> <body><! CDATA Maybe keep track of how much space you have in the FS by periodically sampling it (once in, say, 100MB you write)?    {code:java} 	fs := syscall.Statfs_t{} 	syscall.Statfs(directory, &fs) 	availableSpace := int64(fs.Bavail) * fs.Bsize {code}  and if there is too little space left then panic? Possibly also log a warning about this, or even a metric?  ></body> </Action>
<Action id="71491" issue="46264" author="manish-sethi" type="comment" created="2021-03-02 20:34:34.0" updateauthor="manish-sethi" updated="2021-03-04 17:11:48.0"> <body><! CDATA I investigated this scenario and discovered a bug in goleveldb code. There is one fsync call missing in the goleveldb code, when it generates a new Manifest file at the time of opening the db. I have submitted a simple PR with this fix ( https://github.com/syndtr/goleveldb/pull/351 ).     For those who are interested in details:   With this missing fsync, a disk full on NFS is just one scenario where this corruption is surfaced. In general, an untimely crash, even on a physical machine with a local filesystem, could leave the goleveldb in this state. Still, it is relatively easier to reproduce this issue with a disk full situation on a remote filesystem. Even a small size db suffices for reproducing this, as long as the other random files (e.g., large file(s) filled with /dev/zero) along with the leveldb makes up for the full disk space.     Regarding reproducing this issue, just with the goleveldb alone (i.e., without Fabric), a slight modification in the goleveldb code helps reproduce almost always. This modification is to add a delay in the goleveldb code immediately before  this line|https://github.com/syndtr/goleveldb/blob/5c35d600f0caac04c20d52438103f1a7aa612598/leveldb/session_util.go#L456 . The reason this makes it reproduce in this particular situation is that the code before this line, i.e., the code that writes to the new Manifest file during opening the db, does not recieve an error despite the disk being full. This is because, the NFS client would send these writes asynchronously to the NFS server and will hit into error at a later point in time but that error would remain with NFS client only (would be passed to the application code on the next operation on the file) resulting in the file on the NFS server as empty (or partial written) on disk. This added delay helps in reproducing this issue because, otherwise, the code in the `defer` block of the above mentioned function, deletes the previous Manifest, making the space for new Manifest contents, by the time they arrive on NFS server.     Adding the fsync after writing the Manifest file and before updating other metadata, causes either return an error or guaranteeing that the new Manifest file content has been persisted to disk before proceeding.  ></body> </Action>
<Action id="71498" issue="46264" author="manish-sethi" type="comment" created="2021-03-05 18:05:08.0" updateauthor="manish-sethi" updated="2021-03-08 21:17:47.0"> <body><! CDATA  https://github.com/hyperledger/fabric/pull/2463|https://github.com/hyperledger/fabric/pull/2463  (master)  https://github.com/hyperledger/fabric/pull/2466|https://github.com/hyperledger/fabric/pull/2466  (release-2.3)  https://github.com/hyperledger/fabric/pull/2465|https://github.com/hyperledger/fabric/pull/2465  (release-2.2)  https://github.com/hyperledger/fabric/pull/2464|https://github.com/hyperledger/fabric/pull/2464  (release-1.4)  ></body> </Action>
