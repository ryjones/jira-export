<Action id="23830" issue="16611" author="baohua" type="comment" created="2017-05-13 01:30:13.0" updateauthor="baohua" updated="2017-05-13 01:30:13.0"> <body><! CDATA We can first import the existing PoC code of collecting those log/data from the fabric network.  Backend data collection options could be: * ELK (implemented) * InfluxDB  ></body> </Action>
<Action id="24061" issue="16611" author="qiang0723" type="comment" created="2017-05-19 07:30:38.0" updateauthor="qiang0723" updated="2017-05-19 07:30:38.0"> <body><! CDATA In this task we will complete logs collection, transfer, storage and display.     At present we use elasticsearch as storage. perhaps change to influxDB later.  specific steps:  1. Create Analytics-node or cluster, and install ELK first.  2. On each worknode use logspout to collect logs and send to ELK.  ></body> </Action>
<Action id="24062" issue="16611" author="baohua" type="comment" body="+1!" created="2017-05-19 08:17:45.0" updateauthor="baohua" updated="2017-05-19 08:17:45.0"/>
<Action id="24064" issue="16611" author="qiang0723" type="comment" created="2017-05-19 10:35:05.0" updateauthor="qiang0723" updated="2017-05-19 10:35:05.0"> <body><! CDATA Has already submitted a version. but still needs to be verified again in new environment.  steps need to verify:  1. The layout whether reasonable  2. elasticsearch's cluster yaml config file `elasticsearch.yaml` need some changes to adapt to elasticsearch-5.2.1   https://gerrit.hyperledger.org/r/#/c/9559/       ></body> </Action>
<Action id="24134" issue="16611" author="qiang0723" type="comment" created="2017-05-22 10:42:12.0" updateauthor="qiang0723" updated="2017-05-22 10:42:45.0"> <body><! CDATA Impove cluster configuration file `elasticsearch.yml`:  """  network.host: _site_ cluster.name: cello-analytics node.name: node-1 network.publish_host: MASTER_IP discovery.zen.ping.unicast.hosts:  "MASTER_IP","SLAVE1_IP","SLAVE2_IP"   """  ></body> </Action>
