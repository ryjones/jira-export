Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Affects Version/s,Fix Version/s,Component/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Outward issue link (Blocks),Outward issue link (Blocks),Outward issue link (Blocks),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Relates),Inward issue link (Relates),Inward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Authorized Watchers),Custom field (Business Value),Custom field (Commit Levels),Custom field (Current Status),Custom field (Design),Custom field (Design Status),Custom field (Documentation Impact),Custom field (Documentation Status),Custom field (Epic Color),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Executed),Custom field (Found in Commit),Custom field (Function Test Status),Custom field (Must Fix),Custom field (Original story points),Custom field (Parent Link),Custom field (Rank),Custom field (Release Note),Custom field (Release Note Required),Custom field (Root Cause Analysis),Custom field (SDK Impact),Custom field (Sample/Tutorial),Sprint,Sprint,Sprint,Sprint,Custom field (Steps to Reproduce),Custom field (Story Points),Custom field (System Test Impact),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Team),Custom field (Test Plan),Custom field (Test Result),Custom field (Test Result Details),Custom field (Test Type),Custom field (Triaged),Custom field (Usage),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Workaround),Custom field (gitCommitsReferenced),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Implement auth rule maps in config ledger,INDY-2001,37830,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,20/Feb/19 8:28 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,1.7.1,,,,,0,,,,,"See INDY-1732 for details

*Acceptance criteria*
 * It should be possible to have constraints for every rule in config ledger
 * Rules (keys in auth map) are static and can not be changed by the user; only the constraints are configurable. So, rule_id can be a key in the state, and constraint be the value
 * If there is no constraint for the rule in the config ledger, then default (local) value should be used. Initially all constraints are local.
 * Correctly serialize/deserialize constraints (json?) in a proper format
 * Cover by tests",,,,,,,,,,,,,,INDY-1995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw94",,,,Unset,Unset,Ev-Node 19.04,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"22/Feb/19 9:48 PM;ashcherbakov;Validation will be done in the scope of INDY-2002;;;","22/Feb/19 9:55 PM;anikitinDSR;PR into indy-node:
 [https://github.com/hyperledger/indy-node/pull/1179]

 ;;;",,,,,,,,,,,,,,,,,,,,
Use auth constraints from config ledger for validation,INDY-2002,37831,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Feb/19 8:30 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,1.7.1,,,,,0,,,,,"See INDY-1732 for details.

*Acceptance criteria*
 * Use constraints from config ledger for validation
 * If there is no constraint for the rule in the config ledger, then default (local) value should be used. Initially all constraints are local.
 * Consider using a cache instead of asking state trie each time
 * cover by tests

 ",,,,,,,,,,,,,,INDY-1995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw95",,,,Unset,Unset,Ev-Node 19.04,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"27/Feb/19 11:24 PM;VladimirWork;Pool handled production load normally (no VCs, no ordering gaps, 500k written in the ledger), waiting for INDY-2006 changes to continue validation.;;;","28/Feb/19 3:34 PM;ashcherbakov;Actually we have a separate task for validation: INDY-1995, so I propose to continue validation in the scope of 1995 and close this ticket.;;;",,,,,,,,,,,,,,,,,,,,
Implement a command to set auth constraints,INDY-2003,37832,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,20/Feb/19 8:31 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,1.7.1,,,,,0,,,,,"See INDY-1732 for details

*Acceptance criteria*
 * The command should change the constraint for the given rule_id
 * There should be a rule for sending this command. Initially it's defined locally, but the command can be used to override it in config ledger.
 * Default rule is that the command can be sent by 1 TRUSTEE",,,,,,,,,,,,,,INDY-2006,INDY-1995,,,,,,IS-1201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw96",,,,Unset,Unset,Ev-Node 19.04,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"22/Feb/19 12:07 AM;Toktar;*POA:*
 Add the transaction AUTH_RULE_CHANGE='120' for Config ledger to change authentication rules. In a static validation will be checked:

a correct filling for CONSTRAINT_ID (AND and OR for constraint list and ROLE for a one constraint)

a correct filling for OLD_VALUE (transaction should include this value if AUTH_ACTION == EDIT)
an auth key is exited in auth_map.

In dynamic validation is necessary to verify that the sender has the rights to change auth_map.
 The transaction will have followed structure:
{code:java}
class ClientAuthRuleChangeOperation(MessageBase):
	 schema = (
		(TXN_TYPE, ConstantField(AUTH_RULE_CHANGE)),
                (CONSTRAINT, ConstraintField()),
                (AUTH_ACTION, ChooseField(ADD_PREFIX, EDIT_PREFIX)),
                (FIELD, LimitedLengthStringField(max_length=AUTH_FIELD_LIMIT)),
                (OLD_VALUE, LimitedLengthStringField(max_length=AUTH_FIELD_LIMIT,
                                                     optional=True)),
                (NEW_VALUE, LimitedLengthStringField(max_length=AUTH_FIELD_LIMIT))
         )
    typename = AUTH_RULE_CHANGE
	

class ConstraintField(FieldBase):
	def _specific_validation(self, val):
		pass

class ConstraintListField(MessageValidator):
    schema = (
        (f.CONSTRAINT_ID.nm, ChooseField(e.value for e in ConstraintsEnum)),
        (f.AUTH_CONSTRAINTS.nm, IterableField(ConstraintField())
    )
    typename = AUTH_CONSTRAINT_LIST

class ConstraintEntityField(MessageValidator):
    schema = (
        (f.CONSTRAINT_ID.nm, ChooseField(e.value for e in ConstraintsEnum),
        (f.ROLE.nm, RoleField()),
        (f.SIG_COUNT.nm, NonNegativeNumberField()),
        (f.NEED_TO_BE_OWNER.nm, BooleanField(exceptional_values=False,
											optional=True)),
        (f.METADATA.nm, AnyMapField(exceptional_values={},
                                                          optional=True))
    )
    typename = AUTH_CONSTRAINT

{code}
So, a constraint can contain any level of constraints.
 Example of transaction in JSON:
{code:java}
{""type"": ""120"",
 ""data"": { ""constraint_id"" : ""AND"",
           ""auth_constraints"": [{ ""constraint_id"" : ""ROLE"",
                                  ""role"": ""0"",
                                  ""sig_count"": 1,
                                  ""need_to_be_owner"": false,
                                  ""metadata"": : {}
                                 },
                                 { ""constraint_id"" : ""OR"",
                                   ""auth_constraints"": [{ ""constraint_id"" : ""ROLE"",
                                                           ""role"": ""0"",
                                                           ""sig_count"": 1,
                                                           ""need_to_be_owner"": false,
                                                           ""metadata"": : {}
                                                         },
                                                         { ""constraint_id"" : ""ROLE"",
                                                           ""role"": ""0"",
                                                           ""sig_count"": 1
                                                         }
                                                        ]
                                 }
           }
 }
{code}
or simpler form
{code:java}
{""type"": ""120"",
 ""data"": { ""constraint_id"" : ""ROLE"",
              ""role"": ""0"",
              ""sig_count"": 1,
              ""need_to_be_owner"": false,
              ""metadata"": : {}
            }
 }
{code}
In addition, AuthConstraintChangeHandler should be added to integration later in scope of Pluggable Request Handlers tasks.;;;","28/Feb/19 7:43 PM;ashcherbakov;Implementation will be continued in the scope of INDY-2006.
Validation will be done in the scope of INDY-1995.;;;",,,,,,,,,,,,,,,,,,,,
Fix issue with catchup for payments,INDY-2004,37866,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,21/Feb/19 8:15 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,,,0,,,,,Stub for ST-499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yv",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"21/Feb/19 8:17 PM;ashcherbakov;Duplicates INDY-1972;;;",,,,,,,,,,,,,,,,,,,,,
Stub: Issue with tracker and fees,INDY-2005,37867,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,21/Feb/19 8:19 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,,,0,,,,,Stub for ST-511,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000006r20ec",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"21/Feb/19 8:19 PM;ashcherbakov;Should be fixed by INDY‌-1944;;;",,,,,,,,,,,,,,,,,,,,,
Add updateState method for ConfigReqHandler ,INDY-2006,37872,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,anikitinDSR,anikitinDSR,21/Feb/19 10:34 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,1.7.1,,,,,0,,,,,"As of now, we don't have updateState method into ConfigReqHandler. In this case, after node's starting up we don't have actual auth constraints into config state.
h4. Acceptance criteria:
 * Add updateState method into ConfigReqHandler (indy-node side)
 * Create test, like:
** Send command to change auth constraint
** Restart node
** Check, that state include already changed constraint",,,,,,,,,,INDY-2003,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00007xzr",,,,Unset,Unset,Ev-Node 19.04,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,,,,,,,,,,,"01/Mar/19 5:28 PM;anikitinDSR;Reason:
 * need to state updating functionality into config request handler

Changes:
 * was added updateState method

PR:
 * indy-node: [https://github.com/hyperledger/indy-node/pull/1187]

Version:
 * indy-node: 1.6.837

Testing:
 * this ticket will be tested in the scope of INDY-1995

 ;;;",,,,,,,,,,,,,,,,,,,,,
Make simulation tests of view change use actual ViewChanger code,INDY-2007,37939,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,24/Feb/19 11:11 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,1.7.1,,,,,0,,,,,"*Acceptance criteria*
* ViewChanger should be testable in full isolation
** It shouldn't directly depend on node
** It shouldn't directly depend on system time
* Simulation tests written for view change model in scope of INDY-1897 should use actual ViewChanger",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xz",,,,Unset,Unset,Ev-Node 19.04,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,"24/Feb/19 11:19 PM;sergey.khoroshavin;*Changes*
* ViewChanger was refactored to access Node through ViewChangerDataProvider to make actual dependencies clear and easy to mock if needed
* TimerService interface was introduced to make time based operations (getting current time and scheduling some calls to some later point) mockable
* QueueTimer was created as production implementation of TimerService
* RepeatingTimer was implemented as a wrapper around TimerService to enable repeatable actions
* HasActionQueue in ViewChanger was replaced with combination of TimerService and RepeatingTimers
* TimerModel was created as implementation of TimerService suitable for property-based simulation tests
* Simulation tests were modified to actually use new refactored ViewChanger

*PRs*
https://github.com/hyperledger/indy-plenum/pull/1088
https://github.com/hyperledger/indy-plenum/pull/1093
https://github.com/hyperledger/indy-plenum/pull/1095

*Risk*
Low
;;;",,,,,,,,,,,,,,,,,,,,,
Validator-info doesn't show view change information and sometimes shows node info as unknown,INDY-2008,37955,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ozheregelya,ozheregelya,25/Feb/19 9:53 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,1.7.1,,,,,0,TShirt_S,,,,"*Case 1:*
 Validator-info doesn't show information connected with View Changes. 
 Also following message appears in node logs: 
{code:java}
2019-02-22 15:24:02,629|NOTIFICATION|validator_info_tool.py|Validator info tool fails to execute __node_info because KeyError(1,){code}
validator-info --json: [^validator-info--json.txt]

*Case 2:*
 Fields in node info section sometimes show ""unknown"" value:
!image-2019-02-25-12-59-50-961.png|thumbnail!
[^node1_info.json]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/19 9:59 PM;ozheregelya;image-2019-02-25-12-59-50-961.png;https://jira.hyperledger.org/secure/attachment/16861/image-2019-02-25-12-59-50-961.png","25/Feb/19 10:00 PM;ozheregelya;node1_info.json;https://jira.hyperledger.org/secure/attachment/16862/node1_info.json","25/Feb/19 9:59 PM;ozheregelya;validator-info--json.txt;https://jira.hyperledger.org/secure/attachment/16860/validator-info--json.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xzi",,,,Unset,Unset,Ev-Node 19.04,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,VladimirWork,,,,,,,,,"25/Feb/19 10:10 PM;Toktar;PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/1097]
 * [https://github.com/hyperledger/indy-node/pull/1183];;;","26/Feb/19 12:08 AM;Toktar;*Problem reason:*
 - Validator-info can't dumb the date if a node is received an InstanceCange message.

*Changes:*
 - Fix the bug with store  InstanceCange messages.
 - Add a label for audit ledger in validator info. (https://jira.hyperledger.org/browse/INDY-1944?focusedCommentId=57533&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-57533)

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1097]
 * [https://github.com/hyperledger/indy-node/pull/1183]

*Version:*
 * indy-node 1.6.833 -master
 * (indy-plenum 1.6.696 -master)

*Risk factors:*
 - Problem with Node-info in validator-info

*Risk:*
 - Low

*Test:*
 * test_instance_change_before_vc
 * test_validator_info_file_metrics_count_audit_field_valid

*Recommendations for QA:*

Test validator-info script after one node panicked and sent InstanceChange. Section ""IC_queue"" must contains information that one node votes with some reason. Check validator-info history.

Do view change for all pool

Check that validator-info output has no voters in the section ""IC_queue"" and all field are filled.
Check that all places with transactions count contain an information about audit ledger.;;;","26/Feb/19 8:55 PM;VladimirWork;Build Info:
indy-node 1.6.833

Steps to Validate:
1. Test validator-info script after one node panicked and sent InstanceChange. Section ""IC_queue"" must contains information that one node votes with some reason.
2. Do view change for all pool.
3. Check that validator-info output has no voters in the section ""IC_queue"" and all field are filled.
4. Check that all places with transactions count contain an information about audit ledger.

Actual Results:
Validator-info works as expected.;;;",,,,,,,,,,,,,,,,,,,
The Node upgrade script should be able to upgrade a node to any published version,INDY-2009,37995,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,,lbendixsen,lbendixsen,27/Feb/19 4:24 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,,,0,,,,,"The ledger command to upgrade a node contains a version number, but the script that runs to execute the upgrade requires that the version entered be the same as the most recent that is in the repo.  We need to be able to upgrade to whatever version is required rather than being bound by this restriction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000006r1",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,lbendixsen,mgbailey,VladimirWork,,,,,,,,"27/Feb/19 3:30 PM;ashcherbakov;[~lbendixsen]
Can you please clarify what script you talking about? I'm not aware of any such scripts in Indy Node repo. So, it looks like this is Sovrin Foundation's local script and needs to be addressed in a different Jira (different team?)

 ;;;","27/Feb/19 6:36 PM;VladimirWork;[~lbendixsen] FYI: It is possible to upgrade the pool to *not* the latest version in the repo using pool-upgrade command (e.g. you can upgrade 1.6.80 pool to 1.6.82 right now despite that 1.6.83 is the latest).;;;","28/Feb/19 1:04 AM;mgbailey;This is contrary to what we have been instructed in the past. [~kelly.wilson] can you please comment?;;;","28/Feb/19 1:27 AM;ashcherbakov;It may be not the case initially, but fixed later.;;;","28/Feb/19 3:25 AM;lbendixsen;Thanks Alexander and Vladimir for clarifying comments and verification of current behavior. This ticket can be closed as invalid.;;;",,,,,,,,,,,,,,,,,
Implement a command to get auth constraint,INDY-2010,38026,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,27/Feb/19 5:43 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,1.7.1,,,,,0,,,,,"As an administrator (Trustee?) changing auth rule permissions on the network,  I need to be able to know what is the current permission (constraint) for every action (rule), so that I can use this information to update the permission.


*Acceptance criteria*
 * Implement a command to get the current auth constraint for a particular auth rule

 * 
 ** Command: GET_AUTH_RULE
 ** Input: auth rule fields (the same as in AUTH_RULE cmd)
 ** Output: auth constraint
 ** Who can send: anyone
 * (Optional) Implement a command to get all auth constraints for all auth rules, that is get the whole auth map
 ** Command: GET_ALL_AUTH_RULES
 ** Input: no
 ** Output: auth map (auth_rule_id -> auth constraint)
 ** Who can send: anyone",,,,,,,,,,,,,,IS-1202,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:000008",,,,Unset,Unset,Ev-Node 19.05,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,Toktar,VladimirWork,,,,,,,,"06/Mar/19 9:18 PM;Toktar;[~esplinr] 

We can add one command, which with all parameters produces a specific record from the auth_rule map, and without parameters, gives the entire auth_rule map. Also, we can add a filter. When sending not a complete list of parameters, return all entries that match. That is, when sending only auth_txn= NYM, the client receives a list of records related to the NYM transaction.;;;","07/Mar/19 3:31 AM;esplinr;Great thinking [~Toktar] and [~Artemkaaas]. I think having one command that both returns a single auth rule or all auth rules is an improvement to what I proposed.

It sounds useful to also have a capability to filter for arbitrary matching, but we don't yet know how people will want to use this command and we don't have a common use case that requires the filter. I propose that we avoid that work until people ask for it, and then extend the command. But please consider this idea as a possible future enhancement during the implementation so that in the future it wouldn't be harder than necessary.;;;","12/Mar/19 9:35 PM;Toktar;*PoA:*

Add the GET_AUTH_RULE command to get a constraint for an authentication rule or a full list of rules from Ledger by the auth key parameters.
Two options are possible in a request build:
*1) Getting one rule.*
If the request has a full list of parameters (or without `old_value`), then the reply will contain one constraint for this key.
If the key was found in the state (ledger has a transaction to change rule with this key) then pool return the rule (constraints for the key), the serialized key and a consistency proof for the found transaction.
If the key was not found in the state, a client will receive rule from static auth_rule map and the serialized key without a consistency proof.
A key is an authenticated action in the format {code}action--auth_type--field--old_value--new_value{code}
Request Example:
{code:java}
 {  
      'reqId':572495653,
      'signature':'366f89ehxLuxPySGcHppxbURWRcmXVdkHeHrjtPKNYSRKnvaxzUXF8CEUWy9KU251u5bmnRL3TKvQiZgjwouTJYH',
      'identifier':'M9BJDuS24bqbJNvBRsoGg3',
      'operation':{  
         'field':'role',
         'new_value':'101',
         'type':'121',
         'auth_type':'1',
         'auth_action':'ADD'
      },
      'protocolVersion':2
   }
{code}

Response Example:
{code:java}
 {  
      'op':'REPLY',
      'result':{  
         'type':'121',
         'auth_type':'1',
         'reqId':441933878,
         'identifier':'M9BJDuS24bqbJNvBRsoGg3',
         'new_value':'101',
         'data':{  
            'ADD--1--role--*--101':{  
               'auth_constraints':[  
                  {  
                     'sig_count':1,
                     'role':'0',
                     'constraint_id':'ROLE',
                     'need_to_be_owner':False,
                     'metadata':{  

                     }
                  },
                  {  
                     'sig_count':1,
                     'role':'2',
                     'constraint_id':'ROLE',
                     'need_to_be_owner':False,
                     'metadata':{  

                     }
                  }
               ],
               'constraint_id':'AND'
            }
         },
         'field':'role',
         'state_proof':{  
            'proof_nodes':'+Pz4+pUgQURELS0xLS1yb2xlLS0qLS0xMDG44vjguN57ImF1dGhfY29uc3RyYWludHMiOlt7ImNvbnN0cmFpbnRfaWQiOiJST0xFIiwibWV0YWRhdGEiOnt9LCJuZWVkX3RvX2JlX293bmVyIjpmYWxzZSwicm9sZSI6IjAiLCJzaWdfY291bnQiOjF9LHsiY29uc3RyYWludF9pZCI6IlJPTEUiLCJtZXRhZGF0YSI6e30sIm5lZWRfdG9fYmVfb3duZXIiOmZhbHNlLCJyb2xlIjoiMiIsInNpZ19jb3VudCI6MX1dLCJjb25zdHJhaW50X2lkIjoiQU5EIn0=',
            'root_hash':'DauPq3KR6QFnkaAgcfgoMvvWR6UTdHKZgzbjepqWaBqF',
            'multi_signature':{  
               'signature':'RNsPhUuPwwtA7NEf4VySCg1Fb2NpwapXrY8d64TLsRHR9rQ5ecGhRd89NTHabh8qEQ8Fs1XWawHjbSZ95RUYsJwx8PEXQcFEDGN3jc5VY31Q5rGg3aeBdFFxgYo11cZjrk6H7Md7N8fjHrKRdxo6TzDKSszJTNM1EAPLzyC6kKCnF9',
               'value':{  
                  'state_root_hash':'DauPq3KR6QFnkaAgcfgoMvvWR6UTdHKZgzbjepqWaBqF',
                  'pool_state_root_hash':'9L5CbxzhsNrZeGSJGVVpsC56JpuS5DGdUqfsFsR1RsFQ',
                  'timestamp':1552395470,
                  'txn_root_hash':'4CowHvnk2Axy2HWcYmT8b88A1Sgk45x7yHAzNnxowN9h',
                  'ledger_id':2
               },
               'participants':[  
                  'Beta',
                  'Gamma',
                  'Delta'
               ]
            }
         },
         'auth_action':'ADD'
      }
{code}
*2) Getting all rules.*
 If the request are not contain fields other than txn_type, the response will contain a full list of authentication rules. A consistency proof will not contained in a reply.
Request Example:
{code:java}
 {  
      'reqId':575407732,
      'signature':'4AheMmtrfoHuAEtg5VsFPGe1j2w1UYxAvShRmfsCTSHnBDoA5EbmCa2xZzZVQjQGUFbYr65uznu1iUQhW22RNb1X',
      'identifier':'M9BJDuS24bqbJNvBRsoGg3',
      'operation':{  
         'type':'121'
      },
      'protocolVersion':2
  }
{code}

Response Example:
{code:java}
{  
      'op':'REPLY',
      'result':{  
         'reqId':575407732,
         'type':'121',
         'data':{  
            'ADD--118--action--*--*':{  
               'constraint_id':'ROLE',
               'sig_count':1,
               'metadata':{  

               },
               'need_to_be_owner':False,
               'role':'0'
            },
            'EDIT--1--role--0--':{  
               'constraint_id':'ROLE',
               'sig_count':1,
               'metadata':{  

               },
               'need_to_be_owner':False,
               'role':'0'
            },
            ...
         },
         'identifier':'M9BJDuS24bqbJNvBRsoGg3'
      }
   }
{code};;;","15/Mar/19 6:02 PM;Toktar;*Problem reason:*
 - No command to get current authentication rule or all authentication rules.

*Changes:*
 - Added a command GET_AUTH_RULE=""121"" which with all parameters for an authentication key returns a constraint for this key. And without any parameters (only txn_type) returns all list of authentication rules

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1200]
 * [https://github.com/hyperledger/indy-plenum/pull/1126]
 * [https://github.com/hyperledger/indy-plenum/pull/1121]

*Version:*
 * indy-node 1.6.861 -master
 * (indy-plenum 1.6.726 -master)

*Risk factors:*
 - Problem with a new command

*Risk:*
 - No

*Test:*
 * [test_get_auth_rule.py|https://github.com/hyperledger/indy-node/pull/1200/files#diff-f03e515138eaf1a7305699c31b0b06e7]
 * [get_auth_rule_handler.py|https://github.com/hyperledger/indy-node/pull/1200/files#diff-73f4439b8c963b9c5b57ab5249786889]

*Recommendations for QA:*
 * Send GET_AUTH_RULE without parameters after pool start. Compare with rules from [https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md]
 * Send GET_AUTH_RULE with parameters for an authentication key. Compare with [auth_rules.md|https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md]
 * Write an AUTH_RULE txn. 
 ** Send GET_AUTH_RULE  for this key, check the rule.
 ** Send GET_AUTH_RULE  for getting all rules, check the changed rule.;;;","16/Mar/19 12:14 AM;VladimirWork;Build Info:
indy-node 1.6.861

Steps to Validate:
1. Run getting one rule txn for different fields, new values, auth actions.
2. Run getting all rules txn.
3. Write some rules in the ledger and repeat steps 1 and 2.
4. Check 'data' field for each response.

Actual Results:
All txns return data accoding to filter in case of getting one rule txn and all rules in case of getting all txn (`ledger custom` txn was used since libindy and indy-cli don't support this txn yet so addditional checks will be performed ans system test will be implemented in scope of IS-1202).;;;",,,,,,,,,,,,,,,,,
Stable release 1.7.1,INDY-2011,38027,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,27/Feb/19 6:03 PM,30/Apr/19 7:52 PM,28/Oct/23 2:47 AM,30/Apr/19 6:32 PM,,,1.7.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwrhj:910sx2ui",,,,Unset,Unset,Ev-Node 19.08,Ev-Node 19.09,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"30/Apr/19 7:52 PM;VladimirWork;Indy-node 1.7.1 successfully passed acceptance testing and was moved to stable. See detailed results here: https://docs.google.com/spreadsheets/d/1OVjua8JMwW7RhBWsdd9vSfGvJkgxWxKhEjB3T0yil1U/edit#gid=0;;;",,,,,,,,,,,,,,,,,,,,,
Debug and validation: Apply new multi-signature approach to Token Plugins,INDY-2012,38028,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,27/Feb/19 6:06 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:000006r2",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stable release 1.8.0,INDY-2013,38029,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,27/Feb/19 6:07 PM,01/Jun/19 6:58 PM,28/Oct/23 2:47 AM,01/Jun/19 6:58 PM,,,1.8.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj3",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debug and validation: Apply new request handlers approach to Token Plugins,INDY-2014,38030,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,27/Feb/19 6:30 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:000006r0i",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Genesis file transactions should not be required to be the first transactions on the ledger,INDY-2015,38052,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,mgbailey,mgbailey,28/Feb/19 1:02 AM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,EV-CS,EV-Triaged,help-wanted,,"When an administrator is bringing up a validator node for the first time and joining it to a network, pool and domain genesis files are used for bootstrapping. The transactions of these genesis files are used as the first transactions on the local copy of the ledger being created on the the validator, and thus must be identical to the first transactions on the global ledger.

This should be changed as follows:
 # Add a ""current pool"" file which will contain information similar to the pool ledger, but contains information only on validators that are currently active in the pool. This file will provide connection information for bootstrapping new nodes
 # When a new pool is being brought up for the first time, the current pool file will be identical to the genesis file
 # After the pool ledger download is complete, catchup will be used to create and populate the other ledgers, as is currently done.
 # A utility script should be provided that can be run to verify that the current pool file is  valid, based on the contents of the pool and domain ledgers

 ",,,,,,,,,,,,,,,,,,,,,INDY-1189,,,IS-1200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywg",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,mgbailey,vladimir.demcak,,,,,,,,,"11/Jun/19 11:05 PM;esplinr;The current approach is to update the ""genesis transactions"" to cut the pool ledger at a given time to be the initial state of the pool ledger.  Over time, the size of that file will grow to an inelegant size, but it isn't an immediate concern. (As a note, the number of nodes should always be greater or equal to n-F of the current pool size.)

Changing this behavior will require a change in the catch-up protocol and a change in the communication with clients using libindy.

The Evernym team would be happy to work on the architecture with someone else who wants to work on it, but our current backlog has more immediate concerns.;;;",,,,,,,,,,,,,,,,,,,,,
Integrate testinfra-based system tests to Indy CD,INDY-2016,38123,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,01/Mar/19 10:51 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,,,0,,,,,"Options:
1. Switch all tests to ssh testinfra backend to connect pool nodes (need to configure ssh on jenkins machine).
2. Use docker-in-docker for indyclient contatiner and save docker testinfra backend.",,,,,,,,,,,,,,,,,,,,,INDY-1998,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xxi",,,,Unset,Unset,Ev-Node 19.05,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"05/Mar/19 8:35 PM;VladimirWork;System tests changes: https://github.com/hyperledger/indy-test-automation/pull/14
Jenkinsfile changes: https://github.com/hyperledger/indy-node/pull/1191;;;","12/Mar/19 8:05 PM;VladimirWork;Final changes are done and all new tests are passed in CD: https://build.sovrin.org/job/test-pipelines/job/indy-node-cd-copy/view/change-requests/job/PR-1191/20/testReport/.;;;","13/Mar/19 9:14 PM;VladimirWork;Option 1 has been implemented: all system tests were switched to ssh backend instead of docker backend.

All CI\CD and test automation changes are applied and successfully run: https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-cd/detail/master/856/pipeline;;;",,,,,,,,,,,,,,,,,,,
New node added to MainNet selects unreachable node as primary (0),INDY-2017,38290,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,mgbailey,mgbailey,08/Mar/19 7:59 AM,09/Oct/19 6:31 PM,28/Oct/23 2:47 AM,09/Oct/19 6:31 PM,,,1.13.0,,,,,0,EV-CS,,,,"Observed behavior:
When a new node called sparcnz was added to the MainNet (v. 1.6.82), it was unable to contact the first node in the genesis file (ev1). In spite of this, it selected ev1 as the first primary. This is in spite of the fact that all other nodes in the pool had selected danube as the primary.

After fixing the firewall rule that was preventing a connection to ev1, sparcnz immediately did a view change and selected danube as the first primary.

Expected behavior:
The behavior should have been that sparcnz would select danube as the primary (matching the other validators), in spite of it not being able to contact the first node in the genesis file.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywbj9",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,mgbailey,,,,,,,,,,"12/Mar/19 11:01 PM;esplinr;This problem should be addressed with the addition of the audit ledger.

The audit ledger will store who the master primary is, so new nodes will receive that information from the audit ledger.

We will test this use case once the audit ledger is ready.;;;","09/Oct/19 6:31 PM;esplinr;We rolled out the audit ledger and have not seen this issue. If it manifests again, we will look into it again.;;;",,,,,,,,,,,,,,,,,,,,
Node fails to start after the load,INDY-2018,38355,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,11/Mar/19 8:22 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,1.7.1,,,,,0,TShirt_M,,,,"Build Info:
1.6.846

Steps to Reproduce:
1. Run load test to fill pool, config and domain ledgers with txns and stop it.
2. Stop the node (Node7).
3. Run another load test to fill pool, config and domain ledgers with txns and stop it.
4. Start the node.
5. Check started node service status and all ledgers.

Actual Results:
Node7 has all txn that were written before the stopping but doesn't have all txns that were written after. Indy-node service fails to start with stacktrace:
{noformat}
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]: Traceback (most recent call last):
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 51, in run_node
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     ha=node_ha, cliha=client_ha)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 100, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     config=config)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 246, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     self.network_stacks_init(seed)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 445, in network_stacks_init
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     self.nodestack = cls(**kwargs)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/stacks.py"", line 175, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     mt_outgoing_size=MetricsName.OUTGOING_NODE_MESSAGE_SIZE)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/kit_zstack.py"", line 50, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     mt_outgoing_size=mt_outgoing_size)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/simple_zstack.py"", line 50, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     mt_outgoing_size=mt_outgoing_size)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/zstack.py"", line 102, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     self.setupSigning()
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/zstack.py"", line 327, in setupSigning
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     self.addVerifier(vk)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/zstack.py"", line 330, in addVerifier
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     self.verifiers[verkey] = Verifier(z85.decode(verkey))
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/crypto/nacl_wrappers.py"", line 223, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     key = VerifyKey(key, encoding.HexEncoder)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/crypto/nacl_wrappers.py"", line 73, in __init__
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     key = encoder.decode(key)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/crypto/encoding.py"", line 23, in decode
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]:     return binascii.unhexlify(data)
Mar 11 10:59:25 b5f7a5aa3d32 env[2895]: binascii.Error: Non-hexadecimal digit found
{noformat}
See debug logs from all nodes in the attachment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/19 8:21 PM;VladimirWork;11032019.7z;https://jira.hyperledger.org/secure/attachment/16916/11032019.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000007o",,,,Unset,Unset,Ev-Node 19.06,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,VladimirWork,,,,,,,,,,"11/Mar/19 10:42 PM;VladimirWork;It looks like valid did is *not* a valid value for node dest (target) field but valid verkey is a valid value for this (but both these fields looks like valid base58 strings). Anyway there should be validation for this field on client (libindy) or node side.;;;","20/Mar/19 8:26 PM;Derashe;Problem reason/description:
 - Problem is that validation allowed usage of incorrect Ed25519 keys as a node key. That cause a nodes to crush when it tries to connect to a node with invalid key.

Changes:
 - Additional validation added to forbid incorrect Ed25519 to be used.

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1137]

Version:
 - 
h4. [indy-node: 1.6.874-master|https://github.com/hyperledger/indy-node/releases/tag/1.6.874-master]

Risk:
 - Low

Covered with tests:
 - [https://github.com/hyperledger/indy-node/pull/1213/files#diff-999ae9862d4ff40ce96537278cbfe495]

Recommendations for QA
 - Retest description case and ensure that requests with invalid node's keys will be rejected.;;;","26/Mar/19 1:24 AM;VladimirWork;Build Info:
indy-node 1.6.874

Steps to Validate:
1. Try to add some nodes with invalid parameters.

Actual Results:
Node txns' validation works as expected.;;;",,,,,,,,,,,,,,,,,,,
Debug and Validation: As a user/steward I want to have better understanding of release version and changelog ,INDY-2019,38382,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,12/Mar/19 10:02 PM,02/Apr/19 9:17 PM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,1.7.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1992,,,INDY-2037,INDY-2038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000007m",,,,Unset,Unset,Ev-Node 19.06,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,VladimirWork,,,,,,,,,"21/Mar/19 8:18 PM;VladimirWork;PoA:
1. master before changes -> master after changes with force False/True
2. master after changes to nonexistent/reinstall + cancellation

3. rc before changes -> rc after changes with force False/True
4. rc after changes to nonexistent/reinstall + cancellation

5. stable before changes -> stable after changes with force False/True
6. stable after changes to nonexistent/reinstall + cancellation;;;","28/Mar/19 9:00 PM;andkononykhin;Problem reason:
 * INDY-1992 brought new release and versioning logic for indy-plenum and indy-node but didn't activate it. Also jeknins shared library wasn't fully tested

Changes: 
 * created new shared library release 2.0.1
 * tested, fixed and activated new logic for plenum & node

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1125]
 * [https://github.com/hyperledger/indy-node/pull/1219]
 * [https://github.com/hyperledger/indy-node/pull/1223]
 * [https://github.com/sovrin-foundation/jenkins-shared/pull/6]
 * https://github.com/sovrin-foundation/jenkins-shared/pull/7


Version:
 * plenum: 1.7.0.dev740
 * node: 1.7.0.dev878
 * jenkins library: 2.0.1

Risk factors:
 * new versioning logic impacts upgrade/restart logic
 * new release scheme is quite different from legacy one and required big changes in jenkins library

Risk:
 * High

Covered with tests:
 * unit/integration tests provided for both node and plenum
 * jeknins shared library was tested maually since there is no framework to automate that yet

Recommendations for QA:
 * perform deep testing for upgrade/restart
 * for upgrade txns use the version of the package in PyPI (e.g. 1.7.0.dev878, 1.7.0.rc2, 1.7.0);;;",,,,,,,,,,,,,,,,,,,,
"Schema can't be written with error ""'Version' object has no attribute 'dev'""",INDY-2020,38383,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,12/Mar/19 10:12 PM,02/Apr/19 8:52 PM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,1.7.1,,,,,0,,,,,"*Environment:*
indy-node 1.6.846
AWS pool of 25 nodes

*Steps to Reproduce:*
1. Setup the pool.
2. Connect to this pool using indy-cli.
3. Try to send schema.

*Actual Results:*
Error message appear:
{code:java}
Transaction has been rejected: client request invalid: InvalidClientRequest() [caused by 'Version' object has no attribute 'dev']{code}

*Expected Results:*
Schema should be written.

*Additional Information:*
This issue doesn't reproduces on docker pool.",,,,,,,,,,,,,,,,,,,INDY-2031,,,,,INDY-2021,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xxr",,,,Unset,Unset,Ev-Node 19.05,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ozheregelya,,,,,,,,,,"13/Mar/19 1:22 PM;andkononykhin;Exploration results:
 # recently added dependency `packaging` is imported wrongly: its version is 16.6 instead of 19.0
 # 19.0 should come from repo.sovrin.org and it is really presented
 # but there is also another package _packaging_ of version 16.6 installed as part of _python-pip-whl_ debian package (which is a dependency for _python-pip_)
 # version 16.6. wins 19.0 when pip itself is imported inside plenum/node code since it updates python search path;;;","13/Mar/19 1:25 PM;andkononykhin;As a fast fix PRs to indy-plenum ([https://github.com/hyperledger/indy-plenum/pull/1122)] and indy-node ([https://github.com/hyperledger/indy-node/pull/1203)] were created. But that needs more accurate fix since packaging is not only one package installed as wheels with pip and that plenum/node need. ;;;","14/Mar/19 7:47 PM;andkononykhin;Problem reason: 
- regression in schema txn processing encountered on node 845/846

Changes: 
- added workaround to import packaging module before pip to get correct version of the package that comes form repo.sovrin.org

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1122]
 * [https://github.com/hyperledger/indy-node/pull/1203]

Version:
- indy-node 1.6.855

Risk:
- Low

Covered with tests:
- tested manually

Recommendations for QA
 * on a machine with installed indy-node >= 1.6.855  run pyhton3 interpreter and do the following to check that packagin is imported as 19.0:

{code:java}
Python 3.5.2 (default, Nov 12 2018, 13:43:14) 
[GCC 5.4.0 20160609] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import plenum
>>> import packaging
>>> print(packaging.__version__)
19.0
{code};;;","14/Mar/19 9:05 PM;ozheregelya;*Environment:*
indy-node 1.6.846
AWS pool of 25 nodes

*Steps to Validate:*
1. Setup the pool.
2. Connect to this pool using indy-cli.
3. Run production load.

*Actual Results:*
packaging.__version__ is 19.0.
Production load works without any issues.;;;",,,,,,,,,,,,,,,,,,
Dependency environment is shifted by pip package imports,INDY-2021,38413,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,andkononykhin,andkononykhin,13/Mar/19 1:47 PM,13/Mar/19 1:50 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Plenum and node imports pip for some routine. On Ubuntu 16.04 python3-pip depends on python-pip-whl package which installs set of python wheels:

 
{code:java}
dpkg -L python-pip-whl

...

/usr/share/python-wheels/requests-2.9.1-py2.py3-none-any.whl

/usr/share/python-wheels/pip-8.1.1-py2.py3-none-any.whl

/usr/share/python-wheels/six-1.10.0-py2.py3-none-any.whl

/usr/share/python-wheels/setuptools-20.7.0-py2.py3-none-any.whl

/usr/share/python-wheels/html5lib-0.999-py2.py3-none-any.whl

/usr/share/python-wheels/ipaddress-0.0.0-py2.py3-none-any.whl

/usr/share/python-wheels/packaging-16.6-py2.py3-none-any.whl

/usr/share/python-wheels/retrying-1.3.3-py2.py3-none-any.whl

/usr/share/python-wheels/colorama-0.3.7-py2.py3-none-any.whl

/usr/share/python-wheels/chardet-2.3.0-py2.py3-none-any.whl

/usr/share/python-wheels/distlib-0.2.2-py2.py3-none-any.whl

/usr/share/python-wheels/lockfile-0.12.2-py2.py3-none-any.whl

/usr/share/python-wheels/wheel-0.29.0-py2.py3-none-any.whl

/usr/share/python-wheels/pyparsing-2.0.3-py2.py3-none-any.whl

/usr/share/python-wheels/CacheControl-0.11.5-py2.py3-none-any.whl

/usr/share/python-wheels/progress-1.2-py2.py3-none-any.whl

/usr/share/python-wheels/urllib3-1.13.1-py2.py3-none-any.whl

/usr/share/python-wheels/pkg_resources-0.0.0-py2.py3-none-any.whl

...

{code}
Some of packages are used by plenum/node (e.g. packaging, six) but expected with other versions ([https://github.com/hyperledger/indy-plenum/blob/9edc3856219ac2506032aad293d82777a5497a28/setup.py#L48).]

During import pip updates python search path (sys.path) in such a way that the wheels becomes more prioritized and plenum/node operates with wrong versions of the packages.

 

Options:
 # do not use pip (we use v.9.0.1 but 10.0 rejected any python API)
 # import all dependencies before any pip imports so necessary versions would be cached
 # keep dependencies in plenum/node repos as-is as vendor packages (example: https://github.com/pypa/pipenv/tree/master/pipenv/vendor)

 ",,,,,,,,,,,,,,,,,,,,,INDY-2020,INDY-1701,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ftb:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
POA: Sovrin TestNet lost consensus,INDY-2022,38471,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,mgbailey,mgbailey,15/Mar/19 3:24 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,1.7.1,,,,,0,TShirt_M,,,,"*Acceptance Criteria*
* Examine the issue and identify the root cause.
* Raise associated JIRA issues for addressing the problem.
* If it is easy to fix, then provide a fix.

*Description*
At approximately 2019-03-13 22:18:59 (from the logs) a view change occurred on Sovrin TestNet when the primary 0 was rebooted to apply a system patch. All nodes but one changed to the same primary, but regioit01 did not change. In addition, over the next few hours the domain ledgers on various nodes diverged, with some nodes having 41884 transactions, and others having 41927, or 41928. This is shown in these validator-info extracts:
{code:java}
[all]> show primary
{
    ""Absa"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""Absa:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""EBPI-validation-node"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""EBPI-validation-node:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""NECValidator"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""NECValidator:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""NodeTwinPeek"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""NodeTwinPeek:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""RFCU"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""RFCU:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""SovrinNode"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""SovrinNode:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""Swisscom"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""Swisscom:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""VALIDATOR1"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""VALIDATOR1:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""anonyome"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""anonyome:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""australia"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""australia:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""brazil"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""brazil:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""canada"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""canada:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""dativa_validator"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""dativa_validator:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""england"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""england:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""ibmTest"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""ibmTest:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""korea"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""korea:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""lab10"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""lab10:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""regioit01"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""regioit01:0"": {
                    ""Primary"": ""brazil:0""
                }
            }
        }
    },
    ""singapore"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""singapore:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""sovrin.sicpa.com"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""sovrin.sicpa.com:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""trusted_you"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""trusted_you:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    },
    ""virginia"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""virginia:0"": {
                    ""Primary"": ""canada:0""
                }
            }
        }
    }
}
[all]> reload
Please be patient while I contact all the nodes in the pool for their status...
[all]> show transCount
{
    ""Absa"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41884,
                    ""pool"": 123
                }
            }
        }
    },
    ""EBPI-validation-node"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""NECValidator"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""NodeTwinPeek"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""RFCU"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""SovrinNode"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""Swisscom"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""VALIDATOR1"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""anonyome"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""australia"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41928,
                    ""pool"": 123
                }
            }
        }
    },
    ""brazil"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41928,
                    ""pool"": 123
                }
            }
        }
    },
    ""canada"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""dativa_validator"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""england"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""ibmTest"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""korea"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""lab10"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""regioit01"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41722,
                    ""pool"": 123
                }
            }
        }
    },
    ""singapore"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""sovrin.sicpa.com"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""trusted_you"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    },
    ""virginia"": {
        ""Node_info"": {
            ""Metrics"": {
                ""transaction-count"": {
                    ""1001"": 0,
                    ""config"": 24705,
                    ""ledger"": 41927,
                    ""pool"": 123
                }
            }
        }
    }
}{code}
Consensus was restored using a pool-restart transaction.

We need to determine if there is a fault manifesting here that we have not seen before. Logs have been requested from stewards. If additional logs are needed, please request them.","Sovrin TestNet, running indy-node version 1.6.83",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/19 11:05 PM;mgbailey;NodeTwinPeek_log.tgz;https://jira.hyperledger.org/secure/attachment/16955/NodeTwinPeek_log.tgz","15/Mar/19 3:24 AM;mgbailey;australia.tgz;https://jira.hyperledger.org/secure/attachment/16940/australia.tgz","15/Mar/19 3:24 AM;mgbailey;brazil.log.352.xz;https://jira.hyperledger.org/secure/attachment/16941/brazil.log.352.xz","15/Mar/19 3:24 AM;mgbailey;brazil.log.353.xz;https://jira.hyperledger.org/secure/attachment/16942/brazil.log.353.xz","15/Mar/19 3:24 AM;mgbailey;brazil.log.354.xz;https://jira.hyperledger.org/secure/attachment/16943/brazil.log.354.xz","15/Mar/19 3:24 AM;mgbailey;brazil.log.355.xz;https://jira.hyperledger.org/secure/attachment/16944/brazil.log.355.xz","15/Mar/19 3:24 AM;mgbailey;brazil.log.356.xz;https://jira.hyperledger.org/secure/attachment/16945/brazil.log.356.xz","15/Mar/19 3:24 AM;mgbailey;brazil.tgz;https://jira.hyperledger.org/secure/attachment/16934/brazil.tgz","15/Mar/19 3:24 AM;mgbailey;canada.tgz;https://jira.hyperledger.org/secure/attachment/16938/canada.tgz","15/Mar/19 3:24 AM;mgbailey;england.tgz;https://jira.hyperledger.org/secure/attachment/16936/england.tgz","15/Mar/19 3:24 AM;mgbailey;korea.tgz;https://jira.hyperledger.org/secure/attachment/16939/korea.tgz","15/Mar/19 3:24 AM;mgbailey;regioit01.log.tar.gz;https://jira.hyperledger.org/secure/attachment/16948/regioit01.log.tar.gz","15/Mar/19 3:24 AM;mgbailey;singapore.tgz;https://jira.hyperledger.org/secure/attachment/16935/singapore.tgz","15/Mar/19 3:24 AM;mgbailey;sovrin.sicpa.com.tgz;https://jira.hyperledger.org/secure/attachment/16947/sovrin.sicpa.com.tgz","15/Mar/19 3:24 AM;mgbailey;trusted_you_log.tgz;https://jira.hyperledger.org/secure/attachment/16946/trusted_you_log.tgz","15/Mar/19 3:24 AM;mgbailey;virginia.tgz;https://jira.hyperledger.org/secure/attachment/16937/virginia.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000007x",,,,Unset,Unset,Ev-Node 19.06,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),aronvanammers,Derashe,mgbailey,VladimirWork,,,,,,,,"22/Mar/19 6:33 PM;Derashe; 

*Results of inverstigation.*
 * As we can see from validator info output and log, after primary was restarted, there were no pool consesus lost.
 * But there were problems with two nodes: regioit01 and Absa.
 * Absa was able to finish view_change and start ordering, but stopped after some time.
 * regioit01 wasn't able to correctly start and finish view change, so it couldn't order till it was restarted.

*Action items.*
 * We will continue to do research on reasons of regioit01 fault
 * [~mgbailey] We need to get logs and journalctl info from Absa node, so we could understand what happened to it.;;;","22/Mar/19 11:47 PM;Derashe;[~mgbailey] It also would be very good if we could get journalctl  from regioit01.;;;","23/Mar/19 12:06 AM;mgbailey;I will reach out to the stewards of Absa and regioit01 and request logs from them;;;","25/Mar/19 9:12 PM;Derashe;*Additional results:*
 * As for regioit01, it could not start view change, because of minor bug, which is already fixed in master branch
 ** _startViewChange_ function in view_changer.py need to have condition, which allows it to call _on_strategy_complete_, so it could correctly set _is_preparing_ flag.
 * We've also mentioned INSTANCE_CHANGE, that was initiated by 21 reason (incorrect state tree). That happened because master primary last sent batch was freshness batch. And it wasn't saved in _txn_seq_range_to_3phase_key_ dict. This made primary sent pp with already ordered 3pc number and changed self root hash. This pp was discarded by other nodes and did not changed its root hash. Next 3pc batch will be declined with suspicion code 21.;;;","26/Mar/19 1:00 AM;mgbailey;From the Absa steward:



unfortunately I won’t be able to recover these logs due to the fore-mentioned incident which happened on March 14^th^afternoon. Specifically we’ve lost logs in interval from 7^th^ March  to 14^th^ 15:11GMT of March. 
What exactly happened: On March 14^th^ afternoon I’ve copy-pasted tar command for getting the validator logs and didn’t try to run it against some testfile first – directly tried to package up the validator logs. Maybe the tar command was for a different tar implementation - it failed and wiped out the log file. I am sorry I can’t help with the logs and I’ve learnt some lesson here too.


However, here’s journalctl logs:
-- Logs begin at Wed 2019-03-13 11:19:05 UTC, end at Mon 2019-03-25 08:50:42 UTC. --

Mar 13 11:19:16 validator systemd[1]: Started Indy Node.

Mar 13 11:21:24 validator systemd[1]: Stopping Indy Node...

Mar 13 11:21:24 validator systemd[1]: Stopped Indy Node.

Mar 13 11:36:18 validator systemd[1]: Stopped Indy Node.

Mar 13 11:36:44 validator systemd[1]: Started Indy Node.

Mar 14 14:26:51 validator systemd[1]: Started Indy Node.

Mar 14 14:26:51 validator systemd[1]: Stopping Indy Node...

Mar 14 14:26:51 validator systemd[1]: Stopped Indy Node.

Mar 14 14:26:51 validator systemd[1]: Started Indy Node.

If it helps with the investigation, note that on March 13^th^ 11:36AM GMT we’ve split up our client/node interface[https://indyscan.io/tx/SOVRIN_TESTNET/pool/123] and the node was able to reach other nodes after the operation. Then on March 14^th^, I temporarily enabled new firewall rules, which is when we noticed issues.

Also, as I’ve mentioned before, the firewall rules which seemed to be causing some troubles to Ibm node are not currently activated. So unless something has changed since then, re-enabling these firewall rules may reproduce issues observed on March 14^th^. 

 ;;;","26/Mar/19 8:59 PM;Derashe;[~mgbailey] thanks for the update.;;;","26/Mar/19 11:28 PM;Derashe;Problem reason/description:
 - Freshness and empty batches do not update _txn_seq_range_to_3phase_key_

Changes:
 - Now, freshness and empty batches updates _txn_seq_range_to_3phase_key_

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1140]
 - [https://github.com/hyperledger/indy-plenum/pull/1141]

Version:
 - 
h4. [indy-node: 1.6.875-master|https://github.com/hyperledger/indy-node/releases/tag/1.6.875-master]

Risk factors:
 - no

Risk:
 - Low

Covered with tests:
 - [https://github.com/hyperledger/indy-plenum/pull/1140/files#diff-1ec4662ba693350e6f170e310d7d9414]

Recommendations for QA
 * start 4 nodes pool
 * send some txns (5)
 * wait untill few freshness batch will be sent (5 at least)
 * fast restart primary, so that no view_change happened
 * send some txns asap after restart and ensure that they will be written;;;","27/Mar/19 10:56 PM;VladimirWork;Build Info:
indy-node 1.6.876

Steps to Validate:
1. Start the pool.
2. Send some txns (5-50).
3. Wait untill few freshness batch will be sent (5 at least).
4. Fast restart primary, so that no view_change happened.
5. Send some txns (10-100) asap after restart and ensure that they will be written.

Actual Results:
All txns have written successfully.;;;",,,,,,,,,,,,,,
Nodes can select same primary during view_change,INDY-2023,38487,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,Derashe,Derashe,15/Mar/19 5:26 PM,24/Oct/19 6:38 PM,28/Oct/23 2:47 AM,24/Oct/19 6:38 PM,,,1.11.0,,,,,0,,,,,"Nodes can select same primary during view_change.

This happened in test_primary_selection_after_demoted_primary_node_promotion.",,,,,,,,,,,,,,,,,,,INDY-2059,,,,,INDY-2247,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i011bm:r",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,Toktar,,,,,,,,,,"24/Oct/19 6:37 PM;Toktar;Fixed in scope of https://jira.hyperledger.org/browse/INDY-2247

Remove test_primary_selection_after_demoted_primary_node_promotion because it duplicates test_promotion_before_view_change;;;",,,,,,,,,,,,,,,,,,,,,
Only Trustee or Node owner can be the author of NODE demotion txn regardless of endorsement or auth constraint rules set,INDY-2024,38512,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,15/Mar/19 9:00 PM,12/Dec/19 1:47 AM,28/Oct/23 2:47 AM,12/Dec/19 1:47 AM,,,1.12.1,,,,,0,TShirt_S,,,,"Build Info:
indy-node 1.6.861

Steps to Reproduce:
{noformat}
pool(docker):wallet(docker):did(V4S...e6f):indy> ledger custom {""reqId"":111,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}}]},""field"":""services"",""auth_type"":""0"",""auth_action"":""EDIT"",""old_value"":""['VALIDATOR']"",""new_value"":""[]""},""protocolVersion"":2} sign=true
Response: 
{""op"":""REPLY"",""result"":{""rootHash"":""42TDiKJNQiBegte8KMeQaCAMsHk1JjvqCHfgqkHnQUPx"",""txnMetadata"":{""txnTime"":1552647677,""seqNo"":1},""ver"":""1"",""txn"":{""protocolVersion"":2,""type"":""120"",""data"":{""auth_action"":""EDIT"",""auth_type"":""0"",""constraint"":{""auth_constraints"":[{""metadata"":{},""role"":""101"",""need_to_be_owner"":false,""constraint_id"":""ROLE"",""sig_count"":1}],""constraint_id"":""OR""},""field"":""services"",""old_value"":""['VALIDATOR']"",""new_value"":""[]""},""metadata"":{""reqId"":111,""from"":""V4SGRU86Z58d6TV7PBUe6f"",""digest"":""5d8ec44a9bda929d49ae1200b6a0a110526c6de07ea0259ee50d5c821e11ccb8""}},""auditPath"":[],""reqSignature"":{""type"":""ED25519"",""values"":[{""value"":""3mFrSzSnr9ob3jrxsX2etSxpk953EnM629z2W3NeudkM2tdQZoJvWayf5QBTvzYoXfRkomrfDHcnTiVbjLBqXXzv"",""from"":""V4SGRU86Z58d6TV7PBUe6f""}]}}}
pool(docker):wallet(docker):did(V4S...e6f):indy> ledger custom {""reqId"":111,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}}]},""field"":""services"",""auth_type"":""0"",""auth_action"":""EDIT"",""old_value"":""[]"",""new_value"":""['VALIDATOR']""},""protocolVersion"":2} sign=true
Response: 
{""result"":{""auditPath"":[""42TDiKJNQiBegte8KMeQaCAMsHk1JjvqCHfgqkHnQUPx""],""txn"":{""data"":{""constraint"":{""auth_constraints"":[{""metadata"":{},""need_to_be_owner"":false,""constraint_id"":""ROLE"",""sig_count"":1,""role"":""101""}],""constraint_id"":""OR""},""old_value"":""[]"",""auth_type"":""0"",""auth_action"":""EDIT"",""field"":""services"",""new_value"":""['VALIDATOR']""},""protocolVersion"":2,""metadata"":{""reqId"":111,""digest"":""2c9b6753fed0ab65c6b4868b3cd7b8ab9826f9b65223177723638e8ee3ad7c6c"",""from"":""V4SGRU86Z58d6TV7PBUe6f""},""type"":""120""},""reqSignature"":{""type"":""ED25519"",""values"":[{""value"":""4oxvQ8m4q8tadAiNkqy42j8tzrTMqSRNeMAKB4He4tyh99yZyJnBmuLKxKo6inABQDJTK4U2AAeBzDzumpWXE5aq"",""from"":""V4SGRU86Z58d6TV7PBUe6f""}]},""ver"":""1"",""txnMetadata"":{""txnTime"":1552647697,""seqNo"":2},""rootHash"":""GGM8cUuqNnXT7Y6HYdgbMg8N9P592WTWfouAuX8D2EXo""},""op"":""REPLY""}
pool(docker):wallet(docker):did(V4S...e6f):indy> did use VnxoLY944SQjpctrfzWGaz
Did ""VnxoLY944SQjpctrfzWGaz"" has been set as active
pool(docker):wallet(docker):did(Vnx...Gaz):indy> ledger node target=4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe alias=Node5 services=
{noformat}


Actual Results:
{noformat}
Transaction has been rejected: VnxoLY944SQjpctrfzWGaz is not a steward of node 4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe
{noformat}
Now we have another error here: new TA is not a Steward of Node5 but we have the rule that we should not be the owner to perform node demotion (just TA role is needed).

Expected Results:
Node demotion should be successfull in this case.",,,,,,,,,,,,,,,,,,,,,INDY-1995,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41hux",,,,Unset,Unset,Ev-Node 19.25,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,VladimirWork,,,,,,,,,,"15/Mar/19 9:01 PM;VladimirWork;FYI [~ashcherbakov] [~anikitinDSR];;;","11/Dec/19 10:28 PM;anikitinDSR;Versions to check:
 * indy-plenum: 1.12.1~dev978
 * indy-node: 1.12.1~dev1162;;;","12/Dec/19 1:46 AM;VladimirWork;Build Info:
1.12.1~dev1162

Steps to Validate:
https://github.com/VladimirWork/indy-test-automation/blob/3c59552a55b43ed877ab8bceb1165ba5d6f72dc1/system/indy-node-tests/TestAuthMapMiscSuite.py#L24

Actual Results:
Auth rules for NODE txn work the same as for other txn types.;;;",,,,,,,,,,,,,,,,,,,
Debug and Validation: Restore current 3PC state from audit ledger - Phase 1,INDY-2025,38513,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,15/Mar/19 9:26 PM,15/Apr/19 6:55 PM,28/Oct/23 2:47 AM,12/Apr/19 6:35 PM,,,1.7.1,,,,,0,,,,,"PR: [https://github.com/hyperledger/indy-plenum/pull/1096]

*The following needs to be tested*
 # Regression testing
 ** Load tests:
 *** acceptance load (1943) no fees
 *** NYM txns (10 writes per sec) with forced view change
 *** acceptance load (1943) with forced view change
 *** load to all ledgers with forced view change
 # Non-primary Node(s) join the pool after restart
 ** Docker / system tests (pool with 7 nodes):
 *** Restart 1 Node no load:
 **** send 5 txns; viewNo=0; restart the 6th node  => node set correct viewNo=0 and ppSeqNo=5;
 send more txns => node participates in ordering
 **** do view change so that viewNo=1; send 5 txns; restart the 6th node => node set correct viewNo=1 and ppSeqNo=5;
 send more txns => node participates in ordering
 **** do view change so that viewNo=1; stop the 4th node; send5 txns; start 6th Node => node set correct viewNo=1 and ppSeqNo=5; 
 send more txns => node participates in ordering
 *** Restart 1 Node with load:
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart the 6th node=> node set correct viewNo=1 and ppSeqNo;
 node participates in ordering with the current load
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; stop 6th node; wait for 1 minute; start the 6th Node=> node set correct viewNo=1 and ppSeqNo;
 node participates in ordering with the current load
 *** Restart master primary no load
 **** do view change so that viewNo=1; send 5 txns; restart the 2d node (primary) => node set correct viewNo=1 and ppSeqNo=5;
 send more txns => node participates in ordering
 *** Restart master primary with load
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart the 2d node (primary) => node set correct viewNo=1 and ppSeqNo;
 node participates in ordering with the current load
 *** Restart all nodes at the same time no load:
 **** do view change so that viewNo=1; send 5 txns; restart all nodes (simultaneously)  => all nodes set correct viewNo=1 and ppSeqNo=5;
 send more txns => nodes participate in ordering
 *** Restart n-f-1 nodes no load:
 **** do view change so that viewNo=1; send 5 txns; restart Nodes4-7 (simultaneously)  => all nodes set correct viewNo=1 and ppSeqNo=5;
 send more txns => nodes participate in ordering
 *** Restart f nodes with load
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart Nodes 6 and 7 (simultaneously) => nodes set correct viewNo=1 and ppSeqNo;
 nodes participates in ordering with the current load
 *** Restart n-f-1 nodes with load:
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart Nodes4-7  (simultaneously) => all nodes set correct viewNo=1 and ppSeqNo;
 nodes participates in ordering with the current load
 *** Restart all nodes at the same time with load:
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec; restart all nodes  (simultaneously) => all nodes set correct viewNo=1 and ppSeqNo;
 nodes participates in ordering with the current load
 *** Restart all nodes one by one no load:
 **** do view change so that viewNo=1; send 5 txns; restart all nodes 1 by 1 (from Node1 till Node 7) => all nodes set correct viewNo=1 and ppSeqNo;
 nodes participates in ordering with the current load
 *** Restart all nodes one by one with load:
 **** do view change so that viewNo=1; start load test (to all the ledgers) 10 writes per sec;; restart all nodes 1 by 1 (from Node1 till Node 7) => all nodes set correct viewNo=1 and ppSeqNo;
 nodes participates in ordering with the current load
 ** Load testing on 25 Nodes AWS pool
 *** Restart all nodes (/)
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart all nodes at the same time
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 *** Restart 1 by 1 (/)
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart nodes 1 by 1 (Node1 - Node25)
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 *** Restart f non-primary nodes (/)
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart f nodes at the same time (Nodes 18-25)
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 *** Restart f primary nodes (/)
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart f nodes at the same time (Nodes 1-8)
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 *** Restart n-f-1 non-primary nodes
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart n-f-1 nodes at the same time (Nodes 10-25)
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 *** Restart n-f-1 primary nodes
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart n-f-1 nodes at the same time (Nodes 1-16)
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 *** Restart more than n-f nodes with disabled watchdog
 **** Set ENABLE_INCONSISTENCY_WATCHER_NETWORK to False
 **** Start acceptance load as in 1983
 **** Wait for 1 min
 **** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 **** Restart more than n-f nodes at the same time (Nodes 1-20)
 **** Make sure that every node participates in consensus
 **** Stop the load
 **** Make sure that all nodes have equal data
 # Backup primary continues ordering after restart
 ** Docker / system tests (pool with 7 nodes):
 *** do view change so that viewNo=1; send 5 txns; restart the 3d node (primary on instance 1)
 send more txns => instance 1 orders transactions
 ** Load testing on 25 Nodes AWS pool
 *** Start acceptance load as in 1983
 *** Wait for 1 min
 *** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 *** Restart the 3d node (primary on instance 1)
 *** Make sure that instance 1 orders transaction and not removed
 # Backup non-primary continues ordering after restart
 ** Docker / system tests (pool with 7 nodes):
 *** do view change so that viewNo=1; send 5 txns; restart the 6th node (non-primary on instance 1)
 send more txns => Node 6 orders on instance 1
 ** Load testing on 25 Nodes AWS pool
 *** Start acceptance load as in 1983
 *** Wait for 1 min
 *** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 *** Restart the 6th node (non-primary on instance 1)
 *** Make sure that Node 6 orders on instance 1
 # Correct primaries after demotion and promotion leading to changes of F (6 -> 7; 7 ->6; etc.)
 ** See INDY-1720
 ** Docker / system tests (pool with 7 nodes):
 *** Demote non-primary
 **** do view change so that viewNo=1; send 5 txns;
 **** Demote Node 7
 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order
 *** Demote master primary
 **** do view change so that viewNo=1; send 5 txns;
 **** Demote Node 2 (master primary)
 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order
 *** Demote backup primary
 **** do view change so that viewNo=1; send 5 txns;
 **** Demote Node 3 (primary on instance 1)
 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order
 ** Docker / system tests (pool with 6 nodes):
 *** Add a node
 **** do view change so that viewNo=1; send 5 txns;
 **** Add 7th node
 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order
 *** Promote a non-primary Node
 **** do view change so that viewNo=1; send 5 txns;
 **** Demote Node 7
 **** Promote Node 7
 **** send more txns => make sure that view has changed twice, all nodes have the same primaries on all instances, and the pool can order
 *** Promote master primary
 **** do view change so that viewNo=1; send 5 txns;
 **** Demote Node 3 (master primary for viewNo 2)
 **** Wait till view changed to 2
 **** Promote Node 3
 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order
 *** Promote backup primary
 **** do view change so that viewNo=1; send 5 txns;
 **** Demote Node 4 (backup primary for viewNo 2)
 **** Wait till view changed to 2
 **** Promote Node 4
 **** send more txns => make sure that view has changed, all nodes have the same primaries on all instances, and the pool can order",,,,,,,,,,INDY-1946,,,,INDY-1720,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:000006r20c4",,,,Unset,Unset,Ev-Node 19.07,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,sergey.khoroshavin,Toktar,VladimirWork,,,,,,,"02/Apr/19 8:40 PM;sergey.khoroshavin;First load test on AWS pool showed that nodes sometimes were crashing after view change:
{code}
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]: Traceback (most recent call last):
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     looper.run()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return self.loop.run_until_complete(what)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return future.result()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     raise self._exception
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     result = coro.send(None)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     await self.runOnceNicely()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     msgsProcessed = await self.prodAllOnce()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     s += await n.prod(limit)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 336, in prod
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     c = await super().prod(limit)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 375, in wrapper
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return await f(self, *args, **kwargs)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1326, in prod
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     c += await self.serviceViewChanger(limit)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 375, in wrapper
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return await f(self, *args, **kwargs)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1396, in serviceViewChanger
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     i = await self.serviceViewChangerInbox(limit)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1727, in serviceViewChangerInbox
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     await self.view_changer.serviceQueues(limit)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/view_change/view_changer.py"", line 472, in serviceQueues
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return await self.inBoxRouter.handleAll(self.inBox, limit)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 104, in handleAll
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     await self.handle(item)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 86, in handle
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     res = self.handleSync(msg)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 75, in handleSync
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return self.getFunc(msg[0])(*msg)
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/view_change/view_changer.py"", line 451, in process_vchd_msg
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     self._start_selection()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/view_change/view_changer.py"", line 642, in _start_selection
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     self.provider.select_primaries()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/view_change/node_view_changer.py"", line 87, in select_primaries
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     self._node.select_primaries()
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 3125, in select_primaries
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     replica = self.replicas[i]
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 248, in __getitem__
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]:     return self._replicas[item]
Apr 02 10:29:16 ohioQALive11.qatest.evernym.com env[16927]: KeyError: 2
{code};;;","02/Apr/19 10:57 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev883
plugins 0.9.6~7

Steps to Reproduce:
1. Run 10 nyms/sec load test with forced VCs every 1800 seconds.

Actual Results:
We have 16th and 17th node that *stop ordering* and have view different from other nodes. This nodes *don't catch up* under load and after it so only way to catch up them is service restart. Also there is an exception that described above.

Expected Results:
There should be no stalled nodes at this load rate.

Logs and metrics:
ev@evernymr33:logs/02_04_2019_view_changes_nyms_only_logs.tar.gz
ev@evernymr33:logs/02_04_2019_view_changes_nyms_only_metrics.tar.gz;;;","03/Apr/19 4:35 PM;Derashe;Fix: [https://github.com/hyperledger/indy-plenum/pull/1147];;;","03/Apr/19 8:37 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev884
plugins 0.9.6~7

Steps to Reproduce:
1. Run all ledgers load test (load rate 10) with forced VCs every 1800 seconds.

Actual Results:
We have 1st and 3rd node that *stop ordering* and have view different from other nodes. This nodes *don't catch up* under load and after it so only way to catch up them is service restart. 

Expected Results:
There should be no stalled nodes at this load rate.

Logs and metrics:
ev@evernymr33:logs/03_04_2019_view_changes_all_ledgers_logs.tar.gz
ev@evernymr33:logs/03_04_2019_view_changes_all_ledgers_metrics.tar.gz;;;","04/Apr/19 8:36 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev885
plugins 0.9.6~7

Steps to Reproduce:
1. Run all ledgers load test (load rate 10) with forced VCs every 1800 seconds.

Actual Results:
We have 6th node that stop ordering and have view different from other nodes. This node doesn't catch up under load and after it. Also there were catch up problems with 12th, 18th, 22nd nodes *under load* but after load they catch up successfully.

Expected Results:
There should be no stalled nodes at this load rate.

Logs and metrics:
ev@evernymr33:logs/04_04_2019_view_changes_all_ledgers_logs.tar.gz
ev@evernymr33:logs/04_04_2019_view_changes_all_ledgers_metrics.tar.gz
ev@evernymr33:logs/04_04_2019_interesting_txns.tar.gz;;;","05/Apr/19 9:37 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev887

Steps to Reproduce:
1. Run all ledgers (*without payments*) load test (load rate 10) with forced VCs every 1800 seconds.

Actual Results:
We have 5, 6, 7, 8, 20 nodes that stop ordering and catching up and have view different from other nodes. *After some time the whole pool has lost consensus.*

Expected Results:
There should be no stalled nodes at this load rate. Pool should sustain this load rate more time than just ~10 hours.

Logs and metrics:
ev@evernymr33:logs/05_04_2019_view_changes_all_ledgers_logs.tar.gz
ev@evernymr33:logs/05_04_2019_view_changes_all_ledgers_metrics.tar.gz
ev@evernymr33:logs/05_04_2019_interesting_txns.tar.gz;;;","05/Apr/19 10:58 PM;VladimirWork;*1 by 1 case*

Build Info:
indy-node 1.7.0~dev886

Steps to Reproduce:
1. Start acceptance load with all ledgers except payment ledger.
2. Wait for some time.
3. Provoke view change (stop 1st Node; make sure view has changed; start it back)
4. Restart nodes 1 by 1 (Node1 - Node25).
5. Make sure that every node participates in consensus
6. Stop the load.

Actual Results:
First 11 and last 14 nodes have different amount of txns in the ledgers. Pool has lost consensus and the lesser group of nodes doesn't catch up with the bigger one.

Expected Results:
There should be no stalled nodes and consensus loss.

Logs:
ev@evernymr33:logs/2025_04_04_2019_1_by_1_stop_case.tar.gz;;;","09/Apr/19 5:57 PM;Toktar;[~VladimirWork]
Please, re-test last load. But wait ~1 hour after restarting Node25 and check that a view change not in progress before stopping load test. Thank you!;;;","10/Apr/19 12:02 AM;VladimirWork;Build Info:
indy-node 1.7.0~dev888

Steps to Reproduce:
1. Run all ledgers (without payments) load test (load rate 10) with forced VCs every 1800 seconds.

Actual Results:
We have 3, 6, 7, 22, 23 nodes that stop ordering and catching up and have view different from other nodes.

Expected Results:
There should be no stalled nodes at this load rate. Pool should sustain this load rate more time than just ~10 hours.

Logs and metrics:
ev@evernymr33:logs/09_04_2019_view_changes_all_ledgers_logs.tar.gz
ev@evernymr33:logs/09_04_2019_view_changes_all_ledgers_metrics.tar.gz;;;","11/Apr/19 8:52 PM;VladimirWork;[~Toktar]
> Please, re-test last load. But wait ~1 hour after restarting Node25 and check that a view change not in progress before stopping load test. Thank you!
The issue doesn't reproduce against 1.7.0~dev888.;;;","12/Apr/19 6:35 PM;VladimirWork;All ledgers during production load with payments and forced VCs against the latest master node and plugins are in sync but there is some issue with sovtoken ledger - it has too few txns (7.6k) and there is no writes there but domain ledger looks good (713k txns and still growing). ViewNo is the same for all nodes. << This case investigation and work on all cases untested in scope of this ticket will be continued in INDY-2051. All system tests implemented in scope of this ticket are in https://github.com/hyperledger/indy-test-automation/pull/21 (TestAuditSuite).;;;",,,,,,,,,,,
Node should ensure that target package for upgrade is available during POOL_UPGRADE validation ,INDY-2026,38515,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,15/Mar/19 10:00 PM,02/Apr/19 9:26 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Currently indy-node during POOL_UPGRADE txn dynamic validation doesn't check whether system package with specified package name and target version is available or not:
 * upgrade is scheduled regardless of that
 * that check is delayed until NodeControlTool starts real upgrade

It makes sense to add such check to dynamic validation to reject incorrect POOL_UPGRADE txns earlier.

Options:
 # send request to NodeControlTool to make such validation during dynamic validation phase",,,,,,,,,,,,,,,,,,,,,INDY-1992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00gdj:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix intermittent failures and get rid of timeouts in system tests,INDY-2027,38543,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,18/Mar/19 6:31 PM,21/May/19 10:58 PM,28/Oct/23 2:47 AM,20/May/19 11:02 PM,,,1.9.0,,,,,0,,,,,Fix intermittent failures and get rid of timeouts in system tests,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw960bi",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"19/Mar/19 12:46 AM;VladimirWork;https://github.com/hyperledger/indy-test-automation/pull/20  - PR with fixes and updates;;;","20/May/19 11:02 PM;VladimirWork;PR was merged.;;;",,,,,,,,,,,,,,,,,,,,
As a QA I want system tests to be run in parallel in CD pipeline,INDY-2028,38544,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,VladimirWork,VladimirWork,18/Mar/19 6:51 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,,,0,,,,,As a QA I want system tests to be run in parallel in CD pipeline to reduce test run time.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00000r",,,,Unset,Unset,Ev-Node 19.06,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,"28/Mar/19 9:13 PM;andkononykhin;Problem reason:
 * need to reduce test run time using parallel execution

Changes: 
 * implemented parallel execution per test

PR:
 * https://github.com/hyperledger/indy-node/pull/12191.6.877-master

Version:
 * 1.6.877-master

Risk factors:
 * changes impact CD pipeline

Risk:
 * Low

Covered with tests:
 * no

Recommendations for QA:
 * no, pipeline works;;;",,,,,,,,,,,,,,,,,,,,,
Leverage Consistency Proof feature when deciding the transaction a catchup needs to be performed to,INDY-2029,38632,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,21/Mar/19 4:23 PM,09/Oct/19 5:14 PM,28/Oct/23 2:47 AM,09/Oct/19 5:14 PM,,,1.13.0,,,,,0,,,,,"*Problem:*
 * The first step in the catchup logic (which is with a presence of audit ledger is done for the Audit ledger only) is to decide how many transactions need to be caughtup.
 * ConsistencyProof message containing ledger size and consistency proof is used for this
 ** see [https://github.com/hyperledger/indy-plenum/blob/master/docs/source/catchup.md#ledger-catchup-steps]
 ** [https://github.com/hyperledger/indy-plenum/blob/master/docs/source/diagrams/catchup-procedure.png]
 * As of now, *f+1 equal* ConsistencyProofs are required.
 * It can be hard to get exactly equal consistency proofs during a high load.
 * Example:
 ** NodeA sent ConsistencyProof(seqNoEnd=100)
 ** NodeB sent ConsistencyProof(seqNoEnd=101)
 ** NodeC sent ConsistencyProof(seqNoEnd=102)
 * Here we don't have equal ConsistencyProof.
 * Why it works during the load, is because there is a logic to request missing consistency proofs explicitly. 
In this example the catching up node will request ConsistencyProof(seqNoEnd=101) (sine it requests a median value).
So, eventually we get f+1 equal ConsistencyProofs.
 * *But this is very inefficient, especially during high load*

 

*Acceptance Criteria*
 * Change the ConsistencyProofs processing logic to not require equal ConsistencyProof, but get the transaction number to catchup from the merkle tree associated with received ConsistencyProofs

 ** Each `ConsistencyProof` msg has `hashes` field which is actually a Consistency Proof that a merkle tree with the given root extends the original tree.
 ** The client needs to create a tmp tree from all the valid `ConsistencyProof` msgs (for which `hashes` is a valid proof)
 ** The client needs to get f+1 results forming the highest tree
 ** The result txn_count is the minimum among these f+1  `ConsistencyProof` msgs
 * Cover by unit tests
 * Cover by integration tests

 

 ",,,,,,,,,,,,,,,,,,,,,INDY-2083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwvif:00001yw9v",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"09/Oct/19 5:14 PM;ashcherbakov;It looks like it's not easily possible.;;;",,,,,,,,,,,,,,,,,,,,,
ID sent by client in Revoc txns must match the TXN_ID in the txn,INDY-2030,38633,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Mar/19 4:51 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,"*Problem*
 * ID is one of the field sent by the cleint in Revoc txns
 * This is assumed to match txnId, that is a key of the txn in the State Trie
 * However, as of now Ledger doesn't validate if the passed ID is equal to the generated key (txnId)

*Acceptance criteria*
 * As part of static validation of all Revoc txns, make sure that the passed ID is equal to the state key for this txn.
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,INDY-1554,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c98s",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validator node shows False for consensus,INDY-2031,38689,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Duplicate,andkononykhin,lbendixsen,lbendixsen,23/Mar/19 1:53 AM,02/Apr/19 8:52 PM,28/Oct/23 2:47 AM,02/Apr/19 8:52 PM,,,,,,,,0,TShirt_M,,,,"The danube node on BuilderNet has the same number of txns for all ledgers yet displays 'false' for ""Has_write_consensus"" in validator-info for all 4 ledgers.  The problem was first noticed on Wednesday Mar 20, 2019.  After the problem was noticed, a nym was added to the ledger and the danube node did not update its ledger to match (it was then actually out of sync for a time) but after a restart the node caught up and txn count was correct, but after a while it once again showed that it was out of consensus in validator-info.

Today (Friday Mar 22) all nodes are reporting Has_write_consensus as False for all 4 ledgers (from results of get-validator-info)

Indy node version - 1.6.83 

The new builder net has a total of 4 nodes in it from around the world and I own one of the nodes in it (FoundationBuilder).  I will attach logs from my node and from the danube node, but please note that the danube node validator log covering the time of the original problem are lost and unrecoverable.

 ",,,,,,,,,,,,,,,,,,,,INDY-2020,,,,,,,,,,,"23/Mar/19 2:02 AM;lbendixsen;FoundationBuilder.tgz;https://jira.hyperledger.org/secure/attachment/16978/FoundationBuilder.tgz","23/Mar/19 2:04 AM;lbendixsen;danube-2.tar;https://jira.hyperledger.org/secure/attachment/16980/danube-2.tar","23/Mar/19 2:59 AM;mgbailey;danube.journalctl.txt;https://jira.hyperledger.org/secure/attachment/16983/danube.journalctl.txt","23/Mar/19 2:59 AM;mgbailey;danube.tgz;https://jira.hyperledger.org/secure/attachment/16982/danube.tgz","23/Mar/19 2:04 AM;lbendixsen;journalctl.txt;https://jira.hyperledger.org/secure/attachment/16979/journalctl.txt","23/Mar/19 3:04 AM;mgbailey;validator-info.txt;https://jira.hyperledger.org/secure/attachment/16981/validator-info.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000006r20do",,,,Unset,Unset,Ev-Node 19.07,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,lbendixsen,,,,,,,,,,"23/Mar/19 2:08 AM;lbendixsen;FoundationBuilder.tgz contains the FoundationBuilder.log file up to the time it was uploaded (~Friday, Mar 22 11AM MST).  Danube.tar was the first log file from Danube and danube-2.tar (which probably also contains what was in danube.tar) was collected Mar 21 at about 4pm mountain time. Please let me know what other logs you might need.;;;","02/Apr/19 8:51 PM;andkononykhin;Short summary of things that happened:
 * danube missed one preprepare for freshness txn
 * started catchup but didn't receive the txn during that since catchup in that version (1.6.58 stable) brought only real ledger txns
 * after catchup the node received txns but since one txn is missed it stashed them
 * once new checkpoint came it did catchup again but it didn't help by the same reason
 * ... the same logic repeated

The issues is known and fixed in master in scope of INDY-2022;;;",,,,,,,,,,,,,,,,,,,,
idr_cache may work incorrectly when writing to multiple ledgers,INDY-2032,38790,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ashcherbakov,ashcherbakov,27/Mar/19 4:18 PM,10/Apr/19 7:51 PM,28/Oct/23 2:47 AM,10/Apr/19 7:50 PM,,,1.7.1,,,,,0,TShirt_L,,,,"There are the following errors in the log in a load test with forced view change when writing to all the ledgers (domain, pool, config):
{code:java}
2019-03-24 04:24:16,373|WARNING|idr_cache.py|3PC: Node5: The first created batch has not been committed or reverted and yet another batch is trying to be committed, b""\x954'\x81\x15p[\xb2\x03\xa2\xb5=\x94\x19q?\x03\xa2\xf6_\x8f8N\x13\xa6 |\x14M\xcc,\xc5"" b'\xed0\xc2\x96\xf9\x05\xec\xdc\xcb\xe3\xff\x014\x85\xb8\xd4\xf8\xa5\xc1\xe5\xbe\x8b\xb5.\xe1\x04P\xfb\x7fQ\x96@'
 {code}
The logs can be found at `ev@evernymr33:logs/1993_25_03_2019_view_changes_logs.tar.gz`",,,,,,,,,,,,,,,,,,,,,,,,INDY-2034,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:000006r20c9",,,,Unset,Unset,Ev-Node 19.07,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"09/Apr/19 6:03 PM;ashcherbakov;h2. Issue 1: Causes phantom txns in audit ledger and broken idr_cache (the same as in INDY-2034)

*Problem reason/description:* 
- A node performs view change to view X 
- It does catch-up as part of view change
- Since the node is lagging behind, it caught up till (X+2, Y), that is it got txns from the future view that other nodes are already ordering in.
- The Node finishes view change to X, and it turned out that it's a master primary in view X
{color:#de350b}- The Node applies and sends PrePrepares for view X, although it last ordered is already for view X+2 => it has Z unordered batches 
{color:#172b4d}- The node starts view change to view X+1, and is about to start catchup
- The node reverts unordered batches.{color} It can not revert Z unordered batches, since  their 3PC key (view X) is less than last ordered (view X+2)
{color}=> The node has phantom txns in audit ledger and idr_cache{color:#de350b}
{color}

 

*Changes:* 
- Fix sending of 3PC messages by a primary on an old view during or in between view changes.
- Get rid of stashedOrderedRequest
- Improved replica 3PC validation to allow only Commits during the view change.

 

*PR:*
- [https://github.com/hyperledger/indy-plenum/pull/1151]

 

*Version:*
- 1.7.0.dev888

 

*Risk factors:*
- View Change


*Risk:*
- Med

 

*Covered with tests:*
- test_catchup_to_next_view_during_view_change_0_to_1_then_1_to_2
- test_catchup_to_next_view_during_view_change_by_primary
- unit tests in [test_replica_3pc_validation.py|https://github.com/hyperledger/indy-plenum/pull/1151/files#diff-e82220bb9f03171e61bf26ab539b2d6a]

 

*Recommendations for QA:*
- run load test with forced view change for three ledgers (domain, pool, config) with 10 writes per sec (in the scope of INDY-2025);;;","10/Apr/19 7:50 PM;ashcherbakov;After the fix, the issue is not reproduced during recent load tests (forced view change, writes to all ledgers).;;;",,,,,,,,,,,,,,,,,,,,
TRUSTEE cannot change trust anchor's nym role on ledger from <None> to TRUST_ANCHOR,INDY-2033,38799,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,sklump,sklump,27/Mar/19 11:05 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,plenum,,,,0,EV-CS,node-SDK,quality,TShirt_S,"A variation of INDY-1989 is back: now, acting as the Trustee, I cannot change the role in a user's cryptonym from `<None>` to `TRUST_ANCHOR` - I get

```
(1012) Ledger rejected transaction request: client request invalid: UnauthorizedClientRequest('TRUSTEE can not touch role field since only the owner can modify it',)

```


. Contrast https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md row #3 which states that the Trustee should have permission.

I am running

```
indy_plenum_ver=1.6.726
indy_node_ver=1.6.862
","ubuntu 16.04
indy-sdk=1.8.1-dev-1040
indy_plenum_ver=1.6.726
indy_node_ver=1.6.862",28800,28800,,0%,28800,28800,,,,,,,,,,,,,,,,,,,,,,,,"28/Mar/19 9:30 PM;sklump;conftest-identity_my.py;https://jira.hyperledger.org/secure/attachment/17016/conftest-identity_my.py","29/Mar/19 12:38 AM;sklump;conftest-ta-none-ta-identity_my.py;https://jira.hyperledger.org/secure/attachment/17019/conftest-ta-none-ta-identity_my.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywai",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),mgbailey,Q1Blue,sklump,VladimirWork,,,,,,,,"28/Mar/19 4:41 PM;VladimirWork;Build Info:
indy-node 1.6.862

Steps to Validate:
Case 1:
1. Add nym without role by default Trustee.
2. Change nym's role from None to TRUST_ANCHOR by default Trustee.

Case 2:
0. Add new Trustee by default Trustee.
1. Add nym without role by default Trustee.
2. Change nym's role from None to TRUST_ANCHOR by new Trustee from Step 0.

Actual Results:
Role has been changed successfully in both cases:
{noformat}
{'op': 'REPLY', 'result': {'ver': '1', 'txn': {'protocolVersion': 2, 'type': '1', 'metadata': {'digest': '6eef48815c9f3cb44f87efb93fce53684a22f439af52baa6c9b1f8effd5521db', 'reqId': 1553757472302306201, 'from': 'V4SGRU86Z58d6TV7PBUe6f'}, 'data': {'dest': '7m43drkwwM7UGNFFbtVLHn', 'verkey': '4ggZ47wLFa2du1bUAEV8fKAzkGDAExjSXHCCpF2JBYmU'}}, 'txnMetadata': {'txnId': '39d778f6e55642d47b879c3c5e8004333e5e3a5a68a32ea4c24aec6e1451b5fb', 'txnTime': 1553757472, 'seqNo': 10}, 'rootHash': 'Dbf8A8bvgRXvQRcsDCMLta8ByPhF4hyWtTuCE9Nh3Shg', 'reqSignature': {'type': 'ED25519', 'values': [{'value': 'jGNqrbYLrLfJGoWq7s5qogwLEdYxpuAr3CDssByqk8Bo5t5noux26SfbNeBm6wwu9dh6czUvBT2uHjaWoLaurvQ', 'from': 'V4SGRU86Z58d6TV7PBUe6f'}]}, 'auditPath': ['2sw2eXpQv3ZxWUztZhbDEaJq9yrPvUiNkGMCsEdiunBs', '6NVGJjQVapQ7HV5Xg5y8u5sHjEMp7hEXDhbcrHZ6SZsV']}}

{'op': 'REPLY', 'result': {'ver': '1', 'txn': {'protocolVersion': 2, 'type': '1', 'metadata': {'digest': '8346f2e56f9dd1f7ac0839f837906a53c8a1f63ef00dac34d746981e49e3f712', 'reqId': 1553757490568587424, 'from': 'V4SGRU86Z58d6TV7PBUe6f'}, 'data': {'role': '101', 'dest': '7m43drkwwM7UGNFFbtVLHn'}}, 'auditPath': ['ANETHcNFEudyBeZbM4wCchkan2xitYwWYvdK6gtNncT9', '6NVGJjQVapQ7HV5Xg5y8u5sHjEMp7hEXDhbcrHZ6SZsV'], 'rootHash': 'AEzS3hmN9PtsW9Gf9ykTkFWyyfQ2CipyxAt6FdSdFYXf', 'reqSignature': {'type': 'ED25519', 'values': [{'value': '2oBuj5KG8SxFt3sz9G57cHXTDQwgsc7xGLVZ4Ehj4MyTFrZvgqniDHUiAPmQxmHNRjrwddpZq922PCVhjTzipBCU', 'from': 'V4SGRU86Z58d6TV7PBUe6f'}]}, 'txnMetadata': {'txnId': '39d778f6e55642d47b879c3c5e8004333e5e3a5a68a32ea4c24aec6e1451b5fb', 'txnTime': 1553757490, 'seqNo': 11}}}
{noformat}
---
{noformat}
{'op': 'REPLY', 'result': {'txnMetadata': {'txnTime': 1553758226, 'seqNo': 16, 'txnId': '65c2840f4a85ceef631e04be46ae6db0fc9d3bff5dc64c256d8d8dcc1838c06a'}, 'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '5JmA8cYT6ZyW89dSktseHYLv3u1D4UQZq3ox91fYZ3b1nxudLbsyHMAVAGcuFLu3iSg7vWxV17Jvt8Bp2Lz8MwzL'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'metadata': {'digest': 'a5e2bc8a5fd504e1fdcce05534ffd45aa99cd4f9dc4ece47e1fd9087603bc4e9', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1553758225216949095}, 'data': {'dest': 'UU6dv4unZe4UGGY5VtSm2J', 'verkey': 'FyHwfWsywJfjNxqaAPawRhjSNFgqzkLbFZ5imp4DoboA'}, 'type': '1'}, 'auditPath': ['97YhCeuGC1GmTeECvnoPwbVcbWEyeYE9whyfktmejEYr', '9Cpsh8U3JML7MWNc16VY1anqvz6rzAZQoMqCBypiveV1', '7AMih1DPe2i191CY9CBEqzGbYYsRwY62BQgYfReiDbiD', '6NVGJjQVapQ7HV5Xg5y8u5sHjEMp7hEXDhbcrHZ6SZsV'], 'ver': '1', 'rootHash': 'AKPwa7KTWUyN8sKoD7Tdog1Ur7zaaebtBb2fLaG6jfJn'}}

{'op': 'REPLY', 'result': {'txnMetadata': {'txnTime': 1553758227, 'seqNo': 17, 'txnId': '65c2840f4a85ceef631e04be46ae6db0fc9d3bff5dc64c256d8d8dcc1838c06a'}, 'reqSignature': {'values': [{'from': 'PMvxchTQjSLbXYaCGumABU', 'value': '2LQ3Md8ftRMybMUntPPYjuaG4U8U2izorCmtqURiVqWg4pGBbxrkF7GKHu5774CYMSV4f3LuFeTyEqn2aCDMcd7W'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'metadata': {'digest': 'dbb92129715f3a84f20cedbcc039603a715ddaef5bdaa77fc6395fc825a83fdd', 'from': 'PMvxchTQjSLbXYaCGumABU', 'reqId': 1553758226223028738}, 'data': {'dest': 'UU6dv4unZe4UGGY5VtSm2J', 'role': '101'}, 'type': '1'}, 'auditPath': ['AKPwa7KTWUyN8sKoD7Tdog1Ur7zaaebtBb2fLaG6jfJn'], 'ver': '1', 'rootHash': 'H6kyoofiBGeMp12ZnoJbcMpdktLFkAF9YHm46FF7ayr'}}
{noformat}
;;;","28/Mar/19 4:49 PM;VladimirWork;Build Info:
indy-node 1.6.862

Steps to Reproduce:
1. Add nym with TRUST_ANCHOR  by default Trustee.
2. Change nym's role from TRUST_ANCHOR to None by default Trustee << Actually this is not a valid way to remove role since we should send *empty string* to do that but this txn (with None, not with the '') also returns REPLY from the ledger *so we should fix this behaviour anyway*.
3. Change nym's role from None to TRUST_ANCHOR again.

Actual Results:
There is an error in Step 3:
{'reqId': 1553759057070217822, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reason': ""client request invalid: UnauthorizedClientRequest('TRUSTEE can not touch role field since only the owner can modify it',)"", 'op': 'REJECT'}

Expected Results:
We should reject txn in Step 2 since it contains None instead of empty string maybe since we send None role only at nym's adding and role removing should be made by empty string according to API.

Additional Info:
The case reproduces *for each role* that is different from None: ['TRUSTEE', 'STEWARD', 'TRUST_ANCHOR', 'NETWORK_MONITOR'].;;;","28/Mar/19 9:25 PM;sklump;Role None corresponds to minimal USER, which is a legitimate use case. Empty string corresponds to a role in reset.

To reproduce from indy-sdk (python wrapper), replace in pytest fixture `identity_my` in `indy-sdk/wrappers/python/tests/conftest.py` with:
-----
(there is no excuse for how terrible this editor is for code - please see attachment)
-----

Then edit `indy-sdk/wrappers/python/tests/ledger/test_role.py` to specify minimalist pytest
-----
import pytest


@pytest.mark.asyncio
async def test_role(wallet_handle, pool_handle, identity_trustee1, identity_my):
    pass
-----
, engaging the code in the identy_my fixture.

Then, at the prompt,
$ cd indy-sdk/wrappers/python/tests/ledger
$ docker rm $(docker stop $(docker ps -aq))
$ docker run -d --ip=10.0.0.2 --net=indy_pool_network indy_pool
$ pipenv run pytest -s test_role.py

Result:
-----
Trustee V4SGRU86Z58d6TV7PBUe6f sending role None, my_verkey 3445KbRCWkmRcmUd9mbwjrACKrKTqiVaUdh91toBqd7L for my_did 4mS58yZJ88qnzcUK4ddYwA
.. NYM_REQUEST {
    ""reqId"": 1553775619959271270,
    ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"",
    ""operation"": {
        ""dest"": ""4mS58yZJ88qnzcUK4ddYwA"",
        ""type"": ""1"",
        ""verkey"": ""3445KbRCWkmRcmUd9mbwjrACKrKTqiVaUdh91toBqd7L""
    },
    ""protocolVersion"": 2
}
.. NYM response {
    ""op"": ""REPLY"",
    ""result"": {
        ""reqSignature"": {
            ""values"": [
                {
                    ""value"": ""3493TYErrWMwuDmGpvyzM9gwhz57uX5vh24rGmj3kryc55zeHjPsxUeJpUzeT33ne7iBjZS5BWG74TWroFB8SEKC"",
                    ""from"": ""V4SGRU86Z58d6TV7PBUe6f""
                }
            ],
            ""type"": ""ED25519""
        },
        ""ver"": ""1"",
        ""auditPath"": [
            ""EsY4hbw8MPXuyQTiq43pvwJqak6pGzfKwJKMXoi6uYS7"",
            ""DNHM372JZJoGcxdHdmsj3QSSiomyeZux6ssJXxAJqyvd""
        ],
        ""txn"": {
            ""data"": {
                ""dest"": ""4mS58yZJ88qnzcUK4ddYwA"",
                ""verkey"": ""3445KbRCWkmRcmUd9mbwjrACKrKTqiVaUdh91toBqd7L""
            },
            ""metadata"": {
                ""reqId"": 1553775619959271270,
                ""digest"": ""4980813af45cd063641c654980f8f51cb74dd34c24aa8945ef71f08aece735b2"",
                ""from"": ""V4SGRU86Z58d6TV7PBUe6f""
            },
            ""protocolVersion"": 2,
            ""type"": ""1""
        },
        ""rootHash"": ""58NkzNdFZydAiJyLD4GbHSiu14bmFnYjdNdDyqTfMjyg"",
        ""txnMetadata"": {
            ""seqNo"": 11,
            ""txnId"": ""ca8fd511afb74b16d87097c22763f37f777427f924e97d3f09d59796811444c7"",
            ""txnTime"": 1553775620
        }
    }
}
.. sleeping 5 seconds
.. GET_NYM_REQUEST {
    ""reqId"": 1553775625865158059,
    ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"",
    ""operation"": {
        ""type"": ""105"",
        ""dest"": ""4mS58yZJ88qnzcUK4ddYwA""
    },
    ""protocolVersion"": 2
}
.. GET_NYM response {
    ""result"": {
        ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"",
        ""txnTime"": 1553775620,
        ""type"": ""105"",
        ""dest"": ""4mS58yZJ88qnzcUK4ddYwA"",
        ""seqNo"": 11,
        ""reqId"": 1553775625865158059,
        ""data"": ""{\""dest\"":\""4mS58yZJ88qnzcUK4ddYwA\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":null,\""seqNo\"":11,\""txnTime\"":1553775620,\""verkey\"":\""3445KbRCWkmRcmUd9mbwjrACKrKTqiVaUdh91toBqd7L\""}"",
        ""state_proof"": {
            ""root_hash"": ""G8EqxT6tBT9MpjJ9SvLbbgTzKtZpoJuAc3ZPXKUdwGKB"",
            ""proof_nodes"": ""+QHo+LKgOo/VEa+3SxbYcJfCJ2Pzf3d0J/kk6X0/CdWXloEURMe4j/iNuIt7ImlkZW50aWZpZXIiOiJWNFNHUlU4Nlo1OGQ2VFY3UEJVZTZmIiwicm9sZSI6bnVsbCwic2VxTm8iOjExLCJ0eG5UaW1lIjoxNTUzNzc1NjIwLCJ2ZXJrZXkiOiIzNDQ1S2JSQ1drbVJjbVVkOW1id2pyQUNLcktUcWlWYVVkaDkxdG9CcWQ3TCJ9+QExoNPSP24JsVps7QufK62cHm4MLrVBpYu1VMlThcJrixajgICgmpq6PvRB/76zSDjdvXO+dATJAmHaV82rEVG2ZoAO+TCAoAIbx/TDY2y4OJtZiJtzVNjJICBQpJ4h68cXrBVl0wEvoEQAggmzS6f2NWpUQAFsJZORzSvJtNWsWBopTosPpyIZgICAgKAkcibehQ5iUOtCXD3lQORF05za0YiwT2DavXYCOkmJQqB5qfd+1iq6sbYvfigSUU9IAfb0K8M2fXBa1H5EomVc3KB9JVxOO1bVJRb8Jwgua4GPO/Juk6XhGUySCneElMV7aqAb0qE5bnVw9F5IUz5uGMXwnAHQmog75MzPMOjuL+f3taA7Ohl8US/Ipw9m90WNb/7n5LAxzanxSmitjBCIzSGcGoA="",
            ""multi_signature"": {
                ""value"": {
                    ""state_root_hash"": ""G8EqxT6tBT9MpjJ9SvLbbgTzKtZpoJuAc3ZPXKUdwGKB"",
                    ""pool_state_root_hash"": ""GT89NJKZFSV4i4L2cRjDRCnfeDXFBjsAZn7SBLEdxoJE"",
                    ""timestamp"": 1553775620,
                    ""ledger_id"": 1,
                    ""txn_root_hash"": ""58NkzNdFZydAiJyLD4GbHSiu14bmFnYjdNdDyqTfMjyg""
                },
                ""signature"": ""RUxEZprjtaXjLUQqL8EqqjFb7Hqfz2ptAwURVL8YmoTwWbE9o5noSMVBsUQHByVPuYKhhUDYGCKYz742pLFwfKJJoqwaa76gcZWUQdggb4UdiMAV355SiZoazbWCATf4rgSc5qhyNbU2zL8APKvcHz72HaqVDQAk5rse1x4X39kbD9"",
                ""participants"": [
                    ""Node3"",
                    ""Node2"",
                    ""Node1""
                ]
            }
        }
    },
    ""op"": ""REPLY""
}, data role: [None]


Trustee V4SGRU86Z58d6TV7PBUe6f sending role TRUST_ANCHOR, my_verkey 3445KbRCWkmRcmUd9mbwjrACKrKTqiVaUdh91toBqd7L for my_did 4mS58yZJ88qnzcUK4ddYwA
.. NYM_REQUEST {
    ""reqId"": 1553775625907752398,
    ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"",
    ""operation"": {
        ""dest"": ""4mS58yZJ88qnzcUK4ddYwA"",
        ""role"": ""101"",
        ""type"": ""1"",
        ""verkey"": ""3445KbRCWkmRcmUd9mbwjrACKrKTqiVaUdh91toBqd7L""
    },
    ""protocolVersion"": 2
}
.. NYM response {
    ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"",
    ""reqId"": 1553775625907752398,
    ""reason"": ""client request invalid: UnauthorizedClientRequest('TRUSTEE can not touch verkey field since only the owner can modify it',)"",
    ""op"": ""REJECT""
}
E
-----
;;;","28/Mar/19 9:51 PM;VladimirWork;Please notice that we have two different cases and errors in the main description and in the comment above:
1. We get `TRUSTEE can not touch {color:red}role{color} field since only the owner can modify it` when we demote nym from some role using None instead of '' and than try to promote it back to some role << we should fix this behaviour to reject None-based demotions as an option maybe.
2. We get `TRUSTEE can not touch {color:red}verkey{color} field since only the owner can modify it` when we add nym with None role and than try to add some role to it sending verkey together with new role << this is not a valid case in the current auth map since we should add\remove roles to existing nym without verkeys' sending and this reject is expected.;;;","29/Mar/19 12:17 AM;sklump;Re: #2, good catch. If the trustee builds and sends a NYM request with a verkey value of None along with the role of TRUST_ANCHOR, the test passes. I think I have to change my side a bit to accommodate -- I was sure it used to work at some point if the verkey value was the same as what was on the ledger, but (a) I could be wrong and (b) the right thing is not to send the verkey for a role change.

I'm still not totally convinced it's clean: let me toy with it a bit more.;;;","29/Mar/19 12:22 AM;VladimirWork;> I was sure it used to work at some point if the verkey value was the same as what was on the ledger
Yes, in old auth map it was acceptable but in new auth map we treat it as verkey rotation even if verkey is the same so verkey rotation should be performed without role value filled and role rotation should be performed without verkey value filled in the latest versions.;;;","29/Mar/19 12:45 AM;sklump;I've reproduced case #1 with the attached replacements to conftest.py (conftest-ta-none-ta-identity_my.py). Pytest, in building identity_my fixture as specified, now attempts to proceed with:
1 - TRUST_ANCHOR
2 - '' (for role change)
3 - None (minimal USER)
4 - '' (role change)
5 - TRUST_ANCHOR.

The operation succeeds through #3, but fails on #4 with
-----
.. NYM response {
    ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"",
    ""reason"": ""client request invalid: UnauthorizedClientRequest('TRUSTEE can not touch role field since only the owner can modify it',)"",
    ""reqId"": 1553787420738213443,
    ""op"": ""REJECT""
}
-----
The error message implies a glitch in logic, since the TRUSTEE should have permission to modify the role field. I believe this is the case #1 you've outlined above.

Do we agree that there is still something to look into here?

In particular, moving off role=None (via role='' for replacement step) should either succeed or else documentation should reflect the fact that there is some other process to add a role beyond minimal USER (role=None).;;;","29/Mar/19 12:50 AM;sklump;... as it turns out, moving off Role=None does not require the intermediate step of sending Role=''. If instead the sequence proceeds like this:
1 - TRUST_ANCHOR
2 - '' (for role change)
3 - None (minimal USER)
4 - TRUST_ANCHOR

then the operation completes successfully. Since '' corresponds to ROLE_REMOVE in the indy-sdk comments, and in a sense Role=None (meaning USER), one can argue that removing a role of None doesn't make sense from point of view of pure data.

I will adapt my code to the reality.;;;",,,,,,,,,,,,,,
Phantom transactions in audit ledger,INDY-2034,38818,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,28/Mar/19 4:19 PM,10/Apr/19 7:51 PM,28/Oct/23 2:47 AM,10/Apr/19 7:51 PM,,,1.7.1,,,,,0,TShirt_M,,,,"*Test cases where the issues reproduced*
The issues are reproduced only when we do forced view change tests and write to domain, pool and config ledgers at the same time.
If we write to domain ledger only, the pool successfully survived 120 forced view changes and continues ordering.
I think the main difference here is that when we write to multiple ledgers, we created multiple PrePrepares (for each ledger) almost at the same time which increases a risk of out-of-order messages.

*The problem:*
1) Some nodes have ""phantom"" audit txns that other nodes don't have. Moreover, there is no corresponding main ledger txn written to the ledger for these nodes, so it looks like it's written to audit ledger only.
Moreover, there is not Ordered msg in longs corresponding to this phantom audit txn. So, it looks like it was applied, but not committed explicitly.
2) Audit ledger tried to revert txns while it's empty (LogicError). It happens at the same time, as issue 1, so it looks like it's related.

*Important notes*
1) There are a lot of warnings `The first created batch has not been committed or reverted and yet another batch is trying to be committed` from idr_cahce (first time it's raised for Pool ledger batch). So, it looks like idr_cahce is broken. See INDY-2032.
2) Issues 1 and 2 are reproduced from a very long sequence of view changes, where it's called from the PreViewChange strategy. The specific here is that view change (and hence catchup, and hence revert of unordered batches) is called from Replica, not from Node (see node's `prod`).
It also leads to the fact that `forceProcessedOredered` ordered non-zero count of Ordered messages before starting a view change.
3) There were cases when we were processing a lot of 3PC messages in one run of replica's service method, and stabilized checkpoints at the same time without letting a chance to execute ordered batches.
4) A lot of PrePrepares are discarded because if incorrect time (because the node was lagging behind). But the corresponding batches were eventually ordered. It looks like the time was accepted eventually because a quorum of Prepares was gathered.

*Logs*
874-master:
ev@evernymr33:logs/1993_25_03_2019_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_25_03_2019_view_changes_metrics.tar.gz

",,,,,,,,,,,,,,,,,,,,,INDY-2032,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:000006r20cc",,,,Unset,Unset,Ev-Node 19.07,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"09/Apr/19 6:04 PM;ashcherbakov;h2. Issue 1: Causes phantom txns in audit ledger and broken idr_cache (the same as in INDY-2032)

See the comment in INDY-2032;;;","10/Apr/19 7:50 PM;ashcherbakov;After the fix, the issue is not reproduced during recent load tests (forced view change, writes to all ledgers).;;;",,,,,,,,,,,,,,,,,,,,
Pool is getting out of consensus after a forced view change and writes to all the ledgers,INDY-2035,38819,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,28/Mar/19 4:23 PM,12/Apr/19 8:12 PM,28/Oct/23 2:47 AM,10/Apr/19 6:06 PM,,,1.7.1,,,,,0,TShirt_L,,,,"If there is a forced view change (every 30 mins), and there is a load of 10 writes per sec to all the ledgers (domain, pool, config), then the pool eventually gets out of consensus.

If we write to domain ledger only, the pool successfully survived 120 forced view changes and continues ordering.
I think the main difference here is that when we write to multiple ledgers, we created multiple PrePrepares (for each ledger) almost at the same time which increases a risk of out-of-order messages.

The last runs' results (forced VCs each 1800 seconds / domain+pool+config txns):

874-master:

ev@evernymr33:logs/1993_25_03_2019_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_25_03_2019_view_changes_metrics.tar.gz

876-master:

ev@evernymr33:logs/1993_27_03_2019_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_27_03_2019_view_changes_metrics.tar.gz

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:000006r20e3",,,,Unset,Unset,Ev-Node 19.07,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"28/Mar/19 7:06 PM;sergey.khoroshavin;Load test on last stable failed with most nodes falling out of consensus. Moreover, these nodes crashed with following traceback:
{code}
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]: Traceback (most recent call last):
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     looper.run()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return self.loop.run_until_complete(what)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return future.result()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     raise self._exception
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     result = coro.send(None)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     await self.runOnceNicely()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     msgsProcessed = await self.prodAllOnce()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     s += await n.prod(limit)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 313, in prod
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     c = await super().prod(limit)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 376, in wrapper
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return await f(self, *args, **kwargs)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1357, in prod
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     c += await self.serviceNodeMsgs(limit)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 376, in wrapper
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return await f(self, *args, **kwargs)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1406, in serviceNodeMsgs
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     await self.processNodeInBox()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 376, in wrapper
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return await f(self, *args, **kwargs)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2003, in processNodeInBox
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     await self.process_one_node_message(m)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2007, in process_one_node_message
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     await self.nodeMsgRouter.handle(m)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     res = self.handleSync(msg)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return self.getFunc(msg[0])(*msg)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 363, in wrapper
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return f(self, *args, **kwargs)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 538, in processCatchupRep
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     self.mark_catchup_completed_if_possible(ledger_info)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 667, in mark_catchup_completed_if_possible
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     self.catchupCompleted(ledger_info.id, (cp.viewNo, cp.ppSeqNo))
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 883, in catchupCompleted
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     self.mark_ledger_synced(ledgerId)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 898, in mark_ledger_synced
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     self.postAllLedgersCaughtUp()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2293, in allLedgersCaughtUp
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     self.processStashedOrderedReqs()
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 3569, in processStashedOrderedReqs
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     self.processOrdered(msg)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 363, in wrapper
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return f(self, *args, **kwargs)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2771, in processOrdered
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     ordered.txnRootHash)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 363, in wrapper
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     return f(self, *args, **kwargs)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 3351, in executeBatch
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     state_root, txn_root)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 705, in default_executer
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     pp_time, reqs_keys, state_root, txn_root)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 3441, in commitAndSendReplies
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     committedTxns = reqHandler.commit(len(reqs_keys), stateRoot, txnRoot, ppTime)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/config_req_handler.py"", line 143, in commit
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     committedTxns = super().commit(txnCount, stateRoot, txnRoot, ppTime)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/ledger_req_handler.py"", line 67, in commit
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     txnRoot, ppTime, ts_store=self.ts_store)
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/ledger_req_handler.py"", line 106, in _commit
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]:     .format(ledger.root_hash))
Mar 27 21:59:30 seoulQALive8.qatest.evernym.com env[16600]: common.exceptions.PlenumValueError: variable 'txnRoot', value EeEsMRnQK6JKTvAcL16RJ4Y6KEmnwvVStsANsyLviW14, expected: equal to current ledger root hash HsbcJCXyZ3vH6mAM9zUCqe4pBoQ6tmi837DpUPojBSdc
{code};;;","12/Apr/19 8:12 PM;sergey.khoroshavin;*Problem reason*
Commits received out of order are stashed and periodically checked whether they can be processed. This check didn't take into account ledger state, and could actually process commits and apply batches in the middle of catch-up.

*Changes made*
Necessary checks are added to process_stashed_out_of_order_commits

*PRs*
https://github.com/hyperledger/indy-plenum/pull/1149
https://github.com/hyperledger/indy-plenum/pull/1156

*Covered with tests*
test_catchup_with_skipped_commits
test_catchup_with_skipped_commits_received_before_catchup_audit
test_catchup_with_skipped_commits_received_before_catchup_pool
;;;",,,,,,,,,,,,,,,,,,,,
Use twine to publish packages t PyPI,INDY-2036,38942,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Apr/19 9:07 PM,02/Apr/19 9:07 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Currently we use ""upload"" option of setuptools but it is [deprecated|https://setuptools.readthedocs.io/en/latest/setuptools.html#upload-upload-source-and-or-egg-distributions-to-pypi]  in favor of using [twine|https://pypi.org/project/twine/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00iuf:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CD pipeline should build only missed debian packages,INDY-2037,38943,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Apr/19 9:13 PM,02/Apr/19 9:26 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"In scope of INDY-1992 (INDY-2019) CD pipeline was significantly optimized to reduce unnecessary builds:
 * module package (indy-plenum or indy-node) is built only if missed in debian repo
 * 3rd parties packages are built only *if any of them is missed*

The latter should be improved to built only really missed ones.",,,,,,,,,,,,,,,,,,,,,INDY-1992,INDY-2019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00iun:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get rid of duplicated list of 3rd parties debian packages,INDY-2038,38944,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Apr/19 9:17 PM,24/Apr/19 10:41 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Curretnly for both indy-plenum and indy-node we have two copies of list of 3rd parties that are built.

e.g. indy-plenum

[https://github.com/hyperledger/indy-plenum/blob/35bc655d415c49844847436e0fe3f2ffc3342a98/build-scripts/ubuntu-1604/build-3rd-parties.sh#L68]

[https://github.com/hyperledger/indy-plenum/blob/35bc655d415c49844847436e0fe3f2ffc3342a98/Jenkinsfile.cd#L246]

Similar in indy-node. Need to make a single source of truth.",,,,,,,,,,,,,,,,,,,,,INDY-1992,INDY-2019,,INDY-2076,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00iuv:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Imporve security settings for indy-plenum and indy-node,INDY-2039,38946,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Apr/19 9:25 PM,02/Apr/19 9:25 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"In order to make dev and stable releases more accurate and strict need to improve access rules for the repos:
 # prepare list of people (maintainers) who should have write access to indy-plenum and indy-node
 # (ask Hyperledger to) create ""Indy Node Maintainers"" group for them
 # (ask Hyperledger to) update protection rules for master and stable branches to *Restrict who can push to matching branches* only for:
 ** Indy Node Maintainers group
 ** Sovbot user
 # (ask Hyperledger to) ensure that *Include administrators* is checked for both repos for both master and stable branches",,,,,,,,,,,,,,,,,,,,,INDY-1992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ivb:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automate CD pipeline continuation once release PR is ready to merge,INDY-2040,38947,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Apr/19 9:34 PM,02/Apr/19 9:34 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Currently CD pipeline waits for approval after release PR has been created and expects once approval is received the release PR passed all restrictions and the PR is ready to be merged.

Restrictions for now are:
 * at least one approval review
 * DCO is passed
 * CI is passed (_ci/sovrin-foundation-jenkins/pr-merge_ status)

It can be automated: CD continues automatically once all PR checks are passed.

 ",,,,,,,,,,,,,,,,,,,,,INDY-1992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ivj:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Auto trigger CD pipeline for release-* branches,INDY-2041,38948,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Apr/19 9:42 PM,02/Apr/19 9:42 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Currently we suppress automatic triggering for release-* branches since during the CD pipeline for them new push ""service"" commits happens that will trigger new CD pipelines builds:
 * CD pushes release commits where release version 'X.Y.Z' is set
 * CD may rollback and force push the release commit is something went wrong

Need to implement a better logic to allow auto triggering but deal with possible ""service"" commits. Options:
 # find Jenkins plugin and configure Jenkins projects to skip commits by some conditionals,  e.g. build only:
 ** not-tagged and not from bot user
 ** OR  not-tagged merge commits
 # allow triggering for all commits but:
 ** turn off GitHub notification about build statuses (they actually are not needed)
 ** abort builds for commits by conditional mentioned above",,,,,,,,,,,,,,,,,,,,,INDY-1992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ivr:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indy-Node doesn't reply on the second MINT transaction sending,INDY-2042,39095,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Artemkaaas,Artemkaaas,05/Apr/19 9:38 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,"Indy-Node doesn't reply on the second MINT transaction sending in case of REJECTing the first time
Steps:
1) created MINT transaction
2) signed by a first TRUSTEE
3) sent transaction -> got REJECT  with `UnauthorizedClientRequest(\'Request needs at least 3 signers but only 1 found\',)` as expected
4) signed by a second TRUSTEE
5) sent transaction again

expected: 
     REJECT with `UnauthorizedClientRequest(\'Request needs at least 3 signers but only 1 found\',)`
actual: Node responded with REQACK only.

Here is a test which reproduces the issue https://github.com/Artemkaaas/libsovtoken/commit/c78bba12898807e680d2fe6b46f01aa50e8a8157",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywbki",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Artemkaaas,ashcherbakov,,,,,,,,,,"17/May/19 4:38 PM;ashcherbakov;An easy workaround here is to build a new MINT request and sign it by a proper number of Trustees.;;;",,,,,,,,,,,,,,,,,,,,,
Modify dynamic validation to satisfy multisignature rules,INDY-2043,39097,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,Derashe,Derashe,05/Apr/19 10:47 PM,18/Apr/19 4:44 PM,28/Oct/23 2:47 AM,18/Apr/19 4:43 PM,,,1.7.1,,,,,0,,,,,Modify signature validation to satisfy multisignature rules,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9604",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,Toktar,,,,,,,,,"09/Apr/19 8:50 PM;Toktar;*PoA:*

In this task, it is necessary to expand the dynamic validation for signatures and solve the following problem: when the client without a role sends NODE request, but sets a steward DID, the request must be rejected.


*TODO:*
 * Implement RolesAuthorizer.get_sig_count (). Add getting the number of signatures in the request for a specific role.
 * Remove the logic in is_sig_count_accepted () for sig_count = 1
 * Add a check in static validation of the request that only one field of signatures is filled (SIG or SIGS).
 * Add validation of each field with signatures in the CoreAuthMixin.authenticate () method
 * Add tests:
+Case1:+
 ** Send AUTH_RULE txn to change constraint for the key add_new_trustee to (TRUSTEE, 2) or ((TRUSTEE, 1) and (STEWARD, 2))
 ** Send and check requests to adding a new trustee with
 *** with 2 TRUSTEE signatures
 *** with 3 TRUSTEE signatures
 *** with 1 TRUSTEE signature and 2 STEWARD signatures
 *** with 1 TRUSTEE signature and 3 STEWARD signatures 
 *** with 2 TRUSTEE signatures and 1 STEWARD signature
 ** Send and check a reject with an authentication error for requests to adding a new trustee
 *** without signatures
 *** with 3 client signatures
 *** with 3 STEWARD signatures
 *** with 1 TRUSTEE signature
 *** with 1 TRUSTEE signature and 1 STEWARD signature
 * +Case2:+
 ** Signed and send a NODE request by a client, but put steward DID in an identifier field in the request 
 ** Check that the request was rejected  with an authentication error ;;;","13/Apr/19 2:16 AM;Toktar;indy-plenum PR: https://github.com/hyperledger/indy-plenum/pull/1164
indy-node PR: https://github.com/hyperledger/indy-node/pull/1240;;;","18/Apr/19 4:43 PM;Toktar;*Problem reason:*
 - Auth validation is not implemented.

*Changes:*
 - Added validation
 - Change an auth key format  

*PR:*
 *  [https://github.com/hyperledger/indy-plenum/pull/1164]
 *  [https://github.com/hyperledger/indy-node/pull/1240]
 * [https://github.com/hyperledger/indy-node/pull/1243]
 * [https://github.com/hyperledger/indy-plenum/pull/1171]

*Version:*
 * indy-node 1.6.895 -master
 * (indy-plenum 1.6.765 -master)

*Risk factors:*
 - Problems with a multi-signature validation and with an auth rule checking.

*Risk:*
 - Low

*Test:*
 * [test_auth_multi_sig_for_5_owners.py|https://github.com/hyperledger/indy-node/pull/1240/files#diff-bb119f1832e02141eaca518580218f1f]
 * [test_auth_rule_transaction.py|https://github.com/hyperledger/indy-node/pull/1240/files#diff-35bb9fb57f669358467d5252239e8060]
 * [test_multisig_auth_rule.py|https://github.com/hyperledger/indy-node/pull/1240/files#diff-beff30382b0bf93dc39e8770f45a5cc6]
 * [test_txn_with_different_signature_and_idr.py|https://github.com/hyperledger/indy-node/pull/1240/files#diff-144c55f408ec9383638eab28e153a7aa]

*Recommendations for QA:*
 * Test cases from the ticket INDY-2046;;;","18/Apr/19 4:44 PM;ashcherbakov;Validation will be done in the scope of INDY-2046;;;",,,,,,,,,,,,,,,,,,
A node needs to start a view change for the maximum viewNo it has a quorum of InstanceChanges for,INDY-2044,39107,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,06/Apr/19 12:19 AM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,Test: test_catchup_to_next_view_during_view_change_0_to_2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c92w",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,KitHat,,,,,,,,,,"13/Sep/19 5:18 PM;KitHat;Seems like this test is also failing intermittently because of this issue
test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection;;;",,,,,,,,,,,,,,,,,,,,,
"Sanity check on requests, which payload_digests already written",INDY-2045,39161,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,Derashe,Derashe,09/Apr/19 8:01 PM,23/Oct/19 11:37 PM,28/Oct/23 2:47 AM,23/Oct/19 11:37 PM,,,1.12.1,,,,,0,,,,,"As far as we consider prePrepares with repeated payload_digests as malicious, we may have situation, when some double signed requests get stuck in requests queue and sender would not ever get reply.

We need to do some kind of sanity check to clear such a requests (after ordering or using schedule)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw9y",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,"23/Oct/19 11:37 PM;ashcherbakov;We already have a sanity check to clear old reqs.;;;",,,,,,,,,,,,,,,,,,,,,
Debug and validation: Multi-signature support,INDY-2046,39167,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,09/Apr/19 10:45 PM,17/May/19 4:54 PM,28/Oct/23 2:47 AM,23/Apr/19 12:22 AM,,,1.7.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2055,,,,,,,,,,"12/Apr/19 9:55 PM;VladimirWork;INDY-2046-12-04-2019-stalled-node.tar.gz;https://jira.hyperledger.org/secure/attachment/17094/INDY-2046-12-04-2019-stalled-node.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2093,,,No,,Unset,No,,,"1|hzwvif:00001yw9606",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,VladimirWork,,,,,,,,,"12/Apr/19 9:55 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev891

Steps to Reproduce:
1. Start test pool setting ACCEPTABLE_DEVIATION_PREPREPARE_SECS config parameter to lower value foNode3 (60) and ForceViewChangeFreq = 1800.
2. Stop that node while other nodes are ordering (10 txns/sec).
3. Start the node after some time expecting that the node will perform catchup more than the time set for the mentioned config parameter
send set of txns.
4. Repeat steps 2 and 3 one more time.

Actual Results:
*Node3 has ViewNo 0* but all other nodes have 4 at the end of the test. Node3 *doesn't catch up* txns after Step 4 (but after Step 3 *it does*). There are suspicions codes in logs:
{noformat}
2019-04-12 10:25:39,257|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20
2019-04-12 10:25:40,663|WARNING|node.py|Node3 raised suspicion on node Node4 for Pre-Prepare message has already ordered requests; suspicion code is 42
2019-04-12 10:25:40,663|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 42
2019-04-12 10:25:40,664|WARNING|node.py|Node3 raised suspicion on node Node4 for Pre-Prepare message has already ordered requests; suspicion code is 42
2019-04-12 10:25:40,665|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 42
2019-04-12 10:26:23,384|WARNING|node.py|Node3 raised suspicion on node Node2 for Pre-Prepare message has incorrect reject; suspicion code is 20
2019-04-12 10:26:23,384|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20
2019-04-12 10:26:24,617|WARNING|node.py|Node3 raised suspicion on node Node2 for Pre-Prepare message has incorrect reject; suspicion code is 20
{noformat}
 [^INDY-2046-12-04-2019-stalled-node.tar.gz] ;;;","16/Apr/19 3:37 AM;Derashe;*Validation plan:*

Mention: for all tests 4 nodes pool would be enough
 * Tests on checking pool consistency
 ** Now, request mainly identified by full digest (with signatures and plugins included). This was done to avoid situation, when nodes can consider two txns with same body, but different signatures/plugins, as one. This can be checked by test below.
 *** run pool with default config
 *** create txn (mint, node, nym) node
 **** case #1: multisign it with different , but authorized, nyms
 **** case #2: set different plugin's fields (FEES)
 *** send one, wait for it to be written
 *** send second, ensure that it will be recknacked
 ** Send these txns in a one batch
 *** run pool with modified config (Max3PCBatchSize=2, Max3PCBatchWait=9999, UPDATE_STATE_FRESHNESS=False)
 *** remember last_ordered batch on nodes
 *** remember sizes of a ledgers
 *** create same txns that in prev case, send them one by one (do not pay attention to reply, cause sdk cannot track two requests with same reqId)
 *** wait a bit
 *** ensure that last_ordered == old_last_ordered + 1
 *** ensure that ledger_size == old_ledger_size + 1
 ** Send same requests to check logic did not change
 *** run pool with default config
 *** create requests from prev case and sign it once
 *** send it once, get a reply 
 *** send it second time, get a reply
 * Multisignature functionality
 ** #1
 *** create request, sign it with both signature and multisignature
 *** send it, ensure it reqnacked
 ** #2 
 *** create request, send it without signing
 *** ensure it reqnacked
 * Multisignature rules correct application (test these with most important requests - mint, node, nym. Use only multisignature to sign the request)
 ** #1
 *** set request constraint to None
 *** ensure that any role can send such a request
 ** #2
 *** set request constraint to some role (Steward f.e.)
 *** ensure that only Steward can send such a request
 ** #3
 *** set request constraint to some role or some role (Steward or Trustee f.e.)
 *** ensure that only mentioned roles can send such a request
 *** ensure that request with both of this signatures can be successfully written
 ** #4
 *** set request constraint to some role and some role (Steward and Trustee f.e.)
 *** ensure that no single role can send such a request
 *** ensure that only request, signed with mentioned roles, can be written
 ** #5
 *** repeat step #4 with constraint like ( 2x Steward and/or Trustee)
 ** #6
 *** repeat step #4 with constraint like ( 2x Steward and/or 2x Trustee)
 ** #7
 *** repeat step #4 with constraint like ( 5x Steward and/or 2x Trustee)
 ** #8
 *** repeat step #4 with constraint like ( Steward and Trustee) or (Steward and Trust Anchor);;;","22/Apr/19 7:56 PM;VladimirWork;Cases with rules are blocked by IS-1237.;;;","23/Apr/19 12:21 AM;VladimirWork;Build Info:
indy-node 1.7.0.dev900
libindy 1.8.2~1065

Case with single batch has been checked manually and other cases are implemented in https://github.com/VladimirWork/indy-test-automation/blob/master/system/draft/TestMultiSigSuite.py;;;",,,,,,,,,,,,,,,,,,
Nodes can fail on first start after upgrading from version without audit ledger to version with audit ledger,INDY-2047,39180,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,10/Apr/19 1:45 AM,18/Apr/19 7:20 PM,28/Oct/23 2:47 AM,18/Apr/19 7:20 PM,,,1.7.1,,,,,0,TShirt_M,,,,"*Problem reason*
On first start after upgrade catch-up is done using old protocol (i.e. catching up each ledger independently), and it can happen so that other nodes manage to order some transaction between catching up audit and other ledgers. This can lead to failed attempts to apply PREPREPAREs and halted ordering on some nodes (however ledger won't be corrupted and they could be restarted to try again).

*Proposed solution*
On startup do several rounds of catchup until either audit transaction is caught up or some timeout passed (TBD, 2-3 minutes?)",,,,,,,,,,INDY-2055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:000006r20e",,,,Unset,Unset,Ev-Node 19.07,Ev-Node 19.08,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,"12/Apr/19 4:34 PM;sergey.khoroshavin;*Problem reason*
As in description

*Changes made*
If there were no transactions in audit ledger, but some transactions were caught up in other ledger (using legacy method) continue catchup until either:
* some transactions appear in audit ledger
* no more transactions are caught up

*Versions*
indy-node:  1.7.0.dev891 

*PR*
https://github.com/hyperledger/indy-plenum/pull/1159

*Covered by tests*
https://github.com/skhoroshavin/indy-plenum/blob/64b8c189a72677bfcc8bd2ab4c8e02ad87236006/plenum/test/audit_ledger/test_first_audit_catchup_during_ordering.py

*Risk*
Low

*Risk factors*
Some tricky failure cases may still get through

*Recommendations for QA*
Try the following test case on 25 nodes AWS pool:
* install node version that doesn't have audit ledger
* start production load test and run in for 3 minutes
* stop some non-primary nodes (for example Node10, Node15, Node20)
* after 1 minute send a forced upgrade transaction to pool and start stopped nodes
* check that pool successfully performs upgrade and continues ordering;;;","15/Apr/19 11:57 PM;sergey.khoroshavin;During tests some of nodes crashed with following traceback (including some of those NOT in lagged list):
{code}
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py"", line 104, in process_catchup_rep
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     num_processed = self._process_catchup_txns(txns_already_rcvd_in_catchup)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py"", line 338, in _process_catchup_txns
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self._add_txn(txn)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py"", line 418, in _add_txn
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self._provider.notify_transaction_added_to_ledger(self._ledger_id, txn)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/node_catchup_data.py"", line 58, in notify_transaction_added_to_ledger
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     info.postTxnAddedToLedgerClbk(ledger_id, txn)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2188, in postTxnFromCatchupAddedToLedger
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self.updateSeqNoMap([txn], ledger_id)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 3350, in updateSeqNoMap
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     for txn in committedTxns)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/req_id_to_txn.py"", line 30, in addBatch
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self._keyValueStorage.setBatch(payload)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 126, in setBatch
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     b.put(key, value)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""rocksdb/_rocksdb.pyx"", line 1347, in rocksdb._rocksdb.WriteBatch.put
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""rocksdb/_rocksdb.pyx"", line 103, in rocksdb._rocksdb.bytes_to_slice
{code};;;","16/Apr/19 12:23 AM;VladimirWork;Build Info:
indy-node 1.7.0~dev892

Steps to Reproduce:
1. Install node version that doesn't have audit ledger.
2. Start production load test and run in for 3 minutes.
3. Stop some non-primary nodes (for example Node10, Node15, Node20).
4. After 1 minute send a forced upgrade transaction to pool and start stopped nodes.

Actual Results:
15th and 20th nodes don't catch up and order. There is a stacktrace above at 13th node.

Logs and metrics:
ev@evernymr33:logs/INDY-2047_15_04_2019_logs.tar.gz
ev@evernymr33:logs/INDY-2047_15_04_2019_metrics.tar.gz;;;","16/Apr/19 12:48 AM;sergey.khoroshavin;Some nodes failed to reach consensus after upgrade due to unrelated bug, created a separate issue INDY-2055;;;","18/Apr/19 4:32 PM;ashcherbakov;Fixed in Build: indy-node 1.7.0.dev895 ;;;","18/Apr/19 7:20 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev895

Steps to Reproduce:
1. Install node version that doesn't have audit ledger.
2. Start production load test and run in for 3 minutes.
3. Stop some non-primary nodes (for example Node10, Node15, Node20).
4. After 1 minute send a forced upgrade transaction to pool and start stopped nodes.

Actual Results:
All stopped nodes don't catch up and order under load but catch up and order after load stopping.;;;",,,,,,,,,,,,,,,,
Do not send 3PC batches for all the ledgers at the same time,INDY-2048,39189,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,10/Apr/19 6:02 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,"When we write to multiple ledgers all the time, we create a 3PC batch for all the ledgers at the same time, which increases the probability of out-of-order messages, stashes, lagging nodes, and different time of view change start on nodes.

*Acceptance criteria*
 * Do not send 3PC batches for all the ledgers at the same time
 * Send every next 3PC batch in 1 sec, as for the case of writing to 1 ledger.
 * Freshness updates should also follow this logic and be sent for every ledger not earlier than in 1 sec.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c92r",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a user, I need to know the action_id in the auth_map for a particualr request, so that I don't need to parse auth_map and can understand authorization rules easily",INDY-2049,39196,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,10/Apr/19 9:50 PM,24/Oct/19 5:07 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"This is needed in order to get the fees for a pre-built transaction. See [ST-561|https://sovrin.atlassian.net/browse/ST-561]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx4f:2rzmh9",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Some nodes are stalled and throw an error under load,INDY-2050,39198,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,VladimirWork,VladimirWork,10/Apr/19 10:48 PM,21/Apr/19 10:33 PM,28/Oct/23 2:47 AM,21/Apr/19 10:33 PM,,,1.7.1,,,,,0,TShirt_M,,,,"Build Info:
indy-node 1.7.0~dev888

Steps to Reproduce:
1. Run domain + config load test (load rate 10) with forced VCs every 1800 seconds.

Actual Results:
We have 1 and 22 nodes that stop ordering and catching up and have view different from other nodes. Also there are some stacktraces in journalctl:
{noformat}
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1347, in serviceReplicas
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     return self._process_replica_messages(limit)
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1350, in _process_replica_messages
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     inbox_processed = self.replicas.service_inboxes(limit)
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 100, in service_inboxes
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     sum(replica.serviceQueues(limit) for replica in self._replicas.values())
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 100, in <genexpr>
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     sum(replica.serviceQueues(limit) for replica in self._replicas.values())
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 216, in wrapper
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     return f(self, *args, **kwargs)
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/lib/python3.5/contextlib.py"", line 77, in __exit__
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     self.gen.throw(type, value, traceback)
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 352, in measure_time
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     yield
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 216, in wrapper
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     return f(self, *args, **kwargs)
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1028, in serviceQueues
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     r = self.dequeue_pre_prepares()
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2377, in dequeue_pre_prepares
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     *self.prePreparesPendingPrevPP.iloc[0]):
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1395, in __is_next_pre_prepare
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]:     .format(self, view_no, self.viewNo)
Apr 09 23:32:08 sydneyQALive7.qatest.evernym.com env[8784]: common.exceptions.LogicError: Node7:5 'view_no' 21 is not equal to current view_no 22
{noformat}


Expected Results:
There should be no stalled nodes at this load rate. Pool should sustain this load rate more time than just ~10 hours.

Logs and metrics:
ev@evernymr33:logs/10_04_2019_view_changes_two_ledgers_logs.tar.gz
ev@evernymr33:logs/10_04_2019_view_changes_two_ledgers_metrics.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9602i",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"17/Apr/19 5:54 PM;Toktar;PR: [https://github.com/hyperledger/indy-plenum/pull/1167];;;","17/Apr/19 5:59 PM;Toktar;*Problem 1:* In the method __is_next_pre_prepare we check a Pre-Prepare message without initial validation. We removed the logic error from this method. This is valid because the message verification is done further.
*Problem 2:* 1,22 nodes stopped ordering. ;;;","21/Apr/19 10:32 PM;ashcherbakov;Problem reason/description: 
- If there were some missing PrePrepares (so that a node received, for example, PrePrepares with ppSeqNo=2 and ppSeqNo=4, but didn't receive ppSeqNo=3), then during view change a Node may try process the lag, and raise a LogicError in this case, which looks incorrect, since this situation is expected.

Changes: 
- do not raise logic error anymore

PR:
- fix: [https://github.com/hyperledger/indy-plenum/pull/1167]
- test: https://github.com/hyperledger/indy-plenum/pull/1175

Version:
- Plenum 1.7.0.dev761

Risk factors:
- View change

Risk:
- Low

Covered with tests:
- `test_missing_pp_before_starting_vc`

Recommendations for QA
- it's hard to reproduce it in Load, so integration test should be enough
- make sure that there are no such errors in next load tests;;;",,,,,,,,,,,,,,,,,,,
Debug and Validation: Restore current 3PC state from audit ledger - Phase 2,INDY-2051,39219,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,11/Apr/19 6:56 PM,20/Apr/19 4:35 AM,28/Oct/23 2:47 AM,20/Apr/19 4:35 AM,,,1.7.1,,,,,0,,,,,"Continue debug and validation started in the scope of INDY-2025
 * Load tests:
 ** acceptance load (1943) with forced view change (/)
 ** acceptance load with fees (1943) (!)
 ** Restart n-f-1 non-primary nodes (!) INDY-2060
 *** Start acceptance load as in 1983
 *** Wait for 1 min
 *** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 *** Restart n-f-1 nodes at the same time (Nodes 10-25)
 *** Make sure that every node participates in consensus
 *** Stop the load
 *** Make sure that all nodes have equal data
 ** Restart n-f-1 primary nodes (/)
 *** Start acceptance load as in 1983
 *** Wait for 1 min
 *** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 *** Restart n-f-1 nodes at the same time (Nodes 1-16)
 *** Make sure that every node participates in consensus
 *** Stop the load
 *** Make sure that all nodes have equal data
 ** Restart more than n-f nodes with disabled watchdog (/)
 *** Set ENABLE_INCONSISTENCY_WATCHER_NETWORK to False
 *** Start acceptance load as in 1983
 *** Wait for 1 min
 *** Provoke view change (stop 1st Node; make sure view has changed; start it back)
 *** Restart more than n-f nodes at the same time (Nodes 1-20)
 *** Make sure that every node participates in consensus
 *** Stop the load
 *** Make sure that all nodes have equal data",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:00001yw9603",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"15/Apr/19 10:15 PM;VladimirWork;Production load without fees with forced VCs logs and metrics:
12_04_2019_view_changes_prod_load_no_fees_logs.tar.gz
12_04_2019_view_changes_prod_load_no_fees_metrics.tar.gz;;;","16/Apr/19 4:51 PM;VladimirWork;Production load with fees logs and metrics (we have at least 3 nodes stalled in this case, a lot of VCs and more OOM failures than in case without fees):
ev@evernymr33:logs/INDY-2051_16_04_2019_logs_full.tar.gz
ev@evernymr33:logs/INDY-2051_16_04_2019_metrics_full.tar.gz;;;","17/Apr/19 9:52 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev893

Steps to Reproduce:
1. Start acceptance load as in 1983 (10 txns/sec without fees).
2. Wait for 1 min.
3. Provoke view change (stop 1st Node; make sure view has changed; start it back).
4. Restart n-f-1 nodes at the same time (Nodes 10-25).
5. Stop the load.
6. Check te pool state.
7. Run low rate production load (1 txn/sec without fees) to check stalled nodes' ordering and catching up.

Actual Results:
10, 11, 12 nodes are stalled by all ledgers until Step 7 was performed - they have the same ViewNo (2 at the end of test case) as others but *don't catch up and order at the initial load run but they start catching up and ordering at Step 7*.

Logs and metrics:
ev@evernymr33:logs/INDY-2051_17_04_2019_logs.tar.gz
ev@evernymr33:logs/INDY-2051_17_04_2019_metrics.tar.gz;;;","18/Apr/19 2:55 AM;Toktar;10, 11, 12 nodes has a problem from the task https://jira.hyperledger.org/browse/INDY-2052. After view change nodes have a suspicion code 42 (PP has already ordered) and messages about missing Pre-Prepares. ;;;","19/Apr/19 12:48 AM;ashcherbakov;The last issue is actually related to watermarks, and will be addressed in INDY-2060;;;",,,,,,,,,,,,,,,,,
Revise requests cleanup logic,INDY-2052,39223,,Task,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,11/Apr/19 9:03 PM,25/Oct/19 8:59 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Base statement: requests can be totally cleaned (no reference on node level) if request is not needed anymore
 #  if request is not needed anymore
 ** ordered by all replicas
 ** executed (replied)
 ** no propagate requests are expected (limited to checkpoint borders)
 # OR request is outdated
 ** older than thresholds (propagation and ordering)

Currently there are a set of places in code related to requests cleanup logic:
 # plenum/server/node.py:*_clear_request_for_txn*
 ** on any txn is written to ledger during catchup
 ** for master replica only
 # plenum/server/node.py:*executeBatch*
 ** on any txn is ordered and requests are replied
 # plenum/server/node.py:*check_outdated_reqs*
 ** (periodical) to filter and remove outdated requests
 # plenum/server/replica.py:*clear_requests_and_fix_last_ordered*
 ** on view change completes
 ** for backup replicas only
 # plenum/server/replica.py:*_gc*
 ** on new view starts

Additional related logic (the following methods clear related containers):
 # plenum/server/replica.py_:*catchup_clear_for_backup*
 ** (backups) on master's catchup complete
 # plenum/server/replica.py​_:*_remove_till_caught_up_3pc​*
 ** (backups) on quorum of checkpoints (catchup-like complete)
 ** (master) on catchup complete

Cases which seemed buggy or unclear:
 # any catchup:
 ** _clear_request_for_txn decrements counter for replicas that reference the request but it is called only once per txn and only in scope of master catchup
 ** once any catchup (master or backup) backups clear setPrePrepares and prePrepares containers that would prevent requests cleaning (replicas ""unsubscribe"" from requests referenced in theese containers) in _gc
 ** -> if catch up happens before gc:
 *** (both master and backup) invalid requests won't be cleaned since they can't be caught up
 *** (master and backup in case of cathup caused by missed checkpoints) ordered requests won't cleaned since they are not caught up
 # PrePrepares that are stashed by some reason in processPrePrepare:
 ** stashed in: pre_prepares_stashed_for_incorrect_time, prePreparesPendingFinReqs, prePreparesPendingPrevPP
 # PrePrepares that are reported as suspicious and are not stored in any replica's container (processPrePrepare)

 ",,,,,,,,,,,,,,,,,,,,,INDY-1983,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125s:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch-up should take into account state of other nodes when sending requests,INDY-2053,39224,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,11/Apr/19 9:23 PM,08/May/19 8:31 PM,28/Oct/23 2:47 AM,08/May/19 8:31 PM,,,1.8.0,,,,,0,,,,,"*Problem description*
Current catch-up code doesn't take into account ledger states of other nodes when sending catch-up requests. This can lead to some nodes receiving requests for transactions that they don't have yet. In this case they just reject request, which leads to much longer catch-up since leecher node waits for reply until timeout and then resends request to different nodes. 

*Proposed solution*
Use information from LEDGER_STATUS and CONSISTENCY_PROOF messages gathered by ConsProofService as well as from audit ledger to make sure catch-up requests are sent to nodes that can really fulfill them.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/19 6:02 PM;sergey.khoroshavin;INDY-2053.pdf;https://jira.hyperledger.org/secure/attachment/17181/INDY-2053.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwrhj:910sx2ur",,,,Unset,Unset,Ev-Node 19.08,Ev-Node 19.09,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,"29/Apr/19 6:22 PM;sergey.khoroshavin;*Problem*
As described in issue

*Changes made*
Node doing catch up takes into account information received from consistency proofs about ledger states of other nodes and sends only catchup requests that can be fulfilled. Some additional details can be found here:  [^INDY-2053.pdf] 

*Version*
indy-node 1.7.0.dev914

*PR*
https://github.com/hyperledger/indy-plenum/pull/1173

*Covered by tests*
test_catchup_req_distribution_invariants
test_catchup_uses_only_nodes_with_cons_proofs
test_catchup_from_unequal_nodes_without_reasking
test_catchup_with_only_one_available_node
test_catchup_with_all_nodes_sending_cons_proofs_dead
test_receive_incorrect_catchup_request_for_seq_no_zero

*Risk*
Low

*Risk factors*
Some yet unknown edge cases of new catchup logic may arise

*Recommendations for QA*
On 25-nodes AWS pool run following tests:
* Short catchup of several nodes under load
** Start production load (10 txns/seconds)
** Wait for 5 minutes
** Stop nodes 15-20 for 1 minute
** Make sure they all manage to catch up
** And there are no ERRORs in logs of other nodes indicating that incorrect catch-up request received
* Long catchup of several nodes under load
** Start production load (10 txns/seconds)
** Wait for 5 minutes
** Stop nodes 15-20 for 15 minutes
** Make sure they all manage to catch up
** And there are no ERRORs in logs of other nodes indicating that incorrect catch-up request received
* Forced view change
** Start production load with forced view change period set to 1800 seconds
** Check if pool survives this for 24 hours;;;","06/May/19 7:43 PM;VladimirWork;Build Info:
1.8.0~dev918

Steps to Reproduce:
1. Start production load (10 txns/seconds).
2. Wait for 5 minutes.
3. Stop nodes 15-20 for 1 minute.
4. Make sure they all manage to catch up.

Actual Results:
Nodes 15-20 can't reach other ones under load. There is an error on this nodes:
{noformat}
2019-05-06 09:54:46,001|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 675 is more than ledger size 671
2019-05-06 09:54:53,361|ERROR|seeder_service.py|Node15 cannot build consistency proof: start 672 is more than ledger size 671
2019-05-06 09:56:05,334|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 912 is more than ledger size 791
2019-05-06 09:56:07,300|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 915 is more than ledger size 791
2019-05-06 10:08:49,132|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 1722 is more than ledger size 1702
2019-05-06 10:20:07,318|ERROR|seeder_service.py|Node15 cannot build consistency proof: end 2816 is more than ledger size 2781
{noformat}

Expected Results:
Nodes should catch up under load and there are should be no errors.

Logs:
ev@evernymr33:logs/INDY_2053_06_05_2019_logs.tar.gz;;;","07/May/19 8:21 PM;sergey.khoroshavin;Analysis of failed case (as well as some tests performed on earlier versions of indy-node) showed that:
* it is not a regression
* ERRORs arise from different code path and actually not an issue
* new implementation still has a flaw, however it is easily fixed, although at the cost of lesser efficiency ([PR|https://github.com/hyperledger/indy-plenum/pull/1194] is already opened)
* there is yet another issue with catch-up slowness related to gathering CONSISTENCY_PROOFs, which is outside of scope of this ticket, so another one will be created and linked to;;;","08/May/19 4:43 PM;ashcherbakov;Fixed in 1.8.0.dev923;;;","08/May/19 5:38 PM;sergey.khoroshavin;*Recommendations for QA*
Run short catch-up case:
* Start production load (10 txns/seconds)
* Wait for 5 minutes
* Stop nodes 15-20 for 1 minute
* Start them and see how they catch up

Unfortunately due to how consistency proofs are gathered (which is a problem outside of scope of this issue) catch up is still likely to be slow, however it should perform a little better than before, and there shouldn't be any messages in logs stating that nodes are discarding catch-up requests. In other words, output of the following command should be empty on all nodes:
{code}
xzgrep discarding /var/log/indy/sandbox/* | grep CATCHUP_REQ
{code}
;;;","08/May/19 8:31 PM;VladimirWork;Build Info:
1.8.0~dev923

Steps to Reproduce:
1. Start production load (10 txns/seconds).
2. Wait for 5 minutes.
3. Stop nodes 15-20 for 1 minute.
4. Make sure they all manage to catch up.

Actual Results:
2 nodes from 6 have completed catch up successfully. The rest ones have not even after 1+ hour waitng. There are no messages about discarded catch-up requests on the nodes.

Additional Info:
Issues with catch up are still present and the will be fixed in scope of INDY-<>.
;;;",,,,,,,,,,,,,,,,
Unable to upgrade master version of indy-node,INDY-2054,39323,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Won't Do,andkononykhin,VladimirWork,VladimirWork,15/Apr/19 11:16 PM,18/Apr/19 11:55 PM,28/Oct/23 2:47 AM,18/Apr/19 11:55 PM,,,,,,,,0,,,,,"We cannot write pool upgrade txns with versions like `1.7.0~dev892` or `1.7.0.dev892` due to validation restrictions:
{noformat}
validation error [ClientPoolUpgradeOperation]: version component should contain only digits (version=1.7.0~dev892)

validation error [ClientPoolUpgradeOperation]: version consists of 4 components, but it should contain (2, 3) (version=1.7.0.dev892)
{noformat}

It looks like we will get the same error for rc branch if we will use `X.X.X~rcXX` version pattern so we must keep old version pattern at least for rc and stable branches.

FYI [~ashcherbakov] [~andkononykhin]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00l2f:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,"18/Apr/19 11:37 PM;andkononykhin;Stable versions are compatible since they both have X.Y.Z version form.

The issue is actual for cases when we try to upgrade from versions that used old verioning to non-stable onces with new versioning (e.g. to dev or rc builds).

Here we mostly worry about rc to allow QA validation of coming release 1.7.0 and we may consider the following workaround:
 * during release pipeline we would have *1.7.0~rcN* candidate published to debian repo to *rc* component
 * the CD pipeline is paused here and waits QA approval
 * we may manually repack and publish *1.7.0~rcN* -> *1.6.M* where M is enough to ""win"" current max rc candidate (currently is 1.6.83, thus M>=84). ""Repack"" means that it would include the same content but different debian package metadata in version field.
 * QA will use that version for validation process;;;",,,,,,,,,,,,,,,,,,,,,
Some nodes failed to join consensus after upgrade,INDY-2055,39330,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,16/Apr/19 12:49 AM,18/Apr/19 9:05 PM,28/Oct/23 2:47 AM,18/Apr/19 7:19 PM,,,1.7.1,,,,,0,TShirt_S,,,,"During testing INDY-2047 it was found that during upgrade to version which has INDY-1757 implemented nodes that need to do catchup will fail with traceback:
{code}
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py"", line 104, in process_catchup_rep
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     num_processed = self._process_catchup_txns(txns_already_rcvd_in_catchup)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py"", line 338, in _process_catchup_txns
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self._add_txn(txn)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/catchup_rep_service.py"", line 418, in _add_txn
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self._provider.notify_transaction_added_to_ledger(self._ledger_id, txn)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/catchup/node_catchup_data.py"", line 58, in notify_transaction_added_to_ledger
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     info.postTxnAddedToLedgerClbk(ledger_id, txn)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2188, in postTxnFromCatchupAddedToLedger
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self.updateSeqNoMap([txn], ledger_id)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 3350, in updateSeqNoMap
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     for txn in committedTxns)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/req_id_to_txn.py"", line 30, in addBatch
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     self._keyValueStorage.setBatch(payload)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 126, in setBatch
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:     b.put(key, value)
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""rocksdb/_rocksdb.pyx"", line 1347, in rocksdb._rocksdb.WriteBatch.put
Apr 15 14:08:01 londonQALive13.qatest.evernym.com env[1049]:   File ""rocksdb/_rocksdb.pyx"", line 103, in rocksdb._rocksdb.bytes_to_slice
{code}

This is due to changes in transaction metadata format. 

*Acceptance criteria*
Bug should be fixed
",,,,,,,,,,,,,,INDY-2047,,,,,,,,,,INDY-1757,INDY-2046,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw96097",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,"18/Apr/19 4:33 PM;ashcherbakov;Fixed in Build: indy-node 1.7.0.dev895 ;;;","18/Apr/19 7:18 PM;VladimirWork;The fix has been validated in scope of INDY-2047.;;;","18/Apr/19 7:41 PM;sergey.khoroshavin;*Problem reason*
Transaction metadata format in ledger has changed

*Changes made*
Make it possible to handle old transaction metadata format from new code (so that migration is not needed)
Also improved new seqNoDb format to handle some edge cases.

*Version*
Fixed in Build: indy-node 1.7.0.dev895 

*PR*
https://github.com/hyperledger/indy-plenum/pull/1165

*Covered by tests*
test_catchup_with_old_txn_metadata_digest_format
test_get_digest_old
test_get_payload_digest_old
test_old_txn_metadata_digest_fallback
test_old_txn_metadata_multisig_digest_fallback

test_seq_no_db_signed_request
test_seq_no_db_multisigned_request
test_seq_no_db_unsigned_request
test_req_id_to_txn_add_same_full_and_payload_digests
test_req_id_to_txn_add_batch_same_full_and_payload_digests

*Risk*
Low

*Recommendations for QA*
Retest INDY-2047 (actually already done);;;",,,,,,,,,,,,,,,,,,,
Exception in node_leecher_service.py,INDY-2056,39376,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Cannot Reproduce,VladimirWork,lijamie,lijamie,18/Apr/19 7:54 AM,26/Apr/19 9:26 PM,28/Oct/23 2:47 AM,25/Apr/19 11:43 PM,,,1.7.1,plenum,,,,0,TShirt_S,,,,"This is how to re-produce.
 # Start 3 nodes, say, 1, 2, 3
 # Add #4. 
 # Start #4.

Another way to reproduce.
 # Start 3 nodes, say 1, 2, 3.
 # Stop #1 and wipe the data of #1. 
 # Restart #1.

The following is the stack trace.

File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/node.py"", line 1306, in prod
 c += await self.serviceNodeMsgs(limit)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/metrics_collector.py"", line 375, in wrapper
 return await f(self, *args, **kwargs)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/node.py"", line 1357, in serviceNodeMsgs
 await self.processNodeInBox()
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/metrics_collector.py"", line 375, in wrapper
 return await f(self, *args, **kwargs)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/node.py"", line 1951, in processNodeInBox
 await self.process_one_node_message(m)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/node.py"", line 1955, in process_one_node_message
 await self.nodeMsgRouter.handle(m)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/router.py"", line 86, in handle
 res = self.handleSync(msg)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/router.py"", line 75, in handleSync
 return self.getFunc(msg[0])(*msg)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/metrics_collector.py"", line 362, in wrapper
 return f(self, *args, **kwargs)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/ledger_manager.py"", line 130, in processCatchupRep
 self._node_leecher_inbox.put_nowait((rep, frm))
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/channel.py"", line 59, in put_nowait
 self._observable.notify_sync(msg)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/channel.py"", line 41, in notify_sync
 handler(msg)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/common/channel.py"", line 161, in _process_sync
 return handler(*msg)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/catchup/node_leecher_service.py"", line 96, in _on_ledger_catchup_complete
 self._catchup_till = self._calc_catchup_till(msg.last_3pc)
 File "".../.virtualenvs/venv/lib/python3.5/site-packages/plenum/server/catchup/node_leecher_service.py"", line 184, in _calc_catchup_till
 start_size = ledger.size
AttributeError: 'NoneType' object has no attribute 'size'

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609b",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),lijamie,sergey.khoroshavin,VladimirWork,,,,,,,,,"19/Apr/19 7:23 PM;sergey.khoroshavin;[~lijamie] 
I'm working on this right now and I think there will be a fix soon, however I wanted to clarify a bit some things:
* What node version did you use? (I suspect it was some of dev builds from master)
* Did you use any plugins (which add custom ledgers) in your setup?
* Did you try second scenario with 4 nodes?;;;","20/Apr/19 3:32 AM;lijamie;# I pulled it from the master branch about 2 weeks ago. I am not able to trace the version right now. 
 # I did not use any plugins.
 # Yes, I tried that with 4 nodes. 

I had rolled back to an older version which is about 1 months old. Unfortunately I am not able to reproduce the bug again. My temporary fix is to cast ledger_id to int before Line 183 as the following to avoid the NoneType exception.

   ledger_id = int(ledger_id)
   ledger = self._provider.ledger(ledger_id)
   start_size = ledger.size;;;","23/Apr/19 1:16 AM;VladimirWork;I cannot reproduce this against 1.7.0~dev901 master for both 3 and 4 nodes setups.;;;","23/Apr/19 1:29 AM;sergey.khoroshavin;[~lijamie]
I've analysed current source code and cannot find any case where situation you described is possible. However node_leecher_service is a very recent addition, so it is possible that you somehow managed to get really broken intermediate version. Also during analysis we've found and fixed yet another unhandled case (that's why I was asking you about plugins), so thanks a lot for your report! Please try latest master version and tell us if you still have this problem.;;;",,,,,,,,,,,,,,,,,,
Support docker-in-docker for docker client in CD,INDY-2057,39388,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,VladimirWork,VladimirWork,18/Apr/19 9:47 PM,18/Jun/19 7:43 PM,28/Oct/23 2:47 AM,07/Jun/19 10:24 PM,,,1.9.0,,,,,0,devops,system-tests,,,We should support docker-in-docker for docker client in CD to set up and tear down test docker pools from system tests and get rid of using jenkinsfile and ~/indy-node/environment/docker/pool scripts for this actions.,,,,,,,,,,,,,,,,,,,,,INDY-2058,,,INDY-2125,INDY-2126,INDY-2127,INDY-2131,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjhy",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,"04/Jun/19 10:19 PM;andkononykhin;PR for system tests repo is ready: [https://github.com/hyperledger/indy-test-automation/pull/24]
PR to indy-node will be created once this one is merged.;;;",,,,,,,,,,,,,,,,,,,,,
Improve dockerfiles for system tests,INDY-2058,39389,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,andkononykhin,andkononykhin,18/Apr/19 9:47 PM,14/Jun/19 11:32 PM,28/Oct/23 2:47 AM,14/Jun/19 11:32 PM,,,1.13.0,,,,,0,devops,system-tests,,,"Requirements:
 * dockerfiles should accept parameters
 * system tests CD pipeline should configure docker builds instead of patching",,,,,,,,,,,,,,,,,,,,,,,,INDY-2057,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9wi",,,,Unset,Unset,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,"14/Jun/19 11:32 PM;andkononykhin;This task is not more required for system tests since we got rid of using these dockerfiles for system tests: for now system tests are packed with their own dockerfiles.;;;",,,,,,,,,,,,,,,,,,,,,
Pool performs VC for about 10+ minutes,INDY-2059,39392,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,sergey.khoroshavin,VladimirWork,VladimirWork,18/Apr/19 10:33 PM,26/Apr/19 9:26 PM,28/Oct/23 2:47 AM,19/Apr/19 11:49 PM,,,1.7.1,,,,,0,TShirt_S,,,,"Build Info:
indy-node 1.7.0~dev896

Steps to Reproduce:
1. Install pool of 7 nodes.
2. Demote random node.
3. Stop 2 nodes from remaining ones.
4. Start them back.
5. Promote demoted node back.
6. Try to send NYMs after some time (5+ minutes).

Actual Results:
Pool with 5 nodes of 7 performs VC for very long time (10+ minutes) and actually finishes it but is it ok to do VC not under load with such small amount of nodes in pool?

Expected Resutls:
TBD.",,,,,,,,,,,,,,,,,,,,INDY-2023,,,,,,,,,,,"18/Apr/19 10:38 PM;VladimirWork;2059.tar.gz;https://jira.hyperledger.org/secure/attachment/17112/2059.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609a",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,VladimirWork,,,,,,,,,,"19/Apr/19 11:48 PM;sergey.khoroshavin;According to logs the following has happened:
* pool started in view 0 with Node1 as primary
* Node1 was demoted, triggering a view change to 1 with Node3 becoming primary
* Node1 was promoted, triggering another view change to 2 with Node3 becoming primary again
* Pool was discarding VIEW_CHANGE_DONE messages since primary didn't change
* After timeout (7 minutes) nodes tried another view change to 2 with Node4 becoming primary again
* That last view change took 5 minutes, but was able to complete successfully

This is a known problem described in INDY-2023;;;",,,,,,,,,,,,,,,,,,,,,
Watermarks may not be updated correctly after view change by a lagging node,INDY-2060,39396,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,19/Apr/19 12:46 AM,26/Apr/19 9:27 PM,28/Oct/23 2:47 AM,25/Apr/19 3:51 PM,,,1.8.0,,,,,0,TShirt_M,,,,"*Test case:*
 * A node is lagging behind
 * All nodes start a view change (to view=2 for example)
 * The node also starts view change
 * All fast nodes finish the view change pretty fast and start ordering on new view
 * The lagging node can not finish view change by prepared certificates, but finishes it by timeout only (5 mins), so that by that time all other nodes ordered till, for example,  (2, 175)

*Issue 1:*
 * The lagging node sets watermarks to (0, 300) instead of (100, 400), so that it can not order after 300th batch

*Issue 2:*
 * The lagging node can not stabilize checkpoints and update watermarks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|i00l2k:",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"24/Apr/19 7:19 PM;Toktar;+*Issue 1:*+

*Problem:*
 - The lagging node sets watermarks to (0, 300) instead of (100, 400) when ppSeqNo from last ordering is (1, 350)

*Changes:*
 - Do not change watermarks after setting a primary name in a case when a ppSeqNo in the last ordered value is not 0.

*PR:*
 - 
[test_watermarks_after_view_change.py|https://github.com/hyperledger/indy-plenum/pull/1174/files#diff-3f64c3103497e1919fdb4eb51a8bf17e]

*Version:*
 - Plenum 1.7.0.dev770

*Risk factors:*
 - Ordering after view change

*Risk:*
 - Low

*Covered with tests:*
 - [test_watermarks_after_view_change.py|https://github.com/hyperledger/indy-plenum/pull/1174/files#diff-3f64c3103497e1919fdb4eb51a8bf17e]

*Recommendations for QA:*
 * Tested in scope of INDY-1863

+*Issue 2:*+

Problem with boundary of checkpoints after view change was not found.

Added test [test_view_change_catchup_under_watermarks.py|https://github.com/hyperledger/indy-plenum/pull/1182/files#diff-24eecf3ff20ed4aa13ce73bbebe73946] 
PR: [https://github.com/hyperledger/indy-plenum/pull/1182]
Testing is not required;;;",,,,,,,,,,,,,,,,,,,,,
ATTRIB doesn't have auth rules in auth map,INDY-2061,39403,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,19/Apr/19 8:56 PM,26/Apr/19 9:26 PM,28/Oct/23 2:47 AM,23/Apr/19 10:08 PM,,,1.7.1,,,,,0,TShirt_S,,,,"*Acceptance criteria*
 * There should be an action in auth map to Add a new ATTRIB

 ** Default constraint: owner
 * There should be an action in auth map to Edit an existing ATTRIB
 ** Default constraint: owner",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/19 7:37 PM;VladimirWork;INDY-2061.tar.gz;https://jira.hyperledger.org/secure/attachment/17148/INDY-2061.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609b9",,,,Unset,Unset,Ev-Node 19.08,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,VladimirWork,,,,,,,,,"22/Apr/19 5:41 PM;Derashe;Problem reason/description:
 - Attrib wasn't included in auth_map

Changes:
 - attrib txn included in auth_map. Additional validation.

PR:
 - [https://github.com/hyperledger/indy-node/pull/1246]

Version:
 - 901 dev

Risk factors:
 - no

Risk:
 - Low

Covered with tests:
 - [https://github.com/hyperledger/indy-node/pull/1246/files#diff-0a39e5bf9d3fa2b68ac770621dd032e6]

Recommendations for QA
 * Set specific auth rules for attrib txn (for example ""owner signature AND steward signatures"" for adding attrib, and ""owner AND trustee sigantures"" for editing)
 * Check addition and editing of attrib with these auth_rules (check it for raw/hash/enc attribs)

 ;;;","23/Apr/19 7:37 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev901

Steps to Reproduce:
See steps here: https://github.com/VladimirWork/indy-test-automation/blob/003185e25776a6ebc46ac73f5ba49ba6fbaccaa3/system/draft/TestAuthMapSuite.py#L15

Actual Results:
There is an inconsistent behaviour for ""enc"" parameter while editing attrib - it fails to be edited by Trustee (according to auth rule) and requires Steward signature (like for adding, not for editing):

{noformat}
{'op': 'REJECT', 'reqId': 1556015004616995645, 'reason': ""client request invalid: UnauthorizedClientRequest('Not enough STEWARD signatures',)"", 'identifier': 'BagRmrDHtHpfmrcFUqLx23'}
{noformat}

but for two other parameters ""xhash"" and ""raw"" with the same steps this test passes:

{noformat}
{'result': {'auditPath': ['5ovm2rim3BmujRVvF1dbgG823vEUQAJYJTvntqoPCmCo', 'GS51z2tytPtazYa9hKRaUq8gSrEfBfTdYP59qwKWWij7', '6NVGJjQVapQ7HV5Xg5y8u5sHjEMp7hEXDhbcrHZ6SZsV'], 'txn': {'type': '100', 'data': {'dest': '5CJDA2owSdoyozuXJtJ3mN', 'hash': 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855'}, 'protocolVersion': 2, 'metadata': {'digest': 'a6b8039cc34b8a85cc0de9f6182864146b34c6efd28addb3fbc4e27593810bf7', 'payloadDigest': '3617698a2e36fed47bb88392d70e14f86e5789dfd510db548c9ceafb23adfbca', 'reqId': 1556014990384530371, 'from': '8jnYy6MtgFGinoo42Ken1R'}}, 'ver': '1', 'txnMetadata': {'txnTime': 1556014991, 'txnId': '5CJDA2owSdoyozuXJtJ3mN:1:e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 'seqNo': 14}, 'rootHash': 'CUYdqcvddXSxigt4JvkMSDFzt3eYkY5XMoVMKtR2TseZ', 'reqSignature': {'type': 'ED25519', 'values': [{'from': '8jnYy6MtgFGinoo42Ken1R', 'value': '4tJ1RDXPQgYJqytdG8C7Grqm8KS4Nu7pNKM47eRVabgvUNpmdKf6txZNmDyTBNg9phvEzxbi1ihuCGitDbCja1Do'}]}}, 'op': 'REPLY'}

{'op': 'REPLY', 'result': {'auditPath': ['AP5LYZ9NDM2G5L49VfSmwNttjiinHdmB8EFSHbdnSSgY', 'BHvdbAwJki4RiksaK1gvxQMCxQDir959X8mf6TJiNZ5e'], 'txn': {'type': '100', 'data': {'dest': 'WZzT6zTE6HudUPyqqZB3ZD', 'raw': '{""key"":""value2""}'}, 'metadata': {'from': '95YmX1ravD1DEF2kW8Yg84', 'payloadDigest': 'fc2951d6d0d286ab57b332c25e9e4e72dc1bf71176b1470acb1c28713cdcaa7d', 'reqId': 1556014997464665736, 'digest': '92f0ce1d3e7b5cf14a94b3f1eadb28d094da053f5f3f26d7901ebc3141861804'}, 'protocolVersion': 2}, 'ver': '1', 'txnMetadata': {'txnTime': 1556014998, 'txnId': 'WZzT6zTE6HudUPyqqZB3ZD:1:2c70e12b7a0646f92279f427c7b38e7334d8e5389cff167a1dc30e73f826b683', 'seqNo': 19}, 'rootHash': '2vNPUaJ2BJXDzEgyqMZKjNu3ZhNriDt2FtZfTRHWEpvp', 'reqSignature': {'type': 'ED25519', 'values': [{'from': '95YmX1ravD1DEF2kW8Yg84', 'value': '61ae7W94tK7qPvQbUiJuvMfof8d76XFLmniHKskBvz9gq1vra6DPt5qnzeZSvKSuzXZsBC7qT2yBAW5bRZWzFkhH'}]}}}
{noformat}

Pool's logs are in attachment. [^INDY-2061.tar.gz] ;;;","23/Apr/19 10:07 PM;VladimirWork;xhash and enc parameters have the same key and value so *they can be added only and cannot be edited*.;;;",,,,,,,,,,,,,,,,,,,
2019-03 license scanning results,INDY-2062,39409,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dhuseby,dhuseby,20/Apr/19 4:10 AM,20/Apr/19 4:10 AM,28/Oct/23 2:47 AM,,,,,,,,,0,license,,,,"Issues to address from the latest software license scan. The attached .txt file has the notes, the attached .xls has the details, the attached .spdx file is for scripts to chew on.",,,,,,,,,Not a security issue,,,,,,,,,,,,,,,,,,,,,,"20/Apr/19 4:10 AM;dhuseby;indy-2019-03-27.spdx;https://jira.hyperledger.org/secure/attachment/17122/indy-2019-03-27.spdx","20/Apr/19 4:10 AM;dhuseby;indy-2019-03-27.xlsx;https://jira.hyperledger.org/secure/attachment/17120/indy-2019-03-27.xlsx","20/Apr/19 4:10 AM;dhuseby;indy-notes-2019-03-27.txt;https://jira.hyperledger.org/secure/attachment/17121/indy-notes-2019-03-27.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ljj:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dhuseby,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a Netwrok admin, I need to have a command to set multiple auth rules",INDY-2063,39428,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,22/Apr/19 4:08 PM,22/Jun/19 5:26 AM,28/Oct/23 2:47 AM,23/Apr/19 11:17 PM,,,1.8.0,,,,,0,,,,,"As of now, we have AUTH_RULE txn that can set/modify one AUTH_RULE.

However, there are cases (such as initial bootstrap), when we need to set multiple (or even all) auth rules.

In order to simplify and improve usability, especially in the case of multi-signature, we need to have a command to set multiple auth rules.

Option:
 # Modify existing AUTH_RULE command so that it supports a list of rules to be set
 ** Pros:
 *** Easier and more consistent
 # Creates a new AUTH_RULES command to set multiple rules
 ** Pros
 *** More granular in terms of auth rule permissions for AUTH_RULE/AUTH_RULES, so that we may potentially have different policies on who can change each rule.",,,,,,,,,,,,,,,,,,,INDY-2087,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bji",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,"23/Apr/19 11:17 PM;esplinr;Setting multiple auth rules in a single transaction is likely to further complicate an already complex interaction.

We think this is better handled in an external tool, probably in context of INDY-1962 and [ST-540|https://sovrin.atlassian.net/browse/ST-540].;;;",,,,,,,,,,,,,,,,,,,,,
Issues with catch up and ordering under the load,INDY-2064,39429,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,,VladimirWork,VladimirWork,22/Apr/19 8:47 PM,21/May/19 10:58 PM,28/Oct/23 2:47 AM,09/May/19 12:03 AM,,,1.9.0,,,,,0,,,,,"There were 2 issues found during INDY-1863 testing:

Case 1:
Build Info:
indy-node 1.7.0.dev896

Steps to Reproduce:
1. Run production load test with `Max3PCBatchesInFlight = 4` for about 1 day.
2. Stop the load to try to catch up stalled nodes not under the load.
3. Run low rate load test for a half an hour to try to catch up stalled nodes by ordering batches.

Actual Results:
At least 3 nodes stalled by ViewNo (0 against 8 at the rest ones) and by all ledgers. They don't catch up and order at steps 2 and 3 too.

Logs and metrics:
ev@evernymr33:logs/INDY-1863_19_04_2019_case_1_logs.tar.gz
ev@evernymr33:logs/INDY-1863_19_04_2019_case_1_metrics.tar.gz

Case 2:
indy-node 1.7.0.dev896

Steps to Reproduce:
1. Run production load test with `Max3PCBatchesInFlight = 4` and forced VCs for 8+ hours.
2. Stop the load to try to catch up stalled nodes not under the load.

Actual Results:
Pool has stopped writing due to long several sequential VCs. Nodes have ViewNo from 0 to 30. Pool has written only 41k txns into domain and 99k txns into sovtoken ledger. Also there are some stacktraces in logs and journalctl (see more info in the attachments).

Logs and metrics:
ev@evernymr33:logs/INDY-1863_22_04_2019_case_2_logs.tar.gz
ev@evernymr33:logs/INDY-1863_22_04_2019_case_2_metrics.tar.gz",,,,,,,,,,,,,,,,,,,,,INDY-1863,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bk",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,"23/Apr/19 12:11 AM;sergey.khoroshavin;*Case 1.1*
Node4 stopped ordering at some point with _incorrect state trie_ messages in log. Analysis showed that prior to that event this node was in the middle of applying lots of transactions received through catch-up, and then OOM killer came after it. Most probably it led to partial update of databases. This is a known issue INDY-955;;;","09/May/19 12:03 AM;ashcherbakov;Will be addressed by INDY-2083, INDY-2044, INDY-2048 and PBFT view change;;;",,,,,,,,,,,,,,,,,,,,
Remove out-of-date PyPi packages,INDY-2065,39457,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,esplinr,esplinr,24/Apr/19 12:45 AM,24/Jun/19 7:26 PM,28/Oct/23 2:47 AM,20/Jun/19 7:51 PM,,,1.9.0,,,,,0,,,,,"Evernym PyPI's account holds dozens outdated packages: plenum, plenum-dev, indy-plenum-dev, indy-node-dev, anoncreds ones, some other tmp/dev variations of plenum and node. It makes sense to review and clean.

Questions:
* Should we review the packages first?
* Is there additional clean-up that is needed, such as out-of-date build scripts?",,,,,,,,,,,,,,,,,,,,,,,,INDY-2156,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609l",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,esplinr,,,,,,,,,,"14/Jun/19 12:46 AM;andkononykhin;The email with list of packages and proposed actions (keep/delete/delete later after some deprecation period) has been sent to list of recipients, waiting for responses.;;;","20/Jun/19 7:51 PM;andkononykhin;*Problem reason*: 
- Evernym PyPI account includes a lot of outdated/deprcated packages.

*Changes*: 
The following packages have been left:

#     indy-plenum
#     indy-node
#     sovrin-dev
#     sovrin
#     python3-wrapper-vcx
#     python3-indy
#     indy-crypto
#     indynotifieremail
#     sovrinnotifierawssns
#     sovrin-client-rest-dev
#     sovrinnotifieremail
#     sovrin_installer
#     sovringui

The following packages have been left but are deprecated and should be removed later (deprecation in metadata requires additional pypi release and seems not so needed) :

#     indy-node-dev
#     indy-plenum-dev
#     indy-anoncreds-dev
#     indy-anoncreds
#     python3-indy-crypto
#     indy-sdk

The following packages have been removed:

#     indy-node-authz
#     indy-plenum-authz
#     python3-indy-authz-rc
#     indy-crypto-authz
#     sovrin-client-dev
#     sovrin-node-dev
#     plenum-dev
#     anoncreds-dev
#     indy-node-new-names
#     sovrin-new-names
#     indy-anoncreds-new-names
#     indy-plenum-new-names
#     sovrin-node-repo-merge
#     plenum-repo-merge
#     anoncreds-repo-merge
#     sovrin-common-dev
#     Ledger-dev
#     state-trie-dev
#     stp-dev
#     sovrin-client
#     sovrin-node
#     anoncreds
#     sovrin-common
#     plenum
#     Ledger
#     stp
#     sovrin-node-simple-election
#     sovrin-common-simple-election
#     plenum-simple-election
#     sovrin-node-migrations-test
#     sovrin-common-new-elec
#     plenum-new-elec
#     evernym-state-rocksdb
#     stp-3pc-batch
#     plenum-3pc-batch
#     sovrin-common-3pc-batch
#     ledger-3pc-batch
#     state-trie
#     sovrin-client-3pc-batch
#     sovrin-node-3pc-batch
#     evernym-state
#     ledger-rc
#     stp-rc
#     sovrin-client-rc
#     sovrin-node-rc
#     stp-perf-imp
#     ledger-chunked-file-store
#     sovrin-common-rc
#     plenum-rc
#     anoncreds-rc
#     sovrin-dev-test2
#     sovrin-dev-test1
#     sovrin-dev-test-sif
#     sovrin-dev-test;;;",,,,,,,,,,,,,,,,,,,,
Write Transaction Author Agreement to Config Ledger,INDY-2066,39468,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,24/Apr/19 3:53 PM,14/May/19 2:35 AM,28/Oct/23 2:47 AM,13/May/19 6:04 PM,1.8.0,,1.8.0,,,,,0,,,,,"*Acceptance Criteria*
 * Create a new Transaction Author agreement transaction in the Config Ledger
 ** Txn payload:
 *** 'tag': <any string 256 chars max>
 *** 'text': <any string>
 ** Hash is sha256 calculated against concatentation of 'tag' and 'text':
 `version || text`

 * Store it in the Config State:
 ** ""{{taa:h:<hash>}}"" -> <TAA payload>
 ** ""{{:taa:v:<version>}}"" -> ""{{<hash>}}""
 ** ""{{:taa:latest}}"" -> """"{{<hash>}}""
 * Static validation:
 ** 'hash' must match the content
 * Dynamic validation:
 ** 'tag' must be unique
 * Add config txns into ts_store
 ** Update ts_store on every write into config ledger
 ** key: <CONFIG_PREFIX>:<ts>
 ** value: state_root_hash
 * Add TAA to the auth map
 ** default rule: 3 (TBD) Trustees
 * Cover by tests

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwrhj:910sx2v",,,,Unset,Unset,Ev-Node 19.09,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"30/Apr/19 11:54 PM;sergey.khoroshavin;While implementing this several questions have arised:
* Is agreement text allowed to be empty?
* Should there be any way to disable transaction author agreement?
* How hash should be serialized?;;;","06/May/19 4:30 PM;ashcherbakov;Good questions. 
[~esplinr] may correct me, but I think the answers are the following:
 * TAA's text can be empty.
 * I believe that there needs to be a way to disable TAA. Empty text is equal to absence of TAA.
 * I think that the hash can be serialized the same way, as in Request, that is the digest value as a string of hexadecimal digits (`sha256().hexdigest()`);;;","14/May/19 2:34 AM;sergey.khoroshavin;PRs:
https://github.com/hyperledger/indy-plenum/pull/1196
https://github.com/hyperledger/indy-plenum/pull/1197
https://github.com/hyperledger/indy-plenum/pull/1199
https://github.com/hyperledger/indy-plenum/pull/1201
https://github.com/hyperledger/indy-plenum/pull/1202
https://github.com/hyperledger/indy-node/pull/1281
https://github.com/hyperledger/indy-node/pull/1282
https://github.com/hyperledger/indy-node/pull/1283;;;",,,,,,,,,,,,,,,,,,,
Get Transaction Author Agreement from the config ledger,INDY-2067,39469,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,24/Apr/19 4:00 PM,20/May/19 3:46 PM,28/Oct/23 2:47 AM,20/May/19 3:46 PM,1.8.0,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * Support a read request to get the Transaction Author Agreement from the config ledger
 * Request format
 ** Name: GET_TAA
 ** input parameters:
 *** ""version"" - optional string (max 256)
 *** ""ts"" - optional timestamp string
 *** ""hash"" - optional hash string
 * Static validation:
 ** there can be either 0 or 1 input parameter
 * State proof with BLS multi-sig must be returned
 * How it works:
 ** no parameters - get the latest TA
 *** get ""hash"" by ""last_key"" in config state
 *** get value by ""last_taa""
 ** ""hash"" -get by TAA's hash
 *** get the value from state
 ** ""version"" - get the TAA for the given version

 *** get a ""hash"" from state, and then the value by ""hash""
 ** ""ts"" - get the latest TAA for the given timestamp
 *** get the config state_root for the given ts
 *** get ""last_taa"" value
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwrhj:910sx2w",,,,Unset,Unset,Ev-Node 19.09,Ev-Node 19.10,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"16/May/19 10:36 PM;sergey.khoroshavin;*Changes made*
Implemented GET_TXN_AUTHOR_AGREEMENT_TRANSACTION as [designed|https://github.com/hyperledger/indy-node/blob/master/design/txn_author_agreement.md#get_txn_author_agreement].
For all variants state proofs are generated as follows, depending on parameters supplied:
* no parameters - key is ""2:latest"" and expected value is hexdigest of received TAA
* version - key is ""2:v:version"" and expected value is hexdigest of received TAA
* digest - key is ""2:d:digest"" and expected value is JSON with TAA
{code}
{
    ""lsn"": txn-seqno,
    ""lut"": txn-timestamp,
    ""val"" : {
        ""text"": received-taa-text,
        ""version"": received-taa-version
    },
}
{code}
* timestamp - key is ""2:latest"" and expected value is hexdigest of received TAA, however additional check is required to make sure that received state root hash timestamp is within freshness interval from asked timestamp

*PR*
https://github.com/hyperledger/indy-plenum/pull/1204
https://github.com/hyperledger/indy-plenum/pull/1206
https://github.com/hyperledger/indy-plenum/pull/1208
https://github.com/hyperledger/indy-node/pull/1286

*Covered by tests*
test_get_taa_has_expected_schema
test_get_txn_author_agreement_works_on_clear_state
test_get_txn_author_agreement_cannot_have_more_than_one_parameter
test_get_txn_author_agreement_returns_latest_taa_by_default
test_get_txn_author_agreement_can_return_taa_for_old_version
test_get_txn_author_agreement_can_return_taa_for_current_version
test_get_txn_author_agreement_doesnt_return_taa_for_nonexistent_version
test_get_txn_author_agreement_can_return_taa_for_old_digest
test_get_txn_author_agreement_can_return_taa_for_current_digest
test_get_txn_author_agreement_doesnt_return_taa_for_nonexistent_digest
test_get_txn_author_agreement_can_return_taa_for_old_ts
test_get_txn_author_agreement_can_return_taa_for_fresh_ts
test_get_txn_author_agreement_doesnt_return_taa_when_it_didnt_exist

*Risk*
Low

*Known issues*
Currently SDK doesn't support state proofs from GET_TAA with timestamp, so these tests are skipped, however we do have tests that state proofs returned in this case are still valid.;;;",,,,,,,,,,,,,,,,,,,,,
Write Transaction Author Agreement Acceptance Mechanisms to the Config Ledger,INDY-2068,39470,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,24/Apr/19 4:14 PM,21/May/19 9:42 PM,28/Oct/23 2:47 AM,17/May/19 5:21 PM,1.8.0,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * Create a new TAA_AML transaction in the config ledger for the Transaction Author Agreement Acceptance Mechanism
 ** version
 ** list of
 *** label: 64 chars string
 *** description: 256 chars string
 ** url

 * Static Validation:
 ** the list can not be empty
 * Dynamic validation:
 ** version needs to be unique
 * The mechanisms need to be stored in State as well
 ** Store the latest AML
 ** Store the AML by version
 * Extend the validation of TAA txn:
 ** It can be added to the ledger only if there is already a TAA_AML txn present
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bi2",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,"15/May/19 7:25 PM;Derashe;Plenum [https://github.com/hyperledger/indy-plenum/pull/1205]

Node [https://github.com/hyperledger/indy-node/pull/1285];;;","16/May/19 10:39 PM;Derashe;Problem reason/description:
 - Implement TAA AML txn support, cover it with tests

Changes:
 - TAA AML implemented

PR:
 - Plenum [https://github.com/hyperledger/indy-plenum/pull/1205]

 - Node [https://github.com/hyperledger/indy-node/pull/1285]

Version:
 - indy-node 937

Risk factors:
 - no

Risk:
 - Low

Covered with tests:
 - [https://github.com/hyperledger/indy-plenum/pull/1205/files#diff-6de443758f7bb51cc0e36a1a316c030b]

Recommendations for QA
 * Send correct TAA AML txn, ensure it will be written
 * Send TAA AML txn with ""aml"" field as an empty dict. Ensure that it will be Reqnacked
 * Send TAA AML txn with a certain version. Ensure it written. Send another txn with same version. Ensure second txn will be Rejected;;;",,,,,,,,,,,,,,,,,,,,
Integration tests for auth_rules,INDY-2069,39471,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,,Derashe,Derashe,24/Apr/19 4:35 PM,08/May/19 11:47 PM,28/Oct/23 2:47 AM,08/May/19 11:47 PM,,,,,,,,0,,,,,We need integration tests for auth_rules for every txn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00luv:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,"08/May/19 11:47 PM;ashcherbakov;Done in the scope of INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,
Cover all AUTH_RULE changes by integration tests,INDY-2070,39472,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,24/Apr/19 4:52 PM,07/May/19 9:59 PM,28/Oct/23 2:47 AM,07/May/19 9:57 PM,,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * Write a helper method (to be re-used in plugins) to change multiple AUTH_RULEs

 * 
 ** list of auth rules as an input
 * Write a test for every Action that does the following:
 ** checks that the default rule works
 ** changes Auth rule for the action
 *** consider more complex auth rules with multi-sigs
 ** checks that the new Auth rule works and the old one doesn't work
 * There need to be a map 'action -> test' as an output

 
 # Adding new TRUSTEE                  *Done*
 # Adding new STEWARD                *Done*
 # Adding new TRUST_ANCHOR     *Done*
 # Adding new NETWORK_MONITOR      *Done*
 # Adding new Identity Owner        *Done*
 # Change Trustee to Steward        *Done*
 # Change Trustee to Trust Anchor        *Done*
 # Change Trustee to Network Monitor        *Done*
 # Demote Trustee        *Done*
 # Change Steward to Trustee        *Done*
 # Change Steward to Trust Anchor        *Done*
 # Change Steward to Network Monitor        *Done*
 # Demote Steward        *Done*
 # Change Trust Anchor to Trustee        *Done*
 # Change Trust Anchor to Steward        *Done*
 # Change Trust Anchor to Network Monitor        *Done*
 # Demote Trust Anchor        *Done*
 # Change Network Monitor to Trustee        *Done*
 # Change Network Monitor to Steward        *Done*
 # Change Network Monitor to Trust Anchor        *Done*
 # Demote Network Monitor        *Done*
 # Promote roleless user to Trustee        *Done*
 # Promote roleless user to Steward        *Done*
 # Promote roleless user to Trust Anchor        *Done*
 # Promote roleless user to Network Monitor        *Done*
 # Assign Key to new DID
 # Key Rotation - Artem O *Done*
 # Adding new Schema -  Artem O *Done*
 # Editing Schema -  Artem O not handled by auth_map for now
 # Adding new CLAIM_DEF transaction -  Artem O *Done*
 # Editing CLAIM_DEF transaction -  Artem O *Done*
 # Adding new node to pool - Renata *Done*
 # Adding new node to pool with empty services - Renata *Done*
 # Demotion of node - Renata *Done*
 # Promotion of node - Renata *Done*
 # Changing Node's ip address - Renata *Done*
 # Changing Node's port - Renata *Done*
 # Changing Client's ip address - Renata *Done*
 # Changing Client's port - Renata *Done*
 # Changing Node's blskey - Renata *Done*
 # Starting upgrade procedure - Artem *Done*
 # Canceling upgrade procedure - Artem *Done*
 # Restarting pool command - Artem *Done*
 # Pool config command (like a <code>read only</code> option) - Artem *Done*
 # Change authentification rules - Artem *Done*
 # Getting validator_info from pool - Artem *Done*
 # Adding new REVOC_REG_DEF
 # Editing REVOC_REG_DEF
 # Adding new REVOC_REG_ENTRY
 # Editing REVOC_REG_ENTRY",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwrhj:910sx2uw",,,,Unset,Unset,Ev-Node 19.08,Ev-Node 19.09,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"24/Apr/19 8:05 PM;ashcherbakov;# Adding new TRUSTEE
 # Adding new STEWARD
 # Adding new TRUST_ANCHOR
 # Adding new NETWORK_MONITOR
 # Adding new Identity Owner
 # Change Trustee to Steward
 # Change Trustee to Trust Anchor
 # Change Trustee to Network Monitor
 # Demote Trustee
 # Change Steward to Trustee
 # Change Steward to Trust Anchor
 # Change Steward to Network Monitor
 # Demote Steward
 # Change Trust Anchor to Trustee
 # Change Trust Anchor to Steward
 # Change Trust Anchor to Network Monitor
 # Demote Trust Anchor
 # Change Network Monitor to Trustee
 # Change Network Monitor to Steward
 # Change Network Monitor to Trust Anchor
 # Demote Network Monitor
 # Promote roleless user to Trustee
 # Promote roleless user to Steward
 # Promote roleless user to Trust Anchor
 # Promote roleless user to Network Monitor
 # Assign Key to new DID
 # Key Rotation
 # Adding new Schema
 # Editing Schema
 # Adding new CLAIM_DEF transaction
 # Editing CLAIM_DEF transaction
 # Adding new node to pool
 # Adding new node to pool with empty services
 # Demotion of node
 # Promotion of node
 # Changing Node's ip address
 # Changing Node's port
 # Changing Client's ip address
 # Changing Client's port
 # Changing Node's blskey
 # Starting upgrade procedure
 # Canceling upgrade procedure
 # Restarting pool command
 # Pool config command (like a <code>read only</code> option)
 # Change authentification rules
 # Getting validator_info from pool
 # Adding new REVOC_REG_DEF
 # Editing REVOC_REG_DEF
 # Adding new REVOC_REG_ENTRY
 # Editing REVOC_REG_ENTRY;;;","07/May/19 9:59 PM;anikitinDSR;All the implemented tests was merged into PR:
[https://github.com/hyperledger/indy-node/pull/1266];;;",,,,,,,,,,,,,,,,,,,,
Get Transaction Author Agreement Acceptance Mechanisms from the Config Ledger,INDY-2071,39474,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,24/Apr/19 4:57 PM,22/May/19 10:46 PM,28/Oct/23 2:47 AM,22/May/19 6:12 PM,1.8.0,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * GET_TAA_AML request
 * Input parameters:
 ** ts - optional timestamp string
 ** version- optional string
 * Static validation:
 ** only 0 or 1 parameters are supported
 * How it works
 ** If no parameters - get the latest TAA_AML from state
 ** If `ts` is present - get the config state_root fro the given `ts` from the ts_store, and then get the latest TAA_AML for this state_root
 ** If `version` is present - get the TAA_AML by `tag` from the state
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bi4",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,"21/May/19 10:06 PM;Derashe;Problem reason/description:
 - Implement GET_TAA_AML txn, cover it with tests

Changes:
 - GET_TAA_AML implemented, covered with tests

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1212]

Version:
 - [indy-node 9​41|https://build.sovrin.org/job/indy-node/job/indy-node-cd/job/master/941/]

Risk factors:
 - no

Risk:
 - Low

Covered with tests:
 - [https://github.com/hyperledger/indy-plenum/pull/1212/files#diff-2c6c168b9eb1d58e2845ce58b5c50fa5]

Recommendations for QA:
 * Send few TAA_AML txns. Remember it's versions and timestamps
 * Send GET_TAA_AML with no specified fields, ensure that latest AML txn was replied
 * Send GET_TAA_AML with specified versions of different TAA_AML txns, ensure that appopriate AML txn was replied
 * Send GET_TAA_AML with specified timestamps of different TAA_AML txns, ensure that appopriate AML txn was replied
 * Send GET_TAA_AML with specified versions and timestamp of different TAA_AML txns, ensure that REQ_NACK was replied;;;",,,,,,,,,,,,,,,,,,,,,
Support Transaction Author Agreement in Write Requests,INDY-2072,39475,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,24/Apr/19 5:04 PM,07/May/19 10:33 PM,28/Oct/23 2:47 AM,07/May/19 10:21 PM,1.8.0,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * Support agreement setting in the Request and transaction
 ** Define new request format with an optional values in every Request:
 *** hash of the transaction author agreement
 *** acceptance timestamp
 *** agreement mechanism
 ** Define new transaction format with new fields
 ** Modify signature verification to take into account new fields (they all are part of signature)
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwrhj:910sx2ww",,,,Unset,Unset,Ev-Node 19.09,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,"29/Apr/19 8:43 PM;andkononykhin;PoA:
 # Create temporary test helper API to support (add and sign) new fields
 # Update request format:
 ## update tests for ClientMessageValidator expecting new fields
 ## update ClientMessageValidator to support new new fields
 # Update txn format (tests and plenum.common.txn_util.do_req_to_txn) for new fields
 # Update tests for signature verification routine;;;","07/May/19 10:33 PM;andkononykhin;*Problem reason:*
 - write requests need to be checked regarding proper taa acceptance information to satisfy TAA policy on ledger side

*Changes*:
 - updated schema of request to cover taa acceptance data during static validation

*PR*:
 - [https://github.com/hyperledger/indy-plenum/pull/1191]

*Version*:
 - indy-plenum 1.8.0.dev781

*Risk factors*:
 - broken static validation logic

*Risk*:
 - Low

*Covered with tests:*
 - Tests for client requests were extended

*Recommendations for QA*:
 - It makes sense to postpone QA validation until INDY-2075;;;",,,,,,,,,,,,,,,,,,,,
Validate transaction author agreement as part of consensus,INDY-2073,39476,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,24/Apr/19 5:05 PM,22/May/19 8:14 PM,28/Oct/23 2:47 AM,22/May/19 6:12 PM,1.8.0,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * The author agreement is must have for all Domain transactions
 * Plugins must be able to specify for what ledgers the author agreement is also must have
 * Enhance dynamic validation as follows:
 ** If this is DOMAIN txn, or a Plugin txn from a ledger for which TAA is required - process. Otherwise - OK.
 ** Get the latest TAA (using 'last_taa' key in state)
 ** If there is no TAA - OK
 ** If the TAA's text is empty - OK
 ** Get the latest AML (using 'last_aml' key in state)
 ** If there is no AML - REJECT
 ** Get the TAA's hash and compare with the one in the request. If they are not equal - REJECT
 ** Get the request's timestamp. Make sure that the ts is in the interval [TAA's ts - 2 mins; current PP time + 2 mins]. If not - REJECT
 ** Get the requests' acceptance mechanism string. Make sure that it's present in the latest AML. If not - REJECT

 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bc",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,"07/May/19 10:43 PM;andkononykhin;PoA:
 * taa acceptance for client requests would be performed inside [Node:doDynamicValidation|https://github.com/hyperledger/indy-plenum/blob/c32edb325ef829f68beff473a9b3e4277051b6cd/plenum/server/node.py#L2401] method
 * *LedgerInfo* class would be extended to have *taa_acceptance_required* property which would be set to *True* by default
 * tests plan:
 ** extend tests for LedgerInfo
 ** taa acceptance is required for Domain and not required for other ledgers defined in plenum's Node
 ** client's requests with missed taa acceptance:
 *** pass validation if taa acceptance for corresponded ledger id is not required or not enabled (no TAA txn written to config leder yet)
 *** fail validation otherwise
 ** client requests with taa acceptance:
 *** rejected if
 **** acceptance taaDigest doesn't match latest TAA
 **** acceptance mechanism is unknown
 **** acceptance time is not acceptable
 *** accepted otherwise;;;","22/May/19 6:38 PM;andkononykhin;Problem reason:
 * All write transactions for ledger where it is expected (Domain, Plugin) should include acceptance of transaction author agreement.

Changes:
 * Node's dynamic validation has been improved to check existence and validity of the TAA acceptance.

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1214]
 * [https://github.com/hyperledger/indy-plenum/pull/1213]
 * [https://github.com/hyperledger/indy-plenum/pull/1195]

Version:
 * Indy-Node: 1.8.0.dev941

Risk factors:
 * New validation impacts all write transactions and might prevent them from being written to ledger.

Risk:
 * Low

Covered with tests:
 * Yes, unit and integration

Recommendations for QA
 * check on pool without any TAA activated yet:
 ** any write request with taa acceptance to any ledger should be rejected
 ** any valid write request with no TAA acceptance should  be written
 * activate TAA:
 ** any write request without taa acceptance to Domain ledger should be rejected
 ** any write request with invalid TAA acceptance should be rejected, possible invalid parameters:
 *** wrong TAA digest
 *** unknown TAA acceptance mechanism
 *** wrong time:
 **** less tham TAA txn time - 120 secs
 **** more than PP time + 120 secs where PP time is a txn time for the batch txn that includes the request
 ** any valid write request to Domain ledger with valid taa acceptance should be written
 ** any valid write requests to other ledgers with no TAA acceptance should be written
 ** any valid write requsts to other ledgers with TAA acceptance should be rejected
 * deactivate TAA:
 ** check the same cases as for not activated TAA;;;",,,,,,,,,,,,,,,,,,,,
Update docs for Transaction Author agreement,INDY-2074,39477,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,24/Apr/19 5:05 PM,24/May/19 4:17 PM,28/Oct/23 2:47 AM,24/May/19 4:17 PM,1.8.0,,1.8.0,,,,,0,,,,,"* Update `requests.md` and `transactions.md`
 ** Add info about new TAA fields
 ** Add info about the new txns and requests for TAA and AML
 * Update `auth_map.md` with the information about the `owner` field",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bic",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"24/May/19 12:55 AM;ashcherbakov;*Changes*
 * Added docs for TAA in Plenum
 * Fix/extend docs for AUTH_RULE and GET_AUTH_RULE
 * Add docs about TAA and TAA_AML transactions and requests
 * Add docs about Revocation transactions and requests
 * Extend auth_rules.md with TAA and TAA_AML
- Extend auth_rules.md with a description on who is the owner for each action
- Remove anyone_can_write docs from auth_rules.md
 * Add more tests for error messages when metadata is set

*PRs:*
 * [https://github.com/hyperledger/indy-node/pull/1306]
 * https://github.com/hyperledger/indy-plenum/pull/1216;;;",,,,,,,,,,,,,,,,,,,,,
Transaction Author Agreement: debug and validation,INDY-2075,39478,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,24/Apr/19 5:07 PM,30/May/19 6:46 PM,28/Oct/23 2:47 AM,30/May/19 6:46 PM,1.8.0,,1.8.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bio",,,,Unset,Unset,Ev-Node 19.10,Ev-Node 19.11,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"21/May/19 9:44 PM;VladimirWork;TAA for XFER txns (ST-557) also should be tested in scope of this ticket according to QA recommendations.;;;","27/May/19 11:42 PM;VladimirWork;Case 1:

Build Info:
indy-node 1.8.0~dev943

Steps to Reproduce:
1. Build AML request with empty `aml` parameter (using CLI custom txn or JSON):
{noformat}
{""protocolVersion"": 2, ""operation"": {""type"": ""5"", ""aml"": {}, ""version"": ""1""}, ""identifier"": ""V4SGRU86Z58d6TV7PBUe6f"", ""reqId"": 1}
{noformat}
2. Send it to the ledger.
3. Build and send TAA request.

Actual Results:
Pool returns replies for both requests.

Expected Results:
Pool should discard both requests.
-----
Fixed in 1.8.0~dev981 / 0.9.12~38.
;;;","30/May/19 6:45 PM;VladimirWork;Build Info:
indy-node 1.8.0~dev981

Actual Results:
System tests (TestTAASuite) has been added and run successfully. They are sent in this https://github.com/hyperledger/indy-test-automation/pull/23 PR.;;;",,,,,,,,,,,,,,,,,,,
Defined list of required custom package in indy-node CD pipeline,INDY-2076,39484,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,24/Apr/19 10:41 PM,24/Apr/19 10:41 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Currently indy-node CD pipeline defines set of custom packages that builds itself that prevents unnecessary builds when the packages are already published.

indy-plenum is not built in scope indy-node pipeline but is also required during promotion logic from latest repos. There is an API in jenkins library to skip promotion if not necessary that requires to specify list of packages that are expected.

Need to adjust CD pipeline to make possible such a skipping logic when possible.

Requirement:
 * indy-plenum should be specified with exact expected version
 * version of plenum shouldn't be specified directly in CD pipeline to avoid unnecessary maintenance (version bumping)",,,,,,,,,,,,,,,,,,,,,INDY-2038,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00lwn:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a Network Admin, I need to be able to forbid an action in AUTH_RULE, so that no changes in code are needed",INDY-2077,39487,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,24/Apr/19 11:53 PM,07/Jun/19 7:35 PM,28/Oct/23 2:47 AM,07/Jun/19 7:35 PM,,,1.9.0,,,,,0,,,,,"*Acceptance Criteria*
 * Support an Auth Constraint that forbids an action
 * Apply this to Schema editing
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjgr",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,Toktar,,,,,,,,,"14/May/19 1:50 AM;ashcherbakov;Partially done in https://github.com/hyperledger/indy-node/pull/1279;;;","24/May/19 5:53 PM;Toktar;*PoA:*
 At the moment there are constraints of the type Role, And, Or. It is necessary to add the Forbidden constraint to indicate the unavailability of the execution of any action.
 The validator for Forbidden will immediately return an error with the Forbidden constraint translated into a string.
 Forbidden constraint structure:
{code:java}
{""constraint_id"": 'FORBIDDEN'}
{code}
It is recommended to use this type of constraint as an independent. When used in the expression of many constraints, perceived as a logical 0.

Tests:
 * add unit tests to [test_auth_constraint.py|https://github.com/hyperledger/indy-node/pull/1296/files#diff-e5646c7dd3f8a2b580b869a1a66db99c] 
 * test_get_one_disabled_auth_rule_transaction

 

PR:  [https://github.com/hyperledger/indy-node/pull/1296];;;","05/Jun/19 8:49 PM;ashcherbakov;Please create the following system test:
- send AUTH_RULE to Forbid creation of new Trustees
- try to create a Trustee
- make sure that it's not possible anymore;;;","05/Jun/19 9:48 PM;Toktar;Problem reason:
 * There is no constraint for the forbidden some action.

Changes:
 * Add command FORBIDDEN constraint
 * Add key for auth_map for editing schema with a forbidden constraint.
 * Add tests

PR:
 * [https://github.com/hyperledger/indy-node/pull/1296]

Version:
 * indy-node 1.9.0~dev987 master

Risk factors:
 * AUTH_RULE, AUTH_RULES and GET_AUTH_RULES with forbidden constraint.

Risk:
 * Low

Covered with tests:
 * test_plugin_complex_with_or_rule_with_not_allowed
 * [test_auth_constraint.py|https://github.com/hyperledger/indy-node/pull/1296/files#diff-e5646c7dd3f8a2b580b869a1a66db99c] 

Recommendations for QA:
Test sending AUTH_RULE with forbidden constraint and get it with GET_AUTH_RULE request via sdk functions.;;;","07/Jun/19 7:24 PM;andkononykhin;Implemented new  system test with the following scenario:
 # start new docker pool
 # using trustee1 did send add trustee nym - should pass
 # using trustee1 did set forbidden constraint for that nym operation - should pass
 # using trustee1 did get auth rule for that operation - should reply with forbidden constraint
 # using trustee1 try to add one more new trustee - should be rejected

Run the scenario on pool using the following versions of artifacts:
 * client:
 ** libindy 1.9.0~1122 and python wrapper version 1.9.0-dev-1122
 * pool:
 ** indy-node 1.9.0~dev987
 ** indy-plenum 1.9.0~dev808

Test passed.

For now [the test|https://github.com/hyperledger/indy-test-automation/blob/95cae6e6b02441c7d742425d075be64e5f70bf67/system/draft/TestAuthMapSuite.py#L1029] is a part of [the pull request|https://github.com/hyperledger/indy-test-automation/pull/25] to indy-test-automation repository.;;;",,,,,,,,,,,,,,,,,
Editing of CLAIM_DEF uses auth rules for Adding a Claim Def,INDY-2078,39489,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,25/Apr/19 12:13 AM,21/May/19 10:58 PM,28/Oct/23 2:47 AM,13/May/19 8:04 PM,,,1.9.0,,,,,0,,,,,"*Acceptance criteria*
 * Use correct Auth rules for editing a Claim Def
 * Cover by tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwrhj:910sx2y",,,,Unset,Unset,Ev-Node 19.09,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"07/May/19 10:03 PM;anikitinDSR;Reasons:
 * Need to add auth rule using for validating CLAIM_DEF edit operation

Changes:
 * Added auth rule using for edit operation
 * Covered by tests in the scope of INDY-2070

PR:
 * indy-node: [https://github.com/hyperledger/indy-node/pull/1266]

Recomendation for QA:
 * need to check, that changing default auth_rule for edit CLAIM_DEF action affects for request validation behaviour

 ;;;","13/May/19 8:04 PM;VladimirWork;Build Info:
indy-node 1.8.0~dev926

Steps to Validate:
1. Set various rules for ADD and EDIT for CRED_DEF, REVOC_REG_DEF, REVOC_REG_ENTRY txns and run this actions using DIDs with different roles according to this rules.

Actual Results:
Rules behave as expected. Basic system tests are implemented (will be extended with some tricky cases): https://github.com/VladimirWork/indy-test-automation/blob/master/system/draft/TestAuthMapSuite.py;;;",,,,,,,,,,,,,,,,,,,,
A txn with existing DID without role and verkey can override existing NYM,INDY-2079,39500,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Apr/19 4:30 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywbji",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.7.1,INDY-2080,39508,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,25/Apr/19 7:04 PM,08/May/19 11:49 PM,28/Oct/23 2:47 AM,08/May/19 11:47 PM,,,,,,,,0,,,,,"*Version Information:*
indy-node 1.7.1
indy-plenum 1.7.1
sovrin 1.1.41

*Notices for Stewards:*
(!) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*
(!) *Pool upgrade to sovrin 1.1.32 and above should be performed simultaneously for all nodes due to txn format changes.*
(!) *Pool upgrade to indy-node 1.7.1 should be performed simultaneously for all nodes due to audit ledger.*
(!) *There should be no fees set up.*

*Major Changes*
- Audit Ledger
 -- helps keeping all other ledgers in sync
 -- helps recovering of pool state by new or restarted nodes
 -- can be used for external audit
- Correct support of multi-signatures
- Configurable Auth Rules in config state
- Stability fixes

*Detailed Changelog*

+Fixes:+
INDY-2008 - Validator-info doesn't show view change information and sometimes shows node info as unknown
INDY-2020 - Schema can't be written with error ""'Version' object has no attribute 'dev'""
INDY-2018 - Node fails to start after the load
INDY-2022 - POA: Sovrin TestNet lost consensus
INDY-2047 - Nodes can fail on first start after upgrading from version without audit ledger to version with audit ledger
INDY-2035 - Pool is getting out of consensus after a forced view change and writes to all the ledgers
INDY-1720 - View Change processing - replica ends up with incorrect primaries
INDY-2031 - Validator node shows False for consensus
INDY-2060 - Watermarks may not be updated correctly after view change by a lagging node
INDY-2061 - ATTRIB doesn't have auth rules in auth map
INDY-2050 - Some nodes are stalled and throw an error under load
INDY-2055 - Some nodes failed to join consensus after upgrade

+Changes and Additions:+
INDY-1946 - Implementation: Restore current 3PC state from audit ledger
INDY-1992 - Implementation (not active): As a user/steward I want to have better understanding of release version and changelog
INDY-2001 - Implement auth rule maps in config ledger
INDY-1944 - Add audit ledger
INDY-1984 - INSTANCE_CHANGE messages should be persisted between restarts
INDY-2006 - Add updateState method for ConfigReqHandler
INDY-2002 - Use auth constraints from config ledger for validation
INDY-1945 - Implementation: Improve catch-up to use audit ledger for consistency
INDY-2003 - Implement a command to set auth constraints
INDY-1995 - Debug and validation: Move the auth_map structure to the config ledger
INDY-1554 - Need to enhance write permissions for Revocation transactions
INDY-2010 - Implement a command to get auth constraint
INDY-2016 - Integrate testinfra-based system tests to Indy CD
INDY-2019 - Debug and Validation: As a user/steward I want to have better understanding of release version and changelog
INDY-2028 - As a QA I want system tests to be run in parallel in CD pipeline
INDY-1993 - Debug and Validation: Audit Ledger and improving catch-up to use audit ledger for consistency
INDY-1757 - Need to track same transactions with different multi-signatures
INDY-2025 - Debug and Validation: Restore current 3PC state from audit ledger - Phase 1
INDY-1983 - A Node need to be able to order stashed requests after long catch-ups
INDY-1674 - Need to account fields from PLUGIN_CLIENT_REQUEST_FIELDS when calculating digest
INDY-2046 - Debug and validation: Multi-signature support
INDY-2051 - Debug and Validation: Restore current 3PC state from audit ledger - Phase 2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwrhj:910sx2z",,,,Unset,Unset,Ev-Node 19.09,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Build and publish dummy top level package to test upgrade,INDY-2081,39510,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,25/Apr/19 10:00 PM,25/Apr/19 10:00 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"It makes sense to have some dummy package dependent on indy-node to simplify QA acceptance for upgrade logic which expects cases when we specify some ""top level package"" in Upgrade txn.

Acceptance criteria:
 * automated built logic
 * the package should depends on indy-node
 * publishing should be added to RC pipeline and optionally to master as well, not necessary for stable",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00m13:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Individual nodes can tombstone data,INDY-2082,39619,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,04/May/19 8:29 AM,09/Oct/19 12:28 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Story*

As a steward on an Indy Network, I need to be able to mark individual ledger entries as ""deleted"" so that I can comply with regulatory requirements in my geography that forbid me from serving some items.

*Acceptance Criteria*
 * A steward can mark specific ledger entries with a ""tombstone""
 * When such ledger entries are requested, an error similar to HTTP 451 should be returned.
 * Developers of ledger plugins should be able to mark whether ledgers created by their plugins can use tombstones without breaking functionality (i.e. tombstoning a payment address would break all subsequent payments)
 * When the SDK receives an error similar to HTTP 451, it will try other nodes on the network the same as with any other failed read request.
 * A steward can remove the tombstone on a ledger entry.

*Notes*
 * Types of data which might be forbidden: pornography, terrorist content, accidentally submitted personal data.
 * Tombstone specific to a node strike an reasonable balance between regulatory compliance and concerns of censorship, as the data will remain available on the network unless it is concerning enough for all stewards to agree to tombstone the data.
 * The BLS multi-signature is created when the data is initially written to the ledger, so as long as any single node is willing to return the data, the data will be returned.
 * By default, if the SDK receives enough rejects (F+1), it will stop trying to get the data. But this behavior could be overwritten if the user really wants the data. We could make it the default behavior of the SDK to keep retrying if failures are caused by the 451-style error.",,,,,,,,,,,,,,,,,,,INDY-277,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-5,,Node Tombstones,To Do,No,,Unset,No,,,"1|i00mmn:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce CONS_PROOF timeout to speed up catchup under the load,INDY-2083,39715,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,08/May/19 11:55 PM,11/Jun/19 6:01 PM,28/Oct/23 2:47 AM,11/Jun/19 5:40 PM,,,1.9.0,,,,,0,TShirt_S,,,,"* We require f+1 equal CONS_PROOFs to start a catch-up
 * Under the load, there is a high chance that all nodes will have a different CONS_PROOF (due to different ledger size)
 * In this case required CONS_PROOFs will be requested explicitly
 * CONS_PROOFs will be requested in a timeout, which is dependent on pool size and quite big
 * => This slows down catch-up a lot, especially under the load

*Acceptance criteria*
 * Decide if we need this timeout at all (especially if we get CONS_PROOFs from most of the nodes)
 * Remove or reduce timeout
 * Cover by tests
 * Do load testing of catchup under the load. Make sure that catchup is faster.",,,,,,,,,,,,,,,,,,,,,,,,INDY-1472,INDY-2112,INDY-2029,INDY-1242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjc",,,,Unset,Unset,Ev-Node 19.11,Ev-Node 19.12,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"05/Jun/19 11:07 PM;sergey.khoroshavin;*Protocol proposal*
* Phase A
** Request ledger statuses from all nodes 
** Wait for N-f responses, on (long) timeout log an error and request again
** Sort responses by maximum seq no
** Use seq no of response f+1 as a target seq no
** Enter phase B
* Phase B
** Request consistency proofs for target seq no
** Wait until we have f+1 equal cons proofs with target seq no, on (short) timeout log a warning and request again
** Start actual catch up

*Rationale*

Healthy network requires at least N-f honest nodes up and running, therefore we can expect to receive this number of replies eventually, usually within short timeframe. Failure to receive this amount of replies is a sign of serious connectivity problems at least, and should be logged.

In a worst case scenario our N-f replies can contain f replies from malicious nodes. Following cases are possible
# Malicious nodes report very high sequence numbers. Taking f+1's reply will hit honest node, which means that eventually all honest nodes will have this many transactions, and it is safe to start a catch up to this target. In the worst case we can fail to get enough replies in phase B on the first try, this is why it has much shorter timeout.
# Malicious nodes report very low sequence numbers. Taking f+1's reply will still hit honest node, and f honest nodes have at least this number of transactions, which means it is safe to start a catch up and we should get enough replies in phase B on the first try. In the worst case we won't catch up till the latest available transaction in pool, however in a healthy pool gap between fastest and somewhat lagging node shouldn't be very large, so this shouldn't be a problem.
# Malicious nodes report random sequence numbers. Taking f+1's reply can possibly hit malicious node, however in this case at least one honest node is guaranteed to have at least same number of transactions, which brings us back to case 1.

f+1 equal consistency proofs in phase B are required in order to make sure, that at least one honest node have this many transactions with this target root hash.;;;","07/Jun/19 2:15 AM;sergey.khoroshavin;*Minimal PoA*
* Changes:
** Upon having N-f-1 replies without having f+1 equal consistency proofs send consistency proof request immediately AND schedule second request after timeout
* Cover by integration test:
** Make cons proof request timeout very large
** Prepare a pool with different number of transactions on different nodes + one lagging node
** Make sure lagging node can catch up;;;","07/Jun/19 9:53 PM;sergey.khoroshavin;*Problem reason*
As in description

*Changes made*
* request for equal consistency proofs is sent as soon as we have N-f-1 replies (either cons proofs or ledger statuses)
* consistency proof is asked for f+1's largest known ledger size

*Version*
indy-node 1.9.0~dev997

*PR*
https://github.com/hyperledger/indy-plenum/pull/1235

*Covered by tests*
* test_catchup_from_unequal_nodes_without_waiting

*Risk*
Low

*Recommendation for QA*
Run load test on 25-nodes AWS pool:
* start load test with 10 writes per second
* stop nodes 15-20 for 0.5/1/5/10 minutes
* simultaneously start them
* make sure they manage to catch up;;;","11/Jun/19 1:32 AM;sergey.khoroshavin;*Test performed*
Load test was performed with following command:
{code}
perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}]"" -c 10 -b 10 -l 10
{code}
and nodes stopped for 0.5/1/5/10 minutes

*Expected result*
Nodes should quickly catch up after restart and continue ordering

*Actual result*
In all cases except last (stopping for 10 minutes) all nodes managed to catch up and continue ordering with pool, however they were somewhat slower to do so than expected. In last case (stopping for 10 minutes) 4 of 5 nodes managed to catch up and continued ordering, 1 node remained stuck.

Quick *logs analysis* showed following evidence:
* nodes were quite slow to connect to each other (it took up to 2 minutes in some cases)
* some nodes were unable to connect to each other at all despite numerous reconnection attempts
* nodes that were not restarted reported that they were able to connect to all restarted nodes
* which means that some connections appeared to be one-sided (sic!)
* in all cases initial phase (PreSyncPool) took about 3 minutes, mostly waiting for ledger statuses from other nodes, which were received with quite large delay
* Node17 received just 13 ledger status messages, which was insufficient to continue a catch up
* other phases were significantly faster (not more than 1.5 minutes per phase), however gathering ledger statuses and consistency proofs took some time (~30 seconds)
* it seems like slow reaction was partly due to large non catchup-related traffic (propagates, 3PC messages)

Even though results are not perfect they are still better than all previous attempts. Besides, it seems like failures were partly connected to either connectivity issues or unexpected behavior of zeromq. Further catch-up improvements can be done in scope of other issues, some ideas:
* investigate zmq behavior
* make initial catch up more resilient by adding resending mechanism
* use BLS signatures to reduce number of communication steps in cons proof gathering phase
* consider adding separate communication channel for high priority traffic between nodes;;;","11/Jun/19 5:23 PM;ashcherbakov;We already have a number of tasks for the action items above:
 * https://jira.hyperledger.org/browse/INDY-1472
 * https://jira.hyperledger.org/browse/INDY-2112
 * https://jira.hyperledger.org/browse/INDY-2029
 * https://jira.hyperledger.org/browse/INDY-1242;;;",,,,,,,,,,,,,,,,,
The owner for RevocRegDef must be the author of the corresponding CredDef,INDY-2084,39716,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,08/May/19 11:59 PM,24/Oct/19 5:00 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"* As of now, changing the `owner` in REVOC_REG_DEF's AuthRule doesn't have any affect.
 * However, it may make sense to consider the REVOC_REG_DEF's owner as the owner of the corresponding CredDef to fulfill the following use cases:
 ** As a Network Admin, I need to be able to set an auth constraint for RevocRegDef, so that its creation/editing can be done only by the author of the corresponding CredDef",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2266,,,No,,Unset,No,,,"1|hzwx4f:2rzmg",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indy-CLI requires a DID when doing a mint-prepare (SDK no longer requires),INDY-2085,39762,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,lbendixsen,lbendixsen,lbendixsen,10/May/19 4:53 AM,09/Aug/19 5:08 AM,28/Oct/23 2:47 AM,09/Aug/19 5:08 AM,1.7.1,,,,,,,0,TShirt_S,,,,"I am testing with Indy-CLI version 1.8.1 and when I send a ledger mint-prepare command without ""using"" a DID it returns:

*Error: Invalid structure*

  *Caused by: Plugin returned error*

So then I created an unprivileged DID and prepared the mint transaction using that DID. I then got three trustees to multi-sign the txn.  I then tried ""ledger custom <txn>"" while using a Trustee DID and the response was: 

*Transaction has been rejected: validation error [SafeRequest]: The identifier is not contained in signatures*

So I added a signature for the unprivileged DID that was in the ""identifier"" field along with the three TRUSTEE signatures and got the following error (It might be appropriate for this last to be a separate ticket since I had the required number of TRUSTEE signatures and the command still did not work...)

*Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(3, 4)*

I tried a few other things but could not get the CLI to post the transaction with the unprivileged DID.  

Expected result:  I should be able to send a mint-prepare command without a DID.

Actual result: mint-prepare requires a DID and my testing shows that it probably has to be a TRUSTEE DID.

 ",,,,,,,,,,,,,,,,,,,,,,,,INDY-2103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bir",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Artemkaaas,ashcherbakov,esplinr,lbendixsen,,,,,,,,"13/May/19 5:40 PM;Artemkaaas;Could you share logs or all commands you entered?
I am able to create MINT transaction without a DID

{code:java}
indy> wallet open w2 key=k2
Wallet ""w2"" has been opened
wallet(w2):indy> pool connect p1
Pool ""p1"" has been connected
pool(p1):wallet(w2):indy> load-plugin library=libsovtoken.so initializer=sovtoken_init
Plugin has been loaded: ""libsovtoken.so""
pool(p1):wallet(w2):indy> payment-address create payment_method=sov
Payment Address has been created ""pay:sov:EXCH9fh3NzMvGvJzrtu1vB9qr4ddcvPbuJqch5fenqeg8hcGN""
pool(p1):wallet(w2):indy> ledger mint-prepare outputs=(pay:sov:EXCH9fh3NzMvGvJzrtu1vB9qr4ddcvPbuJqch5fenqeg8hcGN,100)
MINT transaction has been created:
     {""operation"":{""type"":""10000"",""outputs"":[{""address"":""EXCH9fh3NzMvGvJzrtu1vB9qr4ddcvPbuJqch5fenqeg8hcGN"",""amount"":100}]},""reqId"":3166139930,""protocolVersion"":2,""identifier"":""LibsovtokenDid11111111""}
{code}

It works.

The second and the third validation errors really there are. 
*But* it is expected behavior. Seems we need set `submitter_did` as a required field.;;;","14/May/19 12:20 AM;esplinr;[~lbendixsen] Can you share the exact auth_rule you had configured on the ledger when you tried this use case?;;;","14/May/19 12:24 AM;esplinr;Can you also share the version numbers of the other software you were using for this test?
* LibSovToken
* Indy Node
* Indy Token Plugins;;;","14/May/19 1:07 AM;lbendixsen;I cannot yet share the auth_rule because I have not yet written the code needed for ""get_all_auth_rules"".  I will say, though that it should be the default as I have not yet changed it.  I am testing on the BuilderNet which was updated to the latest on 5-6-2019.

LibSovToken - (built on a MAC from master 5-6-2019)

Indy Node - 1.7.1 

Indy Token Plugins - 0.9.11 (Sovrin version 1.1.41);;;","15/May/19 1:51 AM;ashcherbakov;[~lbendixsen]

1) *First issue* (that it's not possible to send MINT without an identifier):
 * It looks like you use stable versions of Node/Plugins, stable CLI, but manually built master libsovtoken, isn't it? 
Can you please try this with the officially built stable version of libsovtoken?
Are you running the test (CLI) on Ubuntu or Mac?
 * If the issue is reproduced on stable, can you please provide exact steps and command you did in CLI?

2) *Second issue: Transaction has been rejected: validation error [SafeRequest]: The identifier is not contained in signatures*
 * This is expected and fine. If there is identifier (author/sender of a txn), and a number of signatures, we do require that the identifier must also sign the txn. 
We require this to avoid tricky attacks. For example, Bob and Malfoy create a CRED_DEF both signing the txn, but use Alice's DID as Identifier. So, there will be a CRED_DEF txn on the ledger for which Alice is the author, while Alice is not aware of this txn at all. 

3) *Third issue:* *Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(3, 4)*
 * This exception may indicate that there is no NYM txn for the non-Trustee DID you created. So, the ledger can not verify 1 of 4 signatures since he's not aware of the public key (verkey) for the signature.
 * Can you please make sure that you sent a NYM txn for the unprivileged DID you use for signing and sending?
 * The error itself is not so descriptive and clear, so we may create a minor bug to fix this error message to avoid confusion in future.;;;","15/May/19 4:34 AM;lbendixsen;Thanks for looking into the issues, Alexander. And sorry for the lack of needed details and steps in the original description, I'll be better next time. :)

1) First Issue:  I am on a MAC and I think I was also not using the latest stable CLI as well as not the latest stable libsovtoken. I built both, because I am on a MAC.  I am uncertain how long it will take me to find MAC released versions of the CLI and libsovtoken, so here is the original command sent after setting up the pool and wallet and using no DID, then success while using the unpermissioned DID.  I will continue trying to find MAC released versions, try again, and post those results in a future comment.

pool(buildernet):wallet(testnet_wallet):indy> ledger mint-prepare outputs=(pay:sov:52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu,1200000000000000000)

*Error: Invalid structure*

  *Caused by: Plugin returned error*

pool(buildernet):wallet(testnet_wallet):indy> did use GcyGfNY7y9WuEqUGdEoAxQ

*Did ""GcyGfNY7y9WuEqUGdEoAxQ"" has been set as active*

pool(buildernet):wallet(testnet_wallet):did(Gcy...AxQ):indy> ledger mint-prepare outputs=(pay:sov:52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu,1200000000000000000)

*MINT transaction has been created:*

     {""operation"":\{""type"":""10000"",""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount"":1200000000000000000}]},""reqId"":3479987566,""protocolVersion"":2,""identifier"":""GcyGfNY7y9WuEqUGdEoAxQ""}

2) ok, understood.

3) I am including the original custom CLI call that failed to use the signature with the non-privileged NYM, and then an extra command from today showing the NYM on the ledger.

Original failed txn USEing non-priveleged DID (including 4 signatures, 3 TRUSTEE and 1 non-priviledged) -

pool(buildernet):wallet(testnet_wallet):did(*Gcy...AxQ*):indy> ledger custom \{""identifier"":""GcyGfNY7y9WuEqUGdEoAxQ"",""operation"":{""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount000""},""protocolVersion"":2,""reqId"":3479987566,""signatures"":\{""5M3i1PbpvEQmTk25EmAY6N"":""4Rsn1mcuK7KMPgy5BZiPGysWyLdQeK7u7xk9a3pmvF4HH2Sid4mzB1e7hVztfSQincW4C1rLAb7wj21JcWpo4fhb"",""6feBTywcmJUriqqnGc1zSJ"":""2Tt9aMeqhyfz1pWoCskDEYL35z2uh1HCihoQ29gWY2gNnJQ5NAKnHD"",""GcyGfNY7y9WuEqUGdEoAxQ"":""3txpUMemuhhUo4iFZbM34CSiUYTUUpF3csdUcyJGLpBYdbnYrEzd37wSg9rDfJVJsHckFQfMLzR9ywJEcK8Yxhj4"",""Kv2YdE5KGgdruMGW6p5w4b"":""paPauVLGkDsM8gXeJsvEzgVQzm67VeZLziWcDKMRjWDARqu8noqgH""}}

*Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(3, 4)*

 

A similar txn request also failed when USEing my Trustee DID for Buildernet (custom txn attempted as a Trustee):

pool(buildernet):wallet(testnet_wallet):did(*5M3...Y6N*):indy> ledger custom \{""identifier"":""GcyGfNY7y9WuEqUGdEoAxQ"",""operation"":{""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount000""},""protocolVersion"":2,""reqId"":3479987566,""signatures"":\{""5M3i1PbpvEQmTk25EmAY6N"":""4Rsn1mcuK7KMPgy5BZiPGysWyLdQeK7u7xk9a3pmvF4HH2Sid4mzB1e7hVztfSQincW4C1rLAb7wj21JcWpo4fhb"",""6feBTywcmJUriqqnGc1zSJ"":""2Tt9aMeqhyfz1pWoCskDEYL35z2uh1HCihoQ29gWY2gNnJQ5NAKnHD"",""GcyGfNY7y9WuEqUGdEoAxQ"":""3txpUMemuhhUo4iFZbM34CSiUYTUUpF3csdUcyJGLpBYdbnYrEzd37wSg9rDfJVJsHckFQfMLzR9ywJEcK8Yxhj4"",""K2w8H44Z4PeRHphoAcFi4n"":""4gV4aSz5X5aBoJpCPhi5R5PUQEQqk7ddYA45m16gLThQjUQivrEn4A"",""Kv2YdE5KGgdruMGW6p5w4b"":""paPauVLGkDsM8gXeJCuwzhpu7r2chqDCFtseuAjka5p234ZYc4isvEzgVQzm67VeZLziWcDKMRjWDARqu8noqgH""}}

*Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(4, 5)*

Finally, here is the request showing that the NYM is on the ledger:

pool(buildernet):wallet(testnet_wallet):did(V5q...2WP):indy> ledger get-nym did=GcyGfNY7y9WuEqUGdEoAxQ

*Following NYM has been received.*

*Metadata:*

+------------------------+-----------------+---------------------+---------------------+

| *Identifier *            | *Sequence Number* | *Request ID*          | *Transaction time*    |

+------------------------+-----------------+---------------------+---------------------+

| V5qJo72nMeF7x3ci8Zv2WP | 37              | 1557858362892512000 | 2019-05-08 17:17:53 |

+------------------------+-----------------+---------------------+---------------------+

*Data:*

+------------------------+------------------------+-------------------------+------+

| *Identifier *            | *Dest *                  | *Verkey*                  | *Role* |

+------------------------+------------------------+-------------------------+------+

| V5qJo72nMeF7x3ci8Zv2WP | GcyGfNY7y9WuEqUGdEoAxQ | ~8puM2StuMtC6CyfSgni6ve | -    |

+------------------------+------------------------+-------------------------+------+;;;","15/May/19 5:20 AM;lbendixsen;Oops, it looks like we do not publish official builds of Indy-CLI or libsovtoken for the MAC.  I will try and make sure I have the ""stable"" release of each and then try the steps again.;;;","15/May/19 7:19 AM;lbendixsen;Here are the commands and their results after getting and building on MAC from the stable branches: (Indy-CLI 1.8.3 and libsovtoken 0.9.8).

pool(buildernet):wallet(testnet_wallet):indy> ledger mint-prepare outputs=(pay:sov:52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu,1000000000000000)

*MINT transaction has been created:*

     {""operation"":\{""type"":""10000"",""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount"":1000000000000000}]},""reqId"":2788483592,""protocolVersion"":2,""identifier"":""LibsovtokenDid11111111""}

pool(buildernet):wallet(testnet_wallet):indy> did use 5M3i1PbpvEQmTk25EmAY6N

*Did ""5M3i1PbpvEQmTk25EmAY6N"" has been set as active*

pool(buildernet):wallet(testnet_wallet):did(5M3...Y6N):indy> ledger sign-multi txn=\{""operation"":{""type"":""10000"",""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount"":1000000000000000}]},""reqId"":2788483592,""protocolVersion"":2,""identifier"":""LibsovtokenDid11111111""}

*Transaction has been signed:*

*{""identifier"":""LibsovtokenDid11111111"",""operation"":\{""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount"":1000000000000000}],""type"":""10000""},""protocolVersion"":2,""reqId"":2788483592,""signatures"":\{""5M3i1PbpvEQmTk25EmAY6N"":""3LSZpjd1avbFQoTfhi9nyBky4VmCd9kZtQHLxHdNw36ix6ERp5SFdxjYwC68iBfnQn4iw8gHpevhfrBbnA4wzM6t""}}*

<other signatures were obtained from other trustees at this point>

 

pool(buildernet):wallet(testnet_wallet):did(5M3...Y6N):indy> ledger custom \{""identifier"":""LibsovtokenDid11111111"",""operation"":{""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount"":1000000000000000}],""type"":""10000""},""protocolVersion"":2,""reqId"":2788483592,""signatures"":\{""5M3i1PbpvEQmTk25EmAY6N"":""3LSZpjd1avbFQoTfhi9nyBky4VmCd9kZtQHLxHdNw36ix6ERp5SFdxjYwC68iBfnQn4iw8gHpevhfrBbnA4wzM6t"",""K2w8H44Z4PeRHphoAcFi4n"":""ndHPu1aYLH75TuQNNCGiecsFypUUYVLbFT23xVpdjbnKALwpiJff36cknwaXpyXWMztBgKi9Wqfbm3b82MNT73a"",""YLCJ5wri6K8asJnHdJXzDP"":""DeJcecf4TUURtVcNTbZL3e4psCfxA3CoVceTkWGYgiJmLSxW5U2PECfHTr9yZkHGBYL9ASMrkxZWeiFNhgWnNiS""}}

*Transaction has been rejected: validation error [SafeRequest]: The identifier is not contained in signatures*

<I did not continue the original test pattern for this case because I cannot sign for the LibsovtokenDid11111111 identifier>

 

As you can see, I was able to build the transaction without a DID, but then it was impossible for me to actually submit the txn.  From the CLI, a mint-prepare command must be run by a Trustee for it to be usable later, it appears.  One reason I continue to pursue this issue is because Artem wrote a minting tool that can build a txn without using a DID and then it is able to successfully send it later.  I don't know which one is right, I just noticed the difference.  

The original summary of this ticket (issue 1) is resolved at this point so please advise on whether a new ticket should be created for subsequent issues needing addressed (if any).;;;","15/May/19 5:26 PM;ashcherbakov;[~lbendixsen]
Thanks for the update and re-testing.

So. let me summarize it.

*Issue1:*
 # was a build issue, and resolved with the correct builds.
 # As for
{quote}As you can see, I was able to build the transaction without a DID, but then it was impossible for me to actually submit the txn. From the CLI, a mint-prepare command must be run by a Trustee for it to be usable later, it appears. One reason I continue to pursue this issue is because Artem wrote a minting tool that can build a txn without using a DID and then it is able to successfully send it later. I don't know which one is right, I just noticed the difference.{quote}

         The situation is the following:
 * On the latest stable, if there is a MINT without a DID created, then libsovtoken puts a default DID there (`LibsovtokenDid11111111 `), and hence we see the issue that you've described
==> *on the latest stable a valid DID must always be specified for MINT txn*
 * On the latest master, if there is a MINT without a DID created, then *libsovtoken doesn't put any default DID (leaving it None)* (see https://github.com/sovrin-foundation/libsovtoken/pull/376).
*==> on the latest master, a MINT without a DID works correctly*

 

*Issue2:* not an issue, but expected behavior.

*Issue3:* 
So, the non-privileged DID is on the ledger, thanks for checking this.
{quote}<other signatures were obtained from other trustees at this point> 
{quote}
Can you please also check, that DIDs for each Trustee also present on the ledger?

In particular, can you please check that the following DIDs are present on the ledger:
 * 5M3i1PbpvEQmTk25EmAY6N
 * 6feBTywcmJUriqqnGc1zSJ
 * K2w8H44Z4PeRHphoAcFi4n
 * Kv2YdE5KGgdruMGW6p5w4b

 

If they all present, are we sure that all the signatures are correct (no keys have been rotated for example?);;;","16/May/19 6:43 AM;lbendixsen;[~ashcherbakov]

Issue 3 response:

All of the DIDs mentioned are trustees on the BuilderNet.

+------------------------+-------------------------+---------+

| *Dest *                  | *Verkey*                  | *Role*    |

+------------------------+-------------------------+---------+

| 5M3i1PbpvEQmTk25EmAY6N | ~7iwFwParUgTffA22Q5Tgvg | TRUSTEE |

+------------------------+-------------------------+---------+

| *Dest *                  | *Verkey*                  | *Role*    |

+------------------------+-------------------------+---------+

| 6feBTywcmJUriqqnGc1zSJ | ~EpnvDMWiFSSu1YksE1Cg3n | TRUSTEE |

+------------------------+-------------------------+---------+

 

+------------------------+------------------------+-------------------------+---------+

| *Identifier *            | *Dest *                  | *Verkey*                  | *Role*    |

+------------------------+------------------------+-------------------------+---------+

| 5M3i1PbpvEQmTk25EmAY6N | K2w8H44Z4PeRHphoAcFi4n | ~Kc1rbBmDKrvudmbu3Pf7cx | TRUSTEE |

+------------------------+------------------------+-------------------------+---------+

 

+------------------------+------------------------+---------------------------------------------+---------+

| *Identifier *            | *Dest *                  | *Verkey*                                      | *Role*    |

+------------------------+------------------------+---------------------------------------------+---------+

| 5M3i1PbpvEQmTk25EmAY6N | Kv2YdE5KGgdruMGW6p5w4b | ZKk1VDyZmBQkqvfvEKXeagNrzzWEPoAFReGBxRdMtca | TRUSTEE |

+------------------------+------------------------+---------------------------------------------+---------+

 

 

Also, in case you need it, here is an excerpt from the log for a BuilderNet node for the ""original"" issue 3 txn request:

2019-05-08 17:18:13,191|INFO|message_processor.py|FoundationBuilder discarding message (\{'identifier': 'GcyGfNY7y9WuEqUGdEoAxQ', 'reqId': 3479987566, 'operation': {'outputs': [{'address': '52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu', 'amount': 1200000000000000000}], 'type': '10000'}, 'protocolVersion': 2, 'signatures': \{'6feBTywcmJUriqqnGc1zSJ': '2Tt9aMeD75EVWowLU2LGJAuFu3ZWaNdMiNAszHcZBqhyfz1pWoCskDEYL35z2uh1HCihoQ29gWY2gNnJQ5NAKnHD', '5M3i1PbpvEQmTk25EmAY6N': '4Rsn1mcuK7KMPgy5BZiPGysWyLdQeK7u7xk9a3pmvF4HH2Sid4mzB1e7hVztfSQincW4C1rLAb7wj21JcWpo4fhb', 'Kv2YdE5KGgdruMGW6p5w4b': 'paPauVLGkDsM8gXeJCuwzhpu7r2chqDCFtseuAjka5p234ZYc4isvEzgVQzm67VeZLziWcDKMRjWDARqu8noqgH', 'GcyGfNY7y9WuEqUGdEoAxQ': '3txpUMemuhhUo4iFZbM34CSiUYTUUpF3csdUcyJGLpBYdbnYrEzd37wSg9rDfJVJsHckFQfMLzR9ywJEcK8Yxhj4'}}, b'3r>%]xnD5chOs^HaDZ:CH+6S}1V/vM=LFc9)qb*s') because InsufficientCorrectSignatures(3, 4)

 

I can attach the full log if you need it.;;;","16/May/19 4:13 PM;ashcherbakov;[~lbendixsen]
As I can see from the `verkey` format (non-abbreviated, no `~`, and 32 bytes instead of 16) for `Kv2YdE5KGgdruMGW6p5w4b`, it may be rotated. 
Can you please double-check that the signature from `Kv2YdE5KGgdruMGW6p5w4b` is really correct and created by a private key matching the verkey (ZKk1VDyZmBQkqvfvEKXeagNrzzWEPoAFReGBxRdMtca) on the ledger?;;;","17/May/19 2:08 AM;lbendixsen;[~ashcherbakov]

I am able to verify that the signature came from Matt Norton.  He very likely tested key rotation on his Trustee DID, so you are accurate with that part of your assessment. At the time he did the key rotation, I remember now, there was a probable bug that occurred where the ledger noted the rotation, but his wallet didn't or vice-versa. So your assessment in that case is also accurate. However: after trying the transaction as noted above with 4 signatures (3 of them being trustees and one of those three being this likely ""bad"" signature) I had the proposed transaction signed 1 more time with a verified good signature (newly created TRUSTEE, with no key rotation).  The results are listed below.  

For clarification, there are 5 total signatures in the attempt shown below, 3 from known valid TRUSTEES with no key rotation bug possible, 1 from a trustee with a probable key rotation bug, and 1 from a non-privileged DID.  A mint transaction was later successful on the BuilderNet with just 3 TRUSTEE signatures (one of which was the additional one added to the txn shown below). The transaction shown below was attempted shortly after the original one noted in this ticket (within a few hours) as part of the original test run.

pool(buildernet):wallet(testnet_wallet):did(Gcy...AxQ):indy> ledger custom \{""identifier"":""GcyGfNY7y9WuEqUGdEoAxQ"",""operation"":{""outputs"":[{""address"":""52CuALbWKBX66sDnmf8zL5HvxFYyjzFNuaibERRNhPgKP1bBu"",""amount000""},""protocolVersion"":2,""reqId"":3479987566,""signatures"":\{""5M3i1PbpvEQmTk25EmAY6N"":""4Rsn1mcuK7KMPgy5BZiPGysWyLdQeK7u7xk9a3pmvF4HH2Sid4mzB1e7hVztfSQincW4C1rLAb7wj21JcWpo4fhb"",""6feBTywcmJUriqqnGc1zSJ"":""2Tt9aMeqhyfz1pWoCskDEYL35z2uh1HCihoQ29gWY2gNnJQ5NAKnHD"",""GcyGfNY7y9WuEqUGdEoAxQ"":""3txpUMemuhhUo4iFZbM34CSiUYTUUpF3csdUcyJGLpBYdbnYrEzd37wSg9rDfJVJsHckFQfMLzR9ywJEcK8Yxhj4"",""K2w8H44Z4PeRHphoAcFi4n"":""4gV4aSz5X5aBoJpCPhi5R5PUQEQqk7ddYA45m16gLThQjUQivrEn4A"",""Kv2YdE5KGgdruMGW6p5w4b"":""paPauVLGkDsM8gXeJCuwzhpu7r2chqDCFtseuAjka5p234ZYc4isvEzgVQzm67VeZLziWcDKMRjWDARqu8noqgH""}}

*Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(4, 5)*

 ;;;","17/May/19 7:47 PM;ashcherbakov;[~lbendixsen]
Thanks for update and checking this. 
The current Indy-Node behavior is that all signatures passed must be valid regardless of the required number of signatures. 

So, even if we need just 3 signatures, but signed the transaction with 4, and 1 of them is invalid, the txn will be rejected.

If you think this is an issue - feel free to create a bug for this.

As for the current task - I think we can close it, can't we?;;;","17/May/19 11:39 PM;lbendixsen;Yes, thanks for the explanations and the help with understanding what was going on.;;;",,,,,,,,
"As a Network Admin, I need to have a validation against invalid settings of AUTH_RULES, so that the Network can not be moved to a state when no txns are accepted",INDY-2086,39791,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,14/May/19 2:02 AM,24/Oct/19 5:01 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Example of validation that can be added to AUTH_RULE txn:
 * Check that the required number of TRUSTEES set is not more than the number of Trustees signed the AUTH_RULE txn.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c98w",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a Trustee(s), I need to have a way to set multiple AUTH_RULES by one command",INDY-2087,39821,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,15/May/19 6:59 PM,22/Jun/19 5:26 AM,28/Oct/23 2:47 AM,07/Jun/19 10:19 PM,,,1.9.0,,,,,0,,,,,"As a Trustee(s), I need to have a way to set multiple AUTH_RULES by one command, so that it's easier for adding/changing auth rules and requires less communication with the Ledger.

*Acceptance criteria*
 * Define the command format to add/edit multiple auth rules (from 1 to all)
 * The auth rules override previous ones
 * Cover by tests",,,,,,,,,,,,,,IS-1265,,,,,,INDY-2063,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjgi",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,Toktar,,,,,,,,,"21/May/19 5:11 PM;Toktar;*PoA:*

 A command to set multiple AUTH_RULEs by one transaction. 
 Transaction AUTH_RULES is not divided into a few AUTH_RULE transactions, and is written to the ledger with one transaction with the full set of rules that come in the request.
 If one rule fails validation, the request with all rules will be rejected.

A request format for the AUTH_RULES transaction:
{code:java}
class AuthRuleField(MessageValidator):
    schema = (
        (CONSTRAINT, ConstraintField(ConstraintEntityField(),
                                     ConstraintListField())),
        (AUTH_ACTION, ChooseField(values=(ADD_PREFIX, EDIT_PREFIX))),
        (AUTH_TYPE, LimitedLengthStringField(max_length=NAME_FIELD_LIMIT)),
        (FIELD, LimitedLengthStringField(max_length=NAME_FIELD_LIMIT)),
        (OLD_VALUE, AuthRuleValueField(optional=True)),
        (NEW_VALUE, AuthRuleValueField())
    )
    
    
class ClientAuthRulesOperation(MessageValidator):
    schema = (
        (TXN_TYPE, ConstantField(AUTH_RULES)),
        (RULES, IterableField(AuthRuleField()))
    )
{code}
Request Example:

 

 
{code:java}
{
    'operation': {
           'type':'122',
           'rules': [
                {'constraint':{  
                     'constraint_id': 'OR',
                     'auth_constraints': [{'constraint_id': 'ROLE', 
                                           'role': '0',
                                           'sig_count': 1, 
                                           'need_to_be_owner': False, 
                                           'metadata': {}}, 
                                                               
                                           {'constraint_id': 'ROLE', 
                                            'role': '2',
                                            'sig_count': 1, 
                                            'need_to_be_owner': True, 
                                            'metadata': {}}
                                           ]
                   }, 
                 'field' :'services',
                 'auth_type': '0', 
                 'auth_action': 'EDIT',
                 'old_value': '[VALIDATOR]',
                 'new_value': '[]'
                },
                ...
           ]
    },
    
    'identifier': '21BPzYYrFzbuECcBV3M1FH',
    'reqId': 1514304094738044,
    'protocolVersion': 1,
    'signature': '3YVzDtSxxnowVwAXZmxCG2fz1A38j1qLrwKmGEG653GZw7KJRBX57Stc1oxQZqqu9mCqFLa7aBzt4MKXk4MeunVj'
}
{code}
Reply Example:
{code:java}
{     'op':'REPLY',
      'result':{  
         'txnMetadata':{  
            'seqNo':1,
            'txnTime':1551776783
         },
         'reqSignature':{  
            'values':[  
               {  
                  'value':'4j99V2BNRX1dn2QhnR8L9C3W9XQt1W3ScD1pyYaqD1NUnDVhbFGS3cw8dHRe5uVk8W7DoFtHb81ekMs9t9e76Fg',
                  'from':'M9BJDuS24bqbJNvBRsoGg3'
               }
            ],
            'type':'ED25519'
         },
         'txn':{  
            'type':'122',
            'data':{
               'rules': [
                    {'constraint':{  
                         'constraint_id': 'OR',
                         'auth_constraints': [{'constraint_id': 'ROLE', 
                                               'role': '0',
                                               'sig_count': 1, 
                                               'need_to_be_owner': False, 
                                               'metadata': {}}, 
                                                                   
                                               {'constraint_id': 'ROLE', 
                                                'role': '2',
                                                'sig_count': 1, 
                                                'need_to_be_owner': True, 
                                                'metadata': {}}
                                               ]
                       }, 
                     'field' :'services',
                     'auth_type': '0', 
                     'auth_action': 'EDIT',
                     'old_value': '[VALIDATOR]',
                     'new_value': '[]'
                    },
                    ...
               ]
            }
            'protocolVersion':2,
            'metadata':{  
               'from':'M9BJDuS24bqbJNvBRsoGg3',
               'digest':'ea13f0a310c7f4494d2828bccbc8ff0bd8b77d0c0bfb1ed9a84104bf55ad0436',
               'reqId':711182024
            }
         },
         'ver':'1',
         'rootHash':'GJNfknLWDAb8R93cgAX3Bw6CYDo23HBhiwZnzb4fHtyi',
         'auditPath':[]
      }
   }
{code}
 

PR: [https://github.com/hyperledger/indy-node/pull/1296];;;","05/Jun/19 8:56 PM;ashcherbakov;Please create the following system tests:
 - Test1:
 ** send GET_AUTH_RULE without parameters (to get all AUTH_RULES)
 ** get the output, and use it as the input for AUTH_RURLES
 - Test2:
 ** send GET_AUTH_RULE without parameters (to get all AUTH_RULES)
 ** get the output, modify a couple of rules
 ** use the modified output as the input for AUTH_RULES
 ** make sure that modified rules are applied
 - Test3:
 ** send AUTH_RULES to modify 1 rule
 ** make sure that modified rule is applied
 - Test4:
 ** send AUTH_RULES to modify 2 rules
 ** make sure that modified rules are applied;;;","05/Jun/19 9:01 PM;Toktar;Problem reason:
 * There is no command to set many auth rules at once.

Changes:
 * Add command AUTH_RULES
 * Add tests

PR:
 * [https://github.com/hyperledger/indy-node/pull/1296]

Version:
 * indy-node 1.9.0~dev987 master

Risk factors:
 * AUTH_RULE, AUTH_RULES and GET_AUTH_RULES requests

Risk:
 * Medium

Covered with tests:
 * [test_auth_rules_transaction.py|https://github.com/hyperledger/indy-node/pull/1296/files#diff-484a99aecf8962df26d645b7a6cdc9a7] 
 * [test/auth_rule/auth_framework/edit_auth_rules.py|https://github.com/hyperledger/indy-node/pull/1296/files#diff-074a0cc7cbfc278e726b4fe7e4e3cb4d] 

Recommendations for QA:
Test integration between indy-sdk and indy-node for AUTH_RULE, AUTH_RULES and GET_AUTH_RULES requests;;;","07/Jun/19 10:21 PM;andkononykhin;Implemented new  system test with the following scenario:
 # start new docker pool
 # using trustee1 did send add steward1 nym - should pass
 # using trustee1 did send add trustee nym - should pass
 # using steward1 did send add steward nym - should be rejected
 # using steward1 did send add trustee nym - should be rejected
 # using trustee1 did and AUTH RULES txn set constraints to allow one steward to perform both nym add trustee and nym add steward operations  - should pass
 # using trustee1 did and get auth rule verfiy that authrules have been set properly
 # check that trustee1 is not allowed to add stewards and trustees any more - expect REJECTs
 # check that steward1 is allowed to do that

Run the scenario on pool using the following versions of artifacts:
 * client:
 ** libindy 1.9.0~1130 and python wrapper version 1.9.0-dev-1130
 * pool:
 ** indy-node 1.9.0~dev987
 ** indy-plenum 1.9.0~dev808

Test passed.

For now [the test|https://github.com/hyperledger/indy-test-automation/blob/ebbd775b3baa008981d10fb9f2a28f54a418fc13/system/draft/TestAuthMapSuite.py#L1065] is a part of [the pull request|https://github.com/hyperledger/indy-test-automation/pull/25] to indy-test-automation repository.;;;",,,,,,,,,,,,,,,,,,
Spike load attempt breaks the pool,INDY-2088,39849,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,16/May/19 8:05 PM,01/Jun/19 6:58 PM,28/Oct/23 2:47 AM,01/Jun/19 6:58 PM,,,1.9.0,,,,,0,,,,,"Build Info:
indy-node 1.8.0~dev928
plugins 0.9.6~24

Steps to Reproduce:
1. Run production load test with fees for 12+ hours from client 1.
2. Try to run the same load as aditional spike from client 2.

Actual Results:
Pool falls into several long VCs that go one by one and we have no write consensus because of it.

Logs:
ev@evernymr33:logs/15_05_2019_prod_fees_spike_VCs_logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj8",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"27/May/19 4:27 PM;ashcherbakov;Looks like we need to re-run the test

 ;;;","30/May/19 1:05 AM;VladimirWork;Build Info:
indy-node 1.8.0~dev981
plugins 0.9.12~38

Steps to Reproduce:
1. Run production load test with fees and spikes:
{noformat}
for s in `seq 1 100` ; do perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":10,\""addr_mint_limit\"":1000000,\""mint_by\"":250000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 10 -b 10 -l 10 --short_stat --log_lvl 50 --load_time 10800 ; sleep 1800 ; done
{noformat}


Actual Results:
There are OOM errors on several nodes and too many failed and nacked txns on writing client:
{noformat}
Clients 10/10 Sent: 35227 Succ: 24107 Failed: 1896 Nacked: 9101 Rejected: 0
{noformat}

Logs:
ev@evernymr33:logs/acceptance_prod_load_fees_spikes_29_05_2019_logs.tar.gz;;;","01/Jun/19 6:58 PM;VladimirWork;There was a much bigger load rate than expected in previous case. Retesting with 0 to 10 txns/sec spikes looks good (there are no rejects or fails during 8 hours).;;;",,,,,,,,,,,,,,,,,,,
Run Plenum's integration tests in CI when preparing an Indy-Node RC,INDY-2089,39850,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,16/May/19 8:53 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,"Since Indy-Node and Indy-Plenum are tightly coupled, we need to make sure that all integration tests from Plenum (there are a lot of them) pass in Indy-Node too, so that Indy-Node doesn't break Plenum's core functionality.

 

Possible options:
 * Run all Indy-Plenum CI/CD tests on every master build of Indy-Node

 * Run all Indy-Plenum CI/CD tests on every RC build of Indy-Node

 * Have nightly builds running all the tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-766,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c98wi",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Extend load script with TAA,INDY-2090,39884,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/May/19 4:13 PM,28/May/19 11:29 PM,28/Oct/23 2:47 AM,28/May/19 11:26 PM,,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * There needs to be am AML and TAA written at the beginning of the load
 * All requests need to be signed against TAA.
 * Make sure that TAA/AML is sent only once",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj",,,,Unset,Unset,Ev-Node 19.10,Ev-Node 19.11,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,"22/May/19 11:49 PM;sergey.khoroshavin;*Changes made*
* load script now accepts two additional parameters
** --taa_text - desired TAA text (default ""test transaction author agreement text"")
** --taa_version - desired TAA version (default ""test_taa"")
* when starting load script checks whether taa_text is empty, and if it is not:
** it writes TAA AML transaction if it is not present in ledger
** it writes TAA transaction with given text and version if it is not present in ledger
* if taa_text is empty
** it writes TAA transaction with given version and empty text if current TAA text is not empty
** if ledger already contains empty TAA text then load script doesn't care about version
* during load TAA acceptance is attached to every transaction if it taa_text is not empty

*PR*
https://github.com/hyperledger/indy-node/pull/1301;;;","28/May/19 11:26 PM;VladimirWork;Build Info:
indy-perf-load 1.1.5

Steps to Validate:
1. Run production load test with default TAA settings (no new parameters are needed).
2. Run production load test with custom TAA settings.

Actual Results:
TAA is set during both cases. TAA is added to requests.

Known Issues:
We have ace condition on first custom TAA setting so we should rerun load script once with custom TAA parameters.;;;",,,,,,,,,,,,,,,,,,,,
"Extend load script with GET_TAA, GET_AML, GET_AUTH_RULES",INDY-2091,39885,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/May/19 4:18 PM,24/May/19 8:44 PM,28/Oct/23 2:47 AM,24/May/19 8:44 PM,,,1.8.0,,,,,0,blocked,,,,"*Acceptance criteria*
 * Load script needs to be able to to send GET_AUTH_RULE
 * Load script needs to be able to to send GET_TAA
 * Load script needs to be able to to send GET_AML",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj1",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,VladimirWork,,,,,,,,,"23/May/19 1:27 AM;Derashe;Problem reason/description:
 - Load script needs to be extended with additional txns

Changes:
 - load script extended

PR:
 - [https://github.com/hyperledger/indy-node/pull/1304]

Version:
 - indy-node 945

Risk factors:
 - no

Risk:
 - Low

Covered with tests:
 - Manually tested with docker pool

Recommendations for QA
 - test load script, by sending new get txns;;;","24/May/19 8:44 PM;VladimirWork;Build Info:
indy-perf-load 1.1.4

Actual Results:
Load script reads GET_TAA, GET_AML, GET_AUTH_RULE without fails\nacks\rejects.;;;",,,,,,,,,,,,,,,,,,,,
Extend load script to have all Auth Rules in config ledger,INDY-2092,39886,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/May/19 4:21 PM,24/May/19 8:17 PM,28/Oct/23 2:47 AM,24/May/19 8:17 PM,,,1.8.0,,,,,0,,,,,"*Acceptance criteria*
 * All Auth Rules needs to be overridden (by default constraints) so that they are put into config ledger. It means that an AUTH_RULE txn needs to be sent for every action during start-up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj2",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,VladimirWork,,,,,,,,,"24/May/19 7:57 PM;andkononykhin;Problem reason:

- load script hasn't utilized get/set auth rule txns necessary to set up custom auth rules constraints with specific metadata

Changes:

- added base (just set the same rules as received from get so they are written to ledger and state) logic of get/set auth rules during init phase of load client

PR:

- https://github.com/hyperledger/indy-node/pull/1303

Version:

- Indy Node 1.8.0.dev947

Risk factors:

- no

Risk:

- Low

Covered with tests:

- tested manually

Recommendations for QA

- start load script and ensure that auth rules are written to config ledger (e.g. check logs);;;","24/May/19 8:17 PM;VladimirWork;Build Info:
indy-perf-load 1.1.4

Actual Results:
Load script writes auth rules in config ledger as expected.;;;",,,,,,,,,,,,,,,,,,,,
Multi-signature support,INDY-2093,39887,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,17/May/19 4:36 PM,09/Oct/19 7:03 PM,28/Oct/23 2:47 AM,09/Oct/19 7:02 PM,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-10,,Multi-signature support,Done,No,,Unset,No,,,"1|i00o3r:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support state proofs for GET_AUTH_RULE without parameters,INDY-2094,39888,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,17/May/19 4:42 PM,24/Oct/19 5:00 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2266,,,No,,Unset,No,,,"1|hzwx4f:2rzmh",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indy-Node 1.9.0 Release,INDY-2095,39889,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/May/19 4:55 PM,08/Jul/19 3:35 PM,28/Oct/23 2:47 AM,08/Jul/19 3:35 PM,,,1.9.0,,,,,0,,,,,,,,,,,,,,,INDY-2163,,,,,,,INDY-2162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969i",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,"27/Jun/19 9:31 PM;andkononykhin;Plenum. Rc #1. https://github.com/hyperledger/indy-plenum/pull/1252;;;","28/Jun/19 1:35 AM;andkononykhin;indy-plenum 1.9.0 has been release;;;","28/Jun/19 1:35 AM;andkononykhin;Node. RC #1. https://github.com/hyperledger/indy-node/pull/1361;;;",,,,,,,,,,,,,,,,,,,
libindy needs to be used for AUTH_RULE and GET_AUTH_RULE in Indy-Node's tests,INDY-2096,39891,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,17/May/19 5:40 PM,28/May/19 12:19 AM,28/Oct/23 2:47 AM,28/May/19 12:19 AM,,,1.8.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj24",,,,Unset,Unset,Ev-Node 19.10,Ev-Node 19.11,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,"28/May/19 12:18 AM;andkononykhin;Problem reason: 
- Indy Node tests didn't use libindy API to build AITH_RULE and GET_AUTH_RULE requests that made integration tests incomplete

Changes: 
- Migrated tests to libindy API builders.

PR:
- https://github.com/hyperledger/indy-node/pull/1305
- https://github.com/hyperledger/indy-node/pull/1314

Version:
-  Indy Node v.1.8.0.dev975

Risk factors:
- no

Covered with tests:
- Tests were the objects of the change. No changes for core code.

Recommendations for QA
- QA acceptacne is not expected;;;",,,,,,,,,,,,,,,,,,,,,
Update Pluggable Req Handlers ,INDY-2097,39892,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,17/May/19 5:49 PM,07/Jun/19 10:17 PM,28/Oct/23 2:47 AM,07/Jun/19 10:17 PM,,,1.9.0,,,,,0,,,,,"* Integrate all recent commands into pluggable request handlers
 ** TAA
 ** AML
 ** AUTH_RULES",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjhv",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"30/May/19 10:32 PM;Toktar;*PoA:*
 * add TxnAuthorAgreementHandler
 * add TxnAuthorAgreementAmlHandler
 * add GetTxnAuthorAgreementAmlHandler
 * add GetTxnAuthorAgreementHandler
 * add AuthRulesHandler
 * update existing handlers
 * add tests

*PRs:*
https://github.com/hyperledger/indy-plenum/pull/1221
https://github.com/hyperledger/indy-node/pull/1329;;;","07/Jun/19 10:17 PM;Toktar;Problem reason:
 * Current handlers need to be updated before integration.

Changes:
 * add TxnAuthorAgreementHandler
 * add TxnAuthorAgreementAmlHandler
 * add GetTxnAuthorAgreementAmlHandler
 * add GetTxnAuthorAgreementHandler
 * add AuthRulesHandler
 * refactoring for existing handlers
 * add tests

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1221]
[https://github.com/hyperledger/indy-node/pull/1329]
 * [https://github.com/hyperledger/indy-plenum/pull/1234]
 * [https://github.com/hyperledger/indy-node/pull/1339]

Version:
 * indy-node 1.9.0~dev991 master

Risk factors:
 * Request Handlers

Risk:
 * Medium

Recommendations for QA:
The task will be tested in a scope of INDY-2108 and INDY-1861;;;",,,,,,,,,,,,,,,,,,,,
Add missing tests for state proof,INDY-2098,39893,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,17/May/19 6:07 PM,07/Jun/19 7:34 PM,28/Oct/23 2:47 AM,07/Jun/19 7:34 PM,,,1.9.0,,,,,0,,,,,"* Add tests to check State Proof for Revocation txns
 * Add tests to check State Proof for Auth Rule txns",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjhc",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,"30/May/19 8:06 PM;Derashe;Problem reason/description: 
- Need more tests for revoc_txns txns

Changes: 
- tests added using sdk builders

PR:
- [https://github.com/hyperledger/indy-node/pull/1327]

Version:
- will be marked later

 ;;;",,,,,,,,,,,,,,,,,,,,,
Load script can't mint 8000000000*100000 sovatoms,INDY-2099,39894,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,17/May/19 6:35 PM,24/May/19 6:52 PM,28/Oct/23 2:47 AM,24/May/19 6:51 PM,,,1.8.0,,,,,0,,,,,"Steps to Reproduce:
1. Get the latest load script:
{code:java}
sudo pip3 install -e 'git+https://github.com/hyperledger/indy-node.git@master#egg=subdir&subdirectory=scripts/performance'
{code}
2. Try to run this:
{code:java}
perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1,\""addr_mint_limit\"":800000000000000,\""mint_by\"":800000000000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k payment -c 1 -b 1 -l 10
{code}

Actual Results:
{noformat}
2019-05-16 14:15:45,358|INFO|libindy.py|indy.libindy.native.sovtoken.api|	src/api/mod.rs:556 | Parsed GET_UTXO response, received: ""[{\""paymentAddress\"":\""pay:sov:2geuA6zB56hCsdS3UMwZMGVPQMHdWmvmREY6bt6PPzWJReRVfp\"",\""source\"":\""txo:sov:LJJjXLVFh3iQHWqyEBHn1fddT516Gt1shinxR3NiLySQu9t7Qy4w5ibpzuKSaa3vMvScCRnARPHsE7L6WBkY5M21oWpZZeb8c3FDCC7R5Sp2qgyWyWX2euxeMH3\"",\""amount\"":800000000000000,\""extra\"":\""\""}]""
2019-05-16 14:15:45,358|INFO|libindy.py|indy.libindy.native.indy.commands|	src/commands/mod.rs:156 | PaymentsCommand command received
2019-05-16 14:15:45,359|INFO|libindy.py|indy.libindy.native.payments_command_executor|	src/commands/payments.rs:196 | ParseGetPaymentSourcesResponseAck command received
2019-05-16 14:15:45,359|INFO|perf_client_fees.py|LoadClient_0|_payment_address_init done
2019-05-16 14:15:45,359|INFO|perf_client_fees.py|LoadClient_0|_post_init done
2019-05-16 14:15:45,359|INFO|perf_client.py|LoadClient_0|call _req_generator.on_pool_create
2019-05-16 14:15:45,359|INFO|perf_client_fees.py|LoadClient_0|_on_pool_create_ext_params done {'max_cred_num': 1, 'addr_txos': {'pay:sov:2geuA6zB56hCsdS3UMwZMGVPQMHdWmvmREY6bt6PPzWJReRVfp': [('txo:sov:LJJjXLVFh3iQHWqyEBHn1fddT516Gt1shinxR3NiLySQu9t7Qy4w5ibpzuKSaa3vMvScCRnARPHsE7L6WBkY5M21oWpZZeb8c3FDCC7R5Sp2qgyWyWX2euxeMH3', 800000000000000)]}, 'pool_fees': {}, 'payment_method': 'sov'}
2019-05-16 14:15:45,359|INFO|perf_client.py|LoadClient_0|call pregen_reqs
2019-05-16 14:15:45,359|ERROR|perf_client.py|LoadClient_0|generate req error Cannot choose from an empty sequence
Traceback (most recent call last):
  File ""/usr/lib/python3.5/random.py"", line 253, in choice
    i = self._randbelow(len(seq))
  File ""/usr/lib/python3.5/random.py"", line 230, in _randbelow
    r = getrandbits(k)          # 0 <= r < 2**k
ValueError: number of bits must be greater than zero

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_client.py"", line 183, in gen_signed_req
    req_data, req = await self._req_generator.generate_request(self._test_did)
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_req_gen.py"", line 73, in generate_request
    req_data = self._gen_req_data()
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_req_gen_payment.py"", line 61, in _gen_req_data
    return self._gen_input_output(1, self._payment_fees)
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_req_gen_payment.py"", line 40, in _gen_input_output
    to_address = random.choice(addrs)
  File ""/usr/lib/python3.5/random.py"", line 255, in choice
    raise IndexError('Cannot choose from an empty sequence')
IndexError: Cannot choose from an empty sequence
2019-05-16 14:15:45,360|INFO|perf_client.py|LoadClient_0|stopped
2019-05-16 14:15:45,361|INFO|perf_processes.py|__main__|load_run stopping...
2019-05-16 14:15:45,361|INFO|perf_processes.py|__main__|load_run stopped
2019-05-16 14:15:45,366|ERROR|base_events.py|asyncio|Task exception was never retrieved
future: <Task finished coro=<LoadClient.run_test() done, defined at /home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_client.py:117> exception=IndexError('Cannot choose from an empty sequence',)>
Traceback (most recent call last):
  File ""/usr/lib/python3.5/random.py"", line 253, in choice
    i = self._randbelow(len(seq))
  File ""/usr/lib/python3.5/random.py"", line 230, in _randbelow
    r = getrandbits(k)          # 0 <= r < 2**k
ValueError: number of bits must be greater than zero

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
    result = coro.send(None)
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_client.py"", line 138, in run_test
    await self.pregen_reqs()
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_client.py"", line 220, in pregen_reqs
    await self.gen_signed_req()
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_client.py"", line 190, in gen_signed_req
    raise e
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_client.py"", line 183, in gen_signed_req
    req_data, req = await self._req_generator.generate_request(self._test_did)
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_req_gen.py"", line 73, in generate_request
    req_data = self._gen_req_data()
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_req_gen_payment.py"", line 61, in _gen_req_data
    return self._gen_input_output(1, self._payment_fees)
  File ""/home/ubuntu/.local/lib/python3.5/site-packages/perf_load/perf_req_gen_payment.py"", line 40, in _gen_input_output
    to_address = random.choice(addrs)
  File ""/usr/lib/python3.5/random.py"", line 255, in choice
    raise IndexError('Cannot choose from an empty sequence')
IndexError: Cannot choose from an empty sequence
{noformat}

Expected Results:
Script should be able to mint 8000000000*100000 sovatoms and run the payment load after that.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609biw",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,VladimirWork,,,,,,,,,,"22/May/19 6:54 PM;sergey.khoroshavin;*Problem reason*
Current implementation of generating payment load requires at least 2 payment addresses, however in this case there was an attempt to run load with just one payment address.

*Changes made*
New extension parameter was added, called mint_addrs_count, with default equal to current payment_addrs_count. This parameter allows to restrict number of payment address count which participate in initial minting. For example, load with default number of payment addresses (which is 100) and minting 8000000000 tokens only to first address can be run with
{code}
perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""mint_addrs_count\"":1,\""addr_mint_limit\"":800000000000000,\""mint_by\"":800000000000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k payment -c 1 -b 1 -l 10
{code}
Of course both payment_addrs_count and mint_addrs_count can be used at the same time, for example load with 500 payment addresses and minting 8000000000 tokens in total to first 3 of them can be run with:
{code}
perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":500,\""mint_addrs_count\"":3,\""addr_mint_limit\"":266666666666666,\""mint_by\"":266666666666666,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k payment -c 1 -b 1 -l 10
{code}

*PR*
https://github.com/hyperledger/indy-node/pull/1299

*Version*
indy-node 1.8.0.dev940;;;","24/May/19 6:51 PM;VladimirWork;Build Info:
indy-node 1.8.0~dev943
indy-perf-load 1.1.4

Steps to Validate:
1. Run load test with production minting using new `mint_addrs_count` parameter.

Actual Results:
Load script works as expected.;;;",,,,,,,,,,,,,,,,,,,,
Vanity did's are not discouraged in software.,INDY-2100,39901,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid: Works as Expected,,burdettadam,burdettadam,18/May/19 1:10 AM,06/Aug/19 11:03 PM,28/Oct/23 2:47 AM,06/Aug/19 11:02 PM,,,,,,,,0,,,,,"*Environment*:
 BuilderNet (indy-node 1.7.1)
 get validator info partial results=""Installed_packages"":[""semver 2.7.9"",""psutil 5.4.3"",""portalocker 0.5.7"",""Pympler 0.5"",""intervaltree 2.1.0"",""python-dateutil 2.6.1"",""timeout-decorator 0.4.0"",""setuptools 38.5.2"",""indy-plenum 1.7.1"",""indy-crypto 0.4.5-23"",""ioflo 1.5.4"",""sortedcontainers 1.5.7"",""python-rocksdb 0.6.9"",""jsonpickle 0.9.6"",""six 1.11.0"",""libnacl 1.6.1"",""orderedset 2.0"",""sha3 0.2.1"",""Charm-Crypto 0.0.0"",""rlp 0.5.1"",""Pygments 2.2.0"",""sovrin 1.1.41"",""indy-anoncreds 1.0.11"",""indy-node 1.7.1"",""distro 1.3.0"",""pyzmq 17.0.0"",""packaging 19.0"",""sovtoken 0.9.11"",""base58 1.0.0"",""sovtokenfees 0.9.11""],""OS_version"":""Linux-4.4.0-1075-aws-x86_64-with-Ubuntu-16.04-xenial"",""indy-node"":""1.7.1"",""sovrin"":""1.1.41""}

*Steps to Reproduce*: 
 open indy-cli, connect to BuilderNet, open Wallet.
 In the indy-cli create a new did and provide a vanity did (any valid base58, 21 character string).
 for example: *did new did=AdamBurdett1111111111*
 next, in the indy-cli use a did in your wallet with permission to anchor nyms to a ledger, I used a TRUSTEE permission did.
 for example: *did use YLCJ5wri6K8asJnHdJXzDP*
 then, anchor the vanity did to the ledger with a nym transaction.
 for example: *ledger nym did=AdamBurdett1111111111 verkey=GAotUN1AdY7nfuSTSMh9vx7fF4oVPVRSoBEtW7RiQh1c role=TRUSTEE*

*Actual Results*:
 Nym request has been sent to Ledger. Metadata:
|From|Sequence Number|Request ID|Transaction time|
|YLCJ5wri6K8asJnHdJXzDP|50|1557437826392951500|2019-05-09 21:37:09|

Data:
|Did|Verkey|Role|
|AdamBurdett1111111111|GAotUN1AdY7nfuSTSMh9vx7fF4oVPVRSoBEtW7RiQh1c|TRUSTEE|

(Nym transaction was successful)

*Expected Results*:
 (Nym transaction was unsuccessful).

*Additional Information*:
 Vanity dids could be discouraged by having the ledger check if the did is part of the verkey. This would not allow vanity dids to be used without a computational expensive discovery process. This will come at a cost of not being able to anchor dids that have had their keys rotated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v986140i",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,esplinr,,,,,,,,,,"22/May/19 10:28 PM;esplinr;We have some concerns with this requirement:
 * By default, the DID is not explicitly passed and LibIndy generates the DID from the first 16 bytes of the verkey. If the user has explicitly passed a custom DID, we are nervous about restricting that. This appears to be a Sovrin specific requirement that will reduce the flexibility of the system and could break other users of Indy.
 * We have previous requirements to allow rotation of the verkey (INDY-290, INDY-827). Though we could do the check just on new DIDs which would prevent vanity DIDs and allow the verkey to be subsequently rotated.
 * Having a strong assumption that the DID matches the beginning of the verkey could allow some correlation. See https://allprivatekeys.com/vanity-address

We agree that best practice is to not have ""vanity DIDs"" on the ledger, but we consider this a documentation requirement (at least in the short term). We should get more feedback from the user community before deciding on this change.;;;","06/Aug/19 11:03 PM;esplinr;Closing this issue as ""Works as Expected"". If people feel strongly about this, we can have another conversation.;;;",,,,,,,,,,,,,,,,,,,,
raw attrib data allows arbitrary data to be anchored to a nym. ,INDY-2101,39903,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,kenebert,burdettadam,burdettadam,18/May/19 2:01 AM,09/Aug/19 6:36 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Environment*:
 BuilderNet (indy-node 1.7.1)
 get validator info partial results=""Installed_packages"":[""semver 2.7.9"",""psutil 5.4.3"",""portalocker 0.5.7"",""Pympler 0.5"",""intervaltree 2.1.0"",""python-dateutil 2.6.1"",""timeout-decorator 0.4.0"",""setuptools 38.5.2"",""indy-plenum 1.7.1"",""indy-crypto 0.4.5-23"",""ioflo 1.5.4"",""sortedcontainers 1.5.7"",""python-rocksdb 0.6.9"",""jsonpickle 0.9.6"",""six 1.11.0"",""libnacl 1.6.1"",""orderedset 2.0"",""sha3 0.2.1"",""Charm-Crypto 0.0.0"",""rlp 0.5.1"",""Pygments 2.2.0"",""sovrin 1.1.41"",""indy-anoncreds 1.0.11"",""indy-node 1.7.1"",""distro 1.3.0"",""pyzmq 17.0.0"",""packaging 19.0"",""sovtoken 0.9.11"",""base58 1.0.0"",""sovtokenfees 0.9.11""],""OS_version"":""Linux-4.4.0-1075-aws-x86_64-with-Ubuntu-16.04-xenial"",""indy-node"":""1.7.1"",""sovrin"":""1.1.41""}

*Steps to Reproduce*: 
 open indy-cli, connect to BuilderNet, open Wallet.
 in indy-cli use a did that is anchored on the ledger.
 for example: *did use AdamBurdett1111111111*
 next, prepare some arbitrary data to be anchored as attrib to a nym, I used a random github users icon encoded as base64. using the indy-cli, anchor that attrib to the nym.
 for example: *ledger attrib did=AdamBurdett1111111111 raw*=

{<img>"":""data:image/png;base64,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""}

*Actual Results*:
 Attrib request has been sent to Ledger.

Metadata:
|From|Sequence Number|Request ID|Transaction time|
|AdamBurdett1111111111|54|1557441758300181700|2019-05-09 22:42:42|

Data:
|Raw value|
|{""<img>"":""data:image/png;base64,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""}|

(Attrib transaction was successful)

*Expected Results*:
 (Attrib transaction was unsuccessful).

*Additional Information*:
 To get this attrib from BuilderNet, connect to BuilderNet ([https://raw.githubusercontent.com/sovrin-foundation/sovrin/master/sovrin/pool_transactions_builder_genesis]) in indy-cli and call get attrib.
 *ledger get-attrib did=AdamBurdett1111111111 raw=<img>*
 copy value of <img> into your browser omni bar, press enter to view the github icon.",,,,,,,,,,,,,,,,,,,,,,,,INDY-578,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwqr3:u",,,,Unset,Unset,CommunityContribution,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,esplinr,,,,,,,,,,"24/May/19 5:52 AM;esplinr;We are monitoring the progress of verifiable credentials with rich data types as it moves through W3C standardization. I expect this concern will be addressed as part of that work.;;;",,,,,,,,,,,,,,,,,,,,,
"As a Network Admin, I would like to use GET_AUTH_RULE output as an input for AUTH_RULE",INDY-2102,39972,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,21/May/19 6:01 PM,24/May/19 9:57 PM,28/Oct/23 2:47 AM,24/May/19 9:57 PM,,,1.8.0,,,,,0,,,,,"* As a Network Admin, I would like to use GET_AUTH_RULE output as an input for AUTH_RULE, so that I don't need to do additional re-formatting.
 * I need to use it in 2 cases:
 ** GET_AUTH_RULE(specific rule) -> AUTH_RULE (set specific rule)
 ** GET_AUTH_RULE(all rules) -> AUTH_RULES (set all rules)\

*Acceptance criteria*
 * Modify GET_AUTH_RULE output to match AUTH_RULE input
 * Fix tests
 * Add tests to check that GET_AUTH_RULE outputs can be used as AUTH_RULE input for 2 cases above",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609biz",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"22/May/19 6:30 PM;Toktar;Problem reason:
 * The last format was inconvenient for use in AUTH_RULE requests.

Changes:
 * Format changed. Now it is enough to add A to the answer to set a new rule.
Example of the new format:
{code:java}
{  
   'result':{  
      'auth_action':'ADD',
      'auth_type':'1',
      'type':'121',
      'new_value':'101',
      'identifier':'M9BJDuS24bqbJNvBRsoGg3',
      'reqId':925809892,
      'data':[  
         {  
            'constraint':{  
               'constraint_id':'OR',
               'auth_constraints':[  
                  {  
                     'constraint_id':'ROLE',
                     'role':'2',
                     'sig_count':1,
                     'metadata':{                       },
                     'need_to_be_owner':False
                  },
                  {  
                     'constraint_id':'ROLE',
                     'role':'0',
                     'sig_count':1,
                     'metadata':{                       },
                     'need_to_be_owner':False
                  }
               ]
            },
            'auth_action':'ADD',
            'auth_type':'1',
            'field':'role',
            'new_value':'101'
         }
      ],
      'field':'role'
   },
   'op':'REPLY'
}{code}

PR:
 * [|https://github.com/sovrin-foundation/token-plugin/pull/232] [https://github.com/hyperledger/indy-node/pull/1295]

Version:
 * indy-node 1.7.0~dev938 master

Risk factors:
 * GET_AUTH_RULE requests

Risk:
 * Low

Covered with tests:
 * [test_get_auth_rule.py|https://github.com/hyperledger/indy-node/pull/1295/files#diff-f03e515138eaf1a7305699c31b0b06e7] 

Recommendations for QA:
Test integration between indy-sdk and indy-node for GET_AUTH_RULE requests;;;","24/May/19 9:56 PM;VladimirWork;It is partially tested by load script 1.1.4 running but additional system test will be added in scope of ST-522.;;;",,,,,,,,,,,,,,,,,,,,
Need to improve error message with invalid signature,INDY-2103,39981,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,21/May/19 10:52 PM,24/Sep/19 6:59 PM,28/Oct/23 2:47 AM,10/Sep/19 5:59 PM,,,1.10.0,,,,,0,EV-CS,TShirt_M,,,"*Steps to Reproduce*
* Create a multisignature transaction (for example: minting)
* Sign it with the number of trustees required by the auth_rule for that transaction
* Sign it with an additional invalid DID signature

*Observed Behavior*
The error message returned is:

*Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(3, 4)*

This is not specific enough. It doesn't help me understand that one of the signatures is invalid.

*Error message should say*
Transaction has been rejected: client request is invalid. It requires X valid signatures and has Y valid signatures and Z invalid signatures. The invalid signatures are [list each invalid signature on a different line.]

The totals of signatures should be displayed even if the values are zero.
If there are no invalid signatures, exclude the lines listing the invalid signatures.",,,,,,,,,,,,,,,,,,,,,INDY-2085,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98632p9",,,,Unset,Unset,Ev-Node 19.17,Ev-Node 19.18,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),donqui,esplinr,VladimirWork,,,,,,,,,"02/Sep/19 4:49 PM;donqui;*POA*
 * Change the InsufficientCorrectSignatures so that it carries the info about which signatures weren't valid
 * Change the exception formatting message so that it uses *reason* attribute of an exception if it exists so that the log contains a exception description instead of exception class name
 * Write a unit test that will validate that the formatting of the exception works
 * Write an integration tests that validates that the error propagated to the client is the one we expect;;;","04/Sep/19 7:55 PM;donqui;*Problem reason/description:*
 - Error string was not clear enought

*Changes:*
 - Change the error message for a case when not enough valid signatures are provided for a multisig transaction
 - Previous:
{code:java}
client request invalid: InsufficientCorrectSignatures(3, 4){code}

 * New:
{code:java}
client request invalid: insufficient number of valid signatures, 2 is required but 1 valid and 1 invalid have been provided. The following signatures are invalid: did=DIDX, signature=SIGX{code}

*PRs:*
 - [https://github.com/hyperledger/indy-plenum/pull/1318]
 - [https://github.com/hyperledger/indy-plenum/pull/1320]

*Version:*
 - plenum: 1.10.0.dev884

 - node: 1.10.0.dev1076
 - sovtoken: sovtoken_1.0.3~dev85 sovtokenfees_1.0.3~dev85 

*Risk:*
 - Low

*Covered with tests:*
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/util/test_common_util.py]
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/signing/test_signing.py]

*Recommendations for QA*
 - send a request that does not have enough valid signatures to be allowed
 - this might also change the way how CLI is showing this message so CLI should be checked;;;","10/Sep/19 5:59 PM;VladimirWork;Build Info:
1.10.0~dev1079
indy-cli 1.11.1~1314

Steps to Validate:
1. https://github.com/VladimirWork/indy-test-automation/blob/108523eeee4ede11b7d0cff19ce4b8fd51b5a87a/system/draft/test_misc.py#L1953
2. Send nyms using did with invalid verkey in wallet.

Actual Results:
New error message appears as expected using both SDK and CLI.;;;",,,,,,,,,,,,,,,,,,,
DevOps CI / CD training,INDY-2104,39987,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,esplinr,esplinr,21/May/19 11:19 PM,21/Jun/19 10:23 PM,28/Oct/23 2:47 AM,21/Jun/19 10:13 PM,,,1.9.0,,,,,0,,,,,"The Sovrin Foundation is accepting responsibility for Indy CI / CD. To support this effort, the Evernym team needs to :
* Ensure that a new member of the team understands CI / CD sufficient to replace Andrey.
* Ensure that new members of the team have sufficient permissions to assist with CI / CD in emergency situations.
* Learn GitLab sufficient to support the work of the Sovrin Foundation on CI / CD items.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609n",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,esplinr,,,,,,,,,,"15/Jun/19 3:07 PM;andkononykhin;PoA:
 # ask Sovrin Foundation, Hyperledger for necessary credentials and access writes
 # prepare docs (jenkins libraries, repo.sovrin.org)
 # make overviews (AWS services, jenkins, repo.sovrin.org);;;","21/Jun/19 10:23 PM;andkononykhin;Problem reason:

- need to share DevOps experience with teammates to learn them how to fix issues in DevOps infrastructure and CI/CD pipelines

Changes:
 * adds docs for jenkins shared libraries:
 ** [https://github.com/sovrin-foundation/jenkins-shared/pull/14]
 ** [https://github.com/sovrin-foundation/aws-codebuild-pipeline-plugin/pull/4]
 * asked Evernym's approval to share a PyPI crednetials
 * created task in linuxfoundation helkpdesk to add more members to administrate DockerHub Indy repo:
 ** [https://jira.linuxfoundation.org/servicedesk/customer/portal/2/IT-16538]
 * provide some how-tos and made set of trainings;;;",,,,,,,,,,,,,,,,,,,,
Indy node pipelines intermittently fails during deb building,INDY-2105,40103,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,23/May/19 11:41 PM,24/May/19 8:01 PM,28/Oct/23 2:47 AM,24/May/19 4:18 PM,,,,,,,,0,devops,,,,"The issue happened when fpm tried to build deb and used `setuptools` for some its routine.

Seems related issues:
 * [https://github.com/pypa/setuptools/issues/196]
 * [https://github.com/pytest-dev/pytest-runner/issues/11]

They provide some info and workarounds since the bug might be not fixed for sure:
 * move `pytest` to the end of the list in `tests_require` (I tried for the indy-node/setup.py - not worked)
 * use version of setuptools `>36.4.0`: currently we [use|https://github.com/hyperledger/indy-node/blob/385fbba18a920512a9dc00de23cf369739912c56/build-scripts/ubuntu-1604/Dockerfile#L10] setuptools from ubuntu-xenial with very old version `20.7.0`. Possible solution is to add something like `RUN pip install -U setuptools` to that dockerfile. I failed to verify this option since cd pipeline started to work without any changes.

Link to failed run and logs:

[https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-cd/detail/master/944/pipeline]
{noformat}
+ fpm --input-type python --output-type deb --architecture amd64 --verbose --python-package-name-prefix python3 --python-bin /usr/bin/python3 --exclude '*.pyc' --exclude '*.pyo' --depends at --depends iptables --depends libsodium18 --no-python-fix-dependencies --maintainer 'Hyperledger <hyperledger-indy@lists.hyperledger.org>' --before-install preinst_node --after-install postinst_node --before-remove prerm --name indy-node --version 1.8.0~dev944 --package /output /tmp/tmp.VAL3k8sGxO{:timestamp=>""2019-05-23T12:23:41.706071+0000"", :message=>""Setting workdir"", :workdir=>""/tmp"", :level=>:info}{:timestamp=>""2019-05-23T12:23:41.928231+0000"", :message=>""fetching package metadata"", :setup_cmd=>""env PYTHONPATH=/var/lib/gems/2.3.0/gems/fpm-1.11.0/lib/fpm/package /usr/bin/python3 setup.py --command-packages=pyfpm get_metadata --output=/tmp/package-python-build-4787beb0d9ecaef836dcb48363b3e229a11a4668836fbeb7f25c9e42d1a5/metadata.json"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.552067+0000"", :message=>""/usr/lib/python3.5/distutils/dist.py:261: UserWarning: Unknown distribution option: 'use_scm_version'"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.552385+0000"", :message=>""  warnings.warn(msg)"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.552595+0000"", :message=>""warning: install_lib: 'build/lib' does not exist -- no Python modules to install"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.552773+0000"", :message=>"""", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.553869+0000"", :message=>""zip_safe flag not set; analyzing archive contents..."", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.557977+0000"", :message=>"""", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.558218+0000"", :message=>""Installed /tmp/tmp.VAL3k8sGxO/.eggs/UNKNOWN-0.0.0-py3.5.egg"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.564966+0000"", :message=>""Traceback (most recent call last):"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.565196+0000"", :message=>""  File \""setup.py\"", line 93, in <module>"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.565376+0000"", :message=>""    'tools/diagnostics/nsreplay',"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.565542+0000"", :message=>""  File \""/usr/lib/python3.5/distutils/core.py\"", line 108, in setup"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.565713+0000"", :message=>""    _setup_distribution = dist = klass(attrs)"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.565868+0000"", :message=>""  File \""/usr/lib/python3/dist-packages/setuptools/dist.py\"", line 269, in __init__"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.566018+0000"", :message=>""    self.fetch_build_eggs(attrs['setup_requires'])"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.566179+0000"", :message=>""  File \""/usr/lib/python3/dist-packages/setuptools/dist.py\"", line 313, in fetch_build_eggs"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.566463+0000"", :message=>""    replace_conflicting=True,"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.566634+0000"", :message=>""  File \""/usr/lib/python3/dist-packages/pkg_resources/__init__.py\"", line 829, in resolve"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.566801+0000"", :message=>""    raise DistributionNotFound(req, requirers)"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.566974+0000"", :message=>""pkg_resources.DistributionNotFound: The 'pytest-runner' distribution was not found and is required by the application"", :level=>:info}{:timestamp=>""2019-05-23T12:23:42.590869+0000"", :message=>""Process failed: /bin/sh failed (exit code 1). Full command was:[\""/bin/sh\"", \""-c\"", \""env PYTHONPATH=/var/lib/gems/2.3.0/gems/fpm-1.11.0/lib/fpm/package /usr/bin/python3 setup.py --command-packages=pyfpm get_metadata --output=/tmp/package-python-build-4787beb0d9ecaef836dcb48363b3e229a11a4668836fbeb7f25c9e42d1a5/metadata.json\""]"", :level=>:error}Terminatedscript returned exit code 1{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj2o",,,,Unset,Unset,Ev-Node 19.10,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,"24/May/19 12:53 AM;andkononykhin;It has appeared again and reproducible. Started to work on.;;;","24/May/19 8:01 PM;andkononykhin;Problem reason:
- Indy Node CD pipeline fails with strange error from 'setuptools' during deb build routine

Changes:
- updated dockerfiles (Node, Plenum) for deb build routine to install recent version of setuptools

PR:
- https://github.com/hyperledger/indy-node/pull/1309
- https://github.com/hyperledger/indy-plenum/pull/1217

Version:
- Indy Node 1.8.0.dev966

Risk factors:
- no

Risk:
- Low

Covered with tests:
- tested manually

Recommendations for QA
- check that there are no such errors in CD pipelines;;;",,,,,,,,,,,,,,,,,,,,
Sovrin MainNet fails to return to consensus following upgrade,INDY-2106,40112,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,24/May/19 7:58 AM,30/May/19 6:31 PM,28/Oct/23 2:47 AM,30/May/19 6:31 PM,,,,,,,,0,,,,,"For an overview of the problems encountered, refer to [https://forum.sovrin.org/t/mainnet-outage-may-20-2019/1179]

Please investigate why the network did not return to consensus following the upgrade, and required that an additional restart transaction be posted by a Trustee.

ev1 node logs are attached.","MainNet, upgrading from 1.6.83 to 1.7.1. Upgrading only indy-node.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/19 7:58 AM;mgbailey;ev1.log.427.xz;https://jira.hyperledger.org/secure/attachment/17254/ev1.log.427.xz","24/May/19 7:58 AM;mgbailey;ev1.log.428.xz;https://jira.hyperledger.org/secure/attachment/17255/ev1.log.428.xz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjhu",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,mgbailey,,,,,,,,,"27/May/19 9:04 PM;anikitinDSR;After analizing logs from ev1 was noticed, that:
 1. Upgrade procedure was not produced simultaneously. On ev1 node upgrade was completed at 16:45:25, but some PREPREPAREs was received at 16:46:29 with old \{viewNo: 3}. It means, that some of the nodes didn't start upgrade.
 2. Catchup of ledger 3 (audit ledger) wasn't completed for ev1 node, because quorum of LEDGER_STATUS wasn't achived by reason, described in previous point. At that time many of nodes doesn't have an audit ledger.
 3. After POOL_RESTART cmd audit ledger completed a cathup procedure and zero view change was finished successfully.

Resume:
 * problem is not critical, because this upgrade follows audit ledger adding (it's very big change) and it was not simultaneous.
 * INDY-2112 was created for preventing this situation in future.

[~mgbailey], can you please check, that all of nodes have synchronized time? Because for now, it looks like force-upgrade doesn't work as we expect.;;;","28/May/19 6:12 PM;ashcherbakov;[~mgbailey]
As it was a one-time issue (due to a tricky additional of a fundamental ledger and not 100% simultaneous update), I think we can close this ticket.;;;",,,,,,,,,,,,,,,,,,,,
Audit ledger was not installed on node that was upgraded manually (Sovrin MainNet),INDY-2107,40113,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,,mgbailey,mgbailey,24/May/19 8:10 AM,24/May/19 11:18 PM,28/Oct/23 2:47 AM,24/May/19 11:18 PM,,,,,,,,0,,,,,"We have a steward on MainNet that had to upgrade manually after the automated upgrade failed on his node. Now when I inspect his status using validator-info, he is not in consensus, and he does not have an audit ledger:


{code:java}
""atbsovrin"": { ""Node_info"": { ""Metrics"": { ""transaction-count"": { ""config"": 7459, ""ledger"": 37976, ""pool"": 74 } } } },
{code}

Is there a plugin or something that should have been installed for this? He did run the migrate(""1.6.83"",""1.7.1"",10) python script. Please diagnose and give mitigation suggestions.

I am attaching the full validator-info for this node, and will request the logs from the manual upgrade session.

The automated upgrade had failed due to insufficient space in /boot, and is not the object of this ticket.",MainNet upgrade from 1.6.83 to 1.7.1,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/19 8:10 AM;mgbailey;atbsovrin_validator_info;https://jira.hyperledger.org/secure/attachment/17256/atbsovrin_validator_info",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609e",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,mgbailey,,,,,,,,,,"24/May/19 6:35 PM;ashcherbakov;[~mgbailey]
Can you please ask for Node logs as well?;;;","24/May/19 11:17 PM;mgbailey;I got this node confused with another. It has not yet been upgraded. Please disregard this ticket.;;;",,,,,,,,,,,,,,,,,,,,
More tests for pluggable request handlers,INDY-2108,40121,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,24/May/19 6:36 PM,17/Jun/19 6:50 PM,28/Oct/23 2:47 AM,17/Jun/19 6:50 PM,,,1.9.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjhw",,,,Unset,Unset,Ev-Node 19.11,Ev-Node 19.12,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,Derashe,Toktar,,,,,,,,"06/Jun/19 8:10 PM;anikitinDSR;h1. PoA
h2. What we need to do

We need to add more unit/integration tests for pluggable request handlers. New handlers and common managers are independent and can be covered by unit tests as well. After integration it into indy-plenum and indy-node source code we can create a couple of integration tests.

Need to create before integration:
 * unit tests for DatabaseManager
 * unit tests for Write/Read/Action managers
 * unit tests for batch handlers

 ;;;","07/Jun/19 5:36 PM;Derashe;Progress
 * db manager - Done
 * managers - in progress
 * batch handlers - Done;;;","11/Jun/19 10:21 PM;Derashe;Problem reason/description:
 - We are lacking of tests for pluggable request handlers

Changes:
 - New tests added

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1238]
 - [https://github.com/hyperledger/indy-plenum/pull/1232]

Risk factors:
 - no

Risk:
 - Low;;;","14/Jun/19 6:49 PM;Toktar;New unit tests for pluggable request handlers:
 * Plenum: [https://github.com/hyperledger/indy-plenum/pull/1240]
 * Node: [https://github.com/hyperledger/indy-node/pull/1349];;;",,,,,,,,,,,,,,,,,,
Publish RC to pypi,INDY-2109,40131,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,24/May/19 11:32 PM,30/May/19 6:45 PM,28/Oct/23 2:47 AM,30/May/19 6:34 PM,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00p64:",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,"30/May/19 6:45 PM;andkononykhin;Problem reason:
 - Need to have PyPI artifacts for rc releases as well.

Changes:
 - Updated jenkins shared library and bumped its version in Indy Plenum and Indy Node.

PR:
 - [https://github.com/hyperledger/indy-node/pull/1315]
 - [https://github.com/hyperledger/indy-node/pull/1318]
 - [https://github.com/hyperledger/indy-plenum/pull/1218]

Version:
 - Indy Plenum 1.8.0.dev805
 - Indy Node 1.8.0.dev978

Risk factors:
 - Broken PyPI publishing

Risk:
 - Low

Covered with tests:
 - Verified manually

Recommendations for QA
 - No, it actually already works, verified on all kinds of builds (master, rc, stable)
 ** [Plenum RC|https://build.sovrin.org/blue/organizations/jenkins/indy-plenum%2Findy-plenum-cd/detail/release-1.8.0/1/pipeline]
 ** [Node|https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-cd/detail/release-1.8.0/2/pipeline];;;",,,,,,,,,,,,,,,,,,,,,
Static validation for NYM requests passes wrong combinations of did and verkey,INDY-2110,40162,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/May/19 2:47 PM,26/May/19 2:47 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,Abbreviated verkey with long (32 bytes) did satisfy current static validation which could lead to crashes / exceptions during dynamic validation.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00pc4:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Indy Node CD pipelines intermittently fails during system tests,INDY-2111,40163,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/May/19 2:52 PM,26/May/19 2:54 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Indy Node CD pipaline fails during tear down (?) phase of system tests. It  might be related with junit reports generating but there is no confidence. Need to explore and fix
{noformat}
- generated xml file: /home/indy/system_tests/system/reports/system_tests_test_ledger.py_report.master.xml -========================= 83 passed in 781.01 seconds ==========================FATAL: exception not rethrownbash: line 1:    52 Aborted                 (core dumped) pytest -l -v --junit-xml=./reports/system_tests_test_ledger.py_report.master.xml ./indy-node-tests/test_ledger.py 2>&1        53 Done                    | tee ./reports/system_tests_test_ledger.py_report.master.txtscript returned exit code 134{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00pcc:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Need to make ""reask_ledger_status"" repeatable",INDY-2112,40174,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,anikitinDSR,anikitinDSR,28/May/19 5:58 PM,22/Jul/19 8:31 PM,28/Oct/23 2:47 AM,22/Jul/19 8:31 PM,,,1.9.1,,,,,0,TShirt_S,,,,"For now, reask_ledger_status it's not repeatable procedure. It's only wait some timeout and send MESSAGE_REQUEST for LEDGER_STATUS. In case, when quorum of LEDGER_STATUS was not achived, catchup for current ledger will not be finished.

_Acceptance Criteria:_
 * make ""reask_ledger_status"" repeatable and stop it when quorum of LEDGER_STATUS is achived
 * tests covering",,,,,,,,,,,,,,,,,,,,,INDY-2083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwvif:00001yw969v94",,,,Unset,Unset,Ev-Node 19.14,Ev-Node 19.15,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,Toktar,VladimirWork,,,,,,,,,"17/Jul/19 8:42 PM;Toktar;*PoA:*
 * Change re-asking LedgerStatuses via QueueTimer to RepeatingTimer
 * Don't add scheduler stopping for a quorum of LEDGER_STATUS because after same LEDGER_STATUSes ConsProofService does a catchup finish and stops all schedulers.
In a case when LEDGER_STATUSes are richer, we ask ConsistencyProofs and stop schedulers before.
 * Change re-asking ConsistencyProofs via QueueTimer to RepeatingTimer
 * Don't add scheduler stopping with the same reasons as for LEDGER_STATUSes 
 * Add a test for LedgerStatuses:
 ** Start a catchup 
 ** Delay LedgerStatuses twice (use MockTimer)
 ** Check that the catchup finished
 * Add a test for ConsistencyProofs
 ** Start a catchup 
 ** Delay ConsistencyProof twice (use MockTimer)
 ** Check that the catchup finished;;;","18/Jul/19 9:50 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/1269;;;","18/Jul/19 11:36 PM;Toktar;*Problem reason:*
 - In a catchup LedgerStatuses re-ask once. If a node doesn't receive a response for this re-ask then catchup will not be finished.
 - In a catchup ConsistencyProofs re-ask once. If a node doesn't receive a response for this re-ask then catchup will not be finished.

*Changes:*
 - Change re-asking LedgerStatuses via QueueTimer to RepeatingTimer
 - Change re-asking ConsistencyProofs via QueueTimer to RepeatingTimer
 - Add tests

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1269]

*Version:*
 * indy-node 1.9.1031 -master
 * (indy-plenum 1.9.844 -master)

*Risk factors:*
 - If the fix is incorrect, it is possible that a catchup will not be completed.

*Risk:*
 - Low

*Test:*
 * [test_node_catchup_with_connection_problem.py|https://github.com/hyperledger/indy-plenum/pull/800/files#diff-8bc091580aee91c53dee265fc1fa708d] 

*Recommendations for QA:*
 * Start a docker pool with 4 nodes
 * Make the Node4 slow (maybe delay all messages for this node)
 * Initiate network delays for the Node4 for delay catchup messages.
 * Start a catchup for the Node4.
 * Reset network delays after 60 seconds.
 * Check that the catchup finished after a while.;;;","22/Jul/19 8:31 PM;VladimirWork;Build Info:
indy-node 1.9.1031
indy-plenum 1.9.844

Steps to Validate:
1. Start a docker pool with 4 nodes.
2. Initiate network delays for the Node4 for delay catchup messages.
3. Start a catchup for the Node4.
4. Reset network delays after 60 seconds.
5. Check that the catchup finished after a while.

Actual Results:
Pool catches up successfully.;;;",,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.8.0,INDY-2113,40205,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,30/May/19 1:13 AM,01/Jun/19 6:42 PM,28/Oct/23 2:47 AM,01/Jun/19 6:42 PM,,,,,,,,0,,,,,"*Version Information:*
indy-node 1.8.0
indy-plenum 1.8.0
sovrin 1.1.45

*Notices for Stewards:*
(!) Payment transaction can return NACK from the pool but in fact it will be eventually ordered (see more details below).
(!) There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.
(!) Pool upgrade to sovrin 1.1.32 and above should be performed simultaneously for all nodes due to txn format changes.
(!) Pool upgrade to indy-node 1.8.0 should be performed simultaneously for all nodes due to audit ledger.

*Major Changes*
- Add Transaction Author Agreement Acceptance Mechanisms and Transaction Author Agreement support
- Configurable Auth rules improvements
- Stability fixes

*Detailed Changelog*

+Fixes:+
INDY-2064 - Issues with catch up and ordering under the load
INDY-2078 - Editing of CLAIM_DEF uses auth rules for Adding a Claim Def
INDY-1709 - Faulty primary can order and write already ordered and written request

+Changes and Additions:+
INDY-2102 - As a Network Admin, I would like to use GET_AUTH_RULE output as an input for AUTH_RULE
INDY-2071 - Get Transaction Author Agreement Acceptance Mechanisms from the Config Ledger
INDY-2072 - Support Transaction Author Agreement in Write Requests
INDY-2073 - Validate transaction author agreement as part of consensus
INDY-2066 - Write Transaction Author Agreement to Config Ledger
INDY-2067 - Get Transaction Author Agreement from the config ledger
INDY-2068 - Write Transaction Author Agreement Acceptance Mechanisms to the Config Ledger
INDY-2053 - Catch-up should take into account state of other nodes when sending requests

+Known Issues:+
INDY-2122 - A client may receive NACK for a payment transfer request, but the transaction will be eventually ordered (payment transferred)
INDY-2024 - Incorrect auth constraint for node demotion",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00pl0:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CATCHUP_REQ (seeder_service.py) is ""spamming"" logs at a rate of 6/sec",INDY-2114,40208,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Cannot Reproduce,,lbendixsen,lbendixsen,30/May/19 4:22 AM,18/Jul/19 7:46 AM,28/Oct/23 2:47 AM,18/Jul/19 7:46 AM,1.7.1,,,,,,,0,TShirt_S,,,,"_Environment_: BuilderNet node FoundationBuilder was updated with the rest of the BuilderNet to indy-node version 1.7.1 on May 6. When we added a new node to the ledger today we noticed spamming

_Steps to Reproduce_: unknown trigger, but seems like the last update (1.7.1) might have something to do with this.  The BuilderNet was updated on May 6 and here is the time stamp in the logs of the first instance of the error: 2019-05-09 07:37:19,863 . (found in FoundationBuilder.log.1.xz)

_Expected Behavior_: No log spamming by seeder_service

_Observed Behavior_: Logs are full of invalid CATCHUP_REQ's.  Please see attached zip files. Right now I am getting 6 of the following, exactly the same, every second(they weren't always exactly like this one):

2019-05-29 14:45:23,800|INFO|seeder_service.py|FoundationBuilder received catchup request: CATCHUP_REQ{'ledgerId': 0, 'catchupTill': 27, 'seqNoStart': 13, 'seqNoEnd': 27} from b'm$A>b-io?3s#ui@iPqPU+pD)1DJD)JNg#$oZ%^]-'

_Notes_: Attachment Foundation1 shows earliest instance of issue.  Foundation shows latest from today.  Attaching all logs in one big zip exceeding max size, but I can attach other logs if needed.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/19 4:54 AM;lbendixsen;Auditledger.out.xz;https://jira.hyperledger.org/secure/attachment/17302/Auditledger.out.xz","30/May/19 4:19 AM;lbendixsen;Foundation.log.xz;https://jira.hyperledger.org/secure/attachment/17276/Foundation.log.xz","30/May/19 4:19 AM;lbendixsen;Foundation1.log.xz;https://jira.hyperledger.org/secure/attachment/17277/Foundation1.log.xz","05/Jun/19 5:00 AM;lbendixsen;poolledger.out;https://jira.hyperledger.org/secure/attachment/17303/poolledger.out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9w9",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,burdettadam,esplinr,lbendixsen,,,,,,,,"30/May/19 5:15 AM;esplinr;Is 'from b'm$A>b-io?3s#ui@iPqPU+pD)1DJD)JNg#$oZ%^]-' a corrupt node name?;;;","31/May/19 12:11 AM;lbendixsen;It looks like this might not be fixed for a while, so In the meantime I am interested to know a workaround for this issue.  Any suggestions?  (restart of the node didn't seem to help)

 

Thanks!;;;","04/Jun/19 10:54 PM;esplinr;The ""corrupt node name"" is probably the public key of the node. When the node handshake does not succeed, the node will not have a valid name to put into the logs and so will display the public key.

This could be caused by a node that isn't initialized correctly. But it is more likely to be triggered by a poorly behaving client that is accessing the node. (This could be related to the catch up experiments [~burdettadam] told me he was doing.)

To validate the error is not being triggered by another node in the pool, in addition to the logs we would need:
* the pool ledger,
* the audit ledger,
* the public key for each node in the pool.

We can then look to see if the public key matches the error message and when it shows up in the ledgers and logs.;;;","05/Jun/19 12:22 AM;ashcherbakov;There is a very high probability that this is client requests since this is CATCHUP_REQ for the whole Pool ledger (ledgerId=0), that is from seqNo=12 (end of genesis txns) till 27 (the current pool ledger size at that time). Only clients sends CATCHUP_REQs this way. Nodes send CATCHUP_REQ differently: they split all txns to the number of nodes (but not less than 5 per node).

So, the CATCHUP_REQ format and data points to a client, not to the other node.

[~esplinr][~lbendixsen] FYI;;;","05/Jun/19 1:08 AM;burdettadam;[~esplinr], this one wasn't me, I have not done any CATCHUP experiments yet. I am still learning about merkle trie roots used in ledger status requests, I am not to the catchup yet.;;;","05/Jun/19 5:19 AM;lbendixsen;As requested, I have attached the pool and audit ledgers of the FoundationBuilder node. 

Here are the Validator DIDs for all of the nodes:
|Node Alias|Target (dest)|
|FoundationBuilder|GVvdyd7Y6hsBEy5yDDHjqkXgH8zW34K74RsxUiUCZDCE|
| vnode1|9Aj2LjQ2fwszJRSdZqg53q5e6ayScmtpeZyPGgKDswT8|
| xsvalidatorec2irl|DXn8PUYKZZkq8gC7CZ2PqwECzUs2bpxYiA5TWgoYARa7|
| danube|52muwfE7EjTGDKxiQCYWr58D8BcrgyKVjhHgRQdaLiMw|
| -uvs_val_node-|-4XgXATccdfzbaoHTN4rUz9sYvfknQ5CQfTF6Qp5T3vyM-|
| makolab01|GnuKuvbdcY9ZU3GwvUYzEo3z5nmh1BhJ8BrrsASQM1Fi|
| ovvalidator|FCLZXHPFAbARuu1vSp26bhFaNQz9sveL1QWvo2KDZjwb|
| datum-sovrin|8t4gWhnbWCnPPkYoHki8zv17WyA8LBM3WhVVsv3ezzh8|
| uvs_val2|BrhY8YyJJnuWGYAybMTW7bse9FcBMRpJXb7hfHoKzJjJ|
| validatedid|5YwvqQySsNSPPM2RRQWGJeuiGgcCG5uD9NvQRR7ASJac|
| OgNode|5aNBs6DToRDNuXamiswdvPhvoGxoLbdEL5XTLdZrv6Xf|
| fetch-ai |2wLQxCe25iBQY2d2oEYwYA7StD3AzdiABjyZcMiRWo5n|
| Certisign|9v2Wx46nMTqFSzCUbdEvMgP8Y2SaaV7EbVcVu4mvnfZL|
| SYGNET1|462gFCdp8eS6SN3MmomACh1Q8XYVAKwYL89dRPYpKY1E|;;;","05/Jun/19 5:31 AM;lbendixsen;[~ashcherbakov] thanks for your helpful comment.  Is there a way to determine which client was spamming the ledger?  I am thinking it was probably one of the several indy-CLIs that I have open on my workstation, as I am one of the few using the BuilderNet so far.  When the buildernet comes back online, I will watch for the spamming to begin again, then try shutting down my CLI's one by one to see if I can track down the culprit.;;;","17/Jul/19 7:01 PM;ashcherbakov;[~lbendixsen]
Is the issue still valid? Can we close it?;;;","18/Jul/19 7:46 AM;esplinr;[~lbendixsen] has not seen this behavior since this was originally reported. It seemed to disappear with the upgrade to 1.8.0. He is not convinced it isn't a bug. We will close as ""Cannot Reproduce"", and will reopen it if we see it again and have more information.;;;",,,,,,,,,,,,,
Make improvements to state proof building on a node side,INDY-2115,40214,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Derashe,Derashe,30/May/19 6:04 PM,24/Oct/19 5:00 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"We need to:
 * Make node build state proofs for GET_REVOC_REG_DELTA for the period of time when node did not exist. We can take such a proofs as first bls signed state.
 * Decide together with SDK side how to handle GET_REVOC_REG_DELTA, when ""from"" timestamp points to absence of REG_ENTRY. Especially, how to build proofs for such a case.
 * Extend tests with above cases (test_state_proof_returned_for_get_revoc_reg_delta)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2266,,,No,,Unset,No,,,"1|hzwx4f:2rzmgi",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
indy-node upgrade problem - No such directory ,INDY-2116,40223,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,lbendixsen,lbendixsen,31/May/19 12:05 AM,06/Aug/19 11:01 PM,28/Oct/23 2:47 AM,,1.7.1,,,,,,,0,help-wanted,,,,"_Environment_: 1 node each in BuilderNet and StagingNet  > manually upgrading indy-node from 1.6.83 to 1.7.1

_Steps to Reproduce_:
 # Manual upgrade of nodes in BuilderNet and StagingNet is accomplished by following the steps in [How to upgrade a node manually|https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI/edit?ts=5ce41633]
 # sudo apt update
 # sudo systemctl stop indy-node
 # sudo systemctl stop indy-node-control
 # sudo apt-get install -y --allow-change-held-packages indy-node indy-plenum python3-indy-crypto libindy-crypto python3-base58 sovrin sovtoken sovtokenfees

_Expected Behavior_: No errors during upgrade.

_Observed Behavior_: The following output occurred after step 5 of ""How to upgrade a --node manually"" (also step 5 in ""steps to reproduce"" above)

..................

The following additional packages will be installed:
  python3-packaging
The following NEW packages will be installed:
  python3-packaging
The following held packages will be changed:
  indy-node indy-plenum
The following packages will be upgraded:
  indy-node indy-plenum
2 upgraded, 1 newly installed, 0 to remove and 116 not upgraded.
Need to get 1,373 kB of archives.
After this operation, 518 kB of additional disk space will be used.
Do you want to continue? [Y/n] Y
Get:1 [https://repo.sovrin.org/deb] xenial/stable amd64 python3-packaging amd64 19.0 [20.0 kB]
Get:2 [https://repo.sovrin.org/deb] xenial/stable amd64 indy-node amd64 1.7.1 [515 kB]
Get:3 [https://repo.sovrin.org/deb] xenial/stable amd64 indy-plenum amd64 1.7.1 [838 kB]
Fetched 1,373 kB in 4s (312 kB/s)    
Selecting previously unselected package python3-packaging.
(Reading database ... 217959 files and directories currently installed.)
Preparing to unpack .../python3-packaging_19.0_amd64.deb ...
Unpacking python3-packaging (19.0) ...
Preparing to unpack .../indy-node_1.7.1_amd64.deb ...
Unpacking indy-node (1.7.1) over (1.6.83) ...
Preparing to unpack .../indy-plenum_1.7.1_amd64.deb ...
Unpacking indy-plenum (1.7.1) over (1.6.58) ...
Setting up python3-packaging (19.0) ...
Setting up indy-plenum (1.7.1) ...
Setting up indy-node (1.7.1) ...
*/var/lib/dpkg/info/indy-node.postinst: line 116: /etc/supervisor/indy-node.conf: No such file or directory*

_..............._

_Notes_: I marked this as low because a workaround allowed us to upgrade the system despite the failure.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00v88:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,lbendixsen,,,,,,,,,,"06/Aug/19 11:01 PM;esplinr;/etc/supervisor is used by Docker.

This part of the script was added in PR 588
https://github.com/hyperledger/indy-node/pull/588

[~smithbk] Can you look at it?;;;",,,,,,,,,,,,,,,,,,,,,
Improve system tests according to review,INDY-2117,40227,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,31/May/19 3:34 AM,06/Nov/19 7:17 PM,28/Oct/23 2:47 AM,06/Nov/19 7:17 PM,,,,,,,,0,,,,,System tests should be improved according to https://github.com/hyperledger/indy-test-automation/pull/23 review.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vl",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"05/Nov/19 11:23 PM;VladimirWork;Done in https://github.com/hyperledger/indy-test-automation/pull/73;;;","06/Nov/19 7:17 PM;VladimirWork;Reviewed and merged.;;;",,,,,,,,,,,,,,,,,,,,
Explore and fix builds discarding for projects on Jenkins,INDY-2118,40232,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,31/May/19 1:05 PM,31/May/19 1:05 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,indy-sdk project is keeping hundreds PRs most of which have been merged. There is a setting to discard old builds but seems it doesn't work. Thus disk on the Jenkins server is not cleaned. Need to explore the issue for that project and check other jobs as well.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00pq4:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a job for scheduled debian packages cleanup on repo server,INDY-2119,40233,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,31/May/19 1:17 PM,31/May/19 1:47 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Currently we keep almost all debian artifacts for all kind of releases (master, rc, stable) from the history beginning. Need to create and automate cleanup routine.

*Acceptance criteria*

1. Which artifacts to keep

([~resplin])
{quote}Stable: Forever

RC: For 1 year

Dev: For 30 days
{quote}
2. How it should be automated: weekly running job on jenkins",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00pqc:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,"31/May/19 1:44 PM;andkononykhin;[~esplinr] [~ashcherbakov] [~sergey.minaev]

I think I can create a configurable Jenkins job (parameters for dev and rc) and we may start with some longer periods of obsolescence for debian artifacts (e.g. no cleanup for rc and a half a  year for dev). I guess it would be enough to keep necessary space on the server taking into account current release intensity.

Some additional notes:

1) as I have just checked about 80% of space is consumed by non-debian (mostly android and ios) artifacts of indy-sdk:

{{40G     android (sdk) }}
 {{2.6G    debian (node) }}
 {{25G     ios (sdk)}}
 {{639M    macos (sdk)}}
 {{100K    rpm (sdk)}}
 {{12G   debian ( sdk )}}
 {{9.4G    windows (sdk)}}

I think they should be covered by the task as well.

2) It would be nice (or better to say we must) to have a backup solution: either server or 
[AWS Backup service|https://aws.amazon.com/backup/] which seems more natural for our AWS based infrastructure.;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade pytest version in plenum,INDY-2120,40237,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,31/May/19 10:07 PM,31/May/19 10:10 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Currently used version: 3.3.1 (two years old)

Current latest: 4.5.0

There are a lot of changes and fixes that might help (markers, cli parameters to control logging, please check full list of changes [here|https://docs.pytest.org/en/latest/changelog.html]).

I guess we would need to update other related test deps: pytest-asyncio, pytest-xdist ...",,,,,,,,,,,,,,,,,INDY-2121,,,,,,,INDY-2121,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00pr8:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade pytest version in Indy Node,INDY-2121,40240,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,31/May/19 10:10 PM,31/May/19 10:10 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Currently used version: 3.3.1 (two years old)

Current latest: 4.5.0

There are a lot of changes and fixes that might help (markers, cli parameters to control logging, please check full list of changes [here|https://docs.pytest.org/en/latest/changelog.html]).

I guess we would need to update other related test deps: pytest-asyncio, pytest-xdist ...",,,,,,,,,,,,,,,,,,INDY-2120,,,INDY-2120,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00prg:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"A client may receive NACK for a payment transfer request, but the transaction will be eventually ordered (payment transferred)",INDY-2122,40241,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,31/May/19 10:45 PM,06/Jun/19 12:04 AM,28/Oct/23 2:47 AM,05/Jun/19 1:19 AM,,,1.8.1,,,,,0,,,,,"*Issue*
 * A client sends a request, and receives NACK: `Client request is discarded since view change is in progress` which means that Pool can not process the request since view change is in progress.
 * The client would think that the request is discarded and not ordered (for example, the payment wasn't transferred)
 * However, after the view change is finished, the request may be silently ordered.

*What it can lead to*
 * The client may think that the payment didn't happen, while it actually happened. 

*Expected behavior*
 # It would be great not to NACK requests during view change. However, we need to implement PBFT view change first to make it happen.
 # If a client receives NACK, then the request must never be ordered
 # Having timeout for requests received during view change may be acceptable.

*Issue cause*
 * A client sends a request X to all the nodes in the pool
 * Pool starts the view change not on all nodes
 ** View change is started on nodes A, C, D
 ** View change hasn't been started yet on node B
 * Nodes A, C, D reply with NACK => the client has f+1 equal NACKs =>client reports NACK
 * Node B accepts the request X and propagates it to all the other nodes
 * Propagate is not disabled during view change, so that the request X will be propagated, put into requests queue and finalized on all the nodes
 * View change is finished, and a new primary is selected
 * The new primary gets requests from the request queue to create a new 3PC batch (PrePrepare)
 * It gets the the request X and put into 3PC batch
 * The request X is ordered on all the nodes
 * Client is not aware of this (since he already received NACK before).

*Fix options*
 * Do not send NACK if view change is in progress. Nevertheless do not process them, so that for the client it will look like a timeout.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj3i",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"05/Jun/19 1:12 AM;sergey.khoroshavin;*Problem reason*
As in description

*Changes made*
During view change incoming client requests are silently discarded (without sending NACKs)

*PR*
https://github.com/hyperledger/indy-plenum/pull/1223

*Version*
indy-node 1.9.0~dev985

*Risk*
Low
;;;","05/Jun/19 1:19 AM;sergey.khoroshavin;*Tests performed*
An 25-nodes AWS pool was set up with following versions:
* indy-plenum 1.9.0~dev807
* indy-node 1.9.0~dev985
* sovtoken & sovtokenfees  0.9.13~39

Periodic forced view change was enabled (each 30 minutes).

The following load test was run for 12 hours:
{code}
perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":10,\""addr_mint_limit\"":1000000,\""mint_by\"":250000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 10 -b 10 -l 10
{code}

*Expected results*
Transactions should be ordered, pool should stay in consensus, no requests should be rejected, especially no *FundsError should be encountered on pool. Limited number of nodes may lag behind or stop ordering due to known problems with current view change, however no ledgers should become corrupted.

*Actual results*
As expected;;;",,,,,,,,,,,,,,,,,,,,
Improve system tests setup routine to create all docker resources in python,INDY-2125,40284,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,03/Jun/19 8:44 PM,10/Jul/19 5:41 PM,28/Oct/23 2:47 AM,,,,,,,,,0,system-tests,,,,"Such docker resources as images and network should be built and cleaned as part of python tests setup routine (fixtures) instead some shell helpers.

Such helpers are already partly presented but they should be improved to support all parameters that might be passed (e.g. version of artifacts, network parameters etc.)",,,,,,,,,,,,,,,,,,,,,INDY-2057,,,INDY-2132,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2165,,,No,,Unset,No,,,"1|hzwx4f:2mo",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use more trusted docker image for system tests client docker container,INDY-2126,40285,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,03/Jun/19 8:55 PM,10/Jul/19 5:41 PM,28/Oct/23 2:47 AM,,,,,,,,,0,system-tests,,,,"Currently we use [https://hub.docker.com/r/teracy/ubuntu/tags] as base image for our dind client which seems quite solid (50K+ pulls) but we should more closer look on it: is it safe to use it for our build pipelines.

As of now there is no official docker dind image based on ubuntu:16.04.

Options:
 # continue to use it
 # switch to another more trusted base image
 # grab the idea of the dind client inside docker container and build out own image based on ubuntu:16.04 (optionally and preferable: push to docker registry). Related links:
 ** [https://github.com/teracyhq/docker-files]
 ** [http://jpetazzo.github.io/2015/09/03/do-not-use-docker-in-docker-for-ci/]
 ** [https://github.com/jpetazzo/dind]
 ** [https://hub.docker.com/_/docker]
 ** [https://hub.docker.com/r/jpetazzo/dind]",,,,,,,,,,,,,,,,,,,,,INDY-2057,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2165,,,No,,Unset,No,,,"1|hzwx4f:2n",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make more system tests to be ready for Indy Node CD pipeline,INDY-2127,40286,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,andkononykhin,andkononykhin,03/Jun/19 8:58 PM,21/Jun/19 10:23 PM,28/Oct/23 2:47 AM,21/Jun/19 9:32 PM,,,,,,,,0,system-tests,,,,"Need to:
 * improve system tests reduce running time (especially for *TestAuditSuite.py* and *TestAuthMapSuite.py*), options:
 ** replace unnecessary delays with eventually based waiters (currently there a lot of delays based on ""sleep"" with quite big sleeping periods)
 ** split test modules into parts
 * makes tokens related tests optional, options
 ** move all tokes tests to separate modules
 ** use some conditional flag to skip those tests
 * review and organize *test_misc.py*

Acceptance criteria:
 # each test module should need not more that a half an hour
 # there should be a possibility to run mixed tests modules without tokens related tests ",,,,,,,,,,,,,,INDY-2152,,,,,,,INDY-2057,,,INDY-2151,INDY-2155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9861",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,"21/Jun/19 9:28 PM;VladimirWork;All action items are made in system tests and on CD side except ""review and organize test_misc.py"" (INDY-2155 is reported).;;;","21/Jun/19 10:23 PM;VladimirWork;https://github.com/hyperledger/indy-test-automation/pull/27 - PR with fixes and updates.;;;",,,,,,,,,,,,,,,,,,,,
All BuilderNet nodes are restarting every 30-50 seconds,INDY-2128,40352,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,lbendixsen,lbendixsen,lbendixsen,04/Jun/19 7:59 AM,06/Jun/19 4:05 PM,28/Oct/23 2:47 AM,06/Jun/19 4:05 PM,1.8.0,,1.8.1,,,,,0,,,,,"*This is very high priority, the BuilderNet is down until this can be fixed or worked around.*

_Environment_: BuilderNet shortly after upgrading to Sovrin 1.1.45 (indy-node version 1.8.0)

_Steps to Reproduce_: 
 # Upgraded BuilderNet to Sovrin 1.1.45
 # All looked fine except 2 nodes did not upgrade properly.
 # When we manually upgraded the SYGNET1 node, things looked weird when trying to re-add it to the pool, so I performed a restart-pool of the entire network (between  approximately  18:15 and 18:45 UTC or log time)
 # Logs for journalctl of indy-node and indy-node-control, FoundationBuilder.log and read_ledger output for all ledgers, are attached. 

_Expected Behavior_: No looping of restarts of nodes

 

_Observed Behavior_: Nodes are restarting every 30-50 seconds

 

_Notes_: When we added the SYGNET1 node last week, there was a problem where the node owner used a different alias to ""init-indy-node"" on his server than what was initially used to run ""ledger node ..."" to add it to the ledger.  This caused confusion on the ledger until we repaired it (or so we thought). The repair involved resending the initial init-indy-node command on the server and then he deleted 3 files with the wrong names from a directory. ",,,,,,,,,,,,,,,,,,,,,,,,INDY-2129,,,,,,,"04/Jun/19 7:58 AM;lbendixsen;1001ledger.out;https://jira.hyperledger.org/secure/attachment/17292/1001ledger.out","04/Jun/19 7:58 AM;lbendixsen;Auditledger.out.xz;https://jira.hyperledger.org/secure/attachment/17291/Auditledger.out.xz","04/Jun/19 7:58 AM;lbendixsen;FoundationBuilder-restart.log.xz;https://jira.hyperledger.org/secure/attachment/17288/FoundationBuilder-restart.log.xz","04/Jun/19 11:06 PM;lbendixsen;audit_transactions.tar;https://jira.hyperledger.org/secure/attachment/17298/audit_transactions.tar","04/Jun/19 7:58 AM;lbendixsen;configledger.out;https://jira.hyperledger.org/secure/attachment/17290/configledger.out","04/Jun/19 7:58 AM;lbendixsen;domainledger.out;https://jira.hyperledger.org/secure/attachment/17289/domainledger.out","04/Jun/19 11:13 PM;sergey.khoroshavin;fix_audit_ledger.py;https://jira.hyperledger.org/secure/attachment/17299/fix_audit_ledger.py","04/Jun/19 7:58 AM;lbendixsen;jrnlctl_control.out;https://jira.hyperledger.org/secure/attachment/17287/jrnlctl_control.out","04/Jun/19 7:58 AM;lbendixsen;jrnlctl_node.out;https://jira.hyperledger.org/secure/attachment/17286/jrnlctl_node.out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bja",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,lbendixsen,sergey.khoroshavin,,,,,,,,"04/Jun/19 6:15 PM;ashcherbakov;*Summary*

The issue is not caused by Pool Upgrade. The upgrade went well. The pool was in consensus and wrote txns.

The issue is caused by the following:
 # An audit ledger-related bug in Plenum (see *Issue Reason* and INDY-2129)
 # A tricky sequence of steps (demotions-promotions) not mentioned in the ticket (see *Sequence of steps led to the issue*)

 

*Next Steps:*
 # Fix the Pool
 The following needs to be done:
 ## Stop all nodes
 ## It would be great to have the audit ledger from one of the Nodes as-is, that is the whole rocksdb database (/var/lib/indy/NETWORK_NAME/data/NODE_NAME/audit_transactions) instead of `read_ledger` output. This is needed to make sure that our fix script will work.
 ## It would be great to have `validator-info` script output (in verbose (-v) mode) from all nodes (a local script output, not remote command, since the pool doesn't response to any requests now)
{code:java}
validator_info -v {code}
 ## Run the script (fixing the data in audit ledger) that we will create and attach here (see *How to recover the Pool*)
 ## Start all nodes
 # Fix the issue: INDY-2129.
 See *How to fix the issue*
 # Issue a hotfix release?

 

*Important Consequences*
 * Please do not demote any nodes on StagingNet and MainNet until the fix from INDY-2129 is applied there
 * If a node has to be demoted, please make sure that it's not in the current list of primaries.

 

*Sequence of steps led to the issue:* 
 * 4:20 PM (UTC): The upgraded finished successfully
 ** ""primaries"":[""datum-sovrin"",""uvs_val2"",""validatedid"",""OgNode"",""fetch-ai""],

 * 5:28:46 PM: ""SYGNET1"" was demoted
 ** primaries changed: 
 ""primaries"":[""datum-sovrin"",""uvs_val2"",""validatedid"",""OgNode""]
 * 5:42:26 PM: The pool was restarted
 * {color:#de350b}*5:43:28 PM**: ""validatedid"" was demoted*{color}
 ** {color:#172b4d}primaries not changed:
 ""primaries"":[""datum-sovrin"",""uvs_val2"",""validatedid""{color},""OgNode""]
 ** *primaries haven't been changed by the pool, but ""validatedid"" is one of the primaries, and it was demoted.*

 * 6:13:40 PM: ""SYGNET1"" was promoted (added to the pool again)
 ** primaries not changed:
 ""primaries"":[""datum-sovrin"",""uvs_val2"",""{color:#de350b}validatedid{color}"",""OgNode""]

 * *{color:#de350b}6:38:09 PM: The pool was restarted{color}*
 ** primaries not changed:
 ""primaries"":[""datum-sovrin"",""uvs_val2"",""{color:#de350b}validatedid{color}"",""OgNode""]
 * The pool wasn't able to start-up correctly after this as it caught the following exception: `LogicError('Audit ledger has inconsistent names of nodes')`. This is caused by the fact, that {color:#172b4d}""validatedid"" is one of the primaries, and it was demoted.{color}
 * The nodes stopped due to the exception, started up (automatically), and catch the exception again in a loop.

 

*Issue Reason:*

INDY-2129 was created.
 * The primaries are not changed (updated) if one of them is demoted and F (number of replicas) is not changed.
 ** For example, we had 6 nodes (F=1), and demoting 1 (F is still 1)
 * The list of primaries is stored in the audit ledger for recovering after restart. If a node (or a pool) is restarted, then the primaries will be restored from the audit ledger. So, if one of the primaries is demoted, it will not be found, and the pool raises `LogicError('Audit ledger has inconsistent names of nodes')`.

 

*How to recover the Pool*

We will create a script that will do the following:
 # Delete audit ledger hash store
 # Modify the last audit txn where the primaries are changed (a txn with seqNo=32322), so that the instead of
{code:java}
 ""primaries"":[""datum-sovrin"",""uvs_val2"",""validatedid"",""OgNode""]`{code}
it will have
{code:java}
""primaries"":[""datum-sovrin"",""uvs_val2"",""OgNode"",""fetch-ai""]{code}
that is `validatedid` is removed
 # When the node is started, the audit ledger hash store will be restored

 

*How to fix the issue*
 * Integration test will be written
 * The primaries in audit ledger needs to be updated if a demoted node is in the list of current primaries
 * A view change needs to be triggered if a demoted node is in the list of current primaries
 * Non-critical (recoverable) cases during node start-up (catch-up, post-catchup, primary selection) should not raise the LogicErrors.
 ** in particular, in `on_catchup_complete` method in `PrimarySelector`

 

 ;;;","04/Jun/19 10:21 PM;esplinr;Root cause analysis:

There was a long standing bug in how demotion was handled, such that the  demoted replica would silently fail. With the introduction of the Audit Ledger we added stricter checking of what we considered an impossible scenario which exposed the problem and resulted in the failure.;;;","04/Jun/19 11:08 PM;lbendixsen;I added attachment audit_transactions.tar with the requested rocksdb files in it.  tarred with ""cjvf"" parms.  ;;;","04/Jun/19 11:16 PM;lbendixsen;I will request 'validator-info -v' from all nodes, but in the meantime, here it is from FoundationBuilder (Sovrin's node):

*ubuntu@ip-172-31-35-134*:*~*$ sudo validator-info -v

""Hardware"":    

     ""HDD_all"":      2692 Mbs 

     ""RAM_all_free"":  28657 Mbs 

     ""RAM_used_by_node"":  880 Mbs   

""Node_info"":   

     ""BLS_key"":      3gmhmqpPLqznZF3g3niodaHjbpsB6TEeE9SpgXgBnZJLmXgeRzJqTLajVwbhxrkomJFTFU4ohDC4ZRXKbUPCQywJuPAQnst8XBtCFredMECn4Z3goi1mNt5QVRdU8Ue2xMSkdLpsQMjCsNwYUsBguwXYUQnDXQXnHqRkK9qrivucQ5Z

     ""Catchup_status"": 

          ""Last_txn_3PC_keys"": 

               ""0"":           

                    ""Certisign"":   

                          None      

                          None      

                    ""OgNode"":      

                          None      

                          None      

                    ""datum-sovrin"": 

                          None      

                          None      

                    ""fetch-ai"":    

                          None      

                          None      

                    ""makolab01"":   

                          None      

                          None      

                    ""uvs_val2"":    

                          None      

                          None      

                    ""vnode1"":      

                          None      

                          None      

                    ""xsvalidatorec2irl"": 

                          None      

                          None      

               ""1"":           

               ""1001"":        

               ""2"":           

               ""3"":           

                    ""Certisign"":   

                          None      

                          None      

                    ""OgNode"":      

                          None      

                          None      

                    ""danube"":      

                          None      

                          None      

                    ""fetch-ai"":    

                          None      

                          None      

                    ""uvs_val2"":    

                          None      

                          None      

                    ""vnode1"":      

                          None      

                          None      

                    ""xsvalidatorec2irl"": 

                          None      

                          None      

          ""Ledger_statuses"": 

               ""0"":            synced    

               ""1"":            not_synced

               ""1001"":         not_synced

               ""2"":            not_synced

               ""3"":            not_synced

          ""Number_txns_in_catchup"": 

               ""0"":            0         

               ""1"":            0         

               ""1001"":         0         

               ""2"":            0         

               ""3"":            0         

          ""Received_LedgerStatus"":            

          ""Waiting_consistency_proof_msgs"": 

               ""0"":            None      

               ""1"":            None      

               ""1001"":         None      

               ""2"":            None      

               ""3"":            None      

     ""Client_ip"":    172.31.128.254

     ""Client_port"":  9702      

     ""Client_protocol"":  tcp       

     ""Committed_ledger_root_hashes"": 

          ""0"":            b'5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'

          ""1"":            b'79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'

          ""1001"":         b'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP'

          ""2"":            b'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'

          ""3"":            b'7nBHLR1xP9yxkRyTVKUKmiCErNQF5mNSheXPWgYyBXdV'

     ""Committed_state_root_hashes"": 

          ""0"":            b'CGuyq6T4ASsqDbSJVn7sAhhybjD4rhjpxSAQkjuPo2Tx'

          ""1"":            b'78LWbD7o46dA2C8cYT8da1kRtLwnsXwJkch5obWdqQLd'

          ""1001"":         b'7Mut4DkZsjAzJwxNRFWmFNN5zi3v9wj3xL28TpbaWDXG'

          ""2"":            b'EDHc8pEcGBq9tTy8zbEA1wFoZy3an5eMo3VJN16Rv2ec'

     ""Count_of_replicas"":  4         

     ""Freshness_status"": 

          ""0"":           

               ""Has_write_consensus"":  True      

               ""Last_updated_time"":  2019-06-04 14:10:35+00:00

          ""1"":           

               ""Has_write_consensus"":  True      

               ""Last_updated_time"":  2019-06-04 14:10:35+00:00

          ""1001"":        

               ""Has_write_consensus"":  True      

               ""Last_updated_time"":  2019-06-04 14:10:35+00:00

          ""2"":           

               ""Has_write_consensus"":  True      

               ""Last_updated_time"":  2019-06-04 14:10:35+00:00

     ""Metrics"":     

          ""Delta"":        0.1       

          ""Lambda"":       240       

          ""Omega"":        20        

          ""average-per-second"": 

               ""read-transactions"":  0.08236989762391617

               ""write-transactions"":  0.0       

          ""avg backup throughput"":  None      

          ""client avg request latencies"": 

               ""0"":            None      

               ""1"":            None      

               ""2"":            None      

               ""3"":            None      

          ""instances started"": 

               ""0"":            7316415.736191218

               ""1"":            7316415.737209168

               ""2"":            7316415.738143472

               ""3"":            7316415.739143314

          ""master throughput"":  None      

          ""master throughput ratio"":  None      

          ""max master request latencies"":  0         

          ""ordered request counts"": 

               ""0"":            0         

               ""1"":            0         

               ""2"":            0         

               ""3"":            0         

          ""ordered request durations"": 

               ""0"":            0         

               ""1"":            0         

               ""2"":            0         

               ""3"":            0         

          ""throughput"": 

               ""0"":            None      

               ""1"":            None      

               ""2"":            None      

               ""3"":            None      

          ""total requests"":  0         

          ""transaction-count"": 

               ""1001"":         10        

               ""audit"":        32368     

               ""config"":       86        

               ""ledger"":       109       

               ""pool"":         32        

          ""uptime"":       60        

     ""Mode"":         discovering

     ""Name"":         FoundationBuilder

     ""Node_ip"":      172.31.35.134

     ""Node_port"":    9701      

     ""Node_protocol"":  tcp       

     ""Replicas_status"": 

          ""FoundationBuilder:0"": 

               ""Last_ordered_3PC"": 

                     0         

                     0         

               ""Primary"":      None      

               ""Stashed_txns"": 

                    ""Stashed_PrePrepare"":  0         

                    ""Stashed_checkpoints"":  0         

               ""Watermarks"":   0:300     

          ""FoundationBuilder:1"": 

               ""Last_ordered_3PC"": 

                     0         

                     0         

               ""Primary"":      None      

               ""Stashed_txns"": 

                    ""Stashed_PrePrepare"":  0         

                    ""Stashed_checkpoints"":  0         

               ""Watermarks"":   0:300     

          ""FoundationBuilder:2"": 

               ""Last_ordered_3PC"": 

                     0         

                     0         

               ""Primary"":      None      

               ""Stashed_txns"": 

                    ""Stashed_PrePrepare"":  0         

                    ""Stashed_checkpoints"":  0         

               ""Watermarks"":   0:300     

          ""FoundationBuilder:3"": 

               ""Last_ordered_3PC"": 

                     0         

                     0         

               ""Primary"":      None      

               ""Stashed_txns"": 

                    ""Stashed_PrePrepare"":  0         

                    ""Stashed_checkpoints"":  0         

               ""Watermarks"":   0:300     

     ""Requests_timeouts"": 

          ""Ordering_phase_req_timeouts"":  0         

          ""Propagates_phase_req_timeouts"":  0         

     ""Uncommitted_ledger_root_hashes"": 

     ""Uncommitted_ledger_txns"": 

          ""0"":           

               ""Count"":        0         

          ""1"":           

               ""Count"":        0         

          ""1001"":        

               ""Count"":        0         

          ""2"":           

               ""Count"":        0         

          ""3"":           

               ""Count"":        0         

     ""Uncommitted_state_root_hashes"": 

          ""0"":            b'CGuyq6T4ASsqDbSJVn7sAhhybjD4rhjpxSAQkjuPo2Tx'

          ""1"":            b'78LWbD7o46dA2C8cYT8da1kRtLwnsXwJkch5obWdqQLd'

          ""1001"":         b'7Mut4DkZsjAzJwxNRFWmFNN5zi3v9wj3xL28TpbaWDXG'

          ""2"":            b'EDHc8pEcGBq9tTy8zbEA1wFoZy3an5eMo3VJN16Rv2ec'

     ""View_change_status"": 

          ""IC_queue"":    

               ""8"":           

                    ""Voters"":      

                         ""FoundationBuilder"": 

                              ""reason"":       43        

                         ""fetch-ai"":    

                              ""reason"":       43        

                         ""uvs_val2"":    

                              ""reason"":       43        

                         ""validatedid"": 

                              ""reason"":       20        

          ""Last_complete_view_no"":  0         

          ""Last_view_change_started_at"":  1970-01-01 00:00:00

          ""VCDone_queue"": 

          ""VC_in_progress"":  False     

          ""View_No"":      0         

     ""did"":          GVvdyd7Y6hsBEy5yDDHjqkXgH8zW34K74RsxUiUCZDCE

     ""verkey"":       46EM3ARvGqR1Z8FBUdQBscfDmDBdk3866oopEjbGjpMsjwVBfVL7cdf

""Pool_info"":   

     ""Blacklisted_nodes"": 

     ""Quorums"":      {'strong': Quorum(9), 'n': 12, 'consistency_proof': Quorum(4), 'timestamp': Quorum(4), 'commit': Quorum(9), 'bls_signatures': Quorum(9), 'view_change_done': Quorum(9), 'reply': Quorum(4), 'checkpoint': Quorum(8), 'same_consistency_proof': Quorum(4), 'view_change': Quorum(9), 'ledger_status': Quorum(8), 'observer_data': Quorum(4), 'election': Quorum(9), 'weak': Quorum(4), 'propagate': Quorum(4), 'backup_instance_faulty': Quorum(4), 'prepare': Quorum(8), 'ledger_status_last_3PC': Quorum(4), 'f': 3}

     ""Reachable_nodes"": 

           ['Certisign', None]

           ['FoundationBuilder', None]

           ['OgNode', None]

           ['danube', None]

           ['fetch-ai', None]

           ['uvs_val2', None]

           ['vnode1', None]

     ""Reachable_nodes_count"":  7         

     ""Read_only"":    False     

     ""Suspicious_nodes"":            

     ""Total_nodes_count"":  12        

     ""Unreachable_nodes"": 

           ['SYGNET1', None]

           ['datum-sovrin', None]

           ['makolab01', None]

           ['ovvalidator', None]

           ['xsvalidatorec2irl', None]

     ""Unreachable_nodes_count"":  5         

     ""f_value"":      3         

""Protocol"":    

""Software"":    

     ""Indy_packages"": 

           hi  indy-node                        1.8.0                                      amd64        Indy node

           hi  indy-plenum                      1.8.0                                      amd64        Plenum Byzantine Fault Tolerant Protocol

           hi  libindy-crypto                   0.4.5                                      amd64        This is the shared crypto libirary for Hyperledger Indy components.

           hi  python3-indy-crypto              0.4.5                                      amd64        This is the official wrapper for Hyperledger Indy Crypto library (https://www.hyperledger.org/projects).

     ""Installed_packages"": 

           sovtokenfees 0.9.12

           semver 2.7.9

           psutil 5.4.3

           portalocker 0.5.7

           Pympler 0.5

           intervaltree 2.1.0

           python-dateutil 2.6.1

           sovrin 1.1.45

           timeout-decorator 0.4.0

           setuptools 38.5.2

           indy-crypto 0.4.5-23

           ioflo 1.5.4

           sortedcontainers 1.5.7

           python-rocksdb 0.6.9

           jsonpickle 0.9.6

           indy-plenum 1.8.0

           six 1.11.0

           libnacl 1.6.1

           orderedset 2.0

           sha3 0.2.1

           rlp 0.5.1 

           Pygments 2.2.0

           distro 1.3.0

           pyzmq 17.0.0

           packaging 19.0

           base58 1.0.0

           sovtoken 0.9.12

           indy-node 1.8.0

     ""OS_version"":   Linux-4.4.0-1075-aws-x86_64-with-Ubuntu-16.04-xenial

     ""indy-node"":    1.8.0     

     ""sovrin"":       1.1.45    

""Update_time"":  Tuesday, June 4, 2019 2:11:35 PM +0000

""response-version"":  0.0.1     

""timestamp"":    1559657495

 

""Configuration"": 

     ""Config"":      

          ""Main_config"": 

                # Current network

                NETWORK_NAME = 'net3'

                # Disable stdout logging

                enableStdOutLogging = False

                # Directory to store ledger.

                LEDGER_DIR = '/var/lib/indy'

                # Directory to store logs.

                LOG_DIR = '/var/log/indy'

                # Directory to store keys.

                KEYS_DIR = '/var/lib/indy'

                # Directory to store genesis transactions files.

                GENESIS_DIR = '/var/lib/indy'

                # Directory to store backups.

                BACKUP_DIR = '/var/lib/indy/backup'

                # Directory to store plugins.

                PLUGINS_DIR = '/var/lib/indy/plugins'

                # Directory to store node info.

                NODE_INFO_DIR = '/var/lib/indy'

                ENABLED_PLUGINS=[]

                ANYONE_CAN_WRITE = False

                UPGRADE_ENTRY = 'sovrin'                          

                ENABLED_PLUGINS.append('sovtoken')

                ENABLED_PLUGINS.append('sovtokenfees')

          ""Network_config"": 

          ""User_config"": 

     ""Genesis_txns"": 

          ""domain_txns"": 

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Phil Windley"",""dest"":""NMjQb59rKTJXKNqVYfcZFi"",""role"":""0"",""verkey"":""Ce9jZ2bQcLRCrY3eT5AbjsU5mXFa4jMF6dDSF21tyeFJ""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":1},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Jason Law"",""dest"":""K2ze2xR8MAxkQscdkboKnD"",""role"":""0"",""verkey"":""~KhvZWs1QSS8bS1RxB55NE3""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":2},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Drummond Reed"",""dest"":""Jv4afJBghiuJ2tiZDduarJ"",""role"":""0"",""verkey"":""AmKnoVceCaEHBCaDjtSYqwLLpmVQLBbZMgAZHfSyv936""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":3},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Peter Simpson"",""dest"":""JX29L7h6UpDNEThiaTYx9N"",""role"":""0"",""verkey"":""AYmPQJTcpsH1YxRQUwwRgGzQVLFf8JRvmuS5LKNZN1gA""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":4},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Ron Amstutz"",""dest"":""7jJe9ArRfRchSKL2sYgFDj"",""role"":""0"",""verkey"":""4fjHSUqU9RmeXWXHV6MnKFDtEyEcBUipovhNCDuei5XW""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":5},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Mawaki Chango"",""dest"":""CPU9r7iLSXcSdn79FzbUvQ"",""role"":""0"",""verkey"":""7CyhcN7gsqo9VxAV1V7ZhiqTetAadY2S4QYcUdo3Uh4p""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":6},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Mike Bailey"",""dest"":""6feBTywcmJUriqqnGc1zSJ"",""role"":""0"",""verkey"":""~EpnvDMWiFSSu1YksE1Cg3n""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":7},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""alias"":""Lynn Bendixsen"",""dest"":""5M3i1PbpvEQmTk25EmAY6N"",""role"":""0"",""verkey"":""~7iwFwParUgTffA22Q5Tgvg""},""metadata"":{},""type"":""1""},""txnMetadata"":\{""seqNo"":8},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""dest"":""V5qJo72nMeF7x3ci8Zv2WP"",""role"":""2"",""verkey"":""~8gjR6mxPymhQV5pEERHYLJ""},""metadata"":\{""from"":""5M3i1PbpvEQmTk25EmAY6N""},""type"":""1""},""txnMetadata"":\{""seqNo"":9},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""dest"":""FzAaV9Waa1DccDa72qwg13"",""role"":""2"",""verkey"":""~D5bNaxMVhMzi2bFVzZRtmG""},""metadata"":\{""from"":""5M3i1PbpvEQmTk25EmAY6N""},""type"":""1""},""txnMetadata"":\{""seqNo"":10},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""dest"":""QuCBjYx4CbGCiMcoqQg1y"",""role"":""2"",""verkey"":""~NqriEDc1sz9yiY2ft2QGmx""},""metadata"":\{""from"":""5M3i1PbpvEQmTk25EmAY6N""},""type"":""1""},""txnMetadata"":\{""seqNo"":11},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""dest"":""VbPQNHsvoLZdaNU7fTBeFx"",""role"":""2"",""verkey"":""~C8m3zsBqvEu1sHoeiGZpht""},""metadata"":\{""from"":""5M3i1PbpvEQmTk25EmAY6N""},""type"":""1""},""txnMetadata"":\{""seqNo"":12},""ver"":""1""}

          ""pool_txns"":   

                {""reqSignature"":{},""txn"":\{""data"":{""data"":{""alias"":""FoundationBuilder"",""blskey"":""3gmhmqpPLqznZF3g3niodaHjbpsB6TEeE9SpgXgBnZJLmXgeRzJqTLajVwbhxrkomJFTFU4ohDC4ZRXKbUPCQywJuPAQnst8XBtCFredMECn4Z3goi1mNt5QVRdU8Ue2xMSkdLpsQMjCsNwYUsBguwXYUQnDXQXnHqRkK9qrivucQ5Z"",""blskey_pop"":""RHWacPhUNc9JWsGNdmWYHrAvvhsow399x3ttNKKLDpz9GkxxnTKxtiZqarkx4uP5ByTwF4kM8nZddFKWuzoKizVLttALQ2Sc2BNJfRzzUZMNeQSnESkKZ7U5vE2NhUDff6pjANczrrDAXd12AjSG61QADWdg8CVciZFYtEGmKepwzP"",""client_ip"":""35.161.146.16"",""client_port"":""9702"",""node_ip"":""50.112.53.5"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""GVvdyd7Y6hsBEy5yDDHjqkXgH8zW34K74RsxUiUCZDCE""},""metadata"":\{""from"":""V5qJo72nMeF7x3ci8Zv2WP""},""type"":""0""},""txnMetadata"":\{""seqNo"":1,""txnId"":""fe991cd590fff10f596bb6fe2362229de47d49dd50748e38b96f368152be29c7""},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""data"":{""alias"":""vnode1"",""blskey"":""t5jtREu8au2dwFwtH6QWopmTGxu6qmJ3iSnk321yLgeu7mHQRXf2ZCBuez8KCAQvFZGqqAoy2FcYvDGCqQxRCz9qXKgiBtykzxjDjYu87JECwwddnktz5UabPfZmfu6EoDn4rFxvd4myPu2hksb5Z9GT6UeoEYi7Ub3yLFQ3xxaQXc"",""blskey_pop"":""QuHB7tiuFBPQ6zPkwHfMtjzWqXJBLACtfggm7zCRHHgdva18VN4tNg7LUU2FfKGQSLZz1M7oRxhhgJkZLL19aGvaHB2MPtnBWK9Hr8LMiwi95UjX3TVXJri4EvPjQ6UUvHrjZGUFvKQphPyVTMZBJwfkpGAGhpbTQuQpEH7f56m1X5"",""client_ip"":""206.189.143.34"",""client_port"":""9796"",""node_ip"":""206.189.143.34"",""node_port"":""9797"",""services"":[""VALIDATOR""]},""dest"":""9Aj2LjQ2fwszJRSdZqg53q5e6ayScmtpeZyPGgKDswT8""},""metadata"":\{""from"":""FzAaV9Waa1DccDa72qwg13""},""type"":""0""},""txnMetadata"":\{""seqNo"":2,""txnId"":""5afc282bf9a7a5e3674c09ee48e54d73d129aa86aa226691b042e56ff9eaf59b""},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""data"":{""alias"":""xsvalidatorec2irl"",""blskey"":""4ge1yEvjdcV6sDSqbevqPRWq72SgkZqLqfavBXC4LxnYh4QHFpHkrwzMNjpVefvhn1cgejHayXTfTE2Fhpu1grZreUajV36T6sT4BiewAisdEw59mjMxkp9teYDYLQqwPUFPgaGKDbFCUBEaNdAP4E8Q4UFiF13Qo5842pAY13mKC23"",""blskey_pop"":""R5PoEfWvni5BKvy7EbUbwFMQrsgcuzuU1ksxfvySH6FC5jpmisvcHMdVNik6LMvAeSdt6K4sTLrqnaaQCf5aCHkeTcQRgDVR7oFYgyZCkF953m4kSwUM9QHzqWZP89C6GkBx6VPuL1RgPahuBHDJHHiK73xLaEJzzFZtZZxwoWYABH"",""client_ip"":""52.50.114.133"",""client_port"":""9702"",""node_ip"":""52.209.6.196"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""DXn8PUYKZZkq8gC7CZ2PqwECzUs2bpxYiA5TWgoYARa7""},""metadata"":\{""from"":""QuCBjYx4CbGCiMcoqQg1y""},""type"":""0""},""txnMetadata"":\{""seqNo"":3,""txnId"":""1972fce7af84b7f63b7f0c00495a84425cce3b0c552008576e7996524cca04cb""},""ver"":""1""}

                {""reqSignature"":{},""txn"":\{""data"":{""data"":{""alias"":""danube"",""blskey"":""3Vt8fxn7xg8n8pR872cvGWNuR7STFzFSPMftX96zF6871wYVTR27aspxGSeEtx9wj8g4D3GdCxHJbQ4FsxQz6TATQswiiZfxAVNjLLUci8WSH4t1GPx9CvGXB2uzDfVnnJyhhnASxJEbvykLUBBFG3fW4tMQixujpowUADz5jHm427u"",""blskey_pop"":""RJpXXLkjRRv9Lk8tJz8LTkhhC7RWjHQcB9CG8J8U8fXT6arTDMYc62zXtToBAmGkGu8Udsmo3Hh7mv4KB9JAf8ufGY9WsnppCVwar7zEXyBfLpCnDhvVcBAzkhRpHmqHygN24DeBu9aH6tw4uXxVJvRRGSbPtxjWa379BmfQWzXHCb"",""client_ip"":""207.180.207.73"",""client_port"":""9702"",""node_ip"":""173.249.14.196"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""52muwfE7EjTGDKxiQCYWr58D8BcrgyKVjhHgRQdaLiMw""},""metadata"":\{""from"":""VbPQNHsvoLZdaNU7fTBeFx""},""type"":""0""},""txnMetadata"":\{""seqNo"":4,""txnId"":""ebf340b317c044d970fcd0ca018d8903726fa70c8d8854752cd65e29d443686c""},""ver"":""1""}

     ""indy-node-control.service"": 

           [Unit]    

           Description=Service for upgrade of existing Indy Node and other operations

           #Requires=indy.service

           #After=indy.service

           After=network.target

                     

           [Service] 

           Type=simple

           EnvironmentFile=/etc/indy/node_control.conf

           ExecStart=/usr/bin/env python3 -O /usr/local/bin/start_node_control_tool $TEST_MODE --hold-ext ${HOLD_EXT}

           Restart=on-failure

           RestartSec=10

           StartLimitBurst=10

           StartLimitInterval=200

           TimeoutSec=300 

           [Install] 

           WantedBy=multi-user.target

     ""indy-node.service"": 

           [Unit]    

           Description=Indy Node

           Requires=indy-node-control.service

           [Service] 

           EnvironmentFile=/etc/indy/indy.env

           ExecStart=/usr/bin/env python3 -O /usr/local/bin/start_indy_node ${NODE_NAME} ${NODE_IP} ${NODE_PORT} ${NODE_CLIENT_IP} ${NODE_CLIENT_PORT}

           User=indy 

           Group=indy

           Restart=on-failure

           RestartSec=10

           StartLimitBurst=10

           StartLimitInterval=200

           TimeoutSec=300

           LimitNOFILE=65536:131072

           [Install] 

           WantedBy=multi-user.target               

     ""indy.env"":    

           NODE_NAME=FoundationBuilder

           NODE_IP=172.31.35.134

           NODE_PORT=9701

           NODE_CLIENT_IP=172.31.128.254

           NODE_CLIENT_PORT=9702

           CLIENT_CONNECTIONS_LIMIT=500

     ""iptables_config"": 

     ""node_control.conf"": 

           # Uncomment this to run agent in test mode:

           #TEST_MODE=--test

           TEST_MODE=

           HOLD_EXT=sovtoken sovtokenfees sovrin;;;","04/Jun/19 11:26 PM;sergey.khoroshavin;*Script for patching audit ledger:* [^fix_audit_ledger.py] 

*This script expects that there is only one node installation with one network on each machine running BuilderNet*
*This script should be run on every machine running BuilderNet*

How to use:
* copy script to target machines
* stop indy-node service
* run script with superuser privileges (it will patch audit ledger transaction log and delete merkle tree databases)
* make sure it outputs ""Patch applied successfully!""
* after all nodes are patched start indy-node service (it will rebuild merkle tree databases on startup, which can take some time)

Script will abort if:
* it cannot open audit ledger database (for example, if it is locked by running indy node)
* it cannot read transaction with expected seq no
* transaction doesn't contain expected primaries

Also script detects if audit ledger was already patched, and reports and aborts if so.;;;",,,,,,,,,,,,,,,,,
Primaries are not updated in audit ledger if one of the primaries is demoted,INDY-2129,40362,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,andkononykhin,ashcherbakov,ashcherbakov,04/Jun/19 6:13 PM,06/Jun/19 4:05 PM,28/Oct/23 2:47 AM,06/Jun/19 4:05 PM,,,1.8.1,,,,,0,,,,,"*Issue Reason:*
 * The primaries are not changed (updated) if one of them is demoted and F (number of replicas) is not changed.
 ** For example, we had 6 nodes (F=1), and demoting 1 (F is still 1)
 * The list of primaries is stored in the audit ledger for recovering after restart. If a node (or a pool) is restarted, then the primaries will be restored from the audit ledger. So, if one of the primaries is demoted, it will not be found, and the pool raises `LogicError('Audit ledger has inconsistent names of nodes')`.

 

 

*How to fix the issue*
 * Integration test must be written
 * The primaries in audit ledger needs to be updated if a demoted node is in the list of current primaries
 * A view change needs to be triggered if a demoted node is in the list of current primaries
 * Non-critical (recoverable) cases during node start-up (catch-up, post-catchup, primary selection) should not raise the LogicErrors.
 ** in particular, in `on_catchup_complete` method in `PrimarySelector`",,,,,,,,,,,,,,,,,,,,,INDY-2128,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj4",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"04/Jun/19 11:47 PM;ashcherbakov;System Test 1:
1) Have 8 nodes in the pool, so that primaries are [A, B, C]
2) Demote node C 
3) Wait for view change
4) Order 1 more txn 
5) Restart the whole pool 
6) Make sure that the pool restarted correctly, and can order txns

System Test 2:
1) Have 8 nodes in the pool, so that primaries are [A, B, C]
2) Stop Node B to delay the view change to viewNo=1
3) Demote node C (it will trigger a view change to viewNo=1 which will trigger a view change timeout since B (next primary) is stopped)
4) Restart the whole pool right after Demote NODE txn is written on all nodes
5) Make sure that the pool restarted correctly, and can order txns;;;","05/Jun/19 12:00 AM;ashcherbakov;Integration tests needs to follow the same scenarios (they can use InstanceChange delays instead of stopping Node B in the second test case);;;",,,,,,,,,,,,,,,,,,,,
Hotfix 1.8.1,INDY-2130,40423,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,ashcherbakov,ashcherbakov,05/Jun/19 10:22 PM,07/Jun/19 12:56 AM,28/Oct/23 2:47 AM,07/Jun/19 12:56 AM,,,1.8.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj3r",,,,Unset,Unset,Ev-Node 19.11,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apply a new Docker-in-docker approach for system tests,INDY-2131,40463,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,07/Jun/19 10:24 PM,19/Jun/19 2:19 AM,28/Oct/23 2:47 AM,18/Jun/19 11:16 PM,,,1.9.0,,,,,0,system-tests,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2057,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00qus:",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,VladimirWork,,,,,,,,,"18/Jun/19 7:42 PM;andkononykhin;Problem reason:
 * INDY-2057 introduced new possibilities of system tests running made that much more simple and configurable, more tests become ready to add to CD pipeline. Need to updated CD pipeline.

Changes:
 * updated system tests running logic in CD pipeline
 * made some hardening of system tests

PR:
 * https://github.com/hyperledger/indy-test-automation/pull/25
 * [https://github.com/hyperledger/indy-test-automation/pull/26]
 * [https://github.com/hyperledger/indy-node/pull/1342]

Version:
 - Indy Node 1.9.0.dev1005

Risk factors:
 - That impacts rc and stable CD pipeline as well.

Risk:
 - Low

Covered with tests:
 - no, tested manually

Recommendations for QA
 * ensure that tests are run within a proper environment and with proper settings and are passed: check recent pipelines where new logic is presented, e.g. [https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-cd/detail/master/1005/pipeline/274/];;;","18/Jun/19 11:16 PM;VladimirWork;Checked in CD pipeline and locally.;;;",,,,,,,,,,,,,,,,,,,,
Fix outdated system tests,INDY-2132,40486,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,09/Jun/19 4:50 PM,01/Nov/19 9:59 PM,28/Oct/23 2:47 AM,,,,,,,,,0,system-tests,,,,"* TestMultiSigSuite.py: test_case_sign_and_multisign relies on old libindy logic which allowed to combine single and multi signs for one message. Need to use some raw APIs as workaround to make wrongly signed request
 * test_upgrade.py: need to fix helpers to provide an ability to dynamically build necessary docker environment, in particular need to have pool with old version of Node installed to test the upgrade from  that version (seems that it's blocked by INDY-2125)",,,,,,,,,,,,,,,,,,,,,INDY-2125,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2165,,,No,,Unset,No,,,"1|hzwx4f:2ni",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,"01/Nov/19 9:59 PM;VladimirWork;TestMultiSigSuite.py: test_case_sign_and_multisign - fixed.;;;",,,,,,,,,,,,,,,,,,,,,
Consensus not achieved until after a pool-restart,INDY-2133,40510,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,lbendixsen,lbendixsen,11/Jun/19 7:03 AM,14/Jun/19 9:46 PM,28/Oct/23 2:47 AM,14/Jun/19 9:46 PM,,,1.9.0,,,,,0,TShirt_M,,,,"_Environment_: BuilderNet (indy-node 1.8.0)

_Steps to Reproduce_: When the BuilderNet failed last week, we had everyone run a script (see issue indy-2128).  All but one node ran the script, so we decided to have everyone restart.  The restart process was gradual, but when I came in to work this morning (16:00 
UTC) there were enough nodes for consensus, but they were reporting as not in consensus.  After a short time I tried a restart and that seemed to fix the issue.  See attached logs.

_Expected Behavior_: Nodes achieve consensus without a restart of all nodes.

_Observed Behavior_: The Nodes did not come into consensus on their own.
_Notes_: [~sergey.khoroshavin]  has output in RC from my monitoring script that shows some of the above mentioned behaviors. He suggested that I enter this ticket and include logs from before the weekend (Start looking at date 2019-06-07) until today (2019-06-10 ~1700 UTC).  See attached log or logs.",,,,,,,,,,,,,,,,,,,,,,,,INDY-2143,INDY-2145,,,,,,"12/Jun/19 2:00 AM;lbendixsen;FoundationBuilder.tgz;https://jira.hyperledger.org/secure/attachment/17316/FoundationBuilder.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00r5u:",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),lbendixsen,sergey.khoroshavin,,,,,,,,,,"11/Jun/19 5:49 PM;sergey.khoroshavin;[~lbendixsen] It looks like attached logs contain records around May 29 only.;;;","12/Jun/19 1:38 AM;lbendixsen;Sorry for the mistake.   I will try again.;;;","14/Jun/19 9:38 PM;sergey.khoroshavin;*Preliminaries*

In order to understand following analysis one needs to know the following things:
* instance change messages are sent with different reason codes
* reason 43 means node found that pool didn't order anything (including freshness updates) for too long (~10 minutes)
* reason 28 means node entered a view change, but failed to finish it before timeout (~7 minutes)
* node increases local view no as soon as it enters a view change
* instance changes with reason 43 are sent to 
** next view if node is not in a view change
** current view if node is in process of view change
* instance changes with reason 28 are always sent to next view

At the time of interest BuilderNet consisted of the following 12 validator nodes:
* ovvalidator
* Certisign
* danube
* vnode1
* datum-sovrin
* uvs_val2
* OgNode
* SYGNET1
* xsvalidatorec2irl
* fetch-ai
* FoundationBuilder
* makolab01

9 active nodes are required for consensus (to initiate a view change, to finish it and to order transactions)

*Analysis*

We have 8 nodes out of 9 required for consenus

{code}
2019-06-07 19:10:46,192|INFO|looper.py|Starting up indy-node
2019-06-07 19:10:47,826|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to xsvalidatorec2irl
2019-06-07 19:10:48,022|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to ovvalidator
2019-06-07 19:10:48,158|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to datum-sovrin
2019-06-07 19:10:48,185|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to Certisign
2019-06-07 19:10:48,221|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to SYGNET1
2019-06-07 19:10:48,678|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode
2019-06-07 19:15:01,862|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to fetch-ai
{code}

danube connects, so we have 9 nodes and we reach consensus

{code}
2019-06-08 07:09:06,482|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode
2019-06-08 07:09:06,590|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to danube
2019-06-08 07:09:06,592|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService transitioning from PreSyncingPool to SyncingAudit
2019-06-08 07:09:25,701|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService transitioning from SyncingAudit to SyncingPool
2019-06-08 07:09:25,706|INFO|node.py|FoundationBuilder caught up to 0 txns in the last catchup
2019-06-08 07:09:25,706|INFO|replica.py|FoundationBuilder:0 set last ordered as (6, 5961)
2019-06-08 07:09:25,711|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode
2019-06-08 07:09:25,707|INFO|node.py|FoundationBuilder started participating
2019-06-08 07:09:25,707|INFO|replica.py|FoundationBuilder:0 setting primaryName for view no 6 to: datum-sovrin:0
{code}

And pool was trying to order something

{code}
2019-06-08 07:09:25,709|INFO|replica_stasher.py|FoundationBuilder:0 unstash 24 messages from future view
{code}

However OgNode seem to be crashing repeatedly (for some reason), bringing pool out of and back into consensus

{code}
2019-06-08 07:09:26,554|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode
... 
2019-06-08 07:12:25,210|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode
{code}

Freshness updates don't happen so nodes try to enter a view change. However there are only 8 of them (FoundationBuilder, Certisign, danube, datum-sovrin, fetch-ai, ovvalidator, SYGNET1, xsvalidatorec2irl), so they cannot actually enter a view change.

{code}
2019-06-08 07:10:02,026|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai
2019-06-08 07:10:47,856|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long
2019-06-08 07:10:55,399|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from Certisign
2019-06-08 07:11:43,366|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from ovvalidator
2019-06-08 07:12:23,133|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from datum-sovrin
2019-06-08 07:13:31,377|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from xsvalidatorec2irl
2019-06-08 07:13:54,652|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1
2019-06-08 07:15:02,026|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai
2019-06-08 07:15:47,865|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long
2019-06-08 07:15:55,393|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from Certisign
2019-06-08 07:16:43,377|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from ovvalidator
2019-06-08 07:17:23,167|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from datum-sovrin
2019-06-08 07:18:31,380|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from xsvalidatorec2irl
2019-06-08 07:18:54,666|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1
2019-06-08 07:19:05,438|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from danube
2019-06-08 07:20:02,036|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai
2019-06-08 07:20:47,872|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long
2019-06-08 07:20:55,404|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from Certisign
2019-06-08 07:21:43,372|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from ovvalidator
2019-06-08 07:22:23,193|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from datum-sovrin
2019-06-08 07:23:31,384|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from xsvalidatorec2irl
2019-06-08 07:23:54,671|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1
2019-06-08 07:24:05,449|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from danube
2019-06-08 07:25:02,042|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from fetch-ai
2019-06-08 07:25:47,879|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 7 since State signatures are not updated for too long
{code}

SYGNET1 manages to enter a view change (this is a bug, INDY-2145)

{code}
2019-06-08 07:20:31,528|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from SYGNET1
{code}

OgNode connects again

{code}
2019-06-08 09:49:22,947|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode
{code}

OgNode broadcasts instance change, so we finally enter and try to finish a view change

{code}
2019-06-08 09:59:21,478|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from OgNode
2019-06-08 09:59:21,478|NOTIFICATION|view_changer.py|VIEW CHANGE: FoundationBuilder initiating a view change to 7 from 6
2019-06-08 09:59:21,806|INFO|view_changer.py|FoundationBuilder is sending ViewChangeDone msg to all : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [(0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'), (1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'), (2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'), (3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'), (1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP')], 'viewNo': 7}
{code}

Other nodes also entered and are trying to finish a view change. However new primary is uvs_val2, and it is offline, so despite having quorum of VIEW_CHANGE_DONE messages view change cannot be finished

{code}
2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from SYGNET1 : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from xsvalidatorec2irl : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from fetch-ai : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from ovvalidator : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,807|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from datum-sovrin : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,845|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from danube : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,859|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from OgNode : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-08 09:59:21,964|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from Certisign : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
{code}

All active nodes except SYGNET1 sent instance change indicating they think that view change is taking too long.

{code}
2019-06-08 10:06:21,488|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from xsvalidatorec2irl
2019-06-08 10:06:21,501|INFO|view_changer.py|VIEW CHANGE: FoundationBuilder sending an instance change with view_no 8 since View change could not complete in time
2019-06-08 10:06:21,505|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from OgNode
2019-06-08 10:06:21,506|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from fetch-ai
2019-06-08 10:06:21,543|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from datum-sovrin
2019-06-08 10:06:21,556|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from ovvalidator
2019-06-08 10:06:21,582|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from danube
2019-06-08 10:06:21,620|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from Certisign
{code}

SYGNET1 is still alive, and is either on a view 6 or already moving to view 7

{code}
2019-06-08 10:08:54,856|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from SYGNET1
{code}

But OgNode somehow managed to start a view change to view 8 (probably due to NDY-2145). At this point we have enough nodes to initiate and finish view changes, however nodes are stuck in different views - due to yet another problem INDY-2143. This is actually very similar to what happened during December outage of Sovrin MainNet.

{code}
2019-06-08 10:09:21,504|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from OgNode
2019-06-08 10:13:21,854|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 9} from OgNode
2019-06-08 10:14:21,511|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from OgNode
2019-06-08 10:19:21,515|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from OgNode
{code}

vnode1 connected, seemingly received quorum of instance changes, entered a view change, couldn't finish it because uvs_val2 is still offline....

{code}
2019-06-10 04:12:00,754|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to vnode1
2019-06-10 04:12:09,730|INFO|view_changer.py|FoundationBuilder's primary selector started processing of ViewChangeDone msg from vnode1 : VIEW_CHANGE_DONE{'name': 'uvs_val2', 'ledgerInfo': [[0, 32, '5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB'], [1, 109, '79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q'], [2, 86, 'GD6oWhf2ZGwmrKRCXrugyz7R1evmPVkrygs9dcBHN9xJ'], [3, 32368, 'HXCtwS6E38kuyBqtwf5Aq2Vg5qBzboqkSm8Em6z82Gfe'], [1001, 10, 'Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP']], 'viewNo': 7}
2019-06-10 04:19:09,443|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 8} from vnode1
{code}

However it seems like it was able to start a view change to view 8 (just like OgNode)

{code}
2019-06-10 04:21:58,644|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from vnode1
2019-06-10 04:26:09,481|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 28, 'viewNo': 9} from vnode1
2019-06-10 04:26:58,659|INFO|view_changer.py|FoundationBuilder received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 8} from vnode1
{code}

There were several restarts of vnode1 and OgNode but they didn't change anything

{code}
2019-06-10 13:35:46,109|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from vnode1
2019-06-10 13:36:45,674|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to vnode1
2019-06-10 15:57:49,370|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode
2019-06-10 15:58:04,579|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode
2019-06-10 16:01:02,002|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder disconnected from OgNode
2019-06-10 16:01:30,750|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode
{code}

Seems like it was time when pool restart action was issued, so everyone restarted

{code}
2019-06-10 16:07:30,003|INFO|looper.py|Starting up indy-node
2019-06-10 16:07:31,921|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to ovvalidator
2019-06-10 16:07:32,452|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to xsvalidatorec2irl
2019-06-10 16:07:33,717|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to fetch-ai
2019-06-10 16:07:35,471|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to SYGNET1
2019-06-10 16:07:35,984|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to datum-sovrin
2019-06-10 16:07:36,511|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to OgNode
2019-06-10 16:07:36,525|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to Certisign
2019-06-10 16:07:36,978|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to vnode1
2019-06-10 16:07:42,439|NOTIFICATION|keep_in_touch.py|CONNECTION: FoundationBuilder now connected to danube
{code}

And it helped nodes to reach consensus and start ordering transactions (including freshness checks)

{code}
2019-06-10 16:07:30,843|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService starting catchup (is_initial=True)
2019-06-10 16:07:37,267|INFO|node.py|FoundationBuilder caught up to 0 txns in the last catchup
2019-06-10 16:07:37,267|INFO|replica.py|FoundationBuilder:0 set last ordered as (6, 5961)
2019-06-10 16:07:37,268|INFO|node.py|FoundationBuilder started participating
2019-06-10 16:07:38,436|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5962, ledger 2, state root EDHc8pEcGBq9tTy8zbEA1wFoZy3an5eMo3VJN16Rv2ec, txn root 4m1yqXFZ7U2ffM3CBqaTGM2e4S2adQQAoq4xbJ2CkAgC, audit root DpkEr9R6xXKJAwePP23ziApsdLamjBvGDAaypD7AWyWZ, requests ordered 1, discarded 0
2019-06-10 16:12:35,452|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5963, ledger 0, state root CGuyq6T4ASsqDbSJVn7sAhhybjD4rhjpxSAQkjuPo2Tx, txn root 5QEs5tSKGoMeW7mnQXAqHDrurVsLFumfJMPay1qu5zSB, audit root 4RQKtFKnK6UkKSh81FSps55NCBBnQnPd2JjE61sCE6DN, requests ordered 0, discarded 0
2019-06-10 16:12:35,460|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5964, ledger 1, state root 78LWbD7o46dA2C8cYT8da1kRtLwnsXwJkch5obWdqQLd, txn root 79YZKB72LSY68dbuYMvFk1rAyRfnMbUdSwVAtuj2vR5q, audit root Dqw3mCtY7sBXo1hF3kVzrtXNAV4SrjqQ1P1WYy1QC2A5, requests ordered 0, discarded 0
2019-06-10 16:12:35,468|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5965, ledger 1001, state root 7Mut4DkZsjAzJwxNRFWmFNN5zi3v9wj3xL28TpbaWDXG, txn root Hd5dtr5cwAodDUdPDUZkdMCHdaR1KdiqQLBUUxCQwRrP, audit root CLQSA2YdcU294SNNNqDnkRxdNCFwKKypoWU8zxn7fGVe, requests ordered 0, discarded 0
2019-06-10 16:12:38,392|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5966, ledger 2, state root EDHc8pEcGBq9tTy8zbEA1wFoZy3an5eMo3VJN16Rv2ec, txn root 4m1yqXFZ7U2ffM3CBqaTGM2e4S2adQQAoq4xbJ2CkAgC, audit root 8tx43aBiMsz5LH8aEv37B1cjf12KP5GDuBpr2uWe7h91, requests ordered 0, discarded 0
2019-06-10 16:15:20,271|INFO|replica.py|FoundationBuilder:0 ordered batch request, view no 6, ppSeqNo 5967, ledger 0, state root FkQAzxr5MHazn9PD9za19GpgHehMsF1rt55fPgf82Cov, txn root 3A5n3zSubMUb2yrt8b6Fui69cbnWTJqpGkuaprU7ZqFJ, audit root HEdseVCJsv46A9q9DtJ3wbWVVRgTxazCgWcU1HPLayRD, requests ordered 1, discarded 0
{code}
;;;","14/Jun/19 9:45 PM;sergey.khoroshavin;*Conclusion*
Nodes failed to reach consensus before pool restart due to combination of two different bugs, corresponding issues are opened:
* INDY-2143 - instance changes for view change timeout are sent only once, but should be sent periodically
* INDY-2145 - very old instance messages are sometimes not discarded;;;",,,,,,,,,,,,,,,,,,
Update PBFT view change plan of attack,INDY-2134,40517,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey.khoroshavin,sergey.khoroshavin,11/Jun/19 9:54 PM,01/Oct/19 7:57 PM,28/Oct/23 2:47 AM,17/Jun/19 4:52 PM,,,,,,,,0,,,,,"*Acceptance criteria*
High level plan of attack for PBFT view change should be created. It should be based on plans created a year ago and take into account changes that happened since then.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwqon:",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"17/Jun/19 4:43 PM;ashcherbakov;*The Plan is updated:*
 # *Sprint 19.12 (06/21):*
 ** INDY-1338: Define Interfaces needed for View Change Service (3 SP) - SK
 # *Sprint 19.13 (07/05):*
 ** INDY-2147: Implement PBFT viewchanger service with most basic functionality (3 SP) - SK
 ** INDY-2139: Extract and integrate ConsensusDataProvider from Replica (5 SP) - AN
 ** INDY-2135: Simulation tests for View Changer (no integration) (3 SP) - SK
 # *Sprint 19.14 (07/19):*
 ** INDY-1337: Modify WriteReqManager to meet Executor interface needs (2 SP) - AS
 ** INDY-2136: Extract Orderer service from Replica (5 SP) - AN
 ** INDY-2137: Extract Checkpointer service from Replica (3 SP) - SK
 # *Sprint 19.15 (08/02):*
 ** INDY-2148: Integrate Orderer and Checkpointer into existing code base (8 SP) - RT/AN
 ** INDY-1335: Enable full ordering of batches from last view that were already ordered, make execution on replicas that executed them no-op (5 SP) - AS
 # *Sprint 19.16 (08/16):*
 ** INDY-2149: Integrate and run simulation tests with Orderer, Checkpointer, Ledger (5 SP) - SK
 ** INDY-1340: Implementation: Integrate PBFT viewchanger service into current codebase (5 SP) - AN
 # *Sprint 19.17 (08/30):*
 ** INDY-2150: Integrate view change property-based tests into CI (3 SP) - AN
 ** INDY-2140: Debug: Integrate PBFT viewchanger service into current codebase (5 SP) - SK
 ** INDY-2138: Document PBFT view change protocol (3 SP) - AS
 # *Sprint 19.18 (09/13):*
 ** INDY-2146: Load testing: PBFT View Change (5 SP) - VS/SK/AS/AN

 ;;;",,,,,,,,,,,,,,,,,,,,,
Simulation  tests for View Changer (no integration),INDY-2135,40523,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,12/Jun/19 12:11 AM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,05/Jul/19 8:49 PM,,,1.10.0,,,,,0,,,,,"*Summary*

We need to implement property-based tests for the new View Changer.


 Original PBFT paper assumes that same batches can be ordered on different nodes with different views. However since we write batch parameters to audit ledger it can diverge on different nodes after view change, which is a big problem.

There could be different solutions to this problem, the most promising idea is not to increase view no of batches that came from old view, and probably add some flag to them. However this is a serious deviation from original protocol and should be analyzed thoroughly.

*Acceptance criteria*
 * protocol update should be designed and proved to be safe
 * proof should be confirmed by randomized simulation tests
",,,,,,,,,,INDY-1338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969ui",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,"08/Jul/19 6:12 PM;sergey.khoroshavin;*PR*
https://github.com/hyperledger/indy-plenum/pull/1246
;;;",,,,,,,,,,,,,,,,,,,,,
Extract Orderer service from Replica,INDY-2136,40547,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,sergey.khoroshavin,sergey.khoroshavin,13/Jun/19 5:15 PM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,19/Jul/19 3:43 PM,,,1.10.0,,,,,0,,,,,,,,,,,,,,,INDY-2139,,,,INDY-1335,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9",,,,Unset,Unset,Ev-Node 19.14,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"11/Jul/19 8:24 PM;ashcherbakov;*PoA*
 * Step 1: Create Orderer Service. It needs to be injected into Replica
 ** It needs to have access to External and Internal buses
 ** It needs to have access to ConsensusDataProvider
 ** It needs to have access to WriteRequestManager
 ** It needs to have StashingRouter instance (see ViewChangerService)
 ** It needs to have 3PCMsgValidator instance
 *** This is a copy of ReplicaValidator where chedkpoint-related logicis removed
 ** _Do not have any references to Node instance!_
 * Step 2: Register 3PC messages processing in External Buses and Stashing Router
 ** just stubs for process methods
 ** call 3PCMsgValidator at the beginning of each method, and return id the returned value != PROCESS (see ViewChangerService)
 * Step 3: Implement process methods for every 3PC message
 ** it can be mostly copy-paste from Repica, but without any usage of Node
 ** Call WriteRequestManager to apply and reject messages
 ** Use internal bus for communication with the Node. 
 ** Use external bus (network) for communication with other nodes. For now the handler for external bus can be a legacy (replica or node) code.
 * Step 4: integrate OrdererService into the Replica (will be done in the scope of INDY-2169)
 ** Bridge legacy logic to the Orderer's service buses
 ** Remove OrdererService-related code from Replica.;;;","19/Jul/19 3:43 PM;ashcherbakov;Steps 1 -3 have been done in PR https://github.com/hyperledger/indy-plenum/pull/1266;;;",,,,,,,,,,,,,,,,,,,,
Extract Checkpointer service from Replica,INDY-2137,40548,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,sergey.khoroshavin,sergey.khoroshavin,13/Jun/19 5:16 PM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,01/Aug/19 9:46 PM,,,1.10.0,,,,,0,,,,,"* Checkpointer Service needs to deal with all Checkpoints-related logic (creation, processing and stabilization of checkpoints as well as updating of watermarks)
 * Checkpointer Service code needs to be taken from Replica into a separate class
 * ConsensusSharedData needs to be used for a common data needed for multiple services (such as Ordering, Checkpointer, View Change)
 ** In particular, watermarks need to be there
 * No references to Replica or Node classes can be there
 * All network communication must be done via ExternalBus
 ** It needs to subscribe to Checkpoint messages and process them
 * Stashing must be implemented via StashingRouter
 * All internal (between services) communication must be done via InternalBus
 * Tests must be adopted


Docs
* https://github.com/hyperledger/indy-plenum/blob/master/design/plenum_2_0_architecture_class.png
* https://github.com/hyperledger/indy-plenum/blob/master/design/plenum_2_0_architecture_object.png
* https://github.com/hyperledger/indy-plenum/blob/master/design/plenum_2_0_communication.png",,,,,,,,,,INDY-2139,,,,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwqof:",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,Toktar,,,,,,,,,,"26/Jul/19 9:57 PM;Toktar;*PoA:*

Implement the CheckpointService. Public API:
 * process_checkpoint (process external Checkpoint messages)
 * process_ordered (process internal Ordered messages)
 * caught_up_till_3pc  (process internal StartBackupCatchup messages)
 * update_watermark_from_3pc
 * reset_checkpoints

PR: [https://github.com/hyperledger/indy-plenum/pull/1279];;;","01/Aug/19 9:46 PM;Toktar;Will be tested in the INDY-2179;;;",,,,,,,,,,,,,,,,,,,,
Document PBFT view change protocol,INDY-2138,40551,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,sergey.khoroshavin,sergey.khoroshavin,13/Jun/19 5:46 PM,24/Dec/19 1:44 AM,28/Oct/23 2:47 AM,20/Dec/19 10:11 PM,,,1.12.1,,,,,0,,,,,"* Add docs for thew new view change protocol (docs, sequence diagrams)
* Update catchup docs (to get rid of dependency between view change and catchup)
* Update audit ledger docs to include info about `digest` and `node_reg`",,,,,,,,,,,,,,,,,,,,INDY-1155,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41huo",,,,Unset,Unset,Ev-Node 19.24,Ev-Node 19.25,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"20/Dec/19 10:12 PM;ashcherbakov;PR: [https://github.com/hyperledger/indy-plenum/pull/1431]

 ;;;",,,,,,,,,,,,,,,,,,,,,
Extract and integrate ConsensusDataProvider from Replica,INDY-2139,40552,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,13/Jun/19 5:56 PM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,17/Jul/19 5:35 PM,,,1.10.0,,,,,0,,,,,,,,,,,,,,,INDY-1338,,,,INDY-2136,INDY-2137,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969uo",,,,Unset,Unset,Ev-Node 19.13,Ev-Node 19.14,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,"17/Jul/19 5:05 PM;Derashe;Problem reason/description: 
- We need to move all consesus data into separated module named ConsensusSharedData

Changes: 
- Consensus data separated

PR:
- [https://github.com/hyperledger/indy-plenum/pull/1262]

Version:
- 841

 

Covered with tests:
- [test_consensus_dp_batches.py|https://github.com/hyperledger/indy-plenum/pull/1262/files#diff-6255cabd306c887e3e5413046e8b96a3];;;",,,,,,,,,,,,,,,,,,,,,
Debug: Integrate PBFT viewchanger service into current codebase,INDY-2140,40553,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,13/Jun/19 6:01 PM,14/Oct/19 2:33 PM,28/Oct/23 2:47 AM,12/Oct/19 1:44 AM,,,1.11.0,,,,,0,,,,,"Acceptance criteria:
 * Makse sure there are no critical/major issues with View Change
 * Unskip tests we added long time ago that failed with the old view change implementation.
 * Make sure all existing tests are unskipped/rewritten/removed and pass
 ** Remove tests related to old view change if necessary
 * Remove code related to old View Change logic

 

Tasks:
 # Unskip all tests (create a ticket if a test can not be unskipped by a reason) - [~sergey.khoroshavin]
 # Code cleanup [~sergey.khoroshavin]
 ** Make sure that `primaries_batch_needed ` is used correctly - DONE
 ** `future_primary_handler.set_node_state()` is called from at least 4 places - DONE
 ** Do we need `self.schedule_view_change_completion_check(self._view_change_timeout)` in `on_view_change_start` - IN PROGRESS
 ** Do we need `replica.on_view_change_start()` - IN PROGRESS
 ** Do we need `primaryChanged` in replica - TBD
 ** Remove `notify_view_change_complete` - TBD
 ** ""VCDone_queue"" in validator info output - _DONE_
 ** Should we still call self.instance_changes.remove_view(self.view_no) - _DONE_
 # Refactor Future Primaries batch needed [~Toktar] - DONE
 # Remove old view change logic [~sergey.khoroshavin] - IN PROGRESS
 # Unskip view change written long time ago that failed with old protocol [~Toktar] - DONE
 # Analyze load tests results [~anikitinDSR] - DONE",,,,,,,,,,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vj",,,,Unset,Unset,Ev-Node 19.20,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,sergey.khoroshavin,Toktar,,,,,,,,"01/Oct/19 10:27 PM;ashcherbakov;Cleanup tasks:
 * Make sure that `primaries_batch_needed ` is used correctly
 * `future_primary_handler.set_node_state()` is called from at least 4 places
 * Do we need `self.schedule_view_change_completion_check(self._view_change_timeout)` in `on_view_change_start`?
 * Do we need `replica.on_view_change_start()`?
 * Do we need `primaryChanged` in replica
 * ""VCDone_queue"" in validator info output
 * Should we still call {{self.instance_changes.remove_view(self.view_no)}}?;;;","01/Oct/19 11:00 PM;ashcherbakov;Items in tests:
 * `test_nodes_removes_request_keys_for_ordered`: why check for `last_ordered` is removed?
 * Why `test_instance_change_from_unknown` and `test_instance_change_from_known`  are skipped? I think we do use INSTANCE_CHANGE msgs.
 * Why `sdk_ensure_pool_functional` is commented in `test_view_change_with_delayed_commits`?;;;","02/Oct/19 10:01 PM;anikitinDSR;Directories with tests:
 view_change -  {color:#00875a}DONE (merged)
 {color:#172b4d}PR:{color}{color}
 [https://github.com/hyperledger/indy-plenum/pull/1366];;;","02/Oct/19 10:41 PM;Toktar;All directories with tests excluding:
 * view_change 
 * primary_selected

{color:#00875a}DONE (a few tests waiting INDY-2229){color};;;","03/Oct/19 6:55 PM;sergey.khoroshavin;Doing cleanups of old view change code, modifying in process tests from primary_selection folder;;;","14/Oct/19 2:33 PM;sergey.khoroshavin;DONE PRs:
https://github.com/hyperledger/indy-plenum/pull/1357
https://github.com/hyperledger/indy-plenum/pull/1360
https://github.com/hyperledger/indy-plenum/pull/1362

In progress PR moved to INDY-2244:
https://github.com/hyperledger/indy-plenum/pull/1371
;;;",,,,,,,,,,,,,,,,
Grab pool data for failed system tests,INDY-2141,40561,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,andkononykhin,andkononykhin,13/Jun/19 10:07 PM,24/Jul/19 9:36 PM,28/Oct/23 2:47 AM,24/Jul/19 9:36 PM,,,,,,,,0,system-tests,,,,"As of now failed system tests are not supplied with any pool data. It makes hard to understand the reasons of failures. It would be useful to grab such data as node logs, journal logs and metrics from the tests pools.

Concerns:
 # some failures are not related to pool functionality and client-pool communication but rather to python code (e.g. syntax errors, key/index errors)
 # in case of many failures there would be too many data, options:
 ** archive data
 ** filter data (e.g. by time, by node, by request id)
 ** omit some data (e.g. there is a fixture to check failures in journal logs, thus these logs might be omitted if no such errors detected)

Acceptance criteria:
 # pool data should be enough to understand system tests failures
 # CD pipeline should be updated to attach that data to pipeline results",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2165,,,No,,Unset,No,,,"1|i00t7f:",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,sergey.khoroshavin,,,,,,,,,,"13/Jun/19 10:16 PM;sergey.khoroshavin;I think the best way to handle pool data (journalctl, node logs and ledgers) is to archive it all AND log somehow start and failure timestamps for each system test;;;","24/Jul/19 12:46 AM;andkononykhin;Problem reason:
 - hard to understand reason of failed tests without logs from nodes

Changes:
 * improved test-automation repo to support optional logs gathering from nodes, for now the following files are collected:
 ** logs from ""/var/log/indy/sandbox/""
 ** files from ""/var/lib/indy/sandbox/"" (genesis and validator info files)
 * updated indy-node pipeline to attach garthered files to list of build artifacts
 * *note*: for now journalctl and ledgers are not gathered

PR:
 * https://github.com/hyperledger/indy-test-automation/pull/45
 * [https://github.com/hyperledger/indy-test-automation/pull/46]
 * https://github.com/hyperledger/indy-node/pull/1386

Version:
 * indy-test-automation, v. 0.8.7
 * indy-node, v. 1.9.0.dev1038

Risk factors:
 * impacts cd pipeline

Risk:
 - Low

Covered with tests:
 - no

Recommendations for QA
 - check the following builds results:
 ** [https://build.sovrin.org/job/test-pipelines/job/indy-node-nightly-test/59/]
 ** https://build.sovrin.org/job/indy-node/job/indy-node-cd/job/master/1038/;;;",,,,,,,,,,,,,,,,,,,,
There is no validation of the ISSUANCE_TYPE field for the transaction REVOC_REG_DEF,INDY-2142,40574,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,Toktar,Toktar,14/Jun/19 1:38 AM,14/Jun/19 9:31 PM,28/Oct/23 2:47 AM,14/Jun/19 8:23 PM,,,1.9.0,,,,,0,TShirt_S,,,,"The field ISSUANCE_TYPE  in the transaction REVOC_REG_DEF can take one of 2 values: ISSUANCE_BY_DEFAULT or ISSUANCE_ON_DEMAND
Now there is no verification that this field contains the correct value.


*Acceptance criteria:*
Add static validation for field ISSUANCE_TYPE .
Add a test with the check of sending an incorrect value in the field ISSUANCE_TYPE .",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|i00rf0:",,,,Unset,Unset,Ev-Node 19.12,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,,,,,,,,,,,"14/Jun/19 6:47 PM;Toktar;PR: [https://github.com/hyperledger/indy-node/pull/1348];;;","14/Jun/19 7:50 PM;Toktar;Problem reason:
 * If client send REVOC_REG_DEF with incorrect ISSUANCE_TYPE and send REVOC_REG_ENTRY with this revoc_reg_def, node break down, because the class for revoc strategy can't be created. 

Changes:
 * Add a static validation for ISSUANCE_TYPE  for REVOC_REG_DEF client messages
 * Add test

PR:
 * [https://github.com/hyperledger/indy-node/pull/1348]

Version:
 * indy-node 1.9.0~dev1001 master

Risk factors:
 * REVOC_REG_ENTRY, REVOC_REG_DEF txns

Risk:
 * Low

Covered with tests:
 * [test_incorrect_revoc_reg_def.py|https://github.com/hyperledger/indy-node/pull/1348/files#diff-7508074f0a1b0b74fc7b71f480448d80] 

Recommendations for QA:
 1) Test correct REVOC_REG_ENTRY, REVOC_REG_DEF requests 
 2) Test case:
 * send REVOC_REG_DEF with incorrect ISSUANCE_TYPE (random string);
 * check a NACK messages in the response;
 * check that the incorrect REVOC_REG_DEF transaction was not recorded in the ledger.

 ;;;",,,,,,,,,,,,,,,,,,,,
When view change takes too long instance change should be sent periodically,INDY-2143,40584,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,sergey.khoroshavin,sergey.khoroshavin,14/Jun/19 5:16 PM,19/Jul/19 9:59 PM,28/Oct/23 2:47 AM,19/Jul/19 9:59 PM,,,1.9.1,,,,,0,,,,,"*Summary*
 Currently when view change takes too long instance change is sent only once. However due to discarding old instance change messages (>2 hours old) this can sometimes lead to failure to start a view change. This already has happened on BuilderNet (see  INDY-2133)

*Acceptance criteria*
 * implement periodic sending of instance changes
 * cover by tests",,,,,,,,,,,,,,,,,,,,,INDY-2133,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98",,,,Unset,Unset,Ev-Node 19.14,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,sergey.khoroshavin,,,,,,,,,,"17/Jul/19 8:21 PM;Derashe;PoA:

1st option: Check that throttler block second instance change. If this can be fixed, avoid 2nd option.

2nd option:
 * We need to reproduce issue in test environment. Using details mentioned in INDY-2133, we can write such a test:
 ** Pool of 7 nodes. f = 2. VCH_D quorum = 5. I_CH quorum = 5.
 ** 2 nodes are ofline
 ** view_change initiated
 ** 4 nodes send I_CH, fifth send I_CH also, but it fails to deliver
 ** fifth node collect quorum for I_CH and send VCH_D.
 ** other nodes can't collect quorum and start new view
 * Check that above tests fails
 * Implement fix - make a node, which has view_change in progress periodically send I_CH again for this ""in progress view""
 * Set appropriate timeout for new feature in the test and recheck it. After this timeout new I_CH should be sent and new view successfully achieved;;;","19/Jul/19 9:14 PM;Derashe;Problem reason/description:
 - Pool can stuck in a troubled situation. It can happen when pool is in progress of unsuccessfull view change and has some view (1 for example) and it is trying to send INSTANCE_CHANGE for the next view (2). But some nodes didn't send I_CH or I_CH was lost. Previous logic assumed only resending I_CH for current view i.e. 1 view.

Changes: 
 - So, after view_change started, new event scheduled. This event checks if view_change is incomplete, than it send I_CH for new view again. INSTANCE_CHANGE_RESEND_TIMEOUT stands for periods of sending

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1270]

Version:
 - plenum 847

Covered with tests:
 - [https://github.com/hyperledger/indy-plenum/pull/1270/files#diff-5eafcbdb347b4bddaa43cea30bd7120f]

Recommendations for QA
 * Please, be aware of situations that can appear in scope of this problem. You can look at it in the test ahead 

 ;;;",,,,,,,,,,,,,,,,,,,,
Propagates with invalid requests can lead to node crashes,INDY-2144,40586,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,14/Jun/19 7:13 PM,05/Jul/19 11:44 PM,28/Oct/23 2:47 AM,25/Jun/19 12:45 AM,,,1.9.0,,,,,0,TShirt_S,,,,"Relevant part of example traceback:
{code}
cze 13 02:48:13 sovrin1 env[75668]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2737, in processPropagate
cze 13 02:48:13 sovrin1 env[75668]:     self.handle_request_if_forced(request)
cze 13 02:48:13 sovrin1 env[75668]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2599, in handle_request_if_forced
cze 13 02:48:13 sovrin1 env[75668]:     req_handler.validate(request)
cze 13 02:48:13 sovrin1 env[75668]:   File ""/usr/local/lib/python3.5/dist-packages/sovtokenfees/static_fee_req_handler.py"", line 167, in validate
cze 13 02:48:13 sovrin1 env[75668]:     super().validate(request)
cze 13 02:48:13 sovrin1 env[75668]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/config_req_handler.py"", line 131, in validate
cze 13 02:48:13 sovrin1 env[75668]:     raise InvalidClientRequest(req.identifier, req.reqId, res)
cze 13 02:48:13 sovrin1 env[75668]: plenum.common.exceptions.InvalidClientRequest: InvalidClientRequest('Version 1.1.46 is not upgradable',)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969g",,,,Unset,Unset,Ev-Node 19.12,Ev-Node 19.13,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,sergey.khoroshavin,VladimirWork,,,,,,,,,"19/Jun/19 11:46 PM;Derashe;Problem reason/description:
 - Forced pool upgrade caused node to fall

Changes:
 - additional validation and handling for forced requests

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1243 |https://github.com/hyperledger/indy-plenum/pull/1243]
 - [https://github.com/hyperledger/indy-node/pull/1351]

Risk factors:
 - no

Risk:
 - Low

Covered with tests:
 - indy_node/test/upgrade/test_node_sustain_invalid_upgrade_txn.py;;;","25/Jun/19 12:44 AM;VladimirWork;Build Info:
indy-node 1.9.0~dev1008

Steps to Validate:
1. Stop any node in the pool.
2. Schedule forced upgrade from 1.9.0~dev1007 to 1.9.0~dev1008 and wait for it's finish.
3. Start stopped node.
4. Schedule another forced upgrade just for 1 node that was stopped and wait for it's finish.

Actual Results:
All nodes are upgraded successfully. There are no exceptions in journalctl.

Additional Info:
Upgrade *to* 1007 version also works as expected.;;;",,,,,,,,,,,,,,,,,,,,
Old instance change messages are sometimes not discarded,INDY-2145,40587,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,14/Jun/19 8:57 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,,,,,"Logs from one node from BuilderNet show following:
{code}
2019-06-03 13:31:49,271|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from makolab012019-06-03 2019-06-03 16:38:52,294|INFO|view_changer.py|VIEW CHANGE: SYGNET1 sending an instance change with view_no 7 since State signatures are not updated for too long
2019-06-03 16:38:52,294|INFO|instance_change_provider.py|InstanceChangeProvider: Discard InstanceChange from makolab01 for ViewNo 7 because it is out of date (was received 11223sec ago)
2019-06-03 16:40:18,790|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from validatedid
2019-06-03 16:40:36,662|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from uvs_val2

2019-06-08 07:10:01,977|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from fetch-ai
2019-06-08 07:10:47,969|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from FoundationBuilder
2019-06-08 07:10:55,408|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from Certisign
2019-06-08 07:11:43,313|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from ovvalidator
2019-06-08 07:12:23,055|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from datum-sovrin
2019-06-08 07:13:31,331|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from xsvalidatorec2irl
2019-06-08 07:13:31,331|NOTIFICATION|view_changer.py|VIEW CHANGE: SYGNET1 initiating a view change to 7 from 6
{code}

It seems like discarding mechanism for old instance change messages is active (it did fired on old makolab01 instance change message), however it didn't fire on many other messages, which led to false view change.","

{code}
2019-06-03 16:38:52,294|INFO|view_changer.py|VIEW CHANGE: SYGNET1 sending an instance change with view_no 7 since State signatures are not updated for too long
2019-06-03 16:38:52,294|INFO|instance_change_provider.py|InstanceChangeProvider: Discard InstanceChange from makolab01 for ViewNo 7 because it is out of date (was received 11223sec ago)
2019-06-03 16:40:18,790|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from validatedid
2019-06-03 16:40:36,662|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'reason': 43, 'viewNo': 7} from uvs_val2

2019-06-08 07:10:01,977|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from fetch-ai
2019-06-08 07:10:47,969|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from FoundationBuilder
2019-06-08 07:10:55,408|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from Certisign
2019-06-08 07:11:43,313|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from ovvalidator
2019-06-08 07:12:23,055|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from datum-sovrin
2019-06-08 07:13:31,331|INFO|view_changer.py|SYGNET1 received instance change request: INSTANCE_CHANGE{'viewNo': 7, 'reason': 43} from xsvalidatorec2irl
2019-06-08 07:13:31,331|NOTIFICATION|view_changer.py|VIEW CHANGE: SYGNET1 initiating a view change to 7 from 6
{code}",,,,,,,,,,,,,,,,,,,,INDY-2133,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c94",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Load testing: PBFT View Change,INDY-2146,40605,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/Jun/19 4:38 PM,11/Oct/19 9:43 PM,28/Oct/23 2:47 AM,11/Oct/19 9:43 PM,,,1.11.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/19 9:12 PM;VladimirWork;Figure1.png;https://jira.hyperledger.org/secure/attachment/17870/Figure1.png","09/Oct/19 9:12 PM;VladimirWork;Figure25.png;https://jira.hyperledger.org/secure/attachment/17871/Figure25.png","01/Oct/19 8:50 PM;VladimirWork;audit_vc_issue_01_10_2019.tar.gz;https://jira.hyperledger.org/secure/attachment/17860/audit_vc_issue_01_10_2019.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w1",,,,Unset,Unset,Ev-Node 19.19,Ev-Node 19.20,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"01/Oct/19 7:46 PM;VladimirWork;Build Info:
indy-node 1.10.0~dev1094

Steps to Reproduce:
1. Run load test without fees and tokens with forced VCs (1800 seconds).

Actual Results:
Pool performed 10 VCs in 30 minutes after the first one.

ev@evernymr33:logs/2146_pbft_no_tokens_forced_vc.tar.gz
ev@evernymr33:logs/performance_results_pbft_forced_01_10_2019.tar.gz;;;","01/Oct/19 8:50 PM;VladimirWork;Build Info:
indy-node 1.10.0~dev1094

Steps to Reproduce:
1. Stop primary (and ensure that primary has changed).
2. Start ex-primary and demote backup primary (and ensure that primary has changed).
3. Promote demoted node (and ensure that primary has changed).

https://github.com/hyperledger/indy-test-automation/blob/f3c0c75e04f416a81f22025146bc5ca9b1e2f718/system/indy-node-tests/TestAuditSuite.py#L153

Actual Results:
Primary didn't change in Step 3 (but before PBFT this test passed). [^audit_vc_issue_01_10_2019.tar.gz] ;;;","01/Oct/19 8:51 PM;VladimirWork;FYI [~ashcherbakov] [~sergey.khoroshavin] [~anikitinDSR] [~Toktar];;;","03/Oct/19 5:52 AM;VladimirWork;Build Info:
indy-node 1.10.0~dev1094

Steps to Reproduce:
1. Run load test (5 txns/sec load rate) with forced VCs (1800) and debug logs.
2. Run load test (10 txns/sec load rate) with forced VCs (1800) and debug logs.

Actual Results:
There are too many VCs during both load tests (30 actual against 6 expected for the last case for example).

ev@evernymr33:logs/pbft_5_per_sec_debug.tar.gz
ev@evernymr33:logs/pbft_10_per_sec_debug.tar.gz;;;","09/Oct/19 9:12 PM;VladimirWork;Build Info:
indy-node 1.11.0~dev1101
plugins 1.0.4~dev102

Steps to Reproduce:
1. Run production load test (with tokens and fees) with forced VCs (1800).

Actual Results:
Pool sustained the load for 8+ hours but we have an issue with 2 and 3 VCs in a row. !Figure1.png|thumbnail!  !Figure25.png|thumbnail! 
ev@evernymr33:logs/pbft_forced_vc_08_10_2019.tar.gz;;;","11/Oct/19 1:01 AM;VladimirWork;INDY-2247 was reported for VC issues found in TestAuditSuite.;;;",,,,,,,,,,,,,,,,
Implement PBFT viewchanger service with most basic functionality ,INDY-2147,40609,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,17/Jun/19 6:32 PM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,19/Jul/19 3:47 PM,,,1.10.0,,,,,0,,,,,"Finalize PBFT View Change Service Implementation. Use interfaces for 3PC State and Network.
 * 3PC State <-> Orderer
 * 3PC State <->Checkpointer
 * 3PC State <-> View Changer
 * Networker <-> Orderer
 * Networker <->Checkpointer
 * Networker <-> View Changer

This better be done using TDD, implementing mocks for network, executor (WriteReqManager), 3PCState, Orderer and Checkpointer as needed.

*Acceptance Criteria:*
 * View Changer Implementation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969ub3",,,,Unset,Unset,Ev-Node 19.13,Ev-Node 19.14,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"15/Jul/19 2:33 PM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/1267;;;","19/Jul/19 3:47 PM;ashcherbakov;- PBFT view change has been implemented according to the protocol 
- Base simulation and unit tests pass

PRs:
- https://github.com/hyperledger/indy-plenum/pull/1267
- https://github.com/hyperledger/indy-plenum/pull/1268;;;",,,,,,,,,,,,,,,,,,,,
Create simulation tests for catchup services,INDY-2148,40610,,Task,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,17/Jun/19 6:35 PM,28/Jan/20 11:41 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"* Use service and bus oriented approach for catchup
* Cleanup catchup logic
* Write sim tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqer",,,,Unset,Unset,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,sergey.khoroshavin,,,,,,,,,"22/Nov/19 7:14 PM;anikitinDSR;Aslo, what we have to do in catchup_service in terms of simulation tests:
 * Move all on_start and on_finish stuffs  from node's side
 * Move select_primaries_after_catchup into internal_bus using. Maybe it should be divided into separate steps or moved to view_change service.;;;","08/Jan/20 9:58 PM;sergey.khoroshavin;*PoA*

* Make catchup services available in simulation tests
** Preliminary refactorings
*** Make master instance buses available in LedgerManager (this is going to be temporary)
*** Replace pre/post catchup callbacks with internal messages
*** Replace postTxnAddedToLedger callback with internal message? (not sure)
*** Make WriteReqHandler and DatabaseManager available in LedgerManager
*** Remove ledger registering functionality from LedgerManager
** Refactor SeederService so that it can be used in simulation tests
*** Remove dependency on channels, subscribe to buses instead
*** Remove dependency on CatchupDataProvider, use DatabaseManager instead
*** Move SeederService out of LedgerManager, create it in:
**** Master instance of Replica (I know this is dirty, and I think it is definitely a subject of discussion, but in order to move it to Node more refactorings would be required, also it would be harder to maintain compatibility with ReplicaService)
**** ReplicaService, so that it is available in simulation tests
** Refactor NodeLeecherService so that it can be used in simulation tests
*** Remove dependency on channels, subscribe to buses instead
**** Would be nice to remove dependency on channels for LedgerLeecherService, ConsProofService and CatchupRepService
*** Remove dependency on CatchupDataProvider, use WriteRequestHandler instead
*** Move NodeLeecherService out of LedgerManager, create it in same places as SeederService (with same rationale)
* Create catchup simulation tests
** Generic catchup simulation test
*** Make one node severely lag behind by temporarily discarding all messages sent to it
*** Use random delays for message delivery
*** Allow randomly lost messages
*** Allow up to n-f-1 offline nodes
*** Simulate malicious nodes?
**** Pretending they have different data
**** Pretending they are ahead of pool
*** Allow ordering during catchup
*** Make sure node catches up to f+1's highest ledger
*** Make sure node can order after finishing catchup if there are more than n-f online nodes (small number of online nodes probably should be a different test case)
** Demotion/promotion simulation tests
* Cleanups
** Probably LedgerManager can be removed after those changes
;;;",,,,,,,,,,,,,,,,,,,,
Integrate and run PBFT View Changer simulation  tests with a real implementation,INDY-2149,40611,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,17/Jun/19 6:38 PM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,14/Sep/19 1:07 AM,,,1.10.0,,,,,0,,,,,"*Acceptance criteria:*
 * Property-based tests need to be integrated with a real (no mocks) implementations of 

 ** View Changer Service
 ** 3PC State
 ** Orderer service
 ** Checkpointer service
 ** WriteReqManager
 * Database manager needs to use real instances of Ledgers and states (especially audit ledger one). It can use in-memory storages to speed-up tests.
 * Network needs to be mocked up.",,,,,,,,,,INDY-2200,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vu",,,,Unset,Unset,Ev-Node 19.18,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,sergey.khoroshavin,,,,,,,,,"31/Aug/19 1:34 AM;sergey.khoroshavin;A must have simulation test:
 * randomly schedule starts of batch ordering (I think 5 batches would be enough)
 * randomly schedule some view changes (3 is enough, but it should be possible for them to interleave with batch creation)
 * randomly simulate sequential view changes (new primary not responded?) so that P and Q contain batches from multiple views and view_no != pp_view_no
 * wait until all view changes are done and all batches are ordered (sim framework will take care of randomly delaying inter-node messages so that if there are flaws in messages processing they will be most certainly caught)
 * random delays in inter-node messages should affect ViewChange messages as well, so that asking ViewChanges from other nodes must be required.
 * make sure all nodes have same state and are in consensus

There might be need to create or mock BlsBft and BlsBftReplica in test start up code. Look for SimPool and TestLedgersBootstrap classes.;;;","31/Aug/19 1:37 AM;sergey.khoroshavin;Nice to have tests:
* catchup of some nodes intermingled with view change on others (may require improvements in catchup code)
* addition/deletion of validator nodes intermingled with view change (will require improvements in LedgersBootstrap, may require improvements in PoolManager and catchup code);;;","14/Sep/19 1:12 AM;anikitinDSR;For now we have:
 * simulation tests with real ordering (adding real requests into requests queue and forwarding them)
 * simulation test with view change injected into ordering process (Now used only 1 seed, because there is some issues related to INDY-1340 and view_change integration process)

All other cases will be implemented after INDY-1340 finalizing and in view_change integration phase.

Current tests was added in PR:
https://github.com/hyperledger/indy-plenum/pull/1328;;;",,,,,,,,,,,,,,,,,,,
Integrate view change property-based tests into CI,INDY-2150,40612,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,17/Jun/19 6:42 PM,01/Oct/19 7:58 PM,28/Oct/23 2:47 AM,23/Jul/19 6:35 PM,,,1.10.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwqnr:",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"23/Jul/19 6:35 PM;ashcherbakov;We don't use hypothesis for now, so this is already integrated. No additional work in needed.;;;",,,,,,,,,,,,,,,,,,,,,
Nightly builds for Indy Node master with system tests,INDY-2151,40714,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,andkononykhin,andkononykhin,20/Jun/19 7:57 PM,09/Jul/19 5:14 PM,28/Oct/23 2:47 AM,08/Jul/19 9:14 PM,,,,,,,,0,devops,system-tests,,,"Current number of available system tests in https://github.com/hyperledger/indy-test-automation repo and their running time is not acceptable for Indy Node master CD pipeline. which is triggered for each commit to master branch.

But we want to be sure that all these tests are passed. Thus it makes sense to run them periodically (nightly). Unit and integration tests might be run as well.

Acceptance criteria:
* build is auto triggered once a day (nightly) on the HEAD of the master branch
* (optional) build can be triggered manually at any time
* all unit/integration/system tests should be run",,,,,,,,,,,,,,,,,,,,,INDY-2127,,,INDY-2152,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969uuw",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,anikitinDSR,,,,,,,,,,"29/Jun/19 1:05 AM;andkononykhin;PR: [https://github.com/hyperledger/indy-node/pull/1356];;;","29/Jun/19 1:34 AM;andkononykhin;Current state and issues:
 * pipeline is ready https://build.sovrin.org/job/indy-node/job/indy-node-nightly/
 * but env that it uses to run system tests might need debugging and fixing:
 ** I see a set of failed tests some of them show real/possible problem of the indy-node or system tests wrong assumptions ([example1|https://build.sovrin.org/job/test-pipelines/job/indy-node-nightly-test/52/testReport/system.indy-node-tests/test_vc/test_vc_py___Run_test_vc_py____1__Upload_test_report___test_vc_by_demotion/], [example2|https://build.sovrin.org/job/test-pipelines/job/indy-node-nightly-test/52/testReport/system.indy-node-tests.TestAuthMapSuite/TestAuthMapSuite/TestAuthMapSuite_py___Run_TestAuthMapSuite_py____5__Upload_test_report___test_case_cred_def_TRUSTEE_0_STEWARD_2_/])
** BUT others seems are related to some env problem (lack of resources, sockets), possible a workaround is needed ([example1|https://build.sovrin.org/job/test-pipelines/job/indy-node-nightly-test/52/testReport/system.indy-node-tests.TestAuthMapSuite/TestAuthMapSuite/TestAuthMapSuite_py___Run_TestAuthMapSuite_py____5__Upload_test_report___test_case_revoc_reg_def_TRUST_ANCHOR_101_TRUSTEE_0_/])

Note. it might be helpful to use other logs since juint includes not full data (e.g. [here|https://build.sovrin.org/job/test-pipelines/job/indy-node-nightly-test/52/]);;;","09/Jul/19 5:14 PM;anikitinDSR;PR for indy-plenum:
 * [https://github.com/hyperledger/indy-plenum/pull/1259]

PRs for indy-node:
 * [https://github.com/hyperledger/indy-node/pull/1376]
 * https://github.com/hyperledger/indy-node/pull/1375

 ;;;",,,,,,,,,,,,,,,,,,,
Add more system tests to Indy Node CD pipeline,INDY-2152,40715,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,andkononykhin,andkononykhin,20/Jun/19 8:09 PM,10/Jul/19 5:37 PM,28/Oct/23 2:47 AM,10/Jul/19 5:37 PM,,,,,,,,0,devops,system-tests,,,"New system tests that are coming would be quite slow and seems their duration unlikely can be changed drastically. Thus, they should be added to CD pipeline conditionally:
 * It is not desirable to increase duration of the master CD pipeline since it would increase the load of the CI server
 * it must have for release process

Acceptance criteria:
 * system tests for master are run partly (only some base and selective checks that are valuable)
 * system tests for rc and stable are run completely",,,,,,,,,,INDY-2127,,,,,,,,,,,INDY-2151,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2165,,,No,,Unset,No,,,"1|hzwvif:00001yw969v983",,,,Unset,Unset,Ev-Node 19.14,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,"10/Jul/19 5:37 PM;ashcherbakov;Done in https://github.com/hyperledger/indy-node/pull/1379;;;",,,,,,,,,,,,,,,,,,,,,
Debug and validation: Pluggable Request Handlers,INDY-2153,40716,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Jun/19 8:32 PM,02/Jul/19 5:46 PM,28/Oct/23 2:47 AM,02/Jul/19 5:46 PM,,,1.9.0,,,,,0,,,,,"We need to run more tests to make sure that nothing is broken:
 * load tests (with plugins and not) checking all types of read and write requests
 * actions (GET_VALIDATOR_INFO, POOL_RESTART)
 * check edge cases
 ** sending unknown request;
 ** sending requests with invalid inputs
 * catchup
 * pool upgrade",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jun/19 7:45 PM;VladimirWork;INDY-2153_ANOTHER_REQNACK_CASE.tar.gz;https://jira.hyperledger.org/secure/attachment/17350/INDY-2153_ANOTHER_REQNACK_CASE.tar.gz","28/Jun/19 11:19 PM;VladimirWork;INDY-2153_CRED_DEF_CASE.tar.gz;https://jira.hyperledger.org/secure/attachment/17366/INDY-2153_CRED_DEF_CASE.tar.gz","27/Jun/19 12:36 AM;VladimirWork;INDY-2153_REQNACK_CASE.tar.gz;https://jira.hyperledger.org/secure/attachment/17344/INDY-2153_REQNACK_CASE.tar.gz","27/Jun/19 1:24 AM;sergey.minaev;no_reply_primary_restart.log;https://jira.hyperledger.org/secure/attachment/17345/no_reply_primary_restart.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yw969c",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.minaev,VladimirWork,,,,,,,,,"27/Jun/19 12:36 AM;VladimirWork;Build Info:
indy-node 1.9.0~dev1009
plugins 0.9.13~47

Steps to Reproduce:
1. Send 50+ invalid requests into the pool during several seconds.

Expected Results:
Get 50+ REQNACKs.

Actual Results:
Get several REQNACKs and several Consensus is impossible errors.

Additional Info:
One month ago this test passed with 100 invalid requests but now it passes with 25 invalid requests only so it looks like some pool degradation during invalid request handling [^INDY-2153_REQNACK_CASE.tar.gz] .;;;","27/Jun/19 1:24 AM;sergey.minaev; [^no_reply_primary_restart.log] Node1 restart after sending the following txn from CLI:

{code}{""protocolVersion"":2,""signature"":""G9kcstBvbjzC99fKJzH3LFDKASnUkDEYj5W8RYtpNxBasKy7pdWnUiSwMz41vL1fRqKdgKJdfkTz9dwuQ2ThLAR"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1,""operation"":{""data"":{""0"":""0""},""type"":""2""}}{code};;;","27/Jun/19 7:45 PM;VladimirWork;Build Info:
indy-node 1.9.0~dev1010
plugins 0.9.13~48

Steps to Reproduce:
1. Send 2000 invalid requests into the pool during several seconds.

Expected Results:
Get 2000 REQNACKs.

Actual Results:
{noformat}
----------------------------- Captured stdout call -----------------------------
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2Cmdr8wXuPBPD7FrQKCXQ1djVKQfmmNhCLSr89dscLGyhgLhxN3vePrWR6ys28jbjbPyWAkktSMSi3Y1obmKVuFG'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'00': '0'}}, 'protocolVersion': 2, 'type': '110', 'metadata': {'payloadDigest': 'edfead466d750645c995b6b3ef89d4315383d31bb80361c05a72d0a6d1a09d10', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1, 'digest': '52c883d2b0335f90a9ed6e8d35beb73b28c6450cc9c9d8e37826ab5b276e9628'}}, 'txnMetadata': {'txnTime': 1561631800, 'seqNo': 1}, 'auditPath': [], 'ver': '1', 'rootHash': 'FsqchxiFhGXUjHyKe9LK3r9UV1e3oWc9vi7Zsk6krwtz'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': 'vxQLWURxPFM1b5ug33bLhL1gRzKDdcXMsuEqRAM18HjdzUVe7vjAnABvouVm7Xrm1DC5yB6aDNvVyTir9sMJx8K'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'00': '0'}}, 'protocolVersion': 2, 'type': '110', 'metadata': {'payloadDigest': '2c0baaca83a559500702120bc2395e03915987dd874cb13a091bb18359220202', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 562949953421313, 'digest': '7cf50c3960fd2903c1245894ed2e0435be83c5267c3315badb711721347adec9'}}, 'txnMetadata': {'txnTime': 1561631801, 'seqNo': 2}, 'auditPath': ['FsqchxiFhGXUjHyKe9LK3r9UV1e3oWc9vi7Zsk6krwtz'], 'ver': '1', 'rootHash': '8qRzTTFh8p3ZpeQnavERHUQ3osTKp9F9XbsLGvyMMKao'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2tRQekmnsCbSUjWTF28k5q4kBLhU2TfG7ZmPiJ8xipWJWqsDyEmHH1MmS3UTJCnZxoU1XH3MKgfe5kNKCmbrV5nN'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'00': '0'}}, 'type': '110', 'metadata': {'payloadDigest': '03cc6862690d0078a5ec2b150659ff4d0aa5b1a98f4ca31f7a8e4992509341ca', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 562949953465857, 'digest': '914f3137c1cc49a15b0ae8bed3d7adfaf2b562d200fcfaf9d315cca40aeb8909'}}, 'txnMetadata': {'txnTime': 1561631802, 'seqNo': 3}, 'rootHash': '3xq7CYKYBmSVgHNsr8zTMLydX79baZi84AkdJPWrDZ7z', 'ver': '1', 'auditPath': ['8qRzTTFh8p3ZpeQnavERHUQ3osTKp9F9XbsLGvyMMKao']}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2BGjcLVL9RpHrzK8AeuEjvoGPkkrnidwghC7anHBaQYvtVG4sfmbgC1142mGqkrvfQmhL9K4zQMtZb7XmoKp3yEz'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'00': '0'}}, 'type': '110', 'metadata': {'payloadDigest': 'a9c521c5b0e2b1458d6e0ee799d9e4fcbb4c060c04e0d03f0930d1bf338030ec', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 562949953466113, 'digest': '3ddb7563639decf5cec722f087130bd71703a75fef908b72af3e2190b01b5a67'}}, 'txnMetadata': {'txnTime': 1561631803, 'seqNo': 4}, 'rootHash': '53KoTqnzVDYBvVfcszFhH32qJMrTCfSDetaHuunwsJEj', 'ver': '1', 'auditPath': ['HMR9MxNHg2TK9pWcEm9D1JzH74MMiEDQQbghaAX919Cb', '8qRzTTFh8p3ZpeQnavERHUQ3osTKp9F9XbsLGvyMMKao']}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '3CPxMooFthiTyAMRY5RBwUK8yYwXGyPGhgut77J9P7RGDA2Z4ckVJKjZ3ozrk1QWtGfMvryU5kMtJcPhroJ3e87Z'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'01': {'0': '1', '00': '0000B'}}}, 'protocolVersion': 2, 'type': '110', 'metadata': {'payloadDigest': '1f62915123b748e7ce16eb32d71d5dd70e02d9acb219f3783b219f92a8058a0c', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 282574488535811, 'digest': 'f4958aecc5364bb4281888e470cc84882e61cee012c26cd0779798a27bd1d21e'}}, 'txnMetadata': {'txnTime': 1561631804, 'seqNo': 5}, 'auditPath': ['53KoTqnzVDYBvVfcszFhH32qJMrTCfSDetaHuunwsJEj'], 'ver': '1', 'rootHash': 'AWGcF7BZXRuPfUCLA9kjwSDZYyaGjyWSUUxcGspgdceS'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '3CPxMooFthiTyAMRY5RBwUK8yYwXGyPGhgut77J9P7RGDA2Z4ckVJKjZ3ozrk1QWtGfMvryU5kMtJcPhroJ3e87Z'}], 'type': 'ED25519'}, 'txn': {'type': '110', 'metadata': {'reqId': 282574488535811, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '1f62915123b748e7ce16eb32d71d5dd70e02d9acb219f3783b219f92a8058a0c', 'digest': 'f4958aecc5364bb4281888e470cc84882e61cee012c26cd0779798a27bd1d21e'}, 'protocolVersion': 2, 'data': {'data': {'01': {'0': '1', '00': '0000B'}}}}, 'txnMetadata': {'txnTime': 1561631804, 'seqNo': 5}, 'auditPath': ['53KoTqnzVDYBvVfcszFhH32qJMrTCfSDetaHuunwsJEj'], 'ver': '1', 'rootHash': 'AWGcF7BZXRuPfUCLA9kjwSDZYyaGjyWSUUxcGspgdceS'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '3CPxMooFthiTyAMRY5RBwUK8yYwXGyPGhgut77J9P7RGDA2Z4ckVJKjZ3ozrk1QWtGfMvryU5kMtJcPhroJ3e87Z'}], 'type': 'ED25519'}, 'txn': {'type': '110', 'metadata': {'payloadDigest': '1f62915123b748e7ce16eb32d71d5dd70e02d9acb219f3783b219f92a8058a0c', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 282574488535811, 'digest': 'f4958aecc5364bb4281888e470cc84882e61cee012c26cd0779798a27bd1d21e'}, 'protocolVersion': 2, 'data': {'data': {'01': {'0': '1', '00': '0000B'}}}}, 'txnMetadata': {'txnTime': 1561631804, 'seqNo': 5}, 'auditPath': ['53KoTqnzVDYBvVfcszFhH32qJMrTCfSDetaHuunwsJEj'], 'ver': '1', 'rootHash': 'AWGcF7BZXRuPfUCLA9kjwSDZYyaGjyWSUUxcGspgdceS'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '3Mpd7VYv7jWSXsAFUe3qWGMH5phXX1VULGMhub9z24qh3hCLbSwhasL8EBGnd1cTKuyC8NHdTbYXLUEs7hiJdohj'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'i1': {'0': '1', '00': '0000B'}}}, 'type': '110', 'metadata': {'reqId': 282574488535811, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '1a35d8262c6cf6629b915db018a2c643e0d2f831ddbe07a150866362e1e958c4', 'digest': 'f5de7a85a0106a0e2ad32e7978f28abc53eb825ae464aac1ede792392e43ca4d'}}, 'txnMetadata': {'txnTime': 1561631805, 'seqNo': 6}, 'auditPath': ['FHTae3ZKgYEAUhKPBU2KzmD3S2Gk2uJ4qjdCRzhmLTxE', '53KoTqnzVDYBvVfcszFhH32qJMrTCfSDetaHuunwsJEj'], 'ver': '1', 'rootHash': 'Gu92BWaAn2eAaWEWn3uBWmmK7Qae8GqwTRqrSZZ6DCoi'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': 'm23Edrrhyd4uVKCX5bBCLuNuabkwhYgob6BRCLyTCX8ZweT5FzjKpJzNytJhvTGuBnVM4nJQgy3T1cV4P6sN2sB'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'i1': {'0': '1', 'z0': '0000B'}}}, 'type': '110', 'metadata': {'reqId': 282574488535811, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '9407d475605eecf628e34514433b768456f425498c624fe7528db02472aae557', 'digest': 'cce6ed0093bcaef69a938957f699083f335d03000f49e925aa747be334286dbc'}}, 'txnMetadata': {'txnTime': 1561631806, 'seqNo': 8}, 'auditPath': ['J4NLY79iHTV7WuSmyCKYBabfLEUrwhurkSZj6fZDhjH2', '6XwyBqRfPdSi5CrLrojMizFS7kbj5tL6YFXJDnE6ZVhf', '53KoTqnzVDYBvVfcszFhH32qJMrTCfSDetaHuunwsJEj'], 'ver': '1', 'rootHash': '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '3G9s2w3jdrgYZRRK2v8mBavchjT5b1XSHSAPKL1aHvTwGNWGnEGDVD4wjBCRmJDDkJARCkLj49ZQajiHaWW9h3hD'}], 'type': 'ED25519'}, 'txn': {'metadata': {'reqId': 5, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '91c0f0ab1f2c39706e0ac9884ed924cf2cd56ca2d09c13a87ba27f77b5301a90', 'digest': '52582284d075a584bd98b2ecbdfbc2d3aaacda59a84ccb353e2a6229375cd9d7'}, 'type': '110', 'protocolVersion': 2, 'data': {'data': {'i1': {'0': '1I01', 'z0': '0000B'}}}}, 'txnMetadata': {'txnTime': 1561631807, 'seqNo': 9}, 'rootHash': '2Lyxpcku7i8cGptkFccN34SJX8bmNHXS6d89bsgf1pk4', 'ver': '1', 'auditPath': ['5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3']}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '3G9s2w3jdrgYZRRK2v8mBavchjT5b1XSHSAPKL1aHvTwGNWGnEGDVD4wjBCRmJDDkJARCkLj49ZQajiHaWW9h3hD'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'i1': {'0': '1I01', 'z0': '0000B'}}}, 'protocolVersion': 2, 'type': '110', 'metadata': {'payloadDigest': '91c0f0ab1f2c39706e0ac9884ed924cf2cd56ca2d09c13a87ba27f77b5301a90', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 5, 'digest': '52582284d075a584bd98b2ecbdfbc2d3aaacda59a84ccb353e2a6229375cd9d7'}}, 'txnMetadata': {'txnTime': 1561631807, 'seqNo': 9}, 'auditPath': ['5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'], 'ver': '1', 'rootHash': '2Lyxpcku7i8cGptkFccN34SJX8bmNHXS6d89bsgf1pk4'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '5MWdX144UW8XwQrotZpJbsfuRWBP4kDpQeRoisD5KAzUu4LkgZQSBbvKYHGr9PL6WbGR9at2ANfMFGFAtqtyxJd8'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'i1': {'0': '1I01', 'z""': '0000B'}}}, 'type': '110', 'metadata': {'payloadDigest': 'e2f7942c957a4cb0600049294be39a629eaceb5c4e613139639b615735104789', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 5, 'digest': 'b331e64d43931e5ea71057e8fefbfe61b17ffeec41053969035e30384b2bb3f8'}}, 'txnMetadata': {'txnTime': 1561631808, 'seqNo': 10}, 'rootHash': '2xYfRnqXRXqNXWAtCRovDHgZPHELdyesDou9xGuzebhx', 'ver': '1', 'auditPath': ['GtXaoNZv7Wi9EMmwZZLhUGH8P2RbTy2uQgn1VCkXu7k4', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3']}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2kDTfDzfz9iEjkcoWwCFncMsZ1xs6TxSz1ZosvN3qKqPjPNpahsNvhBY2G62ss9QxTbXNCikSW6ZAvpdjFN5MAbK'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'i1': {'0': '1I,1', 'z""': '000+B'}}}, 'protocolVersion': 2, 'type': '110', 'metadata': {'payloadDigest': 'aeb58aef4485a42cb1024ca65c2c4cafadb5d4b0304bf2f4246fe578a9cf8035', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 19717, 'digest': '48bfd0ae230cc849f5b6a0943cf34ca81d237b45917ab2880a70404dc9f8c4a2'}}, 'txnMetadata': {'txnTime': 1561631809, 'seqNo': 11}, 'auditPath': ['5uiTBm3DCvJ8uFVMWTeHRaVuR3DzrzS84hVWkRd6HaM', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'], 'ver': '1', 'rootHash': '6Th7AMrGdkLfjsd9B3dDMLBhb2XFQYu4taX6jfKUvaqL'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '5KUzhvC4U52Sa6ZrKMGP37VD2JW4k4qwk9mVctEdozkBmzzuKMDvL6t95CQNpYN7Xf3fW4eEK2PZ8aeh1Sndomzc'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'i1': {'0': '1I,1', 'z""': '`00+B'}}}, 'type': '110', 'metadata': {'reqId': 19717, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '1b85c1822d84029d77eec301804d9f55d3d593409df8c9ef7e8882bf6ab79d71', 'digest': '731224f7c7148e057895d08934b14a58de6ffa7f9efec0c4711c173d9edc0096'}}, 'txnMetadata': {'txnTime': 1561631810, 'seqNo': 12}, 'auditPath': ['9ZNeFuoG7eSqcFhob2uRPsra5BkZiGmVu75hDqUR6dsm', '5uiTBm3DCvJ8uFVMWTeHRaVuR3DzrzS84hVWkRd6HaM', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'], 'ver': '1', 'rootHash': '9ZvVkVEuk2TYSUiJiMShGFfpT6eFxeHvW3VZyS8JEjX1'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '5KUzhvC4U52Sa6ZrKMGP37VD2JW4k4qwk9mVctEdozkBmzzuKMDvL6t95CQNpYN7Xf3fW4eEK2PZ8aeh1Sndomzc'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'i1': {'0': '1I,1', 'z""': '`00+B'}}}, 'type': '110', 'protocolVersion': 2, 'metadata': {'reqId': 19717, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '1b85c1822d84029d77eec301804d9f55d3d593409df8c9ef7e8882bf6ab79d71', 'digest': '731224f7c7148e057895d08934b14a58de6ffa7f9efec0c4711c173d9edc0096'}}, 'txnMetadata': {'txnTime': 1561631810, 'seqNo': 12}, 'auditPath': ['9ZNeFuoG7eSqcFhob2uRPsra5BkZiGmVu75hDqUR6dsm', '5uiTBm3DCvJ8uFVMWTeHRaVuR3DzrzS84hVWkRd6HaM', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'], 'ver': '1', 'rootHash': '9ZvVkVEuk2TYSUiJiMShGFfpT6eFxeHvW3VZyS8JEjX1'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '5GQ1q1S6GgTKXZL7xwTch53L8hxwffLLjEX1nhZvvWJBnnxnBcArVqgR6ZGZJQ1PU6uh6koot81HcuKK2CawyvjM'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'i1': {'0': '1I,1', 'z""': '{00+B'}}}, 'type': '110', 'metadata': {'payloadDigest': '871bfb11c6a0b44fd84f4bd5c15296415faa185b96f7a8ff33ba61efb7bc9145', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 19717, 'digest': 'b6bc6d53d5dcedcf01c8f6cc79de4e31995aca95ba3135889566241d394fb11d'}}, 'txnMetadata': {'txnTime': 1561631811, 'seqNo': 13}, 'rootHash': 'F8mperc2ePFTr4Mq7EdFigpuXrUpJ2eT6oEfHR97hgMH', 'ver': '1', 'auditPath': ['ErMT5dThv1aKLtBmA37LHCWBnWrReENj4iHP325yvCfc', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3']}, 'op': 'REPLY'}
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('validation error [ClientNodeOperation]: missed fields - dest. ',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2Cmdr8wXuPBPD7FrQKCXQ1djVKQfmmNhCLSr89dscLGyhgLhxN3vePrWR6ys28jbjbPyWAkktSMSi3Y1obmKVuFG'}], 'type': 'ED25519'}, 'txn': {'type': '110', 'metadata': {'reqId': 1, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': 'edfead466d750645c995b6b3ef89d4315383d31bb80361c05a72d0a6d1a09d10', 'digest': '52c883d2b0335f90a9ed6e8d35beb73b28c6450cc9c9d8e37826ab5b276e9628'}, 'protocolVersion': 2, 'data': {'data': {'00': '0'}}}, 'txnMetadata': {'txnTime': 1561631800, 'seqNo': 1}, 'auditPath': [], 'ver': '1', 'rootHash': 'FsqchxiFhGXUjHyKe9LK3r9UV1e3oWc9vi7Zsk6krwtz'}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2Cmdr8wXuPBPD7FrQKCXQ1djVKQfmmNhCLSr89dscLGyhgLhxN3vePrWR6ys28jbjbPyWAkktSMSi3Y1obmKVuFG'}], 'type': 'ED25519'}, 'txn': {'metadata': {'reqId': 1, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': 'edfead466d750645c995b6b3ef89d4315383d31bb80361c05a72d0a6d1a09d10', 'digest': '52c883d2b0335f90a9ed6e8d35beb73b28c6450cc9c9d8e37826ab5b276e9628'}, 'type': '110', 'protocolVersion': 2, 'data': {'data': {'00': '0'}}}, 'txnMetadata': {'txnTime': 1561631800, 'seqNo': 1}, 'rootHash': 'FsqchxiFhGXUjHyKe9LK3r9UV1e3oWc9vi7Zsk6krwtz', 'ver': '1', 'auditPath': []}, 'op': 'REPLY'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '21WM3uSn3eXGxVbu9qra6KMWS5FcZRpqWrcMrd6gzL5VvogaDATBKSg5S4gHeNKYqJxGwEhGcuWh6wkyGyHEPyLW'}], 'type': 'ED25519'}, 'txn': {'metadata': {'reqId': 1, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': '2c084496cc7ad3762cb7ce504141a8993db838b2ac27de71dfed7d935a1d8a36', 'digest': '793d4fa9e1ea7730d089d42c3f6c4621a130dc2e21a5184a8a85e8d50b283550'}, 'type': '110', 'protocolVersion': 2, 'data': {'data': {'0': '00'}}}, 'txnMetadata': {'txnTime': 1561631812, 'seqNo': 14}, 'rootHash': 'CW1gQxUjfSLgKhtsL99rWxWfUfQBNEv99vkd37txL1BL', 'ver': '1', 'auditPath': ['EDEMNtxa7PcVwFgFuRP39ScnWGACeqoPFjRi3bg6ekNy', 'ErMT5dThv1aKLtBmA37LHCWBnWrReENj4iHP325yvCfc', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3']}, 'op': 'REPLY'}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('validation error [ClientNodeOperation]: missed fields - dest. ',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2APZn8brJ5fkZ7Uw45xFGVeHzsxpJPBCDrLw7EtnfTBasajSCzZ4kncQuXnoyqWqHD5fDgswpUFkyGumZjDytvmW'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'0': '0'}}, 'type': '110', 'metadata': {'payloadDigest': 'a2df3dd1a282f552c4316163db483b6703e6b4f1f63a37d0f5713d2bd9f30159', 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1, 'digest': '91581149211b47089b9cbb2583ea49722ebdb8dbaa005b4447b1f6de2fbab551'}}, 'txnMetadata': {'txnTime': 1561631813, 'seqNo': 15}, 'rootHash': 'Fqn31gdQoVtNNaXmyAbNRESeZkqvQxjDLDJvAGKXCvHX', 'ver': '1', 'auditPath': ['5pPNHggijcvesTyeDoFad2jnpsSwdpA7vLsXrawkwGqF', 'ErMT5dThv1aKLtBmA37LHCWBnWrReENj4iHP325yvCfc', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3']}, 'op': 'REPLY'}
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2APZn8brJ5fkZ7Uw45xFGVeHzsxpJPBCDrLw7EtnfTBasajSCzZ4kncQuXnoyqWqHD5fDgswpUFkyGumZjDytvmW'}], 'type': 'ED25519'}, 'txn': {'data': {'data': {'0': '0'}}, 'type': '110', 'protocolVersion': 2, 'metadata': {'reqId': 1, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': 'a2df3dd1a282f552c4316163db483b6703e6b4f1f63a37d0f5713d2bd9f30159', 'digest': '91581149211b47089b9cbb2583ea49722ebdb8dbaa005b4447b1f6de2fbab551'}}, 'txnMetadata': {'txnTime': 1561631813, 'seqNo': 15}, 'auditPath': ['5pPNHggijcvesTyeDoFad2jnpsSwdpA7vLsXrawkwGqF', 'ErMT5dThv1aKLtBmA37LHCWBnWrReENj4iHP325yvCfc', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'], 'ver': '1', 'rootHash': 'Fqn31gdQoVtNNaXmyAbNRESeZkqvQxjDLDJvAGKXCvHX'}, 'op': 'REPLY'}
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('validation error [ClientNodeOperation]: missed fields - dest. ',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('validation error [ClientNodeOperation]: missed fields - dest. ',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('validation error [ClientNodeOperation]: missed fields - dest. ',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: 46',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: 55',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: 55',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'reqId': 1, 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'op': 'REQNACK', 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1}
{'reqId': 2, 'reason': ""client request invalid: InvalidClientRequest('invalid type: -109',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQNACK'}
{'result': {'reqSignature': {'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '2APZn8brJ5fkZ7Uw45xFGVeHzsxpJPBCDrLw7EtnfTBasajSCzZ4kncQuXnoyqWqHD5fDgswpUFkyGumZjDytvmW'}], 'type': 'ED25519'}, 'txn': {'protocolVersion': 2, 'data': {'data': {'0': '0'}}, 'type': '110', 'metadata': {'reqId': 1, 'from': 'V4SGRU86Z58d6TV7PBUe6f', 'payloadDigest': 'a2df3dd1a282f552c4316163db483b6703e6b4f1f63a37d0f5713d2bd9f30159', 'digest': '91581149211b47089b9cbb2583ea49722ebdb8dbaa005b4447b1f6de2fbab551'}}, 'txnMetadata': {'txnTime': 1561631813, 'seqNo': 15}, 'auditPath': ['5pPNHggijcvesTyeDoFad2jnpsSwdpA7vLsXrawkwGqF', 'ErMT5dThv1aKLtBmA37LHCWBnWrReENj4iHP325yvCfc', '5DDUzWhNnt6HhZBbcyTXQdnX2vBAjqdTcXFxSQKrGjN3'], 'ver': '1', 'rootHash': 'Fqn31gdQoVtNNaXmyAbNRESeZkqvQxjDLDJvAGKXCvHX'}, 'op': 'REPLY'}
---------------------------------- Hypothesis ----------------------------------
Trying example: test_case_random_req_data(_type=-109, data={'00': '0'}, get_default_trustee=('V4SGRU86Z58d6TV7PBUe6f', 'GJ1SzoWzavQYfNL9XkaJdrQejfztN4XqdsiV4ct3LXKL'), pool_handler=15, reqid=1, self=<TestPropertyBasedSuite.TestPropertyBasedSuite at 0x7fddba179f98>, wallet_handler=18)
Trying example: test_case_random_req_data(_type=110, data={'00': '0'}, get_default_trustee=('V4SGRU86Z58d6TV7PBUe6f', 'GJ1SzoWzavQYfNL9XkaJdrQejfztN4XqdsiV4ct3LXKL'), pool_handler=15, reqid=1, self=<TestPropertyBasedSuite.TestPropertyBasedSuite at 0x7fddba179f98>, wallet_handler=18)
{noformat}
;;;","28/Jun/19 11:20 PM;VladimirWork;Build Info:
indy-node 1.9.0~dev1011
plugins 0.9.13~50

Steps to Reproduce:
1. Add auth rules for cred def - one role can just add it and other role can just edit it.
2. Add new cred def and try to edit it with the same DID.

Expected Results:
Edition by the adder should be rejected according to auth rules.

Actual Results:
Adder can add and then edit cred def:
{noformat}
ADD:
{'result': {'ver': '1', 'txnMetadata': {'txnTime': 1561729501, 'txnId': 'TkKzKyV1be5zc6bR7kgMSe:3:CL:14:TAG1', 'seqNo': 15}, 'auditPath': ['AWf3fXBDJhFbtLZ1GnWesFWb66CBLpFjNCjPapFXZaut', 'k7P1B6qhnJ7FEVTE47aBpsuTSMC8CmpmU58EpZD4M1Y', '6NVGJjQVapQ7HV5Xg5y8u5sHjEMp7hEXDhbcrHZ6SZsV'], 'rootHash': 'HJWj43BUFFYemuKCoMcB6MBUthUeydLs3B81WVk4b4dV', 'txn': {'type': '102', 'protocolVersion': 2, 'data': {'ref': 14, 'data': {'primary': {'rctxt': '77541021566605177289117593847925068870651768066462338922350415702441786163595641837336641570668039165647965442857824729093570685000389181549902350502451092849538726642836454789551068690220337058984676497901998719720303948613931320389679045124330609699096389849637295976053093403755556047326048292511360444280165474483115467809916008240607477197252990795964521194060038962446927895719563094289124633169595137637133185497163840448621571832987139232894563417304651288606423449203584961367219654179837488083493821759455973558534208583710155993609045386127790423510870640323333162299845314853096508739059873194725004655393', 'r': {'name': '37542663674627346394085101315428874045039134485502586305624977525935929386404105077020926064597954643357923813242109511277441298522728872506846882305849626583271783174083855063502651822131906903156134676617852490045112408012499915443726277454259817391441643375289814482483271306314836145934179414818736103342592612170554842048753928879361190519912890794569697923475239328828037314993253663832108615207608210824785810895520282355129633148020081164065541711168644395596944150873421372908424605453864237529988864393569673574119168621134576438474543526067954363015011642526957514078023212472032429371362330885021578655487', 'height': '74780935536527427878140904709536874936897290377270368567668937154977987215247012829000892844373581013109277226250909894353311119991570830535498288402428200109828620973530946805019261864815327880747866005667668807664753767908838101460712874056716697869489501284160962759093859802327815040824127113851385894656458923267608431510752617495270446608791131977459063783799582748621376873553037645141173496667446293903534421075685533546278784900854892912729840861499394319804919041769117412021252500415610256281719318288852431378445680115272435517611681841607451326725060015587654127035553757544200461779472207512696664024778', 'age': '65714529954175948098670762653593578592136226831315089881054782445957487375811894699100839887815758922079322361436924181919713373157619334378819472392261835463044452303328617947895897511520171099338476090408080084805365713343540021696601386751206701256606821927491092462984579009024191291339788951017197565824311506496874864294765053752877997042010020072622775253922753530207003179057381280608093895927521153766314073198571486322723125589204096191944435284941750112274959504483055783076970183573314208325378566689454709606076561154253898837403488495942722889666023257453681332157594326986072902474554942197180651820527', 'master_secret': '1951500274405799036011039689761870811228610073154367242232737620903943455266399752253534629152662273768356719711534626707689671585442885801069302080144686905802081406004983618222267849702115134713315976563828645202384645016430807971716223640907907794549686635736036084553450312806781967248841581248495678201791881853556333926430502837005661256214651819792698816355842338512791050244947350123171261884421798158408970245190098527207188381558888587630463745837694438006901868085931021801104371971050744142850174656177347063328615900229561938830245962382320059976829498879080499294869181181661294234939978123153478324524', 'sex': '62004526349822066491774245152987287158049554279667155219762383639232074443864204657775819374780106222125603195422419349490642527888759516572802083144578910899652967310544097040749451915725590380210732482922184526453452404339939726650614147825817409851385825500596836237422175369720593346325396912137642225647508679044174778243300027285700012752702913813144430159845783620863524021158045173098339298908744578076149945664193413566777167067765823864689456266963472627708305751931642250013525129384426317594388610150448537054201628864355826390868441630235021576187884485503990101853527581886590769580621851136959223123238'}, 's': '18821781573525340440445586819008498850913013337624877854862306597704236037116612330980988577389156033434511279110251715202799368545030373042897164242732884006488280760976415989054055140332257898339302228597966380936846168473902670291726450180962121730862696199490426583909291108312319325497362694610155270727267946236540818226803520793401916821603722192887849260297998420205717866349684715899399135401804577259992316846174422079744867097528963041582642422005948963927538103965596715185910037213329012944772410078002196609975385268395561504693944527015883444977046388285055667353587585970434859892956876645851243917924', 'n': '82809703613778084161904490864991439144934146623192816688946166472393179226372923527302933857530164627889073898519472984743873390853518604776228754163266775811337421416973337534777605939991975330158491915943347552699909014546670399475266166323434343991244139588800985553369899726914050855351063107237398123234454575705278215762921102267995216935755105794892093541634732223908019585229928452280895180457192686971352532514375026408727888494092247900723245360474828299338137407391307445905704973715919881238819155126421796804732437511875819266660130823864958484730293894417403105499674307371821085881398834852830672241889', 'z': '50368224715478653142065901172316681089769239355481656342157776931225296514348342081044257616980007434448539853473982302068522760320799242664072758166535657126553059725346830716452793177026972355797533355344670210093478312648126740947196076064543238063124233969038904211575943585611276430061760363192027543118798150440057718148704426496786674798129468762248905659127677318259546014307466967910600203950698815879699994940653205743333839142516379258200547015040128640816624865636785683210211730061472702836891893730138638444045963856503711823044097651112312005281192025559296575499704352728255891376251086698786551175913'}}, 'tag': 'TAG1', 'signature_type': 'CL'}, 'metadata': {'from': 'TkKzKyV1be5zc6bR7kgMSe', 'reqId': 1561729501537567798, 'payloadDigest': '560f1d50170d4030a3e01b7e589048e4c665aa807b939d0ac563a251e7071e96', 'digest': 'ab5630a83a0972051b7a0a0f0d292def5a9102ea47483006a9be1c305c378d47'}}, 'reqSignature': {'type': 'ED25519', 'values': [{'value': '3qoNzLeLqKkgtN2weVEdjf7vkvFdAuDiYL3Pa7946eXtLSsUMVYkE3gSXDkNjgAt9JUpMLxUrgtWkD5ax2vkrMqZ', 'from': 'TkKzKyV1be5zc6bR7kgMSe'}]}}, 'op': 'REPLY'}

EDIT:
{'result': {'ver': '1', 'txnMetadata': {'txnTime': 1561729502, 'txnId': 'TkKzKyV1be5zc6bR7kgMSe:3:CL:14:TAG1', 'seqNo': 16}, 'auditPath': ['9gbztBuPjSJ9TFdFMNtKnfpSpx6SaBAijuQHbNeRGyHh', 'AWf3fXBDJhFbtLZ1GnWesFWb66CBLpFjNCjPapFXZaut', 'k7P1B6qhnJ7FEVTE47aBpsuTSMC8CmpmU58EpZD4M1Y', '6NVGJjQVapQ7HV5Xg5y8u5sHjEMp7hEXDhbcrHZ6SZsV'], 'rootHash': 'Y47bSyikXTukDV2GmzdAXquaXj9zyGYQAevYTHJMtz8', 'txn': {'metadata': {'from': 'TkKzKyV1be5zc6bR7kgMSe', 'reqId': 3123459003075135596, 'payloadDigest': '3278fc381be50182ac933ad8a2b176b5be6d2a56a73b49c882cc94e088416244', 'digest': '6f94e36314e69425230913b9e06edb706f3000e104ddf0cdf8eeb0c6c02b8eda'}, 'protocolVersion': 2, 'data': {'ref': 14, 'data': {'primary': {'rctxt': '77541021566605177289117593847925068870651768066462338922350415702441786163595641837336641570668039165647965442857824729093570685000389181549902350502451092849538726642836454789551068690220337058984676497901998719720303948613931320389679045124330609699096389849637295976053093403755556047326048292511360444280165474483115467809916008240607477197252990795964521194060038962446927895719563094289124633169595137637133185497163840448621571832987139232894563417304651288606423449203584961367219654179837488083493821759455973558534208583710155993609045386127790423510870640323333162299845314853096508739059873194725004655393', 'r': {'name': '37542663674627346394085101315428874045039134485502586305624977525935929386404105077020926064597954643357923813242109511277441298522728872506846882305849626583271783174083855063502651822131906903156134676617852490045112408012499915443726277454259817391441643375289814482483271306314836145934179414818736103342592612170554842048753928879361190519912890794569697923475239328828037314993253663832108615207608210824785810895520282355129633148020081164065541711168644395596944150873421372908424605453864237529988864393569673574119168621134576438474543526067954363015011642526957514078023212472032429371362330885021578655487', 'height': '74780935536527427878140904709536874936897290377270368567668937154977987215247012829000892844373581013109277226250909894353311119991570830535498288402428200109828620973530946805019261864815327880747866005667668807664753767908838101460712874056716697869489501284160962759093859802327815040824127113851385894656458923267608431510752617495270446608791131977459063783799582748621376873553037645141173496667446293903534421075685533546278784900854892912729840861499394319804919041769117412021252500415610256281719318288852431378445680115272435517611681841607451326725060015587654127035553757544200461779472207512696664024778', 'age': '65714529954175948098670762653593578592136226831315089881054782445957487375811894699100839887815758922079322361436924181919713373157619334378819472392261835463044452303328617947895897511520171099338476090408080084805365713343540021696601386751206701256606821927491092462984579009024191291339788951017197565824311506496874864294765053752877997042010020072622775253922753530207003179057381280608093895927521153766314073198571486322723125589204096191944435284941750112274959504483055783076970183573314208325378566689454709606076561154253898837403488495942722889666023257453681332157594326986072902474554942197180651820527', 'master_secret': '1951500274405799036011039689761870811228610073154367242232737620903943455266399752253534629152662273768356719711534626707689671585442885801069302080144686905802081406004983618222267849702115134713315976563828645202384645016430807971716223640907907794549686635736036084553450312806781967248841581248495678201791881853556333926430502837005661256214651819792698816355842338512791050244947350123171261884421798158408970245190098527207188381558888587630463745837694438006901868085931021801104371971050744142850174656177347063328615900229561938830245962382320059976829498879080499294869181181661294234939978123153478324524', 'sex': '62004526349822066491774245152987287158049554279667155219762383639232074443864204657775819374780106222125603195422419349490642527888759516572802083144578910899652967310544097040749451915725590380210732482922184526453452404339939726650614147825817409851385825500596836237422175369720593346325396912137642225647508679044174778243300027285700012752702913813144430159845783620863524021158045173098339298908744578076149945664193413566777167067765823864689456266963472627708305751931642250013525129384426317594388610150448537054201628864355826390868441630235021576187884485503990101853527581886590769580621851136959223123238'}, 's': '18821781573525340440445586819008498850913013337624877854862306597704236037116612330980988577389156033434511279110251715202799368545030373042897164242732884006488280760976415989054055140332257898339302228597966380936846168473902670291726450180962121730862696199490426583909291108312319325497362694610155270727267946236540818226803520793401916821603722192887849260297998420205717866349684715899399135401804577259992316846174422079744867097528963041582642422005948963927538103965596715185910037213329012944772410078002196609975385268395561504693944527015883444977046388285055667353587585970434859892956876645851243917924', 'n': '123456789', 'z': '50368224715478653142065901172316681089769239355481656342157776931225296514348342081044257616980007434448539853473982302068522760320799242664072758166535657126553059725346830716452793177026972355797533355344670210093478312648126740947196076064543238063124233969038904211575943585611276430061760363192027543118798150440057718148704426496786674798129468762248905659127677318259546014307466967910600203950698815879699994940653205743333839142516379258200547015040128640816624865636785683210211730061472702836891893730138638444045963856503711823044097651112312005281192025559296575499704352728255891376251086698786551175913'}}, 'tag': 'TAG1', 'signature_type': 'CL'}, 'type': '102'}, 'reqSignature': {'type': 'ED25519', 'values': [{'value': '2zg3utJszr1xu1jmisj2LdUpBMsJ4Ed5HeNVHAShVjUBZAFiHzrLpao67z3sbg6LwHRYgEbw14HByrTmYUP9ZMb4', 'from': 'TkKzKyV1be5zc6bR7kgMSe'}]}}, 'op': 'REPLY'}
{noformat}

Logs:
[^INDY-2153_CRED_DEF_CASE.tar.gz] ;;;","02/Jul/19 5:45 PM;VladimirWork;Verified according to PoA, all issues found are fixed in 1.9.0~dev1014.;;;",,,,,,,,,,,,,,,,,
Clean-up Pluggable Request Handlers,INDY-2154,40723,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,21/Jun/19 12:48 AM,08/Jul/19 9:14 PM,28/Oct/23 2:47 AM,08/Jul/19 9:07 PM,,,1.9.1,,,,,0,,,,,"Please do the following:
 * Remove all old ReqHandlers
 * Remove hooks
 * Fix updating of ts_store for config ledger after catchup
 * Move seq_no_db to DatabaseManager
 * Move node_status_db to DatabaseManager
 * Go through all the interfaces once more and check for consistency. In particular:
 ** remove `dynamic_validation` from ActionReqHandler**
 * Move `commitAndSendReplies` into `WriteRequestManager` as much as possible
 * Update docs and class diagrams
 * Do we need `is_valid_ledger_id` in so many places?
 * Move taa validation into `WriteRequestManager`
 * Use TaaReqHandler for taa validation (use static methods?)manager
 * remove `reqProcessors `
 * remove `opVerifiers`
 * Incude trackers into request_manager system
 * make all TODO items in main's of plugins
 * Unify format of chain of responsibility in req_handlers
 * Rewrite old unit tests for new handlers (including [https://github.com/sovrin-foundation/token-plugin/pull/261/commits/396190fcbbea8f80f54fe1f184dffa372ab36e9e)|https://github.com/sovrin-foundation/token-plugin/pull/261/commits/396190fcbbea8f80f54fe1f184dffa372ab36e9e]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yw969uur",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"06/Jul/19 1:57 AM;anikitinDSR;Done issues:
 * remove old req handlers
 * remove hooks (except some replica's hooks, that used if token-plugins)
 * fix updating of ts_store for config ledger after catchup
 * move seq_no_db to DatabaseManager
 * move node_status_db to DatabaseManager
 * Move taa validation into `WriteRequestManager`
 * remove `reqProcessors `
 * remove `opVerifiers`
 * Incude trackers into request_manager system
 * make all TODO items in main's of plugins
 * Update docs and class diagrams;;;","08/Jul/19 9:14 PM;anikitinDSR;PRs.

indy-plenum:
 * [https://github.com/hyperledger/indy-plenum/pull/1247]

indy-node:
 * [https://github.com/hyperledger/indy-node/pull/1371]

token-plugins:
 * [https://github.com/sovrin-foundation/token-plugin/pull/276]

 ;;;",,,,,,,,,,,,,,,,,,,,
Review and organise test_misc.py,INDY-2155,40738,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,21/Jun/19 9:00 PM,08/Nov/19 4:31 PM,28/Oct/23 2:47 AM,08/Nov/19 4:31 PM,,,,,,,,0,,,,,Review and organise into new test suites test_misc.py test cases.,,,,,,,,,,,,,,,,,,,,,INDY-2127,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i013n4:",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"07/Nov/19 5:30 PM;VladimirWork;Done in https://github.com/hyperledger/indy-test-automation/pull/74;;;",,,,,,,,,,,,,,,,,,,,,
Update metadata for PyPI packages,INDY-2156,40758,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,esplinr,andkononykhin,andkononykhin,24/Jun/19 7:26 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,,1.16.0,,,,,0,devops,,,,"Some of packages in Evernym PyPI account miss such important metadata as license, description... Need to review the following packages data and update:
 * indynotifieremail: A tool for stewards of an Indy network to be notified by email when a traffic spike is detected.
 * sovrinnotifierawssns: A tool for Sovrin Stewards to receive notifications.
 * sovrin-client-rest-dev: A library that can be used by a Sovrin client application to check the status of the validation pool.
 * sovrinnotifieremail: A tool for Sovrin Stewards to be notified by email when a traffic spike is detected. Also see the package indynotifieremail.
 * sovringui: A helper to build GUIs for checking the Sovrin pool status.

Acceptance criteria:
 * ensure that each PyPi listing specifies the Apache license,  thelink to the source code and use the description provided above

Options:

Seems there is only one option:
 * reach out the source code (I believe all of them shoould ave some GitHub rpeository)
 * update their metadata
 * use `twine` to upload changes to PyPI as new releases

Additionally here a quote from PyPI:
{quote}Project description and sidebar
To set the 'indynotifieremail' description, author, links, classifiers, and other details for your next release, use the setup() arguments in your setup.py file. Updating these fields will not change the metadata for past releases. Additionally, you must use Twine to upload your files in order to get full support for these fields. See the Python Packaging User Guide for more help.
{quote}",,,,,,,,,,,,,,,,,,,,,INDY-2065,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c97",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,esplinr,,,,,,,,,,"27/Jun/19 12:02 AM;andkononykhin;[~esplinr]  I checked urls to sources for that repos (also I checked `sovringui`). Here is the results:
 * indynotifieremail, sovrinnotifieremail
 ** url: [https://github.com/evernym/sovrin-notifier-email]
 ** `indynotifieremail` was `sovrinnotifieremail` renamed in scope of INDY-995
 * sovrinnotifierawssns
 ** url: [https://github.com/evernym/sovrin-notifier-awssns]
 ** repo is private
 * sovrin-client-rest-dev
 ** url: [https://github.com/evernym/sovrin-client-rest]
 * sovringui
 ** repo is private
 ** url: [https://github.com/evernym/sovringui]
 * sovrin_installer
 ** url: [https://github.com/evernym/sovrin-installer-priv]

Thus, all these packages are not presented in both Sovrin Foundation and Hyperledger repos.

I think we should decide who owns them in both GitHub and PyPI and update/delete then.;;;","10/Jul/19 10:20 PM;esplinr;Todo:
* Discuss with Mike B whether these are still used
* Delete unused pypi packages
* Make the private Evernym repos public
* Turn over the maintained pypi packages (indy-node and indy-sdk) to Sovrin Foundation since they are published in the CD pipeline;;;",,,,,,,,,,,,,,,,,,,,
"TAA acceptance should use date, not time",INDY-2157,40771,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,25/Jun/19 1:59 AM,11/Jul/19 7:33 PM,28/Oct/23 2:47 AM,11/Jul/19 7:33 PM,,,1.9.1,,,,,0,,,,,"*Story*
As a user of an Indy network that enforces a transaction author agreement, I do not want my agreement acceptance time to be recorded with my transactions as it is specific enough that it could allow correlation across my various DIDs and payment addresses.

*Acceptance Criteria*
* When an agreement is anchored to the ledger, only the date needs to be recorded.
* When a transaction is written to the ledger, only the date of the agreement acceptance should be recorded on the ledger as part of the transaction.
* If the signed transaction includes too much precision in the timestamp, the transaction should be rejected with the error: ""TAA timestamp is too precise and is a privacy risk.""
* Validation of agreement acceptance should accept any transaction with the same acceptance date as the current agreement was posted to the ledger, or newer.
* If the timestamp when the author agreement was anchored to the ledger is within two minutes of zero UTC, then future transactions with a user agreement acceptance date of the previous day will also be accepted to account for potential clock skew.

*Note*
* The format of the transaction on the ledger does not need to change from a datetime, we will just ignore the timestamp portion of the datatype.",,,,,,,,,,,,,,IS-1298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1942,,,No,,Unset,No,,,"1|hzwvif:00001yw969ux",,,,Unset,Unset,Ev-Node 19.14,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,Toktar,VladimirWork,,,,,,,,,"05/Jul/19 12:07 AM;Toktar;*PoA:*
 * Add TAA timestamp check to the client request validation that timestamp is date. If timestamp has hours, minutes, seconds or microseconds, reject this request with the _InvalidClientTaaAcceptanceError_ with message _""TAA timestamp is too precise and is a privacy risk.""_
 * Change TAA timestamp validation. Timestamp must belong to the time period from 

{code:java}
(taa_txn_time - TXN_AUTHOR_AGREEMENT_ACCEPTANCE_TIME_BEFORE_TAA_TIME).date(){code}
to

{code:java}
(req_pp_time + TXN_AUTHOR_AGREEMENT_ACCEPTANCE_TIME_AFTER_PP_TIME).date(){code}

 * Add tests:
 ** Send a request with TAA timestamp with hours - wait reject with _""TAA timestamp is too precise and is a privacy risk.""_ message
 ** Send a request with TAA timestamp with minutes - wait reject with _""TAA timestamp is too precise and is a privacy risk.""_ message
 ** Send a request with TAA timestamp with seconds - wait reject with _""TAA timestamp is too precise and is a privacy risk.""_ message
 ** Send a request with TAA timestamp with microseconds - wait reject with _""TAA timestamp is too precise and is a privacy risk.""_ message
 ** Send a request with TAA timestamp with seconds and hours - wait reject with _""TAA timestamp is too precise and is a privacy risk.""_ message
 ** Send a request with TAA timestamp is lower than (taa_txn_time - BEFORE_TAA_TIME)  - wait reject with ""Txn Author Agreement acceptance time is inappropriate:"" message
 ** Send a request with correct TAA timestamp - wait success validation
 ** Send a request with TAA timestamp is higher than (req_pp_time + AFTER_PP_TIME)  - wait reject with ""Txn Author Agreement acceptance time is inappropriate:"" message
 ** Repeat 3 tests above with different combinations of time intervals relative to the borders of the day.;;;","05/Jul/19 5:16 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/1256;;;","09/Jul/19 9:49 PM;Toktar;*Problem reason:*
 - If a client signed two requests with TAA and send the second request after a few minutes after the first request then the second will be rejected.

*Changes:*
 - Check that a TAA timestamp in client requests doesn't contain time, only date.
 - Check that a TAA timestamp  belong to the time period from
{code:java}
(taa_txn_time - TXN_AUTHOR_AGREEMENT_ACCEPTANCE_TIME_BEFORE_TAA_TIME).date(){code}
to
{code:java}
(req_pp_time + TXN_AUTHOR_AGREEMENT_ACCEPTANCE_TIME_AFTER_PP_TIME).date(){code}

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1377]
 * [https://github.com/hyperledger/indy-plenum/pull/1256]

*Version:*
 * indy-node 1.9.1025 -master
 * (indy-plenum 1.9.836 -master)

*Risk factors:*
 - Problem with TAA for the client request validation.

*Risk:*
 - Medium

*Test:*
 * [test_taa_acceptance_validation.py|https://github.com/hyperledger/indy-plenum/pull/1256/files#diff-619bfcc06ab06b2941cabf54af13dd7e] 

*Recommendations for QA:*
 * Create a request with TAA and send it after more than 3 minutes after creation. Check that the transaction from request successfully was ordered.
 * Send a request with TAA timestamp with a time. Check that a reject was received
 * Check that request with TAA timestamp from yesterday will be rejected
 * Check that request with TAA timestamp from tomorrow will be rejected
 * Check that request with TAA timestamp from tomorrow after 1 minute after 00:00 will be passed
 * Check that request with TAA timestamp from yesterday in 23:59 will be passed;;;","11/Jul/19 7:33 PM;VladimirWork;Build Info:
indy-node 1.9.0~dev1028
plugins 1.0.0~dev62

Steps to Validate:
1. Create a request with TAA and send it after more than 3 minutes after creation. Check that the transaction from request successfully was ordered.
2. Send a request with TAA timestamp with a time. Check that a reject was received.
3. Check that request with TAA timestamp from yesterday will be rejected.
4. Check that request with TAA timestamp from tomorrow will be rejected.

Actual Results:
All TAA changes work as expected.;;;",,,,,,,,,,,,,,,,,,
One of state proof cases stopped working,INDY-2158,40817,,Bug,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,Derashe,Derashe,Derashe,27/Jun/19 11:17 PM,24/Oct/19 5:06 PM,28/Oct/23 2:47 AM,,,,,,,,,0,TShirt_S,,,,"Test case with state proof of REVOC_REG_DELTA has been skipped in [https://github.com/hyperledger/indy-node/pull/1359/commits/6501d2908999d608cae8967886352536c58d9eb7]

Unskip it, research why is it failing, rewrite if needed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2266,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c98i",,,,Unset,Unset,Ev-Node 19.14,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Artemkaaas,ashcherbakov,Derashe,,,,,,,,,"11/Jul/19 2:16 PM;Artemkaaas;There was an issue on SDK side.
The fix was provided in PR: https://github.com/hyperledger/indy-sdk/pull/1711
Build version: master-1167;;;","17/Jul/19 6:58 PM;ashcherbakov;We need to switch to the latest SDK and enable tests in Node.;;;","19/Jul/19 3:51 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/1383;;;","20/Jul/19 1:03 AM;Derashe;Need to be fixed still;;;",,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.9.0,INDY-2159,40818,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,27/Jun/19 11:39 PM,08/Jul/19 5:16 PM,28/Oct/23 2:47 AM,08/Jul/19 5:15 PM,,,1.9.0,,,,,0,,,,,"*Version Information*
indy-node 1.9.0
indy-plenum 1.9.0
sovrin 1.1.50


*Notices for Stewards*
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.
Some nodes can fail to send a REJECT or REPLY to client under specific network conditions. See Known Issues for more details.

*Major Changes*
- Pluggable Request Handlers have been implemented

*Detailed Changelog*

    +Major Fixes+
    INDY-2144 - Propagates with invalid requests can lead to node crashes
    INDY-2142 - There is no validation of the ISSUANCE_TYPE field for the transaction REVOC_REG_DEF
    INDY-2083 - Reduce CONS_PROOF timeout to speed up catchup under the load

    +Changes and Additions+
    INDY-2087 - As a Trustee(s), I need to have a way to set multiple AUTH_RULES by one command
    INDY-2127 - Make more system tests to be ready for Indy Node CD pipeline
    INDY-1861 - Integrate new handlers into the codebase
    INDY-1338 - Define Interfaces needed for View Change Service
    INDY-1950 - Rename TRUST_ANCHOR to ENDORSER
    INDY-2134 - Update PBFT view change plan of attack
    INDY-2131 - Apply a new Docker-in-docker approach for system tests
    INDY-2108 - More tests for pluggable request handlers
    INDY-1956 - Remove ANYONE_CAN_WRITE
    INDY-1290 - [Design] ViewChange protocol must be as defined in PBFT
    INDY-1405 - Batch containing some already executed requests should be applied correctly
    INDY-2097 - Update Pluggable Req Handlers
    INDY-2077 - As a Network Admin, I need to be able to forbid an action in AUTH_RULE, so that no changes in code are needed
    INDY-1860 - Create Builders for handlers

    +Known Issues+
    INDY-2164 - Incorrect request validation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969uui",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"08/Jul/19 5:15 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/1372;;;",,,,,,,,,,,,,,,,,,,,,
Intermittent failure during indy-node debian installation,INDY-2160,40819,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,27/Jun/19 11:55 PM,27/Jun/19 11:55 PM,28/Oct/23 2:47 AM,,,,,,,,,0,devops,,,,"Sometimes indy-node installation using apt tool failed because of the strange error related to one of the dependencies:
{code:java}
Setting up python3-pygments (2.2.0) ...
Fatal Python error: Py_Initialize: Unable to get the locale encoding
Traceback (most recent call last):
  File ""/usr/lib/python3.5/encodings/__init__.py"", line 32, in <module>
ImportError: cannot import name 'aliases'
Aborted (core dumped)
dpkg: error processing package python3-pygments (--configure):
 subprocess installed post-installation script returned error exit status 134
Setting up python3-six (1.11.0) ...
Setting up python3-wcwidth (0.1.6+dfsg1-1) ...
dpkg: dependency problems prevent configuration of python3-prompt-toolkit:
 python3-prompt-toolkit depends on python3-pygments; however:
  Package python3-pygments is not configured yet.

dpkg: error processing package python3-prompt-toolkit (--configure):
 dependency problems - leaving unconfigured

...

dpkg: dependency problems prevent configuration of indy-plenum:
 indy-plenum depends on python3-prompt-toolkit (= 0.57-1); however:
  Package python3-prompt-toolkit is not configured yet.
 indy-plenum depends on python3-pygments (= 2.2.0); however:
  Package python3-pygments is not configured yet.

dpkg: error processing package indy-plenum (--configure):
 dependency problems - leaving unconfigured
Setting up python3-timeout-decorator (0.4.0) ...
Setting up python3-distro (1.3.0) ...
dpkg: dependency problems prevent configuration of indy-node:
 indy-node depends on indy-plenum (= 1.9.0~dev827); however:
  Package indy-plenum is not configured yet.

dpkg: error processing package indy-node (--configure):
 dependency problems - leaving unconfigured
Processing triggers for libc-bin (2.23-0ubuntu10) ...
Processing triggers for systemd (229-4ubuntu21.2) ...
Errors were encountered while processing:
 python3-pygments
 python3-prompt-toolkit
 indy-plenum
 indy-node{code}
Possibly it might be related to in-docker installation.

Example (in scope of system-testing): [https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-cd/detail/master/1010/pipeline]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00spw:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After a ledger-triggered upgrade of the Sovrin package failed, indy-node upgraded to an incorrect version",INDY-2161,40866,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,mgbailey,mgbailey,02/Jul/19 3:06 AM,11/Oct/19 9:45 PM,28/Oct/23 2:47 AM,11/Oct/19 9:45 PM,,,1.11.0,,,,,0,EV-CS,TShirt_M,,,"An attempt was made to upgrade Sovrin MainNet that failed due to expired PGP keys on the repo. That is not the issue to be explored in this ticket. What happened afterwards is what needs examination.

The upgrade command that was posted was an attempt to get all nodes on the network onto the same version of the Sovrin package, including sovtoken, which had not previously been installed on MainNet. As such, the upgrade was to sovrin 1.1.41, which has the current version of indy-node installed on MainNet (1.7.1) as a dependency. The current head for indy-node in the repo is 1.8.1.

It appears that following failure of the sovrin package, the upgrade software then installed indy-node 1.8.1. This in spite of:
 * There was a package install error, which should abort the entire process
 * 1.8.1 was *not* the dependent version of the sovrin package being installed

This behavior happened on most nodes. Journalctl and indy logs from ev1 are attached, along with the relevant portion of the config ledger.","Sovrin MainNet, indy-node version 1.7.1, sovrin version various.",,,,,,,,,,,,,,,,,,,,,,,INDY-2245,,,,,,,"02/Jul/19 3:06 AM;mgbailey;config_ledger.txt;https://jira.hyperledger.org/secure/attachment/17378/config_ledger.txt","02/Jul/19 3:06 AM;mgbailey;ev1_log.xz;https://jira.hyperledger.org/secure/attachment/17377/ev1_log.xz","02/Jul/19 3:06 AM;mgbailey;journalctl.txt;https://jira.hyperledger.org/secure/attachment/17376/journalctl.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w29",,,,Unset,Unset,Ev-Node 19.19,Ev-Node 19.20,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),donqui,mgbailey,VladimirWork,,,,,,,,,"10/Oct/19 6:33 AM;donqui;*Findings*
 There was an install error and the process was in fact aborted. What followed was an attempt to rollback any changes made by `apt-get`, and this is what installed the 1.8.1 version.

This happened because the upgrade was from 1.1.6 to 1.1.41:
 * upgrade to 1.1.41 failed because it required sovtoken and sovtokenfees to be specified
 * the rollback tried to rollback to 1.1.6, but because this was an old package it did not have strict dependencies, which caused the newest version to be installed

We now always pin versions and use stric dependencies so a problem like this should never occur.

*Example Test*
 # deploy a pool with 1.9.0 version
 # go to node1 and forcefully update indy crypto packages
 ## apt-get download python3-indy-crypto=0.5.1 libindy-crypto=0.5.1
 ## dpkg --force-all -i *.deb
 # remove apt keys to force PGP errors
 ## apt-key list | grep 'Sovrin-Repo-Master' -B1 | head -n1 | awk ' \{print $2}' | awk -F'/' '\{print $2}'
 # apt-key del \{the outputed num}
 # issue an upgrade command to version 1.9.1

Test demonstrates that in rollback indy-node and indy-plenum versions are pinned as strict dependencies.
{code}
Oct 09 21:10:00 ef9269930a8f env[63]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 09 21:10:00 ef9269930a8f env[63]: Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]
Oct 09 21:10:00 ef9269930a8f env[63]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 09 21:10:00 ef9269930a8f env[63]: Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Oct 09 21:10:01 ef9269930a8f env[63]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Oct 09 21:10:01 ef9269930a8f env[63]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:01 ef9269930a8f env[63]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:01 ef9269930a8f env[63]: Err:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:01 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:01 ef9269930a8f env[63]: Get:7 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1348 kB]
Oct 09 21:10:02 ef9269930a8f env[63]: Err:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:02 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:02 ef9269930a8f env[63]: Get:8 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [991 kB]
Oct 09 21:10:03 ef9269930a8f env[63]: Fetched 2665 kB in 3s (757 kB/s)
Oct 09 21:10:04 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:04 ef9269930a8f env[63]: Building dependency tree...
Oct 09 21:10:04 ef9269930a8f env[63]: Reading state information...
Oct 09 21:10:04 ef9269930a8f env[63]: 49 packages can be upgraded. Run 'apt list --upgradable' to see them.
Oct 09 21:10:04 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:04 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/sdk/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:04 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:04 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/sdk/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:04 ef9269930a8f env[63]: W: Some index files failed to download. They have been ignored, or old ones used instead.
Oct 09 21:10:05 ef9269930a8f env[63]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 09 21:10:05 ef9269930a8f env[63]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 09 21:10:05 ef9269930a8f env[63]: Hit:2 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 09 21:10:06 ef9269930a8f env[63]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 09 21:10:06 ef9269930a8f env[63]: Hit:4 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:06 ef9269930a8f env[63]: Hit:5 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:06 ef9269930a8f env[63]: Err:4 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:06 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:06 ef9269930a8f env[63]: Err:5 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:06 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:06 ef9269930a8f env[63]: Hit:6 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 09 21:10:07 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:08 ef9269930a8f env[63]: Building dependency tree...
Oct 09 21:10:08 ef9269930a8f env[63]: Reading state information...
Oct 09 21:10:08 ef9269930a8f env[63]: 49 packages can be upgraded. Run 'apt list --upgradable' to see them.
Oct 09 21:10:08 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:08 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/sdk/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:08 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:08 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/sdk/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:08 ef9269930a8f env[63]: W: Some index files failed to download. They have been ignored, or old ones used instead.
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '1.0.1-11' for 'libpam-runtime' was not found
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '0.99.7.1' for 'libpam0g' was not found
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '1.18~' for 'init-system-helpers' was not found
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '10.0.0' for 'python3-pip' was not found
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '2.14' for 'libc6' was not found
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '2.1.9' for 'libselinux1' was not found
Oct 09 21:10:10 ef9269930a8f env[63]: E: Version '3.2-14' for 'lsb-base' was not found
Oct 09 21:10:11 ef9269930a8f env[63]: + deps='indy-plenum=1.9.2 indy-node=1.9.2'
Oct 09 21:10:11 ef9269930a8f env[63]: + '[' -z 'indy-plenum=1.9.2 indy-node=1.9.2' ']'
Oct 09 21:10:11 ef9269930a8f env[63]: + echo 'Try to donwload indy version indy-plenum=1.9.2 indy-node=1.9.2'
Oct 09 21:10:11 ef9269930a8f env[63]: Try to donwload indy version indy-plenum=1.9.2 indy-node=1.9.2
Oct 09 21:10:11 ef9269930a8f env[63]: + apt-get -y update
Oct 09 21:10:11 ef9269930a8f env[63]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 09 21:10:11 ef9269930a8f env[63]: Hit:2 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 09 21:10:11 ef9269930a8f env[63]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 09 21:10:11 ef9269930a8f env[63]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 09 21:10:12 ef9269930a8f env[63]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:12 ef9269930a8f env[63]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:12 ef9269930a8f env[63]: Err:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:12 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:12 ef9269930a8f env[63]: Err:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:12 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:13 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:13 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:13 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/sdk/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:13 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:13 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/sdk/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:13 ef9269930a8f env[63]: W: Some index files failed to download. They have been ignored, or old ones used instead.
Oct 09 21:10:13 ef9269930a8f env[63]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-plenum=1.9.2 indy-node=1.9.2
Oct 09 21:10:14 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:14 ef9269930a8f env[63]: Building dependency tree...
Oct 09 21:10:14 ef9269930a8f env[63]: Reading state information...
Oct 09 21:10:14 ef9269930a8f env[63]: You might want to run 'apt-get -f install' to correct these:
Oct 09 21:10:14 ef9269930a8f env[63]: The following packages have unmet dependencies:
Oct 09 21:10:14 ef9269930a8f env[63]:  indy-plenum : Depends: python3-indy-crypto (= 0.4.5) but 0.5.1 is to be installed
Oct 09 21:10:14 ef9269930a8f env[63]: E: Unmet dependencies. Try 'apt-get -f install' with no packages (or specify a solution).
Oct 09 21:10:14 ef9269930a8f env[63]: + ret=100
Oct 09 21:10:14 ef9269930a8f env[63]: + '[' 100 -ne 0 ']'
Oct 09 21:10:14 ef9269930a8f env[63]: + echo 'Failed to obtain indy-plenum=1.9.2 indy-node=1.9.2'
Oct 09 21:10:14 ef9269930a8f env[63]: Failed to obtain indy-plenum=1.9.2 indy-node=1.9.2
Oct 09 21:10:14 ef9269930a8f env[63]: + exit 1
Oct 09 21:10:14 ef9269930a8f env[63]: Upgrade from 1.9.0 to 1.9.2 failed: Command 'upgrade_indy_node ""indy-plenum=1.9.2 indy-node=1.9.2""' returned non-zero exit status 1
Oct 09 21:10:14 ef9269930a8f env[63]: Trying to rollback to the previous version Command 'upgrade_indy_node ""indy-plenum=1.9.2 indy-node=1.9.2""' returned non-zero exit status 1
Oct 09 21:10:14 ef9269930a8f env[63]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 09 21:10:15 ef9269930a8f env[63]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 09 21:10:15 ef9269930a8f env[63]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 09 21:10:15 ef9269930a8f env[63]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 09 21:10:15 ef9269930a8f env[63]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 09 21:10:15 ef9269930a8f env[63]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:16 ef9269930a8f env[63]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:16 ef9269930a8f env[63]: Err:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:16 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:16 ef9269930a8f env[63]: Err:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:16 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:17 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:17 ef9269930a8f env[63]: Building dependency tree...
Oct 09 21:10:17 ef9269930a8f env[63]: Reading state information...
Oct 09 21:10:17 ef9269930a8f env[63]: 49 packages can be upgraded. Run 'apt list --upgradable' to see them.
Oct 09 21:10:17 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:17 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/sdk/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:17 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:17 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/sdk/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:17 ef9269930a8f env[63]: W: Some index files failed to download. They have been ignored, or old ones used instead.
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '1.0.1-11' for 'libpam-runtime' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '0.99.7.1' for 'libpam0g' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '1.18~' for 'init-system-helpers' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '10.0.0' for 'python3-pip' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '2.14' for 'libc6' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '3.3.2-2~' for 'python3' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '1.5' for 'python3-six' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '2.1.9' for 'libselinux1' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: E: Version '3.2-14' for 'lsb-base' was not found
Oct 09 21:10:20 ef9269930a8f env[63]: + deps='indy-plenum=1.9.0 indy-node=1.9.0'
Oct 09 21:10:20 ef9269930a8f env[63]: + '[' -z 'indy-plenum=1.9.0 indy-node=1.9.0' ']'
Oct 09 21:10:20 ef9269930a8f env[63]: + echo 'Try to donwload indy version indy-plenum=1.9.0 indy-node=1.9.0'
Oct 09 21:10:20 ef9269930a8f env[63]: Try to donwload indy version indy-plenum=1.9.0 indy-node=1.9.0
Oct 09 21:10:20 ef9269930a8f env[63]: + apt-get -y update
Oct 09 21:10:21 ef9269930a8f env[63]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 09 21:10:21 ef9269930a8f env[63]: Hit:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 09 21:10:21 ef9269930a8f env[63]: Hit:3 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 09 21:10:21 ef9269930a8f env[63]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 09 21:10:21 ef9269930a8f env[63]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:22 ef9269930a8f env[63]: Err:5 https://repo.sovrin.org/deb xenial InRelease
Oct 09 21:10:22 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:22 ef9269930a8f env[63]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:22 ef9269930a8f env[63]: Err:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 09 21:10:22 ef9269930a8f env[63]:   The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:22 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:23 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:23 ef9269930a8f env[63]: W: An error occurred during the signature verification. The repository is not updated and the previous index files will be used. GPG error: https://repo.sovrin.org/sdk/deb xenial InRelease: The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:23 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:23 ef9269930a8f env[63]: W: Failed to fetch https://repo.sovrin.org/sdk/deb/dists/xenial/InRelease  The following signatures couldn't be verified because the public key is not available: NO_PUBKEY E8BDBE36C8C97811
Oct 09 21:10:23 ef9269930a8f env[63]: W: Some index files failed to download. They have been ignored, or old ones used instead.
Oct 09 21:10:23 ef9269930a8f env[63]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-plenum=1.9.0 indy-node=1.9.0
Oct 09 21:10:23 ef9269930a8f env[63]: Reading package lists...
Oct 09 21:10:23 ef9269930a8f env[63]: Building dependency tree...
Oct 09 21:10:23 ef9269930a8f env[63]: Reading state information...
Oct 09 21:10:23 ef9269930a8f env[63]: indy-node is already the newest version (1.9.0).
Oct 09 21:10:23 ef9269930a8f env[63]: indy-plenum is already the newest version (1.9.0).
{code};;;","10/Oct/19 8:17 PM;VladimirWork;Build Info:
sovrin 1.1.56 stable

Steps to Reproduce:
1. Schedule upgrade to sovrin 1.1.58.
2. Change the repo at any node to RC.

Actual Results:
Upgrade and rollback are failed because of unmet dependencies:
{noformat}
Oct 10 11:07:28 ea01328418cc env[78]: Reading package lists...
Oct 10 11:07:28 ea01328418cc env[78]: Building dependency tree...
Oct 10 11:07:28 ea01328418cc env[78]: Reading state information...
Oct 10 11:07:28 ea01328418cc env[78]: 40 packages can be upgraded. Run 'apt list --upgradable' to see them.
Oct 10 11:07:31 ea01328418cc env[78]: E: Version '1.10.0' for 'indy-node' was not found
Oct 10 11:07:31 ea01328418cc env[78]: /bin/sh: 1: Syntax error: ""("" unexpected
Oct 10 11:07:31 ea01328418cc env[78]: E: No packages found
Oct 10 11:07:31 ea01328418cc env[78]: E: No packages found
Oct 10 11:07:31 ea01328418cc env[78]: E: No packages found
Oct 10 11:07:32 ea01328418cc env[78]: + deps='sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 sovrin=1.1.58'
Oct 10 11:07:32 ea01328418cc env[78]: + '[' -z 'sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 sovrin=1.1.58' ']'
Oct 10 11:07:32 ea01328418cc env[78]: + echo 'Try to donwload indy version sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 sovrin=1.1.58'
Oct 10 11:07:32 ea01328418cc env[78]: Try to donwload indy version sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 sovrin=1.1.58
Oct 10 11:07:32 ea01328418cc env[78]: + apt-get -y update
Oct 10 11:07:33 ea01328418cc env[78]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 10 11:07:33 ea01328418cc env[78]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 10 11:07:33 ea01328418cc env[78]: Hit:3 https://repo.sovrin.org/deb xenial InRelease
Oct 10 11:07:33 ea01328418cc env[78]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 10 11:07:33 ea01328418cc env[78]: Hit:5 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 10 11:07:35 ea01328418cc env[78]: Reading package lists...
Oct 10 11:07:35 ea01328418cc env[78]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 
Oct 10 11:07:36 ea01328418cc env[78]: Reading package lists...
Oct 10 11:07:36 ea01328418cc env[78]: Building dependency tree...
Oct 10 11:07:36 ea01328418cc env[78]: Reading state information...
Oct 10 11:07:37 ea01328418cc env[78]: E: Version '1.10.0' for 'indy-node' was not found
Oct 10 11:07:37 ea01328418cc env[78]: + ret=100
Oct 10 11:07:37 ea01328418cc env[78]: + '[' 100 -ne 0 ']'
Oct 10 11:07:37 ea01328418cc env[78]: + echo 'Failed to obtain sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 sovrin=1.1.58'
Oct 10 11:07:37 ea01328418cc env[78]: Failed to obtain sovtoken libindy-crypto=0.4.5 sovtokenfees=1.0.3 sovtoken=1.0.3 indy-node=1.10.0 sovrin=1.1.58
Oct 10 11:07:37 ea01328418cc env[78]: + exit 1
Oct 10 11:07:37 ea01328418cc env[78]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 10 11:07:37 ea01328418cc env[78]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 10 11:07:37 ea01328418cc env[78]: Hit:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 10 11:07:37 ea01328418cc env[78]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 10 11:07:37 ea01328418cc env[78]: Hit:4 https://repo.sovrin.org/deb xenial InRelease
Oct 10 11:07:40 ea01328418cc env[78]: Hit:5 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 10 11:07:41 ea01328418cc env[78]: Reading package lists...
Oct 10 11:07:42 ea01328418cc env[78]: Building dependency tree...
Oct 10 11:07:42 ea01328418cc env[78]: Reading state information...
Oct 10 11:07:42 ea01328418cc env[78]: 40 packages can be upgraded. Run 'apt list --upgradable' to see them.
Oct 10 11:07:45 ea01328418cc env[78]: /bin/sh: 1: Syntax error: ""("" unexpected
Oct 10 11:07:45 ea01328418cc env[78]: E: No packages found
Oct 10 11:07:45 ea01328418cc env[78]: E: No packages found
Oct 10 11:07:45 ea01328418cc env[78]: E: No packages found
Oct 10 11:07:46 ea01328418cc env[78]: + deps='sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libindy-crypto=0.4.5 sovrin=1.1.56'
Oct 10 11:07:46 ea01328418cc env[78]: + '[' -z 'sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libindy-crypto=0.4.5 sovrin=1.1.56' ']'
Oct 10 11:07:46 ea01328418cc env[78]: + echo 'Try to donwload indy version sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libindy-crypto=0.4.5 sovrin=1.1.56'
Oct 10 11:07:46 ea01328418cc env[78]: Try to donwload indy version sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libindy-crypto=0.4.5 sovrin=1.1.56
Oct 10 11:07:46 ea01328418cc env[78]: + apt-get -y update
Oct 10 11:07:46 ea01328418cc env[78]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 10 11:07:47 ea01328418cc env[78]: Hit:2 https://repo.sovrin.org/deb xenial InRelease
Oct 10 11:07:49 ea01328418cc env[78]: Hit:3 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 10 11:07:49 ea01328418cc env[78]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 10 11:07:50 ea01328418cc env[78]: Hit:5 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 10 11:07:51 ea01328418cc env[78]: Reading package lists...
Oct 10 11:07:51 ea01328418cc env[78]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libi
Oct 10 11:07:53 ea01328418cc env[78]: Reading package lists...
Oct 10 11:07:53 ea01328418cc env[78]: Building dependency tree...
Oct 10 11:07:53 ea01328418cc env[78]: Reading state information...
Oct 10 11:07:53 ea01328418cc env[78]: indy-plenum is already the newest version (1.9.2).
Oct 10 11:07:53 ea01328418cc env[78]: libindy-crypto is already the newest version (0.4.5).
Oct 10 11:07:53 ea01328418cc env[78]: sovrin is already the newest version (1.1.56).
Oct 10 11:07:53 ea01328418cc env[78]: sovtokenfees is already the newest version (1.0.2).
Oct 10 11:07:53 ea01328418cc env[78]: sovtoken is already the newest version (1.0.2).
Oct 10 11:07:53 ea01328418cc env[78]: indy-node is already the newest version (1.9.2).
Oct 10 11:07:53 ea01328418cc env[78]: Some packages could not be installed. This may mean that you have
Oct 10 11:07:53 ea01328418cc env[78]: requested an impossible situation or if you are using the unstable
Oct 10 11:07:53 ea01328418cc env[78]: distribution that some required packages have not yet been created
Oct 10 11:07:53 ea01328418cc env[78]: or been moved out of Incoming.
Oct 10 11:07:53 ea01328418cc env[78]: The following information may help to resolve the situation:
Oct 10 11:07:53 ea01328418cc env[78]: The following packages have unmet dependencies:
Oct 10 11:07:53 ea01328418cc env[78]:  sovrin : Depends: sovtoken (= 1.0.2) but 1.0.3 is to be installed
Oct 10 11:07:53 ea01328418cc env[78]:  sovtoken : Depends: indy-node (= 1.10.0) but 1.9.2 is to be installed
Oct 10 11:07:53 ea01328418cc env[78]: E: Unable to correct problems, you have held broken packages.
Oct 10 11:07:53 ea01328418cc env[78]: + ret=100
Oct 10 11:07:53 ea01328418cc env[78]: + '[' 100 -ne 0 ']'
Oct 10 11:07:53 ea01328418cc env[78]: + echo 'Failed to obtain sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libindy-crypto=0.4.5 sovrin=1.1.56'
Oct 10 11:07:53 ea01328418cc env[78]: Failed to obtain sovtoken indy-plenum=1.9.2 sovtoken=1.0.2 sovtokenfees=1.0.2 indy-node=1.9.2 libindy-crypto=0.4.5 sovrin=1.1.56
Oct 10 11:07:53 ea01328418cc env[78]: + exit 1
{noformat}
So no packages were upgraded because of strict dependencies.
The issue with failing rollback will be fixed in scope of separate ticket.
;;;",,,,,,,,,,,,,,,,,,,,
Indy-Node 1.9.1 Release,INDY-2162,40907,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,esplinr,03/Jul/19 10:09 PM,02/Aug/19 8:59 PM,28/Oct/23 2:47 AM,02/Aug/19 8:54 PM,,,1.9.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,INDY-2095,,,,,,INDY-2197,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t78:",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sovrin GPG keys should be updated in all scripts and dockerfiles,INDY-2163,40923,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,anikitinDSR,sergey.khoroshavin,sergey.khoroshavin,04/Jul/19 7:44 PM,08/Jul/19 5:51 PM,28/Oct/23 2:47 AM,08/Jul/19 5:49 PM,,,,,,,,0,devops,,,,Due to recent rotation of sovrin keys all mentions of old key 68DB5E88 needs to be replaced by new one: CE7709D068DB5E88,,,,,,,,,,,,,,INDY-2095,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969uv",,,,Unset,Unset,Ev-Node 19.13,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,sergey.khoroshavin,,,,,,,,,,"08/Jul/19 5:51 PM;anikitinDSR;PR's:
 indy-plenum:
 * [https://github.com/hyperledger/indy-plenum/pull/1257]
 indy-node:
 * [https://github.com/hyperledger/indy-node/pull/1374]
 token-plugin:
 * [https://github.com/sovrin-foundation/token-plugin/pull/275]

For now image with master's repository is:
 * hyperledger/indy-core-baseci:0.0.3-master
 and for stable:
 * hyperledger/indy-core-baseci:0.0.3-stable

 ;;;",,,,,,,,,,,,,,,,,,,,,
Incorrect request validation,INDY-2164,40927,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,05/Jul/19 12:09 AM,23/Jul/19 7:10 PM,28/Oct/23 2:47 AM,23/Jul/19 7:10 PM,1.8.0,1.9.0,1.9.1,,,,,0,EV-CS,TShirt_M,,,"Build Info:
sovrin 1.1.49

Steps to Reproduce:
1. Upgrade AWS persistent pool from sovrin 1.1.46 to sovrin 1.1.49 with force=False.
2. Try to write DID with verkey that already exists in ledger.

Actual Results:
Client receives *Consensus is impossible: Pool timeout* error since there are responses with REJECTs from 3-5 nodes from 25 only.

Expected Results:
Client should receive REJECTs from all nodes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/19 12:10 AM;VladimirWork;client_output_2164.txt;https://jira.hyperledger.org/secure/attachment/17380/client_output_2164.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969uw",,,,Unset,Unset,Ev-Node 19.14,Ev-Node 19.15,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,"05/Jul/19 9:16 PM;VladimirWork;The issue reproduces against Persistent AWS pool only (both for 1.1.45 and 1.1.49 sovrin).
The same case against Load AWS pool (clear ledger and without upgrade sequence) returns no errors (both for 1.1.46 and 1.1.49 sovrin).;;;","06/Jul/19 12:11 AM;Toktar;*PoA:*
 * Add  to ZStack.open()
{code:java}
self.listener.setsockopt(zmq.ROUTER_MANDATORY, 1){code}

 * Rename _transmitThroughListener_ to __transmit_one_msg_through_listener_
 ** _add a catching of zmq.Again and zmq.ZMQError(with 113 code)_
 ** _add parameter need_to_resend to method reply_
 * Add the method _transmitThroughListener_ which will
 ** store a new message in the __pending___client___messages_ dictionary _remoteName  ->_ (last_update_ts, List[ msg ])
 ** do __transmit_one_msg_through_listener()_ for every message for  _remoteName_ from the  _ _pending___client___messages_ 
 ** remove every sent message from the dictionary, if it doesn't need re-sending
 * Add waiting messages re-sending by schedule every RESEND_CLIENT_MSG_TIMOUT via QueueTimer
 * Add removing of old messages from __pending___client___messages_ by REMOVE_CLIENT_MSG_TIMOUT for every client messages sending 
 * Limit size of __pending___client___messages_ by constant PENDING_CLIENT_MESSAGES_LIMIT
 * Limit size of the list of messages in value from __pending___client___messages_ by constant PENDING_MESSAGES_FOR_ONE_CLIENT_LIMIT
 * Add unit tests;;;","17/Jul/19 4:36 PM;Toktar;PR: [https://github.com/hyperledger/indy-plenum/pull/1260];;;","17/Jul/19 10:38 PM;VladimirWork;Testing PoA:

1. Check fix against 7~11 nodes docker pool with traffic shaping tool simulating bad network.
2. Check fix against Persistent AWS pool (extracted from the latest backup and manually upgraded to master).
3. Get actual builder net / staging net genesis txns and valid DID and check fix against any of this pools.
4. Explore issue with successful and unsuccessful responses after pool restart (if initial issue will be reproduced in scope of previous steps).;;;","17/Jul/19 11:19 PM;Toktar;*Problem reason:*
 - When a node sends a message to unknown client, the message will be skipped without any exceptions.

*Changes:*
 - Messages that have not been sent are sent later with other messages for this client.

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1382]
 * [https://github.com/hyperledger/indy-plenum/pull/1260]

*Version:*
 * indy-node 1.9.1030 -master
 * (indy-plenum 1.9.843 -master)

*Risk factors:*
 - Messages for a client will not be sent.

*Risk:*
 - Medium

*Test:*
 * [test_send_client_msgs_with_delay_reqs.py|https://github.com/hyperledger/indy-plenum/pull/1260/files#diff-72ea619b115b8f258e236251093d03b0] 
 * [test_stashed_client_messages.py|https://github.com/hyperledger/indy-plenum/pull/1260/files#diff-b7fd8e10b3f29bccc881665051168d87] 

*Recommendations for QA:*
 * Start a docker pool with 4 nodes
 * Block traffic between Node2, Node3, Node4 and a client
 * Send invalid request which shouldn't pass a dynamic validation.
 * Unblock traffic between Node2 and client
 * Send valid request.
 * Check that the client get the quorum(2) of rejects.
 * Check that the client received a reply for the second request;;;","23/Jul/19 7:09 PM;VladimirWork;Build Info:
indy-node 1.9.0~1034

Steps to Validate:
1. Start a docker pool with 4 nodes.
2. Block traffic between Node2, Node3, Node4 and a client.
3. Send invalid request which shouldn't pass a dynamic validation.
4. Unblock traffic between Node2 and client.
5. Send valid request.

Actual Results:
Client receives reject and reply for 1st and 2nd request respectively.

Additional Info:
Case is also verified against persistent AWS pool (where the issue was found).;;;",,,,,,,,,,,,,,,,
System Tests,INDY-2165,40953,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,09/Jul/19 10:27 PM,10/Jul/19 5:39 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-12,,System Tests,To Do,No,,Unset,No,,,"1|i00thw:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CD pipeline system tests running improvements,INDY-2166,40954,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,anikitinDSR,VladimirWork,VladimirWork,09/Jul/19 10:40 PM,10/Jul/19 5:36 PM,28/Oct/23 2:47 AM,10/Jul/19 5:36 PM,,,,,,,,0,devops,system-tests,,,"1. Run system tests for sovrin or indy-node + plugins packages.
 Option: run nightly pipeline twice - for latest indy-node and for latest plugins (+indy-node that is bind to this plugins).

2. Run test for rc/stable always for sovrin package.
 Option: move this pipeline to sovrin CD instead of indy-node CD.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2165,,,No,,Unset,No,,,"1|i00ti4:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"10/Jul/19 5:34 PM;ashcherbakov;Will be done in https://sovrin.atlassian.net/browse/ST-610;;;",,,,,,,,,,,,,,,,,,,,,
Integrate PrimarySelector into View Change Service,INDY-2167,40970,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,10/Jul/19 5:14 PM,01/Oct/19 7:59 PM,28/Oct/23 2:47 AM,08/Aug/19 6:52 PM,,,1.10.0,,,,,0,,,,,"ViewChangerService needs to correctly select next primary taking into account pool state (demoted/promoted nodes, etc.).

So, it needs to do it the same way as PrimarySelector does.",,,,,,,,,,,,,,,,,,,,,,,,INDY-2201,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwqo7:",,,,Unset,Unset,Ev-Node 19.15,Ev-Node 19.16,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"05/Aug/19 3:57 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-plenum/pull/1290;;;","06/Aug/19 6:16 PM;ashcherbakov;*Changes:*
 * Primary Selector has been simplified without changing the logic (still selects in round robin according to the historical order of adding nodes)
 * Primary Selector is integrated into View Change Service and tests

*PR:*

[https://github.com/hyperledger/indy-plenum/pull/1290];;;","08/Aug/19 6:54 PM;ashcherbakov;In the current task the current logic were ported. 
Most probably the logic can be simplified. This will be done in https://jira.hyperledger.org/browse/INDY-2201;;;",,,,,,,,,,,,,,,,,,,
GitLab CI Research,INDY-2168,40973,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,10/Jul/19 10:27 PM,12/Feb/20 11:44 PM,28/Oct/23 2:47 AM,14/Aug/19 10:45 PM,,,,,,,,0,,,,,"*Acceptance Criteria*
 * Learn about GitLab CI
 * Evaluate efforts to move Indy SDK from Jenkins to GitLab
 * Estimate effort of moving Indy Node from Jenkins to GitLab CI

*Current version of GitLab CI/CD*: 12.1

*The things to check* (requirements for the CI server, filling is in progress):
 +CI+
||Requirement||Supported||Notes||
|build PRs (from forks / branches) on a merged result|{color:#de350b}no{color}|see [comment|https://jira.hyperledger.org/browse/INDY-2168?focusedCommentId=62504&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-62504]|
|pass build statuses back to GitHub|{color:#de350b}no {color}|see [comment|https://jira.hyperledger.org/browse/INDY-2168?focusedCommentId=62504&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-62504]|
|do not use pipeline changes in PRs from untrusted authors (non-maintainers)|{color:#de350b}no {color}|see [link|https://docs.gitlab.com/12.1/ce/ci/merge_request_pipelines/index.html#important-notes-about-merge-requests-from-forked-projects] where they mention that it's not possible (in 12.1)|
|ability to run jobs in parallel and sequentially|yes|[link|https://docs.gitlab.com/12.1/ce/ci/yaml/README.html#stages]|
|(SDK) Ability to share artifacts between jobs|yes|[link1|https://docs.gitlab.com/12.1/ce/ci/caching/index.html#cache-vs-artifacts], [link2|https://docs.gitlab.com/ee/ci/yaml/README.html#dependencies] |
|support dockers|yes|[link|https://docs.gitlab.com/12.1/runner/executors/docker.html] |
|(SDK) support linked jobs running on multiple runners in scope of one pipeline (services)|{color:#de350b}seems no{color} |as far as i understand [services|https://docs.gitlab.com/ee/ci/yaml/README.html#services] are run on the same runner only, but [the kubernetes executor|https://docs.gitlab.com/12.1/runner/executors/kubernetes.html] should be checked as an option|
|(nice to have) support (parse and display in UI) junit-xml style test reports|yes |[link1|https://docs.gitlab.com/12.1/ce/ci/yaml/README.html#artifactsreportsjunit], [link2|https://docs.gitlab.com/12.1/ce/ci/junit_test_reports.html] |

+CD+
||Requirement|| ||Supported||Notes||
|Keep secrets safe| | | |
| |run CD pipelines only on trusted runners|{color:#de350b}seems no{color}|[supported|https://docs.gitlab.com/12.1/ce/ci/pipelines.html#security-on-protected-branches] only for GitLab repositories protected branches, no support for GitHub |
| |do not run any non-CD pipelines where CD ones are run OR provide strong env isolation|{color:#de350b}no {color}|even for GitLab repositories there is no way to protect pipeline against changes from untrusted PRs (mentioned above)|
| |ability to mask secret values in logs|yes |[link|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#masked-variables] |
|support the following secret types:| | | |
| |secret text (token)|yes|[link|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#variable-types] |
| |username / password pars|yes|as two variables|
| |secret files|yes |[link|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#variable-types] |
| |ssh keys|yes|as one secret file [link|https://docs.gitlab.com/12.1/ce/ci/ssh_keys/README.html] |
| |(nice to have) ssh keys with passphrases|no |only variables and files are [supported|https://docs.gitlab.com/12.1/ce/ci/variables/README.html#variable-types]| |",,,,,,,,,,,,,,,,,,,,,,,,IS-1496,IS-1497,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7e:i",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,esplinr,SteveGoob,,,,,,,,"01/Aug/19 6:48 PM;andkononykhin;[~esplinr] [~ashcherbakov] [~sergey.khoroshavin]

While working on that task and learning GitLab CI/CD I've encountered some facts that might block any transition to that CI engine.

Please check the following docs and related issues on GitLab ticket tracking:
 # Gitlab CI/CD doesn't support pipeline triggering for GitHub PRs from forks:
 ** [https://docs.gitlab.com/12.1/ce/ci/merge_request_pipelines/index.html#requirements-and-limitations]
 ** [https://gitlab.com/gitlab-org/gitlab-ee/issues/5667]
 # GitLab CI/CD doesn't have good support merge requests from GitLab repositories forks as well: CI status for merge requests form forks are not trusted since pipelines are run on forked projects infrastructure for security reasons. The links:
 ** [https://docs.gitlab.com/12.1/ce/ci/merge_request_pipelines/#important-notes-about-merge-requests-from-forked-projects]
 ** [https://docs.gitlab.com/12.1/ce/ci/merge_request_pipelines/pipelines_for_merged_results/index.html#requirements-and-limitations]
 ** [https://gitlab.com/gitlab-org/gitlab-ee/issues/11934]

+When it might be changed+
 * There are a set of tasks related to that and some of them are in progress and attached to coming milestones (in optimistic scenario - in a few months).

+Current possible workarounds (for GitHub PRs from forks)+
 * Mentioned [Issue 5667|https://gitlab.com/gitlab-org/gitlab-ee/issues/5667]  refers to a set of workarounds that might help:
 ## [the first|https://gitlab.com/gitlab-org/gitlab-ee/issues/5667#note_144850858]: requires manual efforts
 ## [the second|https://gitlab.com/gitlab-org/gitlab-ee/issues/5667#note_144893904]:  more automated, introduces bot, requires bot configuration and deployment efforts
 ## [the third|https://gitlab.com/gitlab-org/gitlab-ee/issues/5667#note_146778387]:  also a bot, seems the most promising
 * BUT the strong concern mentioned there regarding all workarounds is about the security: secrets (usually exposed for CD pipeline) might be disclosed and shared hardware might be impacted by ""bad"" PRs from untrusted forks

Thus, my vision:
 * GitLab CI/CD for now is not enough ready to support Continuous Integration (I mean exactly that only: integration of changes in scope of any kind of change requests) OSS projects since it can't securely serve PRs from forks (even with mentioned workarounds)
 * CD pipelines (with all their secrets) can't be combined with CI on GitLab CI/CD since they would share resources and I currently don't see (understand) ways to separate them

I see only one option with GitLab CI/CD for now:
 * keep repositories on GitHub, move only CI for indy projects keeping CD on Jenkins, requires:
 ** apply some workaround to support PR from GitHub forks

[~esplinr]  :
 * does Sovrin Foundation is aware of that blockers?
 * do they have any better workarounds / plans to overcome that?;;;","02/Aug/19 6:24 PM;andkononykhin;[~SteveGoob] [~ashcherbakov] [~sergey.khoroshavin]

Even with mentioned workarounds for CI it's not safe to run pipelines for untrusted PRs when any secrets are exposed or protected runners are set up: GitLab server uses single yaml file for all stages (test, deploy) and all kind of triggers (branch, PR). A [link|https://docs.gitlab.com/12.1/ce/ci/merge_request_pipelines/index.html#important-notes-about-merge-requests-from-forked-projects].

Until GitLab fixes that I think we should consider one more option as an addition to one mentioned in the previous comment:
 * I think it might be reasonable to use two GitLab CI/CD  separate instances for CI and CD
 ** CI server:
 *** its tokens for runners are published
 *** doesn't expose any env variables with secrets
 *** processes merge requests only
 ** CD server:
 *** only trusted runners are attached
 *** works with secrets defined in the project/server settings (not in a yaml which is a part of the repository code)
 *** uses custom path to GitLab yaml file: a [link|https://docs.gitlab.com/12.1/ce/user/project/pipelines/settings.html#custom-ci-config-path]
 *** processes branches only;;;","02/Aug/19 7:58 PM;ashcherbakov;I would like to summarize our security concerns.

There are two main groups of concerns:
 # *Untrusted code runs in a protected environment with an access to protected data, so that secrets can be leaked and compromised, or malicious artifacts can be produced.* 

 * 
 ** We must make sure that this never happens
 ** We need to make sure that a random (potentially malicious) PR from community fork on GitHub is never executed by protected runners on GitLab CI, and it can never get access to secretes regardless of how badly yaml file is changed in the PR

 # *Untrusted code runs in unprotected environment, so that it can temporarily or forever crash an unprotected runner*
 ** This has less priority than the first, but may also be dangerous, since it may lead to all unprotected runners broken/unavailable because of changes in yaml and malicious scripts
 ** It looks like there is no protection against this since Gitlab CI always used a a versions of yaml changed in the PR (see https://gitlab.com/gitlab-org/gitlab-ce/issues/20826). There are some workarounds (need to try) to allow running by trusted contributors only, but it may be not so convenient for community and a bit against CI purpose.;;;","03/Aug/19 7:22 AM;SteveGoob;[~andkononykhin], [~ashcherbakov], [~esplinr],

I spent the last 4 hours writing up responses to your questions, my findings, and references on the security issues this project entails, but Jira just refreshed and I lost my entire response. >: ( I might write it up again later but for now, I'm just going to summarize my findings:

* We can properly protect credentials using protected runners and defining secrets in environment variables in the local configurations of trusted runners.
* Assuming the above, we can prevent credentials from being leaked in logs.
* Cache poisoning on CI is a valid concern and is unavoidable, given that both the runners and code are untrusted.
* Arbitrary code execution on runners through the gitlab-ci.yml file is a concern on CI, but assuming it is protected on protected branches using the above fixes, this should not affect our delivery pipelines. To protect CI runners, we must use sandboxed environments everywhere (yes, including MacOS and Windows). That topic requires further investigation, (kubernetes, virtualbox runners, etc). 
* We cannot solve the impersonation problem without splitting up our CI/CD into separate repos. (A malicious community runner can be selected for CD instead of just CI). However, assuming the above fixes, we can keep them from directly accessing the secrets required to publish packages.
* Because of the impersonation problem, we are vulnerable to cache poisoning attacks in CD. Disabling cache/artifacts could be a solution, but one that would require a large amount of duplicated work in the pipeline. Artifact signing could also be used to verify the authenticity of the artifact, but adds a lot of extra complexity to the pipeline.


Most of the remaining problems are solved by splitting up the repos for CI and CD (most solutions get easier as well). I am in favor of this approach, personally. However, arbitrary code execution via gitlab-ci.yml and cache poisoning in the CI pipeline will still be security issues. If we can properly set up sandboxing, and restrict creation of community runners to trusted members of the community, then we should be greatly reduce the risk of tampering in the project and potential damage to community runners. 

It is worth mentioning that the cache poisoning problem would be an issue for literally any CI system when considering the addition of community runners. It is not Gitlab specific.;;;","03/Aug/19 7:23 AM;SteveGoob;*Note:* this all assumes that we solve the github PR problem, as [~andkononykhin] mentioned.;;;","03/Aug/19 7:25 AM;SteveGoob;I believe the way forward would be to first construct a new CD pipeline on Gitlab and then leave Jenkins to do CI in the meantime. This avoids the PR problem and shenanigans with untrusted pipeline code and runners. What are everyone else's thoughts?;;;","05/Aug/19 3:17 PM;ashcherbakov;{quote}construct a new CD pipeline on Gitlab and then leave Jenkins to do CI in the meantime
{quote}
[~SteveGoob]
I would prefer to do the opposite. We are facing a critical issue on Jenkins CI when testing SDK, since the current agents are not powerful enough. 
So, moving to GitLab CI (assuming that we have more runners, and these runners are more powerful), is a very high priority. 
Another options would be to improve the current agents on Jenkins....

As for the next steps to try, I would try to run it with GitHab PRs from forks. As for security concerns, it really depends on how these forks will work, and whether CI can run on unprotected runners only (we need to double-check this, since protected concept is GitLab specific, and we need to make sure it works properly with GitHub forks).;;;","07/Aug/19 5:37 AM;SteveGoob;[~ashcherbakov], that's a fair point. However, if replacing the Jenkins CI is such an urgent task, it would make sense to omit the community runner functionality for the initial deployment, in order to simplify development and get it moved over sooner. 

So how's this for the plan:
# Enable PR mirroring in from Github to Gitlab
# Build a ""trusted"" CI pipeline
# Reach feature parity for the pipeline. (Test what's being tested in Jenkins and successfully report to github)
# Switch to Gitlab for CI.

Then we can move forward from there. Sound good to everyone? 

cc. [~andkononykhin], [~esplinr][~sergey.khoroshavin];;;","08/Aug/19 8:22 PM;ashcherbakov;Sounds like a good plan.
I agree that we should start with enabling PRs mirroring and make them working (items 1 - 3 from the plan).
However not sure that we should do the 4th item (switching to GitLab) without having (or being able to have) more runners.

Nevertheless, I think that `community runner functionality` task doesn't have a lot of risks, this is rather a technical work, so we can postpone it.
Supporting PRs from GitHub has some risk, so I would do it first as proposed by the plan.;;;","14/Aug/19 10:45 PM;esplinr;We have done the research expected by this story, and will raise future issues for additional research into our future CI pipeline.

An additional topic for investigation would be figuring out how to use GitHub Actions to integrate with GitLab CI. cc [~SteveGoob];;;",,,,,,,,,,,,
Integrate OrderingService into Replica,INDY-2169,41257,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,17/Jul/19 9:58 PM,01/Oct/19 7:59 PM,28/Oct/23 2:47 AM,16/Aug/19 10:20 PM,,,1.10.0,,,,,0,,,,,"* Most of Replica code will be removed in favor of OrderingService
* Replica will be a bridge between new approach (OrderingService) and legacy Node
* All tests must pass",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98633",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"08/Aug/19 10:13 PM;Toktar;PR: [https://github.com/hyperledger/indy-plenum/pull/1280];;;","16/Aug/19 9:25 PM;Toktar;*Problem reason:*
 - The OrderingService is not used in the Replica.

*Changes:*
 - Integrate the OrderingService  to the Replica
 - Remove old storages and methods from the Replica
 - Update tests

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1413]
 * [https://github.com/hyperledger/indy-plenum/pull/1299]
 * [https://github.com/hyperledger/indy-plenum/pull/1280]

*Version:*
 * token-plugin 1.0.2 ~78 -master
 * indy-node 1.9.2.dev1057 -master
 * (indy-plenum 1.9.2.dev868 -master)

*Risk factors:*
 - Transaction ordering
 - Replica removing
 - Ordering after catchup
 - Ordering after view change

*Risk:*
 - Medium

*Recommendations for QA:*
 * Do a production load
 * Test view change, catchup and replica removing during the load test;;;",,,,,,,,,,,,,,,,,,,,
Create new DID is impossible without DID-based signature,INDY-2170,41297,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,,sergey.minaev,sergey.minaev,18/Jul/19 8:35 PM,24/Jul/19 12:16 AM,28/Oct/23 2:47 AM,19/Jul/19 11:46 PM,,,,,,,,0,,,,,"Steps to reproduce (with samples from CLI):
 1) set auth_rules to allow DID creation without any signatures
{code}
ledger auth-rule txn_type=NYM action=ADD field=role new_value=0 constraint={""constraint_id"":""ROLE"",""need_to_be_owner"":false,""role"":""*"",""sig_count"":0}
{code}
2) try to send NYM txn without signatures
{code}
pool(test_local):wallet(w1):did(V4S...e6f):indy> ledger custom {""identifier"":""H8qjhVJ8Xmge5pnHiP53Lg"",""operation"":{""dest"":""H8qjhVJ8Xmge5pnHiP53Lg"",""type"":""1"",""verkey"":""~QYSTFV75TxqjZMQ3399bbH"",""role"":""0""},""protocolVersion"":2,""reqId"":1563447469020587500}
Transaction has been rejected: client request invalid: MissingSignature()
{code}
Expected behavior: txn should be accepted
 Actual result: txn is rejected
{code:java}
""{\""op\"":\""REQNACK\"",\""identifier\"":\""H8qjhVJ8Xmge5pnHiP53Lg\"",\""reqId\"":1563447469020587500,\""reason\"":\""client request invalid: MissingSignature()\""}""
{code}
Note: in case of plugin installed on the system and auth_rules with metada usage, the error message is slightly different ""InsufficientCorrectSignatures(0, 1)""",,,,,,,,,,,,,,,,,,,INDY-2171,,IS-1320,INDY-1999,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98614",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,sergey.minaev,,,,,,,,,,"19/Jul/19 11:42 PM;esplinr;The use case originally considered in this bug is being addressed in INDY-2171.

However, this bug identifies two problems with the implementation of auth_rules:
* The documentation should be clear that the number of signatures specified in auth_rules is the number of DID based signatures and does not consider signatures used by ledger plugins (such as payment signatures).
* auth_rules that specify that domain transactions with less than 1 signature should be rejected as invalid (this wouldn't apply to transaction types added by plugins)
** They would circumvent any ledger author agreements
** They could allow people to anchor transactions attributed to DIDs that they do not control;;;","19/Jul/19 11:44 PM;esplinr;We will improve the documentation, but we won't be changing auth_rules to require always at least 1 signature. This will complicate dealing with transactions types added by ledger plugins (such as payment plugins). This would also prevent future potential use cases that might be useful for developers. See the note in INDY-2171. We'll accept the current behavior as adequate and will reconsider if we have feedback that it should be changed.;;;","19/Jul/19 11:46 PM;esplinr;We will close this bug as ""deferred"". We addressed the key use case in INDY-2171, but this use case of not requiring any signature is one we don't think we should do at this time. Even though rules that allow a zero signatures will not work in practice, we believe that it will be rare for people to try to do that once we address INDY-2171.;;;",,,,,,,,,,,,,,,,,,,
New DIDs can be created without endorsers,INDY-2171,41301,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,18/Jul/19 11:28 PM,30/Jul/19 5:02 AM,28/Oct/23 2:47 AM,30/Jul/19 5:02 AM,,,1.9.1,,,,,0,,,,,"*Story*
As an administrator of an Indy network, I want to be able to configure auth_rules such that new network users can write their own nyms to the public ledger so that the ledger can be managed in a completely decentralized way.

*Acceptance Criteria*
* The ledger can be configured so that new DIDs can be anchored without a signature from an entity already on the ledger.
** The configuration happens in auth_rules.
* The transaction that anchors the DID is signed with the DID that will be added to the ledger.
** This ensures that any transaction author agreements are signed.
** This ensures that someone cannot anchor a DID that they do not control with their private keys.

*Notes*
* It is expected that decentralized public write access allows unknown users to anchor their own nym to the ledger if they provide payment through a payment plugin without going through an endorser for approval.
* Allowing anonymous users to anchor nyms will avoid certain ledger checks for validity that could reduce the system's resilience to denial of service attacks, but we don't consider that to be a significant risk. Other levels of DOS protection needs to be instituted by the network administrators regardless of the ledger checks, such as firewall rules to blacklist hostile connections.
* We considered whether we should allow transactions besides nym creation to be allowed without on-ledger signatures because use cases with peer DIDs that could benefit. We decided against doing this work because we don't have customer demand or clear requirements.
* We also considered whether transactions should be allowed without any signatures at all for some development use cases, as previously considered in INDY-2170. But we similarly decided not to do this work without clear demand.
",,,,,,,,,,,,,,,,,,,,INDY-2170,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9860o",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,esplinr,VladimirWork,,,,,,,,"22/Jul/19 7:45 PM;Derashe;PoA

For implementing such a functionality, we need to make changes to a few request handling modules.
 * Signature validation (validateClientMsg) - block, executed before static validation, where we check request's signature(s). We need to add more validaton to _getVerkey_ method. So it could validate message without identifier stored in ledger/state, in case this is nym request and return verkey from operation
 * Auth rules validation.
 ** New optional boolean field ""non_ledger_did"" added to the AuthAction and constraints structure. 
 ** Additional dynamic validation in nym_handler, so it will check if identifier == dest in case of non-ledger addition. It will also pass AuthAction with non_ledger_did==True in case identifier DID doesn't exist. 
 ** Validation for non_ledger_did at RolesAuthorizer
 ** New non_ledger_did AUTH_RULE flag
 ** By default, non_ledger_did will mostly be set as False. For now it seems reasonable to turn it on only for nym txn.
 * Cover this changes with tests.
 ** Unit and integration tests in indy-node and plenum
 ** Payment cases on plugin's side;;;","26/Jul/19 8:47 PM;Derashe;Problem reason/description: 
 We need to implement non-ledger writing feature and ability to enable/disable it. 

Changes: 
 Feature implemented by modifying signature verification and is configurable by auth_rules

PR:
 [https://github.com/hyperledger/indy-plenum/pull/1276]

[https://github.com/hyperledger/indy-node/pull/1396]

[https://github.com/sovrin-foundation/token-plugin/pull/283]

Version:
 indy-node 1.9.1~dev1042

Recommendations for QA
 * tests for indy node:
 ** Ensure that a DID not present on the ledger can not create new NYMs by default.
 ** set AuthConstraint for nym transaction like: role=*, off_ledger_signature=True, sig_count=1. Ensure that nym that has not been written on ledger, can send nym txn of himself, with his signature included (that is it needs to be signed against the newly created key)
 ** set AuthConstraint for nym transaction like: role=*, off_ledger_signature=True, sig_count=2. Ensure that nym that has not been written on ledger, can send nym txn of himself, with his signature and someone's on-ledger did signature included.
 ** set AuthConstraint for nym transaction like: role=*, off_ledger_signature=True, sig_count=0. Ensure that nym that has not been written on ledger, can send nym txn of himself, with his signature included.
 ** set AuthConstraint for nym transaction like: role=*, off_ledger_signature=False, sig_count=1. Ensure that this nym cannot send txn.
 * test for plugins:
 ** set AuthConstraint for nym transaction like: role=trustee OR role = '*' off_ledger_signature=True metadata=\{plguin_requirements: amount}. Ensure that trustee can write nym without plugin requirements and that off-ledger nym can write only with satisfying plugin requirements;;;","26/Jul/19 9:33 PM;ashcherbakov;Also please note, that
 * `off_ledger_signature=True ` can be set for any ( * ) role only.
 * An off-ledger DID must sign the txn by the newly created DID (that is identifier=dest and signature is created against verkey). Otherwise signature verification will not pass.
 * GET_AUTH_RULE reply contains the new `off_ledger_signature` field regardless of whether it has been ever set for a rule.

 ;;;","30/Jul/19 5:01 AM;VladimirWork;Build Info:
1.9.1~dev1043
plugins 1.0.1~dev69

Build Info:
1. Set AuthConstraint for nym transaction like: role=*, off_ledger_signature=True, sig_count=1. Ensure that nym that has not been written on ledger, can send nym txn of himself, with his signature included (that is it needs to be signed against the newly created key).
2. Set AuthConstraint for nym transaction like: role=*, off_ledger_signature=True, sig_count=2. Ensure that nym that has not been written on ledger, can send nym txn of himself, with his signature and someone's on-ledger did signature included.
3. Set AuthConstraint for nym transaction like: role=*, off_ledger_signature=True, sig_count=0. Ensure that nym that has not been written on ledger, can send nym txn of himself, with his signature included.
4. Set AuthConstraint for nym transaction like: role=*, off_ledger_signature=False, sig_count=1. Ensure that this nym cannot send txn.
5. Set AuthConstraint for nym transaction like: role=trustee OR role = '*' off_ledger_signature=True metadata={plguin_requirements: amount}. Ensure that trustee can write nym without plugin requirements and that off-ledger nym can write only with satisfying plugin requirements.

Actual Results:
Feature works as expected.;;;",,,,,,,,,,,,,,,,,,
batched.py is repeatedly logging RID removal warnings to a nodes log file,INDY-2172,41321,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,donqui,lbendixsen,lbendixsen,19/Jul/19 10:08 AM,09/Oct/19 7:12 PM,28/Oct/23 2:47 AM,09/Oct/19 7:12 PM,1.9.0,,1.11.0,,,,,0,EV-CS,TShirt_M,,,"_Environment_: BuilderNet FoundationBuilder Node is running Sovrin version 1.1.50 (indy-node v1.9.0) . This issue is occurring on several nodes, and on different Networks also. I noticed a similar issue in the SovrinNode logs on the StagingNet (logs can be included for that Node also, if needed)

 

_Steps to Reproduce_: 

The use case where this is occurring is on the Sovrin Networks.  These networks have nodes that are added to and removed from them pretty regularly. The Buildernet, for example, has been fairly consistent where nodes are usually added to it and not removed, but there have been 2 cases on the BuilderNet where a node was moved to the MainNet and removed from the BuilderNet.  It is also quite normal forthere to be errors while getting a Node added to the BuilderNet where we will need to change the Nodes Ports/Keys/IP addresses and other things while trying to get them up and running in consensus.  There have also been cases where the Steward Keys have to be replaced so the node will need brought in under the new Steward NYM.

 

_Expected Behavior_: Issue seems like it should be fixed rather than continuing to report that it is still a problem in the log file.

 

_Observed Behavior_: Issue is reported at least 4 times per minute in the log file.  There is probably a root cause of an underlying issue needing resolved.


_Notes_:",,,,,,,,,,,,,,,,,,,,,,,,INDY-2243,,,,,,,"21/Sep/19 1:03 AM;lbendixsen;FB-infolog.tar.bz2;https://jira.hyperledger.org/secure/attachment/17842/FB-infolog.tar.bz2","20/Jul/19 2:03 AM;lbendixsen;FoundationBuilder.log.xz;https://jira.hyperledger.org/secure/attachment/17614/FoundationBuilder.log.xz","19/Sep/19 3:12 AM;mgbailey;australia.log.tar.bz2;https://jira.hyperledger.org/secure/attachment/17829/australia.log.tar.bz2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w2r",,,,Unset,Unset,Ev-Node 19.17,Ev-Node 19.18,Ev-Node 19.19,Ev-Node 19.20,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),donqui,esplinr,lbendixsen,,,,,,,,,"19/Jul/19 11:55 PM;esplinr;[~lbendixsen] Can you please include the log file?;;;","10/Sep/19 6:38 PM;donqui;[~lbendixsen] Could you please provide me with a scenario or a series of steps that led to this behavior I am having trouble reproducing it?

 ;;;","12/Sep/19 2:43 AM;lbendixsen;I am sorry, but I do not know what causes the issue to occur besides my guesses in the original description (1. removing nodes from the pool, perhaps nodes that are in the genesis file, 2. Changing the ips/ports of nodes, 3. Changing the Steward nym after addition of a node and then re-adding the node with a new Steward DID).  I might be able to provide more hints/guesses if I knew what these specific rid's were referencing.  How do I figure that out?

It is happening on at least the BuilderNet and the StagingNet at this point.  Here is an excerpt that I got just now from tailing the log file on a StagingNet server (Sovrin)

2019-09-11 17:26:15,991|WARNING|batched.py|CONNECTION: SovrinNode has removed rid BHXePWAv2F3fhXtwgjmkzAJtCM1XVB7ZDfypEkpN7uNd

2019-09-11 17:26:25,055|WARNING|batched.py|CONNECTION: SovrinNode has removed rid 6Rw6fxUtoVdCgVs4VSawFYF4Kix8nCHFXWk8EALTDkc5

2019-09-11 17:26:31,201|WARNING|batched.py|CONNECTION: SovrinNode has removed rid BHXePWAv2F3fhXtwgjmkzAJtCM1XVB7ZDfypEkpN7uNd

2019-09-11 17:26:40,286|WARNING|batched.py|CONNECTION: SovrinNode has removed rid 6Rw6fxUtoVdCgVs4VSawFYF4Kix8nCHFXWk8EALTDkc5

2019-09-11 17:26:46,012|WARNING|batched.py|CONNECTION: SovrinNode has removed rid BHXePWAv2F3fhXtwgjmkzAJtCM1XVB7ZDfypEkpN7uNd

2019-09-11 17:26:46,643|INFO|seeder_service.py|SovrinNode received ledger status: LEDGER_STATUS\{'protocolVersion': 2, 'merkleRoot': '7z1u3Rh6hydSbzi9P2C1e4LJ4bCermGr1HQ1b5s81xW8', 'ledgerId': 0, 'viewNo': None, 'ppSeqNo': None, 'txnSeqNo': 127} from b'iBcy&1eE@/biSqz1j54m>{4t?p**aR@zhp{)7vTK'

2019-09-11 17:26:46,644|INFO|seeder_service.py|SovrinNode sending consistency proof: CONSISTENCY_PROOF\{'seqNoStart': 127, 'hashes': ['7z3ZWXDTYXJd7hkHfMxw9MHjFBfMkWTWrNCphv4n1k3X', 'tQJyoXbJnjg4FyKqZXdCqdUUhqGFK7jwSimQ426FXLy', 'EBFt7oCpPzmGgbqathUZJRfYC7zn8ZDNXSSFpcCnTdfS', '6idJSG7xjjFqTeFCn6yaRhq4gSuvrV1ZZaNYY7F7H8J', '7fr4JQXUCVTtcrD2jSDXKjvyMYTzaiw52xTH4Cgdpc95', '2oycntybkivHAVXDMZPNazgsJnhaGMU2hvBbbMCEf8tz', 'Fbq95zj8GWZMqdTWab7BQd9qpkKkFaCzdv1kdzHuDJrt', '6v5B5z6RTtvJrMh1J5q6UxrLfQYd6XuDFZBQExXwPVsu', 'BkxxHCQXgw8F5XkXvhjkwVDZ9FguzMwSt57vRmjrTB7L'], 'ledgerId': 0, 'viewNo': 0, 'ppSeqNo': 0, 'newMerkleRoot': 'EVf6FCk1W6gNNqW9jtg8X37LnGxuUWoDgsyrzt3Luf8C', 'oldMerkleRoot': '7z1u3Rh6hydSbzi9P2C1e4LJ4bCermGr1HQ1b5s81xW8', 'seqNoEnd': 138} to b'iBcy&1eE@/biSqz1j54m>{4t?p**aR@zhp{)7vTK'

2019-09-11 17:26:55,078|WARNING|batched.py|CONNECTION: SovrinNode has removed rid 6Rw6fxUtoVdCgVs4VSawFYF4Kix8nCHFXWk8EALTDkc5

2019-09-11 17:27:01,217|WARNING|batched.py|CONNECTION: SovrinNode has removed rid BHXePWAv2F3fhXtwgjmkzAJtCM1XVB7ZDfypEkpN7uNd;;;","20/Sep/19 8:55 PM;donqui;Problem reason/description:
 - too many logs about removed RIDs that do not get cleared once the cause is resolved

Changes:
 - log level changed from WARNING to INFO as the logs are useful for debugging and the issues described in the ticket could not be reproduced

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1340]

Version:
 - plenum: 1.10.0.dev902
 - node: 1.10.0.dev1087
 - sov: sovtoken_1.0.3~dev97 sovtokenfees_1.0.3~dev97

Risk:
 - Low

Recommendations for QA
 - Start a pool with 4 nodes
 - demote node4
 - restart indy-node service on Node4
 - logs should start appearing with INFO level
 - promote Node4
 - logs should stop;;;","21/Sep/19 12:58 AM;lbendixsen;Changing the log level is an unacceptable solution to this issue.  I did not enter the ticket because I wanted fewer lines in my log file.  I entered the ticket because I thought that there is an underlying issue where RID are repeated being removed (or at least attempted according to the message) and that seems like at least a waste of cycles and at most an indication that something isn't working as expected.

I am not sure why this is difficult to reproduce, so please help me understand what else you need.  I did a reset of the buildernet over the last few weeks (meaning we completely removed the data directory with all of the ledgers and started over with the genesis files) and the issue started up within a few seconds of me adding my node in to the pool.  danube was the first node connected to the pool, and mine (FoundationBuilder) was the second, and we had to wait a few days for the other 2 genesis nodes(vnode1 and xsvalidatorec2irl) to add themselves in to the initial pool.  Here is an excerpt showing the few seconds difference between starting up and seeing the rid message and I will also attach the whole log.  

{{2019-09-03 17:12:58,224|NOTIFICATION|looper.py|Looper shut down in 0.010 seconds.}}{{2019-09-11 22:54:45,499|INFO|looper.py|Starting up indy-node}}{{2019-09-11 22:54:45,603|INFO|ledger.py|Starting ledger...}}{{2019-09-11 22:54:45,616|INFO|ledger.py|Recovering tree from transaction log}}{{2019-09-11 22:54:45,644|INFO|ledger.py|Recovered tree in 0.027606574818491936 seconds}}{{2019-09-11 22:54:45,684|INFO|ledger.py|Starting ledger...}}{{2019-09-11 22:54:45,697|INFO|ledger.py|Recovering tree from transaction log}}{{2019-09-11 22:54:45,725|INFO|ledger.py|Recovered tree in 0.02819257229566574 seconds}}{{2019-09-11 22:54:45,770|INFO|ledger.py|Starting ledger...}}{{2019-09-11 22:54:45,783|INFO|ledger.py|Recovering tree from transaction log}}{{2019-09-11 22:54:45,810|INFO|ledger.py|Recovered tree in 0.027425142005085945 seconds}}{{2019-09-11 22:54:45,856|INFO|ledger.py|Starting ledger...}}{{2019-09-11 22:54:45,869|INFO|ledger.py|Recovering tree from transaction log}}{{2019-09-11 22:54:45,897|INFO|ledger.py|Recovered tree in 0.028343386948108673 seconds}}{{2019-09-11 22:54:45,995|NOTIFICATION|node_bootstrap.py|BLS: BLS Signatures will be used for Node FoundationBuilder}}{{2019-09-11 22:54:45,995|INFO|pool_manager.py|FoundationBuilder sets node FoundationBuilder (GVvdyd7Y6hsBEy5yDDHjqkXgH8zW34K74RsxUiUCZDCE) order to 17}}{{2019-09-11 22:54:45,995|INFO|pool_manager.py|FoundationBuilder sets node vnode1 (9Aj2LjQ2fwszJRSdZqg53q5e6ayScmtpeZyPGgKDswT8) order to 6}}{{2019-09-11 22:54:45,995|INFO|pool_manager.py|FoundationBuilder sets node xsvalidatorec2irl (DXn8PUYKZZkq8gC7CZ2PqwECzUs2bpxYiA5TWgoYARa7) order to 17}}{{2019-09-11 22:54:45,995|INFO|pool_manager.py|FoundationBuilder sets node danube (52muwfE7EjTGDKxiQCYWr58D8BcrgyKVjhHgRQdaLiMw) order to 6}}{{2019-09-11 22:54:46,024|INFO|notifier_plugin_manager.py|Found notifier plugins: []}}{{2019-09-11 22:54:46,049|INFO|notifier_plugin_manager.py|Found notifier plugins: []}}{{2019-09-11 22:54:46,049|INFO|node_bootstrap.py|<indy_node.server.node_bootstrap.NodeBootstrap object at 0x7fa572b5e710> found state to be empty, recreating from ledger}}{{2019-09-11 22:54:46,052|INFO|node_bootstrap.py|<indy_node.server.node_bootstrap.NodeBootstrap object at 0x7fa572b5e710> initialized pool state: state root 2VNAcq8b6Bg7ePF4FBRUe6duUBytP35eYduGzSyMwFuz}}{{2019-09-11 22:54:46,052|INFO|node_bootstrap.py|<indy_node.server.node_bootstrap.NodeBootstrap object at 0x7fa572b5e710> found state to be empty, recreating from ledger}}{{2019-09-11 22:54:46,052|INFO|node_bootstrap.py|<indy_node.server.node_bootstrap.NodeBootstrap object at 0x7fa572b5e710> initialized config state: state root DfNLmH4DAHTKv63YPFJzuRdeEtVwF5RtVnvKYHd8iLEA}}{{2019-09-11 22:54:46,052|INFO|node_bootstrap.py|<indy_node.server.node_bootstrap.NodeBootstrap object at 0x7fa572b5e710> found state to be empty, recreating from ledger}}{{2019-09-11 22:54:46,059|INFO|node_bootstrap.py|<indy_node.server.node_bootstrap.NodeBootstrap object at 0x7fa572b5e710> initialized domain state: state root 4RbC2qVepj1kn2jdmvemxtwpYKfrsA9XowGGAUs7m79Y}}{{2019-09-11 22:54:46,061|INFO|stacks.py|FoundationBuilderC: clients connections tracking is enabled.}}{{2019-09-11 22:54:46,061|INFO|stacks.py|FoundationBuilderC: client stack restart is enabled.}}{{2019-09-11 22:54:46,062|INFO|node.py|FoundationBuilder updated its pool parameters: f 1, totalNodes 4, allNodeNames \{'FoundationBuilder', 'danube', 'xsvalidatorec2irl', 'vnode1'}, requiredNumberOfInstances 2, minimumNodes 3, quorums \{'view_change_ack': Quorum(2), 'prepare': Quorum(2), 'checkpoint': Quorum(2), 'commit': Quorum(3), 'backup_instance_faulty': Quorum(2), 'n': 4, 'strong': Quorum(3), 'view_change': Quorum(3), 'reply': Quorum(2), 'weak': Quorum(2), 'ledger_status': Quorum(2), 'f': 1, 'election': Quorum(3), 'ledger_status_last_3PC': Quorum(2), 'observer_data': Quorum(2), 'timestamp': Quorum(2), 'same_consistency_proof': Quorum(2), 'consistency_proof': Quorum(2), 'view_change_done': Quorum(3), 'bls_signatures': Quorum(3), 'propagate': Quorum(2)}}}{{2019-09-11 22:54:46,098|NOTIFICATION|plugin_loader.py|skipping plugin plugin_firebase_stats_consumer[class: typing.Dict<~KT, ~VT>] because it does not have a 'pluginType' attribute}}{{2019-09-11 22:54:46,098|NOTIFICATION|plugin_loader.py|skipping plugin plugin_firebase_stats_consumer[class: <class 'plenum.server.stats_consumer.StatsConsumer'>] because it does not have a 'pluginType' attribute}}{{2019-09-11 22:54:46,098|NOTIFICATION|plugin_loader.py|skipping plugin plugin_firebase_stats_consumer[class: <enum 'Topic'>] because it does not have a 'pluginType' attribute}}{{2019-09-11 22:54:46,098|NOTIFICATION|plugin_loader.py|skipping plugin plugin_firebase_stats_consumer[class: <class 'plenum.server.plugin.stats_consumer.stats_publisher.StatsPublisher'>] because it does not have a 'pluginType' attribute}}{{2019-09-11 22:54:46,098|INFO|plugin_loader.py|plugin FirebaseStatsConsumer successfully loaded from module plugin_firebase_stats_consumer}}{{2019-09-11 22:54:46,098|NOTIFICATION|plugin_loader.py|skipping plugin plugin_firebase_stats_consumer[class: <class 'plenum.server.plugin_loader.HasDynamicallyImportedModules'>] because it does not have a 'pluginType' attribute}}{{2019-09-11 22:54:46,099|INFO|replica.py|FoundationBuilder:0 set watermarks as 0 300}}{{2019-09-11 22:54:46,099|INFO|replica_stasher.py|FoundationBuilder:0 unstash 0 out of watermarks messages}}{{2019-09-11 22:54:46,100|NOTIFICATION|replicas.py|FoundationBuilder added replica FoundationBuilder:0 to instance 0 (master)}}{{2019-09-11 22:54:46,100|INFO|replicas.py|reset monitor due to replica addition}}{{2019-09-11 22:54:46,100|INFO|replica.py|FoundationBuilder:1 set watermarks as 0 300}}{{2019-09-11 22:54:46,100|INFO|replica_stasher.py|FoundationBuilder:1 unstash 0 out of watermarks messages}}{{2019-09-11 22:54:46,101|NOTIFICATION|replicas.py|FoundationBuilder added replica FoundationBuilder:1 to instance 1 (backup)}}{{2019-09-11 22:54:46,101|INFO|replicas.py|reset monitor due to replica addition}}{{2019-09-11 22:54:46,102|INFO|node.py|total plugins loaded in node: 0}}{{2019-09-11 22:54:46,146|INFO|node_runner.py|Going to integrate plugin: sovtoken}}{{2019-09-11 22:54:46,191|INFO|ledger.py|Starting ledger...}}{{2019-09-11 22:54:46,205|INFO|ledger.py|Recovering tree from transaction log}}{{2019-09-11 22:54:46,232|INFO|ledger.py|Recovered tree in 0.027781154960393906 seconds}}{{2019-09-11 22:54:46,246|INFO|node_runner.py|Integrated plugin: sovtoken}}{{2019-09-11 22:54:46,303|INFO|node_runner.py|Going to integrate plugin: sovtokenfees}}{{2019-09-11 22:54:46,303|INFO|node_runner.py|Integrated plugin: sovtokenfees}}{{2019-09-11 22:54:46,303|INFO|motor.py|FoundationBuilder changing status from stopped to starting}}{{2019-09-11 22:54:46,305|INFO|stacks.py|CONNECTION: FoundationBuilder listening for other nodes at 172.31.35.134:9701}}{{2019-09-11 22:54:46,340|INFO|node.py|FoundationBuilder first time running...}}{{2019-09-11 22:54:46,340|INFO|node.py|FoundationBuilder processed 0 Ordered batches for instance 0 before starting catch up}}{{2019-09-11 22:54:46,340|INFO|node.py|FoundationBuilder processed 0 Ordered batches for instance 1 before starting catch up}}{{2019-09-11 22:54:46,340|INFO|node.py|FoundationBuilder reverted 0 batches before starting catch up}}{{2019-09-11 22:54:46,340|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService starting catchup (is_initial=True)}}{{2019-09-11 22:54:46,340|INFO|node_leecher_service.py|FoundationBuilder:NodeLeecherService transitioning from Idle to PreSyncingPool}}{{2019-09-11 22:54:46,340|INFO|cons_proof_service.py|FoundationBuilder:ConsProofService:0 starts}}{{2019-09-11 22:54:46,342|INFO|kit_zstack.py|CONNECTION: FoundationBuilder found the following missing connections: danube, xsvalidatorec2irl, vnode1}}{{2019-09-11 22:54:46,342|INFO|zstack.py|CONNECTION: FoundationBuilder looking for danube at 173.249.14.196:9701}}{{2019-09-11 22:54:46,344|INFO|zstack.py|CONNECTION: FoundationBuilder looking for xsvalidatorec2irl at 52.209.6.196:9701}}{{2019-09-11 22:54:46,345|INFO|zstack.py|CONNECTION: FoundationBuilder looking for vnode1 at 206.189.143.34:9797}}{{2019-09-11 22:54:46,729|WARNING|batched.py|CONNECTION: FoundationBuilder has removed rid HsD7niKXgMFxckXNeSE1Np8cfgMDs8aHpYxSWWBA4BDp}}{{2019-09-11 22:54:46,762|WARNING|batched.py|CONNECTION: FoundationBuilder has removed rid 4gauyZ56Y4WsbFEtzDEU2uAwrBRs95iKrMLoQjFNkst1}}{{2019-09-11 22:54:47,401|WARNING|batched.py|CONNECTION: FoundationBuilder has removed rid DacmhXMW9xYRCy6R3fAj54iCKCD8BdJnwyMGgt329cdy}};;;","21/Sep/19 7:32 AM;esplinr;It's useful to know that you are seeing this on the reset Builder Net. We'll discuss it again as a team.;;;","08/Oct/19 10:27 PM;donqui; 

*Findings:*

The error in the logs is a sign that we are receiving messages from a node that was once a part of the network but for some reason it was removed from it. As it was removed we do not communicate to it, don't send announcements or other transactions, but we still allow for it to connect to get the ledger state (catch-up) and check if it has been promoted back.

When a node is demoted it still has a established zmq connection to all other nodes. If a connection to one or more nodes is lost demoted node tries to reconnect but never manages to confirm that the connection is really established. This is because it does not receive pongs for his pings as other nodes do not respond to messages from the demoted nodes. As the demoted node does not receive the pong for it's ping it assumes that it was a network issue and tries to reconnect indefinitely.

Snippets of logs showing this behavior are bellow:

 

*Node3 (demoted node):*
{code:java}
$ tail -f /var/log/indy/sandbox/Node3.log
2019-10-08 13:12:44,048|DEBUG|node.py|Node3 sending message MESSAGE_REQUEST{'params': {'ledgerId': 0}, 'msg_type': 'LEDGER_STATUS'} to 2 recipients: ['Node2', 'Node1']
2019-10-08 13:12:44,049|TRACE|batched.py|Node3 sending msg b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node2
2019-10-08 13:12:44,049|TRACE|zstack.py|Node3 transmitting message b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node2 by socket 178 46152880
2019-10-08 13:12:44,050|WARNING|zstack.py|Remote Node2 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings
2019-10-08 13:12:44,050|TRACE|batched.py|Node3 sending msg b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node1
2019-10-08 13:12:44,050|TRACE|zstack.py|Node3 transmitting message b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node1 by socket 184 46220592
2019-10-08 13:12:44,050|WARNING|zstack.py|Remote Node1 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings
2019-10-08 13:12:45,772|DEBUG|kit_zstack.py|Node3 matched remote Node4 HA(host='10.0.0.5', port=9707)
2019-10-08 13:12:45,772|DEBUG|kit_zstack.py|Node3 matched remote Node2 HA(host='10.0.0.3', port=9703)
2019-10-08 13:12:45,772|DEBUG|kit_zstack.py|Node3 matched remote Node1 HA(host='10.0.0.2', port=9701)
2019-10-08 13:12:45,773|DEBUG|zstack.py|Node3 pinged Node2
2019-10-08 13:12:45,773|DEBUG|zstack.py|Node3 pinged Node1
2019-10-08 13:12:45,774|TRACE|kit_zstack.py|Node3 next check for retries in 2.00 seconds
2019-10-08 13:12:45,789|TRACE|batched.py|Node3 sending msg b'pi' to Node2
2019-10-08 13:12:45,789|TRACE|zstack.py|Node3 transmitting message b'pi' to Node2 by socket 178 46152880
2019-10-08 13:12:45,791|TRACE|batched.py|Node3 sending msg b'pi' to Node1
2019-10-08 13:12:45,792|TRACE|zstack.py|Node3 transmitting message b'pi' to Node1 by socket 184 46220592

{code}
 

*Node1 (participating node):*
{code:java}
$ tail -f /var/log/indy/sandbox/Node1.log | grep 'removed rid'
2019-10-08 13:10:55,422|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:10:57,424|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:10:59,483|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:11:01,455|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:11:03,457|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
{code}
 

*Node2 (participating node):*
{code:java}
$ tail -f /var/log/indy/sandbox/Node2.log
2019-10-08 13:13:29,949|TRACE|zstack.py|Node2 got ping from 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:29,949|DEBUG|zstack.py|Node2 ponged 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:29,950|WARNING|batched.py|CONNECTION: Node2 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:29,951|DEBUG|message_processor.py|Node2 discarding message deque([b'po']) because CONNECTION: rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN no longer available
2019-10-08 13:13:30,842|TRACE|has_action_queue.py|Node2 running action flush_metrics with id 879
2019-10-08 13:13:30,843|TRACE|has_action_queue.py|Node2 scheduling action flush_metrics with id 882 to run in 10.0 seconds
2019-10-08 13:13:30,843|TRACE|has_action_queue.py|Node2 running action checkPerformance with id 878
2019-10-08 13:13:30,843|TRACE|node.py|Node2 checking its performance
2019-10-08 13:13:30,843|TRACE|node.py|Node2 ordered no new requests
2019-10-08 13:13:30,844|TRACE|has_action_queue.py|Node2 scheduling action checkPerformance with id 883 to run in 10 seconds
2019-10-08 13:13:31,242|DEBUG|kit_zstack.py|Node2 matched remote Node1 HA(host='10.0.0.2', port=9701)
2019-10-08 13:13:31,243|DEBUG|kit_zstack.py|Node2 matched remote Node4 HA(host='10.0.0.5', port=9707)
2019-10-08 13:13:31,243|TRACE|kit_zstack.py|Node2 next check for retries in 2.00 seconds
2019-10-08 13:13:32,007|TRACE|zstack.py|Node2 got 1 messages through listener
2019-10-08 13:13:32,008|TRACE|zstack.py|Node2 got ping from 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:32,008|DEBUG|zstack.py|Node2 ponged 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:32,009|WARNING|batched.py|CONNECTION: Node2 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:32,009|DEBUG|message_processor.py|Node2 discarding message deque([b'po']) because CONNECTION: rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN no longer available{code}
 

*Implications:*
 # Unnecessary traffic because of constant pings sent by the demoted node in a desperate attempt to reconnect to the lost remote
 #  CPU cycles spent on handling these messages by the participating nodes
 # Logs constantly getting filled by lines saying that we got a message from a node that for some reason is no longer participating in the pool

 

*Potential Solutions:*
 # Add a config option telling the node that it should not try to connect as it is demoted – would require a manual action on the side of the demoted node.
 # Allow catch-up but close the connections and stop the communication after that if the status of the node hasn't changed.
 # Decrease the frequency at which we try to talk to the pool if we are demoted – increase the timeout between requests and shutdown if you don't get a pong.
 # Change the logic so that the participating nodes connect to the demoted node - demoted node would only listen to incoming messages (push vs pull or a combo of both).
 # Add a poison pill message that would tell the node to shut down as it is still demoted limiting the number of attempts to 1. Would require moving ping/pong logic up so that we can see if a node is demoted or not, log a meaningful message, and send an appropriate response.;;;","09/Oct/19 5:21 PM;donqui;Closing this ticket and creating a new one for handling the identified issues.

We changed the log level for the error messages so that whoever is running a node can change it and not see the errors in the logs, and another way of solving this would be an update of the firewall rules that would drop any traffic coming from nodes that should not be participating in the network any-more.;;;",,,,,,,,,,,,,,
Transaction authors don't need to be endorsers,INDY-2173,41323,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,19/Jul/19 3:53 PM,28/Aug/19 6:15 PM,28/Oct/23 2:47 AM,31/Jul/19 1:22 AM,,,1.9.1,,,,,0,,,,,"*Story*
As a transaction author, I need my transactions to be written to the ledger preserving me as the author without my needing to accept the responsibilities of an endorser so that I can focus on my business. Instead, I will have a business relationship with an endorser who will endorse my transactions.

*Goals*
* It is easy to tell from the ledger who is the endorser and who is the transaction author for each transaction.
* A transaction author can use a different transaction endorser for future transactions, including updates to attribs and key rotations.
* The transaction must use the author key to sign the transaction author agreement regardless of whether an endorser signature is also needed.

*Acceptance Criteria*
* Analyze the work required.
* Raise issues for completing the work.

*Notes*
* The transaction author field is intended to be used for all future permissions.
* The endorser field only needs to be recorded in order to:
** Allow auditability, that the transaction was written with correct permissions
** Allow the network administrator to bill the endorser for the write
* Sub-endorsers or chains of endorsers would be desirable for a global network, but are not required at this time. We recognize that in the future adding multiple levels of endorsers or delegated endorsers could be a breaking change.
** Endorsers could be a list, but it would still require the transaction author to know all endorsers at the time the transaction is created. That list could be built off-ledger by sharing a draft transaction.
* Suggested user flow:
** Transaction author identifies the need for creating a transaction
** Transaction author identifies which endorser they want to use
** Transaction author includes the DID of the endorser in the transaction, and signs the transaction.
** Transaction author passes the signed transaction to the endorser
** Endorser adds their signature
** Endorser submits the multi-signed transaction to the ledger
** If the endorser will not accept the transaction, and the transaction author wants to try a different endorser, the transaction author must recreate the transaction in order to include the new endorser DID.
* Token holders can also choose to ""endorse"" third party transactions so that they get submitted to the ledger. See [ST-508|https://sovrin.atlassian.net/browse/ST-608]",,,,,,,,,,,,,,,,,,,,,IS-1325,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7f:u",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"26/Jul/19 9:50 PM;ashcherbakov;*Changes:*
 * Made changes as described in [https://github.com/hyperledger/indy-node/blob/master/design/transaction_endorder.md|https://github.com/hyperledger/indy-node/blob/master/design/transaction_endorser.md]
 ** Added optional `endorser` field into Request (top-level along with `identifier`)
 ** Included `endorser` field into transaction (in txn metadata along with `from`)
 ** Extended client request valdiation to require both Author's and Endorser's signatures if endorser is specified.
 ** Added unit and integration tests

*PRs*
 * [https://github.com/hyperledger/indy-plenum/pull/1281]
 * [https://github.com/hyperledger/indy-plenum/pull/1282]
 * [https://github.com/hyperledger/indy-node/pull/1397]

*Risk:*
 * Low

*Build version*
 * indy-node 1.9.1.dev1043

*Recommendation for QA:*
 * Wait until `indy_append_request_endorser` is implemented in SDK (IS-1325)
 * Implement the following tests (see [https://github.com/hyperledger/indy-node/pull/1397/files#diff-50239dfe5bc00fee55f3d7dac8bd2a52):]
 ** Positive test case as described in [https://github.com/hyperledger/indy-node/blob/master/design/transaction_endorder.md#proposed-workflow]
 ** Author who is not endorser can not send a request requiring endorser permissions (such as SCHEMA, CLAIM_DEF)
 ** Author's signature is required if endorser is appended
 ** Endorser's signature is required if endorser is appended;;;","31/Jul/19 1:22 AM;VladimirWork;Build Info:
indy-node 1.9.1~dev1043
plugins 1.0.1~dev69
libindy 1.10.1~1218

Actual Results:
Feature works as expected, system test has been implemented https://github.com/VladimirWork/indy-test-automation/blob/be88d93f38e4c2310a950e5e05acac261e2d8f8a/system/draft/test_misc.py#L1202;;;",,,,,,,,,,,,,,,,,,,,
Investigate request queue growing and OOM failures during load,INDY-2174,41325,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,19/Jul/19 7:09 PM,19/Jul/19 7:09 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Case 1:
Logs and metrics for indy-node 1.9.0~dev1014 + plugins 0.9.13~dev58. OOM has appeared after ~13 hours of production load with payments and fees.
ev@evernymr33:logs/ST-600-logs.tar.gz
ev@evernymr33:logs/ST-600-metrics.tar.gz

Case 2:
Logs and metrics for indy-node 1.9.0~dev1014. There was no OOM after ~14 hours of production load without payments and fees.
ev@evernymr33:logs/ST-600-without-plugins-logs.tar.gz
ev@evernymr33:logs/ST-600-without-plugins-metrics.tar.gz

Case 3:
Logs and metrics for indy-node 1.9.0~dev1014 + plugins 0.9.13~dev58 with enabled GC stats (there were OOM failures as well).
ev@evernymr33:logs/ST-600-with-gc-logs.tar.gz
ev@evernymr33:logs/ST-600-with-gc-metrics.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ul8:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Responses to client validator info requests very slow on a StagingNet node,INDY-2175,41338,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,mgbailey,mgbailey,20/Jul/19 7:31 AM,17/Aug/19 1:10 AM,28/Oct/23 2:47 AM,15/Aug/19 8:10 AM,,,,,,,,0,EV-CS,,,,"A node on the Sovrin StagingNet, sovrin.sicpa.com, responds to validator-info commands from clients (indy-cli or other script) in about 34 seconds – much slower than all other nodes. Running from the local command line, the steward reports that it returns results in less than 2 seconds, which is normal. The steward has provided some other diagnostics as well, and there are no obvious problems:
{code:java}
skrieg@snextsovrin01v:~$ top -n 1
top - 10:30:45 up 16 days,  4:27,  1 user,  load average: 0.19, 0.27, 0.31
Tasks: 198 total,   2 running, 196 sleeping,   0 stopped,   0 zombie
%Cpu(s): 16.1 us,  0.7 sy,  0.0 ni, 83.1 id,  0.1 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 24690072 total, 22262812 free,   756816 used,  1670444 buff/cache
KiB Swap:  1999868 total,  1999868 free,        0 used. 23515916 avail Mem
 
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
16267 indy      20   0 1869952 606508  17904 R  25.0  2.5 345:19.09 python3
    1 root      20   0  185396   6132   4188 S   0.0  0.0   0:25.88 systemd
   2 root      20   0       0      0      0 S   0.0  0.0   0:00.13 kthreadd
    3 root      20   0       0      0      0 S   0.0  0.0   0:02.52 ksoftirqd/0
    5 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/0:0H
    7 root      20   0       0      0      0 S   0.0  0.0   1:07.01 rcu_sched
    8 root      20   0       0      0      0 S   0.0  0.0   0:00.00 rcu_bh
    9 root      rt   0       0      0      0 S   0.0  0.0   0:00.37 migration/0
   10 root      rt   0       0      0      0 S   0.0  0.0   0:02.62 watchdog/0
   11 root      rt   0       0      0      0 S   0.0  0.0   0:02.79 watchdog/1
   12 root      rt   0       0      0      0 S   0.0  0.0   0:00.64 migration/1
   13 root      20   0       0      0      0 S   0.0  0.0   0:10.89 ksoftirqd/1
   15 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kworker/1:0H
   16 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kdevtmpfs
   17 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 netns
   18 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 perf
   19 root      20   0       0      0      0 S   0.0  0.0   0:00.44 khungtaskd
   20 root       0 -20       0      0      0 S   0.0  0.0   0:00.75 writeback
   21 root      25   5       0      0      0 S   0.0  0.0   0:00.00 ksmd
   22 root      39  19       0      0      0 S   0.0  0.0   0:00.00 khugepaged
   23 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 crypto
   24 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kintegrityd
   25 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 bioset
   26 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kblockd
   27 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 ata_sff
   28 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 md
   29 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 devfreq_wq
   34 root      20   0       0      0      0 S   0.0  0.0   0:00.00 kswapd0
   35 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 vmstat
   36 root      20   0       0      0      0 S   0.0  0.0   0:00.00 fsnotify_mark
   37 root      20   0       0      0      0 S   0.0  0.0   0:00.00 ecryptfs-kthrea
   53 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 kthrotld
   54 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 acpi_thermal_pm
   55 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 bioset
   56 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 bioset
   57 root       0 -20       0      0      0 S   0.0  0.0   0:00.00 bioset
skrieg@snextsovrin01v:~$ free -m
              total        used        free      shared  buff/cache   available
Mem:          24111         738       21741          56        1631       22965
Swap:          1952           0        1952
skrieg@snextsovrin01v:~$ sudo lsof -p 16267 -n | wc -l
438
{code}
Please examine the logs and see if you have suggestions to explain this behavior. The biggest concern is whether this slow response behavior extends to other client requests. If it is only slow for validator info requests, we can live with that. But if this node also takes 34 seconds to respond to other client queries, that is a problem.",,,,,,,,,,,,,,,,,,,,,,,,INDY-2182,,,,,,,"20/Jul/19 7:31 AM;mgbailey;sovrin.sicpa.com.log.xz;https://jira.hyperledger.org/secure/attachment/17617/sovrin.sicpa.com.log.xz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98614s",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,mgbailey,,,,,,,,,,"06/Aug/19 10:54 PM;esplinr;We  believe that this was caused by the memory profiler that was addressed in INDY-2182.

After Staging Net is updated in August, we expect to see the problem improve.;;;","14/Aug/19 11:54 PM;mgbailey;Following the upgrade, this problem is resolved. Thanks!;;;","15/Aug/19 8:10 AM;esplinr;Thank you for confirming.;;;",,,,,,,,,,,,,,,,,,,
indy-node broken by indy-plenum and python-dateutil ,INDY-2176,41343,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ashcherbakov,NeolithEra,NeolithEra,21/Jul/19 9:41 PM,29/Aug/19 5:07 PM,28/Oct/23 2:47 AM,12/Aug/19 10:28 PM,,,1.9.2,,,,,0,build-system,EV-Triaged,help-wanted,,"Hi, users are unable to run _*indy-node*_ due to dependency conflict with *_python-dateutil_* package.

As shown in the following full dependency graph of indy-node, indy-node requires *python-dateutil* (the latest version), while _*indy-plenum==1.9.0.dev847*_ requires _*python-dateutil==2.6.1*_.

According to pip's ""first found wins"" installation strategy, *python-dateutil==2.8.0* is the actually installed version. However,  *python-dateutil==2.8.0* does not satisfy *==2.6.1* .

 

*Dependency tree-------*
{code:java}
// code placeholder
indy-node<version range:>
| +-distro<version range:==1.3.0>
| +-indy-plenum<version range:==1.8.1>
| | +-base58<version range:==1.0.0>
| | +-indy-crypto<version range:==0.4.5>
| | | +-pytest<version range:>
| | +-intervaltree<version range:==2.1.0>
| | | +-sortedcontainers<version range:>
| | +-ioflo<version range:==1.5.4>
| | +-jsonpickle<version range:==0.9.6>
| | +-leveldb<version range:>
| | +-libnacl<version range:==1.6.1>
| | +-msgpack-python<version range:==0.4.6>
| | +-orderedset<version range:==2.0>
| | +-packaging<version range:==19.0>
| | +-pip<version range:<10.0.0>
| | +-portalocker<version range:==0.5.7>
| | +-prompt-toolkit<version range:==0.57>
| | +-psutil<version range:==5.4.3>
| | +-pygments<version range:==2.2.0>
| | +-pympler<version range:==0.5>
| | +-python-dateutil<version range:==2.6.1>
| | +-python-rocksdb<version range:==0.6.9>
| | | +-setuptools<version range:>=25>
| | +-pyzmq<version range:==17.0.0>
| | +-rlp<version range:==0.5.1>
| | +-semver<version range:==2.7.9>
| | +-sha3<version range:==0.2.1>
| | +-six<version range:==1.11.0>
| | +-sortedcontainers<version range:==1.5.7>
| | +-ujson<version range:==1.33>
| +-python-dateutil<version range:>
| +-timeout-decorator<version range:==0.4.0> 
{code}
Thanks for your help.
 Best,
 Neolith",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9861403",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,NeolithEra,,,,,,,,,"06/Aug/19 10:51 PM;esplinr;Thank you for the issue report. We will look into it when we have the chance.

If someone submits a PR, we will prioritize reviewing that quickly.;;;","08/Aug/19 10:51 PM;esplinr;This PR is pending DCO:
https://github.com/hyperledger/indy-node/pull/1389;;;","12/Aug/19 10:28 PM;ashcherbakov;Done in https://github.com/hyperledger/indy-node/pull/1411;;;",,,,,,,,,,,,,,,,,,,
Use audit ledger in Checkpoints,INDY-2177,41364,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,23/Jul/19 6:32 PM,01/Oct/19 7:59 PM,28/Oct/23 2:47 AM,31/Aug/19 1:15 AM,,,1.10.0,,,,,0,,,,,"As of now, Checkpoints calculate digests based on the digests of PrePrepares forming the Checkpoints. It can lead to a problem when the Checkpoint is started not from the beginning. For example we caught-up till ppSeqNo=53, so the current code will start the next Checkpoint from 54 instead of 0. So, this Checkpoint will have different digest comparing to other nodes participating in consensus (these nodes have all 100 PrePrepares forming the Checkpoint, but caught-up node has only 46).
 This can be critical in a new view change protocol relying on checkpoints a lot. 
 With a presence of audit ledger we can use audit ledger's root hash as a digest to solve the issue mentioned above.

*Acceptance criteria*
 * Use audit ledger's root hash instead of digests
 * Checkpoint's startSeqNo can be equal to always 0. In general, it's not needed at all, but we have to keep it for backward compatibility.
 * Simplify checkpoint logic in general",,,,,,,,,,,,,,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98632qi",,,,Unset,Unset,Ev-Node 19.16,Ev-Node 19.17,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"13/Aug/19 12:50 AM;sergey.khoroshavin;*PoA*
* Use audit ledger root hash as checkpoint digest
* Always set checkpoints startSeqNo to 0
* Change internal representation (inside CheckpointService) of received checkpoints to dict (view_no, pp_seq_no, audit_digest) -> List[node_alias] and simplify logic as much as possible
** for example it might not make sense to do stashing checkpoints from future view anymore
* Make sure there is always at least one checkpoint stored in ConsensusSharedData
* Temporarily change stable_checkpoint in ConsensusSharedData to tuple (view_no, pp_seq_no) (it can be changed back to plain number after INDY-1336 is implemented)

Almost every step is a separate small PR with tests fixed;;;","31/Aug/19 1:14 AM;sergey.khoroshavin;*PR*
https://github.com/hyperledger/indy-plenum/pull/1296
https://github.com/hyperledger/indy-plenum/pull/1304

*Changes made*
* Audit ledger root hash is used as checkpoint digest
* Checkpoints startSeqNo is set to 0 and no longer used
* Internal representation of received checkpoints is changed as in PoA

PR (https://github.com/hyperledger/indy-plenum/pull/1317) with ""make sure there is always at least one checkpoint stored"" functionality was raised, however it turned out it is blocked by INDY-1336, so it was decided to move this into scope of INDY-1340;;;",,,,,,,,,,,,,,,,,,,,
Request missing ViewChange messages when receiving NewView ,INDY-2178,41365,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,23/Jul/19 6:40 PM,23/Sep/19 10:53 PM,28/Oct/23 2:47 AM,23/Sep/19 10:53 PM,,,1.11.0,,,,,0,,,,,"*Acceptance criteria:*
* Implement requesting of `ViewChange` messages we don't have a Node for the given `NewView`.

Have a look at https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf, `New-View Message Processing`",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vo",,,,Unset,Unset,Ev-Node 19.18,Ev-Node 19.19,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"11/Sep/19 11:55 PM;Toktar;*PoA*:

If a node received a NewView message but has not enough ViewChange messages then request a ViewChange message from nodes which didn't send it.
 * Add logic of ViewChange messages requesting to ViewChangeService.
 * Add handler to consensus/message_handlers 
 * Rename MessageReq3pcService to MessageReqService
 * If a node is future primary then answer MessageRep(ViewChange)
If a node isn't future prinary then answer MessageRep(ViewChangeAsk)
 * Check that all storages work correctly. 
 * Add tests with delaying ViewChange messages ** ;;;","23/Sep/19 10:53 PM;Toktar;*Problem reason:*
 - We need to request missing ViewChange messages if a node lost it. Without ViewChange messages the node can't finish a view change.

*Changes:*
 - add ViewChangeHandler
 - integrate ViewChange requesting
 - add tests

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1342]
 * [https://github.com/hyperledger/indy-plenum/pull/1335]

*Version:*
 * indy-plenum 1.9.2.dev906 -master

*Risk factors:*
 - Problems with new ViewChangeService

*Risk:*
 - Low

*Tests:*
 * test_process_view_change_started
 * test_invalid_keys
 * [test_sim_view_change_with_delays.py|https://github.com/hyperledger/indy-plenum/pull/1335/files#diff-2e6b5d1ef931eddfebb1905e1cece4e7] 
 * [test_message_req_view_change.py|https://github.com/hyperledger/indy-plenum/pull/1335/files#diff-a85c78d89ba6280f36b968a594547667] 

*Recommendations for QA:*
 * Will be tested in scope of the task INDY-2223;;;",,,,,,,,,,,,,,,,,,,,
Integrate Checkpointer Service into Replica,INDY-2179,41366,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,23/Jul/19 6:46 PM,01/Oct/19 8:00 PM,28/Oct/23 2:47 AM,07/Aug/19 4:56 PM,,,1.10.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Aug/19 8:17 PM;VladimirWork;Figure_1.png;https://jira.hyperledger.org/secure/attachment/17653/Figure_1.png","06/Aug/19 8:17 PM;VladimirWork;Figure_2.png;https://jira.hyperledger.org/secure/attachment/17652/Figure_2.png","06/Aug/19 8:17 PM;VladimirWork;Figure_3.png;https://jira.hyperledger.org/secure/attachment/17651/Figure_3.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwqnz:",,,,Unset,Unset,Ev-Node 19.15,Ev-Node 19.16,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"05/Aug/19 4:27 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/1289;;;","05/Aug/19 6:47 PM;Toktar;*Problem reason:*
 - The CheckpointService is not used in the Replica.

*Changes:*
 - Integrate the CheckpointService  to the Replica
 - Remove old storages and methods from the Replica
 - Update tests

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1289]
 * [https://github.com/hyperledger/indy-node/pull/1405]

*Version:*
 * token-plugin 1.0.2 ~75-master
 * indy-node 1.9.2.dev1050 -master
 * (indy-plenum 1.9.2.dev861 -master)

*Risk factors:*
 - Catchup with checkpoints will not start

*Risk:*
 - Medium

*Recommendations for QA:*
 * Start a production load
 * Brake a not primary node for 2 checkpoints (200 batches)
 * Check that the node successfully caught up.;;;","06/Aug/19 7:52 PM;VladimirWork;Build Info:
indy-node 1.9.2~dev1050
plugins 1.0.2~dev75

Steps to Reproduce:
1. Run production load for 8+ hours.
2. Stop 10th node and start it after few minutes.
3. Wait for 10th node catchup.
4. Stop the load.

Actual Results:
10th node catch up successfully at Step 3 but after Step 4 3rd and 10th nodes have less amount of txn in all ledgers (276827 in domain) than all other nodes. Also 25th node has stalled a bit (277974 in domain, maybe because of OOM failure that appeared in journalctl). All 3 stalled nodes don't catch up after stopping the load for 2+ hours.

Logs and metrics:
ev@evernymr33:logs/INDY-2179-06-08-2019.tar.gz;;;","07/Aug/19 4:52 PM;VladimirWork;Build Info:
indy-node 1.9.2~dev1050
plugins 1.0.2~dev75

Steps to Reproduce:
1. Run production load with half load rate.
2. Stop 15th node and start it after few minutes.
3. Wait for 15th node catchup.
4. Stop the load.
5. Check that all nodes in pool are in sync.

Actual Results:
Stopped node catches up and orders successfully under load. All nodes are in sync after the load.;;;",,,,,,,,,,,,,,,,,,
One node can't catch up,INDY-2180,41368,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,VladimirWork,VladimirWork,23/Jul/19 10:43 PM,24/Jul/19 6:45 PM,28/Oct/23 2:47 AM,24/Jul/19 6:45 PM,,,,,,,,0,,,,,"Build Info:
indy-node 1.9.0~dev1036

Steps to Reproduce:
https://github.com/hyperledger/indy-test-automation/blob/163a56b21c5df792b26595d00211047b16311baa/system/indy-node-tests/TestAuditSuite.py#L13

Actual Results:
1st node can't catch up after stopping and starting.

Additional Info:
This issue reproduces intermittently (1..2 times from 10 attempts).",,,,,,,,,,,,,,,,,,,,,,,,INDY-2183,,,,,,,"23/Jul/19 10:43 PM;VladimirWork;catchup-failure.tar.gz;https://jira.hyperledger.org/secure/attachment/17622/catchup-failure.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7f:o",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"24/Jul/19 6:26 PM;ashcherbakov;*General conclusion*
 * There are no issues with catchup, it was finished successfully
 * The problem was with ZMQ connection: Node1 wasn't able to connect to two nodes (node 4 and node 7), but these two nodes connected to Node1. A separate ticket INDY-2183 is created for this issue.

*Details*
 # Node1 connected to *3 nodes only*:

{code:java}
2019-07-23 13:19:20,385|NOTIFICATION|keep_in_touch.py|Node1's connections changed from {'Node5', 'Node3'} to {'Node5', 'Node3', 'Node2'} {code}
This was enough (F+1) to finish catchup, so Node 1 caught up till (2, 3)
 # The pool continued ordering while Node 1 was doing catchup, and ordered till ~(2,30)
 # Node 1 stashed all 3PC messages received during catchup and started unstashing it when catchup was finished.
 # However, since Node1 connected to F+1 nodes only, but needed at least N-F nodes for quorum, it wasn't able to apply stashed 3PC messages and continue ordering.

*Missing connection to two nodes*
 * Node1 was trying to connect to Nodes 4 and 7 all the time. It re-created sockets and sent pings, but didn't receive pongs.

{code:java}
.....
2019-07-23 13:19:42,029|TRACE|remote.py|disconnecting socket 147
2019-07-23 13:19:42,029|TRACE|remote.py|connecting socket 158 to remote Node7:HA(host='10.0.0.8', port=9713), addr: tcp://0.0.0.0:0;10.0.0.8:9713
2019-07-23 13:19:42,029|DEBUG|zstack.py|Node1 pinged Node7
--
2019-07-23 13:19:42,030|TRACE|remote.py|disconnecting socket 154
2019-07-23 13:19:42,030|TRACE|remote.py|connecting socket 147 to remote Node4:HA(host='10.0.0.5', port=9707), addr: tcp://0.0.0.0:0;10.0.0.5:9707
2019-07-23 13:19:42,031|DEBUG|zstack.py|Node1 pinged Node4
--
2019-07-23 13:20:12,046|TRACE|remote.py|disconnecting socket 158
2019-07-23 13:20:12,046|TRACE|remote.py|connecting socket 155 to remote Node7:HA(host='10.0.0.8', port=9713), addr: tcp://0.0.0.0:0;10.0.0.8:9713
2019-07-23 13:20:12,047|DEBUG|zstack.py|Node1 pinged Node7
--
2019-07-23 13:20:12,047|TRACE|remote.py|disconnecting socket 147
2019-07-23 13:20:12,048|TRACE|remote.py|connecting socket 174 to remote Node4:HA(host='10.0.0.5', port=9707), addr: tcp://0.0.0.0:0;10.0.0.5:9707
2019-07-23 13:20:12,048|DEBUG|zstack.py|Node1 pinged Node4
--
2019-07-23 13:20:42,053|TRACE|remote.py|disconnecting socket 155
2019-07-23 13:20:42,056|TRACE|remote.py|connecting socket 147 to remote Node7:HA(host='10.0.0.8', port=9713), addr: tcp://0.0.0.0:0;10.0.0.8:9713
2019-07-23 13:20:42,056|DEBUG|zstack.py|Node1 pinged Node7
..... {code}
 * However, Nodes 4 and 7 were able to connect to Node1:

{code:java}
2019-07-23 13:17:12,365|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node3', 'Node7', 'Node5', 'Node6'} to {'Node1', 'Node3', 'Node7', 'Node5', 'Node6'}
------------
2019-07-23 13:17:12,378|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node6', 'Node4', 'Node3', 'Node5'} to {'Node6', 'Node4', 'Node3', 'Node5', 'Node1'}
 {code}
 * A possible issue here is that Nodes4 and 7 were able to connect and didn't re-created (close and create) the socket anymore. However, Node1 wasn't able and started to close and re-create sockets. For some reasons, Node4 and 7 were not able to detect that Node1's socket is closed.
 It may indicate a problem with closing a socket, or a problem in a monitor socket responsible for getting Disconnect events.
 * The hypothesis above is proved by the fact that Node 1 also experience problems with connection to Node2, but was eventually able to connect to it. Node 2 at the same time also wasn't able to connect to Node1 and was re-creating sockets the same way as Node 1 did:

{code:java}
Node1:
2019-07-23 13:17:11,961|TRACE|remote.py|connecting socket 157 to remote Node2:HA(host='10.0.0.3', port=9703), addr: tcp://0.0.0.0:0;10.0.0.3:9703
2019-07-23 13:17:11,961|INFO|zstack.py|CONNECTION: Node1 looking for Node2 at 10.0.0.3:9703
2019-07-23 13:17:11,961|DEBUG|zstack.py|Node1 pinged Node2
--
2019-07-23 13:17:41,976|DEBUG|remote.py|disconnecting remote Node2:HA(host='10.0.0.3', port=9703)
2019-07-23 13:17:41,977|TRACE|remote.py|disconnecting socket 157
2019-07-23 13:17:41,977|TRACE|remote.py|connecting socket 162 to remote Node2:HA(host='10.0.0.3', port=9703), addr: tcp://0.0.0.0:0;10.0.0.3:9703
2019-07-23 13:17:41,977|DEBUG|zstack.py|Node1 pinged Node2
2019-07-23 13:17:41,978|TRACE|kit_zstack.py|Node1 next check for retries in 15.00 seconds
--
2019-07-23 13:18:11,996|DEBUG|remote.py|disconnecting remote Node2:HA(host='10.0.0.3', port=9703)
2019-07-23 13:18:11,997|TRACE|remote.py|disconnecting socket 162
2019-07-23 13:18:11,997|TRACE|remote.py|connecting socket 155 to remote Node2:HA(host='10.0.0.3', port=9703), addr: tcp://0.0.0.0:0;10.0.0.3:9703
2019-07-23 13:18:11,997|DEBUG|zstack.py|Node1 pinged Node2
2019-07-23 13:18:11,999|TRACE|kit_zstack.py|Node1 next check for retries in 15.00 seconds
--
2019-07-23 13:18:42,012|DEBUG|remote.py|disconnecting remote Node2:HA(host='10.0.0.3', port=9703)
2019-07-23 13:18:42,012|TRACE|remote.py|disconnecting socket 155
2019-07-23 13:18:42,012|TRACE|remote.py|connecting socket 169 to remote Node2:HA(host='10.0.0.3', port=9703), addr: tcp://0.0.0.0:0;10.0.0.3:9703
2019-07-23 13:18:42,013|DEBUG|zstack.py|Node1 pinged Node2
2019-07-23 13:18:42,013|TRACE|kit_zstack.py|Node1 next check for retries in 15.00 seconds
--
2019-07-23 13:19:12,025|DEBUG|remote.py|disconnecting remote Node2:HA(host='10.0.0.3', port=9703)
2019-07-23 13:19:12,025|TRACE|remote.py|disconnecting socket 169
2019-07-23 13:19:12,025|TRACE|remote.py|connecting socket 161 to remote Node2:HA(host='10.0.0.3', port=9703), addr: tcp://0.0.0.0:0;10.0.0.3:9703
2019-07-23 13:19:12,026|DEBUG|zstack.py|Node1 pinged Node2

--------

Node2:
9-07-23 13:14:51,340|TRACE|remote.py|disconnecting socket 109
2019-07-23 13:14:51,340|TRACE|remote.py|Node1:HA(host='10.0.0.2', port=9701) closing monitor socket
2019-07-23 13:14:51,340|TRACE|remote.py|connecting socket 115 to remote Node1:HA(host='10.0.0.2', port=9701), addr: tcp://0.0.0.0:0;10.0.0.2:9701
2019-07-23 13:14:51,341|DEBUG|zstack.py|Node2 pinged Node1
2019-07-23 13:14:51,341|TRACE|kit_zstack.py|Node2 next check for retries in 15.00 seconds
--
2019-07-23 13:15:21,349|DEBUG|remote.py|disconnecting remote Node1:HA(host='10.0.0.2', port=9701)
2019-07-23 13:15:21,349|TRACE|remote.py|disconnecting socket 115
2019-07-23 13:15:21,349|TRACE|remote.py|connecting socket 109 to remote Node1:HA(host='10.0.0.2', port=9701), addr: tcp://0.0.0.0:0;10.0.0.2:9701
2019-07-23 13:15:21,349|DEBUG|zstack.py|Node2 pinged Node1
2019-07-23 13:15:21,350|TRACE|kit_zstack.py|Node2 next check for retries in 15.00 seconds
--
2019-07-23 13:15:51,861|DEBUG|remote.py|disconnecting remote Node1:HA(host='10.0.0.2', port=9701)
2019-07-23 13:15:51,862|TRACE|remote.py|disconnecting socket 109
2019-07-23 13:15:51,862|TRACE|remote.py|connecting socket 115 to remote Node1:HA(host='10.0.0.2', port=9701), addr: tcp://0.0.0.0:0;10.0.0.2:9701
2019-07-23 13:15:51,862|DEBUG|zstack.py|Node2 pinged Node1
2019-07-23 13:15:51,863|TRACE|kit_zstack.py|Node2 next check for retries in 15.00 seconds
--
2019-07-23 13:16:21,877|DEBUG|remote.py|disconnecting remote Node1:HA(host='10.0.0.2', port=9701)
2019-07-23 13:16:21,877|TRACE|remote.py|disconnecting socket 115
2019-07-23 13:16:21,878|TRACE|remote.py|connecting socket 109 to remote Node1:HA(host='10.0.0.2', port=9701), addr: tcp://0.0.0.0:0;10.0.0.2:9701
2019-07-23 13:16:21,878|DEBUG|zstack.py|Node2 pinged Node1
2019-07-23 13:16:21,879|TRACE|kit_zstack.py|Node2 next check for retries in 15.00 seconds
--
2019-07-23 13:16:51,894|DEBUG|remote.py|disconnecting remote Node1:HA(host='10.0.0.2', port=9701)
2019-07-23 13:16:51,894|TRACE|remote.py|disconnecting socket 109
2019-07-23 13:16:51,894|TRACE|remote.py|connecting socket 109 to remote Node1:HA(host='10.0.0.2', port=9701), addr: tcp://0.0.0.0:0;10.0.0.2:9701
2019-07-23 13:16:51,895|DEBUG|zstack.py|Node2 pinged Node1
2019-07-23 13:16:51,895|DEBUG|zstack.py|Node2 pinged Node6
--
2019-07-23 13:19:20,296|DEBUG|zstack.py|Node2 pinged Node5
2019-07-23 13:19:20,296|TRACE|zstack.py|Node2 stashed pongs: set()
2019-07-23 13:19:20,296|TRACE|remote.py|connecting socket 153 to remote Node1:HA(host='10.0.0.2', port=9701), addr: tcp://0.0.0.0:0;10.0.0.2:9701
2019-07-23 13:19:20,296|INFO|zstack.py|CONNECTION: Node2 looking for Node1 at 10.0.0.2:9701
2019-07-23 13:19:20,297|DEBUG|zstack.py|Node2 pinged Node1
 {code};;;",,,,,,,,,,,,,,,,,,,,,
Invesigate long VC in test_vc_by_demotion in CD,INDY-2181,41371,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,24/Jul/19 12:50 AM,02/Aug/19 8:57 PM,28/Oct/23 2:47 AM,02/Aug/19 8:57 PM,,,,,,,,0,,,,,"Build Info:
1.9.0~dev1036

Steps to Reproduce:
https://github.com/hyperledger/indy-test-automation/blob/163a56b21c5df792b26595d00211047b16311baa/system/indy-node-tests/test_vc.py#L38

Actual Results:
VC can't be completed for more than *480* seconds.

Additional Info:
https://build.sovrin.org/job/test-pipelines/job/indy-node-nightly-test/59/artifact/",,,,,,,,,,,,,,,,,,,,,,,,INDY-2182,,,,,,,"24/Jul/19 12:53 AM;VladimirWork;archive.zip;https://jira.hyperledger.org/secure/attachment/17623/archive.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98609",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"24/Jul/19 12:54 AM;VladimirWork;Logs are added to the ticket.;;;","24/Jul/19 6:32 PM;ashcherbakov;*Problem reason*
 * It looks like the reason is a very long GET_VALIDATOR_INFO execution (6 sec) which is done synchronously blocking main execution.
 * It affects connections, so Nodes started to disconnect from each other after such long delays.
 * The main reason of long execution GET_VALIDATOR_INFO  is memory profiling.

*Suggested fix:*
 * Remove memory profiling from GET_VALIDATOR_INFO execution
 * Do not change output JSON structure, just put N/A into memory-related fields.

 ;;;","24/Jul/19 6:55 PM;anikitinDSR;Also, we can reduce frequency of get_validator_info sending for checking primary status in system's tests.

 ;;;","26/Jul/19 9:27 PM;ashcherbakov;The test still fails intermittently even with the GET_VALIDATOR_INFO fix, but in a different place: https://github.com/hyperledger/indy-test-automation/blob/163a56b21c5df792b26595d00211047b16311baa/system/indy-node-tests/test_vc.py#L49

 

*Reasons for failing:*
 * The test doesn't wait for View Change after promotion of Primary (as it does after demotion). So, clients can not send requests (and the test fails) just because view change is in progress.
 * The view change is so long because of INDY-2023: after promotion, the pool selects the same Primary as was on the previous view, so all nodes are waiting for the new View Change triggered after timeout (7 minutes!)

*Recommendation:*
 * Fix the test to wait for view change after promotion before sending any new requests
 * Remove the test from the list of ones running for master
 * Either skip the test or increase timeout for waiting for view (it needs to be > 420 sec)
 * Write a new test where the last (7th) node is demoted and promoted. In this test view change after promotion should be fast and INDY-2023 will not be reproduced. Such test can be included into a list of test running on master;;;","02/Aug/19 8:56 PM;VladimirWork;1. test_vc_by_demotion_primary was fixed and skipped.
2. test_vc_by_demotion_last was implemented and will be sent in the next PR after several local runs.;;;",,,,,,,,,,,,,,,,,
Memory profiling needs to be removed from GET_VALIDATOR_INFO output,INDY-2182,41378,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,24/Jul/19 6:35 PM,26/Jul/19 11:15 PM,28/Oct/23 2:47 AM,26/Jul/19 9:56 PM,,,1.9.1,,,,,0,,,,,"* GET_VALIDATOR_INFO execution may be quite long (6 sec) and it's done synchronously blocking main execution.
 * It affects connections, so Nodes started to disconnect from each other after such long delays.
 * The main reason of long execution GET_VALIDATOR_INFO  is memory profiling.
 * This can lead to system tests failing where GET_VALIDATOR_INFO is used a lot (see INDY-2171)

*Suggested fix:*
 * Remove memory profiling from GET_VALIDATOR_INFO execution
 * Do not change output JSON structure, just put N/A into memory-related fields.",,,,,,,,,,,,,,,,,,,,,INDY-2181,INDY-2175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7f:i",,,,Unset,Unset,Ev-Node 19.15,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,esplinr,lbendixsen,mgbailey,,,,,,,"24/Jul/19 8:30 PM;anikitinDSR;Reasons:
 * need to remove memory_profiler from GET_VALIDATOR_INFO cmd output

Changes:
 * Output of GET_VALIDATOR_INFO cmd includes only empty value for 'Memory_profiler' field. In the other words, we don't yet collect memory profiler info.

Version:
 * indy-node: 1.9.0.dev1039

Recommendation for QA:
 * setup pool with trace logs
 * send GET_VALIDATOR_INFO cmd
 * check logs for time, spending for processing GET_VALIDATOR_INFO action (string between 'Transaction .* with type 119 started' and 'Transaction .* with type 119 finished')
 * Also, take a look at the nightly builds for `test_vc.py` fails;;;","25/Jul/19 6:12 AM;esplinr;I'm glad that the team looked into INDY-2181 and has a good theory for what the cause is.

However, we shouldn't be hasty in deciding on the right fix. Specifically, there are trade-offs to this change that we should discuss as a group. I am nervous about losing this visibility into how the production network functions. Has anyone checked with [~dfarns] for his monitoring of the Sovrin Network?

I am also nervous about our team making unilateral changes without collaborating with other maintainers of Indy networks.

Some alternative approaches that should be considered:
1. Stop returning this data.
2. Do nothing, i.e. allow some validator-info responses to be slow.
3. Have a separate command that collects and returns this data at an appropriate interval.;;;","25/Jul/19 5:17 PM;ashcherbakov;[~esplinr]
I totally agree that this should not be a silent change, and it needs to be communicated. That's why we started the discussion with [~lbendixsen] and [~mgbailey]. We will talk with Dan as well.

A couple of things about the `Memory_profiler`:

1) This field is *not* for showing a total RAM. We have another fields for this: `_RAM_all_free_`, `_RAM_used_by_node_`. *These RAM fields are not removed* *and can (and should) be used for memory analysis.* HDD fields were also *not* removed.
Memory_profiler fields are used for showing sizes of low level python objects used by the application. Understanding and using this info requires good understanding of code internals.
2) This field was added there as rather a debug information when we were hunting OOM issues. It wasn't a request from the customer, and it wasn't announced to anyone. So, I'll be really surprised if anyone actively uses this info. So, from my point of view, the issue is that we forgot to remove the field that time.
3) The concern with having this field is not minor at all. Keeping the field can lead to
- This stops node from doing anything for 6 secs on a rather small system tests environment. I would expect the delay can be more significant if we have a production Net running for days and weeks.
- Although only specific roles can run the command, this is an easy to use attack vector. One can shot nodes one by one making the pool unavailable at all, or leading to constant view changes (plus remember that we still don't have PBFT View change in stable).  
- It can lead to node disconnections.

As a conclusion, I agree that this is a trade off, and it must be discussed with other Maintainers, but taking into account the history of this field and reasoning, it's safer to remove it in the next Release.
We can discuss if we need an Option 3 in further releases if it's needed.;;;","26/Jul/19 12:19 AM;esplinr;Thank you Alex for the explanation about memory_profiler. You are correct that RAM_all_free and RAM_used_by_node are the fields I was worried about losing.

Also, thank you for the explanation about why collecting memory_profiler data is such a problem. I agree that it shouldn't be available in production. Do we want to keep the code for generating the report on the python objects in debug builds of Indy Node for the next time we need to do memory profiling?

I hadn't noticed that the issue doesn't discuss the HDD fields. Those were only mentioned in the chat. I'm glad we decided to keep them.;;;","26/Jul/19 12:25 AM;mgbailey;Could this be related to INDY-2175 as well?;;;","26/Jul/19 12:33 AM;ashcherbakov;[~mgbailey]
In theory yes, it can be the reason of INDY-2175. But we need to investigate the logs to say for sure.;;;","26/Jul/19 2:10 AM;lbendixsen;Thanks to all for the comments and explanations.  I am in agreement with this fix and I appreciate being consulted to make sure we do the right thing with items of this nature.;;;","26/Jul/19 9:56 PM;ashcherbakov;GET_VALIDATOR_INFO execution time changed from 6 secs to 0.1-0.2 secs.

It also fixed the mentioned system test from INDY-2181 (at least the part affected by this issue).;;;",,,,,,,,,,,,,,
A node may not be able to connect to another node if another node was able to connect ,INDY-2183,41379,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,24/Jul/19 6:43 PM,24/Sep/19 6:59 PM,28/Oct/23 2:47 AM,09/Sep/19 9:41 PM,,,1.10.0,,,,,0,TShirt_L,,,,"*How the issues is reproduced*
 * See logs from INDY-2180: Node1 wasn't able to connect to two nodes (node 4 and node 7), but these two nodes connected to Node1.

 

*Details*
 * Node1 was trying to connect to Nodes 4 and 7 all the time. It re-created sockets and sent pings, but didn't receive pongs.

{code:java}
.....
2019-07-23 13:19:42,029|TRACE|remote.py|disconnecting socket 147
2019-07-23 13:19:42,029|TRACE|remote.py|connecting socket 158 to remote Node7:HA(host='10.0.0.8', port=9713), addr: tcp://0.0.0.0:0;10.0.0.8:9713
2019-07-23 13:19:42,029|DEBUG|zstack.py|Node1 pinged Node7
--
2019-07-23 13:19:42,030|TRACE|remote.py|disconnecting socket 154
2019-07-23 13:19:42,030|TRACE|remote.py|connecting socket 147 to remote Node4:HA(host='10.0.0.5', port=9707), addr: tcp://0.0.0.0:0;10.0.0.5:9707
2019-07-23 13:19:42,031|DEBUG|zstack.py|Node1 pinged Node4
--
2019-07-23 13:20:12,046|TRACE|remote.py|disconnecting socket 158
2019-07-23 13:20:12,046|TRACE|remote.py|connecting socket 155 to remote Node7:HA(host='10.0.0.8', port=9713), addr: tcp://0.0.0.0:0;10.0.0.8:9713
2019-07-23 13:20:12,047|DEBUG|zstack.py|Node1 pinged Node7
--
2019-07-23 13:20:12,047|TRACE|remote.py|disconnecting socket 147
2019-07-23 13:20:12,048|TRACE|remote.py|connecting socket 174 to remote Node4:HA(host='10.0.0.5', port=9707), addr: tcp://0.0.0.0:0;10.0.0.5:9707
2019-07-23 13:20:12,048|DEBUG|zstack.py|Node1 pinged Node4
--
2019-07-23 13:20:42,053|TRACE|remote.py|disconnecting socket 155
2019-07-23 13:20:42,056|TRACE|remote.py|connecting socket 147 to remote Node7:HA(host='10.0.0.8', port=9713), addr: tcp://0.0.0.0:0;10.0.0.8:9713
2019-07-23 13:20:42,056|DEBUG|zstack.py|Node1 pinged Node7
..... {code}
 * However, Nodes 4 and 7 were able to connect to Node1:

{code:java}
2019-07-23 13:17:12,365|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node3', 'Node7', 'Node5', 'Node6'} to {'Node1', 'Node3', 'Node7', 'Node5', 'Node6'}
------------
2019-07-23 13:17:12,378|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node6', 'Node4', 'Node3', 'Node5'} to {'Node6', 'Node4', 'Node3', 'Node5', 'Node1'}
 {code}
 * A possible issue here is that Nodes4 and 7 were able to connect and didn't re-created (close and create) the socket anymore. However, Node1 wasn't able and started to close and re-create sockets. For some reasons, Node4 and 7 were not able to detect that Node1's socket is closed.
 It may indicate a problem with closing a socket, or a problem in a monitor socket responsible for getting Disconnect events.
 * The hypothesis above is proved by the fact that Node 1 also experience problems with connection to Node2, but was eventually able to connect to it. Node 2 at the same time also wasn't able to connect to Node1 and was re-creating sockets the same way as Node 1 did.

*Suggested fix*
 * Write a test that tries to reproduce the issue:

 ** Make Node1 connected to Node2, but Node2 not connected so that Node2 re-creates sockets to Node1, but Node1 doesn't.
 * Fix the issue:
 ** Check that closing of sockets work properly
 ** Check that monitor events work
 ** Re-connect a socket to a connected node if we continue getting PINGs from this node which mean that Node1 can not connect to us.",,,,,,,,,,,,,,,,,,,INDY-2202,,INDY-2180,,,INDY-2222,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwqr3:x",,,,Unset,Unset,Ev-Node 19.17,Ev-Node 19.18,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"05/Sep/19 11:20 PM;ashcherbakov;*Issue reason*
- it looks like ZMQ sockets can not re-establish connections when re-creating sockets from the first time, so multiple attempts may be needed
- each attempt to re-create a socket has been done every 30 secs only

*Change*
- increase the frequency of re-connect re-tries to 8 secs (try every 2 secs; re-send pings 3 times and recreate socket on the 4th time)

*PR*
- https://github.com/hyperledger/indy-plenum/pull/1315
- https://github.com/hyperledger/indy-node/pull/1439

*Test*
- `stp_zmq/test/test_reconnect_multi.py `

*Recommendations for QA*
- check how the changes affect system tests
- run load test 
- run load test with one node disconnected to make sure that more attempts to re-connect doesn't break anything

*Build*
- 1.10.0.dev1077
- 1.0.3~dev86;;;","09/Sep/19 9:41 PM;VladimirWork;Build Info:
indy-node 1.10.0.dev1077
plugins 1.0.3~dev86

Actual Results:
There are no unexpected failures in system tests (in CD and locally). Production load test looks good but there are some issues with BLS signatures during production load test with one node stopped (reported in INDY-1932).;;;",,,,,,,,,,,,,,,,,,,,
Move signature verification into Pluggable Req Handlers,INDY-2184,41380,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,24/Jul/19 8:20 PM,08/Jan/20 5:12 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Move logic from `ReqAuthenticator` and `ClientAuthNr` etc. into pluggable req handlers.
 * A method `verify_signatures(request)` needs to be present in `WriteRequestManager` and `WriteRequestHandler`.
 * It can use a common verification logic from one of the WriteRequestHandler's parent classes.
 * NymReqHandler needs to override to support new NYM use case (see INDY-2171)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i59x",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pool timeout on get_revoc_reg request during state proof reading,INDY-2185,41454,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,26/Jul/19 11:31 PM,24/Oct/19 5:00 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Build Info:
libindy 1.10.1~1213
libsovtoken 1.0.0~77
indy-node 1.9.1~dev1040

Steps to Reproduce:
1. Write revoc_reg_def into ledger.
2. Turn down all nodes in pool except one.
2. Get revoc_reg with timestamp *after* revoc_reg_def was written to ledger.
3. Get revoc_reg with timestamp *before* revoc_reg_def was written to ledger.

Actual Results:
Step 2 is successful but Step 3 returns PoolLedgerTimeout error (it reproduces intermittently).

Expected Results:
Step 2 and Step 3 both should return replies.

Additional Info:
According to investigation from SDK team there is no state proof data in the pool reply in Step 3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/19 12:44 AM;VladimirWork;client_log;https://jira.hyperledger.org/secure/attachment/17627/client_log","26/Jul/19 11:27 PM;VladimirWork;client_log;https://jira.hyperledger.org/secure/attachment/17626/client_log","26/Jul/19 11:30 PM;VladimirWork;get-revoc-reg-failure.tar.gz;https://jira.hyperledger.org/secure/attachment/17625/get-revoc-reg-failure.tar.gz","27/Jul/19 12:44 AM;VladimirWork;get-revoc-reg-success.tar.gz;https://jira.hyperledger.org/secure/attachment/17628/get-revoc-reg-success.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2266,,,No,,Unset,No,,,"1|hzwx4f:2rzmfi",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"27/Jul/19 12:44 AM;VladimirWork;The same case against the same setup but successful for get_revoc_reg:  [^client_log]  [^get-revoc-reg-success.tar.gz] ;;;",,,,,,,,,,,,,,,,,,,,,
Ubuntu 20.04 support,INDY-2186,41505,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 9:54 PM,26/Mar/20 10:29 PM,28/Oct/23 2:47 AM,,,,,,,,,1,,,,,"Ubuntu 18.04 (bionic beaver) should be supported by the source code and CI/CD/Nightly pipelines of the indy-node and indy-plenum.

We need to simultaneously support two versions Ubuntu, i.e. we need to support Ubuntu 16.04 alongside Ubuntu 18.04. We will drop support for Ubuntu 16.04 when we add support for the next LTS release (probably Ubuntu 20.04).",,,,,,,,,,,,,,,,,,,,,INDY-1090,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-7,,Ubuntu 20.04 support,To Do,No,,Unset,No,,,"1|i00vhn:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,barotashish25,esplinr,HimangshuPan,,,,,,,,"26/Mar/20 10:29 PM;esplinr;Now that we are so close to the release of Ubuntu 20.04, we will skip supporting Ubuntu 18.04.;;;",,,,,,,,,,,,,,,,,,,,,
Support python3.8 by indy-plenum,INDY-2187,41506,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 9:57 PM,27/Mar/20 10:03 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,,,,,"Acceptance criteria:
 # All indy-plenum tests should pass in python3.8 env (default in ubuntu 20.04)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixws6",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support python3.8 by indy-node,INDY-2188,41507,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 9:58 PM,27/Mar/20 9:13 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,,,,,"Acceptance criteria:
 # All indy-node tests should pass in python3.8 env (default in ubuntu 20.04)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixws9",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set up indy-plenum CI pipeline for ubuntu 20.04 env,INDY-2189,41508,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 10:01 PM,27/Mar/20 8:51 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,devops,,,,"Options:
 # run 20.04 env along with current 16.04 one
 # run 20.04 env nightly for master, CI for PRs checks only 16.04

Acceptance criteria:
 # all tests should be run by CI server
 # CI process (PRs verification) shouldn't be impacted drastically (much longer)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwsi",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,"30/Jul/19 10:47 PM;andkononykhin;*PoA*
 # Update scripts (dockerfiles) to support bionic env
 # Update pipelines to run tests in new env. Options:
 ## Run in parallel with xenial testing to verify PRs.
 *** pros: provides faster feedback before changes actually been merged
 *** cons: requires double resources and/or increases CI pipeline time.
 *** Options:
 #### as standalone pipeline with it own PR status check
 ***** pros: more verbose GitHub statuses
 ***** cons: more CI and GitHub configuration routine, copy-paste netween pipelines
 #### in scope of the same pipeline
 ***** pros and cons: opposite
 ## Run nightly:
 *** pros and cons: opposite

My vision that current infrastructure doesn't have a power to support parallel CI testing. Until it is chaged (either GitLab or Jenkins based solution or other ...) nightly builds is the only possible choice.;;;",,,,,,,,,,,,,,,,,,,,,
Set up indy-node CI pipeline for ubuntu 20.04 env,INDY-2190,41509,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 10:01 PM,27/Mar/20 8:51 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,devops,,,,"Options:
 # run 20.04 env along with current 16.04 one
 # run 20.04 env nightly for master, CI for PRs checks only 16.04

Acceptance criteria:
 # all tests should be run by CI server
 # CI process (PRs verification) shouldn't be impacted drastically (much longer)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwt",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,"30/Jul/19 10:49 PM;andkononykhin;(a copy of [comment|https://jira.hyperledger.org/browse/INDY-2189?focusedCommentId=62394&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-62394])

 *PoA*
 # Update scripts (dockerfiles) to support bionic env
 # Update pipelines to run tests in new env. Options:
 ## Run in parallel with xenial testing to verify PRs.
 *** pros: provides faster feedback before changes actually been merged
 *** cons: requires double resources and/or increases CI pipeline time.
 *** Options:
 #### as standalone pipeline with it own PR status check
 ***** pros: more verbose GitHub statuses
 ***** cons: more CI and GitHub configuration routine, copy-paste netween pipelines
 #### in scope of the same pipeline
 ***** pros and cons: opposite
 ## Run nightly:
 *** pros and cons: opposite

My vision that current infrastructure doesn't have a power to support parallel CI testing. Until it is chaged (either GitLab or Jenkins based solution or other ...) nightly builds is the only possible choice.;;;",,,,,,,,,,,,,,,,,,,,,
Publish indy-plenum debian artifacts to ubuntu 20.04 distro,INDY-2191,41510,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 10:03 PM,27/Mar/20 8:51 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,devops,,,,"Need to update indy-plenum CD pipeline to publish to both ubuntu 16.04 and 20.04 distros.

Acceptance criteria:
 # CD should publish bionic artifacts to the same debian repository as for xenial
 # indy-plenum package should be published along with its 3rd parties dependencies that not presented in canonical repositories
 # artifacts should be installable",,,,,,,,,,INDY-2196,,,,INDY-2193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwti",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,montasser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Publish indy-node debian artifacts to ubuntu 20.04 distro,INDY-2192,41511,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 10:04 PM,27/Mar/20 8:51 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,devops,,,,"Need to update indy-node CD pipeline to publish to both ubuntu 16.04 and 20.04 distros.

Acceptance criteria:
 # CD should publish bionic artifacts to the same debian repository as for xenial
 # indy-node package should be published along with its 3rd parties dependencies that not presented in canonical repositories
 # artifacts should be installable",,,,,,,,,,INDY-2196,,,,INDY-2193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwu",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,barotashish25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support ubuntu 20.04 in system tests,INDY-2193,41512,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 10:07 PM,27/Mar/20 8:51 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,1,system-tests,,,,"Need to update system tests to support ubuntu 20.04 testing env.

Acceptance criteria:
 # test pool can be run in ubuntu 20.04 env (all nodes the same env for now)
 # client might be run in any env
 # no tests regression",,,,,,,,,,INDY-2191,INDY-2192,,,INDY-2195,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwui",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,barotashish25,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endorser field must be present in GET requests with state proofs,INDY-2194,41513,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,30/Jul/19 10:08 PM,24/Oct/19 5:02 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"As of now the only way to check what Endorser was used to submit a transaction is to look at the transaction itself (GET_TXN request). This looks fine from most of the current use cases point of view (for example, Sovrin scripts).

But it would be great to know who was transaction Endorser when querying data via other GET requests with state proofs such as GET_NYM, GET_SCHEMA, GET_CLAIM_DEF, GET_REVOC_REG_DEF, GET_REVOC_REG_ENTRY.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx4f:2rzmhi",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
System testing on CI server in ubuntu 20.04 env,INDY-2195,41514,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,30/Jul/19 10:09 PM,27/Mar/20 8:51 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,devops,system-tests,,,"Options:
 # nightly build pipeline for ubuntu 20.04
 # both nightly and CD pipelines check system tests in ubuntu 20.04 env
 # nightly for master, CD only for releases

Acceptance criteria:
 # system tests in 20.04 env should be run by CI server automatically
 ** We may reduce the frequency of system tests run. For example, one nigh run tests on 16.04 only, and the other night on 20.04 only.
 # CD for master shouldn't require much more time than it is now
 # CD during release process must check system tests in both env

Notes:
 * We need to continue building and testing Ubuntu 16.04 for some period of time, but Ubuntu 20.04 should be our focus. Most automated tests should be with Ubunut 20.04, and Ubuntu 16.04 only needs sanity checks and release tests.",,,,,,,,,,INDY-2193,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwv",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explore python dependencies in ubuntu 20.04 canonical debian repositories,INDY-2196,41515,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,donqui,andkononykhin,andkononykhin,30/Jul/19 11:12 PM,30/May/20 3:31 AM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,devops,,,,"Currently we have set of pinned top level dependencies in both indy-plenum and indy-node. Transitional (dependencies of the dependencies)  dependencies' versions are not managed and might use fuzzy constraints.

Seems canonical repositories for LTS releases are rarely updated for python debian packages it doesn't lead to any observable issues (at least we haven't encountered them yet).

But in ubuntu 20.04 we likely would have new set of versions of that dependencies and that might shift the env.

The general idea is to fix testing env and recommend it for production usage as only one tested. Please refer to tasks INDY-1701 for more details. For python there was no standard way for that in the past but there were a set of options (INDY-1706). It might be different now.

The goal of the task is to make decision:
 # is it possible to keep the same dependency list (tree) on both platforms
 # dependencies versions for each (both)

 

 ",,,,,,,,,,,,,,INDY-2191,INDY-2192,,,,,,INDY-1701,INDY-1706,,INDY-1702,INDY-1701,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2186,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqek",,,,Unset,Unset,Ev-Node 20.06,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,esplinr,,,,,,,,,,"31/Jul/19 12:27 AM;andkononykhin;*PoA*
 * list all dependencies (top level + transitional) that are used on clean xenial env (installed during indy-node installation)
 * similar for bionic, options
 ** may try to install from indy xenial
 ** manually check available versions using dpkg / apt tools
 * compare and make a decision regarding possibility of xenial deps set usage for bionic. I believe it is possible in any case since missed packages / versions might be packed from sources and published to sovrin debian repository
 * inspect dependencies compatibility and their release notes and make a decision:
 ** is it possible (and reasonable) to keep the same env for both platforms, if yes - options:
 *** keep xenial env
 *** move to bionic env
 *** use some other env: bump dependencies to some versions compatible with each other and appropriate for indy projects
 ** otherwise - decide which set of deps is acceptable for each platform
 * make a decision regarding the ways of locking the test environments to ones that will be expected on production (INDY-1706 is useful but might be outdated)
 * choose which transitional dependencies might be not locked / pinned: since we would need to pin them for debian as well it would require to add them to debian package 'Depends' field making it quite long. Thus, it makes sense to not pin deps that might be considered as stable or are not updated for the platform. 

I vote for the same env for both platforms since:
 * python is a platform independent language in general (but for sure requires some attention to file system related operation, some platform dependent modules)
 * it seems much easier to maintain (other platform might come into play) and test;;;","24/Sep/19 5:11 AM;esplinr;The current Python packages we are building because they are newer than Ubuntu 16.04:
https://repo.sovrin.org/deb/pool/xenial/stable-latest/p/

I would be surprised if we could remove the custom packages for a specific version of pyzmq and rocksdb, but I think we can take the repository defaults for the other packages. python-base58 might be an Indy specific package contributed by Evernym.;;;","30/Oct/19 6:22 AM;esplinr;As we enable support for Ubunut 18.04, we need to rethink our dependency pinning. We took a very conservative line in 2018 because we were seeing a lot of library churn, but things have been more stable in 2019. We currently see more problems from too strict pinning than we do with too loose dependency management.

Daniel does a good job explaining appropriate pinning here:
https://jira.hyperledger.org/browse/INDY-1701?focusedCommentId=50863&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-50863

Critical dependencies probably should continue to be explicitly pinned.

Some major dependencies should probably be pinned to a version less than the next major version, so we keep API stability but also get security updates.

Most dependencies shouldn't need to be pinned anymore so that people can experiment and report problems they encounter with new versions. ;;;",,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.9.1,INDY-2197,41540,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,31/Jul/19 8:54 PM,02/Aug/19 11:18 PM,28/Oct/23 2:47 AM,02/Aug/19 9:00 PM,,,,,,,,0,,,,,"*Version Information*
indy-node 1.9.1
indy-plenum 1.9.1
sovrin 1.1.52

*Notices for Stewards*
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.

*Major Changes*
- New DIDs can be created without endorsers
- Transaction authors don't need to be endorsers
- TAA acceptance should use date, not time
- Bug fixes

*Detailed Changelog*

+Major Fixes+
INDY-2164 - Incorrect request validation
INDY-2112 - Need to make ""reask_ledger_status"" repeatable
INDY-2143 - When view change takes too long instance change should be sent periodically

+Changes and Additions+
INDY-2171 - New DIDs can be created without endorsers
INDY-2173 - Transaction authors don't need to be endorsers
INDY-2141 - Grab pool data for failed system tests
INDY-2182 - Memory profiling needs to be removed from GET_VALIDATOR_INFO output
INDY-2147 - Implement PBFT viewchanger service with most basic functionality
INDY-2136 - Extract Orderer service from Replica
INDY-2139 - Extract and integrate ConsensusDataProvider from Replica
INDY-2157 - TAA acceptance should use date, not time
INDY-2154 - Clean-up Pluggable Request Handlers

+Known Issues+
-
",,,,,,,,,,,,,,,,,,,,,INDY-2162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00vor:",,,,Unset,Unset,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"02/Aug/19 9:00 PM;VladimirWork;Release notes PRs in indy-node and sovrin repos were sent and merged.;;;",,,,,,,,,,,,,,,,,,,,,
Endorser field can contian a DID with a known role only,INDY-2198,41603,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,02/Aug/19 10:13 PM,13/Aug/19 11:42 PM,28/Oct/23 2:47 AM,13/Aug/19 5:49 PM,,,1.9.2,,,,,0,,,,,"If Endorser points to a DID without a specific role (Node role; common user), then it will be unknown who should be charged.

So, Endorser can point to DIDs with known (not-None) roles only such as TRUSTEE. ENDORSER, STEWARD, etc.

Question:
* Can the endorser field contain any defined role (transaction author is an undefined role), or should it only contain DIDs with the role of endorser?",,,,,,,,,,,,,,,,,,,,,INDY-2199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9861403i",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,VladimirWork,,,,,,,,,"06/Aug/19 5:18 PM;ashcherbakov;[~esplinr]

In the scope of INDY-2199 the following checks will be implemented:
 Endorser's role must be ENDORSER.;;;","06/Aug/19 6:13 PM;ashcherbakov;*Changes:*
 * Endorser field can have ENDORSER role only

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1408]

*Tests*
 * [test_endorser_authorizer.py|https://github.com/hyperledger/indy-node/pull/1408/files#diff-7a01e890f0bd1f1d4fb782a25e21272b] (unit)
 * [test_send_by_endorser.py|https://github.com/hyperledger/indy-node/pull/1408/files#diff-50239dfe5bc00fee55f3d7dac8bd2a52] (integration)

*Risk:*
 * Low

*Build:* 
 * indy-node 1.9.2.dev1054

*Recommendation for QA:*
 * Review existing tests
 * Implement system tests;;;","07/Aug/19 1:49 AM;esplinr;I spoke with Drummond, and we need to make a minor change.

The Endorser field should only contain DIDs with an on-ledger role of Endorser. Trustees and Stewards should not be endorsing 3rd party transactions with their non-Endorser DIDs.

This is important for the Sovrin use case, were Trustees do not have an agreement to pay for writes, and Stewards have a different agreement than most Endorsers. But it is also appropriate for other networks who might have different policies governing endorsers. The endorser field in the transaction should be explicitly tied to the endorser role as an Indy concept.

Thank you for your efforts.;;;","07/Aug/19 4:46 PM;ashcherbakov;[~esplinr]
Thank you for clarification, the PR has been updated.

BTW what about a situation when a user collaborated with a malicious Steward and a Steward sends SCHEMA/CLAIM_DEF as a Steward (so that Endorser is not needed), and then the Steward gives all the keys to the user, and a user can use this SCHEMAs/CLAIM_DEFs without paying anything to SF?;;;","09/Aug/19 6:27 AM;esplinr;Thank you for pointing out this potential circumvention. We should advise network administrators that if they are concerned about that scenario they should disallow Stewards from creating schema and claim definitions in the auth_rules for their networks. (cc [~lbendixsen]);;;","13/Aug/19 5:48 PM;VladimirWork;Verified in scope of INDY-2199.;;;",,,,,,,,,,,,,,,,
Endorsers must be specified within the transaction,INDY-2199,41605,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,02/Aug/19 10:51 PM,29/Aug/19 5:09 PM,28/Oct/23 2:47 AM,13/Aug/19 5:53 PM,1.9.1,,1.9.2,,,,,0,,,,,"Steps to Reproduce:
* Configure auth_rules such that the ledger accepts the transaction with one endorser signature
* Create a transaction without specifying an endorser DID
* Have the author sign the transaction
* Have an endorser sign the transaction
* Submit to the ledger

Expected behavior:
* The transaction is rejected because the endorser signature does not match the endorser field

Observed behavior:
* The transaction is accepted",,,,,,,,,,,,,,,,,,,,,INDY-1999,,,INDY-2198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9861402",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,VladimirWork,,,,,,,,,"05/Aug/19 4:20 PM;ashcherbakov;*PoA:*
 * *Changes*
 ** If the number of  `signatures` with ENDORSER/STEWARD/TRUSTEE roles is greater than one, and the Author's DID doesn't have ENDORSER/STEWARD/TRUSTEE role, then check that Endorser field is specified.
 *** If there is just one signature in `signatures` field, then the current validation makes sure that this is the Author's signature, so no need to require Endorser field
 *** If Author's role is ENDORSER/STEWARD/TRUSTEE, then he or she doesn't require endorsement even in case of multi-sig, since it can be an action, requiring, for example, 3 TRUSRTEEs.
 ** Check that endorser's role is ENDORSER
 * *Where*
 ** In a new EndorserAuthorizer;;;","06/Aug/19 6:13 PM;ashcherbakov;*Changes:*
 * Endorser field can have ENDORSER role only
 * Endorser field must be explicitly present if transaction is trying to be endorsed with a multi-sig (see the PoA above)

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1408]
 * [https://github.com/hyperledger/indy-node/pull/1410]

*Tests*
 * [test_endorser_authorizer.py|https://github.com/hyperledger/indy-node/pull/1408/files#diff-7a01e890f0bd1f1d4fb782a25e21272b] (unit)
 * [test_send_by_endorser.py|https://github.com/hyperledger/indy-node/pull/1408/files#diff-50239dfe5bc00fee55f3d7dac8bd2a52] (integration)

*Risk:*
 * Low

*Build:*
 * indy-node 1.9.2.dev1054
 * sovtoken 1.0.2~dev76

*Recommendation for QA:*
 * Review existing tests
 * Implement system tests;;;","13/Aug/19 5:53 PM;VladimirWork;Build Info:
indy-node 1.9.2.dev1054
plugins 1.0.2~dev76

Steps to Validate:
https://github.com/VladimirWork/indy-test-automation/blob/work-in-progress/system/indy-node-tests/TestEndorserSuite.py

Actual Results:
All endorser constraints work as expected.;;;",,,,,,,,,,,,,,,,,,,
Use production-like environment in simulation tests,INDY-2200,41696,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,08/Aug/19 6:49 PM,01/Oct/19 8:00 PM,28/Oct/23 2:47 AM,13/Aug/19 11:26 PM,,,1.10.0,,,,,0,,,,,"Currently simulation tests run in environment which is agnostic of ledgers. However in order to make sure our implementation is really correct these tests need to interact with ledgers (especially audit ledger) in the same way production code interacts with them. In other words - simulation tests should create ledgers and register request handlers with the same code which runs in production.

The problem here is that production code that performs initialization is still tightly coupled with node, which is slow and hard to initialize (due to network, databases and lots of other cruft), so simulation tests would take forever to run. So, we need to decouple node initialization code and make it possible to run with in-memory storages instead of RocksDB

*Acceptance criteria*
* Make ledgers and request handlers initialization code agnostic of node
* Make it possible to use in-memory ledger storage in tests
* Use new initialization code in simulation tests and make sure they are still deterministic and run in sensible time",,,,,,,,,,,,,,INDY-2149,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969v9861404",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,"13/Aug/19 11:43 PM;sergey.khoroshavin;*Changes made*
* initialization of ledgers and request managers extracted to LedgersBoostrap from NodeBootstrap
* LedgersBootstrap can work without filesystem access (using in-memory storages for ledgers and genesis txns)
* simulation tests use LedgersBootstrap to initialize write request manager

*PR*
https://github.com/hyperledger/indy-plenum/pull/1294
https://github.com/hyperledger/indy-node/pull/1412;;;",,,,,,,,,,,,,,,,,,,,,
Simplify Primary Selection logic,INDY-2201,41697,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,,ashcherbakov,ashcherbakov,08/Aug/19 6:53 PM,10/Oct/19 4:12 PM,28/Oct/23 2:47 AM,10/Oct/19 4:12 PM,,,1.13.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2167,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c9r",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"10/Oct/19 4:12 PM;ashcherbakov;Duplicates INDY-2241;;;",,,,,,,,,,,,,,,,,,,,,
VC didn't complete in 480 seconds,INDY-2202,41786,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,,VladimirWork,VladimirWork,14/Aug/19 3:58 PM,15/Aug/19 7:32 PM,28/Oct/23 2:47 AM,15/Aug/19 7:32 PM,,,,,,,,0,,,,,"Build Info:
indy-node 1.9.2~dev1056

Steps to Reproduce:
1. Install 7 nodes pool.
2. Demote 7th.
3. Check for VC completion.

Actual Results:
VC didn't complete in 480 seconds.

Expected Results:
VC should complete in 480 seconds.",,,,,,,,,,,,,,,,,,,,INDY-2183,,,,,,,,,,,"14/Aug/19 3:58 PM;VladimirWork;long_vc_14_08.zip;https://jira.hyperledger.org/secure/attachment/17671/long_vc_14_08.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7e:o",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"14/Aug/19 10:22 PM;VladimirWork;Looks like nodes sent IC messages to each other after 7th node demotion but there were no VC for some reason.;;;","15/Aug/19 4:28 PM;VladimirWork;This test failed again in nightly node pipeline so maybe it's not an intermittent failure but some regression.;;;","15/Aug/19 4:29 PM;VladimirWork;FYI [~ashcherbakov] [~sergey.khoroshavin];;;","15/Aug/19 6:54 PM;ashcherbakov;It looks like the reason why View Change hasn't started is connection problems between nodes when NodeA thinks that it's connected to NodeB while NodeB is not connected to NodeA.
This is the same issue as in INDY-2183;;;","15/Aug/19 6:55 PM;ashcherbakov;Node1:
{code}
2019-08-13 23:20:52,589|DEBUG|zstack.py|Node1 pinged Node7
2019-08-13 23:20:52,590|DEBUG|zstack.py|Node1 pinged Node5
2019-08-13 23:20:52,595|DEBUG|zstack.py|Node1 pinged Node3
2019-08-13 23:20:52,596|DEBUG|zstack.py|Node1 pinged Node6
2019-08-13 23:20:52,603|DEBUG|zstack.py|Node1 pinged Node2
2019-08-13 23:20:52,604|DEBUG|zstack.py|Node1 pinged Node4
2019-08-13 23:20:53,867|TRACE|zstack.py|Node1 got pong from Node4
2019-08-13 23:20:53,869|NOTIFICATION|keep_in_touch.py|Node1's connections changed from set() to {'Node4'}
2019-08-13 23:20:53,889|TRACE|zstack.py|Node1 got pong from Node4
2019-08-13 23:20:54,970|TRACE|zstack.py|Node1 got pong from Node5
2019-08-13 23:20:54,974|NOTIFICATION|keep_in_touch.py|Node1's connections changed from {'Node4'} to {'Node4', 'Node5'}
2019-08-13 23:20:55,068|TRACE|zstack.py|Node1 got pong from Node5
2019-08-13 23:20:59,668|DEBUG|view_changer.py|Node1's view_changer sending INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,675|DEBUG|node.py|Node1 received node message from Node2: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,881|DEBUG|node.py|Node1 received node message from Node5: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,903|DEBUG|node.py|Node1 received node message from Node4: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:07,596|DEBUG|zstack.py|Node1 pinged Node2
2019-08-13 23:21:07,596|DEBUG|zstack.py|Node1 pinged Node3
2019-08-13 23:21:07,596|DEBUG|zstack.py|Node1 pinged Node6
2019-08-13 23:21:07,628|TRACE|zstack.py|Node1 got pong from Node3
2019-08-13 23:21:07,644|TRACE|zstack.py|Node1 got pong from Node2
2019-08-13 23:21:07,645|NOTIFICATION|keep_in_touch.py|Node1's connections changed from {'Node4', 'Node5'} to {'Node3', 'Node2', 'Node4', 'Node5'}
2019-08-13 23:21:07,671|TRACE|zstack.py|Node1 got pong from Node6
2019-08-13 23:21:07,700|NOTIFICATION|keep_in_touch.py|Node1's connections changed from {'Node3', 'Node2', 'Node4', 'Node5'} to {'Node3', 'Node2', 'Node4', 'Node5', 'Node6'}
{code}

Node3:
{code}
2019-08-13 23:20:54,574|DEBUG|zstack.py|Node3 pinged Node7
2019-08-13 23:20:54,600|DEBUG|zstack.py|Node3 pinged Node2
2019-08-13 23:20:54,601|DEBUG|zstack.py|Node3 pinged Node5
2019-08-13 23:20:54,601|DEBUG|zstack.py|Node3 pinged Node1
2019-08-13 23:20:54,602|DEBUG|zstack.py|Node3 pinged Node4
2019-08-13 23:20:54,603|DEBUG|zstack.py|Node3 pinged Node6
2019-08-13 23:20:54,644|TRACE|zstack.py|Node3 got pong from Node2
2019-08-13 23:20:54,644|NOTIFICATION|keep_in_touch.py|Node3's connections changed from set() to {'Node2'}
2019-08-13 23:20:54,716|TRACE|zstack.py|Node3 got pong from Node1
2019-08-13 23:20:54,717|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node2'} to {'Node1', 'Node2'}
2019-08-13 23:20:54,765|TRACE|zstack.py|Node3 got pong from Node4
2019-08-13 23:20:54,766|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node1', 'Node2'} to {'Node1', 'Node4', 'Node2'}
2019-08-13 23:20:54,921|TRACE|zstack.py|Node3 got pong from Node5
2019-08-13 23:20:54,923|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node1', 'Node4', 'Node2'} to {'Node1', 'Node4', 'Node5', 'Node2'}
2019-08-13 23:20:55,319|TRACE|zstack.py|Node3 got pong from Node7
2019-08-13 23:20:55,329|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node1', 'Node4', 'Node5', 'Node2'} to {'Node1', 'Node4', 'Node7', 'Node5', 'Node2'}
2019-08-13 23:20:55,536|TRACE|zstack.py|Node3 got pong from Node7
2019-08-13 23:20:59,867|DEBUG|node.py|Node3 received node message from Node2: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,870|DEBUG|node.py|Node3 received node message from Node5: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,909|DEBUG|node.py|Node3 received node message from Node4: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,935|DEBUG|node.py|Node3 received node message from Node7: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:07,710|DEBUG|node.py|Node3 received node message from Node1: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:08,853|DEBUG|node.py|Node3 received node message from Node4: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:09,041|DEBUG|node.py|Node3 received node message from Node2: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:09,130|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node1', 'Node4', 'Node7', 'Node5', 'Node2'} to {'Node1', 'Node4', 'Node5', 'Node2'}
2019-08-13 23:21:09,581|DEBUG|zstack.py|Node3 pinged Node6
2019-08-13 23:21:09,610|TRACE|zstack.py|Node3 got pong from Node6
2019-08-13 23:21:09,611|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node1', 'Node4', 'Node5', 'Node2'} to {'Node1', 'Node4', 'Node5', 'Node6', 'Node2'}
2019-08-13 23:29:14,915|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node1', 'Node4', 'Node5', 'Node6', 'Node2'} to {'Node5', 'Node4', 'Node6', 'Node2'}
2019-08-13 23:29:15,222|NOTIFICATION|keep_in_touch.py|Node3's connections changed from {'Node5', 'Node4', 'Node6', 'Node2'} to {'Node4', 'Node6', 'Node5'}
{code}

Node2:
{code}
2019-08-13 23:20:54,127|NOTIFICATION|keep_in_touch.py|Node2's connections changed from set() to {'Node4', 'Node1'}
2019-08-13 23:20:54,829|NOTIFICATION|keep_in_touch.py|Node2's connections changed from {'Node4', 'Node1'} to {'Node5', 'Node1', 'Node4'}
2019-08-13 23:20:55,475|NOTIFICATION|keep_in_touch.py|Node2's connections changed from {'Node5', 'Node1', 'Node4'} to {'Node5', 'Node1', 'Node7', 'Node4'}
2019-08-13 23:20:59,673|NOTIFICATION|keep_in_touch.py|Node2's connections changed from {'Node5', 'Node1', 'Node7', 'Node4'} to {'Node5', 'Node1', 'Node4'}
2019-08-13 23:20:59,940|DEBUG|node.py|Node2 received node message from Node5: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,941|DEBUG|node.py|Node2 received node message from Node4: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:09,015|NOTIFICATION|keep_in_touch.py|Node2's connections changed from {'Node5', 'Node1', 'Node4'} to {'Node5', 'Node1', 'Node6', 'Node3', 'Node4'}
2019-08-13 23:21:07,722|DEBUG|node.py|Node2 received node message from Node1: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:29:14,911|NOTIFICATION|keep_in_touch.py|Node2's connections changed from {'Node5', 'Node1', 'Node6', 'Node3', 'Node4'} to {'Node5', 'Node6', 'Node3', 'Node4'}
{code}

Node4:
{code}
2019-08-13 23:20:53,877|NOTIFICATION|keep_in_touch.py|Node4's connections changed from set() to {'Node1'}
2019-08-13 23:20:54,159|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1'} to {'Node1', 'Node2'}
2019-08-13 23:20:55,041|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1', 'Node2'} to {'Node1', 'Node2', 'Node5'}
2019-08-13 23:20:55,195|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1', 'Node2', 'Node5'} to {'Node1', 'Node2', 'Node6', 'Node5'}
2019-08-13 23:20:55,516|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1', 'Node2', 'Node6', 'Node5'} to {'Node1', 'Node2', 'Node6', 'Node5', 'Node7'}
2019-08-13 23:20:59,691|DEBUG|node.py|Node4 received node message from Node2: INSTANCE_CHANGE{'reason': 46, 'viewNo': 1}
2019-08-13 23:20:59,886|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1', 'Node2', 'Node6', 'Node5', 'Node7'} to {'Node1', 'Node2', 'Node6', 'Node5'}
2019-08-13 23:20:59,893|DEBUG|node.py|Node4 received node message from Node5: INSTANCE_CHANGE{'reason': 46, 'viewNo': 1}
2019-08-13 23:21:07,700|DEBUG|node.py|Node4 received node message from Node1: INSTANCE_CHANGE{'reason': 46, 'viewNo': 1}
2019-08-13 23:21:08,844|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1', 'Node2', 'Node6', 'Node5'} to {'Node1', 'Node2', 'Node6', 'Node5', 'Node3'}
2019-08-13 23:29:14,912|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node1', 'Node2', 'Node6', 'Node5', 'Node3'} to {'Node2', 'Node6', 'Node5', 'Node3'}
2019-08-13 23:29:15,219|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node2', 'Node6', 'Node5', 'Node3'} to {'Node6', 'Node5', 'Node3'}
2019-08-13 23:29:15,487|NOTIFICATION|keep_in_touch.py|Node4's connections changed from {'Node6', 'Node5', 'Node3'} to {'Node6', 'Node5'}
{code}

Node5:
{code}
2019-08-13 23:20:54,983|NOTIFICATION|keep_in_touch.py|Node5's connections changed from set() to {'Node2'}
2019-08-13 23:20:55,051|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node2'} to {'Node3', 'Node1', 'Node2'}
2019-08-13 23:20:55,093|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node3', 'Node1', 'Node2'} to {'Node4', 'Node3', 'Node1', 'Node2'}
2019-08-13 23:20:55,370|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node4', 'Node3', 'Node1', 'Node2'} to {'Node4', 'Node3', 'Node1', 'Node2', 'Node6'}
2019-08-13 23:20:55,667|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node4', 'Node3', 'Node1', 'Node2', 'Node6'} to {'Node3', 'Node2', 'Node4', 'Node6', 'Node7', 'Node1'}
2019-08-13 23:20:59,678|DEBUG|node.py|Node5 received node message from Node2: INSTANCE_CHANGE{'reason': 46, 'viewNo': 1}
2019-08-13 23:20:59,872|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node3', 'Node2', 'Node4', 'Node6', 'Node7', 'Node1'} to {'Node4', 'Node3', 'Node1', 'Node2', 'Node6'}
2019-08-13 23:20:59,897|DEBUG|node.py|Node5 received node message from Node4: INSTANCE_CHANGE{'reason': 46, 'viewNo': 1}
2019-08-13 23:21:07,690|DEBUG|node.py|Node5 received node message from Node1: INSTANCE_CHANGE{'reason': 46, 'viewNo': 1}
2019-08-13 23:29:14,908|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node4', 'Node3', 'Node1', 'Node2', 'Node6'} to {'Node4', 'Node3', 'Node2', 'Node6'}
2019-08-13 23:29:15,215|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node4', 'Node3', 'Node2', 'Node6'} to {'Node4', 'Node3', 'Node6'}
2019-08-13 23:29:15,485|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node4', 'Node3', 'Node6'} to {'Node4', 'Node6'}
2019-08-13 23:29:15,736|NOTIFICATION|keep_in_touch.py|Node5's connections changed from {'Node4', 'Node6'} to {'Node6'}
{code}

Node6:
{code}
2019-08-13 23:20:55,246|NOTIFICATION|keep_in_touch.py|Node6's connections changed from set() to {'Node4'}
2019-08-13 23:20:55,568|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node4'} to {'Node5', 'Node4'}
2019-08-13 23:20:55,611|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5', 'Node4'} to {'Node5', 'Node4', 'Node1', 'Node3', 'Node2'}
2019-08-13 23:20:55,642|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5', 'Node4', 'Node1', 'Node3', 'Node2'} to {'Node4', 'Node7', 'Node1', 'Node5', 'Node2', 'Node3'}
2019-08-13 23:20:59,862|DEBUG|node.py|Node6 received node message from Node2: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,888|DEBUG|node.py|Node6 received node message from Node5: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,888|DEBUG|node.py|Node6 received node message from Node4: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:20:59,925|DEBUG|node.py|Node6 received node message from Node7: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:07,710|DEBUG|node.py|Node6 received node message from Node1: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:09,054|DEBUG|node.py|Node6 received node message from Node2: INSTANCE_CHANGE{'viewNo': 1, 'reason': 46}
2019-08-13 23:21:09,703|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node4', 'Node7', 'Node1', 'Node5', 'Node2', 'Node3'} to {'Node5', 'Node4', 'Node1', 'Node3', 'Node2'}
2019-08-13 23:29:14,920|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5', 'Node4', 'Node1', 'Node3', 'Node2'} to {'Node5', 'Node4', 'Node3', 'Node2'}
2019-08-13 23:29:15,212|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5', 'Node4', 'Node3', 'Node2'} to {'Node5', 'Node4', 'Node3'}
2019-08-13 23:29:15,490|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5', 'Node4', 'Node3'} to {'Node5', 'Node4'}
2019-08-13 23:29:15,741|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5', 'Node4'} to {'Node5'}
2019-08-13 23:29:16,027|NOTIFICATION|keep_in_touch.py|Node6's connections changed from {'Node5'} to set()
{code}

Node7:
{code}
2019-08-13 23:20:55,550|NOTIFICATION|keep_in_touch.py|Node7's connections changed from set() to {'Node2', 'Node3'}
2019-08-13 23:20:55,590|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node2', 'Node3'} to {'Node2', 'Node3', 'Node4', 'Node1'}
2019-08-13 23:20:55,670|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node2', 'Node3', 'Node4', 'Node1'} to {'Node2', 'Node3', 'Node4', 'Node6', 'Node1'}
2019-08-13 23:20:55,686|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node2', 'Node3', 'Node4', 'Node6', 'Node1'} to {'Node4', 'Node3', 'Node5', 'Node6', 'Node2', 'Node1'}
2019-08-13 23:29:14,918|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node4', 'Node3', 'Node5', 'Node6', 'Node2', 'Node1'} to {'Node2', 'Node5', 'Node3', 'Node4', 'Node6'}
2019-08-13 23:29:15,212|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node2', 'Node5', 'Node3', 'Node4', 'Node6'} to {'Node5', 'Node3', 'Node4', 'Node6'}
2019-08-13 23:29:15,488|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node5', 'Node3', 'Node4', 'Node6'} to {'Node6', 'Node4', 'Node5'}
2019-08-13 23:29:15,736|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node6', 'Node4', 'Node5'} to {'Node6', 'Node5'}
2019-08-13 23:29:16,027|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node6', 'Node5'} to {'Node6'}
2019-08-13 23:29:16,269|NOTIFICATION|keep_in_touch.py|Node7's connections changed from {'Node6'} to set()
{code}



;;;","15/Aug/19 7:32 PM;ashcherbakov;Will be fixed in the scope of INDY-2183;;;",,,,,,,,,,,,,,,,
5 of 25 nodes can't catch up after load and pool restart,INDY-2203,41800,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid: Environment Issue,,VladimirWork,VladimirWork,15/Aug/19 1:20 AM,28/Aug/19 6:20 PM,28/Oct/23 2:47 AM,15/Aug/19 7:36 PM,,,,,,,,0,,,,,"Build Info:
indy-node 1.9.2~dev1054
plugins 1.0.2~dev76

Steps to Reproduce:
1. Run production load test (with plugins) for 12+ hours.
2. Stop the load and wait for stalled nodes catch up.
3. Stop and start nodes after several minutes and wait for catch up.

Actual Results:
5, 8 ,14, 19, 21 nodes stalled and didn't catch up.

Additional Info:
After second pool restart bad nodes catch up normally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Aug/19 4:36 PM;VladimirWork;Figure_1.png;https://jira.hyperledger.org/secure/attachment/17682/Figure_1.png","15/Aug/19 4:36 PM;VladimirWork;Figure_2.png;https://jira.hyperledger.org/secure/attachment/17681/Figure_2.png","15/Aug/19 4:36 PM;VladimirWork;Figure_3.png;https://jira.hyperledger.org/secure/attachment/17680/Figure_3.png","15/Aug/19 4:36 PM;VladimirWork;Figure_4.png;https://jira.hyperledger.org/secure/attachment/17679/Figure_4.png","15/Aug/19 4:36 PM;VladimirWork;Figure_5.png;https://jira.hyperledger.org/secure/attachment/17678/Figure_5.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7e:u",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,VladimirWork,,,,,,,,,,"15/Aug/19 4:26 PM;VladimirWork;Logs and metrics: ev@evernymr33:logs/prod_load_node_1054_14_08_2019_after_restart.tar.gz;;;","15/Aug/19 4:29 PM;VladimirWork;FYI [~ashcherbakov] [~sergey.khoroshavin];;;","15/Aug/19 7:36 PM;sergey.khoroshavin;I've checked logs from failed nodes and they were doing client stack restart 10 seconds before shutdown, which was seemingly abnormal. It happened on Aug 13 around 22:05 UTC. Unfortunatelly journalctl was lost, so stack trace is not available. Good nodes were also doing client stack restart during that timeframe, so it might actually not be connected, but it still looks suspicious.

Anyways, this problem is not related to catch up, because nodes were not doing catch up when they shut down, and they obviously couldn't do catch up afterwards because they remained offline.;;;",,,,,,,,,,,,,,,,,,,
1.9.2 Release,INDY-2204,41805,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,15/Aug/19 4:57 PM,02/Sep/19 3:33 PM,28/Oct/23 2:47 AM,02/Sep/19 3:33 PM,,,1.9.2,,,,,0,,,,,"*Release Goal*
Release everything that is ready. Mostly bug fixes.

*Acceptance Criteria*
Regular instructions:
* Code is tested
* Review upstream releases to decide what should be included in this build
* Build is produced with automated CI / CD
** Official build is tagged as released in Git
** Official builds should be the only builds that are not marked in GitHub as ""pre-release""
* Prepare basic documentation on new features
* Prepare the Release Notes
** Link to the documentation on new features
** Stored as a CHANGELOG.md in the root of the repo
** Latest release at the top, above the release notes for all previous versions
* The release should be on GitHub “Releases” tab.
** Release notes should be in the “Description"" field of the GitHub release artifact
* The release is marked in JIRA
* Add the correct fixVersion to included issues
* Enable others to use the release:
** Email stakeholders with links to the artifacts and release notes",,,,,,,,,,,,,,,,,INDY-2210,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98632p",,,,Unset,Unset,Ev-Node 19.17,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"22/Aug/19 5:38 PM;ashcherbakov;[~esplinr] [~lbendixsen]
 Taking into account the issue INDY-2211, we have the following options for this Release
 # Do one hotfix release and apply it ASAP:
 ** INDY-2211 fixes only
 # Do one hotfix release and apply it ASAP:
 ** INDY-2211 fixes
 ** Endorser fixes
 # Do one hotfix release and apply it by regular schedule:
 ** INDY-2211 fixes
 ** Endorser fixes
 # Do two releases:
 ## Hotfix release as in Option 1
 ## Regular release with all changes from master
 # Do two releases:
 ## Hotfix release as in Option 2
 ## Regular release with all changes from master
 # Do one release with all changes from master (including INDY-2211) and apply it ASAP
 # Do one release with all changes from master (including INDY-2211) and apply it by regular schedule

 ;;;","28/Aug/19 6:34 PM;ashcherbakov;We choice option Option 3.

The release needs to have the following changes:
 * INDY-1954
 ** https://github.com/hyperledger/indy-plenum/pull/1303
 ** https://github.com/hyperledger/indy-plenum/pull/1308
 * INDY-2215
 ** https://github.com/hyperledger/indy-plenum/pull/1305
 * INDY-2218
 ** https://github.com/hyperledger/indy-plenum/pull/1309
 * INDY-2211
 ** https://github.com/hyperledger/indy-node/pull/1418
 ** [https://github.com/hyperledger/indy-node/pull/1420]
 ** https://github.com/hyperledger/indy-node/pull/1428
 * INDY-2176
 ** https://github.com/hyperledger/indy-node/pull/1411
 * INDY-2198 and INDY-2199
 ** [https://github.com/hyperledger/indy-node/pull/1408]
 ** https://github.com/hyperledger/indy-node/pull/1410;;;","29/Aug/19 5:27 PM;VladimirWork;*Version Information*
indy-node 1.9.2
indy-plenum 1.9.2
sovrin

*Notices for Stewards*
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.

*Major Changes*
- Stability fixes
- Endorser support fixes and improvements
- Improving GET_TXN to be able to query just one node the same way as for other GET requests

*Detailed Changelog*

+Major Fixes+
INDY-2211 - New nodes added after last upgrade (1.9.1) are not in consensus
INDY-2176 - indy-node broken by indy-plenum and python-dateutil
INDY-2218 - Issue with non utf-8 decoding
INDY-2199 - Endorsers must be specified within the transaction
INDY-2215 - One node doesn't catch up

+Changes and Additions+
INDY-1954 - As a user, I need to be able to know what was the last update time of the ledger when querying a txn via GET_TXN request
INDY-2198 - Endorser field can contian a DID with a known role only

+Known Issues+
-;;;",,,,,,,,,,,,,,,,,,,
Automated load test analysis,INDY-2205,41806,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,15/Aug/19 6:15 PM,14/Sep/19 1:07 AM,28/Oct/23 2:47 AM,13/Sep/19 9:18 PM,,,,,,,,0,,,,,"*Story*
 As a developer or QA analyzing load test results, I need to have all common errors and exception already grepped and reported, so that I can understand faster what's going on.

It would save a lot of time if load script automation could grep all common errors and exception in logs (each Node logs and journalctl).

 

*Acceptance criteria*

The result of load tests need to have the following information (as a report):
 * How many transactions have been written on every node
 * How many transaction expected to be written (from the load script point of view)
 * How many view changes happened
 * Graphs from each nodes
 * Metrics summary from each node (`get_metrics` output)
 * Get validator info result from each node
 * Journalctk analysis:
 ** output all exceptions (if any)
 * Logs analysis:
 ** All ERROR log messages
 ** The following patterns:
 *** `blacklisting`
 *** `invalid BLS signature`
 *** `request digest is incorrect`
 *** `has incorrect digest`
 *** `time not acceptable`
 *** `incorrect state trie root`
 *** `incorrect transaction tree root`
 *** `has incorrect reject`
 *** `error in plugin field`
 *** `incorrect audit ledger`
 *** <to be continued......>",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwvif:00001yw969vw",,,,Unset,Unset,Ev-Node 19.18,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"03/Sep/19 7:51 PM;VladimirWork;PoA:

Option 1 (more complex way):
Process all things using ansible but before this we must:
  - add build_metrics_from_csv to indy-node's setup.py
  - somehow save figures on nodes before pulling
  - process ansible playbook output (ansible callbacks?) to return human readable report

Option 2 (faster way):
Fix pool_get_logs (pull metrics summary, already done) a bit and process all this things with python on local machine to return readable csv or json.;;;","05/Sep/19 11:55 PM;VladimirWork;Also there must be new ansible script to pull client metrics for item 2.;;;","06/Sep/19 3:36 AM;VladimirWork;Initial implementation: https://github.com/VladimirWork/indy-test-automation/blob/work-in-progress/system/perf_res_processor.py;;;","11/Sep/19 6:19 PM;VladimirWork;Ansible playbook update is merged: https://github.com/hyperledger/indy-node/blob/master/scripts/ansible/pool_get_logs.yml
Processor script is merged: https://github.com/hyperledger/indy-test-automation/blob/master/system/perf_res_processor.py;;;",,,,,,,,,,,,,,,,,,
Design and implement production-like system test with complex pool creation,INDY-2206,41807,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,15/Aug/19 6:24 PM,15/Aug/19 7:50 PM,28/Oct/23 2:47 AM,15/Aug/19 7:50 PM,,,,,,,,0,,,,,"Design and implement production-like system test that contains minimal 4 nodes pool installation, 7 or more nodes adding (to increase f several times and provoke VCs), multiple nodes demotion and promotion (to change f several times and provoke VCs) and txns sending during all this actions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00x3n:",,,,Unset,Unset,Ev-Node 19.16,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"15/Aug/19 6:34 PM;VladimirWork;https://github.com/hyperledger/indy-test-automation/blob/master/system/draft/TestProductionSuite.py;;;",,,,,,,,,,,,,,,,,,,,,
"Implement helper script for AWS pool operations with snapshots, volumes and instances",INDY-2207,41808,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,15/Aug/19 6:26 PM,27/Aug/19 11:36 PM,28/Oct/23 2:47 AM,27/Aug/19 11:36 PM,,,,,,,,0,,,,,"Implement helper script for AWS pool operations with snapshots, volumes and instances to reduce switching time between upgrade and load AWS setups.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98632x",,,,Unset,Unset,Ev-Node 19.17,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"15/Aug/19 6:35 PM;VladimirWork;https://github.com/VladimirWork/indy-test-automation/blob/work-in-progress/system/aws_persistent_deployment.py;;;","27/Aug/19 11:35 PM;VladimirWork;Reviewed and merged into https://github.com/hyperledger/indy-test-automation/blob/master/system/aws_persistent_deployment.py

Instance and volume operations have been implemented. Snapshot operations will be implemented in scope of separate ticket.;;;",,,,,,,,,,,,,,,,,,,,
Integration of Services: Cleanup,INDY-2208,41834,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,16/Aug/19 5:48 PM,01/Oct/19 8:00 PM,28/Oct/23 2:47 AM,30/Aug/19 2:20 PM,,,1.10.0,,,,,0,,,,,"The following tech debt is left after integration of Ordering and Checkpointer services into existing code base:
 * There are similar checks if a message is for the given instance (`inst_id` checks). It would be great to have a common place with these checks.
 * `replica_unstash` needs to be either removed from stashing router, or renamed to something replica-agnostic (something like `unstash_handler`)
 * remove `l` prefix from Ordering service methods
 * `l_update_watermark_from_3pc` needs to be removed from Ordering Service
 * simplify `primaries_batch_needed` logic
 * Cleanup internal messages (remove unused ones)
 * `requestQueues` needs to be removed from Shared Data
 * Cleanup what's stored in Shared Data (make sure that only the data used by more than 1 service is there)
 * Remove hooks
 * remove process_requested_prepare from Replica
 * Fixes in `_bootstrap_consensus_data` (get rid of doing actions twice)
 * Remove `to_nodes` from Replica's `send`
 * `_init_internal_bus` subscribed twice
 * Answer/apply fixes for the comments in test from https://github.com/hyperledger/indy-plenum/pull/1280",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98632v",,,,Unset,Unset,Ev-Node 19.17,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"19/Aug/19 8:24 PM;Toktar;*PoA:*
 * add fixes from the description - [https://github.com/hyperledger/indy-plenum/pull/1300]
 * remove ConsensusDataHelper, hooks - [https://github.com/hyperledger/indy-plenum/pull/1312]
 * add an internal_bus to each replica - [https://github.com/hyperledger/indy-plenum/pull/1310]
 * bump indy-node in plugins - [https://github.com/sovrin-foundation/token-plugin/pull/298];;;","30/Aug/19 2:20 PM;Toktar;Will be tested in scope of other tasks for services integration.;;;",,,,,,,,,,,,,,,,,,,,
Need to support OrderingService integration for nsreplay util,INDY-2209,41837,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,anikitinDSR,anikitinDSR,16/Aug/19 8:23 PM,16/Aug/19 8:23 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,Due to OrderingService integration need to add corresponding changes to nsreplay util.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00x8z:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.10.0 Release,INDY-2210,41870,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,esplinr,19/Aug/19 10:32 PM,06/Oct/19 4:44 AM,28/Oct/23 2:47 AM,06/Oct/19 4:44 AM,,,1.10.0,,,,,0,,,,,"*Release Goal*
Release everything that is ready. Especially:
* Improved view change algorithm

*Acceptance Criteria*
Regular instructions:
* Code is tested
* Review upstream releases to decide what should be included in this build
* Build is produced with automated CI / CD
** Official build is tagged as released in Git
** Official builds should be the only builds that are not marked in GitHub as ""pre-release""
* Prepare basic documentation on new features
* Prepare the Release Notes
** Link to the documentation on new features
** Stored as a CHANGELOG.md in the root of the repo
** Latest release at the top, above the release notes for all previous versions
* The release should be on GitHub “Releases” tab.
** Release notes should be in the “Description"" field of the GitHub release artifact
* The release is marked in JIRA
* Add the correct fixVersion to included issues
* Enable others to use the release:
** Email stakeholders with links to the artifacts and release notes",,,,,,,,,,,,,,,,,,INDY-2204,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vi",,,,Unset,Unset,Ev-Node 19.19,Ev-Node 19.20,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,VladimirWork,,,,,,,,,"30/Sep/19 9:04 PM;ashcherbakov;Let's do the release against well-tested version indy-node 1.10.0~dev1084 (as in INDY-2226) and cherry-pick pyzmq fixes there;;;","04/Oct/19 8:47 PM;VladimirWork;*Version Information*
indy-node 1.10.0
indy-plenum 1.10.0
sovrin 1.1.58

*Notices for Stewards*
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.
PBFT View Change was implemented but not enabled so old View Change is active now.

*Major Changes*
- PBFT View Change implementation (not enabled yet) and corresponding code improvements 
- BLS multi-signature fixes and improvements
- The latest version of ZMQ library support
- Stability fixes

*Detailed Changelog*

+Major Fixes+
INDY-2233 - GET_TXN doesn't work with old libindy
INDY-2103 - Need to improve error message with invalid signature
INDY-2183 - A node may not be able to connect to another node if another node was able to connect
INDY-2212 - ZMQError: Address already in use when restarting client stack

+Changes and Additions+
INDY-2228 - All ledgers in a batch need to be BLS multi-signed
INDY-2226 - Drop ppSeqNo on Backups after View Change
INDY-2220 - Move 3PC Message Request logic into a separate service
INDY-2213 - Bump pyzmq to the latest version
INDY-2208 - Integration of Services: Cleanup
INDY-2179 - Integrate Checkpointer Service into Replica
INDY-2177 - Use audit ledger in Checkpoints
INDY-2169 - Integrate OrderingService into Replica
INDY-2167 - Integrate PrimarySelector into View Change Service
INDY-2150 - Integrate view change property-based tests into CI
INDY-2149 - Integrate and run PBFT View Changer simulation tests with a real implementation
INDY-2147 - Implement PBFT viewchanger service with most basic functionality
INDY-2139 - Extract and integrate ConsensusDataProvider from Replica
INDY-2137 - Extract Checkpointer service from Replica
INDY-2136 - Extract Orderer service from Replica
INDY-2135 - Simulation tests for View Changer (no integration)
INDY-1340 - Implementation: Make PBFT view change working
INDY-1339 - Implement network, executor, orderer and checkpointer as adaptors for existing codebase
INDY-1338 - Define Interfaces needed for View Change Service
INDY-1337 - Modify WriteReqManager to meet Executor interface needs
INDY-1336 - Stop resetting ppSeqNo (and relying on this) in new view
INDY-1335 - Enable full ordering of batches from last view that have been already ordered, make execution on replicas that executed them no-op

+Known Issues+
INDY-2222 - One node doesn't catch up after promotion;;;",,,,,,,,,,,,,,,,,,,,
New nodes added after last upgrade (1.9.1) are not in consensus,INDY-2211,41898,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,VladimirWork,lbendixsen,lbendixsen,20/Aug/19 7:57 AM,29/Aug/19 5:03 PM,28/Oct/23 2:47 AM,28/Aug/19 12:48 AM,1.9.0,1.9.1,1.9.2,,,,,0,EV-CS,,,,"The upgrade of the buildernet to 1.9.1 (or possibly earlier) seems to have caused a consensus issue. The new nodes have all of their ledgers up to date except for the audit ledger, but all ledgers are showing ""False"" for consensus in validator-info results for nodes that were recently added.  Logs attached for 3 ""bad"" nodes recently added and 1 ""good"" node (FoundationBuilder) that has been there since the beginning of the BuilderNet.

 

Environment:  BuilderNet, current, Sovrin 1.1.52, indy-node 1.9.1.

Steps to reproduce:
 # All new nodes since Aug 5 buildernet upgrade show the listed issue, and it is possible that this has been the case since a previous upgrade.
 # Multiple new nodes added for the past several months have shown connectivity issues also, so we are seeing possible general ""add node to existing network"" issues since ~indy-node version 1.8.0 (perhaps not an exact version, just a general feel to give you an idea of how long we have seen issues that might be related to adding new nodes to an existing network)

Expected Behavior: 

    When adding a new node, the new node should catch up and remain in consensus over time.

Observed Behavior:

    1. New nodes come onboard and appear to be working fine for some time (unknown whether it is hours or days that they work fine) then at some point they all go out of consensus and standard procedures to return them to consensus fail.  Standard procedures currently include restarting the node, and restarting the whole network.

    2. This comment might be unrelated to the current ticket, but the mitrecorp node onboarded prior to 8-5-2019 had numerous issues with client and node connections that went unexplained and unrepaired. 

 

Notes:

The following logs show that all of the ""bad"" nodes have the following, or similar, entries in them that the ""good"" node(Foundation Builder) does not have:

2019-08-19 16:37:15,658|WARNING|replica.py|cpqd:0 missing PRE-PREPAREs from 5521 to 5528, going to request 2019-08-19 16:37:15,666|INFO|replica.py|Queueing pre-prepares due to unavailability of previous pre-prepares. PREPREPARE{'ppTime': 1566232635, 'instId': 0, 'digest': 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 'poolStateRootHash': 'BizBrS1S9x4jQcxiPiSRC3gaVoW6MsByWrZ5Ywk2VBPX', 'viewNo': 19, 'stateRootHash': 'BJMXmahaPEnF1skqqnqFcShWCci2ebf9r3q9q4xDCtvK', 'txnRootHash': 'DRkNKUmQK5ZxyLkx7tGaKuYaQzJZtZquWmeFXXfAtUuq', 'discarded': '[]', 'final': True, 'sub_seq_no': 0, 'reqIdr': (), 'ppSeqNo': 5529, 'auditTxnRootHash': 'Qziz6HDW7mLH8ci2ZhQqFYG6P1KZzxNpZhqEFMKtg5G', 'ledgerId': 2} from danube:0

Here is a list of recent nodes added, when they were added, and issues encountered:

OG Node - May 10 - no issues

fetch-AI - May 14 - Connectivity issues (resolved and possibly unrelated to this issue)

certisign - May 24 - no issues

mitrecorp - June 24 - Connectivity, consensus, and uptime issues, logs not yet attached

cpqd - Aug 8 - PREPREPARE issue

blockscale - Aug 14 - PREPREPARE issue

spaceman - Aug 15 - PREPREPARE issue

 

Recent upgrades to BuilderNet and the indy-node version:

May 6 - 1.7.1

June 3 - 1.8.0 (followed by 1.8.1 a week later)

July 8 - 1.9.0

Aug 5 - 1.9.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/19 12:50 AM;lbendixsen;FoundationBuilder-indy2211-state.tar.bz2;https://jira.hyperledger.org/secure/attachment/17714/FoundationBuilder-indy2211-state.tar.bz2","23/Aug/19 12:50 AM;lbendixsen;FoundationBuilder-indy2211-txns.tar.bz2;https://jira.hyperledger.org/secure/attachment/17715/FoundationBuilder-indy2211-txns.tar.bz2","20/Aug/19 8:04 AM;lbendixsen;FoundationBuilder.log.13.xz;https://jira.hyperledger.org/secure/attachment/17696/FoundationBuilder.log.13.xz","20/Aug/19 7:56 AM;lbendixsen;blockscalesolutions.log.bz2;https://jira.hyperledger.org/secure/attachment/17695/blockscalesolutions.log.bz2","20/Aug/19 7:56 AM;lbendixsen;cpqd.log.bz2;https://jira.hyperledger.org/secure/attachment/17694/cpqd.log.bz2","24/Aug/19 3:33 AM;anikitinDSR;remove_config_state.py;https://jira.hyperledger.org/secure/attachment/17729/remove_config_state.py","20/Aug/19 7:57 AM;lbendixsen;spacemannode_081919.log.bz2;https://jira.hyperledger.org/secure/attachment/17693/spacemannode_081919.log.bz2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v986144",,,,Unset,Unset,Ev-Node 19.17,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,esplinr,lbendixsen,mgbailey,sergey.khoroshavin,Toktar,VladimirWork,,,,"21/Aug/19 11:52 PM;sergey.khoroshavin;[~lbendixsen]
Changed priority of this issue to Highest since initial analysis of logs showed that ""bad"" nodes ended up with corrupted state. Unfortunatelly it is not apparent from logs why this happened, so the following is needed:
* output of validator info from good and bad nodes (most interesting info is packages versions)
* output of journalctl from good and bad nodes from same period as logs
* archives with full contents of /var/lib/indy/<network name>/data from one good and one bad node

Also it makes sense to do the following experiment on ONE of bad nodes:
* stop indy-node service
* delete following directories located in /var/lib/indy/<network name>/data:
** config_state
** domain_state
** pool_state
** sovtoken_state
* start indy-node service, this should take some time because node will be restoring deleted state databases
* see whether node managed to enter into consensus or not
;;;","22/Aug/19 2:00 AM;sergey.khoroshavin;It seems like issue happened due to accidental change in state application of AUTH_RULE transaction between 1.9.0 and 1.9.1 versions. This means that if AUTH_RULEs were written to network running indy-node 1.9.0 then after upgrade to 1.9.1 newly added nodes will fall out of consensus as soon as they get requests that require writes to config ledger. Most probably proper solution is to write *migration that recreates state databases and patches audit ledger contents*. Mechanism of failure is following:
* suppose we have pool running 1.9.0
* writing AUTH_RULE appends it to config ledger and applies some changeset X to config state
* also config state root hash H is added to audit ledger
* after upgrading pool to 1.9.1 neither config state nor audit ledger is changed
* if we add new node it will start catching up ledgers
* so it catches up that AUTH_RULE transaction, appends it to config ledger and applies changeset X' to config state
* however audit ledger is caught up as is, so it is same as on all other pool
* X' is different from X because application of AUTH_RULE to state is different in 1.9.0 and 1.9.1
* no errors show up because after catch up only ledger root hashes are checked, not state root hashes
* as soon as write request to config ledger is issued it will try to apply new changeset Y to config state and communicate it between nodes, however new state root hashes will be different on old and new nodes since result of application (X, Y) is different from (X', Y), and new node falls out of consensus

It is possible to resort to simple solution and just recreate state databases on all old nodes, however this will leave audit ledger unchanged and external audit will show that new state doesn't contain some of state root hashes present in audit ledger.

Also I need to note that only newly added nodes are affected by this problem, old nodes will remain in consensus.;;;","22/Aug/19 5:43 AM;esplinr;Thank you [~sergey.khoroshavin] for looking into this. We need to preserve the ability to audit ledger history, so it sounds like a migration of the audit ledger back to a consistent state is necessary. The August release can be delayed to fix this if necessary.;;;","22/Aug/19 4:06 PM;ashcherbakov;[~lbendixsen] [~sergey.khoroshavin] [~esplinr]
 The next steps and fixes depend on the content on config ledgers on Builder, Staging and Main Nets, and, in particular, the time when AUTH_RULE transactions have been written.
 So, we still need the ledgers and states from at least one node.

I suggest to do the following now:
 # Do not write any AUTH_RULE transactions to any nets until the fix is applied
 # Do not add any Nodes to any nets until the fox is applied
 # For the Nets with no AUTH_RULE txns written after 1.9.1:
 ** Do a fix to not write OFF_LEDGER_SIGNATURE  if it's not explicitly set
 ** Issue a hotfix release
 ** Clean up all ledgers and state on Builder Nodes added after 1.9.1 release and stop the nodes (or demote to have enough nodes for consensus).
 ** Apply the hotfix to all Nets ASAP
 ** This will prevent the issue from possible happening on Staging and Main Nets, as well as fix the nodes on Builder Nets (they will catchup properly after update)
 # For the Nets with AUTH_RULE txns written after 1.9.1 but where the audit history is not so critical (Builder Net?)
 ** Do the same as in Step 3
 ** Cleanup config state on all Nodes in the Net
 # For the Nets with AUTH_RULE txns written after 1.9.1 but where the audit history is  critical (Staging and Main Net?)
 ** Do the same as in Step 3
 ** Create a migration script:
 *** Cleanup the config state
 *** Cleanup config ledger merklee tree (it will be automatically recreated from txn log after restart)
 *** Go through the audit ledger; on every config ledger batch apply the corresponding number of config ledger transactions to the config state; modify config state in audit transaction is it's not equal to the existing one.
 *** Modify audit ledger to point to correct config ledger state
 ** The upgrade needs to be forced for these nets to apply the migration script

Please note that if we clear config state, then already added state proofs will not work for config transactions (for example TAA).;;;","22/Aug/19 5:39 PM;ashcherbakov;The issue may also affect our next release. See the options in https://jira.hyperledger.org/browse/INDY-2204;;;","22/Aug/19 5:54 PM;sergey.khoroshavin;[~lbendixsen]
I'd like to second Alex request to send contents of ledgers from one of correct nodes from each of Builder, Staging and Main Net. This data is located in */var/lib/indy/<network name>/data*;;;","22/Aug/19 11:52 PM;mgbailey;[~sergey.khoroshavin] Do you need MainNet ledger data, if no new nodes have been added to it?;;;","23/Aug/19 12:59 AM;lbendixsen;[~sergey.khoroshavin] [~ashcherbakov]

I have added states and transactions for the FoundationBuilder Node from the BuilderNet.  The StagingNet's files are huge for this so it will take me a bit longer to get those tarred and uploaded with the current 10M limit.  It would help to know which individual files are needed, or if the whole _state and _transactions directories are required for this request.;;;","23/Aug/19 5:00 PM;Toktar;*Problem reason:*
 - OFF_LEDGER_SIGNATURE always writes to a state

*Changes:*
 - Add a fix to not write OFF_LEDGER_SIGNATURE  if it's not explicitly set
 - Add tests

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1418]

*Version:*
 * indy-node 1.9.2.dev1061 -master

*Risk factors:*
 - Problems with an inconsistency of states

*Risk:*
 - Medium

*Tests:*
 * test_auth_constraint_without_off_ledger_sig_from_dct_succesfull
 * test_auth_rule_state_format;;;","23/Aug/19 9:13 PM;ashcherbakov;[~esplinr] [~lbendixsen] [~mgbailey]
 Let me summarize our decisions:
 * Fix:
 ** See above
 ** Will be included into next Release
 * Release:
 ** Do a common release with this fix included according to regular schedule
 * Migration script:
 ** Do not do a migration script
 * How to recover consensus:
 ## The best way to recover and avoid such issue for newly added nodes:
 *** wait for the update
 *** resetting (clean up) config state on ALL Nodes (including the ones in consensus)
 *** Please note that the audit legder still be inconsistent in terms of history, so an audit script (for a config ledger and state) will fail on Builder Net. The only correct way to avoid this would be real migration touching audit ledger.
 ## If there are no config transactions will be written, then a simple restart of the nodes experiencing the issue will help.
 It still means that Item 1 needs to be done later (upgrade + state cleanup)
 ## If config txns are planned to be written before the upgrade, then consensus can be recovered by manual copy of config state from a valid (in consensus) node.
 It still means that Item 1 needs to be done later (upgrade + state cleanup);;;","23/Aug/19 11:36 PM;VladimirWork;Verified against indy-node setup.;;;","24/Aug/19 3:34 AM;anikitinDSR;[^remove_config_state.py] was created for fixing problem.
[~VladimirWork], please check that, this script working correctly for test pool.;;;","26/Aug/19 7:04 PM;VladimirWork;Verified against sovrin setup (but upgraded using `apt install` because of INDY-2216).;;;","28/Aug/19 12:40 AM;VladimirWork;Verified against indy-node and sovrin setups with config migration script.

https://github.com/VladimirWork/indy-test-automation/blob/acb03d2da0e16ae81e30e964736232cec41c129c/system/draft/test_misc.py#L1529;;;",,,,,,,,
ZMQError: Address already in use when restarting client stack,INDY-2212,41916,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Aug/19 10:33 PM,23/Sep/19 10:41 PM,28/Oct/23 2:47 AM,03/Sep/19 12:17 AM,,,1.10.0,,,,,0,,,,,"*The issue:*

`ZMQError: Address already in use when restarting client stack` error (and hence service crashes and restarts) when restarting client stack due to max limit of client connecitons

*Expected behaviour:*

No crashes

*Logs:* {color:#1d1c1d}ev@evernymr33:logs/20_08_2019_broken_pool.tar.gz{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98632p4",,,,Unset,Unset,Ev-Node 19.17,Ev-Node 19.18,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"20/Aug/19 10:38 PM;ashcherbakov;*PoA:*
 # Try to increase the timeout when binding a listener socket from 10 secs to 120 secs 
 # Consider pyzmq update to the latest version (we use 17.0.0 but the latest version is 18.1.0)

I think ZMQ needs to be updated because of the following reasons:
 * [https://github.com/zeromq/libzmq/releases/tag/v4.2.4:] fix race condition with ZMQ_LINGER socket option (looks similar to the issues we see)
 * [https://github.com/zeromq/libzmq/releases/tag/v4.3.2:] a remote, unauthenticated client connecting to a
libzmq application, running with a socket listening with CURVE
encryption/authentication enabled, may cause a stack overflow and
overwrite the stack with arbitrary data, due to a buffer overflow in
the library. Users running public servers with the above configuration
are highly encouraged to upgrade as soon as possible, as there are no
known mitigations. All versions from 4.0.0 and upwards are affected.
Thank you Fang-Pen Lin for finding the issue and reporting it!;;;","22/Aug/19 6:05 PM;ashcherbakov;It looks like bumping ZMQ is not a trivial task since with the bum some tests started to fail. INDY-2213 is created for this.;;;","22/Aug/19 6:08 PM;ashcherbakov;*Changes:*
 * Iincreased the timeout when binding a listener socket from 10 secs to 120 secs

*PR:* 
[https://github.com/hyperledger/indy-plenum/pull/1302]

*Risk:* Low

*Recommendation for QA:*
 * Run load with a lot of client requests so that client stack is restarted quite often. Make sure there are no errors.
 * Run similar tests in Docker (scripts/client_connections/just_connect_N_times.py can be used).;;;","03/Sep/19 12:17 AM;VladimirWork;Build Info:
1.10.0~dev1070

Steps to Validate:
1. Run reading load test with 1000+ simultaneous connections against 25 nodes AWS pool.
2. Run reading load test with ~400 simultaneous connections against 7 nodes docker pool.

Actual Results:
There are no `ZMQError: Address already in use when restarting client stack` errors in logs. Client stack reset works as expected.;;;",,,,,,,,,,,,,,,,,,
Bump pyzmq to the latest version,INDY-2213,41965,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,22/Aug/19 6:04 PM,30/Sep/19 9:30 PM,28/Oct/23 2:47 AM,28/Sep/19 12:38 AM,,,1.10.0,,,,,0,,,,,"pyzmq needs to be updated to the latest version (we use 17.0.0 but the latest version is 18.1.0)

In particular, ZMQ needs to be updated because of the following reasons:
 * [https://github.com/zeromq/libzmq/releases/tag/v4.2.4:] fix race condition with ZMQ_LINGER socket option (looks similar to the issues we see)
 * [https://github.com/zeromq/libzmq/releases/tag/v4.3.2:] a remote, unauthenticated client connecting to a
 libzmq application, running with a socket listening with CURVE
 encryption/authentication enabled, may cause a stack overflow and
 overwrite the stack with arbitrary data, due to a buffer overflow in
 the library. Users running public servers with the above configuration
 are highly encouraged to upgrade as soon as possible, as there are no
 known mitigations. All versions from 4.0.0 and upwards are affected.
 Thank you Fang-Pen Lin for finding the issue and reporting it!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vf",,,,Unset,Unset,Ev-Node 19.19,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,donqui,VladimirWork,,,,,,,,,"22/Sep/19 1:37 AM;donqui;h2. *New version of pyzmq is not working, this is a troubleshooting report on why that is.*
h3.  
h3. *Initial Analysis:*

System version of libzmq --> 4.1.4-7

Bundled versions:
 - 17.0.0 --> 4.1.6
 - 18.1.0 --> 4.3.2

We are always using the bundled version, the system version does not seem to be used:
 - I deleted the systemlib and everything works
 - I installed the system version and the version reported by zmq does not match
 - *lsof* shows that the local lib is always loaded regardless if we have a system one installed or not

It seems that there is a difference with 18.1.0 when it is installed with:
 - pip install pyzmq==18.1.0 --install-option=""--zmq=bundled"" --no-cache

With the before mentioned way of installing we do not have any degradation.

*NOTE*: Tests are intermittently failing even with the old version. Whatever problem we are experiencing with
 the new version is just exacerbating the problem so we are seeing it more frequently.
h3. *In depth analysis with a sample program*

 

*17.0.0: pip install pyzmq==17.0.0:*
{code:java}
└─ $ ▶ lsof -p 30133 | grep -E 'libzmq|sodium'
python 30133 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/libzmq-0576c57a.so.5.0.2

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs
└─ $ ▶ ldd libzmq-0576c57a.so.5.0.2
 linux-vdso.so.1 => (0x00007ffccc199000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f71e4cc0000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f71e4aa3000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f71e4721000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f71e4418000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f71e404e000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f71e3e38000)
 /lib64/ld-linux-x86-64.so.2 (0x00007f71e5136000){code}
 

*18.1.0: pip install pyzmq==18.1.0*
{code:java}
└─ $ ▶ lsof -p 30216 | grep -E 'libzmq|sodium'
python 30216 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/libsodium-bcf9f097.so.23.3.0
python 30216 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/libzmq-1358af2c.so.5.2.2

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs
└─ $ ▶ ldd libsodium-bcf9f097.so.23.3.0
 linux-vdso.so.1 => (0x00007ffd00fe6000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fa913f22000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fa913b58000)
 /lib64/ld-linux-x86-64.so.2 (0x00007fa914464000)

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs
└─ $ ▶ ldd libzmq-1358af2c.so.5.2.2
 linux-vdso.so.1 => (0x00007ffd589ec000)
 libsodium-bcf9f097.so.23.3.0 => /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/././libsodium-bcf9f097.so.23.3.0 (0x00007f365c5bf000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f365c3b7000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f365c19a000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f365be18000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f365bb0f000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f365b745000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f365b52f000)
 /lib64/ld-linux-x86-64.so.2 (0x00007f365cbbc000){code}
 

*17.0.0: pip install pyzmq==17.0.0 --install-option=""--zmq=bundled"" --no-cache*
{code:java}
└─ $ ▶ lsof -p 30622 | grep -E 'sodium|libzmq'
python 30622 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/libzmq.cpython-35m-x86_64-linux-gnu.so

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq
└─ $ ▶ ldd libzmq.cpython-35m-x86_64-linux-gnu.so
 linux-vdso.so.1 => (0x00007fff15efd000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f6caa53e000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f6caa1bc000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f6ca9fa6000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f6ca9d89000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f6ca99bf000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f6ca96b6000)
 /lib64/ld-linux-x86-64.so.2 (0x00007f6caa9ca000)
{code}
 

*18.1.0: pip install pyzmq==18.1.0 --install-option=""--zmq=bundled"" --no-cache*
{code:java}
└─ $ ▶ lsof -p 31117 | grep -E 'sodium|libzmq'
python 31117 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/libzmq.cpython-35m-x86_64-linux-gnu.so

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq
└─ $ ▶ ldd libzmq.cpython-35m-x86_64-linux-gnu.so
 linux-vdso.so.1 => (0x00007ffe643af000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fba0def4000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fba0db72000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fba0d869000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fba0d653000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fba0d436000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fba0d06c000)
 /lib64/ld-linux-x86-64.so.2 (0x00007fba0e3bb000)
{code}
h3. *In depth Unit Test Analysis*

 

*Tests are passing with 17.0.0:*
{code:java}
└─ $ ▶ lsof -p 31353 | grep -iE 'libzmq|sodium'
python 31353 /usr/lib/x86_64-linux-gnu/libsodium.so.18.0.1
python 31353 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/libzmq-0576c57a.so.5.0.2

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs
└─ $ ▶ ldd libzmq-0576c57a.so.5.0.2
 linux-vdso.so.1 => (0x00007ffdb5129000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007ff918839000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff91861c000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff91829a000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff917f91000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff917bc7000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff9179b1000)
 /lib64/ld-linux-x86-64.so.2 (0x00007ff918caf000)
{code}
 

*Tests are NOT passing with 18.1.0:*
{code:java}
└─ $ ▶ lsof -p 31445 | grep -iE 'libzmq|sodium'
python 31445 /usr/lib/x86_64-linux-gnu/libsodium.so.18.0.1
python 31445 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/libsodium-bcf9f097.so.23.3.0
python 31445 /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/libzmq-1358af2c.so.5.2.2

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs
└─ $ ▶ ldd libzmq-1358af2c.so.5.2.2
 linux-vdso.so.1 => (0x00007ffc07be3000)
 libsodium-bcf9f097.so.23.3.0 => /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/.libs/././libsodium-bcf9f097.so.23.3.0 (0x00007fc327e4a000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007fc327c42000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007fc327a25000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007fc3276a3000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007fc32739a000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007fc326fd0000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007fc326dba000)
 /lib64/ld-linux-x86-64.so.2 (0x00007fc328447000)
{code}
 

*Tests are passing with 18.1.0 installed with --install-option=""–zmq=bundled"":*
{code:java}
└─ $ ▶ lsof -p 31968 | grep -iE 'libzmq|sodium'
python /usr/lib/x86_64-linux-gnu/libsodium.so.18.0.1
python /home/nemanja/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq/libzmq.cpython-35m-x86_64-linux-gnu.so

(indy-node) nemanja @ dev-machine ~/.virtualenvs/indy-node/lib/python3.5/site-packages/zmq
└─ $ ▶ ldd libzmq.cpython-35m-x86_64-linux-gnu.so
 linux-vdso.so.1 => (0x00007ffd68cfd000)
 librt.so.1 => /lib/x86_64-linux-gnu/librt.so.1 (0x00007f03c8852000)
 libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007f03c84d0000)
 libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007f03c81c7000)
 libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007f03c7fb1000)
 libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f03c7d94000)
 libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f03c79ca000)
 /lib64/ld-linux-x86-64.so.2 (0x00007f03c8d19000)
{code};;;","24/Sep/19 3:30 AM;donqui; 
{code:java}
Success:

zstack.py                  385 ERROR    Alpha Event: {'value': 29, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 8, 'description': 'EVENT_LISTENING'}
zstack.py                  650 ERROR    Connecting to: Beta
zstack.py                  669 ERROR    Connected...
zstack.py                  678 ERROR    Sending Ping Pong...
zstack.py                  682 ERROR    Ping Pong Sent
zstack.py                  650 ERROR    Connecting to: Alpha
zstack.py                  669 ERROR    Connected...
zstack.py                  678 ERROR    Sending Ping Pong...
zstack.py                  682 ERROR    Ping Pong Sent
zstack.py                  385 ERROR    Alpha Event: {'value': 46, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 32, 'description': 'EVENT_ACCEPTED'}
zstack.py                  385 ERROR    Alpha Event: {'value': 0, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED'}
zstack.py                  385 ERROR    Beta Event: {'value': 38, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 8, 'description': 'EVENT_LISTENING'}
zstack.py                  385 ERROR    Beta Event: {'value': 42, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 32, 'description': 'EVENT_ACCEPTED'}
zstack.py                  385 ERROR    Beta Event: {'value': 0, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED'}
zstack.py                  385 ERROR    Beta Event: {'value': 42, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 512, 'description': 'EVENT_DISCONNECTED'}
zstack.py                  385 ERROR    Beta Event: {'value': 41, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 32, 'description': 'EVENT_ACCEPTED'}
zstack.py                  385 ERROR    Beta Event: {'value': 0, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED'}
zstack.py                  385 ERROR    Beta Event: {'value': 41, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 512, 'description': 'EVENT_DISCONNECTED'}
zstack.py                  385 ERROR    Beta Event: {'value': 41, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 32, 'description': 'EVENT_ACCEPTED'}
zstack.py                  385 ERROR    Beta Event: {'value': 0, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED'}
zstack.py                  385 ERROR    Alpha Event: {'value': 46, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 512, 'description': 'EVENT_DISCONNECTED'}
zstack.py                  385 ERROR    Alpha Event: {'value': 42, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 32, 'description': 'EVENT_ACCEPTED'}
zstack.py                  385 ERROR    Alpha Event: {'value': 0, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED'}
zstack.py                  385 ERROR    Beta Event: {'value': 38, 'endpoint': b'tcp://127.0.0.1:6112', 'event': 128, 'description': 'EVENT_CLOSED'}
zstack.py                  385 ERROR    Alpha Event: {'value': 29, 'endpoint': b'tcp://127.0.0.1:6111', 'event': 128, 'description': 'EVENT_CLOSED'}
zstack.py                  385 ERROR    Beta Event: {'value': 0, 'endpoint': b'', 'event': 1024, 'description': 'EVENT_MONITOR_STOPPED'}
zstack.py                  385 ERROR    Alpha Event: {'value': 0, 'endpoint': b'', 'event': 1024, 'description': 'EVENT_MONITOR_STOPPED'}
{code}
{code:java}
Failures

zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 8, 'description': 'EVENT_LISTENING', 'value': 29}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6114', 'event': 8, 'description': 'EVENT_LISTENING', 'value': 38}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6114', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 42}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6114', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6114', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 47}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6114', 'event': 512, 'description': 'EVENT_DISCONNECTED', 'value': 42}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6114', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 42}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 512, 'description': 'EVENT_DISCONNECTED', 'value': 42}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 512, 'description': 'EVENT_DISCONNECTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 512, 'description': 'EVENT_DISCONNECTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 512, 'description': 'EVENT_DISCONNECTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 32, 'description': 'EVENT_ACCEPTED', 'value': 40}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6113', 'event': 4096, 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0}
eventually.py              190 ERROR    checkStacksConnected failed; not trying any more because 17 seconds have passed; args were ([Alpha, Beta],)
looper.py                  253 ERROR    Error while running coroutine eventually: AssertionError()


zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_LISTENING', 'value': 29, 'event': 8}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6116', 'description': 'EVENT_LISTENING', 'value': 38, 'event': 8}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6116', 'description': 'EVENT_ACCEPTED', 'value': 42, 'event': 32}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6116', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_ACCEPTED', 'value': 46, 'event': 32}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6116', 'description': 'EVENT_DISCONNECTED', 'value': 42, 'event': 512}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6116', 'description': 'EVENT_ACCEPTED', 'value': 40, 'event': 32}
zstack.py                  385 ERROR    Beta Event: {'endpoint': b'tcp://127.0.0.1:6116', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_DISCONNECTED', 'value': 46, 'event': 512}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_ACCEPTED', 'value': 42, 'event': 32}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_DISCONNECTED', 'value': 42, 'event': 512}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_ACCEPTED', 'value': 46, 'event': 32}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_DISCONNECTED', 'value': 46, 'event': 512}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_ACCEPTED', 'value': 54, 'event': 32}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_DISCONNECTED', 'value': 54, 'event': 512}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_ACCEPTED', 'value': 46, 'event': 32}
zstack.py                  385 ERROR    Alpha Event: {'endpoint': b'tcp://127.0.0.1:6115', 'description': 'EVENT_HANDSHAKE_SUCCEEDED', 'value': 0, 'event': 4096}
eventually.py              190 ERROR    checkStacksConnected failed; not trying any more because 17 seconds have passed; args were ([Alpha, Beta],)
looper.py                  253 ERROR    Error while running coroutine eventually: AssertionError(){code}
 ;;;","27/Sep/19 2:12 AM;donqui;Problem reason/description: 
- security vulnerabiliities
- ZMQ_LINGER bug
- other bugs and perf improvements

Changes: 
- pyzmq updated
- changes to install and build scripts

PR:
- https://github.com/hyperledger/indy-plenum/pull/1348
- https://github.com/hyperledger/indy-plenum/pull/1350
- https://github.com/hyperledger/indy-plenum/pull/1351

Version:
- plenum: 1.10.0.dev911
- node:   1.10.0.dev1091
- token: sovtoken_1.0.3~dev98 sovtokenfees_1.0.3~dev98

Risk factors:
- zmq not working correctly 
- current intermittent zmq problems might be exacerbated 

Risk:
- Med

Covered with tests:
- Test ref on github

Recommendations for QA
- system tests
- load tests
- test that cause INDY-2222;;;","28/Sep/19 12:37 AM;VladimirWork;Build Info:
indy-node 1.10.0~dev1091
plugins 1.0.3~dev98

Steps to Validate:
1. Run production load test and check results.
2. Run all catchup system tests and check results.
3. Check nightly CD pipeline results for this build.

Actual Results:
Production load test ran over 12 hours sucessfully, there was an issue with 6 VCs after 2 primary stoppings but it will be investigated separately.
All system tests pased except catchup ones so new pyzmq didn't fix INDY-2222 issue.;;;",,,,,,,,,,,,,,,,,,
Repeat: Prove production stability of an Indy network,INDY-2214,41992,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,VladimirWork,esplinr,esplinr,23/Aug/19 9:59 AM,26/Dec/19 10:29 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"We have made a lot of changes to Indy Node, Indy SDK, and Plugins. We need to repeat the tests from last year and measure improvements.

*Acceptance Criteria*
 Perform a test of an Indy network that has the following attributes:
 * The ledger is pre-loaded with 1 million transactions
 * Pool size at least matches the number of network nodes initially expected in the Sovrin network (currently 25 nodes).
 * 1K concurrent clients
 * Over a 3 hour period induce a sustained throughput of *15* write transactions per second and 100 read transactions per second on average.
 * Write load is a mixture of:
 ** writing credentials schema (5%),
 ** writing credential definition (5%)
 ** revoke registry definition (5%)
 ** revoke registry update (5%)
 ** write DID to ledger (20%)
 ** write payment to ledger (45%)
 ** write attrib to ledger (15%)
 * Read load is a mixture of:
 ** read DID from ledger (45%)
 ** read credential schema (10%)
 ** read credential definition (10%)
 ** read revoke registry definition (10%)
 ** read revoke registry delta (10%)
 ** read attrib from ledger (10%)
 ** read payment balance from ledger (5%)
 * Write response time should be less that 5 seconds (would also like a report of the average).
 * Read response time should be less than 1 second (would also like a report of the average).
* Fees are enabled using the Sovrin Token plugins.

Near the end of the test, evaluate whether the ledger is keeping pace such that it could likely sustain the load indefinitely, or whether it is falling behind and will need a period to complete processing the queued load.

Any problems found will be logged in JIRA as separate issues for independent prioritization.",,,,,,,,,,INDY-1355,INDY-1356,INDY-1357,INDY-1358,,,,,INDY-1343,,,INDY-1448,,,INDY-1448,INDY-1460,INDY-1477,INDY-1478,INDY-1483,INDY-1607,INDY-1717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2250,,,,,,,,,"1|hzwvif:00001yw969w4c91x",,,,,,,,,,,8.0,,,,,,,,,,,,esplinr,krw910,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One node doesn't catch up,INDY-2215,42000,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,23/Aug/19 4:05 PM,29/Aug/19 12:10 AM,28/Oct/23 2:47 AM,29/Aug/19 12:10 AM,,,1.9.2,,,,,0,,,,,"Build Info:
indy-node 1.9.2~dev1061

Steps to Reproduce:
https://github.com/hyperledger/indy-test-automation/blob/931ad5f2ac070f325bf3708d5cc48bfb158e643a/system/indy-node-tests/TestAuditSuite.py#L13

Actual Results:
6th node doesn't catch up after all restarts in 200 seconds.

Expected Results:
All nodes should catch up at the end of test.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/19 4:05 PM;VladimirWork;node1.2019-08-22T232648.tgz;https://jira.hyperledger.org/secure/attachment/17720/node1.2019-08-22T232648.tgz","23/Aug/19 4:05 PM;VladimirWork;node2.2019-08-22T232649.tgz;https://jira.hyperledger.org/secure/attachment/17721/node2.2019-08-22T232649.tgz","23/Aug/19 4:05 PM;VladimirWork;node3.2019-08-22T232649.tgz;https://jira.hyperledger.org/secure/attachment/17722/node3.2019-08-22T232649.tgz","23/Aug/19 4:05 PM;VladimirWork;node4.2019-08-22T232649.tgz;https://jira.hyperledger.org/secure/attachment/17723/node4.2019-08-22T232649.tgz","23/Aug/19 4:05 PM;VladimirWork;node5.2019-08-22T232650.tgz;https://jira.hyperledger.org/secure/attachment/17724/node5.2019-08-22T232650.tgz","23/Aug/19 4:05 PM;VladimirWork;node6.2019-08-22T232650.tgz;https://jira.hyperledger.org/secure/attachment/17726/node6.2019-08-22T232650.tgz","23/Aug/19 4:05 PM;VladimirWork;node7.2019-08-22T232651.tgz;https://jira.hyperledger.org/secure/attachment/17725/node7.2019-08-22T232651.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98614k",,,,Unset,Unset,Ev-Node 19.17,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,"26/Aug/19 3:53 PM;Toktar;*PoA:*

For the initial catchup the method ConsProofService.start() is called with request_ledger_statuses=False. It means that LedgerStatuses will not be re-asked in case of their loss.

+ToDo:+
 * re-ask  LedgerStatuses for all catchups
 * update tests

PR: [https://github.com/hyperledger/indy-plenum/pull/1305];;;","26/Aug/19 6:46 PM;Toktar;*Problem reason:*
 - LedgerStatuses are not re-asked for the initial catchup 

*Changes:*
 - re-ask  LedgerStatuses for all catchups
 - update tests

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1305]
 * [https://github.com/hyperledger/indy-node/pull/1423]

*Version:*
 * indy-node 1.9.2.dev1066 -master
 * indy-plenum 1.9.2.dev874 -master

*Risk factors:*
 - Problems with a catchup

*Risk:*
 - Low

*Tests:*
 * [test_node_catchup_with_connection_problem.py|https://github.com/hyperledger/indy-plenum/pull/1305/files#diff-8bc091580aee91c53dee265fc1fa708d] 

*Recommendations for QA:*
 * Start a pool with 7 nodes in a docker.
 * Stop the Node7.
 * Order any transaction.
 * Block all input messages for the Node7 (except for pings).
 * Start Node 7.
 * Wait a few minutes and unblock input messages for the Node7.
 * Check that Node7 successfully catch-upped;;;","29/Aug/19 12:10 AM;VladimirWork;Build Info:
indy-node 1.9.2~dev1067
plugins 1.0.2~dev81

Steps to Validate:
New test was implemented and run: https://github.com/VladimirWork/indy-test-automation/blob/bc266b089bf10838b2e81c29a6740ab16b39703c/system/draft/test_misc.py#L1809
TestAuditSuite (that found the issue) and TestCatchUpSuite also pass.

Actual Results:
Catchup works as expected.;;;",,,,,,,,,,,,,,,,,,,
Sovrin master package upgrade is broken,INDY-2216,42022,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,26/Aug/19 5:49 PM,10/Dec/19 7:56 PM,28/Oct/23 2:47 AM,10/Dec/19 6:33 PM,,,,,,,,0,,,,,"Build Info:
sovrin 1.1.135
indy-node 1.9.0~dev1014
plugins 1.0.0~dev59

Steps to Reproduce:
1. Try to upgrade pool to sovrin 1.1.136 using `pool upgrade`.

Actual Results:
{noformat}
Aug 26 07:33:25 fc7eefacd976 env[84]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Aug 26 07:33:25 fc7eefacd976 env[84]: Get:1 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]
Aug 26 07:33:25 fc7eefacd976 env[84]: Get:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [109 kB]
Aug 26 07:33:25 fc7eefacd976 env[84]: Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Aug 26 07:33:25 fc7eefacd976 env[84]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Aug 26 07:33:25 fc7eefacd976 env[84]: Get:5 http://archive.ubuntu.com/ubuntu xenial/universe Sources [9802 kB]
Aug 26 07:33:26 fc7eefacd976 env[84]: Get:6 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [136 kB]
Aug 26 07:33:26 fc7eefacd976 env[84]: Get:7 https://repo.sovrin.org/deb xenial InRelease [28.4 kB]
Aug 26 07:33:26 fc7eefacd976 env[84]: Get:8 https://repo.sovrin.org/deb xenial/master amd64 Packages [263 kB]
Aug 26 07:33:26 fc7eefacd976 env[84]: Get:9 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [924 kB]
Aug 26 07:33:29 fc7eefacd976 env[84]: Get:10 http://security.ubuntu.com/ubuntu xenial-security/restricted amd64 Packages [12.7 kB]
Aug 26 07:33:29 fc7eefacd976 env[84]: Get:11 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [579 kB]
Aug 26 07:33:31 fc7eefacd976 env[84]: Get:12 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [6119 B]
Aug 26 07:33:40 fc7eefacd976 env[84]: Get:13 http://archive.ubuntu.com/ubuntu xenial/main amd64 Packages [1558 kB]
Aug 26 07:33:43 fc7eefacd976 env[84]: Get:14 http://archive.ubuntu.com/ubuntu xenial/restricted amd64 Packages [14.1 kB]
Aug 26 07:33:43 fc7eefacd976 env[84]: Get:15 http://archive.ubuntu.com/ubuntu xenial/universe amd64 Packages [9827 kB]
Aug 26 07:33:55 fc7eefacd976 env[84]: Get:16 http://archive.ubuntu.com/ubuntu xenial/multiverse amd64 Packages [176 kB]
Aug 26 07:33:55 fc7eefacd976 env[84]: Get:17 http://archive.ubuntu.com/ubuntu xenial-updates/universe Sources [328 kB]
Aug 26 07:33:56 fc7eefacd976 env[84]: Get:18 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 Packages [1304 kB]
Aug 26 07:33:58 fc7eefacd976 env[84]: Get:19 http://archive.ubuntu.com/ubuntu xenial-updates/restricted amd64 Packages [13.1 kB]
Aug 26 07:33:58 fc7eefacd976 env[84]: Get:20 http://archive.ubuntu.com/ubuntu xenial-updates/universe amd64 Packages [983 kB]
Aug 26 07:33:59 fc7eefacd976 env[84]: Get:21 http://archive.ubuntu.com/ubuntu xenial-updates/multiverse amd64 Packages [19.1 kB]
Aug 26 07:33:59 fc7eefacd976 env[84]: Get:22 http://archive.ubuntu.com/ubuntu xenial-backports/main amd64 Packages [7942 B]
Aug 26 07:33:59 fc7eefacd976 env[84]: Get:23 http://archive.ubuntu.com/ubuntu xenial-backports/universe amd64 Packages [8807 B]
Aug 26 07:33:59 fc7eefacd976 env[84]: Fetched 26.6 MB in 34s (777 kB/s)
Aug 26 07:34:00 fc7eefacd976 env[84]: Reading package lists...
Aug 26 07:34:00 fc7eefacd976 env[84]: Building dependency tree...
Aug 26 07:34:00 fc7eefacd976 env[84]: Reading state information...
Aug 26 07:34:00 fc7eefacd976 env[84]: 35 packages can be upgraded. Run 'apt list --upgradable' to see them.
Aug 26 07:34:02 fc7eefacd976 env[84]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Aug 26 07:34:02 fc7eefacd976 env[84]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Aug 26 07:34:02 fc7eefacd976 env[84]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Aug 26 07:34:02 fc7eefacd976 env[84]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Aug 26 07:34:02 fc7eefacd976 env[84]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Aug 26 07:34:03 fc7eefacd976 env[84]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Aug 26 07:34:05 fc7eefacd976 env[84]: Reading package lists...
Aug 26 07:34:05 fc7eefacd976 env[84]: Building dependency tree...
Aug 26 07:34:05 fc7eefacd976 env[84]: Reading state information...
Aug 26 07:34:05 fc7eefacd976 env[84]: 35 packages can be upgraded. Run 'apt list --upgradable' to see them.
Aug 26 07:34:10 fc7eefacd976 env[84]: /bin/sh: 1: Syntax error: ""("" unexpected
Aug 26 07:34:10 fc7eefacd976 env[84]: E: No packages found
Aug 26 07:34:10 fc7eefacd976 env[84]: E: No packages found
Aug 26 07:34:10 fc7eefacd976 env[84]: E: No packages found
Aug 26 07:34:11 fc7eefacd976 env[84]: + deps='indy-plenum=1.9.2~dev871 indy-node sovrin=1.1.136'
Aug 26 07:34:11 fc7eefacd976 env[84]: + '[' -z 'indy-plenum=1.9.2~dev871 indy-node sovrin=1.1.136' ']'
Aug 26 07:34:11 fc7eefacd976 env[84]: + echo 'Try to donwload indy version indy-plenum=1.9.2~dev871 indy-node sovrin=1.1.136'
Aug 26 07:34:11 fc7eefacd976 env[84]: Try to donwload indy version indy-plenum=1.9.2~dev871 indy-node sovrin=1.1.136
Aug 26 07:34:11 fc7eefacd976 env[84]: + apt-get -y update
Aug 26 07:34:11 fc7eefacd976 env[84]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Aug 26 07:34:11 fc7eefacd976 env[84]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Aug 26 07:34:11 fc7eefacd976 env[84]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Aug 26 07:34:11 fc7eefacd976 env[84]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Aug 26 07:34:12 fc7eefacd976 env[84]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Aug 26 07:34:13 fc7eefacd976 env[84]: Reading package lists...
Aug 26 07:34:13 fc7eefacd976 env[84]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-plenum=1.9.2~dev871 indy-node sovrin=1.1.136
Aug 26 07:34:15 fc7eefacd976 env[84]: Reading package lists...
Aug 26 07:34:15 fc7eefacd976 env[84]: Building dependency tree...
Aug 26 07:34:15 fc7eefacd976 env[84]: Reading state information...
Aug 26 07:34:15 fc7eefacd976 env[84]: The following additional packages will be installed:
Aug 26 07:34:15 fc7eefacd976 env[84]:   sovtoken
Aug 26 07:34:15 fc7eefacd976 env[84]: The following held packages will be changed:
Aug 26 07:34:15 fc7eefacd976 env[84]:   indy-node indy-plenum sovrin
Aug 26 07:34:15 fc7eefacd976 env[84]: The following packages will be upgraded:
Aug 26 07:34:15 fc7eefacd976 env[84]:   indy-node indy-plenum sovrin sovtoken
Aug 26 07:34:17 fc7eefacd976 env[84]: 4 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.
{noformat}

Sovtokenfees package is absent in dependency tree and is not upgraded.

Expected Results:
All dependent packages must be upgraded.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41iro",,,,Unset,Unset,Ev-Node 19.25,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"10/Dec/19 7:56 PM;VladimirWork;Verified in scope of INDY-2303.;;;",,,,,,,,,,,,,,,,,,,,,
Confusing fields in get auth rules response,INDY-2217,42023,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.minaev,sergey.minaev,26/Aug/19 7:18 PM,24/Oct/19 5:01 PM,28/Oct/23 2:47 AM,,1.9.1,,,,,,,0,,,,,"The response on GET_AUTH_RULES request contains fields txnTime and seqNo. And they both are equal to null in any case. If some auth rules already updated by the network administrator it seems confusing as the state really updated and it's expected to see the seq no and update time from the latest transaction

Steps to reproduce:

1) write some custom auth rules to the ledger
2) perform get auth rules request (e.g. without parameters)

Observed behavior:
txnTime and seqNo are nulls

Expected behavior:
txnTime and seqNo from latest update transaction or completely skipped as not applicable ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2266,,,No,,Unset,No,,,"1|hzwx4f:2rzmfr",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.minaev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Issue with non utf-8 decoding,INDY-2218,42026,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,27/Aug/19 12:53 AM,29/Aug/19 5:06 PM,28/Oct/23 2:47 AM,29/Aug/19 1:00 AM,,,1.9.2,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v98614o",,,,Unset,Unset,Ev-Node 19.17,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"28/Aug/19 11:47 PM;anikitinDSR;Reason:
 * need to process messages with not decoded identifier as invalid and reject it.

Changes:
 * added check in verification phase, that identifier can be decoded

Version:
 * indy-node: 1.9.2.1068

QA validation:
 * setup pool
 * using libindy build which can send requests with not-decoded-utf-8 identifiers send some requests to pool and check, that nodes not restart.
 * also, please check logs for messages like ""Identifier <some identifier> is not decoded into UTF-8 string.""

 

 ;;;","29/Aug/19 1:00 AM;VladimirWork;Build Info:
indy-node 1.9.2~dev1068

Steps to Validate:
1. Setup pool.
2. Using libindy build which can send requests with not-decoded-utf-8 identifiers send some requests to pool and check, that nodes not restart.
3. Check logs for messages like ""Identifier <some identifier> is not decoded into UTF-8 string."".

Actual Results:
Pool rejects connections with not decoded utf-8 identifiers. Pool works with normal connections successfully.;;;",,,,,,,,,,,,,,,,,,,,
Intermitted test failing on hyperledger's jenkins,INDY-2219,42027,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,anikitinDSR,anikitinDSR,27/Aug/19 1:09 AM,27/Aug/19 1:09 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Following tests are failed intermittent:
 * test_send_earlier_then_first_entry_by_default
 * test_send_get_revoc_reg_earlier_then_first_entry

Logs are attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/19 1:08 AM;anikitinDSR;test-result-node.prd-ubuntu1604-x86_64-4c-16g-652.txt;https://jira.hyperledger.org/secure/attachment/17738/test-result-node.prd-ubuntu1604-x86_64-4c-16g-652.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00y9n:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move 3PC Message Request logic into a separate service ,INDY-2220,42074,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,28/Aug/19 12:37 AM,01/Oct/19 8:00 PM,28/Oct/23 2:47 AM,13/Sep/19 9:18 PM,,,1.10.0,,,,,0,,,,,"We need to have Node and Replica independent service to process 3PC message requests and responses.

Other services will communicate with it via internal messages and buses.",,,,,,,,,,,,,,,,,,,,,,,,INDY-2225,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vr",,,,Unset,Unset,Ev-Node 19.18,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"28/Aug/19 11:11 PM;Toktar;*PoA:*
 * add MessageReq3pcService
 ** fields:
 *** internal_bus
 *** network
 *** shared_data
 *** handlers
 ** methods:
 *** process_missing_message - receives Missing3pcMessage from OrderingService throw internal_bus
 *** process_message_req - receives MessageReq from network and answer to network using shared_data
 *** process_message_rep - receives MessageRep from network, validate message and put a 3pc message to external bus for process it.
 * change PreprepareHandler, PrepareHandler, CommitHandler
 ** fields:
 *** shared_data
 *** requested_messages - Dict[Tuple[int, int], Optional[Tuple[str, str, str]]]
 ** new methods
 *** prepare_msg_to_request - instead _request_three_phase_msg
 *** validate_reply_msg - instead _process_requested_three_phase_msg
 *** gc - to cleaning requested_messages;;;","13/Sep/19 1:39 AM;Toktar;*Problem reason:*
 - We need a service to processing MessageReq and MessageRep messages.

*Changes:*
 - add Missing3pcMessage 
 - integrate Missing3pcMessage 
 - add tests

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1306]
 * [https://github.com/hyperledger/indy-node/pull/1445]

*Version:*
 * indy-node 1.9.2.dev1082 -master
 * indy-plenum 1.9.2.dev893 -master

*Risk factors:*
 - Problems with a message requesting

*Risk:*
 - Medium

*Tests:*
 * [test_incorrect_message_req_processing.py|https://github.com/hyperledger/indy-plenum/pull/1306/files#diff-9166a1e032975bf7f38a6a6eaf6f1d50] 
 * [test_message_req_commit.py|https://github.com/hyperledger/indy-plenum/pull/1306/files#diff-e2ee39cd69209dd324042843d63ca873] 
 * [test_message_req_prepare.py|https://github.com/hyperledger/indy-plenum/pull/1306/files#diff-fa0ee3a276b46a8b07592472c8729cfe] 
 * [test_message_req_preprepare.py|https://github.com/hyperledger/indy-plenum/pull/1306/files#diff-19b4dfa6cc9b92bcf5c0a1ddc0eba942] 
 * [test_message_req_processing.py|https://github.com/hyperledger/indy-plenum/pull/1306/files#diff-46078107b8e2a7c2ec8c4f04d9e54880] 

*Recommendations for QA:*
 * Production load test;;;",,,,,,,,,,,,,,,,,,,,
WARNING messages incorrectly logged if tokens are not used,INDY-2221,42115,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,mgbailey,mgbailey,29/Aug/19 6:37 AM,16/Jan/20 11:08 PM,28/Oct/23 2:47 AM,16/Jan/20 11:08 PM,,,1.12.2,,,,,0,EV-CS,,,,"Typically the WARNING log level is used for serious issues, such as those that prevent a transaction from being written to the ledger. Unfortunately, we now see WARNINGS being logged in some cases for valid transactions.

On StagingNet, we have authorization policies set so that a transaction can either be written by any user if paid for by a token, or by user with an appropriate privileged role if no token is paid. The issue described occurs if the second approach is taken. For example, a transaction endorser, which is configured to be authorized to write schemas, writes a schema without paying for it in tokens. This transaction is successful, but misleadingly generates a WARNING message in the node logs:


{code:java}
2019-08-26 14:23:17,693|DEBUG|node.py|australia received propagated request: PROPAGATE{'senderClient': 'mhi]exDNsaMDh$yuG$d?Qcg<OJ=x1pdC=)BJ[R(k', 'request': {'reqId': 1566829387617365918, 'operation': {'type': '101', 'data': {'version': '180.348.197', 'name': 'account', 'attr_names': ['bsb', 'debit', 'name', 'product name', 'credit', 'account']}}, 'protocolVersion': 2, 'identifier': 'KS34fHidcfwQxvqNCZUpUb', 'signature': 'ZkdWK2u94nw6EUgRrnUxS1mz3bALoky4yezi1h8Z2VjminyjzzD38mEV2dHjL3ZG3yLzVN1uM7rUJpqwCN694Pi'}}
2019-08-26 14:23:17,737|DEBUG|node.py|australia authenticated {'KS34fHidcfwQxvqNCZUpUb'} signature on propagate request 1566829387617365918
...
2019-08-26 14:23:17,833|DEBUG|auth_cons_strategies.py|Using auth constraint from state
2019-08-26 14:23:17,834|WARNING|fees_authorizer.py|Validation error: Fees are required for this txn type{code}
My guess as to what is occurring is that when fees are configured in the auth policies, they are always (appropriately) checked first to see if payment is provided. If no fee is found, a WARNING is generated and then it proceeds on to check if other auth policies are satisfied.

The desired behavior is that no WARNING message is generated until it is determined that *all* auth policies are unsatisfied and so the transaction will fail to be posted to the ledger. This behavior may be happening in more than just schema writes, and the fix should be applied universally.","Sovrin Networks, running indy 1.9.1",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ism",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,mgbailey,,,,,,,,,,"09/Jan/20 12:22 AM;anikitinDSR;Log level was dropped to DEBUG and available in versions:
 * sovtokenfees 1.0.7~dev144
 * sovtoken 1.0.7~dev144

PR:
 * [https://github.com/sovrin-foundation/token-plugin/pull/381];;;",,,,,,,,,,,,,,,,,,,,,
One node doesn't catch up after promotion,INDY-2222,42195,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,KitHat,VladimirWork,VladimirWork,05/Sep/19 4:24 PM,25/Oct/19 9:54 PM,28/Oct/23 2:47 AM,12/Oct/19 1:36 AM,,,1.11.0,,,,,0,TShirt_L,,,,"Build Info:
indy-node 1.10.0~dev1076

Steps to Reproduce:
https://github.com/hyperledger/indy-test-automation/blob/6eddf52dfdbf8ff1ceaee5e597e7429af9f36d88/system/indy-node-tests/TestCatchUpSuite.py#L42

Actual Results:
Node 8 doesn't catch up after promotion.",,,,,,,,,,,,,,,,,,,,,INDY-2183,,,INDY-2253,,,,,,,"25/Sep/19 11:42 PM;VladimirWork;catchup_failures.PNG;https://jira.hyperledger.org/secure/attachment/17850/catchup_failures.PNG","05/Sep/19 4:24 PM;VladimirWork;node1.2019-09-04T233937.tgz;https://jira.hyperledger.org/secure/attachment/17765/node1.2019-09-04T233937.tgz","06/Sep/19 4:38 PM;VladimirWork;node1.2019-09-05T233932.tgz;https://jira.hyperledger.org/secure/attachment/17794/node1.2019-09-05T233932.tgz","06/Sep/19 4:39 PM;VladimirWork;node1.2019-09-06T015245.tgz;https://jira.hyperledger.org/secure/attachment/17802/node1.2019-09-06T015245.tgz","20/Sep/19 9:47 PM;VladimirWork;node1.2019-09-19T235857.tgz;https://jira.hyperledger.org/secure/attachment/17840/node1.2019-09-19T235857.tgz","05/Sep/19 4:24 PM;VladimirWork;node2.2019-09-04T233938.tgz;https://jira.hyperledger.org/secure/attachment/17766/node2.2019-09-04T233938.tgz","06/Sep/19 4:38 PM;VladimirWork;node2.2019-09-05T233933.tgz;https://jira.hyperledger.org/secure/attachment/17793/node2.2019-09-05T233933.tgz","06/Sep/19 4:39 PM;VladimirWork;node2.2019-09-06T015246.tgz;https://jira.hyperledger.org/secure/attachment/17801/node2.2019-09-06T015246.tgz","20/Sep/19 9:47 PM;VladimirWork;node2.2019-09-19T235858.tgz;https://jira.hyperledger.org/secure/attachment/17839/node2.2019-09-19T235858.tgz","05/Sep/19 4:24 PM;VladimirWork;node3.2019-09-04T233939.tgz;https://jira.hyperledger.org/secure/attachment/17767/node3.2019-09-04T233939.tgz","06/Sep/19 4:38 PM;VladimirWork;node3.2019-09-05T233934.tgz;https://jira.hyperledger.org/secure/attachment/17792/node3.2019-09-05T233934.tgz","06/Sep/19 4:39 PM;VladimirWork;node3.2019-09-06T015247.tgz;https://jira.hyperledger.org/secure/attachment/17800/node3.2019-09-06T015247.tgz","20/Sep/19 9:47 PM;VladimirWork;node3.2019-09-19T235900.tgz;https://jira.hyperledger.org/secure/attachment/17841/node3.2019-09-19T235900.tgz","05/Sep/19 4:24 PM;VladimirWork;node4.2019-09-04T233939.tgz;https://jira.hyperledger.org/secure/attachment/17768/node4.2019-09-04T233939.tgz","06/Sep/19 4:38 PM;VladimirWork;node4.2019-09-05T233935.tgz;https://jira.hyperledger.org/secure/attachment/17791/node4.2019-09-05T233935.tgz","06/Sep/19 4:39 PM;VladimirWork;node4.2019-09-06T015248.tgz;https://jira.hyperledger.org/secure/attachment/17799/node4.2019-09-06T015248.tgz","05/Sep/19 4:24 PM;VladimirWork;node5.2019-09-04T233940.tgz;https://jira.hyperledger.org/secure/attachment/17769/node5.2019-09-04T233940.tgz","06/Sep/19 4:38 PM;VladimirWork;node5.2019-09-05T233936.tgz;https://jira.hyperledger.org/secure/attachment/17790/node5.2019-09-05T233936.tgz","06/Sep/19 4:39 PM;VladimirWork;node5.2019-09-06T015249.tgz;https://jira.hyperledger.org/secure/attachment/17798/node5.2019-09-06T015249.tgz","20/Sep/19 9:47 PM;VladimirWork;node5.2019-09-19T235902.tgz;https://jira.hyperledger.org/secure/attachment/17838/node5.2019-09-19T235902.tgz","05/Sep/19 4:24 PM;VladimirWork;node6.2019-09-04T233941.tgz;https://jira.hyperledger.org/secure/attachment/17770/node6.2019-09-04T233941.tgz","06/Sep/19 4:38 PM;VladimirWork;node6.2019-09-05T233937.tgz;https://jira.hyperledger.org/secure/attachment/17789/node6.2019-09-05T233937.tgz","06/Sep/19 4:39 PM;VladimirWork;node6.2019-09-06T015249.tgz;https://jira.hyperledger.org/secure/attachment/17797/node6.2019-09-06T015249.tgz","20/Sep/19 9:47 PM;VladimirWork;node6.2019-09-19T235904.tgz;https://jira.hyperledger.org/secure/attachment/17837/node6.2019-09-19T235904.tgz","05/Sep/19 4:24 PM;VladimirWork;node7.2019-09-04T233942.tgz;https://jira.hyperledger.org/secure/attachment/17771/node7.2019-09-04T233942.tgz","06/Sep/19 4:38 PM;VladimirWork;node7.2019-09-05T233937.tgz;https://jira.hyperledger.org/secure/attachment/17788/node7.2019-09-05T233937.tgz","20/Sep/19 9:47 PM;VladimirWork;node7.2019-09-19T235905.tgz;https://jira.hyperledger.org/secure/attachment/17836/node7.2019-09-19T235905.tgz","05/Sep/19 4:24 PM;VladimirWork;node8.2019-09-04T233943.tgz;https://jira.hyperledger.org/secure/attachment/17772/node8.2019-09-04T233943.tgz","06/Sep/19 4:38 PM;VladimirWork;node8.2019-09-05T233938.tgz;https://jira.hyperledger.org/secure/attachment/17787/node8.2019-09-05T233938.tgz","06/Sep/19 4:39 PM;VladimirWork;node8.2019-09-06T015251.tgz;https://jira.hyperledger.org/secure/attachment/17796/node8.2019-09-06T015251.tgz","20/Sep/19 9:47 PM;VladimirWork;node8.2019-09-19T235906.tgz;https://jira.hyperledger.org/secure/attachment/17835/node8.2019-09-19T235906.tgz","05/Sep/19 4:24 PM;VladimirWork;node9.2019-09-04T233943.tgz;https://jira.hyperledger.org/secure/attachment/17773/node9.2019-09-04T233943.tgz","06/Sep/19 4:37 PM;VladimirWork;node9.2019-09-05T233939.tgz;https://jira.hyperledger.org/secure/attachment/17786/node9.2019-09-05T233939.tgz","06/Sep/19 4:38 PM;VladimirWork;node9.2019-09-06T015252.tgz;https://jira.hyperledger.org/secure/attachment/17795/node9.2019-09-06T015252.tgz","20/Sep/19 9:47 PM;VladimirWork;node9.2019-09-19T235908.tgz;https://jira.hyperledger.org/secure/attachment/17834/node9.2019-09-19T235908.tgz",,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vii",,,,Unset,Unset,Ev-Node 19.19,Ev-Node 19.20,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),KitHat,Toktar,VladimirWork,,,,,,,,,"05/Sep/19 4:30 PM;VladimirWork;There was the same issue with 9th node but during node disconnection and connection against 1.10.0~dev1073 version: https://build.sovrin.org/blue/rest/organizations/jenkins/pipelines/indy-node/pipelines/indy-node-nightly/runs/92/nodes/90/steps/464/log/?start=0.;;;","05/Sep/19 5:17 PM;Toktar;Node8 after promote connected only to Node5 and Node7. And didn't catchup because didn't  have a quorum of LedgerStatuses.;;;","06/Sep/19 4:39 PM;VladimirWork;Issues with catchup are still present in 1.10.0~dev1078 indy-node setup:
https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-nightly/detail/indy-node-nightly/94/pipeline/94
[^node9.2019-09-05T233939.tgz]
[^node8.2019-09-05T233938.tgz]
[^node7.2019-09-05T233937.tgz]
[^node6.2019-09-05T233937.tgz]
[^node5.2019-09-05T233936.tgz]
[^node4.2019-09-05T233935.tgz]
[^node3.2019-09-05T233934.tgz] 
[^node2.2019-09-05T233933.tgz]
[^node1.2019-09-05T233932.tgz]

and 1.0.3~dev86 plugins + 1.10.0~dev1077 indy-node setup:
https://build.sovrin.org/blue/organizations/jenkins/sovrin%2Fsovrin-nightly/detail/sovrin-nightly/51/pipeline/85
[^node9.2019-09-06T015252.tgz]
[^node8.2019-09-06T015251.tgz]
[^node6.2019-09-06T015249.tgz]
[^node5.2019-09-06T015249.tgz]
[^node4.2019-09-06T015248.tgz]
[^node3.2019-09-06T015247.tgz]
[^node2.2019-09-06T015246.tgz]
[^node1.2019-09-06T015245.tgz] ;;;","12/Sep/19 11:08 PM;VladimirWork;It looks like we have more intermittent failures of TestCatchUpSuite (especially test_case_out_of_network) after indy-node ..1077 and plugins ..86 versions according to CD pipelines and local runs.;;;","20/Sep/19 9:47 PM;VladimirWork;Test failed against the latest master tonight:
{noformat}

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node1 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node2 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node3 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node4 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node5 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node6 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node7 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node8 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'263\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node9 'read_ledger --type=domain --count'"", exit_status=0, stdout=b'13\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node1 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node2 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node3 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node4 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node5 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node6 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node7 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node8 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'259\n', stderr=None)

INFO     testinfra:base.py:241 RUN CommandResult(command=b""ssh node9 'read_ledger --type=audit --count'"", exit_status=0, stdout=b'2\n', stderr=None)

ERROR    system.utils:utils.py:244 check_pool_is_in_sync failed; not trying any more because 200 seconds have passed; args were ()
{noformat}

9th node doesn't catch up and order anything (it has 13 domain txns and 2 audit).

Full logs: [^node9.2019-09-19T235908.tgz]  [^node8.2019-09-19T235906.tgz]  [^node7.2019-09-19T235905.tgz]  [^node6.2019-09-19T235904.tgz]  [^node5.2019-09-19T235902.tgz]  [^node2.2019-09-19T235858.tgz]  [^node1.2019-09-19T235857.tgz]  [^node3.2019-09-19T235900.tgz] ;;;","24/Sep/19 6:56 PM;VladimirWork;All 4 catchup tests pass if we reduce the number of nyms written and read between nodes' operations to 1 (4 total in test case).;;;","24/Sep/19 8:35 PM;KitHat;Processed logs from the latest nightly build. Looks like Node9 finished connection:

{noformat}
2019-09-23 23:51:57,070|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node8
2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node2
2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node6
2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node7
2019-09-23 23:52:14,739|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node3
2019-09-23 23:52:19,030|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node1
2019-09-23 23:52:19,030|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node5
2019-09-23 23:52:19,030|NOTIFICATION|keep_in_touch.py|CONNECTION: Node9 now connected to Node4
{noformat}

But has not received quorum of LEDGER_STATUS messages to start catch-up:

{noformat}
2019-09-23 23:51:56,098|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8
2019-09-23 23:52:01,943|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8
2019-09-23 23:52:01,943|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8
2019-09-23 23:52:01,944|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8
2019-09-23 23:52:01,945|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node8
2019-09-23 23:52:18,404|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,405|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,405|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,416|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,789|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,800|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,801|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,802|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node2
2019-09-23 23:52:18,966|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4
2019-09-23 23:52:20,232|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,234|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,234|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,235|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,236|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,237|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,772|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,773|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,774|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,775|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,776|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,785|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4
2019-09-23 23:52:20,786|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4
2019-09-23 23:52:20,787|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4
2019-09-23 23:52:20,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4
2019-09-23 23:52:20,788|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node4
2019-09-23 23:52:20,789|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node1
2019-09-23 23:52:20,828|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,830|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,831|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,836|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,841|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,842|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
2019-09-23 23:52:20,843|INFO|seeder_service.py|Node9 received ledger status: LEDGER_STATUS{'viewNo': None, 'txnSeqNo': 9, 'merkleRoot': '9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER', 'protocolVersion': 2, 'ppSeqNo': None, 'ledgerId': 0} from Node5
{noformat}

So the the LEDGER_STATUS message has been received only from Node1, Node2, Node4, Node5 and Node8 -- and we need 6 for quorum. Node 9 requested LEDGER_STATUS from all nodes but it was lost somehow on a way back:
{noformat}
434730 2019-09-23 23:56:54,843|TRACE|batched.py|Node3 sending msg b'{""msg_type"":""LEDGER_STATUS"",""op"":""MESSAGE_RESPONSE"",""params"":{""ledgerId"":0},""msg"":{""ledgerId"":0,""merkleRoot"":""9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER"",""ppSeqNo"":null,""protocolVersion"":2,""txnSeqNo"":9,""viewNo"":null}}' to Node9
434731 2019-09-23 23:56:54,843|TRACE|zstack.py|Node3 transmitting message b'{""msg_type"":""LEDGER_STATUS"",""op"":""MESSAGE_RESPONSE"",""params"":{""ledgerId"":0},""msg"":{""ledgerId"":0,""merkleRoot"":""9fgdioj4F7HrpqFVdvH1juGyXjSsTsG1vfjXVwczZsER"",""ppSeqNo"":null,""protocolVersion"":2,""txnSeqNo"":9,""viewNo"":null}}' to Node9 by socket 114 35992384  
{noformat};;;","25/Sep/19 11:42 PM;VladimirWork;Test with 100 nyms sent on each step that checks all nodes reachability (after 9th node connection) fails in the same place as test with 100 nyms sent on each step that doesn't check reachability: !catchup_failures.PNG|thumbnail! 
So it looks like with 1 and 10 nyms sent we have reachability and perform catchup and with 100 nyms *we have reachability but don't perform catchup*.

FYI [~ashcherbakov] [~KitHat];;;","09/Oct/19 10:28 PM;KitHat;Problem reason: 
- Some nodes can not connect to reconnected (after different occasions) nodes

Changes: 
- after 4 consecutive pings node will reconnect connected remote.

PR:
- https://github.com/hyperledger/indy-plenum/pull/1363


Version:
- indy-node master #1102

Risk factors:
- reconnection cycles

Risk:
- Med

Covered with tests:
- https://github.com/KitHat/indy-plenum/blob/1d0e1ac8368f877dcec890aa3c68c78963ad33f0/plenum/test/zstack_tests/test_ping_reconnection.py

Recommendations for QA
- The best test would be reconnections under load with checks of reachability (but without viewchanges);;;","10/Oct/19 8:20 PM;VladimirWork;Catchup tests have failed at 50 txns sent locally and at 25 and 50 txns sent in indy-node nightly pipeline so it looks like we still have the issue here.

https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-nightly/detail/indy-node-nightly/129/tests;;;","12/Oct/19 1:10 AM;KitHat;So, I have investigated the logs and it is another issue (or an issue with tests)
It looks like the node that is turned on first isn't awaited to get all connections before writing new transactions. (However nodes that have not restarted in some way receive messages)
As a result Node8 does get some 3PC messages but not enough to collect strong consensus.
Then the new mechanism of reconnection fires off and old 3PC messages are not resent -- so Node8 stays behind.
We may have it caught up if we will send more messages.
I suggest to modify the test or to move the work to another story -- the problem with connections looks to be fixed.;;;",,,,,,,,,,,
Basic integration tests with a new View Change protocol need to pass,INDY-2223,42321,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,12/Sep/19 6:12 PM,01/Oct/19 8:02 PM,28/Oct/23 2:47 AM,30/Sep/19 3:46 PM,,,1.11.0,,,,,0,,,,,"*Acceptance criteria:*
 * A new View Change protocol should work
 * Basic integration tests for View Change should pass
 * Some of existing integration tests can be skipped and processed in INDY-2140
 * Some of existing integration tests may be removed or rewritten if they are specific to the old view change protocol",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w0i",,,,Unset,Unset,Ev-Node 19.19,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"12/Sep/19 6:15 PM;ashcherbakov;PoA:
 # [DONE] Make sure that ViewChange and NewView messages work correctly when sent via network layer (serialized and deserialized). As of now it will not work, since it uses tuples and Checkpoint messages which will be deserialized to dicts and lists.
 # [DONE] Investigate why not all simulation tests pass
# [DONE] Fix the processing of Instance Changes 
# [DONE] Fix primaries in new view audit txns when only 1 batch has been ordered on some nodes 
# [DONE] Improve timeouts in simulation tests to reproduce two issues above (plus potentially lack of ViewChange requests)
# [DONE] Fix last pre-prepare time when reverting bacthes before View Change
# [DONE] Write a couple of simple integration tests for view change
 ** Test 1:
 *** Send `NeedViewChange` messages to all the nodes
 *** Make sure that view change finished
 ** Test 2:
 *** Order a couple of batches on all nodes
 *** Send `NeedViewChange` messages to all the nodes
 *** Make sure that view change finished
 ** Test 3:
 *** Stop next primary
 *** Send `NeedViewChange` messages to all the nodes
 *** Make sure that view change finished
 ** Test 4:
 *** Delay Commits on all nodes except Node4
 *** Order a couple of batches on all nodes except Node 4 (it needs to be prepared  on Node 
 *** Send `NeedViewChange` messages to all the nodes
 *** Make sure that view change finished, all nodes have the same data, and the pool is functional (new requests can be written)
 ** Test 5:
 *** Delay Commits on all nodes except Node4
 *** Make sure a request is ordered
 *** Make sure a request is pre-prepared on Node4
 *** Reset delay without processing
 *** Stop Node2 and
 *** Send `NeedViewChange` messages to all the nodes
 *** Make sure that viewno=2, all nodes have the same data, and the pool is functional (new requests can be written)
** Test 6:
*** Lagging one node more then 1 checkpoint but less then needed to catchup.
*** Forcing view change.
*** Ensure, that lagged node will try to reorder batches after pool's stable checkpoint
*** Send requests for next stable checkpoint
*** Ensure, that lagged node will start catch-up and continue ordering as other nodes (set last_ordered with new view_no)
# [IN PROGRESS] Create a build with a new View Change Service integrated [~sergey.khoroshavin] 
 ** OrderingServiceMsgValidator into Ordering Service
 ** Modify View Changer to switch to new View Change Service
 ** Skip old tests in view-change folder (and potentially catch-up folder)
 ** Try to fix other tests if they can be fixed quickly; otherwise skip them as well
 ** Modify the tests from the previous item to start a real view change (by performance degradation for example) instead of explicit message sending
;;;","25/Sep/19 7:58 PM;ashcherbakov;Follow-up tasks:
* INDY-2229
* INDY-2230
* INDY-2231

Other possible future improvements:
# Consider starting catchup after view change immediately if a Replica doesn't have a checkpoint from the NewView (lagged behind).
# Consider removing primaries from PrePrepare, and do a View Change on every change of N. ;;;","30/Sep/19 3:45 PM;ashcherbakov;*PRs:*
 * https://github.com/hyperledger/indy-plenum/pull/1333
 * [https://github.com/hyperledger/indy-plenum/pull/1334]
 * https://github.com/hyperledger/indy-plenum/pull/1336
 * https://github.com/hyperledger/indy-plenum/pull/1338
 * https://github.com/hyperledger/indy-plenum/pull/1341
 * https://github.com/hyperledger/indy-plenum/pull/1344
 * https://github.com/hyperledger/indy-plenum/pull/1347
 * https://github.com/hyperledger/indy-plenum/pull/1349
 * https://github.com/hyperledger/indy-plenum/pull/1345;;;","30/Sep/19 3:46 PM;ashcherbakov;The work will be continued in the scope of INDY-2140 and INDY-2146;;;",,,,,,,,,,,,,,,,,,
Recover from a situation when View Change is finished on >= N-F of other nodes,INDY-2224,42322,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,12/Sep/19 6:17 PM,22/Oct/19 5:35 PM,28/Oct/23 2:47 AM,22/Oct/19 12:01 AM,,,1.11.0,,,,,0,,,,,Request `NewView` from a Primary if a Node sees that other nodes finished view change,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c9i",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,Toktar,VladimirWork,,,,,,,,"17/Oct/19 9:33 PM;Toktar;*PoA:*

Request `NewView` from a Primary if  when node sends Instance change with reason 28 ('View change could not complete in time').

Schedule reasking NewView for every NEW_VIEW_TIMEOUT if waiting_for_new_view==True and NewView  was not received. Process MessageRep only from the future primary and send MessageRep only by the primary node.

Add tests:
 * simulation test with delaying NewView messages an
 * unit tests
 ** for sending MessageReq(NewView)
 ** for process MessageReq(NewView) and sending MessageRep(NewView) (by primary and non primary)
 ** for process MessageRep(NewView) (by primary and non primary);;;","18/Oct/19 12:09 AM;sergey.khoroshavin;Regarding simulation tests - I propose instead of adding separate test introduce random ""loss"" of NewView messages into existing one.;;;","18/Oct/19 12:27 AM;Toktar;[PR: https://github.com/hyperledger/indy-plenum/pull/1375|https://github.com/hyperledger/indy-plenum/pull/1375];;;","19/Oct/19 12:13 AM;Toktar;*Problem reason:*
 - If a node lost NewView message, it can't finish a view change.

*Changes:*
 - Request NewView from a primary if the message wasn't received after NEW_VIEW_TIMEOUT seconds.

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1375]

*Version:*
 * indy-node 1.11.0.dev1110 -master
 * indy-plenum 1.11.0.dev935 -master

*Risk factors:*
 - Problems with re-asking 3pc and view change messages

*Risk:*
 - Low

*Tests:*
 * [test_message_req_new_view.py|https://github.com/hyperledger/indy-plenum/pull/1375/files#diff-89de6cfdbee182a531a78bdb43c775f7] 
 * [test_sim_view_change.py|https://github.com/hyperledger/indy-plenum/pull/1375/files#diff-59b79d040115d7127b1ee697def2ed89] 
 * [test_view_change_with_lost_new_view.py|https://github.com/hyperledger/indy-plenum/pull/1375/files#diff-0df670bc5e24bf7848a0c2c8c246d609] 

*Recommendations for QA:*
 *  Start a docker pool for 4 nodes
 * Delay on Node4 messages from Node2
 * Start a view change on all nodes.
 * Reset delays after 2 min
 * Wait 1 min
 * Check that all nodes finished view change.;;;","22/Oct/19 12:01 AM;VladimirWork;Build Info:
indy-node 1.11.0~dev1110

Steps to Validate:
1. Start a docker pool for 4 nodes.
2. Delay on Node4 messages from Node2.
3. Start a view change on all nodes.
4. Reset delays after 2 min.
5. Wait 1 min.
6. Check that all nodes finished view change.

Actual Results:
Pool finished VC successfully.;;;",,,,,,,,,,,,,,,,,
Run production load test to validate moving 3PC Message Request logic into a separate service,INDY-2225,42344,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,13/Sep/19 9:27 PM,26/Sep/19 8:37 PM,28/Oct/23 2:47 AM,26/Sep/19 8:37 PM,,,,,,,,0,,,,,"Run production load test to validate moving 3PC Message Request logic into a separate service against:
indy-node 1.9.2~dev1082 master
indy-plenum 1.9.2~dev893 master
plugins 1.0.3~dev92 master",,,,,,,,,,,,,,,,,,,,,INDY-2220,,,,,,,,,,"17/Sep/19 1:00 AM;VladimirWork;Figure.png;https://jira.hyperledger.org/secure/attachment/17819/Figure.png","17/Sep/19 1:00 AM;VladimirWork;report.png;https://jira.hyperledger.org/secure/attachment/17818/report.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w2u",,,,Unset,Unset,Ev-Node 19.19,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"17/Sep/19 12:57 AM;VladimirWork;Build Info:
indy-node 1.9.2~dev1082 master
indy-plenum 1.9.2~dev893 master
plugins 1.0.3~dev92 master

Actual Results:
Pool lost consensus after ~9 hours under production load.

ev@evernymr33:logs/performance_results_16_09_2019.tar.gz
ev@evernymr33:logs/2225_16_09_2019.tar.gz;;;","17/Sep/19 1:00 AM;VladimirWork;FYI [~ashcherbakov][~sergey.khoroshavin];;;",,,,,,,,,,,,,,,,,,,,
Drop ppSeqNo on Backups after View Change,INDY-2226,42353,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Sep/19 1:13 AM,01/Oct/19 8:02 PM,28/Oct/23 2:47 AM,20/Sep/19 6:51 PM,,,1.10.0,,,,,0,,,,,"As was discovered in INDY-1336, not dropping ppSeqNo after view change on backup instances may stop instances from ordering if they have been removed by some nodes. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w09",,,,Unset,Unset,Ev-Node 19.19,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"18/Sep/19 7:34 PM;anikitinDSR;Versions to check:
* indy-node: 1.10.0~dev1084
* plugins: 1.3.0~95

 ;;;","20/Sep/19 6:50 PM;VladimirWork;Build Info:
indy-node 1.10.0~dev1084
plugins 1.3.0~dev95

Steps to Validate:
1. Run production load test with and without forced VCs.

Actual Results:
Backup replicas are ordering after all VCs.

Logs and metrics:

ev@evernymr33:logs/performance_results_19_09_2019.tar.gz
ev@evernymr33:logs/2226_19_09_2019_no_vc.tar.gz

ev@evernymr33:logs/performance_results_20_09_2019.tar.gz
ev@evernymr33:logs/2226_20_09_2019_force_vc.tar.gz;;;",,,,,,,,,,,,,,,,,,,,
Improve automated load test analysis,INDY-2227,42370,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,17/Sep/19 12:50 AM,18/Nov/19 9:18 PM,28/Oct/23 2:47 AM,18/Nov/19 9:18 PM,,,,,,,,0,,,,,"PoA:

Option 1:
Split pool_get_logs playbook into 2 playbooks for summary and for the whole data and perform jctl and log `grep` remotely, add default metric plotter to setup.py, plot figures remotely, save all results (grepped files and figures) and pull it using the first script. Maybe it will be reasonable to run perf_res_processor script in the first playbook too to avoid 2 separate runs.

Option 2:
Use ansible python3 package instead of yaml playbooks (to keep all actions and logic in python files) and implement 2 scripts described in Option 1 in python not in yaml.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41g",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"14/Nov/19 12:37 AM;VladimirWork;Initial version: https://github.com/hyperledger/indy-test-automation/pull/76;;;","18/Nov/19 9:18 PM;VladimirWork;Merged.;;;",,,,,,,,,,,,,,,,,,,,
All ledgers in a batch need to be BLS multi-signed,INDY-2228,42466,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,KitHat,ashcherbakov,ashcherbakov,24/Sep/19 7:07 PM,24/Sep/19 7:10 PM,28/Oct/23 2:47 AM,24/Sep/19 7:08 PM,,,1.10.0,,,,,0,,,,,"If a batch writes to multiple ledgers (it can be when plugins are installed; see for example https://sovrin.atlassian.net/browse/ST-623), then only the main ledger is BLS multi-signed.
It means that
- GET_TXN for the second ledger will not support BLS multi-signed audit proofs until the second ledger is multi-signed again. 
- Get requests for the second ledger will not support BLS multi-signed state proofs until the second ledger is multi-signed again. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vg",,,,Unset,Unset,Ev-Node 19.19,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"24/Sep/19 7:08 PM;ashcherbakov;Please see details and PoA in https://sovrin.atlassian.net/browse/ST-623;;;",,,,,,,,,,,,,,,,,,,,,
A lagging Node need to be able to start catchup by checkpoints when view change is in progress,INDY-2229,42490,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,Toktar,ashcherbakov,ashcherbakov,25/Sep/19 10:28 PM,24/Oct/19 4:50 PM,28/Oct/23 2:47 AM,24/Oct/19 4:50 PM,,,1.12.1,,,,,0,,,,,"Let's consider the following situations:
* A node started view change
* A node didn't receive NewView for some reasons while other nodes received it and finished view change
* So, other nodes finished view change and started ordering while a lagging node is still in `waiting_for_new_view` (view change in progress) state. 

Expected behavior here is that the lagging node should receive a quorum of 2 stable checkpoints from other nodes and do a catchup which should finish its view change. 

*Acceptance criteria:*
1. Write integration tests:
* Delay (without processing) receiving of  NewView by Gamma
* Start a view change and wait until it finished on all nodes except Gamma
* Order by all nodes except Gamma. Order 2 stable checkpoints
* Make sure that Gamma eventually caught up all transactions, finished view change, and can participate in ordering

2. Do the following changes in code:
* Process checkpoints from other nodes during view change
* Make sure that we can start catchup during view change if receive a quorum of 2 stable checkpoints from other nodes
* Make sure that finish of catchup finishes view change as well. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c8i",,,,Unset,Unset,Ev-Node 19.20,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,Toktar,,,,,,,,,"17/Oct/19 12:31 AM;Toktar;*PoA:*

Consider the following cases:
1. Catchup before view change. View change messages should be stashed during catchup and unstashed after this.
2. After starting the view change, we have a quorum of checkpoints before the view change.
Ex: NodeX.last_orderd = (0, 50); other_nodes.last_orderd = (0, 250),
All nodes start view change 
NodeX received quorum of checkpoints [101, 200] and start catchup[last_oredered, 200].
View change messages should be stashed during catchup and unstashed after this and NodeX will finish the view change.
3. After starting the view change, we have a quorum of checkpoints from the next view.
Ex: NodeX.last_orderd = (0, 150); other_nodes.last_orderd = (0, 250),
All nodes start view change 
NodeX received quorum of checkpoints [201, 300] and start catchup[last_oredered, 300].
View change and 3pc messages should be stashed during catchup
After this waiting_for_new_view = False for finish the view change. Unstash view change messages, unstash 3pc messages.

In fact, last 2 items about cases when catchup finished with primary changing or not.;;;","17/Oct/19 9:01 PM;sergey.khoroshavin;It looks like it makes sense to implement simpler and safer solution instead - request missing NewView messages (INDY-2224).

Other ideas:
- always react with ViewChangeAck to ViewChange messages _before_ checking whether they should be processed, discarded or stashed. That way even if we're in a long catch up new primary would be able to get our witness of other nodes ViewChange messages
- upon receiving NewView message if it requires ordering from checkpoint that we don't have - start catch up immediately, don't wait for checkpoints;;;","24/Oct/19 4:50 PM;ashcherbakov;We are not going to do this since it looks dangerous to change checkpoints during view change.
The issue with infinite view changes is addressed and fixed by the following:
1) INDY-2224
2) Sending InstanceChanges to the next view by timeout + Instances Changes by Nodes who finished view change but can not order (freshness checks) + regular view changes due to Aardvark;;;",,,,,,,,,,,,,,,,,,,
A Primary lagging behind a stable chedkpoints should not send NewView,INDY-2230,42491,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,25/Sep/19 11:21 PM,10/Oct/19 11:45 PM,28/Oct/23 2:47 AM,10/Oct/19 11:31 PM,,,1.11.0,,,,,0,,,,,"*Acceptance criteria:*
 1. Write an integration test
 * Write CHK_FREQ number of batches on all nodes except the next Primary
 * Do a View Change
 * Make sure that View Change is finished and viewNo == originalViewNo + 2
 * Make sure that pool can order

2. Do changes in code:
  * Make sure that a Primary doesn't send NewView if it doesn't have a checkpoint calculated for a NewView",,,,,,,,,,,,,,,,,,,,,,,,INDY-2237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vm",,,,Unset,Unset,Ev-Node 19.20,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"10/Oct/19 11:45 PM;anikitinDSR;It will be tested in the scope of INDY-2140 and INDY-2146.;;;",,,,,,,,,,,,,,,,,,,,,
Do not stabilize checkpoint after the view change if a Replica doesn't have this checkpoint,INDY-2231,42493,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,25/Sep/19 11:27 PM,08/Oct/19 12:50 AM,28/Oct/23 2:47 AM,08/Oct/19 12:50 AM,,,1.11.0,,,,,0,,,,,"*Acceptance criteria:*
* Modify `plenum/test/view_change_service/test_lag_by_checkpoint.py` to send just 1 Checkpoint after the View Change
* Do a change in a Checkpointer Service to not stabilize a checkpoint from a NewView if replica doesn't have this checkpoint",,,,,,,,,,,,,,,,,,,,,,,,INDY-2237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vn",,,,Unset,Unset,Ev-Node 19.20,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"03/Oct/19 8:49 PM;ashcherbakov;*Changes:*
 * do not stabilize checkpoints from NewView during view change on nodes lagging behind (that is if a node doesn't have this checkpoint)
 * change quorum (from weak to strong certificate) when calculating a checkpoint for NewView. This is needed to make sure that view change is finished and nodes can order without catchup before processing NewView. Now the checkpoint can be lower, so more re-ordering may be needed. See INDY-2237 for details.
 * added simulation tests with random seeds
 * fixes in stabilization of checkpoints after catch-up
 * fixes in ordering after catch-up and view change
 * fixes and improvements in tests

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1359]

*Covered by tests:*
 * plenum/test/consensus/view_change/test_new_view_builder.py
 * plenum/test/consensus/view_change/test_sim_view_change.py
 * plenum/test/view_change_service/test_lag_by_checkpoint.py

*Recommendation for QA:*
 * Do a load test in the scope of INDY-2146

*Risk:*
 * Low;;;",,,,,,,,,,,,,,,,,,,,,
Node doesn't catch up after pool ledger writes,INDY-2232,42495,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,26/Sep/19 1:20 AM,27/Sep/19 10:12 PM,28/Oct/23 2:47 AM,27/Sep/19 10:12 PM,,,,,,,,0,,,,,"Build Info:
indy-node 1.10.0~dev1087
plugins 1.0.3~dev97

Steps to Reproduce:
1. Setup pool of 8 nodes.
2. Stop 8th node.
3. Write 25 node txns with empty SERVICES.
4. Check that 7 nodes are in sync.
5. Start 8th node.
6. Write another 25 node txns with empty SERVICES.
7. Check that 8 nodes are in sync.

Actual Results:
8th node stalls for 2 pool, 1 domain and 7 audit txns and doesn't catch up.

Expected Results:
8th node must catch up all ledgers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49",,,,Unset,Unset,Ev-Node 19.19,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"26/Sep/19 1:22 AM;VladimirWork;Debug logs: ev@evernymr33:logs/pool_ledger_catchup.tar.gz;;;","26/Sep/19 1:25 AM;VladimirWork;System test: https://github.com/VladimirWork/indy-test-automation/blob/2db2a986bc72e7d4ffd5c36ed5c362ebaabe08c7/system/indy-node-tests/TestCatchUpSuiteExtended.py#L34;;;","26/Sep/19 8:52 PM;VladimirWork;Test passes with 1 and 100 txns written but fails with 25 and 50.;;;","26/Sep/19 9:41 PM;ashcherbakov;A quick analysis showed the following:
 * We start Node8 and start sending 50 more requests *at the same time*
 * This means that most probably Node8 will not catchup all these new 50 txns, but only part of them (the ones already ordered at the time of catchup start and gathering ledger statuses and consistency proofs)
 * As we have a case with ordering during catchup, Node8 stashes all 3PC messages received during catchup.
 * It also receives Propagates for these requests, but *discards them since there is no NYM transaction used to sign the requests in Propagates yet* (we haven't caught up domain ledger yet).
 * Once Node8 finished catcup (in caught up about 20 of 50 txns in this test run; and ended up with last ordered = (0, 82)), it starts unstashing and applying 3PC messages
 * Most of the requests are not finalized, so it requests Propagates from other nodes
 * It was able to order till (0, 99) requesting missing Propagates.
 * But other nodes stabilized a checkpoint then and removed all requests below 100.
 * So, Node 8 wasn't able to process next PrePrepares since it didn't have propagated requests, and there is no way to get it from other nodes because they already removed them.

 

If we sent more requests, the node would be able to catchup and continue ordering.;;;","27/Sep/19 1:00 AM;ashcherbakov;Next steps and recommendation:
 * Exetnd/modify tests:
 ** Test1: send less than 100 (CHK_FREQ) transactions in total. This will test that we can catchup during ordering and can apply messages stashed during catchup. The test should pass.
 ** Test2: wait until Node8 caught up before sending more requests. This will test that cathup of Pool ledger works, and we can continue ordering. The test should pass
 ** Test3: send about 200 more transactions after the new node joins. This will test that although the Node can not start ordering immediately because of the issue described above, the node will catchup and continue ordering after a quorum of checkpoints from other nodes.
 * Fix the issue described above: it looks like a correct fix is to do INDY-1960;;;","27/Sep/19 1:04 AM;VladimirWork;The same issue with config ledger has bee catched: we don't wait for catchup before ordering, write more than 100 config txns during test (2x55) and don't send any additional txns before final catchup checking.

ev@evernymr33:logs/config_catchup_failure.tar.gz;;;","27/Sep/19 1:48 AM;ashcherbakov;The reason why the test with config failed is because Node8 didn't have connection to Node1 after restart. Since Node1 is a Primary, Node 8 didn't receive any PrePrepares at all, that's why it wasn't able to order anything after catchup. 

So, it looks like this is the same issue as in INDY-2222;;;",,,,,,,,,,,,,,,
GET_TXN doesn't work with old libindy,INDY-2233,42548,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,VladimirWork,ashcherbakov,ashcherbakov,30/Sep/19 10:42 PM,03/Oct/19 11:03 PM,28/Oct/23 2:47 AM,03/Oct/19 5:21 PM,,,1.10.0,,,,,0,,,,,"*Issue:*
 * GET_TXN for old libindy which doesn't support audit proof for GET_TXN may return ""consensus is impossible"".
 * This is intermittent issue, and its probability depends on the pool size.

*Reason:*
 * The latest indy-node support introduces BLS multisig for GET_TXN
 * GET_TXN has a new field ""multi_sig"" which is a bls multi-sig (signature + list of participants)
 * List of participants can be different on nodes depending on the combination of commits they used for consensus.
 * So, client can not get F+1 equal replies because of difference in participant field.

*Proposed fix:*
 * Since libindy always removes ""stateProof"" field from any request, the fix is to move ""multi_sig"" field to ""stateProof""
 * Pros:
 ** Compatible with older version
 ** Audit Proof checks will work for the latest (September release) SDK
 * Cons:
 ** Audit Proof  will not work with the current (August) SDK with the latest (September) Node. May be OK since this is just temporarily and doesn't cause any issues (fallback to consensus will work well)
 ** Not so good from the clean API point of view (state proof means not state proof, but just multi-sigs)

*Test:*
 * Open connection to the pool
 * Rotate BLS keys (ideally on F+1 nodes)
 * Send a domain txn
 * Do GET_TXN

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,IS-1383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i",,,,Unset,Unset,Ev-Node 19.20,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,KitHat,VladimirWork,,,,,,,,,"03/Oct/19 5:14 PM;KitHat;Problem reason: 
- because the set of multi-signatures in request can differ consensus in old libindy versions was not collected

Changes: 
- moved `multi_signature` to `state_proof`

PR:
- https://github.com/hyperledger/indy-plenum/pull/1356


Version:
- indy-node master 1.10.0~dev1095
- indy-sdk master 1.11.1~1343

Risk factors:
- some state proof corner cases

Risk:
- Low

Covered with tests:
- https://github.com/hyperledger/indy-plenum/pull/1356/files#diff-25d6b0f795369aa431667e42226f8392R11

Recommendations for QA
- test state proof on old version, test that requests on the versions with old changes that they are collected properly.;;;","03/Oct/19 5:20 PM;VladimirWork;Build Info:
indy-node 1.10.0~dev1096
libindy 1.11.1~1343

Steps to Validate:
https://github.com/VladimirWork/indy-test-automation/blob/a4eb6609a11264467de1b013a7ba7b2ad7e0ca1e/system/draft/test_misc.py#L2023

Actual Results:
System test with blskeys rotation and GET_TXN passes.;;;",,,,,,,,,,,,,,,,,,,,
Run nightly system tests for old versions of libindy as well,INDY-2234,42553,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,ashcherbakov,ashcherbakov,30/Sep/19 11:28 PM,08/Jan/20 5:08 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"We do have nightly system tests against the current version on indy-node and the current version of libindy.

We need to test compatibility between the current indy-node and old versions of libindy.

*Acceptance criteria*
 * Configure CD pipeline to run nightly test against previous stable version of libindy
 * It doesn't need to be done every night, so find out a proper frequency (twice per week?)
 * Do the changes (tagging?) in tests if needed to not run tests that doesn't make sense for old version (new feature not supported by the old libindy).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ic",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save PrePrepare's BatchID in audit ledger and restore list of preprepares and prepares on node startup,INDY-2235,42589,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,02/Oct/19 9:52 PM,30/Oct/19 12:15 AM,28/Oct/23 2:47 AM,30/Oct/19 12:15 AM,,,1.11.0,,,,,0,,,,,"*Problem*
 * In order to successfully finish ViewChange (not go to infinite loop), we need a quorum of `preprepared` and `prepared` messages (P and Q) in consensus shared data.
 * We guarantee to get the quorum if all nodes are up and no new nodes are added
 * But if nodes are restarted or just added, they don't have `preprepared` and `prepared` filled up.
 * Please note, that this can help with requests alкeady ordered, as well as with the new nodes joining the pool. If there are requests in flight (not ordered on all nodes) like in INDY-2238, then this will not help and can still lead to infinite view change. Persisiting of 3PC messages (INDY-2238) looks like the only solution here.

*Acceptance criteria*
 * Add PrePrepare's digest to audit ledger
 ** See [https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/batch_handlers/audit_batch_handler.py]
 ** Write unit tests:
 *** [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/audit_ledger/helper.py#L17]
 *** [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/audit_ledger/test_audit_ledger_handler.py]
 * Restore `preprepared` and `prepared` in consensus shared data when a node starts up (after initial catch-up) from the audit Ledger
 ** See [https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/consensus/consensus_shared_data.py]
 ** See [https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/consensus/ordering_service.py#L2163]
 ** Write unit tests:
 *** [https://github.com/hyperledger/indy-plenum/tree/master/plenum/test/consensus/order_service]

 * Unskip `test_view_change_after_back_to_quorum_with_disconnected_primary`, make sure it passes
 * Consider writing more integration tests, for example:
 ** Test1:
 *** Delay commits on Node1
 *** Send a request and make sure it's ordered on Nodes2-4
 *** Restart Nodes2-4
 *** Start view change
 *** Make sure that view change is finished, and all nodes have equal data
 ** Test2:
 *** Delay commits on Nodes1 and 2
 *** Send a request and make sure it's ordered on Node3 and 4
 *** Restart Nodes 3 and 4
 *** Start view change
 *** Make sure that view change is finished, and all nodes have equal data",,,,,,,,,,,,,,,,,,,,,INDY-2238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vj4",,,,Unset,Unset,Ev-Node 19.20,Ev-Node 19.21,Ev-Node 19.22,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,donqui,VladimirWork,,,,,,,,"02/Oct/19 10:00 PM;anikitinDSR;Test for indicating the problem is:



test_view_change_after_back_to_quorum_with_disconnected_primary;;;","26/Oct/19 8:36 AM;donqui;  Problem reason/description:
 - As nodes lost preprepares and prepares on restart when view change was triggered they did not have enough information for re-ordering phase and because of that view change did not execute properly on all nodes.

Changes:
 - Save preprepares digest in audit ledger as part of audit transaction
 - Restore preprepared/prepared after catchup from the audit ledger

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1383]
 - [https://github.com/hyperledger/indy-plenum/pull/1385]

Version:
 - plenum:  1.11.0.dev941 
 - node: 1.11.0.dev944
 - sovtoken: sovtoken_1.0.4~dev111 sovtokenfees_1.0.4~dev111 

Risk factors:
 - possible that the problem will happen in cases where we have a checkpoint interval that contains old audit txns and new ones with the digest. In this case will not restore them and we might see the same issue
 - changing the txn format may present a breaking change with some unknown consequences

Risk:
 - Med

Covered with tests:
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits_on_half_of_the_nodes_and_restart_of_the_other_half.py]
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits_on_half_of_the_nodes_and_restart_of_that_half.py]
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits_on_one_node_and_restart_of_other_nodes.py]
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_catchup_with_3pc/test_preprepares_and_prepares_recovery_after_catchup.py]
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary.py]
 - [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change/test_view_change_after_back_to_quorum_with_disconnected_primary_and_slow_node.py]

Recommendations for QA
 - I would try to perform the same scenario as in the test [https://github.com/hyperledger/indy-plenum/pull/1383/files#diff-aa4abf76aa908a6c1a45e30589edecc9]
 -- Delay commits on Node1
 -- Send a request and make sure it's ordered on Nodes2-4
 -- Restart Nodes2-4
 -- Start view change
 -- Make sure that view change is finished, and all nodes have equal data;;;","30/Oct/19 12:15 AM;VladimirWork;Build Info:
indy-node 1.11.0~dev1119

Steps to Validate:
1. Delay commits on Node1.
2. Send a request and make sure it's ordered on Nodes2-4.
3. Restart Nodes2-4.
4. Start view change.
5. Make sure that view change is finished and all nodes have equal data.

Actual Results:
All nodes are in sync at the end of the test.;;;",,,,,,,,,,,,,,,,,,,
Start View change on receiving a quorum of ViewChange messages,INDY-2236,42606,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,03/Oct/19 5:55 PM,19/Nov/19 11:44 PM,28/Oct/23 2:47 AM,19/Nov/19 11:44 PM,,,1.12.0,,,,,0,,,,,"*Problem*
 * A node may not receive a quorum of InstanceChanges for some reasons, but the rest of the pool may start a View Change.
 * Currently the node will have to wait until the quorum of checkpoints from other nodes is received, and do a catchup. Until this is will not be able to order.
 * Old view change logic contained to start a view change on a quorum of ViewChangeDone messages (`_next_view_indications `). We need to so something similar with a new view change protocol.

*Acceptance Criteria*
 * Start a view change on a quorum (F+1?) of ViewChange messages from other nodes
 * Add tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40a",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"13/Nov/19 11:30 PM;anikitinDSR;h1. PoA

Need to add logic for starting view change procedure without quorum of InstanceChange messages but by quorum of ViewChange msgs.
 The main reason is that by some reason we may not to receive the needed count of IC but rest of pool may already start View Change procedure. In that case, node will be out of ordering and will be waiting for catchup by checkpoints (2 unstable checkpoints for 100 batches each).
h2. Steps to implementation
 # Need to define quorum of ViewChange messages for forced VC procedure starting. There are 2 options here:
 Option 1. We can use the same quorum as for ViewChange (n - f) in normal way and don't change NewView message validation. But there is a case, when node don't receive enough count of ViewChange and it will pass this VC either.
 Option 2. We can use for this situation quorum F + 1, but take into account that NewView validation process should be changed too.

            Option 1 takes less time for implementation, but in some cases cannot solve the problem.

          2. After receiving quorum of ViewChange messages, view change process can be continued in a normal way (if proposed view no more than current).

          3. Need to write integration and simulation tests and make sure that all of current tests are passed.;;;","19/Nov/19 11:28 PM;anikitinDSR;PRs:
 * indy-plenum  [https://github.com/hyperledger/indy-plenum/pull/1401]
 * indy-node       [https://github.com/hyperledger/indy-node/pull/1511]

Implementation is placed in versions:
 * indy-plenum    1.12.0.dev955
 * indy-node         1.12.0.dev1133;;;",,,,,,,,,,,,,,,,,,,,
Start catch-up before processing NewView if the node is behind the NewView's checkpoint,INDY-2237,42607,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,03/Oct/19 5:57 PM,08/Jan/20 5:55 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Problem*
 * According to Fig 4 in [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf,] we select the highest Checkpoint that has a weak certificate (F+1 nodes) for NewView.
 * The requests will be re-ordered only after this checkpoint
 * So, there can be N-F-1 nodes that can not re-order requests (and since view change will not finish correctly) because they haven't ordered yet all requests before the chosen in NewView checkpoint
 * As a workaround, we now require a strong certificate (N-F nodes) when calculating the checkpoint (see INDY-2231). This may increase the number of requests to be re-ordered.
 * If a new Primary doesn't have the selected checkpoint, than according to INDY-2230 we just don't send a NewView and will wait till timeout to select the next Primary

*References*
 * [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf]
 * INDY-2231
 * INDY-2230

*The fix*

Quote from `New-View Message Processing` in [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf]
{quote}It obtains any requests in X that it is missing and if it does not have thecheckpoint with sequence numberh, it also initiates the protocol to fetch themissing state
{quote}
So, we need to start a catch-up till the checkpoint in NewView before going forward.

*Acceptance criteria*
 * Start a catchup if the node receives a NewView and doesn't have a checkpoint selected in NewView

 * 
 ** F+1 nodes are enough to finish the catch-up
 * Do not finish View Change (wait_for_new_view should still be True) until this catch-up is finished
 * Catch-up needs to be done till checkpoint from NewView
 * Go back to weak certificate in `NewViewBuilder`'s `calc_checkpoint`

 ",,,,,,,,,,,,,,,,,,,,,INDY-2230,INDY-2231,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ibr",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Persist 3PC messages during Ordering,INDY-2238,42608,,Task,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,03/Oct/19 6:24 PM,25/Nov/20 10:00 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Problem*
 * Let's consider the following example:

 ** All 4 nodes prepared a batch with ppSeqNo=1
 ** All nodes sent commits, but only the 1st Node got then (and possibly committed/ordered the batch)
 ** Other 3 nodes stopped before receiving Commits
 ** Only 2 nodes are restarted
 ** After restart the 1st Node has a Batch with ppSeqNo=1 in its preprepared and prepared queues.
 ** Other 2 nodes don't have it in preprepared and prepared queues (and INDY-2235 will not help to restore it since it hasn't been ordered yet).
 ** The 3d Node is still stopped.
 ** View Change is started
 ** View Change will not be finished, since according to Fig4 from [https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/p398-castro-bft-tocs.pdf,] decision procedure can not be complete.
 ** The problem that we don't have quorums to complete the decision procedure:
 *** F=1 so we can not trust the 1st Node that committed the batch
 *** But if we finish the View Change without taking into account the fact that 1st Node may potentially commit the batch, the 1st Node's ledger will be different from others forever

*Acceptance criteria*
 * Persist all 3PC messages before sending them to others (or before storing them to preprepared and prepared)
 * Restored stored 3PC messages (put to preprepared and prepared) when node starts up (after catch-up is finsihed).
 * Write tests (or unskip existing tests showing the problem)",,,,,,,,,,,,,,,,,,,INDY-472,,,,,INDY-2235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwyi",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,donqui,,,,,,,,,,"27/Oct/19 9:44 PM;donqui;A test for this issue that one can develop against:
[https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change_with_delays/test_view_change_with_delayed_commits_on_all_but_one_node_and_restart_of_those_nodes.py];;;",,,,,,,,,,,,,,,,,,,,,
Make sure that only valid OldViewPrePrepares are processed,INDY-2239,42610,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,Toktar,ashcherbakov,ashcherbakov,03/Oct/19 9:07 PM,07/Nov/19 2:19 PM,28/Oct/23 2:47 AM,07/Nov/19 2:19 PM,,,1.12.0,,,,,0,,,,,"*Problem*
 * When a replica doesn't have a PrePrepare associated with a batch from NewView (that is a PrePrepare to be re-ordered), it requests it from other nodes.
 * We don't have digital signatures for messages, so we can not trust that a PrePrepare for view X returned by a non-Primary for view X is valid.
 * We can not ask only a primary for view X (previous view) in this case since it can be malicious or offline (that's why we did view change).
 * So, we are asking all nodes
 * As of now, we get an old view PrePrepare from any Node, and try to apply it, which may lead to some problems (raising Suspicious on a new primary which can be honest) if this is not the original PrePrepare we are looking for

*Acceptance criteria*
 * Write an integration test reproducing the issue:

 ** Delay PrePrepares on 1 Node (without processing)
 ** Delay receiving of OldViewPrePrepareRequest on all nodes but 1
 ** Patch OldViewPrePrepareRequest  processing on the node it isn't delayed by returning an invalid PrePrepare
 ** Start a view change
 ** Make sure it's finished (NewView is sent)
 ** Make sure that the lagging node received OldViewPrePrepareReply from the malicious node
 ** Reset delay for OldViewPrePrepareRequest  on other nodes
 ** make sure that the lagging node was able to re-order everything
 ** make sure the pool is functional
 * Analyze the issue (if it exists and how severe it is)
 * Do a fix if needed
 ** Wait for F+1 equal OldViewPrePrepareReply for example",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vjm",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,"07/Nov/19 2:19 PM;Toktar;Task is done in the PR [https://github.com/hyperledger/indy-plenum/pull/1394]
But the merge of this PR does not make sense, since it will be overwritten by changes in the task INDY-2285;;;",,,,,,,,,,,,,,,,,,,,,
Limit the number of pre-upgrade backups,INDY-2240,42643,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,06/Oct/19 4:52 AM,06/Oct/19 4:52 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,We should limit the number of pre-upgrade backups to the last 1 or 2 and remove elder ones because they are created after each upgrade and take too much disk space.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i011lf:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove primaries field from audit ledger,INDY-2241,42651,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,,sergey.khoroshavin,sergey.khoroshavin,08/Oct/19 2:42 AM,24/Oct/19 4:54 PM,28/Oct/23 2:47 AM,24/Oct/19 4:54 PM,,,,,,,,0,,,,,"* Do View Change on every change of N
 * Simplify the formula for primary calculation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c99",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tools for efficiently searching the ledger,INDY-2242,42684,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,09/Oct/19 5:08 PM,09/Oct/19 5:08 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-14,,Ledger Searching,To Do,No,,Unset,No,,,"1|i011sz:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change the way we handle messages coming from removed remotes,INDY-2243,42685,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,donqui,donqui,09/Oct/19 5:38 PM,10/Oct/19 12:18 AM,28/Oct/23 2:47 AM,,,,,plenum,,,,0,,,,,"*Problem description*

When a node is demoted it still has a established zmq connection to all other nodes. If a connection to one or more nodes is lost demoted node tries to reconnect but never manages to confirm that the connection is really established. This is because it does not receive *pongs* for his *pings* as other nodes do not respond to messages from the demoted nodes (they are dropped in batched.py). As the demoted node does not receive the pong for it's ping it assumes that it was a network issue and tries to reconnect indefinitely.

This leads to:
 # Unnecessary traffic because of constant pings sent by the demoted node in a desperate attempt to reconnect to the lost remote (nodes that do not get pongs try to ping forever)
 # CPU cycles spent on handling these messages by the participating nodes (nodes are acting on the ping and generating a pong that is being dropped by batched.py)
 # Logs constantly getting filled by lines saying that we got a message from a node that for some reason is no longer participating in the pool (Node has removed rid)

Snippets of logs showing this behaviour are bellow:

*Node3 (demoted node):*
{code:bash}
$ tail -f /var/log/indy/sandbox/Node3.log
2019-10-08 13:12:44,048|DEBUG|node.py|Node3 sending message MESSAGE_REQUEST{'params': {'ledgerId': 0}, 'msg_type': 'LEDGER_STATUS'} to 2 recipients: ['Node2', 'Node1']
2019-10-08 13:12:44,049|TRACE|batched.py|Node3 sending msg b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node2
2019-10-08 13:12:44,049|TRACE|zstack.py|Node3 transmitting message b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node2 by socket 178 46152880
2019-10-08 13:12:44,050|WARNING|zstack.py|Remote Node2 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings
2019-10-08 13:12:44,050|TRACE|batched.py|Node3 sending msg b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node1
2019-10-08 13:12:44,050|TRACE|zstack.py|Node3 transmitting message b'{""op"":""MESSAGE_REQUEST"",""params"":{""ledgerId"":0},""msg_type"":""LEDGER_STATUS""}' to Node1 by socket 184 46220592
2019-10-08 13:12:44,050|WARNING|zstack.py|Remote Node1 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings
2019-10-08 13:12:45,772|DEBUG|kit_zstack.py|Node3 matched remote Node4 HA(host='10.0.0.5', port=9707)
2019-10-08 13:12:45,772|DEBUG|kit_zstack.py|Node3 matched remote Node2 HA(host='10.0.0.3', port=9703)
2019-10-08 13:12:45,772|DEBUG|kit_zstack.py|Node3 matched remote Node1 HA(host='10.0.0.2', port=9701)
2019-10-08 13:12:45,773|DEBUG|zstack.py|Node3 pinged Node2
2019-10-08 13:12:45,773|DEBUG|zstack.py|Node3 pinged Node1
2019-10-08 13:12:45,774|TRACE|kit_zstack.py|Node3 next check for retries in 2.00 seconds
2019-10-08 13:12:45,789|TRACE|batched.py|Node3 sending msg b'pi' to Node2
2019-10-08 13:12:45,789|TRACE|zstack.py|Node3 transmitting message b'pi' to Node2 by socket 178 46152880
2019-10-08 13:12:45,791|TRACE|batched.py|Node3 sending msg b'pi' to Node1
2019-10-08 13:12:45,792|TRACE|zstack.py|Node3 transmitting message b'pi' to Node1 by socket 184 46220592
{code}

*Node1 (participating node):*
{code:bash}
$ tail -f /var/log/indy/sandbox/Node1.log | grep 'removed rid'
2019-10-08 13:10:55,422|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:10:57,424|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:10:59,483|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:11:01,455|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:11:03,457|WARNING|batched.py|CONNECTION: Node1 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
{code}
 
*Node2 (participating node):*
{code:bash}
$ tail -f /var/log/indy/sandbox/Node2.log
2019-10-08 13:13:29,949|TRACE|zstack.py|Node2 got ping from 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:29,949|DEBUG|zstack.py|Node2 ponged 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:29,950|WARNING|batched.py|CONNECTION: Node2 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:29,951|DEBUG|message_processor.py|Node2 discarding message deque([b'po']) because CONNECTION: rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN no longer available
2019-10-08 13:13:30,842|TRACE|has_action_queue.py|Node2 running action flush_metrics with id 879
2019-10-08 13:13:30,843|TRACE|has_action_queue.py|Node2 scheduling action flush_metrics with id 882 to run in 10.0 seconds
2019-10-08 13:13:30,843|TRACE|has_action_queue.py|Node2 running action checkPerformance with id 878
2019-10-08 13:13:30,843|TRACE|node.py|Node2 checking its performance
2019-10-08 13:13:30,843|TRACE|node.py|Node2 ordered no new requests
2019-10-08 13:13:30,844|TRACE|has_action_queue.py|Node2 scheduling action checkPerformance with id 883 to run in 10 seconds
2019-10-08 13:13:31,242|DEBUG|kit_zstack.py|Node2 matched remote Node1 HA(host='10.0.0.2', port=9701)
2019-10-08 13:13:31,243|DEBUG|kit_zstack.py|Node2 matched remote Node4 HA(host='10.0.0.5', port=9707)
2019-10-08 13:13:31,243|TRACE|kit_zstack.py|Node2 next check for retries in 2.00 seconds
2019-10-08 13:13:32,007|TRACE|zstack.py|Node2 got 1 messages through listener
2019-10-08 13:13:32,008|TRACE|zstack.py|Node2 got ping from 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:32,008|DEBUG|zstack.py|Node2 ponged 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:32,009|WARNING|batched.py|CONNECTION: Node2 has removed rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN
2019-10-08 13:13:32,009|DEBUG|message_processor.py|Node2 discarding message deque([b'po']) because CONNECTION: rid 6KTs7Q9Lng5uX6oWCkVifiJ6hSpkdHiRijAsXtAunnGN no longer available
{code}


*Proposed Solution*
 # Move the ping-pong handling logic to a higher part of the stack where we have the knowledge of an exact status of a node that is communicating with us - now it is handled by zstack. This way we can know if a node trying to connect is demoted, malicious, or something else and act accordingly.
# Log IP addresses, content and other valuable information of requests coming from nodes that should not be communicating with the pool. This will allow for automation of the blacklisting process (fail2ban), and better better understanding of the problems.
# Respond with a special message (poison pill) to demoted/removed nodes that will tell them `You cannot participate in the pool, please shut-down`. After the promotion the node can be started, establish the connection, catch-up, and start participating in the network.

*Acceptance Criteria*
* Nodes receiving the requests should not act on the messages but should instead drop them immediately.
* Nodes that are trying to reconnect to the pool which they are not a part of any-more should not do that indefinitely.
* Change the log messages so that they contain good descriptions of the problem. This will help operators in identifying and understanding the problem.
* Change the documentation for the promotion/demotion process",,,,,,,,,,,,,,,,,,,,,INDY-2172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i011t7:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),donqui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PBFT View Change Debug: Part 2,INDY-2244,42700,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,10/Oct/19 4:13 PM,01/Nov/19 4:31 PM,28/Oct/23 2:47 AM,25/Oct/19 8:45 PM,,,1.11.0,,,,,0,,,,,"* Make sure that all the tests are unskipped - {color:#0747a6}DONE{color}
 * Finish cleanup
 ** Do we need `self.schedule_view_change_completion_check(self._view_change_timeout)` in `on_view_change_start` - [DONE|https://github.com/hyperledger/indy-plenum/pull/1371]
 ** Do we need `replica.on_view_change_start()` - [DONE|https://github.com/hyperledger/indy-plenum/pull/1371]
 ** Remove `notify_view_change_complete` - [DONE|https://github.com/hyperledger/indy-plenum/pull/1371]
 ** Do we need `primaryChanged` in replica - TBD
 * Do more load testing
 * Improve simulation tests if needed
 * Fix the issue with primary selection when Node txns are in flight IN PROGRESS [~anikitinDSR]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ki",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"25/Oct/19 8:44 PM;ashcherbakov;What has been done:
* More cleanup: https://github.com/hyperledger/indy-plenum/pull/1371
* Fix the issue with primary selection when Node txns are in flight: https://github.com/hyperledger/indy-plenum/pull/1381 - In progress, will be finished in INDY-2267
* More tests unskipped
* A new issue is found (rather an edge case): https://jira.hyperledger.org/browse/INDY-2262
* A lot of load tests are one
** Some results are good
** Latest results are no so good; INDY-2268 is created for this;;;",,,,,,,,,,,,,,,,,,,,,
Improve the upgrade rollback process,INDY-2245,42701,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,donqui,donqui,10/Oct/19 5:15 PM,10/Oct/19 5:15 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Problem description*

Rollback script does not remember versions of packages installed on the system, but it depends on strict versions of dependencies specified in the packages it tries to rollback.

This may lead to some problems during the rollback process, when some packages were manually updated, when some packages are removed (like sovtoken), when we are dealing with older packages that do not have strict dependencies, or when some newer packages were installed during upgrade but the upgrade failed after that.

*Acceptance Criteria*

Explicitly save versions of all packages, taking the snapshot of a state of the system at the moment of the upgrade and revert to that state",,,,,,,,,,,,,,,,,,,,,INDY-2161,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i011wj:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),donqui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 1.11.0,INDY-2246,42710,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,10/Oct/19 10:34 PM,01/Nov/19 7:34 PM,28/Oct/23 2:47 AM,01/Nov/19 7:34 PM,,,1.11.0,,,,,0,,,,,"*Release Goal*
Release everything that is ready. Especially
 * Improved view change algorithm

*Acceptance Criteria*
Regular instructions:
 * Code is tested
 * Review upstream releases to decide what should be included in this build
Build is produced with automated CI / CD
Official build is tagged as released in Git
Official builds should be the only builds that are not marked in GitHub as ""pre-release""
Prepare basic documentation on new features
Prepare the Release Notes
Link to the documentation on new features
Stored as a CHANGELOG.md in the root of the repo
Latest release at the top, above the release notes for all previous versions
The release should be on GitHub “Releases” tab.
Release notes should be in the “Description"" field of the GitHub release artifact
The release is marked in JIRA
Add the correct fixVersion to included issues
Enable others to use the release:
Email stakeholders with links to the artifacts and release notes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vj9",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"31/Oct/19 5:07 PM;VladimirWork;*Version Information*
indy-node 1.11.0
indy-plenum 1.11.0
sovrin 1.1.60

*Notices for Stewards*
Please be careful with demoting/promoting/adding nodes (see Known Issues for details).
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.

*Major Changes*
- Switch to PBFT View Change protocol
- Stability fixes

*Detailed Changelog*

+Major Fixes+
INDY-2222 - One node doesn't catch up after promotion
INDY-2248 - A Replica may process messages from other Replicas
INDY-2268 - Up to F nodes are out of consensus after >3 hours of load
INDY-2274 - A Node may not connect to another Node after restart
INDY-2247 - Two View Changes happen during master or backup primary demotion

+Changes and Additions+
INDY-2140 - Debug: Integrate PBFT viewchanger service into current codebase
INDY-2178 - Request missing ViewChange messages when receiving NewView
INDY-2223 - Basic integration tests with a new View Change protocol need to pass
INDY-2224 - Recover from a situation when View Change is finished on >= N-F of other nodes
INDY-2230 - A Primary lagging behind a stable chedkpoints should not send NewView
INDY-2231 - Do not stabilize checkpoint after the view change if a Replica doesn't have this checkpoint
INDY-2235 - Save PrePrepare's BatchID in audit ledger and restore list of preprepares and prepares on node startup
INDY-2244 - PBFT View Change Debug: Part 2
INDY-2257 - Optimize Propagate logic

+Known Issues+
If there are NODE txns for adding/removing nodes interleaved with View Changes (not any view changes, but a specific subset), then either up to F or all Nodes may not be able to finish view change. Please see the details and conditions when it may happen in INDY-2262.
INDY-2262 - All nodes need to select the same primary during view change
INDY-2275 - A Node missing a View Change may not be able to finish it if NODE txns have been sent
INDY-2276 - A new node joining the pool during the view change may not be able to start ordering immediately;;;",,,,,,,,,,,,,,,,,,,,,
Two View Changes happen during master or backup primary demotion,INDY-2247,42716,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,VladimirWork,VladimirWork,11/Oct/19 1:01 AM,24/Oct/19 3:56 PM,28/Oct/23 2:47 AM,24/Oct/19 3:55 PM,,,1.11.0,,,,,0,,,,,"Build Info:
indy-node 1.10.0~dev1094

Steps to Reproduce:
1. Stop primary (and ensure that primary has changed).
2. Start ex-primary and demote backup primary (and ensure that primary has changed).
3. Promote demoted node (and ensure that primary has changed).

https://github.com/hyperledger/indy-test-automation/blob/f3c0c75e04f416a81f22025146bc5ca9b1e2f718/system/indy-node-tests/TestAuditSuite.py#L153

Actual Results:
Primary didn't change in Step 3 (but before PBFT this test passed) for master and backup primary cases (params 0 and 1).
We have View 3 at the end for all 3 cases: master primary, backup primary, non-primary demotion/promotion.
Primaries changing during all cases:
{noformat}
param 0 (master primary):
Primary at the beginning is 1
Primary after service stop is 2
Primary after service start is 2
Primary before demotion is 2
Primary after demotion is 4

param 1 (backup primary):
Primary at the beginning is 1
Primary after service stop is 2
Primary after service start is 2
Primary before demotion is 2
Primary after demotion is 4

param 5 (non-primary):
Primary at the beginning is 1
Primary after service stop is 2
Primary after service start is 2
Primary before demotion is 2
Primary after demotion is 3
Primary after promotion is 4
{noformat}
Before PBFT changes all cases passed like the last one now.
",,,,,,,,,,,,,,,,,,,,,INDY-2023,,,,,,,,,,"11/Oct/19 1:01 AM;VladimirWork;audit_vc_issue_01_10_2019.tar.gz;https://jira.hyperledger.org/secure/attachment/17873/audit_vc_issue_01_10_2019.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49k",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"23/Oct/19 2:35 AM;Toktar;PoA:
Propose a new view change if selected master primary is same with previous one
PR: [https://github.com/hyperledger/indy-plenum/pull/1379];;;","23/Oct/19 11:40 PM;Toktar;*Problem reason:*
 - If we demote and promote a node which change f and standing before the current primary in historical order, this triggers a view change and a new master primary will same with the previous.

*Changes:*
 - In case when new maser primary node is same with the previous, trigger new View change.

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1483]
 * [https://github.com/hyperledger/indy-plenum/pull/1379]

*Version:*
 * indy-node 1.11.0.dev1115 -master
 * indy-plenum 1.11.0.dev939 -master

*Risk factors:*
 - Problems with ViewChange

*Risk:*
 - Low

*Tests:*
 * [test_promotion_before_view_change.py|https://github.com/hyperledger/indy-plenum/pull/1379/files#diff-d3e16825f968132a181d5fe835d3812b] 

*Recommendations for QA:*
 * The system test test_case_demote_master_backup_non_primary should pass;;;","24/Oct/19 3:55 PM;ashcherbakov;The test passes now;;;",,,,,,,,,,,,,,,,,,,
A Replica may process messages from other Replicas,INDY-2248,42722,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,11/Oct/19 5:36 PM,22/Oct/19 5:35 PM,28/Oct/23 2:47 AM,15/Oct/19 10:01 PM,,,1.11.0,,,,,0,,,,,"*General Issue*
 * The node that was demoted/promoted may have less replicas than others for some period of time
 * If the Node receives a 3PC message with an InstID for an unknown replicas, it's propagated to all Replicas according to `pass_message` in `Replicas`
 * Ordering Service doesn't check for instId when processing 3PC messages

*Specific failure:*

In particular, it led to the following in [https://build.sovrin.org/job/indy-node/job/indy-node-nightly/130/testReport/junit/system.indy-node-tests/test_vc/test_vc_py___Run_test_vc_py____2__Upload_test_report___test_vc_by_demotion_last/]
 * A node was demoted and then promoted
 * It still has 2 Replicas
 * It has PrePrepare ppSeqNo=1 for master stored
 * It receives PrePrepare ppSeqNo=1 for instId=2 (3d Replica)
 * PrePrepare is propagated to all nodes including master
 * PrePrepare for Backup replica overrides existing correct PrePrepare for master for ppSeqNo=1
 * The node does a View change
 * The node need to re-order a batch with ppSeqNo=1
 * It get a Commit ppSeqNo=1
 * The corresponding PrePrepare is now Backup's one (incorrectly overridden)
 * BLS signature is verified: backup PrePrepare doesn't have poolRootHash, and we fail with ""TypeError: a bytes-like object is required (also str), not 'NoneType'""

*Acceptance Criteria*
 * Do not propagated 3pc messages for unknown replicas to all replicas
 * Validate 3PC message instanceId in Ordering service
 * Write tests

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49k9",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"14/Oct/19 5:59 PM;Toktar;PR: [https://github.com/hyperledger/indy-plenum/pull/1372];;;","15/Oct/19 1:19 AM;Toktar;*Problem reason:*
 - A message for unknown replica sends to all replicas

*Changes:*
 - stash a message for future replica and don't send it
 - don't send messages for unknown replicas
 - send mesages without instance id to all replicas

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1372]
 * [https://github.com/hyperledger/indy-node/pull/1474]

*Version:*
 * indy-node 1.11.0.dev1106 -master
 * indy-plenum 1.11.0.dev929 -master

*Risk factors:*
 - Problems with a view change

*Risk:*
 - Low

*Tests:*
 * message_request_service:
 ** test_process_message_req_incorrect_inst_id
 ** test_process_missing_message_incorrect_inst_id
 ** test_process_message_rep_invalid_inst_id
 * ordering_service
 ** test_discard_incorrect_inst_id
 ** test_discard_old_view_pp_req_with_incorrect_inst_id
 ** test_discard_old_view_pp_rep_with_incorrect_inst_id
 * node
 ** test_send_message_without_inst_id_to_replica
 ** test_send_message_to_one_replica
 ** test_send_message_to_incorrect_replica
 ** test_send_message_for_all_without_inst_id

*Recommendations for QA:*
 *  system.indy-node-tests.test_vc.test_vc_by_demotion_last should pass;;;","15/Oct/19 10:01 PM;VladimirWork;Build Info:
indy-node 1.11.0.dev1106
indy-plenum 1.11.0.dev929

Steps to Validate:
1. Run VC and consensus tests with check_no_failures fixture.

Actual Results:
All tests passed without tracebacks in journalctl.;;;",,,,,,,,,,,,,,,,,,,
Tools to help with System Administration,INDY-2249,42723,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,11/Oct/19 6:51 PM,11/Oct/19 7:00 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-8,,Admin Tools,To Do,No,,Unset,No,,,"1|i0120j:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove the need for RBFT replicas,INDY-2250,42724,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,11/Oct/19 6:56 PM,20/Dec/19 11:21 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Why:
* RBFT is much slower (N^3 vs N^2) than other PBFT-like protocols as it runs F+1 instances of the Protocol
* RBFT benefit (more sensitive to malicious Primary and slower performance) is not so critical in INDY cases. Probability of malicious Primary (degrading performance) is not so high in Permissioned networks. And it can be caught by not so expensive ways.
* Plenum doesn't fully implement all RBFT assumptions. Implementing a real RBFT (really independent replicas) requires significant effort
* A lot of issues in Plenum are related to processing of requests between protocol instances. If we have just 1 instance, the code would be much easier, cleaner and stable
* As we implement PBFT View Change, moving to Aardvark (https://www.usenix.org/legacy/events/nsdi09/tech/full_papers/clement/clement.pdf) looks pretty easy and straightforward. 
* Aardvark has regular view changes where the time each node can be a primary correlates with the node's performance. 

=>
We propose moving to Aardvark in order to
1) Improve Stability and fix a lot of stability issues by this
2) Improve Performance


",,,,,,,,,,,,,,,,,,,,,INDY-2251,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-10,,Remove Replicas,To Do,No,,Unset,No,,,"1|i0120r:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with the RBFT replicas,INDY-2251,42725,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,11/Oct/19 8:50 PM,21/Nov/19 3:22 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-12,,Replica Fixes,To Do,No,,Unset,No,,,"1|i0120z:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improving efficiency of BLS signature verification,INDY-2252,42747,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,lovesh,lovesh,15/Oct/19 3:39 AM,05/Nov/19 6:54 PM,28/Oct/23 2:47 AM,,,,,plenum,,,,0,performance,,,,"This ticket is motivated by the assumption that BLS signing is done by more computationally powerful machines but the verification needs to be done by less powerful machines (clients) as well hence signature size and speed can be traded-off for faster verification. Also, this change will make current signatures incompatible with future signatures so this ticket should be considered when migration is done (for the curve).

Currently, we have signatures in group G1 (and hence messages are also hashed in G1) whereas the verification key is in G2. This makes the signing and aggregation fast and keeps signatures short but it makes the verification more expensive since operations are more expensive in G2 compared to G1. But this situation can be reversed by keeping signatures in G2 and the verification keys in G1. The signing, verification and aggregation algorithms don't have to change. Also, this has to be supported in the underlying crypto library so a compile time flag can be used to configure such behavior, [an example|https://github.com/lovesh/signature-schemes/tree/master/bls]",,,,,,,,,,,,,,,,,,,,,INDY-2280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,,Unset,,INDY-775,,,No,,Unset,No,,,"1|i0125n:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),lovesh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make all catchup system tests pass,INDY-2253,42768,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,16/Oct/19 5:56 PM,25/Oct/19 9:54 PM,28/Oct/23 2:47 AM,24/Oct/19 11:13 PM,,,,,,,,0,,,,,Please take into account findings from INDY-2222,,,,,,,,,,,,,,,,,,,,,INDY-2222,INDY-2261,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i011bk:",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"17/Oct/19 12:34 AM;VladimirWork;Done in https://github.com/hyperledger/indy-test-automation/pull/65, need to check it in CD because there are still intermittent catchup failures even with 200 extra txns written in the end of each test.;;;","17/Oct/19 12:36 AM;VladimirWork;Also we have catchup failure with forcing catchup with 200 extra txns in the latest nightly pipeline: https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-nightly/detail/indy-node-nightly/135/tests -> TestCatchUpSuiteExtended.py / Run TestCatchUpSuiteExtended.py / [8] Upload test report / test_case_config_ledger[False-55-200] – system.indy-node-tests.TestCatchUpSuiteExtended.TestCatchUpSuiteExtended;;;","17/Oct/19 11:10 PM;VladimirWork;Build Info:
indy-node 1.11.0~dev1107
plugins 1.0.4~dev105

Steps to Reproduce:
https://github.com/hyperledger/indy-test-automation/blob/0779b086b45dbbe6aa4e1a150b980bff644faf1e/system/indy-node-tests/TestCatchUpSuite.py#L18
(we checked all nodes reachability (True) and sent 200 extra txns to force catchup)

Actual Results:
Node9 has 12 domain and 1 audit txn in the end of test instead of 612 and 611 respectively:
{noformat}
DOMAIN LEDGER SYNC: ['612', '612', '612', '612', '612', '612', '612', '612', '12']
AUDIT LEDGER SYNC: ['611', '611', '611', '611', '611', '611', '611', '611', '1']
{noformat}

Logs:
ev@evernymr33:logs/2253.tar.gz;;;","22/Oct/19 8:20 PM;ashcherbakov;Analysis result and suggestions:

*test_consensus_restore_after_f_plus_one*
* IN PROGRESS [~Toktar]

*TestAuditSuite.test_case_demote_master_backup_non_primary[0]*
* INDY-2247 [~Toktar]

*TestCatchUpSuite REQUIRE CHANGES IN TESTS*
* There are two reasons why this test is failing:
** If check_reachability=True, then it fails on checks for the Node before the last one ([-2] Node), since we don't do reachability checks for it.
** If check_reachability=False, then it may fail because we don't send enough requests to start catchup.
* Suggested fixes:
1) If check_reachability=True, call `ensure_all_nodes_online` after [-2] Node is started
2) If check_reachability=False, then there are 2 options:
O1: Send N requests; Call `ensure_all_nodes_online`; Send 200 reqs
O2: Send requests in a loop until all nodes are in the same state (do not send more than 1000).;;;","22/Oct/19 8:24 PM;ashcherbakov;*TestCatchUpSuiteExtended.test_case_token_ledger REQUIRE  CHANGES IN TESTS*
Suggested Changes: add `ensure_all_nodes_online` check right after starting the node regardless of `wait_catchup_before_ordering` flag.
;;;","22/Oct/19 9:44 PM;Toktar;*test_consensus_restore_after_f_plus_one*
Suggested Changes:  **  add catchup and reachability check after service_start for nodes Node4 and Node5 (# 5/7 online - can w+r);;;","23/Oct/19 10:18 PM;VladimirWork;Done in https://github.com/hyperledger/indy-test-automation/pull/69;;;","24/Oct/19 11:13 PM;VladimirWork;All tests have been updated, merged to master and bumped to both pipelines.;;;","25/Oct/19 9:54 PM;ashcherbakov;The reason of all failing tests now is reconnection issue. We have INDY-2261 to process this;;;",,,,,,,,,,,,,
A new node added to Sovrin MainNet failed to reach consensus,INDY-2254,42804,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,lbendixsen,lbendixsen,18/Oct/19 8:15 AM,01/Nov/19 12:36 AM,28/Oct/23 2:47 AM,01/Nov/19 12:36 AM,1.9.1,,1.12.0,,,,,0,,,,,"_Environment_: MainNet - Sovrin - 1.1.52   Node - 1.9.1

_Steps to Reproduce_: 

During the attempt to add sicpa to MainNet, here is the sequence of steps as near as I can recall:

1) Sicpa was on StagingNet, thus needed to downgrade when attempting to switch to MainNet.

2) Stopped Sicpa's indy-node service and did all of the steps to switch them to mainnet. (Downgrade occurred here) All ip addresses, keys and etc stayed the same, just got new genesis files and onboarded them.  (init-indy-node)

3) Sicpa ran the ledger node command to add themselves to MainNet.

4) (Forgot to remove Sicpa from StagingNet before adding them to MainNet) Sicpa restarted indy-node here

5) Consensus would not occur.  Several nodes would not connect (whitelist issues) including the primary, so no primary was selected either.

6) Veridium went out of consensus 

7) After running for a few hours, remembered to remove Sicpa from StagingNet

8) Restarted Sicpa and Veridium nodes (about Noon MDT on Tuesday Oct 15) 

9) Restart had no noticable effect, so I had Sicpa remove themselves from consensus

10) Thought that maybe out of consensus was due to not being able to connect to enoug hnodes, so I had Sicpa add back in (occurred early Wednesday)

11) IceNode went out of Consensus

12) Fearing that nodes went out because Sicpa added in, I hed them remove themselves again. they removed themselves Wednesday between ~8-10A MDT

13) Icenode regained consensus, Veridium did not.

 

_Expected Behavior_:  

Node should recover when conditions are corrected? (My bad for not removing them from StagingNet where Sicpa was sort of on 2 networks at once?)

_Observed Behavior_: 

Node has not recovered.  Will attempt removing data directory to reset ledgers and try again soon (after Mondays MainNet upgrade)

_Notes_:

Logs from Sicpa (noted above) (journal.log) and Ev1 (Never left consensus) are included below.  Will add logs for Veridium (still out of consensus) when received. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/19 8:17 AM;lbendixsen;ev1_journalctl.tar;https://jira.hyperledger.org/secure/attachment/17891/ev1_journalctl.tar","18/Oct/19 8:24 AM;lbendixsen;ev1_logs.tgz;https://jira.hyperledger.org/secure/attachment/17892/ev1_logs.tgz","18/Oct/19 8:16 AM;lbendixsen;journal.log;https://jira.hyperledger.org/secure/attachment/17890/journal.log","18/Oct/19 8:26 AM;lbendixsen;sicpa.tar.bz2;https://jira.hyperledger.org/secure/attachment/17893/sicpa.tar.bz2",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vji",,,,Unset,Unset,Ev-Node 19.21,Ev-Node 19.22,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,lbendixsen,Toktar,,,,,,,,,"30/Oct/19 5:38 AM;esplinr;[~Toktar]: would you provide an update on your progress here? Do you have sufficient information to analyze the issue?;;;","31/Oct/19 10:29 PM;Toktar;We saw three potential problems considered, but only the first one ended up being real:
||Issue||Result||
|Veridium was started with incorrect genesis transactions.|Veridium has been started correctly. It is in consensus now/|
|There are two folders (VeridiumIDC
 VeridiumIDCC) on the Veridium node.|VeridiumIDC for client keys and VeridiumIDC for pool keys. It is expected|
|Node Sicpa lost consensus|Due to the restart of the nodes, a view change happened and spent a couple of minutes. This could be seen as a loss of consensus.
 The good news, a new PBFT view change takes no more than 30 seconds.|;;;",,,,,,,,,,,,,,,,,,,,
Remove all backup Replicas,INDY-2255,42905,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Oct/19 6:16 PM,20/Dec/19 11:54 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Only master Replica is needed in Aardvark protocol (https://www.usenix.org/legacy/events/nsdi09/tech/full_papers/clement/clement.pdf)

Acceptance Criteria:
* A flag to turn off all Backup Replicas
* A build for testing
* Tests adaption and code cleanup will be continued in INDY-2260
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49j",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do regular view changes,INDY-2256,42906,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Oct/19 6:17 PM,20/Dec/19 11:54 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"* Do regular view changes as required by Aardvark protocol (https://www.usenix.org/legacy/events/nsdi09/tech/full_papers/clement/clement.pdf)
* Implement this in a way to turn off or on (off by default)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ir",,,,Unset,Unset,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize Propagate logic,INDY-2257,42907,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,21/Oct/19 7:10 PM,25/Oct/19 4:47 PM,28/Oct/23 2:47 AM,25/Oct/19 4:47 PM,,,1.11.0,,,,,0,,,,,"Do not send as many propagates as of now.
* Primary still uses finalized requests to be included into PrePrepare
* Non-primaries process requests from PrePrepare if they are aware of it (either received a request or at least one Propagate). This is a step towards Aardvark protocol. Also we have digital signatures, so are sure requests came from the clients. In order to check that Primary doesn't skip any requests, other means and checks are needed in any case (regardless if this is RBFT or Aardvark), and this has nothing to do with finalized requests.
* Return even non-finalized requests when processing Propagate MessageReq

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|i011bl:",,,,Unset,Unset,Ev-Node 19.21,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"22/Oct/19 1:02 AM;ashcherbakov;This may be partially done in https://github.com/hyperledger/indy-plenum/pull/1377;;;","22/Oct/19 4:12 PM;ashcherbakov;Changes:
* As in description

PR:
* https://github.com/hyperledger/indy-plenum/pull/1377

Risk:
* Medium

Build:
* indy-node 1.11.0.dev1112

Recommendation for QA:
* load test 10 txns per sec
;;;","25/Oct/19 4:47 PM;ashcherbakov;Looks like no issues with this.;;;",,,,,,,,,,,,,,,,,,,
Get rid of Propagates,INDY-2258,42909,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Oct/19 9:11 PM,20/Dec/19 11:21 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"We may get rid of Propagates at all once move to Aardvark:
Primary can send the whole req instead of just reqId

Propagate phase is one of the bottlenecks, so we may see performance improvements if remove it.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c98ri",,,,Unset,Unset,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update Documentation with a new Consensus Protocol,INDY-2259,42935,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,22/Oct/19 5:37 PM,20/Dec/19 11:54 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49jr",,,,Unset,Unset,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debug move to Aardvark: Phase 1,INDY-2260,42936,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,22/Oct/19 5:42 PM,20/Dec/19 11:54 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49j9",,,,Unset,Unset,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Audit manual reconnection logic in ZStack ,INDY-2261,42960,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,donqui,ashcherbakov,ashcherbakov,23/Oct/19 10:16 PM,11/Nov/19 3:50 PM,28/Oct/23 2:47 AM,11/Nov/19 3:50 PM,,,1.12.0,,,,,0,,,,,"*Issue (see INDY-2274)*
* System tests doing restart fail intermittently
* Some nodes can not join the pool if they restart during the load 

Zstack tries to do handle reconnections manually by using ping/pongs and reconnecting sockets. It looks like that may lead to issues. Moreover, it looks like ZMQ is able to handle reconnection by it own.

*Acceptance Criteria*
* Prove that the current logic is really needed, or prove that we can live without it
* If latter then
** Get rid of ping/pongs
** Assume that we connected to a Node once we receive any message from it
** Still use monitor socket to detect disconnections (it's needed to initiate view change when Primary is disconnected) ",,,,,,,,,,,,,,,,,,,,,,,,INDY-2253,INDY-2289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969vju",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,donqui,,,,,,,,,,"26/Oct/19 12:33 AM;ashcherbakov;*PoA*
 * Check if we really need manual reconnections in ZStack. Maybe ZMQ can handle reconnections by its own?
 ** Use separate Contexts in Integration tests (see https://github.com/hyperledger/indy-plenum/pull/1387)
 ** Try to run system tests not reconnecting sockets (see RETRY_SOCKET_RECONNECT flags in https://github.com/hyperledger/indy-plenum/pull/1387)
 ** Try to run system tests not send ping/pongs (see RETRY_CONNECT flags in https://github.com/hyperledger/indy-plenum/pull/1387)
 ** Create a new system ZMQ tests (not related to Plenum) with client and server in different processes and check if we have any issues with disconections there;;;","08/Nov/19 1:01 AM;donqui;Curve Auth causes not issues
 Linger options cause no issues
 Keep Alive options cause no issues
All other socket options are also ok

The problem seems to be caused by the monitoring sockets, this method specifically:
{code:java}
    @staticmethod
    def _get_monitor_events(socket, non_block=True):
        # It looks strange to call get_monitor_socket() each time we
        # want to get it instead of get it once and save reference.
        # May side effects here, will create a ticket to check and clean
        # up the implementation.
        return []
        monitor = socket.get_monitor_socket()
{code}
when it is short circuited (notice the return [] as the first statement) ZMQ level re-connects work fine, if the 
{code:java}
monitor = socket.get_monitor_socket()
{code}
line is called and return is after that, ZMQ level re-connection does not work.

Next steps are to investigate why this is causing the issue, and see what our possible alternatives could be.;;;","08/Nov/19 8:29 PM;donqui;Test conducted with test scripts shows that when monitor socket is created and connectivity is lost, after some time the zmq re-connection stops working.

Test scripts can be found here: https://github.com/donqui/zmq-sandbox

In case of plenum/node when ping-pong and socket recreation are disabled, and the connection to one remote is lost for a period of time longer than ~120seconds the entire zmq stack crashes in a way, and node becomes unresponsive.;;;","08/Nov/19 8:54 PM;donqui;While investigating I opened an issue on the PYZMQ side to see if they have see this behavior before, and if there are known solutions:
https://github.com/zeromq/pyzmq/issues/1340;;;","11/Nov/19 3:50 PM;ashcherbakov;Conclusion: it's possible to get rid of manual reconnection logic. This will be done in the scope of INDY-2289.;;;",,,,,,,,,,,,,,,,,
All nodes need to select the same primary during view change,INDY-2262,42973,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,24/Oct/19 4:10 PM,25/Nov/19 9:07 PM,28/Oct/23 2:47 AM,20/Nov/19 6:08 PM,,,1.12.0,,,,,0,,,,,"*Issue*
If there are NODE txns for adding/removing nodes interleaved with View Changes (not any view changes, but a specific subset), then either
1) Up to F Nodes may not be able to finish view change 
OR
2) All nodes will not be able to finish view change 

*Workaround*
Restart should help

*When issue is reproduced during demotion*
Let X be a demoted Node seqno (historical order of nodes how they have been added), N - number of nodes, viewNo - current viewNo.
* If  *X >= (viewNo mod N) + 3*, then no issue is possible.
* If  *X < (viewNo mod N) + 3*, then issue *may* be reproduced depending on network conditions. 

*When issue is reproduced during new Node adding*
Let N - number of nodes, viewNo - current viewNo.
* If  *N != (viewNo mod N) + 1*, then no issue is possible.
* If  *N == (viewNo mod N) + 1*, then issue *may* be reproduced depending on network conditions. 


*Example 1: F nodes out of consensus till next View Change after new Node is added*
1) The pool has 24 Nodes, and the current viewNo is 23, so that master Primary is Node24 (the last one)
2) Node25 is added (NODE txn is sent)
3) N-F Nodes applied NODE txn, and F Nodes haven't applied it yet
4) View Change is started
5) N-F Nodes selected Node25 as a Primary and F Nodes selected Node1 as a Primary
6) N-F Nodes finished view change, but F Nodes didn't 
7) If the next View Change is started, the F Nodes will be able to finish it and join consensus

*Example 2: F nodes out of consensus till next View Change after a Node is demoted*
1) The pool has 24 Nodes, and the current viewNo is 10
2) Node10 is demoted
3) N-F Nodes applied NODE txn, and F Nodes haven't applied it yet
4) View Change is started
5) N-F Nodes selected Node12 as a Primary and F Nodes selected Node11 as a Primary
6) N-F Nodes finished view change, but F Nodes didn't 
7) If the next View Change is started, the F Nodes will be able to finish it and join consensus

*Example 3: All nodes can not finish view change when adding a Node*
In Example 1, if more than F nodes have a different state than others (didn't apply NODE txn for example), then all nodes will not be able to finish the view change

*Example 4: All nodes can not finish view change when demoting a Node*
In Example 2, if more than F nodes have a different state than others (didn't apply NODE txn for example), then all nodes will not be able to finish the view change

*Example 5: No issues when a Node is demoted*
If current viewNo is 22 in Example 1, then there is no issue

*Example 6: No issues when a Node is demoted*
If Node 12 is demoted in Example 2, then there is no issue


*Problem Reason*
* As of now, nodes select a Primary at the beginning of View Change (once receive a quorum of Instance Changes).
* Primary selection depends on the current node registry state which depends on the Node's uncommitted state (Node txns to add/remove nodes in flight)
* Node may come to view change in different states
* => The may select different primaries and view change may never finish on some nods since they expect NewView from the Primary they selected

*Acceptance criteria*
* Write integration tests reproducing the issue
** Test 1 
*** Initial state: Pool with 4 Nodes, viewNo=3, currentPrimary - Node4
*** Delay X msg on Y Nodes
**** params: X = PrePrepare/Commit; Y=1 Node/2 Nodes/3 Nodes/4 Nodes
*** Add a NODE txn to add a new Node (Node5)
*** Start a View Change   
*** Make sure that it's finished on all nodes, all nodes have the same state and can order
** Test 2
*** Initial state: Pool with 5 Nodes, viewNo=3, currentPrimary - Node4
*** Delay X msg on Y Nodes
**** params: X = PrePrepare/Commit; Y=1 Node/2 Nodes/3 Nodes/4 Nodes
*** Add a NODE txn to remove Node5
*** Start a View Change   
*** Make sure that it's finished on all nodes, all nodes have the same state and can order
** Test 3
*** Initial state: Pool with 5 Nodes, viewNo=2, currentPrimary - Node3
*** Delay X msg on Y Nodes
**** params: X = PrePrepare/Commit; Y=1 Node/2 Nodes/3 Nodes/4 Nodes
*** Add a NODE txn to demote Node3
*** Make sure that view change is started and finished on all nodes, all nodes have the same state and can order
* Do a fix

*Possible fix*
* Do not commit nodeRegistry (current list of nodes used for Primary selection) till the next View Change, so that all nodes can select the same Primaries regardless of their state.
* Do view change on every change of nodeRegistry",,,,,,,,,,,,,,,,,,,,,,,,INDY-2298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i402",,,,Unset,Unset,Ev-Node 19.22,Ev-Node 19.23,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"29/Oct/19 9:15 PM;ashcherbakov;*PoA Main Idea*
* Extend Audit Ledger with the information about the current node registry
* Calculate Primaries at the beginning of new view based on the Node Reg as it was at the beginning of last view where at least 1 txn has been ordered
* Do view changes on every change of node reg
 
*PoA*
 1) Write the integration tests above *DONE*: https://github.com/hyperledger/indy-plenum/pull/1397

 2) Extend Audit txn with information about the current node registry *DONE*, https://github.com/hyperledger/indy-plenum/pull/1397
 * Create `NodeRegistryBatchHandler` and register it for Pool Ledger batches (make sure it's called before `AuditBatchHandler`) 
 ** `NodeRegistryBatchHandler` updates `ThreePcBatch` with the information about the current node registry (after applying the current Batch) as a list of Nodes
 ** It needs to track the uncommitted nodeReg and revert it properly according to the general BatchHandler API  
 * Extend `AuditBatchHandler` to put the current nodeReg there
 ** Use delta approach as for primaries if nodeReg hasn't been changed to reduce the txn size

 3) Change primary calculation to use the nodeReg as it was at the beginning of the last view where at least 1 txn has been ordered *DONE* https://github.com/hyperledger/indy-plenum/pull/1398
 * Extend `NodeRegistryBatchHandler` with methods to get
 ** uncommitted nodeReg
 ** committed nodeReg
 ** nodeReg at the beginning of the last view where at least 1 txn has been ordered
 ** update nodeReg at the beginning of the last view correctly when doing revert of uncommitted state
 * Extend PrimarySelector to use new logic: - *DONE* https://github.com/hyperledger/indy-plenum/pull/1398
 ** Inject NodeRegistryBatchHandler
 ** Use the given viewNo, nodeReg at the beginning of the last view (from NodeRegistryBatchHandler) and round robin to select primaries
 * Either remove primaries field from the Audit Ledger, or calculate it every time using PrimarySelector
 * Remove FuturePrimariesBatchHandler
 
4) Update `NodeRegistryBatchHandler` after catchup *DONE* https://github.com/hyperledger/indy-plenum/pull/1397
* Calculate current nodeReg from audit ledger DONE
* Calculate nodeReg at the beginning of the last view from audit ledger TBD
* Either get the primaries from the audit ledger (as of now), or calculate it using `PrimarySelector` (based on nodeReg at the beginning of the last view). TBD

5) Send Instance Change after a Node txn changing the number of Nodes is sent *DONE*

6) For nodes jumping from view X to view X+2 *DONE* https://github.com/hyperledger/indy-plenum/pull/1398
* They need to restore primaries at  the beginning of view X+1 from the (uncommitted) audit ledger
* They If they don't have any txns from X+1 view in audit ledger, then they should fallback to the nodeReg of the beginning of view X. 
* Another option to restore primaries for view X+1: 
** add current primary into INSTANCE CHANGE message
** Get the primaries from the quorum of INSTANCE CHANGEs (if node is doing a view change to view X+2, then it has a quorum of INSTANCE CHANGES with primaries for view X+1);;;","07/Nov/19 12:27 AM;ashcherbakov;The current work is in https://github.com/ashcherbakov/indy-plenum/tree/indy-2262;;;","15/Nov/19 9:27 PM;ashcherbakov;*Main Changes:*
* Implemented NodeRegHandler to store uncommitted and committed node regs, as well as nod regs for previous view
* Store nodeReg in audit ledger
* Use node registry as it was at the beginning of last view for Primary Selection
* Do view change on every change of node registry length (adding, promoting, demoting nodes)
* Get rid of Future Primaries Batch Handler
* Get rid of primaries in PrePrepares
* Added integration tests reproducing the issue

*Other Changes:*
* Process Checkpoints for future views to be able to caught up and cleanups of Checkpointer validator
* Simplify PrimarySelector logic
* Get rid of outdated Config parameters related to view change
* Remove outdated code and cleanup
* Some random fixes and improvements

*PRs:*
* https://github.com/hyperledger/indy-plenum/pull/1397
* https://github.com/hyperledger/indy-plenum/pull/1400
* https://github.com/hyperledger/indy-plenum/pull/1398
* https://github.com/hyperledger/indy-plenum/pull/1395

*Build:*
* indy-node 1.12.0.dev1132 
* sovrin plugins 1.0.5.dev115

*Recommendation for QA:*
* System tests for different combinations of adding/promoting/demoting nodes
* Send NODE txn one by one (waiting for Reply) as well as in batch;;;","20/Nov/19 6:08 PM;VladimirWork;Build Info:
indy-node 1.12.0~dev1132
plugins 1.0.5~dev115

Steps to Validate:
1. Run TestProductionSuite.
2. Run https://github.com/hyperledger/indy-test-automation/blob/f460054667c2e9720417ac9514d07082b54d6cd8/system/draft/test_misc.py#L1978

Actual Results:
All tests pass successfully.;;;",,,,,,,,,,,,,,,,,,
Improve simulation tests to include processing of InstanceChanges,INDY-2263,42974,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,24/Oct/19 4:27 PM,07/Dec/19 2:01 AM,28/Oct/23 2:47 AM,06/Dec/19 11:13 PM,,,1.12.1,,,,,0,,,,,"* Allow starting new view changes by a quorum of Instance Changes
",,,,,,,,,,,,,,,,,,,,,,,,INDY-2276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40c",,,,Unset,Unset,Ev-Node 19.23,Ev-Node 19.24,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"12/Nov/19 6:39 PM;sergey.khoroshavin;*PoA*
* Introduce InstanceChangeService
** Created by node and test ReplicaService
** Subscribes to RaisedSuspicion to warn and send instance change
*** Probably it makes sense to introduce new internal message for simplicity and after implementation is complete merge old and one
** Subscribes to InstanceChange messages, sends NeedViewChange messages
* Make InstanceChangerService the only place where InstanceChange message is sent
* Remove ""bad seeds"" from simulation tests, make sure they pass now
* Remove old ViewChanger;;;","06/Dec/19 7:58 AM;sergey.khoroshavin;*PR* https://github.com/hyperledger/indy-plenum/pull/1405;;;","06/Dec/19 11:07 PM;sergey.khoroshavin;*Problem description*
Current view change triggering mechanics is spread across legacy ViewChanger class and monolithic Node, which are not unit testable. As a consequence this logic was not tested by simulation tests. Also code was quite tangled and hard to follow.

*Solution*
Move view change triggering code into separate self-contained services.

*PR*
https://github.com/hyperledger/indy-plenum/pull/1405

*Versions*
sovtoken 1.0.6~dev128
indy-node 1.12.1~dev1154
indy-plenum 1.12.1~dev974

*Risk*
Medium

*Risk factors*
There was quite a big refactoring, some of logic could be missing

*Recommendations for QA*
Testing will be done in scope of INDY-2148;;;",,,,,,,,,,,,,,,,,,,
Cleanup Removing all backup Replicas,INDY-2264,42975,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,24/Oct/19 4:44 PM,26/Dec/19 10:29 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Acceptance Criteria*
* Remove all code related to backup instances
* Make sure all tests pass",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c91r",,,,Unset,Unset,,,,,(Please add steps to reproduce),13.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 1.12.0,INDY-2265,42976,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,24/Oct/19 4:53 PM,30/Nov/19 12:58 AM,28/Oct/23 2:47 AM,30/Nov/19 12:58 AM,,,1.12.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i409i",,,,Unset,Unset,Ev-Node 19.24,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"28/Nov/19 6:34 PM;VladimirWork;*Version Information*
indy-node 1.12.0
indy-plenum 1.12.0
sovrin 1.1.62

*Notices for Stewards*
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.

*Major Changes*
- Improve primary selection algorithm
- Take into account transaction history when recovering state for new nodes
- Fix new nodes adding when there are old AUTH_RULE or plugin transactions

*Detailed Changelog*

+Major Fixes+
INDY-2283 - The problem with a config state
INDY-2287 - Node loses consensus and unreachable by clients
INDY-2254 - A new added node failed to reach consensus

+Changes and Additions+
INDY-2262 - All nodes need to select the same primary during view change
INDY-2292 - Take into account txn history when recovering state from the ledger for new nodes
INDY-2298 - Do not restore Primaries from the audit ledger
INDY-2236 - Start View change on receiving a quorum of ViewChange messages
INDY-2267 - PBFT View change: cleanup and debug Part 3
INDY-2275 - A Node missing a View Change may not be able to finish it if NODE txns have been sent
INDY-2285 - PrePrepare's Digest need to take into account all PrePrepare's fields
INDY-2295 - Investigate reasons of hundreds VCs during 15 txns per sec production load
INDY-2307 - Support historical req handlers for non-config ledgers

+Known Issues+
INDY-2308 - A node lagging behind may not be able to finish view change if nodes have been added/demoted
INDY-2306 - GET_CRED_DEF for a Schema with a lot of attributes may fail with Timeout
INDY-2024 - Only Trustee or Node owner can be the author of NODE demotion txn regardless of endorsement or auth constraint rules set;;;",,,,,,,,,,,,,,,,,,,,,
State Proof Improvements,INDY-2266,42977,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,24/Oct/19 4:58 PM,24/Oct/19 5:06 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-1,,State Proof Improvements,To Do,No,,Unset,No,,,"1|i012vf:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PBFT View change: cleanup and debug Part 3,INDY-2267,42990,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,25/Oct/19 4:48 PM,08/Nov/19 11:27 PM,28/Oct/23 2:47 AM,08/Nov/19 11:27 PM,,,1.12.0,,,,,0,,,,,"* Fix the issue with primary selection when Node txns are in flight IN PROGRESS [~anikitinDSR]** (INDY-2275)
 * Finish cleanup
 ** Do we need `primaryChanged` in replica - TBD
 ** Cleanup methods we call in services after catchup
 * More load tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vjj",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"02/Nov/19 2:29 AM;anikitinDSR;PR:
https://github.com/hyperledger/indy-plenum/pull/1392;;;",,,,,,,,,,,,,,,,,,,,,
Up to F nodes are out of consensus after >3 hours of load,INDY-2268,42992,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,ashcherbakov,ashcherbakov,25/Oct/19 8:40 PM,07/Nov/19 3:26 PM,28/Oct/23 2:47 AM,01/Nov/19 4:32 PM,,,1.11.0,,,,,0,,,,,"*Acceptance Criteria*
 * Identify the cause of the performance bottleneck
 * Raise issues for addressing the problem",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vj7",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,Toktar,,,,,,,,,"26/Oct/19 1:09 AM;ashcherbakov;*PoA*
* Analyze old load tests results
** Analyze 1.9.2 acceptance load results
** Analyze other old load results
** For every load test we need the following information:
*** How many nodes lagged behind eventually
*** How many nodes lagged behind during the load
*** How much time did the pool stay after the load had been stopped and before the pool had been shut down (that is how much time lagged nodes had to recover)
*** How many Replicas have been deleted by the end of the load
*** Were there any node crashes and restarts
* Do more load tests
** prod load 10 txns per sec on 1.9.2 stable (runs right now on Pool 1), stop it, and see if lagged nodes can catchup
** prod load 10 txns per sec on 1.9.2 stable with the same client version as was at the time of 1.9.2 RC acceptance (Pool 1)
** prod load 10 txns per sec on the latest master with Max3PCBatchWait=3 (TBD on Pool 2)
** prod load 10 txns per sec on the latest master with PRE_PREPARE_REQUEST_ENABLED=False, PREPARE_REQUEST_ENABLED=False, COMMIT_REQUEST_ENABLED=False, PROPAGATE_REQUEST_ENABLED=False (Pool 1)
* Compare metrics
** Get and compare 10 min interval metrics from the latest 1.9.2 load and from the ""good"" master one (at time when no lagged nodes)
** Get and compare 10 min interval metrics from the latest 1.9.2 load and from the ""bad"" master one (for lagged nodes)
** Get and compare 10 min interval metrics from  ""good"" new and from the ""bad"" new one
** Get and compare 10 min interval metrics from old 1.9.2 load (at time of RC) and from the ""good"" new one
** Get and compare 10 min interval metrics from old 1.9.2 load (at time of RC)  and from the ""bad"" new one;;;","26/Oct/19 6:13 PM;sergey.khoroshavin;Some preliminary results:
* in old load on 1.9.2 (that was performed about two months ago) only one node started lagging behind, which happened more than after 3 hours of load
* in all current loads on 1.9.2 about 5 nodes started lagging behind, with 1-2 nodes starting lagging just in 2 hours from start
* current 1.9.2 load results seem to be the same both with old and freshly created pools
* during load test on latest master with Max3PCBatchWait=3 on Pool2 no nodes lagged behind after more than 8 hours of load
* currently we have load on latest master with Max3PCBatchWait=3 on Pool1 and so far (2 hours) no nodes are lagging behind;;;","26/Oct/19 6:19 PM;sergey.khoroshavin;Next things which I'm going to analyse and report:
* reason and exact time when node started lagging behind in old load on 1.9.2 that was conducted 2 months ago
* reason and exact times when nodes started lagging behind in fresh load tests on 1.9.2;;;","28/Oct/19 8:41 AM;Toktar;+*indy-node 1.9.1*+ 
 ev@evernymr33:logs/acc_prod_load_logs_metrics_1_1_51.tar.gz
 - start time of the load: 2019-08-01 07:20

*Behind nodes:*
 - 2019-08-01 07:46:59 - node4 - long work of the looper (up to 1000 sec), suspicion of a long verification of bls signatures - did not start ordering after restart
 - 2019-08-01 07:49:26 - node19 - propagates didn't finalize, lots re-asks - began to ordering after restart.

*Result at the end of the load test:*
 - 1 node behind (node4)
 - 1 node stopped ordering (node4)
 - 2 restarts due to memory overflow (node4, node19)

+*indy-node 1.9.2*+ 
 ev@evernymr33:logs/prod_load_1_9_2_new_clients_25_10_2019.tar.gz
 - start time of the load: 2019-10-25 10:49:59

*Behind nodes:* 
 * 2019-10-25 13:30:37 - node6 - too many requests for propagates(not finalized) and pre-prepares(incorrect time)
 * 2019-10-25 12:52:42 - node8 - too many requests for propagates and pre-prepares
 * 2019-10-25 14:59:49 - node12 - too many requests for propagates and pre-prepares 
 * 2019-10-25 12:59:42 - node19 - too many requests for propagates and pre-prepares
 * 2019-10-25 12:52:42 - node23 - too many requests for propagates and pre-prepares
 * 2019-10-25 14:28:54 - node25 - too many requests for propagates and pre-prepares

*Result at the end of the load test:*
 - 0 node behind 
 - 0 node stopped ordering
 - 0 restarts

+*indy-node 1.9.2*+ - in progress
 ev@evernymr33:logs/prod_load_1_9_2_another_old_clients_26_10_2019.tar.gz
 - start time of the load: 2019-10-26 07:19

*Behind nodes:* 
 * 2019-10-25  - node3 - too many requests for propagates(not finalized) and pre-prepares(incorrect time)
 * 2019-10-25  - node4 - too many requests for propagates and pre-prepares
 * 2019-10-25  - node10 - too many requests for propagates and pre-prepares 
 * 2019-10-25  - node13 - too many requests for propagates and pre-prepares
 * 2019-10-25  - node20 - too many requests for propagates and pre-prepares
 * 2019-10-25  - node21 - too many requests for propagates and pre-prepares

 ;;;","28/Oct/19 7:37 PM;sergey.khoroshavin;Proposing analysis plan for each load:
* pool start time (first ""Starting up indy-node"")
* load stop time (grep ""0 ordered batch request"" with non-zero requests)
* how many nodes were lagging after ~15 minutes from load stop (grep ""0 ordered batch request"" around required time)
* how many and when were replica removal events (grep ""removed replica"")
* how many and when were restart events (grep ""Starting up indy-node"")
* what were avg/max_node_prod_time and avg_monitor_avg_latency like (looking at graphs)
* how many seconds it took for node_prod_time, 
* when nodes started lagging behind (walk in 2000 batch steps with {code}xzgrep ""0 ordered batch request"" Node* | grep ""ppSeqNo 2000,""{code})
* when nodes started having non-equal input queues (walk in 200 batch steps backward with {code}xzgrep -E ""0 ordered batch request|0 - checkpoint_service processed"" NodeN* | grep 3800 | sort -k '2,4' -t ':' | head -n 25{code})
* check for suspicious activity near start time of having non-equal input queues
** non-zero max_send_message_req_time/send_message_req_time_count_per_sec
** reconnections (grep ""reconnect"")
** ansible check ins (grep ""ansible"" in journalctl);;;","28/Oct/19 8:41 PM;ashcherbakov;Let's put results into https://docs.google.com/spreadsheets/d/1ExApShBQ1mLgAtpk2w9scROu7ukfU51ZTqPkSp0wLLk/edit#gid=0;;;","31/Oct/19 6:21 PM;Toktar;In scope of the task, the logs of 16 load tests were analyzed. The results can be seen in the table: https://docs.google.com/spreadsheets/d/1ExApShBQ1mLgAtpk2w9scROu7ukfU51ZTqPkSp0wLLk
And change time to waiting a new batch from 1 sec to 3 sec: https://github.com/hyperledger/indy-plenum/pull/1388
*Conclusion*
1. New features for checking BLS signatures have expected increase of the processing time of commit messages.
2. Starting from version 1.9.2 we receive more commit messages.
3. The new versions of the indy-node do not contain degradation, but have lower performance due to BLS changes.
4. The solution with the formation of a batch every 3 seconds solves the performance problem, but increases the message delay time to 3.9 seconds. This is acceptable, but not well.
5. During many tests, 1-3 replicas are deleted, which positively affects performance and allows lagging nodes to resume work due to a decrease the number of 3pc messages.

It turns out that removing replicas will significantly improves performance and allows to return the formation time of the batch to 1 second. That is, this change will allow a pool to have high performance and fast response speed to the client.;;;",,,,,,,,,,,,,,,
Get rid of removing bad Replicas,INDY-2269,42993,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Oct/19 9:18 PM,29/Oct/19 11:11 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125s:i",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid OOM because of Requests queue growing,INDY-2270,42994,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Oct/19 9:19 PM,25/Oct/19 9:19 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125s:r",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Design: Make sure all Replicas do the same work,INDY-2271,42995,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Oct/19 9:19 PM,25/Oct/19 10:08 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125s:k",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change performance thresholds so that the pool is more sensitive to performance degradation,INDY-2272,42996,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Oct/19 9:22 PM,25/Oct/19 9:22 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125s:o",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Be able to order more than 10 reqs per sec,INDY-2273,42997,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Oct/19 9:23 PM,25/Oct/19 9:23 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125s:m",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A Node may not connect to another Node after restart,INDY-2274,43010,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,28/Oct/19 5:50 PM,30/Oct/19 7:09 PM,28/Oct/23 2:47 AM,30/Oct/19 7:09 PM,,,1.11.0,,,,,0,,,,,"*Issue*
* System tests doing restart fail intermittently
* Some nodes can not join the pool if they restart during the load ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969vj6",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"28/Oct/19 5:51 PM;ashcherbakov; * Check if we introduced some regression during recent changes related to ZMQ
 ** Possible regression causes:
 *** New ZMQ
 *** Changes in RETRY_TIMEOUT_RESTRICTED and MAX_RECONNECT_RETRY_ON_SAME_SOCKET: https://github.com/hyperledger/indy-plenum/pull/1315
*** INDY-2222 (reconnect sockets on pings): https://github.com/hyperledger/indy-plenum/pull/1363
*** Do not send Batches to stashed https://github.com/hyperledger/indy-plenum/pull/1376
 ** Possible items to check:
 *** Run system tests (the deal with reconnect catchup suite for example) on master + RETRY_TIMEOUT_RESTRICTED = 15 and MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 1
 *** Run the same system tests on 1.9.2 stable (old ZMQ version)
 *** Run the same system tests on 1.9.2 stable + RETRY_TIMEOUT_RESTRICTED = 2 and MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 3
 *** Run the tests as it was on 1.9.2 but with a new ZMQ
 *** Debug with ZMQ integration tests in Plenum as well (test_reconnect_multi);;;","28/Oct/19 5:53 PM;ashcherbakov;Changes:
* Added ROUTER_HANDOVER option so that we always use the latest connection with the same ID (all our Nodes use the same ID equal to the pubkey).

PR:
* https://github.com/hyperledger/indy-plenum/pull/1386

Version:
* 1.11.0.dev1119
Recommendation for QA:
* make sure all system tests pass
* do load with restarting nodes;;;","28/Oct/19 5:58 PM;ashcherbakov;Currently we have just 2 system tests failing:
* test_case_demoting
** It looks like it has an issue in the test: it expects 1 node disconnecting while all nodes are connected at that time
** Suggested fix in test:
*** Replace ` ensure_all_nodes_online(pool_handler, wallet_handler, trustee_did, unreached=1)` by ` ensure_all_nodes_online(pool_handler, wallet_handler, trustee_did, unreached=0)`
* test_case_pool_ledger (waiting for connection, not waiting for catchup)
** The reason for failing is INDY-1960
*** A node is doing catchup while new requests (new NYM + NODE for this new NYM) are being sent
*** Since we perform signature verification as part of static validation, a Node receives a NODE txn before it gets the corresponding NYM, so it rejects it
** Suggested fix in test:
*** Keep the test as it is but skip with a reference to INDY-1960
*** In the current test use the same NYM for a NODE txn;;;","29/Oct/19 7:17 PM;VladimirWork;PR with updated tests: https://github.com/hyperledger/indy-test-automation/pull/71;;;",,,,,,,,,,,,,,,,,,
A Node missing a View Change may not be able to finish it if NODE txns have been sent,INDY-2275,43016,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,29/Oct/19 1:29 AM,05/Nov/19 5:24 PM,28/Oct/23 2:47 AM,05/Nov/19 5:24 PM,,,1.12.0,,,,,0,,,,,"*Issue*
* There are NODE txns affecting primary selection (promotion/demotion/new nodes + specific viewNo)
* A node didn't finish view change from X to X+1 while the pool finished, and the node tries to do a view change from X to X+2
* A node doesn't know who was the Primary on view X+1 during re-ordering, so it gets wrong audit ledger (since Primary is part of audit txn)
* The Node will have to wait for a quorum of checkpoints from other nodes (~200 batches) to be able to continue ordering

*Possible fix*
* Keep a map of primaries for every view (it needs to be cleaned up on GC)
* Get primaries from Audit ledger's uncommitted txns if a primary for view X is unknown
* Add Primaries to PrePrepare
* Get Primaries from PrePrepare if it can not be get audit ledger as well (it means re-ordering after the view change, so that we can trust such primaries as they have prepared certificate). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969vjo",,,,Unset,Unset,Ev-Node 19.22,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"05/Nov/19 5:24 PM;ashcherbakov;Changes:
* Track primaries in each view
* Add Primaries to PrePrepare
* Restore primaries from the audit ledger if the replica hasn't ordered anything in the given view yet
* Restore primaries from the PrePrepare if the replica hasn't ordered anything in the given view yet and the batch is not on audit yet (this means this is re-ordering so we can trust prepared PrePrepares)

PR:
https://github.com/hyperledger/indy-plenum/pull/1381

Build:
1.11.0.dev1123

Recommendation for QA:
- unit and integrations tests are there
- it's hard to reproduce it in system tests, so just run a load test;;;",,,,,,,,,,,,,,,,,,,,,
A new node joining the pool during the view change may not be able to start ordering immediately,INDY-2276,43017,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,29/Oct/19 1:49 AM,08/Jan/20 5:11 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Issue*
If a new NODE txn is sent and a new Node is started immediately after this, then the Node may node join the consensus immediately, but only after 200 batches. 

*Workaround*
Wait for some time before starting the newly added node. Make sure that view change happened after NODE txn is sent, and the pool started ordering in new View, and only then start a new Node. 

*Issue Reason*
If a new Node joins the Pool when view change is in progress and don't get enough Instance Changes to start the View Change as well, it will not caught up the new view and new primaries, so it will not be able to order until the quorum of checkpoints (200 batches) from other nodes is received
The situation gets worse by the fact that we write new viewNo and update primaries only when we finish re-ordering which may be done not immediately. 



*Possible fix*
* If a Node didn't start a view change to view X but sees a quorum of COMMIT messages for originalViewNo = X, then it needs to do catcup.
* There are chances that the COMMITs are not ordered yet on nodes, so catchup will not move the Node to view X. But then the Node can repeat the procedure until it moves to view X.",,,,,,,,,,,,,,,,,,,,,INDY-2308,INDY-2289,INDY-2263,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41it",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"04/Dec/19 4:58 PM;ashcherbakov;Before implementing a solution, we need to check how other fixes affected the behaviour and whether the ticket is still valid:
1) INDY-2289 - check if Node will receive INSTANCE_CHANGES (because of ZMQ will send them) even if it's started when view change is already in progress
2) INDY-2263 - stashes INSTANCE_CHANGES during catchup (see https://github.com/hyperledger/indy-plenum/blob/c61176b21fbea22f0940935aacd3e87c76e59052/plenum/server/consensus/view_change_trigger_service.py#L82);;;",,,,,,,,,,,,,,,,,,,,,
Come back to a better client latency,INDY-2277,43022,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,29/Oct/19 4:04 AM,26/Dec/19 10:29 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"We changed the frequency of 3PC batches (PrePrepares) sent fro 1 to 3 secs. This improves stability and performance a lot, but decreases the latency for write requests (throughput VS latency trade off).
Once we remove the Replicas, it should be safe to go back to more frequent 3PC Batches and better latency for write requests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c91v",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One node is not in sync after demotion and promotion with the rest ones,INDY-2278,43054,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,31/Oct/19 4:51 PM,14/Nov/19 5:29 PM,28/Oct/23 2:47 AM,14/Nov/19 5:29 PM,,,1.12.0,,,,,0,,,,,"Build Info:
indy-node 1.11.0~rc1

Steps to Reproduce:
https://github.com/VladimirWork/indy-test-automation/blob/work-in-progress/system/draft/TestProductionSuite.py

Actual Results:
Node3 has 10 less txns on this step (so it neither catchup nor order them):
{noformat}
        # promote initial 2nd node by owner - VC
        primary6, _, _ = await get_primary(pool_handler, wallet_handler, trustee_did)
        await eventually(
            promote_node, pool_handler, wallet_handler, stewards['steward2'], 'Node2', pool_info['Node2']
        )
        await ensure_primary_changed(pool_handler, wallet_handler, trustee_did, primary6)
        await ensure_all_nodes_online(pool_handler, wallet_handler, trustee_did)
>       await ensure_pool_is_in_sync(node_ids=list(range(2, 12)))

TestProductionSuite.py:261: 
{noformat}

Expected Results:
All nodes must be in sync.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i403",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"31/Oct/19 7:06 PM;VladimirWork;Logs: ev@evernymr33:logs/2278.tar.gz;;;","13/Nov/19 11:01 PM;VladimirWork;It looks like test passes against the latest master.;;;","14/Nov/19 5:29 PM;VladimirWork;TestProductionSuite remains the same and is bumped to CD. It passes against the latest master.;;;",,,,,,,,,,,,,,,,,,,
Research Libra HotStuff implementation,INDY-2279,43100,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,esplinr,05/Nov/19 4:33 AM,17/Feb/20 4:59 PM,28/Oct/23 2:47 AM,17/Feb/20 4:59 PM,,,2.0,,,,,0,,,,," 

Project documentation:

[https://developers.libra.org/docs/crates/consensus]

Summary:

[https://medium.com/ontologynetwork/hotstuff-the-consensus-protocol-behind-facebooks-librabft-a5503680b151]

Protocol paper:

[https://arxiv.org/pdf/1803.05069.pdf]

 

We need to find out how stable Libra HotStuff implementation is .
 Timebox the effort to 4 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** Characteristics of current deployments in production usage
 ** Current project roadmap
 ** Health of the open source community / level of investment
 ** Rough estimate of work required to bring it to the same level as Plenum
 ** Rough understanding of performance
 ** Rough understanding of the difficulty of migrating from the existing Plenum to this implementation based
 * A recommendation on whether to research more or kill the proposal
 * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

 

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",,,,,,,,,,,,,,,,,,INDY-1614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqx",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,"17/Feb/20 4:59 PM;ashcherbakov;Research is done by Sam Smith. 

*Summary*
* protocol looks good, probably the best among PBFT-like (Tendermint, RBFT, etc.)
* protocol has protection against Follow-the-leader DDoS attacks.
* However, implementation is not live yet, so this is not an option this year. 


;;;",,,,,,,,,,,,,,,,,,,,,
Improve BLS signature performance,INDY-2280,43107,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,05/Nov/19 6:14 PM,10/Dec/19 7:51 PM,28/Oct/23 2:47 AM,10/Dec/19 7:51 PM,,,1.12.1,,,,,0,,,,,"Currently about 15% of all time is spent on validation of BLS signatures in Commits (see metrics below).
One of the reason, is that we have to sign both Domain and Token states if FEEs are enabled.

*Acceptance Criteria*
- Investigate how we can improve performance (INDY-2252 is one of the ways)
- Do necessary fixes. 
- Create follow-up tickets for further improvements if needed.


*Metrics*
Start time: 2019-10-22 00:04:00
End time: 2019-10-22 00:14:00
Duration: 0:10:00
Number of messages processed in one looper run:
   Node:   236 samples, 94906.00 None, 26.00/402.14/1000.00 min/avg/max, 356.38 stddev
   Client: 236 samples, 5917.00 None, 4.00/25.07/51.00 min/avg/max, 10.70 stddev
Seconds passed between looper runs:
   236 samples, 591.61 None, 0.33/2.51/5.37 min/avg/max, 1.10 stddev
Number of messages in one transport batch:
   5808 samples, 341460.00 None, 2.00/58.79/134.00 min/avg/max, 28.76 stddev
Node message size, bytes:
   Outgoing: 5808 samples, 240228528.00 None, 7028.00/41361.66/130387.00 min/avg/max, 25096.80 stddev
   Incoming: 94906 samples, 240158125.00 None, 53.00/2530.48/90366.00 min/avg/max, 5589.89 stddev
Client message size, bytes:
   Outgoing: 11399 samples, 14249723.00 None, 81.00/1250.09/11572.00 min/avg/max, 1851.20 stddev
   Incoming: 5917 samples, 7277834.00 None, 644.00/1229.99/9478.00 min/avg/max, 1947.02 stddev
Number of requests in one 3PC batch:
   Created: 521 samples, 5954.00 None, 0.00/11.43/28.00 min/avg/max, 3.48 stddev
   Ordered: 482 samples, 5477.00 None, 0.00/11.36/27.00 min/avg/max, 3.33 stddev
Time spent on write request processing, ms:
   10.59/16.06/264.80 ms min/avg/max, 4.32 stddev
Monitor throughput, TPS
   Master: 8.22/9.53/10.31 min/avg/max, 0.45 stddev
   Backup: 8.26/9.51/10.17 min/avg/max, 0.44 stddev
Monitor latency, seconds
   Master: 4.30/13.92/39.08 min/avg/max, 7.93 stddev
   Backup: 4.21/13.79/38.48 min/avg/max, 8.09 stddev
RAM info:
   Available, Mb: 6504.41/6536.23/6540.99 min/avg/max, 5.75 stddev
   Node RSS, Mb: 1099.84/1103.71/1113.77 min/avg/max, 3.82 stddev
   Node VMS, Mb: 1542.84/1547.76/1550.84 min/avg/max, 3.93 stddev
Additional statistics:
   Client incoming/outgoing: 0.52 messages, 0.51 traffic
   Node incoming/outgoing traffic: 1.00
   Node/client traffic: 22.31
   Node traffic per batch: 996652.81
   Node traffic per request: 87709.81
Profiling info:
   BACKUP_REQUEST_PROCESSING_TIME : 45020 samples, 0.07 seconds, 0.00/0.00/0.05 ms min/avg/max, 0.00 stddev
   REQUEST_PROCESSING_TIME : 5954 samples, 95.62 seconds, 10.59/16.06/264.80 ms min/avg/max, 4.32 stddev
   GC_GEN0_TIME : 3898 samples, 1.28 seconds, 0.07/0.33/4.13 ms min/avg/max, 0.54 stddev
   GC_GEN1_TIME : 354 samples, 1.39 seconds, 1.78/3.94/6.34 ms min/avg/max, 0.91 stddev
   GC_GEN2_TIME : 32 samples, 8.28 seconds, 218.85/258.84/310.44 ms min/avg/max, 21.70 stddev
   NODE_PROD_TIME : 236 samples, 591.60 seconds, 328.18/2506.76/5371.07 ms min/avg/max, 1103.59 stddev
   SERVICE_REPLICAS_TIME : 236 samples, 331.08 seconds, 28.46/1402.89/3807.80 ms min/avg/max, 740.81 stddev
   SERVICE_NODE_MSGS_TIME : 236 samples, 199.49 seconds, 264.44/845.30/1594.13 ms min/avg/max, 311.43 stddev
   SERVICE_CLIENT_MSGS_TIME : 236 samples, 9.70 seconds, 9.56/41.09/120.63 ms min/avg/max, 18.61 stddev
   SERVICE_NODE_ACTIONS_TIME : 236 samples, 2.68 seconds, 0.01/11.35/70.10 ms min/avg/max, 20.99 stddev
   SERVICE_VIEW_CHANGER_TIME : 236 samples, 0.01 seconds, 0.03/0.03/0.07 ms min/avg/max, 0.01 stddev
   SERVICE_OBSERVABLE_TIME : 236 samples, 0.02 seconds, 0.01/0.10/0.26 ms min/avg/max, 0.05 stddev
   SERVICE_OBSERVER_TIME : 236 samples, 0.00 seconds, 0.01/0.01/0.03 ms min/avg/max, 0.00 stddev
   FLUSH_OUTBOXES_TIME : 236 samples, 48.27 seconds, 22.03/204.52/523.24 ms min/avg/max, 108.24 stddev
   SERVICE_NODE_LIFECYCLE_TIME : 236 samples, 1.25 seconds, 2.29/5.30/16.60 ms min/avg/max, 1.52 stddev
   SERVICE_CLIENT_STACK_TIME : 236 samples, 0.29 seconds, 0.21/1.21/3.04 ms min/avg/max, 0.51 stddev
   SERVICE_MONITOR_ACTIONS_TIME : 236 samples, 0.01 seconds, 0.00/0.03/1.09 ms min/avg/max, 0.12 stddev
   SERVICE_TIMERS_TIME : 236 samples, 0.01 seconds, 0.01/0.05/0.46 ms min/avg/max, 0.10 stddev
   SERVICE_NODE_STACK_TIME : 236 samples, 149.57 seconds, 187.95/633.75/1286.02 ms min/avg/max, 247.05 stddev
   PROCESS_NODE_INBOX_TIME : 236 samples, 49.91 seconds, 73.57/211.49/323.07 ms min/avg/max, 68.53 stddev
   SEND_TO_REPLICA_TIME : 233458 samples, 2.69 seconds, 0.01/0.01/0.15 ms min/avg/max, 0.00 stddev
   NODE_CHECK_PERFORMANCE_TIME : 52 samples, 0.04 seconds, 0.59/0.74/1.08 ms min/avg/max, 0.14 stddev
   NODE_CHECK_NODE_REQUEST_SPIKE : 10 samples, 0.00 seconds, 0.02/0.03/0.04 ms min/avg/max, 0.01 stddev
   UNPACK_BATCH_TIME : 59490 samples, 122.06 seconds, 0.19/2.05/313.04 ms min/avg/max, 6.12 stddev
   VERIFY_SIGNATURE_TIME : 142496 samples, 15.47 seconds, 0.01/0.11/236.43 ms min/avg/max, 0.66 stddev
   SERVICE_REPLICAS_OUTBOX_TIME : 236 samples, 13.09 seconds, 0.52/55.48/174.41 ms min/avg/max, 36.29 stddev
   NODE_SEND_TIME : 15602 samples, 1.36 seconds, 0.04/0.09/1.27 ms min/avg/max, 0.04 stddev
   NODE_SEND_REJECT_TIME : no samples
   VALIDATE_NODE_MSG_TIME : 429527 samples, 116.12 seconds, 0.05/0.27/311.31 ms min/avg/max, 2.20 stddev
   INT_VALIDATE_NODE_MSG_TIME : 429527 samples, 51.20 seconds, 0.03/0.12/310.87 ms min/avg/max, 2.15 stddev
   PROCESS_ORDERED_TIME : 4605 samples, 11.98 seconds, 0.04/2.60/64.74 ms min/avg/max, 7.78 stddev
   MONITOR_REQUEST_ORDERED_TIME : 4605 samples, 0.56 seconds, 0.02/0.12/0.73 ms min/avg/max, 0.09 stddev
   EXECUTE_BATCH_TIME : 482 samples, 11.22 seconds, 0.99/23.28/64.10 ms min/avg/max, 9.13 stddev
   SERVICE_REPLICA_QUEUES_TIME : 236 samples, 281.30 seconds, 2.63/1191.94/3411.61 ms min/avg/max, 668.92 stddev
   SERVICE_BACKUP_REPLICAS_QUEUES_TIME : 1888 samples, 36.64 seconds, 1.35/19.41/303.59 ms min/avg/max, 14.13 stddev
   PROCESS_PREPREPARE_TIME : 521 samples, 162.39 seconds, 2.01/311.69/799.74 ms min/avg/max, 146.81 stddev
   PROCESS_PREPARE_TIME : 14365 samples, 55.33 seconds, 0.01/3.85/415.72 ms min/avg/max, 27.95 stddev
   PROCESS_COMMIT_TIME : 14526 samples, 167.67 seconds, 0.01/11.54/89.20 ms min/avg/max, 10.89 stddev
   PROCESS_CHECKPOINT_TIME : 114 samples, 0.33 seconds, 0.10/2.92/125.58 ms min/avg/max, 14.42 stddev
   SEND_PREPREPARE_TIME : no samples
   SEND_PREPARE_TIME : 521 samples, 0.18 seconds, 0.26/0.35/3.76 ms min/avg/max, 0.16 stddev
   SEND_COMMIT_TIME : 462 samples, 1.50 seconds, 2.12/3.24/12.12 ms min/avg/max, 0.97 stddev
   SEND_CHECKPOINT_TIME : 5 samples, 0.00 seconds, 0.25/0.25/0.26 ms min/avg/max, 0.00 stddev
   CREATE_3PC_BATCH_TIME : no samples
   ORDER_3PC_BATCH_TIME : 482 samples, 5.49 seconds, 7.84/11.38/38.69 ms min/avg/max, 2.76 stddev
   BACKUP_PROCESS_PREPREPARE_TIME : 4256 samples, 14.71 seconds, 0.69/3.46/274.13 ms min/avg/max, 5.33 stddev
   BACKUP_PROCESS_PREPARE_TIME : 145205 samples, 17.57 seconds, 0.01/0.12/271.80 ms min/avg/max, 0.93 stddev
   BACKUP_PROCESS_COMMIT_TIME : 145781 samples, 12.65 seconds, 0.01/0.09/137.15 ms min/avg/max, 0.49 stddev
   BACKUP_PROCESS_CHECKPOINT_TIME : 1058 samples, 1.02 seconds, 0.09/0.96/79.24 ms min/avg/max, 5.37 stddev
   BACKUP_SEND_PREPREPARE_TIME : no samples
   BACKUP_SEND_PREPARE_TIME : 4256 samples, 0.59 seconds, 0.11/0.14/4.57 ms min/avg/max, 0.07 stddev
   BACKUP_SEND_COMMIT_TIME : 2994 samples, 0.51 seconds, 0.09/0.17/55.01 ms min/avg/max, 1.05 stddev
   BACKUP_SEND_CHECKPOINT_TIME : 40 samples, 0.01 seconds, 0.17/0.19/0.25 ms min/avg/max, 0.02 stddev
   BACKUP_CREATE_3PC_BATCH_TIME : no samples
   BACKUP_ORDER_3PC_BATCH_TIME : 4123 samples, 2.92 seconds, 0.39/0.71/137.03 ms min/avg/max, 2.95 stddev
   PROCESS_PROPAGATE_TIME : 136579 samples, 33.41 seconds, 0.14/0.24/9.34 ms min/avg/max, 0.13 stddev
   PROCESS_MESSAGE_REQ_TIME : no samples
   PROCESS_MESSAGE_REP_TIME : 1115 samples, 0.25 seconds, 0.13/0.22/0.93 ms min/avg/max, 0.09 stddev
   PROCESS_LEDGER_STATUS_TIME : no samples
   PROCESS_CONSISTENCY_PROOF_TIME : no samples
   PROCESS_CATCHUP_REQ_TIME : no samples
   PROCESS_CATCHUP_REP_TIME : no samples
   PROCESS_REQUEST_TIME : 5917 samples, 2.50 seconds, 0.20/0.42/7.38 ms min/avg/max, 0.23 stddev
   SEND_PROPAGATE_TIME : 5917 samples, 2.23 seconds, 0.24/0.38/1.15 ms min/avg/max, 0.11 stddev
   SEND_MESSAGE_REQ_TIME : 5386 samples, 0.38 seconds, 0.02/0.07/8.47 ms min/avg/max, 0.14 stddev
   SEND_MESSAGE_REP_TIME : no samples
   BLS_VALIDATE_PREPREPARE_TIME : 4777 samples, 11.64 seconds, 0.00/2.44/67.94 ms min/avg/max, 8.04 stddev
   BLS_VALIDATE_COMMIT_TIME : 77201 samples, 160.54 seconds, 0.00/2.08/69.12 ms min/avg/max, 6.20 stddev
   BLS_UPDATE_PREPREPARE_TIME : no samples
   BLS_UPDATE_COMMIT_TIME : 462 samples, 1.39 seconds, 1.95/3.00/6.25 ms min/avg/max, 0.86 stddev
   DESERIALIZE_DURING_UNPACK_TIME : no samples",,,,,,,,,,,,,,,,,,,,,,,,INDY-2281,INDY-2252,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41hul",,,,Unset,Unset,Ev-Node 19.24,Ev-Node 19.25,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,sergey.khoroshavin,,,,,,,,,"03/Dec/19 7:33 PM;ashcherbakov;*PoA:*
1) Do not fill and check BLS sigs twice (in `blsMultiSig` and `blsMultiSigs`). Keep it in `blsMultiSigs` only. This should reduce the time spend for BLS up to 50%.
2) Do not do unnecessary decoding/encoding calling `bls_from_str` for a public key every time we do verification
3) Do not validate BLS sigs for already ordered Commits (as part of View Change re-ordering)
4) Do not verify individual BLS sigs in commits:
- Verify aggregated BLS sig only when receive a quorum of commits (in assumption that in most of the cases it's valid)
- If aggregated BLS sig is invalid, then validate each individual COMMIT

5) Validate BLS sigs on Primary only
- Do not validate BLS sig for Commits on non-Primary replicas
- Do not calculate aggregated BLS sig on non-Primary replicas while Ordering
- If a Primary doesn't have requests to be sent when it's time for the next Batch, but there are requests on the previous batch ordered (so that BLS aggregated sig is not calculated for them yet), then send a Freshness empty batch now (doesn't wait till the next Freshness check).

Items 1-3 must be done in this ticket. Items 4 and 5 can be done in a follow-up ticket.;;;","05/Dec/19 10:28 PM;ashcherbakov;https://jira.hyperledger.org/browse/INDY-2309 is created for Items 4 and 5.;;;","05/Dec/19 10:33 PM;ashcherbakov;*Changes*
 * Items 1-3 are implemented

*PRs*
 * [https://github.com/hyperledger/indy-plenum/pull/1423] - Items 1 and 3
 * https://github.com/hyperledger/indy-plenum/pull/1425 - Item 2

*Version*
 * Items 1 and 3: Plenum 1.12.1.dev971, Node 1.12.1.dev1152
 * Items 2: TBD

*Risk*
 * Low

*Recommendation for QA*
 - run load test, get and compare BLS verification metrics (BLS_VALIDATE_COMMIT_TIME, BLS_VALIDATE_PREPREPARE_TIME)
 ** it may make sense to run and compare metrics for Items 1,3 only, and for all Items (builds 1152 and TBD) to check how Item2 improved situation and how expensive base58 serialization is.
 - make sure that load tests for adding/demoting/restartiong nodes pass
 - make sure that load tests for BLS key rotation pass;;;","06/Dec/19 10:56 PM;sergey.khoroshavin;*Versions*
sovtoken 1.0.6~dev128
indy-node 1.12.1~dev1154
indy-plenum 1.12.1~dev974
 ;;;","10/Dec/19 7:48 PM;anikitinDSR;After small performance investigations:

(After 2 hours from load_test begining)
h3. With BLS changes:
{quote}{color:#0747a6}PROCESS_COMMIT_TIME : 34151 samples, 290.08 seconds total, 0.02/8.49/39.05 ms min/avg/max, 7.18 stddev{color}

{color:#0747a6}PROCESS_PREPARE_TIME : 34152 samples, 7.85 seconds total, 0.01/0.23/5.15 ms min/avg/max, 0.55 stddev{color}
 {color:#0747a6} BLS_VALIDATE_COMMIT_TIME : 229264 samples, 274.67 seconds total, 0.00/1.20/31.99 ms min/avg/max, 3.85 stddev{color}
 {color:#0747a6} BLS_UPDATE_COMMIT_TIME : 1423 samples, 3.23 seconds total, 1.11/2.27/4.63 ms min/avg/max, 0.90 stddev{color}
{quote}
h3. Without BLS changes:
{quote}{color:#0747a6}PROCESS_COMMIT_TIME : 28847 samples, 408.46 seconds total, 0.02/14.16/59.18 ms min/avg/max, 11.01 stddev{color}
 {color:#0747a6} PROCESS_PREPARE_TIME : 28848 samples, 8.09 seconds total, 0.01/0.28/6.69 ms min/avg/max, 0.78 stddev{color}
 {color:#0747a6} BLS_VALIDATE_COMMIT_TIME : 193776 samples, 390.80 seconds total, 0.00/2.02/39.29 ms min/avg/max, 6.25 stddev{color}
 {color:#0747a6} BLS_UPDATE_COMMIT_TIME : 1202 samples, 4.09 seconds total, 1.95/3.40/6.00 ms min/avg/max, 1.05 stddev{color}
{quote}
As we can see from results we have almost 2 times performance increasing for processing commit messages.

Also we have the same situation on the 18th hour of load test:
{quote}{color:#0747a6}{{PROCESS_COMMIT_TIME : 32640 samples, 280.24 seconds total, 0.02/8.59/56.84 ms min/avg/max, 7.31 stddev}}
 {{BLS_VALIDATE_COMMIT_TIME : 220720 samples, 265.20 seconds total, 0.00/1.20/50.64 ms min/avg/max, 3.89 stddev}}{color}
{quote}
 ;;;",,,,,,,,,,,,,,,,,
[Design] Avoid signing BLS twice and do BLS signatures over Audit Ledger,INDY-2281,43108,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,05/Nov/19 6:52 PM,05/Nov/19 6:55 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Currently BLS signature is created against the Ledger the 3PC batch is created for. If there are FEEs coming with the txn, then Token Ledger will be signed as well, so we create and verify BLS multi-sig twice for every COMMIT in case of FEEs used. 

This was done to avoid expensive breaking changes. However, we can do just one BLS signature if we sign over Audit Ledger.

*Acceptnce Criteria*
* Design how BLS multi-signature can de done over Audit Ledger
* Consider changes on both Node and SDK sides
* Consider how to deal with this breaking change (increment Protocol Version)
* Consider how to migrate existing BLS data and stores",,,,,,,,,,,,,,,,,,,,,INDY-2280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c92i",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Optimize Message Request logic,INDY-2282,43109,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,05/Nov/19 7:07 PM,08/Jan/20 5:55 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"If a Node experiences network issues, it may start requesting 3PC messages. If the node is lagging behind, then it may start lagging behind even more since because of these message requests. 
*Acceptance criteria*
* Check if we need message requests at all (we use TCP-based ZMQ, so we should not really loose messages).
* Optimize message request logic to not request too much. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ibu",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The MainNet problem with a config state.,INDY-2283,43133,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,lbendixsen,Toktar,Toktar,06/Nov/19 1:22 AM,20/Nov/19 11:00 PM,28/Oct/23 2:47 AM,20/Nov/19 11:00 PM,,,1.12.0,,,,,0,,,,,"_Environment_: MainNet - Sovrin - 1.1.58   Node - 1.10.0

_Steps to Reproduce_: 
 # *On August 19th* the MainNet was upgraded up to *indy-node 1.9.1*. This version contained a problem with a config state which reproduced on BuilderBet. We fixed and discussed the issue here: INDY-2211
 # In order to not affect other Nets, it was suggested to not send any AUTH_RULE txns until the issue is fixed, that it until 1.9.2 is applied (see https://jira.hyperledger.org/browse/INDY-2211?focusedCommentId=63169&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-63169)
 # However, *On September 5th*, two *AUTH_RULES* transactions were written to Main Net (it still had indy-node 1.9.1 version with the issue)
 # *September 27th*. Only then MainNet was upgraded up to *indy-node 1.9.2* where the problem with config state for AUTH_RULE was fixed.

_Affects:_ New nodes can't be promoted.

_Options to solve the problem:_
 # _Write a migration script and execute it on all nodes. Migrate current config state and change an audit ledger._
_Pros:_ Fixes the issues with adding new nodes and external auditability of config ledger
_Cons:_ Change an audit ledger doesn't safe (high risk) and doesn't сorrespond to the ideology of the blockchain.
 # _Send AUTH_RULE transaction equal to the two AUTH_RULES wrote on September 5th, and clean-up config state (or the whole data) on Sicpa Node_
_Pros:_ Fixes the issues with adding new nodes; low risk
_Cons:_ External audit of the config ledger for the period from 5 Sep till the two AUTH_RULE txns are re-sent will show different config states (it affects external audit only, and only for the given period of time).
# _Take into account txn history when recovering state from the ledger for new nodes_
_Pros:_ Fixes the issues with adding new nodes and external auditability of config ledger; low risk
_Cons:_ Not so good from Indy code point of view; requires either a hotfix release or waiting for the next stable release

It may make sense to do Option 2 right now regardless if we do Options 1 or 3 or not. This should allow to add new nodes to MainNet. If we think that the issues with auditability are real issues, we may do Options 1 or 3 in addition.

As for non-auditable config ledger for this 2 months: ""non-auditbale"" is probably too strong. It's auditable, but if someone will try to apply txns one by one to all ledger and states using the latest version of code, and compare the txn root hashes with the one in the corresponding audit txn, he will see a difference in config ledger root hash for that period of time. An excuse and workaround can be: at that time there was a different version of code (a different algorithm) which provided a bit different values. So, if someone will use that (1.9.1) version of code to apply txns, he will get exactly the same values as in audit ledger, and can see that we are not cheating.
",,,,,,,,,,,,,,,,,,,,,INDY-2291,INDY-2292,INDY-2293,INDY-2284,INDY-2287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vjli",,,,Unset,Unset,Ev-Node 19.22,Ev-Node 19.23,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,sergey.khoroshavin,Toktar,,,,,,,,"06/Nov/19 9:14 PM;sergey.khoroshavin;*Estimates for Option 1*
* Development: 2-3 days
** Script itself - generic implementation, which performs audit and applies fixes to diverged transactions in audit ledger - up to 2 days (some work was already started when we experienced problems with BuilderNet)
** Migration - call audit fixup script during upgrade - up to 1 day
* Testing: 4-5 days
** Test script on ledgers from MainNet - couple of hours, but getting data itself can take day or more
** Negative scenario - start pool with 1.10.0 (which doesn't have this problem), write some transactions (including problematic auth rules), test upgrade with migration, ensure that state and audit ledger are untouched, and pool is functional - test itself probably is half a day, but if it uncovers some problems it might be needed to be done again, so it makes sense to book at least 1 day for it
** Positive scenario - start pool with 1.9.1 (version with problem), write some transactions (including problematic auth rules), test upgrade with migration, ensure that state and audit ledger are patched, pool is functional, and new node can join after that - test itself could take up to full day (we'll need to test full chain of upgrades), but if it uncovers some problems it might be needed to done again, so it makes sense to book at least 2 days for it

*Overall*: it looks like whole task can be done in 6-8 days by one developer, but given high risks and high stakes (patching ledgers on MainNet) I would book full sprint (10 days) for 1 developer.
;;;","08/Nov/19 3:44 AM;esplinr;We are tracking the concern specific to the Sovrin Network here:
[https://sovrin.atlassian.net/browse/SN-9]

We'll pick the resolution on that issue.;;;","08/Nov/19 11:29 PM;Toktar;After a detailed analysis of the logs and states, the assumption of the problem with AUTH_RULEs was confirmed. We need to select the preferred option and apply it.;;;","11/Nov/19 10:19 PM;ashcherbakov;Proposed solution:
1) Do Option 2 (send 2 AUTH_RULE txns)
2) If we care about auditability, then do either Option 1 (INDY-2291) or Option 3 (INDY-2292);;;","20/Nov/19 11:00 PM;ashcherbakov;Will be fixed by INDY-2292;;;",,,,,,,,,,,,,,,,,
Implement system tests to check all states' consistency,INDY-2284,43140,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,06/Nov/19 6:14 PM,12/Nov/19 11:18 PM,28/Oct/23 2:47 AM,12/Nov/19 11:18 PM,,,,,,,,0,,,,,Implement integration and system tests to check all states' consistency.,,,,,,,,,,,,,,,,,,,,,INDY-2283,,,INDY-2290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41f",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"06/Nov/19 6:18 PM;VladimirWork;PoA for system level:

1. Write all types of txns with various parameters.
2. Perform pool upgrade.
3. Add node to the pool.
4. Check pool functionality.

Existing docker upgrade test (https://github.com/hyperledger/indy-test-automation/blob/master/system/draft/test_upgrade_docker_7.py) can be improved to cover this case.
;;;","12/Nov/19 12:46 AM;VladimirWork;Done in https://github.com/hyperledger/indy-test-automation/pull/75;;;","12/Nov/19 11:18 PM;VladimirWork;Merged.;;;",,,,,,,,,,,,,,,,,,,
PrePrepare's Digest need to take into account all PrePrepare's fields,INDY-2285,43142,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,06/Nov/19 10:18 PM,28/Nov/19 5:33 PM,28/Oct/23 2:47 AM,28/Nov/19 5:31 PM,,,1.12.0,,,,,0,,,,,"As of now, PrePrepare's digest is created over a list of requests proposed by this PrePrepare. It doesn't take into account other fields such as txn roots, timestamp, etc. 
So, a malicious Primary may send PrePrepare with the same requests (that is the same digest in the current implementation) but with different other fields (audit roots, timestamp, etc.) to every Node. 
This is not so big problem for common ordering since ViewChange will be triggered. 
But this is a problem for View Change when we do re-ordering, since nodes may have different PrePrepares for the same digest and will not be able to finish re-ordering (and hence finish view change).

*Accepted criteria:*
* Write an integration test:
** Test 1:
*** Malicious Primary sends correct and equal PrePrepares to F+1 Npdes, and sends PrePrepares with the same list of requests but with different audit txn roots to other nodes 
*** Make sure that view change is started and successfully finished in this case. The requests sent in the PrePrepare above are expected to be ordered.  
** Test 2:
*** Malicious Primary sends PrePrepares with the same list of requests but with different audit txn roots to all nodes
*** Make sure that view change is started and successfully finished in this case. The requests sent in the PrePrepare above are expected to NOT be ordered.  
* Do a fix:
** Calculate PrePrepares' digest against all PrePrepare's fields 

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i408",,,,Unset,Unset,Ev-Node 19.23,Ev-Node 19.24,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"26/Nov/19 5:03 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/1411;;;","26/Nov/19 6:50 PM;Toktar;*Problem reason:*
 - A malicious primary node can send different PrePrepares to n-f-1 and f nodes. In this case f nodes will have an incorrect state tree and will not be able to ordering during 2 next checkpoints
 - Any malicious node can send OldNewPrePrepareReply with incorrect PrePrepare (changed ppTime). A node receiving  incorrect PrePrepare will apply it and will not be able to ordering during 2 next checkpoints 

*Changes:*
 - change logic of a digest generating for PrePrepares
 - add checks for messages with incorrect digest to OldViewPrePrepareReply processing

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1411]

*Version:*
 * sovrin - #172 - master
 * token-plugin #120 - master
 * indy-node 1.12.0.dev1141 -master
 * indy-plenum 1.12.0.dev965 -master

*Risk factors:*
 - Problems with a validation of PrePrepare messages
 - Problems with re-ordering after a view change

*Risk:*
 - Medium

*Tests:*
 * [test_old_view_pre_prepare_reply_processing.py|https://github.com/hyperledger/indy-plenum/pull/1411/files#diff-07a332330721af7358ea3b0332d1e106] 
 * [test_primary_send_incorrect_pp.py|https://github.com/hyperledger/indy-plenum/pull/1411/files#diff-6d7b44ce8565ea010d21c36b60456008] 

*Recommendations for QA:*
 * Production load test

 * 
 ** force view change every 1 hour during 3 hours
 ** After force view change demote, promote a non primary node.
 ** Check that all nodes can order after 30min ;;;","27/Nov/19 9:57 PM;VladimirWork;Build Info:
indy-node 1.12.0~dev1141
plugins 1.0.5~dev120

Steps to Reproduce:
1. Run production laod test with forced VCs.
2. Demote 10th node for 30 seconds and promote it back.
3. Demote 20th node for 120 seconds and promote it back.

Actual Results:
10th node lags after promotion and doesn't catch up.

ev@evernymr33:logs/27_11_2019_forced_vc.tar.gz
ev@evernymr33:logs/27_11_2019_forced_vc_full_data.tar.gz;;;","28/Nov/19 5:31 PM;VladimirWork;The issue found will be fixed in scope of INDY-2308.;;;",,,,,,,,,,,,,,,,,,
Improve simulation tests to include NODE txns,INDY-2286,43208,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,07/Nov/19 6:39 PM,03/Dec/19 1:09 AM,28/Oct/23 2:47 AM,03/Dec/19 1:09 AM,,,1.12.1,,,,,0,,,,,"* Extend view change during ordering cases with NODE txs
** Start with a simple sending of different combination of NODE txns interleaved with view changes
** Consider adding new Replicas in sim tests once a new NODE is added/promoted as well as removing Replicas when  there is NODE txn for demotion
It may be easier to do this after INDY-2262 is done.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i409r",,,,Unset,Unset,Ev-Node 19.23,Ev-Node 19.24,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"03/Dec/19 1:05 AM;anikitinDSR;PR with simulation tests:
 * [https://github.com/hyperledger/indy-plenum/pull/1409]

Added NODE txn ordering with mixed domain transactions and manually VC forcing.

 ;;;",,,,,,,,,,,,,,,,,,,,,
Node on Sovrin MainNet loses consensus and unreachable by clients,INDY-2287,43218,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,08/Nov/19 3:40 AM,20/Nov/19 10:59 PM,28/Oct/23 2:47 AM,20/Nov/19 10:59 PM,,,1.12.0,,,,,0,EV-CS,,,,"There are 2 issues with the VeridiumIDC node on MainNet that need to be debugged.

*The first issue:*
This may be related to issues we are seeing on about 4 other nodes on the three Sovrin networks, so it may be becoming prevalent enough that it is becoming a serious issue. The issue is that the node is not maintaining consensus with the network. In the attached logs the indy-node service was restarted at about 16:15 and was in consensus following the restart. An ATTRIB transaction was written to the ledger 5 or 10 minutes later, at which point consensus was lost.

All settings and configuration on the node appear to be correct. I am linking this ticket to another that appears to have similar symptoms in INDY. It also appears related to this Sovrin ticket: [https://sovrin.atlassian.net/browse/SN-8]. I am attaching logs, local output of validator-info, and files from the data directory. I will also include files from ev1, which is the primary on MainNet.

 

*The second issue:*
All attempts to get-validator-info remotely are failing, whether from indy-cli or from a custom script. The client port is reachable by TCP from clients, and it appears that the zmq protocol is not blocked. The other validator nodes are able to communicate on the node port. In short, I believe that the communication pathways are clear, and that the issue may be a failure while gathering the metrics. That being said, the validator-info command run locally from the command line is functioning properly.

 ","Sovrin MainNet, running 1.10.0",,,,,,,,,,,,,,,,,,,,INDY-2283,,,,,,,,,,"08/Nov/19 3:24 AM;mgbailey;VeridiumIDC.log.xz;https://jira.hyperledger.org/secure/attachment/17918/VeridiumIDC.log.xz","08/Nov/19 3:24 AM;mgbailey;config.zip;https://jira.hyperledger.org/secure/attachment/17920/config.zip","08/Nov/19 3:39 AM;mgbailey;ev1_config_state_txns.tgz;https://jira.hyperledger.org/secure/attachment/17917/ev1_config_state_txns.tgz","08/Nov/19 3:24 AM;mgbailey;validator_info_7_nov.txt;https://jira.hyperledger.org/secure/attachment/17919/validator_info_7_nov.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969vjl9",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,mgbailey,Toktar,,,,,,,,,"12/Nov/19 2:45 AM;Toktar;*For the second issue*
VeridiumIDC didn't receive any client messages. So, clients are not connected to VeridiumIDC.
Maybe a problem in different ip addresses. In the NODE transaction ""client_ip"": ""18.197.183.58"" and on the node ""Client_ip"": 10.0.50.26
But maybe it's ok for specific VeridiumIDC interfaces.;;;","20/Nov/19 10:59 PM;ashcherbakov;Will be fixed by INDY-2292;;;",,,,,,,,,,,,,,,,,,,,
Apply existing PrePrepares for future batches after catchup,INDY-2288,43233,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,08/Nov/19 10:29 PM,08/Jan/20 5:09 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Issue*
* A node has all PrePrepares till (3, 230) (for example), but last ordered is (3, 55)
* A node starts a catchup (so uncommitted state is reverted)
* A node caught up till (3, 202).
* A node receives PrePrepare (3, 231) after catchup. It sees that its last PrePrepare is (3, 230), so it starts to apply it. 
* But PrePrepares from last ordered (3,202) till (3,230) are not applied (reverted during catchup), so the node will have incorrect state trie

*Fix*
* Re-apply all PrePrepares higher that lastCaughtup3PC after catchup
* Write an integration test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ibc",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Enable zeroMQ auto-reconnection,INDY-2289,43241,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,donqui,donqui,09/Nov/19 2:02 AM,11/Dec/19 12:52 AM,28/Oct/23 2:47 AM,11/Dec/19 12:50 AM,,,1.12.1,,,,,0,,,,,"*Issue Description*

Today we use monitor sockets for:
 # detecting and getting a list of disconnected/unreachable nodes (in Validator Info for example)
 # triggering a View Change when a Primary is disconnected

This logic seems to cause some unexpected behavior in zmq (like zmq not being able to reconnect by itself - INDY-2261), so we need to see why monitor sockets are preventing this, and fix it.

If there is no conclusion the logic for which we are now using monitor sockets should be implemented with heartbeats.

*Note*

An issue for the monitor behavior is opened with PyZMQ community:
 * [https://github.com/zeromq/pyzmq/issues/1340]

Check the status of this issue before making changes to the code because it can potentially provide us with
 * an easy fix if we are not using monitor sockets correctly, or
 * a new version without this problem

if this turns out to be the case, this ticket should be ignored and a follow up should be created.

*Acceptance Criteria*
 * zeroMQ auto-reconnection works
 * All existing tests pass
 * New logic covered with unit and integration tests

 ",,,,,,,,,,,,,,,,,,,,,INDY-2261,,,INDY-2276,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40i",,,,Unset,Unset,Ev-Node 19.23,Ev-Node 19.24,Ev-Node 19.25,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),donqui,sergey.khoroshavin,VladimirWork,,,,,,,,,"27/Nov/19 9:35 PM;donqui;An issues opened on the libzmq side as pyzmq maintainers did not respond https://github.com/zeromq/libzmq/issues/3745;;;","04/Dec/19 6:13 PM;donqui;Problem reason/description:
 - zeroMQ reconnection did not work
 - monitor socket blocked entire zeroMQ lib

Changes:
 - monitor socket closed when we notice that there is no connection to a remote

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1422]

Version:
 - Plenum:  1.12.1.dev970

Risk factors:
 - networking not working or working differently
 - re-connection not working
 - existing behavior in relation to message delivery changed

Risk:
 - High

Covered with tests:
 - Test ref on github

Recommendations for QA
 - system tests
 - perf tests
 - manual re-connection tests with a pool of nodes if necessary (simple reconnect, requests sent during node disconnect, view change events, catchup);;;","06/Dec/19 10:58 PM;sergey.khoroshavin;*Versions*
sovtoken 1.0.6~dev128
indy-node 1.12.1~dev1154
indy-plenum 1.12.1~dev974;;;","10/Dec/19 12:38 AM;VladimirWork;We have test_consensus_n_and_f_changing system test failed intermittently: https://build.sovrin.org/blue/organizations/jenkins/indy-node%2Findy-node-cd/detail/master/1155/tests after merging this fix.
It fails always at the same step: we start two nodes to reach write consensus and 15 seconds are not enough for this now (if I change 15 to 30 or 60 tests passes) but before the fix it didn't fail.;;;","10/Dec/19 12:39 AM;VladimirWork;FYI [~donqui][~sergey.khoroshavin];;;","10/Dec/19 12:50 AM;donqui;[~VladimirWork] [~sergey.khoroshavin] I've found this test failing previously: [https://build.sovrin.org/job/indy-node/job/indy-node-cd/job/master/1135/]

I'll check the logs out to see if I can see if it relates to theses changes.;;;","11/Dec/19 12:52 AM;VladimirWork;Root cause of the CD issue was found and it is not connected to zmq fixes so system tests are ok. Load test results with this fixes also look good.;;;",,,,,,,,,,,,,,,
Implement integration tests to check all states' consistency,INDY-2290,43266,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,VladimirWork,VladimirWork,11/Nov/19 9:42 PM,11/Nov/19 9:42 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,Implement integration tests to check all states' consistency.,,,,,,,,,,,,,,,,,,,,,INDY-2284,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i014eb:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write a script to upgrade audit ledger for the new state recovered from ledger,INDY-2291,43267,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,11/Nov/19 10:14 PM,15/Nov/19 12:14 AM,28/Oct/23 2:47 AM,13/Nov/19 12:47 AM,,,,,,,,0,,,,,"As a result of bugs (INDY-2283), we have live Indy networks that have incorrect merkle hash calculations in the audit history. This prevents new nodes from coming into consensus in these pools.

We should migrate the history to be correct.",,,,,,,,,,,,,,,,,,,,,INDY-2292,INDY-2293,,INDY-2283,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40p",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,"13/Nov/19 12:47 AM;ashcherbakov;Migrating the history of a durable ledger is ""cheating"". It is preferable to special case the logic in that time period to arrive at the incorrect merkle hash.

We are doing INDY-2292 instead;;;",,,,,,,,,,,,,,,,,,,,,
Take into account txn history when recovering state from the ledger for new nodes,INDY-2292,43268,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,11/Nov/19 10:15 PM,27/Nov/19 9:25 PM,28/Oct/23 2:47 AM,25/Nov/19 4:23 PM,,,1.12.0,,,,,0,,,,,"Due to undetected bugs in the way properties are added to the node state, there are live Indy ledgers with incorrectly calculated hashes. This prevents new nodes in those pools from entering into consensus.

We should special case the merkle hash calculation so that entries in the audit ledger are created using the logic that was used by historical versions of Indy this allows new nodes to arrive at the same merkle root as all other nodes in the pool.

*Acceptance criteria:*
 * Take into account the txn version as it was at the time of applying txn when recovering Config state from the ledger (as part of catchup or as part of recover state from the ledger procedure)
 * Implement it in a generic way using a txn version concept
 * Get the ""virtual"" txn version using the POOL_UPGRADE txns from the Config Ledger
 * The transaction history must explicitly account for auth_rules transactions in version 1.9.1 (see INDY-2283).
 ** A comment exists in the code linking to this INDY-2283 so that future developers can easily find the context for the change.
 * _Evaluate if needed:_ There needs to also be a way to allow ledger plugins to specify a historical calculation (see SN-8 for a historical problem with Sovrin Fees in Indy 1.7.1)",,,,,,,,,,,,,,,,,,,,,,,,INDY-2283,INDY-2291,INDY-2293,,,,,"21/Nov/19 12:39 AM;VladimirWork;2292.tar.gz;https://jira.hyperledger.org/secure/attachment/17958/2292.tar.gz","21/Nov/19 8:51 PM;VladimirWork;2292_token_state.tar.gz;https://jira.hyperledger.org/secure/attachment/17961/2292_token_state.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i401",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,Toktar,VladimirWork,,,,,,,,"12/Nov/19 9:09 PM;Toktar;*PoA:*
PoA:
1. Add a new class StateVersionController:
self._version - str - version of current restoring process. Shouldn't cleared (This affects the need to logically break for restoring phases or not)
set_version(txn) - check txn type and schedule update self._version if this is UPGRADE_POOL (use first, second or medium time)
2. Add to DataBaseManager init() creation of StateVersionController
4. Add to WriteRequestManager method restore_state(txn, request=None, isCommitted=False), which will set version, call update state and commit
5. Call restore_state instead update_state in Node.postTxnFromCatchupAddedToLedger()
6. Call restore_state instead update_state in LedgersBootstrap._init_state_from_ledger()
7. Add to WriteRequestHandler.update_state choose a method to update state
___
8. Add integration tests to restoring a state without real upgrades.;;;","15/Nov/19 12:26 AM;esplinr;It might not be needed at this time to have logic for using historical calculation by ledger plugins, as INDY-2293 would be sufficient to fix Sovrin Staging Net (since we don't need to preserve the auditability of history on Staging Net).;;;","21/Nov/19 12:39 AM;VladimirWork;Logs for the issue with KeyError:  [^2292.tar.gz] ;;;","21/Nov/19 1:37 AM;ashcherbakov;The KeyError issue is related to INDY-2262 and fixed in https://github.com/hyperledger/indy-plenum/pull/1410;;;","21/Nov/19 8:51 PM;VladimirWork;Logs for the issue with sovtoken state:  [^2292_token_state.tar.gz] ;;;","26/Nov/19 4:28 PM;Toktar;*Test1* - for StagingNet
 # Start pool of 7 nodes
 # Upgrade to 1.1.24
 # Write down set_fees
 # Upgrade to the last stable version
 # Upgrade to master
 # Write new set_fees
 # Add a new Node
 # Restart one of the nodes with the state removing.
 # Check that all nodes can order.


*Test2 -* MainNet
 # Start pool of 7 nodes
 # Upgrade to 1.1.52
 # Write auth_rule1 (without specifying off_ledger value) and other transactions
 # Upgrade to the last stable version
 # Upgrade to master
 # Write new auth_rule2
 # Add a new Node
 # Restart one of the nodes with the state removing.
 # Check that all nodes can order.;;;",,,,,,,,,,,,,,,,
Recreate config state on all Staging Net nodes as part of migration script,INDY-2293,43269,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,11/Nov/19 10:20 PM,18/Nov/19 9:48 PM,28/Oct/23 2:47 AM,13/Nov/19 12:48 AM,,,,,,,,0,,,,,"The problem discovered in INDY-2283 could be addressed by recreating the config state, but that won't fix the historical audit ledger entries that might be needed to perform a ledger audit.",,,,,,,,,,,,,,,,,,,,,INDY-2292,,,INDY-2283,INDY-2291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40q",,,,Unset,Unset,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,"13/Nov/19 12:48 AM;ashcherbakov;We are doing INDY-2292 instead;;;","15/Nov/19 12:30 AM;esplinr;This might be the best way to address SN-8, since we don't need to preserve the auditibality of historical transactions on the Sovrin Staging Net. It should not be used to fix problems with Sovrin Main Net.;;;","18/Nov/19 9:48 PM;ashcherbakov;We are going to do a historical fix in the scope of INDY-2292 instead;;;",,,,,,,,,,,,,,,,,,,
Get rid of transport batches,INDY-2294,43270,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,11/Nov/19 10:28 PM,07/Dec/19 3:01 AM,28/Oct/23 2:47 AM,07/Dec/19 12:17 AM,,,1.12.1,,,,,0,,,,,"*Acceptance criteria:*
- Do same load tests with transport batches on and off (TRANSPORT_BATCH_ENABLED = False)
- Compare metrics
- Turn off batches in production if we see better performance
- Remove batches-related code (optionally, can be done in a followup task).
",,,,,,,,,,,,,,,,,,,,,INDY-1677,,,,,,,,,,"06/Dec/19 10:35 PM;anikitinDSR;Node1_with_batches.png;https://jira.hyperledger.org/secure/attachment/18011/Node1_with_batches.png","06/Dec/19 10:34 PM;anikitinDSR;Node1_without_batches.png;https://jira.hyperledger.org/secure/attachment/18010/Node1_without_batches.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40ow",,,,Unset,Unset,Ev-Node 19.24,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"19/Nov/19 6:49 PM;VladimirWork;TRANSPORT_BATCH_ENABLED = False;;;","22/Nov/19 9:35 PM;VladimirWork;Production load results with disabled transport batch:
ev@evernymr33:logs/21_11_2019_transport_batch_off.tar.gz
ev@evernymr33:logs/21_11_2019_transport_batch_off_full_data.tar.gz;;;","05/Dec/19 2:44 AM;anikitinDSR;After small investigations. 
Node1 was as example and some metrics for _*second*_ and _*third*_ hours:


|| ||batches||OFF||batches||ON||
||Parameter||Seconds Total||Samples count||Seconds Total||Samples count||
|NODE_PROD_TIME|6659.87|202667|6675.48|175561|
|UNPACK_BATCH_TIME|0|0|969.61|712199|

 

Throughputs:

 
||Batches OFF||Batches ON||
|9.61/9.97/10.24|9.46/9.97/10.16|

 

Number of 3PC batches:

 
||Batches OFF||Batches ON||
|3307|3795|

 

Total count of requests is almost the same, 71767 vs 71801 (OFF and ON situation).
h2. As result:
 * as we can see from previous statistics result with ""Batches OFF"" looks not worse then with ""Batches ON""
 * UNPACK_BATCH_TIME (969 seconds) was spent for other purposes
 * all other metrics are look almost the same;;;","05/Dec/19 5:14 PM;anikitinDSR;||Batches OFF||Batches ON||
|Start time: 2019-11-19 12:08:00
End time: 2019-11-19 14:08:00
Duration: 2:00:00
Number of messages processed in one looper run:
 Node: 202667 samples, 3299263.00 sum, 0.00/16.28/965.00 min/avg/max, 44.30 stddev
 Client: 202667 samples, 135079.00 sum, 0.00/0.67/54.00 min/avg/max, 1.94 stddev
Seconds passed between looper runs:
 202667 samples, 7195.36 sum, 0.00/0.04/2.50 min/avg/max, 0.10 stddev
Number of messages in one transport batch:
 3299322 samples, 3299322.00 sum, 1.00/1.00/1.00 min/avg/max, 0.00 stddev
Node message size, bytes:
 Outgoing: 3299322 samples, 2716104815.00 sum, 52.00/823.23/9572.00 min/avg/max, 1568.88 stddev
 Incoming: 3299263 samples, 2576672944.00 sum, 52.00/780.98/9572.00 min/avg/max, 1539.07 stddev
Client message size, bytes:
 Outgoing: 297806 samples, 427242273.00 sum, 81.00/1434.63/11664.00 min/avg/max, 1811.49 stddev
 Incoming: 135079 samples, 112902045.00 sum, 348.00/835.82/9479.00 min/avg/max, 1494.41 stddev
Number of requests in one 3PC batch:
 Created: 3307 samples, 71767.00 sum, 0.00/21.70/64.00 min/avg/max, 8.72 stddev
 Ordered: 3307 samples, 71764.00 sum, 0.00/21.70/64.00 min/avg/max, 8.71 stddev
Time spent on write request processing, ms:
 8.69/14.36/261.22 ms min/avg/max, 3.91 stddev
Monitor throughput, TPS
 Master: 9.61/9.97/10.24 min/avg/max, 0.10 stddev
 Backup: 9.68/9.96/10.20 min/avg/max, 0.08 stddev
Monitor latency, seconds
 Master: 2.81/3.64/7.63 min/avg/max, 0.51 stddev
 Backup: 1.43/2.63/6.16 min/avg/max, 0.53 stddev
RAM info:
 Available, Mb: 6533.40/6593.07/6652.84 min/avg/max, 27.26 stddev
 Node RSS, Mb: 991.03/1050.45/1109.21 min/avg/max, 27.38 stddev
 Node VMS, Mb: 1516.68/1564.15/1627.43 min/avg/max, 30.00 stddev
Additional statistics:
 Client incoming/outgoing: 0.45 messages, 0.26 traffic
 Node incoming/outgoing traffic: 0.95
 Node/client traffic: 9.80
 Node traffic per batch: 1600477.10
 Node traffic per request: 73752.55
Profiling info:
 BACKUP_REQUEST_PROCESSING_TIME : 574162 samples, 1.03 seconds total, 0.00/0.00/0.09 ms min/avg/max, 0.00 stddev
 REQUEST_PROCESSING_TIME : 71767 samples, 1030.62 seconds total, 8.69/14.36/261.22 ms min/avg/max, 3.91 stddev
 GC_GEN0_TIME : 32595 samples, 7.11 seconds total, 0.07/0.22/10.84 ms min/avg/max, 0.12 stddev
 GC_GEN1_TIME : 2963 samples, 5.57 seconds total, 1.12/1.88/3.36 ms min/avg/max, 0.34 stddev
 GC_GEN2_TIME : 169 samples, 35.97 seconds total, 67.30/212.85/292.23 ms min/avg/max, 45.74 stddev
 NODE_PROD_TIME : 202667 samples, 6659.87 seconds total, 4.44/32.86/2495.16 ms min/avg/max, 103.27 stddev
 SERVICE_REPLICAS_TIME : 202667 samples, 2617.48 seconds total, 0.20/12.92/1504.53 ms min/avg/max, 72.19 stddev
 SERVICE_NODE_MSGS_TIME : 202667 samples, 2474.17 seconds total, 1.89/12.21/726.61 ms min/avg/max, 26.10 stddev
 SERVICE_CLIENT_MSGS_TIME : 202667 samples, 291.61 seconds total, 0.10/1.44/108.59 ms min/avg/max, 3.76 stddev
 SERVICE_NODE_ACTIONS_TIME : 202667 samples, 33.18 seconds total, 0.00/0.16/80.00 ms min/avg/max, 2.64 stddev
 SERVICE_VIEW_CHANGER_TIME : 202667 samples, 3.59 seconds total, 0.01/0.02/7.38 ms min/avg/max, 0.02 stddev
 SERVICE_OBSERVABLE_TIME : 202667 samples, 1.87 seconds total, 0.01/0.01/0.19 ms min/avg/max, 0.01 stddev
 SERVICE_OBSERVER_TIME : 202667 samples, 1.30 seconds total, 0.00/0.01/0.07 ms min/avg/max, 0.00 stddev
 FLUSH_OUTBOXES_TIME : 202667 samples, 678.89 seconds total, 0.01/3.35/169.12 ms min/avg/max, 8.88 stddev
 SERVICE_NODE_LIFECYCLE_TIME : 202667 samples, 490.56 seconds total, 1.94/2.42/17.03 ms min/avg/max, 0.56 stddev
 SERVICE_CLIENT_STACK_TIME : 202667 samples, 31.04 seconds total, 0.09/0.15/12.71 ms min/avg/max, 0.12 stddev
 SERVICE_MONITOR_ACTIONS_TIME : 202667 samples, 0.86 seconds total, 0.00/0.00/0.35 ms min/avg/max, 0.00 stddev
 SERVICE_TIMERS_TIME : 202667 samples, 6.82 seconds total, 0.01/0.03/30.40 ms min/avg/max, 0.71 stddev
 SERVICE_NODE_STACK_TIME : 202667 samples, 1898.19 seconds total, 1.86/9.37/547.16 ms min/avg/max, 19.08 stddev
 PROCESS_NODE_INBOX_TIME : 202667 samples, 569.88 seconds total, 0.00/2.81/266.18 ms min/avg/max, 7.43 stddev
 SEND_TO_REPLICA_TIME : 1576826 samples, 19.55 seconds total, 0.01/0.01/7.07 ms min/avg/max, 0.01 stddev
 NODE_CHECK_PERFORMANCE_TIME : 698 samples, 0.50 seconds total, 0.55/0.71/1.15 ms min/avg/max, 0.13 stddev
 NODE_CHECK_NODE_REQUEST_SPIKE : 119 samples, 0.00 seconds total, 0.02/0.03/0.06 ms min/avg/max, 0.01 stddev
 UNPACK_BATCH_TIME : no samples
 VERIFY_SIGNATURE_TIME : 1857516 samples, 66.62 seconds total, 0.01/0.04/170.37 ms min/avg/max, 0.18 stddev
 SERVICE_REPLICAS_OUTBOX_TIME : 202667 samples, 158.35 seconds total, 0.01/0.78/159.21 ms min/avg/max, 7.09 stddev
 NODE_SEND_TIME : 137557 samples, 13.67 seconds total, 0.04/0.10/12.15 ms min/avg/max, 0.06 stddev
 NODE_SEND_REJECT_TIME : no samples
 VALIDATE_NODE_MSG_TIME : 3299263 samples, 1126.03 seconds total, 0.06/0.34/293.14 ms min/avg/max, 1.50 stddev
 INT_VALIDATE_NODE_MSG_TIME : 3299263 samples, 487.06 seconds total, 0.03/0.15/292.64 ms min/avg/max, 1.47 stddev
 PROCESS_ORDERED_TIME : 32692 samples, 145.37 seconds total, 0.04/4.45/158.89 ms min/avg/max, 14.31 stddev
 MONITOR_REQUEST_ORDERED_TIME : 32692 samples, 5.71 seconds total, 0.02/0.17/2.86 ms min/avg/max, 0.09 stddev
 EXECUTE_BATCH_TIME : 3307 samples, 138.09 seconds total, 1.02/41.76/158.11 ms min/avg/max, 20.83 stddev
 SERVICE_REPLICA_QUEUES_TIME : 202667 samples, 2194.82 seconds total, 0.05/10.83/1426.85 ms min/avg/max, 67.64 stddev
 SERVICE_BACKUP_REPLICAS_QUEUES_TIME : 1621336 samples, 241.41 seconds total, 0.01/0.15/266.37 ms min/avg/max, 1.13 stddev
 PROCESS_PREPREPARE_TIME : no samples
 PROCESS_PREPARE_TIME : 79368 samples, 20.91 seconds total, 0.01/0.26/8.30 ms min/avg/max, 0.75 stddev
 PROCESS_COMMIT_TIME : 79368 samples, 1108.37 seconds total, 0.02/13.96/69.64 ms min/avg/max, 10.85 stddev
 PROCESS_CHECKPOINT_TIME : 792 samples, 4.56 seconds total, 0.09/5.76/257.23 ms min/avg/max, 29.31 stddev
 SEND_PREPREPARE_TIME : 3307 samples, 0.31 seconds total, 0.04/0.09/0.21 ms min/avg/max, 0.02 stddev
 SEND_PREPARE_TIME : no samples
 SEND_COMMIT_TIME : 3307 samples, 11.70 seconds total, 2.11/3.54/6.54 ms min/avg/max, 1.05 stddev
 SEND_CHECKPOINT_TIME : 33 samples, 0.01 seconds total, 0.23/0.25/0.36 ms min/avg/max, 0.03 stddev
 CREATE_3PC_BATCH_TIME : 3307 samples, 1041.17 seconds total, 1.26/314.84/1089.34 ms min/avg/max, 133.41 stddev
 ORDER_3PC_BATCH_TIME : 3307 samples, 37.07 seconds total, 7.70/11.21/20.55 ms min/avg/max, 2.51 stddev
 BACKUP_PROCESS_PREPREPARE_TIME : 29385 samples, 32.52 seconds total, 0.57/1.11/194.34 ms min/avg/max, 1.19 stddev
 BACKUP_PROCESS_PREPARE_TIME : 687973 samples, 72.09 seconds total, 0.01/0.10/8.28 ms min/avg/max, 0.05 stddev
 BACKUP_PROCESS_COMMIT_TIME : 705707 samples, 76.04 seconds total, 0.01/0.11/12.19 ms min/avg/max, 0.16 stddev
 BACKUP_PROCESS_CHECKPOINT_TIME : 6936 samples, 13.69 seconds total, 0.08/1.97/263.18 ms min/avg/max, 13.08 stddev
 BACKUP_SEND_PREPREPARE_TIME : no samples
 BACKUP_SEND_PREPARE_TIME : 29385 samples, 4.92 seconds total, 0.11/0.17/193.23 ms min/avg/max, 1.13 stddev
 BACKUP_SEND_COMMIT_TIME : 29379 samples, 3.76 seconds total, 0.09/0.13/0.41 ms min/avg/max, 0.03 stddev
 BACKUP_SEND_CHECKPOINT_TIME : 289 samples, 0.06 seconds total, 0.16/0.21/0.32 ms min/avg/max, 0.03 stddev
 BACKUP_CREATE_3PC_BATCH_TIME : no samples
 BACKUP_ORDER_3PC_BATCH_TIME : 29385 samples, 20.08 seconds total, 0.42/0.68/9.32 ms min/avg/max, 0.18 stddev
 PROCESS_PROPAGATE_TIME : 1722437 samples, 422.97 seconds total, 0.13/0.25/253.36 ms min/avg/max, 0.51 stddev
 PROCESS_MESSAGE_REQ_TIME : 2 samples, 0.00 seconds total, 0.24/0.34/0.44 ms min/avg/max, 0.14 stddev
 PROCESS_MESSAGE_REP_TIME : 88 samples, 0.02 seconds total, 0.16/0.23/0.41 ms min/avg/max, 0.06 stddev
 PROCESS_LEDGER_STATUS_TIME : no samples
 PROCESS_CONSISTENCY_PROOF_TIME : no samples
 PROCESS_CATCHUP_REQ_TIME : no samples
 PROCESS_CATCHUP_REP_TIME : no samples
 PROCESS_REQUEST_TIME : 135079 samples, 142.15 seconds total, 0.20/1.05/14.00 ms min/avg/max, 0.88 stddev
 SEND_PROPAGATE_TIME : 71767 samples, 29.75 seconds total, 0.24/0.41/252.92 ms min/avg/max, 2.04 stddev
 SEND_MESSAGE_REQ_TIME : 331 samples, 0.02 seconds total, 0.02/0.07/0.34 ms min/avg/max, 0.07 stddev
 SEND_MESSAGE_REP_TIME : 2 samples, 0.00 seconds total, 0.21/0.29/0.37 ms min/avg/max, 0.11 stddev
 BLS_VALIDATE_PREPREPARE_TIME : 29385 samples, 0.15 seconds total, 0.00/0.00/0.05 ms min/avg/max, 0.00 stddev
 BLS_VALIDATE_COMMIT_TIME : 523078 samples, 1063.63 seconds total, 0.00/2.03/69.02 ms min/avg/max, 6.23 stddev
 BLS_UPDATE_PREPREPARE_TIME : 3307 samples, 0.05 seconds total, 0.00/0.02/0.06 ms min/avg/max, 0.01 stddev
 BLS_UPDATE_COMMIT_TIME : 3307 samples, 10.96 seconds total, 1.95/3.31/6.19 ms min/avg/max, 1.02 stddev
 DESERIALIZE_DURING_UNPACK_TIME : no samples|Start time: 2019-11-28 16:27:00
End time: 2019-11-28 18:27:00
Duration: 2:00:00
Number of messages processed in one looper run:
 Node: 175561 samples, 1393116.00 sum, 0.00/7.94/438.00 min/avg/max, 21.47 stddev
 Client: 175561 samples, 135509.00 sum, 0.00/0.77/45.00 min/avg/max, 2.14 stddev
Seconds passed between looper runs:
 175561 samples, 7195.02 sum, 0.00/0.04/2.57 min/avg/max, 0.11 stddev
Number of messages in one transport batch:
 1150080 samples, 3512564.00 sum, 1.00/3.05/48.00 min/avg/max, 3.17 stddev
Node message size, bytes:
 Outgoing: 1150080 samples, 2918893172.00 sum, 52.00/2537.99/98020.00 min/avg/max, 4938.53 stddev
 Incoming: 1393116 samples, 2763151701.00 sum, 52.00/1983.43/72216.00 min/avg/max, 3304.24 stddev
Client message size, bytes:
 Outgoing: 298321 samples, 434098352.00 sum, 81.00/1455.14/11715.00 min/avg/max, 1890.26 stddev
 Incoming: 135509 samples, 113037119.00 sum, 348.00/834.17/9478.00 min/avg/max, 1492.82 stddev
Number of requests in one 3PC batch:
 Created: 3795 samples, 71801.00 sum, 0.00/18.92/67.00 min/avg/max, 6.32 stddev
 Ordered: 3794 samples, 71762.00 sum, 0.00/18.91/67.00 min/avg/max, 6.32 stddev
Time spent on write request processing, ms:
 8.05/14.24/271.03 ms min/avg/max, 3.87 stddev
Monitor throughput, TPS
 Master: 9.46/9.97/10.16 min/avg/max, 0.10 stddev
 Backup: 9.74/9.96/10.20 min/avg/max, 0.08 stddev
Monitor latency, seconds
 Master: 2.81/3.80/7.91 min/avg/max, 0.49 stddev
 Backup: 1.64/2.75/4.40 min/avg/max, 0.46 stddev
RAM info:
 Available, Mb: 6480.10/6539.96/6622.66 min/avg/max, 38.67 stddev
 Node RSS, Mb: 955.23/1038.41/1099.82 min/avg/max, 39.71 stddev
 Node VMS, Mb: 1446.88/1522.20/1577.88 min/avg/max, 32.73 stddev
Additional statistics:
 Client incoming/outgoing: 0.45 messages, 0.26 traffic
 Node incoming/outgoing traffic: 0.95
 Node/client traffic: 10.39
 Node traffic per batch: 1497639.66
 Node traffic per request: 79179.02
Profiling info:
 BACKUP_REQUEST_PROCESSING_TIME : 574135 samples, 1.03 seconds total, 0.00/0.00/0.05 ms min/avg/max, 0.00 stddev
 REQUEST_PROCESSING_TIME : 71801 samples, 1022.40 seconds total, 8.05/14.24/271.03 ms min/avg/max, 3.87 stddev
 GC_GEN0_TIME : 33798 samples, 7.55 seconds total, 0.06/0.22/9.32 ms min/avg/max, 0.12 stddev
 GC_GEN1_TIME : 3072 samples, 5.88 seconds total, 1.11/1.91/3.50 ms min/avg/max, 0.36 stddev
 GC_GEN2_TIME : 180 samples, 37.60 seconds total, 92.74/208.86/278.26 ms min/avg/max, 37.97 stddev
 NODE_PROD_TIME : 175561 samples, 6675.48 seconds total, 4.59/38.02/2568.74 ms min/avg/max, 113.71 stddev
 SERVICE_REPLICAS_TIME : 175561 samples, 2791.49 seconds total, 0.21/15.90/1813.40 ms min/avg/max, 80.84 stddev
 SERVICE_NODE_MSGS_TIME : 175561 samples, 2555.06 seconds total, 1.96/14.55/1298.23 ms min/avg/max, 30.51 stddev
 SERVICE_CLIENT_MSGS_TIME : 175561 samples, 286.11 seconds total, 0.10/1.63/161.30 ms min/avg/max, 4.09 stddev
 SERVICE_NODE_ACTIONS_TIME : 175561 samples, 31.35 seconds total, 0.00/0.18/94.09 ms min/avg/max, 2.71 stddev
 SERVICE_VIEW_CHANGER_TIME : 175561 samples, 3.07 seconds total, 0.01/0.02/0.10 ms min/avg/max, 0.01 stddev
 SERVICE_OBSERVABLE_TIME : 175561 samples, 1.79 seconds total, 0.01/0.01/9.80 ms min/avg/max, 0.03 stddev
 SERVICE_OBSERVER_TIME : 175561 samples, 1.04 seconds total, 0.00/0.01/0.10 ms min/avg/max, 0.00 stddev
 FLUSH_OUTBOXES_TIME : 175561 samples, 505.70 seconds total, 0.01/2.88/241.69 ms min/avg/max, 6.48 stddev
 SERVICE_NODE_LIFECYCLE_TIME : 175561 samples, 439.10 seconds total, 2.01/2.50/15.92 ms min/avg/max, 0.59 stddev
 SERVICE_CLIENT_STACK_TIME : 175561 samples, 28.71 seconds total, 0.09/0.16/4.00 ms min/avg/max, 0.13 stddev
 SERVICE_MONITOR_ACTIONS_TIME : 175561 samples, 0.79 seconds total, 0.00/0.00/0.23 ms min/avg/max, 0.00 stddev
 SERVICE_TIMERS_TIME : 175561 samples, 6.80 seconds total, 0.01/0.04/34.90 ms min/avg/max, 0.79 stddev
 SERVICE_NODE_STACK_TIME : 175561 samples, 1974.37 seconds total, 1.93/11.25/1036.38 ms min/avg/max, 22.68 stddev
 PROCESS_NODE_INBOX_TIME : 175561 samples, 575.48 seconds total, 0.00/3.28/263.21 ms min/avg/max, 8.15 stddev
 SEND_TO_REPLICA_TIME : 1790343 samples, 20.86 seconds total, 0.01/0.01/0.18 ms min/avg/max, 0.00 stddev
 NODE_CHECK_PERFORMANCE_TIME : 694 samples, 0.50 seconds total, 0.55/0.72/1.10 ms min/avg/max, 0.13 stddev
 NODE_CHECK_NODE_REQUEST_SPIKE : 119 samples, 0.00 seconds total, 0.02/0.03/0.06 ms min/avg/max, 0.01 stddev
 UNPACK_BATCH_TIME : 712199 samples, 969.61 seconds total, 0.19/1.36/282.22 ms min/avg/max, 2.91 stddev
 VERIFY_SIGNATURE_TIME : 1857961 samples, 67.14 seconds total, 0.01/0.04/245.16 ms min/avg/max, 0.33 stddev
 SERVICE_REPLICAS_OUTBOX_TIME : 175561 samples, 157.19 seconds total, 0.01/0.90/172.08 ms min/avg/max, 7.47 stddev
 NODE_SEND_TIME : 146382 samples, 14.13 seconds total, 0.04/0.10/8.22 ms min/avg/max, 0.05 stddev
 NODE_SEND_REJECT_TIME : no samples
 VALIDATE_NODE_MSG_TIME : 4224994 samples, 1197.91 seconds total, 0.05/0.28/279.36 ms min/avg/max, 1.36 stddev
 INT_VALIDATE_NODE_MSG_TIME : 4224994 samples, 530.72 seconds total, 0.03/0.13/278.80 ms min/avg/max, 1.33 stddev
 PROCESS_ORDERED_TIME : 37119 samples, 143.79 seconds total, 0.04/3.87/171.94 ms min/avg/max, 12.27 stddev
 MONITOR_REQUEST_ORDERED_TIME : 37119 samples, 5.76 seconds total, 0.02/0.16/1.02 ms min/avg/max, 0.08 stddev
 EXECUTE_BATCH_TIME : 3794 samples, 136.31 seconds total, 1.01/35.93/171.09 ms min/avg/max, 17.17 stddev
 SERVICE_REPLICA_QUEUES_TIME : 175561 samples, 2353.75 seconds total, 0.05/13.41/1745.39 ms min/avg/max, 75.74 stddev
 SERVICE_BACKUP_REPLICAS_QUEUES_TIME : 1404488 samples, 260.38 seconds total, 0.01/0.19/251.97 ms min/avg/max, 1.17 stddev
 PROCESS_PREPREPARE_TIME : no samples
 PROCESS_PREPARE_TIME : 91056 samples, 23.23 seconds total, 0.01/0.26/6.88 ms min/avg/max, 0.74 stddev
 PROCESS_COMMIT_TIME : 91056 samples, 1272.87 seconds total, 0.02/13.98/71.64 ms min/avg/max, 10.88 stddev
 PROCESS_CHECKPOINT_TIME : 912 samples, 4.51 seconds total, 0.09/4.94/245.92 ms min/avg/max, 25.01 stddev
 SEND_PREPREPARE_TIME : 3795 samples, 0.34 seconds total, 0.05/0.09/0.22 ms min/avg/max, 0.02 stddev
 SEND_PREPARE_TIME : no samples
 SEND_COMMIT_TIME : 3794 samples, 13.22 seconds total, 2.13/3.49/6.67 ms min/avg/max, 1.05 stddev
 SEND_CHECKPOINT_TIME : 38 samples, 0.01 seconds total, 0.23/0.28/0.61 ms min/avg/max, 0.07 stddev
 CREATE_3PC_BATCH_TIME : 3795 samples, 1033.97 seconds total, 1.29/272.46/953.36 ms min/avg/max, 100.51 stddev
 ORDER_3PC_BATCH_TIME : 3794 samples, 42.56 seconds total, 7.72/11.22/20.24 ms min/avg/max, 2.70 stddev
 BACKUP_PROCESS_PREPREPARE_TIME : 33320 samples, 35.43 seconds total, 0.59/1.06/5.55 ms min/avg/max, 0.31 stddev
 BACKUP_PROCESS_PREPARE_TIME : 776284 samples, 77.69 seconds total, 0.01/0.10/8.28 ms min/avg/max, 0.05 stddev
 BACKUP_PROCESS_COMMIT_TIME : 799857 samples, 84.21 seconds total, 0.01/0.11/9.29 ms min/avg/max, 0.16 stddev
 BACKUP_PROCESS_CHECKPOINT_TIME : 7872 samples, 14.05 seconds total, 0.08/1.78/250.63 ms min/avg/max, 11.01 stddev
 BACKUP_SEND_PREPREPARE_TIME : no samples
 BACKUP_SEND_PREPARE_TIME : 33320 samples, 5.32 seconds total, 0.11/0.16/2.70 ms min/avg/max, 0.04 stddev
 BACKUP_SEND_COMMIT_TIME : 33319 samples, 4.18 seconds total, 0.09/0.13/0.43 ms min/avg/max, 0.03 stddev
 BACKUP_SEND_CHECKPOINT_TIME : 328 samples, 0.07 seconds total, 0.17/0.21/1.15 ms min/avg/max, 0.09 stddev
 BACKUP_CREATE_3PC_BATCH_TIME : no samples
 BACKUP_ORDER_3PC_BATCH_TIME : 33325 samples, 21.92 seconds total, 0.42/0.66/9.04 ms min/avg/max, 0.19 stddev
 PROCESS_PROPAGATE_TIME : 1722452 samples, 420.81 seconds total, 0.13/0.24/214.63 ms min/avg/max, 0.26 stddev
 PROCESS_MESSAGE_REQ_TIME : no samples
 PROCESS_MESSAGE_REP_TIME : 19 samples, 0.00 seconds total, 0.16/0.24/0.37 ms min/avg/max, 0.07 stddev
 PROCESS_LEDGER_STATUS_TIME : no samples
 PROCESS_CONSISTENCY_PROOF_TIME : no samples
 PROCESS_CATCHUP_REQ_TIME : no samples
 PROCESS_CATCHUP_REP_TIME : no samples
 PROCESS_REQUEST_TIME : 135509 samples, 139.54 seconds total, 0.19/1.03/13.73 ms min/avg/max, 0.84 stddev
 SEND_PROPAGATE_TIME : 71768 samples, 28.93 seconds total, 0.24/0.40/214.39 ms min/avg/max, 1.08 stddev
 SEND_MESSAGE_REQ_TIME : 55 samples, 0.00 seconds total, 0.02/0.08/0.25 ms min/avg/max, 0.07 stddev
 SEND_MESSAGE_REP_TIME : no samples
 BLS_VALIDATE_PREPREPARE_TIME : 33320 samples, 0.17 seconds total, 0.00/0.00/0.08 ms min/avg/max, 0.00 stddev
 BLS_VALIDATE_COMMIT_TIME : 593844 samples, 1221.31 seconds total, 0.00/2.06/71.32 ms min/avg/max, 6.27 stddev
 BLS_UPDATE_PREPREPARE_TIME : 3795 samples, 0.05 seconds total, 0.00/0.01/0.08 ms min/avg/max, 0.01 stddev
 BLS_UPDATE_COMMIT_TIME : 3794 samples, 12.37 seconds total, 1.97/3.26/6.33 ms min/avg/max, 1.01 stddev
 DESERIALIZE_DURING_UNPACK_TIME : no samples|;;;","06/Dec/19 7:07 PM;VladimirWork;The latest stable load test results:

ev@evernymr33:logs/stable_transport_false_05_12_2019.tar.gz
ev@evernymr33:logs/stable_transport_false_05_12_2019_full_data.tar.gz

ev@evernymr33:logs/stable_transport_true_05_12_2019.tar.gz
ev@evernymr33:logs/stable_transport_true_05_12_2019_full_data.tar.gz;;;","06/Dec/19 10:16 PM;anikitinDSR;After the last load test we have the next results:
||Batches ON||Batches OFF||
| Start time: 2019-12-05 16:09:00
End time: 2019-12-05 17:09:00
Duration: 1:00:00
Number of messages processed in one looper run:
 Node: 131723 samples, 817121.00 sum, 0.00/6.20/525.00 min/avg/max, 19.54 stddev
 Client: 131723 samples, 67646.00 sum, 0.00/0.51/56.00 min/avg/max, 1.76 stddev
Seconds passed between looper runs:
 131723 samples, 3596.79 sum, 0.00/0.03/2.21 min/avg/max, 0.09 stddev
Number of messages in one transport batch:
 675096 samples, 1445333.00 sum, 1.00/2.14/36.00 min/avg/max, 2.50 stddev
Node message size, bytes:
 Outgoing: 675096 samples, 1390210946.00 sum, 51.00/2059.28/116993.00 min/avg/max, 5176.64 stddev
 Incoming: 817121 samples, 1308621918.00 sum, 51.00/1601.50/84224.00 min/avg/max, 3099.80 stddev
Client message size, bytes:
 Outgoing: 147448 samples, 233628939.00 sum, 81.00/1584.48/11567.00 min/avg/max, 2186.06 stddev
 Incoming: 67646 samples, 56434172.00 sum, 348.00/834.26/9478.00 min/avg/max, 1490.49 stddev
Number of requests in one 3PC batch:
 Created: 1203 samples, 35876.00 sum, 0.00/29.82/66.00 min/avg/max, 10.19 stddev
 Ordered: 1202 samples, 35877.00 sum, 0.00/29.85/66.00 min/avg/max, 10.17 stddev
Time spent on write request processing, ms:
 8.57/15.61/393.49 ms min/avg/max, 4.44 stddev
Monitor throughput, TPS
 Master: 9.56/9.97/10.18 min/avg/max, 0.11 stddev
 Backup: 9.68/9.96/10.17 min/avg/max, 0.09 stddev
Monitor latency, seconds
 Master: 2.91/3.66/6.89 min/avg/max, 0.70 stddev
 Backup: 1.39/2.30/4.24 min/avg/max, 0.53 stddev
RAM info:
 Available, Mb: 6554.63/6585.47/6633.20 min/avg/max, 24.63 stddev
 Node RSS, Mb: 945.44/992.30/1021.79 min/avg/max, 24.49 stddev
 Node VMS, Mb: 1417.66/1470.18/1496.41 min/avg/max, 21.49 stddev
Additional statistics:
 Client incoming/outgoing: 0.46 messages, 0.24 traffic
 Node incoming/outgoing traffic: 0.94
 Node/client traffic: 9.30
 Node traffic per batch: 2245285.24
 Node traffic per request: 75224.60
Profiling info:
 BACKUP_REQUEST_PROCESSING_TIME : 287024 samples, 0.52 seconds total, 0.00/0.00/0.05 ms min/avg/max, 0.00 stddev
 REQUEST_PROCESSING_TIME : 35876 samples, 560.02 seconds total, 8.57/15.61/393.49 ms min/avg/max, 4.44 stddev
 GC_GEN0_TIME : 15711 samples, 3.90 seconds total, 0.07/0.25/1.33 ms min/avg/max, 0.11 stddev
 GC_GEN1_TIME : 1428 samples, 2.79 seconds total, 1.13/1.96/3.73 ms min/avg/max, 0.41 stddev
 GC_GEN2_TIME : 69 samples, 19.21 seconds total, 126.95/278.35/388.66 ms min/avg/max, 69.92 stddev
 NODE_PROD_TIME : 131723 samples, 3262.88 seconds total, 4.34/24.77/2208.51 ms min/avg/max, 90.11 stddev
 SERVICE_REPLICAS_TIME : 131723 samples, 1188.19 seconds total, 0.21/9.02/1408.88 ms min/avg/max, 61.51 stddev
 SERVICE_NODE_MSGS_TIME : 131723 samples, 1301.38 seconds total, 1.85/9.88/752.74 ms min/avg/max, 25.13 stddev
 SERVICE_CLIENT_MSGS_TIME : 131723 samples, 153.77 seconds total, 0.10/1.17/118.34 ms min/avg/max, 3.47 stddev
 SERVICE_NODE_ACTIONS_TIME : 131723 samples, 20.42 seconds total, 0.00/0.16/75.66 ms min/avg/max, 2.86 stddev
 SERVICE_VIEW_CHANGER_TIME : 131723 samples, 2.88 seconds total, 0.01/0.02/0.11 ms min/avg/max, 0.01 stddev
 SERVICE_OBSERVABLE_TIME : 131723 samples, 1.51 seconds total, 0.01/0.01/0.21 ms min/avg/max, 0.01 stddev
 SERVICE_OBSERVER_TIME : 131723 samples, 1.01 seconds total, 0.00/0.01/0.09 ms min/avg/max, 0.00 stddev
 FLUSH_OUTBOXES_TIME : 131723 samples, 240.54 seconds total, 0.01/1.83/185.07 ms min/avg/max, 5.32 stddev
 SERVICE_NODE_LIFECYCLE_TIME : 131723 samples, 308.26 seconds total, 1.88/2.34/11.64 ms min/avg/max, 0.52 stddev
 SERVICE_CLIENT_STACK_TIME : 131723 samples, 19.44 seconds total, 0.08/0.15/7.89 ms min/avg/max, 0.11 stddev
 SERVICE_MONITOR_ACTIONS_TIME : 131723 samples, 0.70 seconds total, 0.00/0.01/0.98 ms min/avg/max, 0.00 stddev
 SERVICE_TIMERS_TIME : 131723 samples, 4.38 seconds total, 0.01/0.03/30.00 ms min/avg/max, 0.70 stddev
 SERVICE_NODE_STACK_TIME : 131723 samples, 1021.91 seconds total, 1.82/7.76/648.64 ms min/avg/max, 18.60 stddev
 PROCESS_NODE_INBOX_TIME : 131723 samples, 275.26 seconds total, 0.00/2.09/408.18 ms min/avg/max, 6.96 stddev
 SEND_TO_REPLICA_TIME : 584279 samples, 7.84 seconds total, 0.01/0.01/0.62 ms min/avg/max, 0.01 stddev
 NODE_CHECK_PERFORMANCE_TIME : 350 samples, 0.25 seconds total, 0.57/0.72/1.10 ms min/avg/max, 0.13 stddev
 NODE_CHECK_NODE_REQUEST_SPIKE : 60 samples, 0.00 seconds total, 0.02/0.03/0.05 ms min/avg/max, 0.01 stddev
 UNPACK_BATCH_TIME : 267376 samples, 362.02 seconds total, 0.18/1.35/392.97 ms min/avg/max, 3.27 stddev
 VERIFY_SIGNATURE_TIME : 928673 samples, 35.22 seconds total, 0.01/0.04/374.79 ms min/avg/max, 0.41 stddev
 SERVICE_REPLICAS_OUTBOX_TIME : 131723 samples, 75.65 seconds total, 0.02/0.57/158.40 ms min/avg/max, 6.07 stddev
 NODE_SEND_TIME : 60227 samples, 6.80 seconds total, 0.04/0.11/0.41 ms min/avg/max, 0.04 stddev
 NODE_SEND_REJECT_TIME : no samples
 VALIDATE_NODE_MSG_TIME : 1712682 samples, 566.63 seconds total, 0.05/0.33/389.21 ms min/avg/max, 1.73 stddev
 INT_VALIDATE_NODE_MSG_TIME : 1712682 samples, 242.38 seconds total, 0.03/0.14/388.84 ms min/avg/max, 1.69 stddev
 PROCESS_ORDERED_TIME : 12111 samples, 69.02 seconds total, 0.04/5.70/157.32 ms min/avg/max, 18.14 stddev
 MONITOR_REQUEST_ORDERED_TIME : 12111 samples, 2.80 seconds total, 0.02/0.23/0.96 ms min/avg/max, 0.10 stddev
 EXECUTE_BATCH_TIME : 1202 samples, 65.60 seconds total, 1.03/54.58/156.03 ms min/avg/max, 24.43 stddev
 SERVICE_REPLICA_QUEUES_TIME : 131723 samples, 998.29 seconds total, 0.05/7.58/1274.98 ms min/avg/max, 58.33 stddev
 SERVICE_BACKUP_REPLICAS_QUEUES_TIME : 1053784 samples, 98.12 seconds total, 0.01/0.09/382.79 ms min/avg/max, 1.06 stddev
 PROCESS_PREPREPARE_TIME : no samples
 PROCESS_PREPARE_TIME : 28848 samples, 8.09 seconds total, 0.01/0.28/6.69 ms min/avg/max, 0.78 stddev
 PROCESS_COMMIT_TIME : 28847 samples, 408.46 seconds total, 0.02/14.16/59.18 ms min/avg/max, 11.01 stddev
 PROCESS_CHECKPOINT_TIME : 288 samples, 3.04 seconds total, 0.09/10.56/470.35 ms min/avg/max, 54.44 stddev
 SEND_PREPREPARE_TIME : 1203 samples, 0.13 seconds total, 0.04/0.11/0.24 ms min/avg/max, 0.03 stddev
 SEND_PREPARE_TIME : no samples
 SEND_COMMIT_TIME : 1202 samples, 4.39 seconds total, 2.12/3.65/6.33 ms min/avg/max, 1.08 stddev
 SEND_CHECKPOINT_TIME : 12 samples, 0.00 seconds total, 0.24/0.26/0.35 ms min/avg/max, 0.03 stddev
 CREATE_3PC_BATCH_TIME : 1203 samples, 565.00 seconds total, 1.31/469.66/1238.65 ms min/avg/max, 176.14 stddev
 ORDER_3PC_BATCH_TIME : 1202 samples, 13.60 seconds total, 7.86/11.31/21.39 ms min/avg/max, 2.52 stddev
 BACKUP_PROCESS_PREPREPARE_TIME : 10909 samples, 13.90 seconds total, 0.58/1.27/4.63 ms min/avg/max, 0.36 stddev
 BACKUP_PROCESS_PREPARE_TIME : 254554 samples, 26.66 seconds total, 0.01/0.10/1.15 ms min/avg/max, 0.05 stddev
 BACKUP_PROCESS_COMMIT_TIME : 261857 samples, 28.98 seconds total, 0.02/0.11/5.31 ms min/avg/max, 0.17 stddev
 BACKUP_PROCESS_CHECKPOINT_TIME : 2664 samples, 6.43 seconds total, 0.08/2.41/374.01 ms min/avg/max, 18.07 stddev
 BACKUP_SEND_PREPREPARE_TIME : no samples
 BACKUP_SEND_PREPARE_TIME : 10909 samples, 1.80 seconds total, 0.11/0.16/2.31 ms min/avg/max, 0.05 stddev
 BACKUP_SEND_COMMIT_TIME : 10909 samples, 1.47 seconds total, 0.09/0.13/0.45 ms min/avg/max, 0.03 stddev
 BACKUP_SEND_CHECKPOINT_TIME : 111 samples, 0.02 seconds total, 0.17/0.21/0.30 ms min/avg/max, 0.03 stddev
 BACKUP_CREATE_3PC_BATCH_TIME : no samples
 BACKUP_ORDER_3PC_BATCH_TIME : 10909 samples, 8.30 seconds total, 0.41/0.76/5.15 ms min/avg/max, 0.20 stddev
 PROCESS_PROPAGATE_TIME : 861027 samples, 212.62 seconds total, 0.13/0.25/314.91 ms min/avg/max, 0.58 stddev
 PROCESS_MESSAGE_REQ_TIME : 1 samples, 0.00 seconds total, 0.63/0.63/0.63 ms min/avg/max, 0.00 stddev
 PROCESS_MESSAGE_REP_TIME : 4 samples, 0.00 seconds total, 0.24/0.30/0.39 ms min/avg/max, 0.06 stddev
 PROCESS_LEDGER_STATUS_TIME : no samples
 PROCESS_CONSISTENCY_PROOF_TIME : no samples
 PROCESS_CATCHUP_REQ_TIME : no samples
 PROCESS_CATCHUP_REP_TIME : no samples
 PROCESS_REQUEST_TIME : 67646 samples, 70.39 seconds total, 0.19/1.04/12.09 ms min/avg/max, 0.85 stddev
 SEND_PROPAGATE_TIME : 35876 samples, 15.16 seconds total, 0.24/0.42/297.77 ms min/avg/max, 1.72 stddev
 SEND_MESSAGE_REQ_TIME : 7 samples, 0.00 seconds total, 0.03/0.13/0.24 ms min/avg/max, 0.09 stddev
 SEND_MESSAGE_REP_TIME : 1 samples, 0.00 seconds total, 0.56/0.56/0.56 ms min/avg/max, 0.00 stddev
 BLS_VALIDATE_PREPREPARE_TIME : 10909 samples, 0.06 seconds total, 0.00/0.01/0.07 ms min/avg/max, 0.00 stddev
 BLS_VALIDATE_COMMIT_TIME : 193776 samples, 390.80 seconds total, 0.00/2.02/39.29 ms min/avg/max, 6.25 stddev
 BLS_UPDATE_PREPREPARE_TIME : 1203 samples, 0.02 seconds total, 0.00/0.02/0.06 ms min/avg/max, 0.01 stddev
 BLS_UPDATE_COMMIT_TIME : 1202 samples, 4.09 seconds total, 1.95/3.40/6.00 ms min/avg/max, 1.05 stddev
 DESERIALIZE_DURING_UNPACK_TIME : no samples|Start time: 2019-12-05 11:01:00
End time: 2019-12-05 12:01:00
Duration: 1:00:00
Number of messages processed in one looper run:
 Node: 88076 samples, 1706868.00 sum, 0.00/19.38/1000.00 min/avg/max, 58.52 stddev
 Client: 88076 samples, 61201.00 sum, 0.00/0.69/64.00 min/avg/max, 2.12 stddev
Seconds passed between looper runs:
 88076 samples, 3599.97 sum, 0.00/0.04/2.81 min/avg/max, 0.12 stddev
Number of messages in one transport batch:
 1706556 samples, 1706556.00 sum, 1.00/1.00/1.00 min/avg/max, 0.00 stddev
Node message size, bytes:
 Outgoing: 1706556 samples, 1391664048.00 sum, 52.00/815.48/9573.00 min/avg/max, 1543.12 stddev
 Incoming: 1706868 samples, 1321583069.00 sum, 52.00/774.27/9573.00 min/avg/max, 1513.17 stddev
Client message size, bytes:
 Outgoing: 133727 samples, 189464936.00 sum, 81.00/1416.80/11623.00 min/avg/max, 1847.12 stddev
 Incoming: 61201 samples, 54469349.00 sum, 214.00/890.01/9479.00 min/avg/max, 1558.92 stddev
Number of requests in one 3PC batch:
 Created: 1675 samples, 37441.00 sum, 0.00/22.35/64.00 min/avg/max, 9.43 stddev
 Ordered: 1673 samples, 37407.00 sum, 0.00/22.36/64.00 min/avg/max, 9.43 stddev
Time spent on write request processing, ms:
 5.35/14.89/293.61 ms min/avg/max, 3.53 stddev
Monitor throughput, TPS
 Master: 9.58/10.38/13.34 min/avg/max, 0.91 stddev
 Backup: 9.79/10.37/13.27 min/avg/max, 0.90 stddev
Monitor latency, seconds
 Master: 2.87/3.89/8.40 min/avg/max, 0.79 stddev
 Backup: 1.45/2.70/4.49 min/avg/max, 0.57 stddev
RAM info:
 Available, Mb: 6554.62/6602.31/6666.24 min/avg/max, 33.70 stddev
 Node RSS, Mb: 900.86/964.67/1011.45 min/avg/max, 33.69 stddev
 Node VMS, Mb: 1377.75/1440.27/1477.75 min/avg/max, 30.06 stddev
Additional statistics:
 Client incoming/outgoing: 0.46 messages, 0.29 traffic
 Node incoming/outgoing traffic: 0.95
 Node/client traffic: 11.12
 Node traffic per batch: 1621785.49
 Node traffic per request: 72533.14
Profiling info:
 BACKUP_REQUEST_PROCESSING_TIME : 299248 samples, 0.52 seconds total, 0.00/0.00/0.05 ms min/avg/max, 0.00 stddev
 REQUEST_PROCESSING_TIME : 37441 samples, 557.44 seconds total, 5.35/14.89/293.61 ms min/avg/max, 3.53 stddev
 GC_GEN0_TIME : 17263 samples, 4.89 seconds total, 0.06/0.28/1.70 ms min/avg/max, 0.14 stddev
 GC_GEN1_TIME : 1570 samples, 3.30 seconds total, 1.17/2.10/4.00 ms min/avg/max, 0.40 stddev
 GC_GEN2_TIME : 89 samples, 21.63 seconds total, 107.85/243.07/363.19 ms min/avg/max, 50.37 stddev
 NODE_PROD_TIME : 88076 samples, 3352.29 seconds total, 4.40/38.06/2813.42 ms min/avg/max, 124.29 stddev
 SERVICE_REPLICAS_TIME : 88076 samples, 1366.56 seconds total, 0.20/15.52/1557.78 ms min/avg/max, 83.84 stddev
 SERVICE_NODE_MSGS_TIME : 88076 samples, 1242.78 seconds total, 1.87/14.11/1087.70 ms min/avg/max, 34.96 stddev
 SERVICE_CLIENT_MSGS_TIME : 88076 samples, 131.06 seconds total, 0.10/1.49/127.26 ms min/avg/max, 3.93 stddev
 SERVICE_NODE_ACTIONS_TIME : 88076 samples, 18.06 seconds total, 0.00/0.21/66.90 ms min/avg/max, 3.11 stddev
 SERVICE_VIEW_CHANGER_TIME : 88076 samples, 2.05 seconds total, 0.01/0.02/0.12 ms min/avg/max, 0.01 stddev
 SERVICE_OBSERVABLE_TIME : 88076 samples, 1.08 seconds total, 0.01/0.01/0.20 ms min/avg/max, 0.01 stddev
 SERVICE_OBSERVER_TIME : 88076 samples, 0.69 seconds total, 0.00/0.01/0.08 ms min/avg/max, 0.00 stddev
 FLUSH_OUTBOXES_TIME : 88076 samples, 346.47 seconds total, 0.01/3.93/236.53 ms min/avg/max, 11.01 stddev
 SERVICE_NODE_LIFECYCLE_TIME : 88076 samples, 212.07 seconds total, 1.92/2.41/13.16 ms min/avg/max, 0.57 stddev
 SERVICE_CLIENT_STACK_TIME : 88076 samples, 14.10 seconds total, 0.09/0.16/4.09 ms min/avg/max, 0.13 stddev
 SERVICE_MONITOR_ACTIONS_TIME : 88076 samples, 0.49 seconds total, 0.00/0.01/0.30 ms min/avg/max, 0.00 stddev
 SERVICE_TIMERS_TIME : 88076 samples, 3.32 seconds total, 0.01/0.04/28.02 ms min/avg/max, 0.69 stddev
 SERVICE_NODE_STACK_TIME : 88076 samples, 949.07 seconds total, 1.84/10.78/712.68 ms min/avg/max, 25.61 stddev
 PROCESS_NODE_INBOX_TIME : 88076 samples, 290.78 seconds total, 0.00/3.30/400.33 ms min/avg/max, 9.80 stddev
 SEND_TO_REPLICA_TIME : 809037 samples, 10.38 seconds total, 0.01/0.01/5.42 ms min/avg/max, 0.01 stddev
 NODE_CHECK_PERFORMANCE_TIME : 348 samples, 0.26 seconds total, 0.56/0.74/1.15 ms min/avg/max, 0.14 stddev
 NODE_CHECK_NODE_REQUEST_SPIKE : 60 samples, 0.00 seconds total, 0.02/0.03/0.05 ms min/avg/max, 0.01 stddev
 UNPACK_BATCH_TIME : no samples
 VERIFY_SIGNATURE_TIME : 959032 samples, 36.89 seconds total, 0.01/0.04/7.50 ms min/avg/max, 0.14 stddev
 SERVICE_REPLICAS_OUTBOX_TIME : 88076 samples, 81.57 seconds total, 0.01/0.93/286.95 ms min/avg/max, 7.95 stddev
 NODE_SEND_TIME : 71171 samples, 7.63 seconds total, 0.04/0.11/0.60 ms min/avg/max, 0.05 stddev
 NODE_SEND_REJECT_TIME : no samples
 VALIDATE_NODE_MSG_TIME : 1706868 samples, 584.49 seconds total, 0.06/0.34/363.92 ms min/avg/max, 1.74 stddev
 INT_VALIDATE_NODE_MSG_TIME : 1706868 samples, 255.33 seconds total, 0.03/0.15/363.53 ms min/avg/max, 1.72 stddev
 PROCESS_ORDERED_TIME : 16771 samples, 74.73 seconds total, 0.04/4.46/239.85 ms min/avg/max, 14.46 stddev
 MONITOR_REQUEST_ORDERED_TIME : 16771 samples, 3.14 seconds total, 0.02/0.19/0.99 ms min/avg/max, 0.10 stddev
 EXECUTE_BATCH_TIME : 1673 samples, 70.77 seconds total, 1.06/42.30/238.48 ms min/avg/max, 21.41 stddev
 SERVICE_REPLICA_QUEUES_TIME : 88076 samples, 1149.13 seconds total, 0.05/13.05/1429.15 ms min/avg/max, 78.64 stddev
 SERVICE_BACKUP_REPLICAS_QUEUES_TIME : 704608 samples, 124.66 seconds total, 0.01/0.18/337.59 ms min/avg/max, 1.35 stddev
 PROCESS_PREPREPARE_TIME : no samples
 PROCESS_PREPARE_TIME : 40152 samples, 10.93 seconds total, 0.02/0.27/11.81 ms min/avg/max, 0.75 stddev
 PROCESS_COMMIT_TIME : 40152 samples, 559.48 seconds total, 0.02/13.93/73.22 ms min/avg/max, 10.83 stddev
 PROCESS_CHECKPOINT_TIME : 408 samples, 3.04 seconds total, 0.09/7.46/345.50 ms min/avg/max, 37.57 stddev
 SEND_PREPREPARE_TIME : 1675 samples, 0.16 seconds total, 0.04/0.10/0.23 ms min/avg/max, 0.03 stddev
 SEND_PREPARE_TIME : no samples
 SEND_COMMIT_TIME : 1673 samples, 5.90 seconds total, 2.13/3.53/11.60 ms min/avg/max, 1.08 stddev
 SEND_CHECKPOINT_TIME : 17 samples, 0.00 seconds total, 0.24/0.25/0.34 ms min/avg/max, 0.03 stddev
 CREATE_3PC_BATCH_TIME : 1675 samples, 563.82 seconds total, 1.31/336.61/889.47 ms min/avg/max, 145.95 stddev
 ORDER_3PC_BATCH_TIME : 1673 samples, 18.79 seconds total, 7.92/11.23/22.87 ms min/avg/max, 2.54 stddev
 BACKUP_PROCESS_PREPREPARE_TIME : 15098 samples, 17.97 seconds total, 0.58/1.19/263.99 ms min/avg/max, 2.18 stddev
 BACKUP_PROCESS_PREPARE_TIME : 354818 samples, 37.09 seconds total, 0.01/0.10/8.28 ms min/avg/max, 0.06 stddev
 BACKUP_PROCESS_COMMIT_TIME : 362716 samples, 39.04 seconds total, 0.01/0.11/9.66 ms min/avg/max, 0.17 stddev
 BACKUP_PROCESS_CHECKPOINT_TIME : 3576 samples, 7.20 seconds total, 0.08/2.01/315.68 ms min/avg/max, 13.79 stddev
 BACKUP_SEND_PREPREPARE_TIME : no samples
 BACKUP_SEND_PREPARE_TIME : 15098 samples, 2.45 seconds total, 0.11/0.16/2.50 ms min/avg/max, 0.04 stddev
 BACKUP_SEND_COMMIT_TIME : 15089 samples, 1.96 seconds total, 0.09/0.13/2.16 ms min/avg/max, 0.04 stddev
 BACKUP_SEND_CHECKPOINT_TIME : 149 samples, 0.03 seconds total, 0.17/0.21/0.34 ms min/avg/max, 0.03 stddev
 BACKUP_CREATE_3PC_BATCH_TIME : no samples
 BACKUP_ORDER_3PC_BATCH_TIME : 15098 samples, 10.77 seconds total, 0.41/0.71/9.45 ms min/avg/max, 0.20 stddev
 PROCESS_PROPAGATE_TIME : 897831 samples, 216.33 seconds total, 0.13/0.24/267.62 ms min/avg/max, 0.49 stddev
 PROCESS_MESSAGE_REQ_TIME : no samples
 PROCESS_MESSAGE_REP_TIME : 59 samples, 0.01 seconds total, 0.14/0.25/0.48 ms min/avg/max, 0.08 stddev
 PROCESS_LEDGER_STATUS_TIME : no samples
 PROCESS_CONSISTENCY_PROOF_TIME : no samples
 PROCESS_CATCHUP_REQ_TIME : no samples
 PROCESS_CATCHUP_REP_TIME : no samples
 PROCESS_REQUEST_TIME : 61201 samples, 57.68 seconds total, 0.19/0.94/12.83 ms min/avg/max, 0.85 stddev
 SEND_PROPAGATE_TIME : 37410 samples, 16.32 seconds total, 0.23/0.44/267.30 ms min/avg/max, 2.29 stddev
 SEND_MESSAGE_REQ_TIME : 213 samples, 0.02 seconds total, 0.02/0.08/0.32 ms min/avg/max, 0.08 stddev
 SEND_MESSAGE_REP_TIME : no samples
 BLS_VALIDATE_PREPREPARE_TIME : 15098 samples, 0.09 seconds total, 0.00/0.01/8.03 ms min/avg/max, 0.07 stddev
 BLS_VALIDATE_COMMIT_TIME : 268345 samples, 534.67 seconds total, 0.00/1.99/72.82 ms min/avg/max, 6.15 stddev
 BLS_UPDATE_PREPREPARE_TIME : 1675 samples, 0.03 seconds total, 0.00/0.02/0.05 ms min/avg/max, 0.01 stddev
 BLS_UPDATE_COMMIT_TIME : 1673 samples, 5.51 seconds total, 1.94/3.29/11.23 ms min/avg/max, 1.04 stddev
 DESERIALIZE_DURING_UNPACK_TIME : no samples|;;;","06/Dec/19 10:35 PM;anikitinDSR;Node1 without batches:

!Node1_without_batches.png|thumbnail!

Node1 with batches:

!Node1_with_batches.png|thumbnail!;;;","07/Dec/19 12:19 AM;anikitinDSR;PR with disabling transport batches:
 * [https://github.com/hyperledger/indy-plenum/pull/1427];;;",,,,,,,,,,,,,,
Investigate reasons of hundreds VCs during 15 txns per sec production load,INDY-2295,43364,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,VladimirWork,VladimirWork,18/Nov/19 9:54 PM,20/Nov/19 7:24 PM,28/Oct/23 2:47 AM,20/Nov/19 7:24 PM,,,1.12.0,,,,,0,,,,,"Investigate reasons of hundreds VCs during 15 txns per sec production load against the latest stable packages.

Logs and metrics:
ev@evernymr33:logs/performance_results_12_11_2019.tar.gz
ev@evernymr33:logs/stable_15_writes_per_sec_12_11_2019.tar.gz
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40o9",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"20/Nov/19 7:21 PM;ashcherbakov;Result of analysis:
* The load was more than the pool was able to process, so eventually it started to lagging behind and start a view change because of the latency
* Some nodes started to re-order during view change, and, in particular, they re-ordered old PrePrepares for old requests (eventually with a noticable latency). Backups on the other hand started ordering new requests with pretty low latency. So, view changes continue to be triggered because of this. 

Next Steps:
* INDY-2299 is created to deal with this issue and avoid false positive view changes

Other issues found in logs:
* `Identifier {} is not decoded into UTF-8 string` is logged somtimes. 
** SDK always sends identifiers in UTF-8 format
** So, either this is a bug in SDK (the identifier looks pretty short and suspicious), or something from node-to-node communication
* `Commit message has invalid BLS signature` (see INDY-1932)
;;;",,,,,,,,,,,,,,,,,,,,,
Consensus with X<(f+1) genesis nodes down,INDY-2296,43379,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,vladimir.demcak,vladimir.demcak,19/Nov/19 10:12 PM,11/Dec/19 10:58 PM,28/Oct/23 2:47 AM,,,,,documentation,plenum,validator-info,,0,Documentation,help-wanted,,,"I don't know if this is a bug but really cannot find any reason why the following happens. I cannot find it in any documentation and based on system 3*F+1 it should work.
 For me it looks like a bug but maybe I misunderstood something.

I have a feeling I need to do something with genesis_transaction file. I dont understand how or if it's possible to connect a client/agent to the pool with genesis file if the half of the genesis nodes are not reachable. If this is the reason It would be good to have a section or few lines in documentation for that. I saw INDY-2015 it looks similar but not sure if it is the same problem I am describing here.

 

*The problem:*

Assume we have 4genesis nodes + 6 new nodes and I remove 3 genesis nodes, I have 7 running nodes (1 genesis and 6 newly added nodes). Failed (F) nodes F=3 and total N=10 nodes so I assume the consensus should be met. 

 

*How to reproduce:*
 # Let's start pool with pool_start.sh - [https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/pool_start.sh] and with 4 nodes.
 # Once the pool is running add 6 new nodes as described in [add_node|[https://github.com/hyperledger/indy-node/blob/master/docs/source/add-node.md]] documentation
 # Now we have 10 reachable nodes.
 !image-2019-11-19-14-05-26-244.png!
 # Now, let’s remove 3 genesis nodes. and wait for view changes as below.
 !image-2019-11-19-14-06-10-641.png!
 # As we can see we have F=3 failed nodes. So based on formula 3*F+1 it should be ok for having consensus (3*3+1 = 10 -> indeed we have 10 Nodes in total and 7 running)

 

*Problem with pool restart:*

Another thing I have observed is restarting pool. When I run pool-restart from a running node (genesis or one of the newly added nodes) like below:
{code:java}
pool(sandbox):wallet(xyz):did(VVV...EEE):indy> ledger pool-restart action=start
{code}
I get timeouts for non-genesis nodes:
{code:java}
Restart pool response for node Node10:
+------------------------+---------------------+--------+----------+
| From                   | Request Id          | Action | Datetime |
+------------------------+---------------------+--------+----------+
| TrusteeDid000000000000 | 1574257396040735100 | start  | -        |
+------------------------+---------------------+--------+----------+
Restart pool response for node Node8:
+------------------------+---------------------+--------+----------+
| From                   | Request Id          | Action | Datetime |
+------------------------+---------------------+--------+----------+
| TrusteeDid000000000000 | 1574257396040735100 | start  | -        |
+------------------------+---------------------+--------+----------+
Restart pool node NewNode1 timeout.
Restart pool node NewNode5 timeout.
Restart pool node NewNode4 timeout.
Restart pool node Node1 timeout.
Restart pool response for node Node9:
+------------------------+---------------------+--------+----------+
| From                   | Request Id          | Action | Datetime |
+------------------------+---------------------+--------+----------+
| TrusteeDid000000000000 | 1574257396040735100 | start  | -        |
+------------------------+---------------------+--------+----------+
Restart pool node Node3 timeout.
Restart pool response for node Node6:
+------------------------+---------------------+--------+----------+
| From                   | Request Id          | Action | Datetime |
+------------------------+---------------------+--------+----------+
| TrusteeDid000000000000 | 1574257396040735100 | start  | -        |
+------------------------+---------------------+--------+----------+
Restart pool node Node2 timeout.
Restart pool node NewNode6 timeout.
Restart pool node Node4 timeout.
Restart pool response for node Node7:
+------------------------+---------------------+--------+----------+
| From                   | Request Id          | Action | Datetime |
+------------------------+---------------------+--------+----------+
| TrusteeDid000000000000 | 1574257396040735100 | start  | -        |
+------------------------+---------------------+--------+----------+
Restart pool node NewNode3 timeout.
Restart pool node NewNode2 timeout.
Restart pool node Node5 timeout.
{code}
I completely understand why I have timeouts for Node0-Node5 (The nodes I've stopped) but dont understand why I get timeout for running NewNodes.

 
 ","indy-node: 1.12.0.dev1125
indy-node: 1.11.0
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Dec/19 6:46 PM;vladimir.demcak;debug.zip;https://jira.hyperledger.org/secure/attachment/18030/debug.zip","19/Nov/19 10:05 PM;vladimir.demcak;image-2019-11-19-14-05-26-244.png;https://jira.hyperledger.org/secure/attachment/17950/image-2019-11-19-14-05-26-244.png","19/Nov/19 10:06 PM;vladimir.demcak;image-2019-11-19-14-06-10-641.png;https://jira.hyperledger.org/secure/attachment/17949/image-2019-11-19-14-06-10-641.png","11/Dec/19 6:37 PM;vladimir.demcak;image-2019-12-11-10-37-05-654.png;https://jira.hyperledger.org/secure/attachment/18024/image-2019-12-11-10-37-05-654.png","11/Dec/19 6:38 PM;vladimir.demcak;image-2019-12-11-10-38-10-228.png;https://jira.hyperledger.org/secure/attachment/18025/image-2019-12-11-10-38-10-228.png","11/Dec/19 6:40 PM;vladimir.demcak;image-2019-12-11-10-40-07-200.png;https://jira.hyperledger.org/secure/attachment/18026/image-2019-12-11-10-40-07-200.png","11/Dec/19 6:40 PM;vladimir.demcak;image-2019-12-11-10-40-54-926.png;https://jira.hyperledger.org/secure/attachment/18027/image-2019-12-11-10-40-54-926.png","11/Dec/19 6:43 PM;vladimir.demcak;image-2019-12-11-10-43-48-709.png;https://jira.hyperledger.org/secure/attachment/18028/image-2019-12-11-10-43-48-709.png","11/Dec/19 6:44 PM;vladimir.demcak;image-2019-12-11-10-44-33-206.png;https://jira.hyperledger.org/secure/attachment/18029/image-2019-12-11-10-44-33-206.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i014y3:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,vladimir.demcak,,,,,,,,,,"27/Nov/19 11:20 PM;ashcherbakov;[~vladimir.demcak]
Thank you for raising the issue.
Since pool-restart command doesn't require consensus, it looks like that new nodes haven't been added properly (so that they don't see client requests).
Can you please provide log files (/var/log/) or the full output of validator info?;;;","11/Dec/19 6:47 PM;vladimir.demcak;[~ashcherbakov] Thanks for your comment! Sorry for my late reply.

It looks like I had non-accessible (invalid) IP address due typo for client on the new nodes and because of this the client was not able to send the restart request to the new nodes.

However while I was debugging the issue I found another weird behavior .

1. start 4 genesis nodes
{code:java}
pool_start.sh 4
{code}
2. add 4 new nodes as 
    a) docker container 
    b) add new node to pool with steward.

the output should look like:
 !image-2019-12-11-10-37-05-654.png!
 3. Remove genesis nodes with Trustee:
{code:java}
ledger node target=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv alias=Node1 services=
ledger node target=8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb alias=Node2 services=
ledger node target=DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya alias=Node3 services=
ledger node target=4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA alias=Node4 services=
{code}
the output should look like:
 !image-2019-12-11-10-38-10-228.png!
 4. Add new node without running docker container (physically the node will not be running) 

so only ""b) add new node to pool with steward.""
 the output should look like: 
 !image-2019-12-11-10-40-54-926.png!

(Everything should work correctly. Eg creating new schema)
 5. Add new node with docker container and steward (with current pool transactions - output from ""read_ledger --type pool"") will add the node to the pool

   a) docker container 
    b) add new node to pool with steward.

*Test: try to add new schema:* 
{code:java}
pool(sandbox):wallet(wallet):did(V4S...e6f):indy> ledger schema version=1.0 attr_names=undergrad,last_name,first_name,birth_date,postgrad,expiry_date name=Degree2
Transaction response has not been received
{code}
!image-2019-12-11-10-44-33-206.png!  

For some reason ViewChange has been executed. 
 I am adding logs for all nodes: [^debug.zip] 

 ;;;",,,,,,,,,,,,,,,,,,,,
get TAA should return the hash,INDY-2297,43385,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,esplinr,esplinr,20/Nov/19 7:50 AM,29/Jan/20 8:28 AM,28/Oct/23 2:47 AM,06/Dec/19 9:48 PM,,,1.12.1,,,,,0,,,,,"The API call to get the transaction author agreement should return the hash along with the text, so that applications can confirm that they have correctly computed the hash.

[https://github.com/hyperledger/indy-node/blob/master/docs/source/requests.md#get_transaction_author_agreement]",,,,,,,,,,,,,,,,,,,,,,,,IS-1473,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40ozr",,,,Unset,Unset,Ev-Node 19.24,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,Toktar,,,,,,,,,,"28/Nov/19 7:52 PM;Toktar;*PoA:*
 * Add 'digest'  to 'data' in GET_TRANSACTION_AUTHOR_AGREEMENT reply

{code:java}
{
    'op': 'REPLY', 
    'result': {
        'type': '6',
        'identifier': 'L5AD5g65TDQr1PPHHRoiGf',
        'reqId': 1514308188474704,
        
        'version': '1.0',
        
        'seqNo': 10,
        'txnTime': 1514214795,

        'data': {
            ""digest"": ""6cee82226c6e276c983f46d03e3b3d10436d90b67bf33dc67ce9901b44dbc97c"",
            ""version"": ""1.0"",
            ""text"": ""Please read carefully before writing anything to the ledger"",
        },

        'state_proof': {...}
       
    }
}
{code}

 * Extend tests for GET_TRANSACTION_AUTHOR_AGREEMENT
 * Add 'digest' to value in state for TAA
 * Bump transactions version (in scope of INDY-2302)
 * Add a handler to restore old transactions from a config ledger to a config state.  (in scope of INDY-2302);;;","06/Dec/19 9:47 PM;Toktar;*Problem reason:*
 - User can't compare TAA digest from the ledger and SDK

*Changes:*
 - Add a digest to reply of get TAA
 - Add a digest to state (for a correct work of state proofs)

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1424]
 * [https://github.com/hyperledger/indy-node/pull/1530]

*Risk factors:*
 - get TAA transaction

*Risk:*
 - Low

*Tests:*
 * [test_get_txn_author_agreement.py|https://github.com/hyperledger/indy-plenum/pull/1424/files#diff-60992bba3ed203306547867122942f84] 

*Recommendations for QA:*
 * check that transaction for getting a TAA returns a digest


Will be tested in scope of INDY-2302;;;",,,,,,,,,,,,,,,,,,,,
Do not restore Primaries from the audit ledger,INDY-2298,43389,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Nov/19 5:27 PM,25/Nov/19 4:16 PM,28/Oct/23 2:47 AM,25/Nov/19 4:16 PM,,,1.12.0,,,,,0,,,,,"We made Primary selection pretty deterministic in the scope of INDY-2262.
Because of the situation reproduced in SN-10 on MainNet after upgrade that some lagging nodes restored primaries from the audit that aren't equal to the current primaries on most of the Nodes, we will need to do the following:

*Changes:*
* Do not restore Primaries from the audit ledger after initial catchup, but rather calculate them using the common primary selection logic

*Integration Test*
* Mock writing primaries to audit so that it always writes Node4 as a Primary
* Restart Nodes 3 and 4
* Make sure that these Nodes selected the same Primary as the other Nodes have (Node1) and the the pool can ordering

*System Test*
* Install version 1.11.0, pool of 4 Nodes
* Demote Node1 => view change to viewNo=1, Primary - Node3
* Promote Node1 => view change to viewNo=3 (skip viewNo=2 since the same Primary is chosen), Primary - Node4
* Update 3 Nodes to the latest master
* Make sure that they order anything (NODE_UPGRADE txns for example). 
* Update the 4th Node
* Make sure that the 4th node selected the same Primary as other 3 nodes and all nodes are in consensus
",,,,,,,,,,,,,,,,,,,,,INDY-2262,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1302,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i401i",,,,Unset,Unset,Ev-Node 19.23,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"21/Nov/19 5:29 PM;ashcherbakov;*PR*
https://github.com/hyperledger/indy-plenum/pull/1406

*Build*
indy-node 1.12.0.dev1138
plugins 1.0.5~dev118;;;",,,,,,,,,,,,,,,,,,,,,
Backups should start ordering in new view only after master instance ordered till prepared cert from NewView,INDY-2299,43390,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Nov/19 7:17 PM,26/Dec/19 12:07 AM,28/Oct/23 2:47 AM,26/Dec/19 12:07 AM,,,1.12.1,,,,,0,,,,,"As INDY-2295 and other load tests showed, it's quite common that multiple view changes happen in a row because master needs to re-order old requests (with potentially non-zero latency getting bigger because of view change), but backups start ordering new requests with pretty low latency.
So, the difference can easily get more than our limit (20 secs), and a new view change will be triggered. 

*Acceptance criteria:*
* Integration tests reproducing the issue
* Do not start ordering on backups until ReOrdered is sent on master",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ir9",,,,Unset,Unset,Ev-Node 19.25,Ev-Node 19.26,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"11/Dec/19 11:43 PM;anikitinDSR;h1. PoA
 * need to write test, which shows difference in latency and throughputs when master's replica needs to reorder several batches
 * need to rename ReOrderedInNewView internal message to ReAppliedInNewView, because for now it's not about finishing re-ordering but only about applying all of needed PrePrepares from previous view
 * need to add new flag for backups which can allow to order on it after reordering finished
 * add new internal message to allow ordering continue after master's reordering;;;","25/Dec/19 12:13 AM;ashcherbakov;Fixes:
 * like in the PoA

PR:
 * https://github.com/hyperledger/indy-plenum/pull/1435

Version:
 * indy-node 1.12.1.dev1169

Risk:
 * Medium

Recommendation for QA:
 * Run load test with forced view change every 30 minute
 * Check that there is no multiple view changes (that is view changed started right after the previous one has finished);;;","25/Dec/19 12:21 AM;ashcherbakov;There was another issue found while fixing the issue (it was caught by simulation test BTW):
 * If the next Primary didn't receive a PrePrepare from old view that other nodes ordered, it will reject all Prepares during re-ordering phase and will not be able to participate in ordering (there will be a gap).

Fixed in
 * [https://github.com/hyperledger/indy-plenum/pull/1435]

Covered by tests
 * [plenum/test/view_change_service/test_delay_pre_prepares.py|https://github.com/hyperledger/indy-plenum/pull/1435/files#diff-6abdab437ed91aea64dde789d7818294] (integration)
 * [plenum/test/consensus/order_service/test_ordering_process_prepare.py|https://github.com/hyperledger/indy-plenum/pull/1435/files#diff-83804e3bf91590410647af9174d47725] (unit);;;","25/Dec/19 5:12 PM;VladimirWork;Build Info:
indy-node 1.12.1~dev1169
plugins 1.0.6~dev137

Actual Results:
We have 42 ViewNo after 15.5 hours of production load rate.

Expected Results:
There should be 31 or 32 ViewNo at the end of load test for this duration.;;;","26/Dec/19 12:07 AM;VladimirWork;We perform more VCs than expected using forced VC parameter in config so it is ok to have plus one third ViewNo than expected.;;;",,,,,,,,,,,,,,,,,
The canonicalization rules of schema defs and cred defs should match,INDY-2300,43392,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,20/Nov/19 11:51 PM,20/Nov/19 11:51 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"The canonicalization rules of schema defs and cred defs are different. This appears to have happened by accident over time. 

*Notes*

This requires a complicated fix:
 * It changes the transaction format, so we would need to increment the version.
 * Indy SDK would have to change to support the new format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i014z7:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use strings unless UTF-8 is really necessary,INDY-2301,43411,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,21/Nov/19 11:18 PM,25/Nov/19 9:09 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Currently there are lots of places in code where strings and bytes are used interchangeably for things that should have binary or plain ASCII representations. This leads to lots of type checks and conversions in code, which makes it harder to understand and sometimes significantly slower to run. This should be avoided.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c926",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow multiple active TAAs,INDY-2302,43413,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,esplinr,esplinr,22/Nov/19 12:59 AM,25/Dec/19 7:30 PM,28/Oct/23 2:47 AM,20/Dec/19 4:31 PM,,,1.12.1,,,,,0,,,,,"*Story*

As an organization using an Indy Network, I need to adopt the Transaction Author Agreement at the time my legal team approves it, rather than coordinating closely with the network administrators for when to cut-over to a new agreement.

Current TAA flow
 * Network governance adopts a TAA, and then publishes it on a web site.
 * Users of the network can review the TAA.
 * After the advisory period has expired, it is published to the ledger and immediately enforced and the old agreement is immediately retired.

Problems
 * Users of the network have to wait to adopt the new TAA until the advisory period has expired and it is on the ledger even if they have already accepted it.
 * Users of the network have to immediately switch versions of the TAA in their software at the time the new TAA is on the ledger.
 * Users of the network can't store their acceptance in their software configuration until the TAA is on the ledger and the official hash is available.

Revised TAA flow
 * Network governance adopts a TAA, and it enters the public advisory period.
 * It is immediately put on the ledger.
 * When the new agreement's public advisory period has ended and the old agreement is no longer acceptable, network governance marks the old agreement as retired.

*Acceptance Criteria*
 * When a new TAA is put on the ledger, old agreements are not automatically rejected. Marking a TAA as ""retired"" is done in a separate transaction from adding a new TAA.
 * A transaction that includes acceptance of any non-retired TAA should be accepted by the ledger.
 * A query to the ledger for the TAA which does not specify a specific version should always return the newest TAA.
 * Documentation is updated.

*Notes*
 * The number of acceptable TAAs is determined by the network admin based on when agreements are marked as retired. It is expected that only one TAA will be active most of the time, but during a transition period two TAAs may be active. It is expected that there will never be more than two TAAs active at a time. However, we don't need to enforce that on the ledger.
 * The current logic that a TAA can be accepted the day before it gets on the ledger (due to clock skew) should be preserved.",,,,,,,,,,,,,,,,,,,,,IS-1427,,,INDY-1942,INDY-2313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-14,,TAA,To Do,No,,Unset,No,,,"1|hzwvif:00001yw969w49i40ozk",,,,Unset,Unset,Ev-Node 19.24,Ev-Node 19.25,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,lbendixsen,sergey.khoroshavin,Toktar,,,,,,,"28/Nov/19 9:08 PM;Toktar;*PoA:*
 * Add a handler to restore old transactions from a config ledger to a config state.
 * Add transaction versioning mechanism.
 * Bump transactions version 
 * Add a new transaction TRANSACTION_AUTHOR_AGREEMENT_STATUS to change TAA status. 

{code:java}
{
    'operation': {
        'type': '8'
        'digest': '6cee82226c6e276c983f46d03e3b3d10436d90b67bf33dc67ce9901b44dbc97c',
        'retired': True,
    },
    
    'identifier': '21BPzYYrFzbuECcBV3M1FH',
    'reqId': 1514304094738044,
    'protocolVersion': 2,
    'signature': '3YVzDtSxxnowVwAXZmxCG2fz1A38j1qLrwKmGEG653GZw7KJRBX57Stc1oxQZqqu9mCqFLa7aBzt4MKXk4MeunVj',
}{code}
 * 
 ** for disabling TAA: \{ TAA digest, 'retired': True}
 ** for enabling TAA: \{ TAA digest, 'retired': False}
 * Add 'retired' to value in state for TAA. 'retired' is False by default and is added to state only with value True
 * Extend GET_TRANSACTION_AUTHOR_AGREEMENT reply: add the flag 'retired' 
 * Add TRANSACTION_AUTHOR_AGREEMENT_STATUS  to the documentation and update for GET_TRANSACTION_AUTHOR_AGREEMENT 
 * Extend tests for GET_TRANSACTION_AUTHOR_AGREEMENT
 * Add unit and integration tests for a new txn.
 * Add system, unit and integration tests for versioning mechanism. 

*Questions for [~esplinr]:*
 # Is it ok if a TRANSACTION_AUTHOR_AGREEMENT transaction would not contain a ""retired"" field? Adding of the field would be logical, but I think, we do not want to change the format of the transaction without necessary.
 # To identify the TAA that needs to be made inactive, its digest will be used. If I understand correctly, we may also use the version, but I am afraid that this may introduce unnecessary confusion. Can I use only digest?;;;","28/Nov/19 10:51 PM;Toktar;We have a new idea by [~sergey.minaev]:

As another option we can add a new not required parameter `active` to TRANSACTION_AUTHOR_AGREEMENT and don't add a new transaction TRANSACTION_AUTHOR_AGREEMENT_STATUS.
 I think this options will be more understandable for a final user. In fact, we will make TAA available for the editing, not just for the adding.;;;","04/Dec/19 1:32 AM;esplinr;Your question caused me to think about TAA retirement a bit differently. It isn't so much a boolean (active or retired) as it that the TAA applies during a time period (the creation date until the retired date). I'm not concerned with how exactly TAA retirement is represented on the ledger so long as it is easy to tell when a TAA applies.

Managing the retirement of a TAA by editing the TAA transaction seems reasonable. It is not a requirement that a retired TAA can be ""unretired"", but the network administrator might appreciate having an easy way to correct a potential mistake.

The version number is more natural to use as a TAA identifier than the digest. When network governance is creating a new transaction author agreement, they attach a version number to it. They then use that version number whenever they talk about the TAA. If we only allow retiring a TAA by the digest, the administrator will have to do a lookup by version number to get the digest to complete the transaction.;;;","04/Dec/19 6:18 AM;Toktar;[~esplinr] Thanks a lot for answer!
If a TAA applies during a time period what about time for a TAA signature?
I want to say that we changed a way of a signature time validation in the task INDY-2157. How should this change now? We can use:
 * time of new TAA adding
 * the first time of TAA activation
 * TAA activity periods
 * or other variants

I think all options will take the same time to implementation and testing. So, we can use one that will be more suitable.;;;","04/Dec/19 6:24 AM;esplinr;The validation of the TAA acceptance time should continue to match INDY-2157:
 * We only track the date, to avoid correlation attacks
 * If a timestamp is within a few minutes of the date (before or after), then we attribute it to clock skew, and accept it.;;;","06/Dec/19 10:34 PM;Toktar;[~VladimirWork] System tests:
 *Test1*
 * The versioning mechanism:
 ** start pool on an any(!) previous version (without start upgrades)
 ** write TAA1 with some text in the field ""text""
 ** write TAA2 with empty ""text""
 ** upgrade pool to the last master version (with apt without POOL_UPGRADE)
 ** write TAA2 _retired_=timestamp(in the past)
 ** write TAA3 with _retired_=timestamp, check that the request is rejected
 *** Cannot create a transaction author agreement with a 'retired' field.
 ** write TAA3 without _retired_
 ** write TAA3 with _retired_=timestamp, check that the request is rejected
 *** The latest transaction author agreement cannot be retired.
 ** add a new node, check it writes a new config txns
 ** send GET_TAA for TAA1, check that reply
 *** doesn't contain _digest_
 *** doesn't contain _ratified_date_
 *** doesn't contain _retired_
 *** contains version
 *** contains text
 ** send GET_TAA for TAA2, check that reply
 *** contains _digest_
 *** contains _ratified_date_
 *** contains _retired_
 *** contains version
 *** contains text(empty)
 ** send GET_TAA for TAA3, check that reply:
 *** contains  _digest_
 *** contains _ratified_date_
 *** doesn't contain _retired_
 *** contains version
 *** contains text
 ** send NYM with signed TAA1 - pass
 ** send NYM with signed TAA2 - reject (TAA retired)
 ** send NYM with signed TAA3 - pass
 ** write TAA2 with _retired_=None
 ** send GET_TAA for TAA2, check that reply:
 *** contains  _digest_
 *** _contains_ _ratified_date_
 *** _doesn't_ contain _retired_
 *** contains version
 *** contains text
 ** send NYM with signed TAA2 - pass
 ** send NYM with signed TAA2 (before TAA2 creation data) - reject

*Test2*
 **
 * write TAA1
 * write TAA2 
 * send GET_TAA for TAA1, TAA2 check that replies
 ** contains  _digest_
 ** _contains_ _ratified_date_
 ** _doesn't_ contain _retired_
 ** contains version
 ** contains text
 * send NYM with signed TAA1 - pass
 * send NYM with signed TAA2 - pass
 * write TAA2 with retired=timestamp(in the past)
 * send GET_TAA for TAA2 check that reply
 ** contains  _digest_
 ** _contains_ _ratified_date_
 ** contains _retired_
 ** contains version
 ** contains text
 * send NYM with signed TAA1 - pass
 * send NYM with signed TAA2 - reject
 * send TRANSACTION_AUTHOR_AGREEMENT_DISABLE 
 * send GET_TAA for TAA1, TAA2 check that replies
 ** contains  _digest_
 ** _contains_ _ratified_date_
 ** contains _retired_
 ** contains version
 ** contains text
 * send NYM with signed TAA1 - reject
 * send NYM with signed TAA2 - reject;;;","06/Dec/19 11:45 PM;Toktar;*New PoA:*
 * versioning mechanism
 ** old handler *- done*
 ** txn versioning for all txn types - CURRENT_TXN_VERSIONS *- done*
 ** add test for the old handler *- done*
 ** add an integration test: *- in progress*
 *** patch CURRENT_TXN_VERSIONS - ""1"" for TAA and use the old handler for a state updating
 *** write TAA
 *** unpatch all
 *** write a new TAA
 *** add a new node
 *** write a new TAA
 *** check that all nodes have same data
 * change update_state in the TAA handler in indy-plenum
 ** add _digest_ to value  *- done*
 ** add _activation_time (txn_time_ if it's the first change of retired from False to True)  *- done*
 ** add _retired_ if it's _True  *- done*_
 ** use _text_ from previous value if it's not contains in the txn  *- done*
 ** add _taa:active_list_ with list of active TAAs _*- in progress*_
 ** add a last TAA from _taa:active_list_ to the key  ""latest"" if _retired=True *- in progress*_
 ** _update unit tests_  _*- in progress*_
 * change client request validation in _do_taa_validation()_  _*- in progress*_
 ** refactoring for the old method
 ** 
||TAA is required||There are active TAAs||Request contains valid TAA signature||Validation result||
|-|+/-|-| PASS|
|-|+/-|+|REJECT|
|+|-|-|PASS|
|+|-|+|REJECT|
|+|+|-|REJECT|
|+|+|+|PASS|

 ** add unit tests

 * Client TAA request
 ** add _retired_ to client request
 ** make _text_ optional
 * add integration test like a system test but without upgrades
 * update a documentation for TAA and GET_TAA;;;","10/Dec/19 9:22 AM;sergey.khoroshavin;[~esplinr] we discussed with team how deferred TAA retirement can be implemented and came to several options. Before discussing them with you I'd like to introduce several *assumptions about requirements* which affect available implementation options and costs.
 # Nodes should consistently stop accepting write transactions signed with old TAA as soon as retirement date and time has come, based on current primary node time (which is checked by all other validator nodes in pool and is written into ledger with each batch)
 # It is not important for a client to always be 100% sure that TAA has really expired or not based on just read TAA transaction
 # One of the following:
 ## It is okay that GET_TAA(latest) will always return latest TAA, which is not necessarily active (GET_TAA(latest) is an existing txn, which is currently used to get digest of active TAA)
 ## It is okay that GET_TAA(latest) can sometimes return inactive TAA, but will snap to correct version as soon as somebody attempts write to ledger (including automatic freshness check, if there is no external load at all)
 ## Retirement date of last submitted TAA should be later than retirement dates of all previous TAAs or marked as ""retire immediately""
 # We need a new GET_TAA_LIST transaction which returns list of digests + retirement dates of TAAs, which can contain both active and some of already retired TAAs, but is eventually garbage collected so it doesn't accumulate all retired TAAs and grow indefinitely
 # One of the following:
 ## It is okay to break Indy SDK API so that in order to reliably retrieve active TAA client must explicitly use new GET_TAA_LIST transaction and figure it out himself
 ## It is NOT okay to break Indy SDK API, this suboption is not compatible with 3.1, and it still can have quirks when combined with 3.2 or 3.3 - see next comment
 # When retiring last remaining TAA (that is - scheduling turning off TAA) one of the following holds true:
 ## It is okay for client with skewed clock to have short period of time when it gets rejects after TAA is turned off (it can happen if it thinks TAA is still active, while on pool it is already disabled). This period can be shortened even further when if picking option 3.2
 ## It is okay to accept transactions with any TAA digest if TAA is fully disabled on the pool
 # Newly written TAA takes into effect immediately, there is no deferred ""activation time"" (after fleshing out other requirements we could actually allow deferred activation time, but some of them can make this feature quire tricky, so I would refrain from implementing it for now);;;","10/Dec/19 11:26 PM;esplinr;More conversation about how we manage timestamps.
 * There are two timestamps: the TAA Acceptance, and the Transaction Submission.
 * We will check that TAA acceptance is within an acceptable clock skew of when a TAA is submitted on the ledger.
 * We will not check that TAA acceptance is correctly associated with when a TAA is retired on the ledger, because a transaction signed with a retired TAA will be rejected regardless.
 * We will not apply a clock skew adjustment when checking if a transaction is signed with a retired TAA because a transaction signed with a retired TAA should always be rejected. Now that there is overlap, users would have had time to accept the new TAA.;;;","11/Dec/19 1:20 AM;sergey.khoroshavin;I've updated a bit requirements assumptions comment (Option 5.2, basically replaced ""try not to break Indy SDK API by doing hacks under the hood"" with ""do not break Indy SDK""). 

In the meantime, we've found some more insights:
* If going with 3.3 route, which is restricting retirement date of last submitted TAA and avoiding introducing internal TAA_UPDATE transaction - it works for most of cases except deferred retirement of last TAA (in other word - deferred turning off TAA). In that case ""latest"" state key won't be cleared at all, which can be worked around on node side, but it breaks client-side compatibility (now clients expect that if TAA is inactive then GET_TAA(latest) will return empty value instead of some digest)
* If going with 3.2 route, which means introduction of internal TAA_UPDATE transaction - GET_TAA(latest) in some cases still can return digest of already retired TAA for short periods of time (from tens of seconds to 5 minutes range, depending on implementation), but since we already settled on the fact that assumption 2 is okay this should be okay as well
;;;","11/Dec/19 8:12 AM;esplinr;{quote}
1. Nodes should consistently stop accepting write transactions signed with old TAA as soon as retirement date and time has come, based on current primary node time (which is checked by all other validator nodes in pool and is written into ledger with each batch)
{quote}

This is correct. No transaction should be accepted with a retired TAA. If a client submits a TAA that it thinks is valid, but the TAA is retired due to clock-skew, the transaction should be rejected an the client should resubmit after accepting an active TAA.

{quote}
2. It is not important for a client to always be 100% sure that TAA has really expired or not based on just read TAA transaction
{quote}

This is correct.  It is highly unlikely that we hit one of the edge cases mentioned below. If we hit such an edge case, the client will be expected to query for the active TAA a second time and resubmit the transaction showing acceptance of that TAA.

The newest TAA anchored to the ledger will only be retired if:
1. A TAA should no longer be enforced. In which case there will be no other active TAAs.
2. A mistake is discovered with the TAA on the ledger, so it was retired in order to revert back to the previous version. This would be a temporary case, until the corrected TAA can be added to the ledger. If necessary, the previous TAA could be re-added.

{quote}
3. One of the following:
# It is okay that GET_TAA(latest) will always return latest TAA, which is not necessarily active (GET_TAA(latest) is an existing txn, which is currently used to get digest of active TAA)
# It is okay that GET_TAA(latest) can sometimes return inactive TAA, but will snap to correct version as soon as somebody attempts write to ledger (including automatic freshness check, if there is no external load at all)
# Retirement date of last submitted TAA should be later than retirement dates of all previous TAAs or marked as ""retire immediately""
{quote}

I dislike the first option. It isn't useful to return a retired TAA. I worry about accidentally entering into a situation where all TAAs are retired and there is no enforced TAA. We need to ensure it is easy to detect this situation if it were to happen.

The second option is acceptable.

The third option is also acceptable.

We won't be able to tell whether the second or third behavior is better until we have some real-world experience. I suggest we do the one that is easiest, and we can evaluate if changes are necessary. We can discuss further if the two are similar in effort.

And, interleaving your most recent comments:

{quote}
* If going with 3.2 route, which means introduction of internal TAA_UPDATE transaction - GET_TAA(latest) in some cases still can return digest of already retired TAA for short periods of time (from tens of seconds to 5 minutes range, depending on implementation), but since we already settled on the fact that assumption 2 is okay this should be okay as well
{quote}

Correct. This is acceptable.

{quote}
* If going with 3.3 route, which is restricting retirement date of last submitted TAA and avoiding introducing internal TAA_UPDATE transaction - it works for most of cases except deferred retirement of last TAA (in other word - deferred turning off TAA). In that case ""latest"" state key won't be cleared at all, which can be worked around on node side, but it breaks client-side compatibility (now clients expect that if TAA is inactive then GET_TAA(latest) will return empty value instead of some digest)
{quote}

I don't understand this comment, but I might be able to help clarify anyway.

The most common scenario is that the latest TAA on the ledger has no retirement date set. When a new TAA is approved and added to the ledger, the retirement date of the previous TAA will be set. It would be unexpected to establish a retirement date without having the new agreement ready. This would encourage people who don't want to follow the current agreement to just wait for the agreement to expire.

Does it simplify the implementation if we add a restriction such that the latest TAA is not allowed to have a retirement date set in the future? There would still need to be a way to retire it immediately and leave the ledger not enforcing a TAA, but that shouldn't be too easy because we don't want it to happen by accident.

{quote}
4. We need a new GET_TAA_LIST transaction which returns list of digests + retirement dates of TAAs, which can contain both active and some of already retired TAAs, but is eventually garbage collected so it doesn't accumulate all retired TAAs and grow indefinitely
5. One of the following:
# It is okay to break Indy SDK API so that in order to reliably retrieve active TAA client must explicitly use new GET_TAA_LIST transaction and figure it out himself
# It is NOT okay to break Indy SDK API, this suboption is not compatible with 3.1, and it still can have quirks when combined with 3.2 or 3.3 - see next comment
{quote}

I agree that this would be useful, but I'm not convinced that we need it at this time. I don't see the use of returning an incomplete list. Searching for historical TAAs is one special case of the larger epic we already have to improve searching the ledger. People can always pull the entire ledger history to see what TAAs were previously on the ledger, the same as any other ledger object.

{quote}
6. When retiring last remaining TAA (that is - scheduling turning off TAA) one of the following holds true:
# It is okay for client with skewed clock to have short period of time when it gets rejects after TAA is turned off (it can happen if it thinks TAA is still active, while on pool it is already disabled). This period can be shortened even further when if picking option 3.2
# It is okay to accept transactions with any TAA digest if TAA is fully disabled on the pool
{quote}

The first is acceptable.

The second is also acceptable. It is preferable to rejecting transactions that include a TAA digest when there is no TAA enforced.

{quote}
7. Newly written TAA takes into effect immediately, there is no deferred ""activation time"" (after fleshing out other requirements we could actually allow deferred activation time, but some of them can make this feature quire tricky, so I would refrain from implementing it for now)
{quote}

This is correct. There is no requirement at this time for deferring the activation of a TAA that is written to the ledger. As soon as a TAA is approved and written to the ledger, it should be possible for those who want to use it in favor of existing TAAs to do so.;;;","11/Dec/19 11:39 PM;sergey.khoroshavin;Thanks for clarifications. There are a couple of things left I think.

If we go *3.3 route* (which is just restrict how TAAs could be retired):

bq. Does it simplify the implementation if we add a restriction such that the latest TAA is not allowed to have a retirement date set in the future? There would still need to be a way to retire it immediately and leave the ledger not enforcing a TAA, but that shouldn't be too easy because we don't want it to happen by accident.

Did I understand you correctly that:
* you're okay that we cannot schedule stopping enforcing TAAs on the ledger, in other words - if you need to turn off TAA completely you need to send some (special) transaction that will turn it off immediately
* you're okay that we either:
** cannot retire immediately last submitted TAA (for example if it was submitted by error) or
** can retire immediately last submitted TAA with restriction that previously submitted TAA doesn't have retirement date set (otherwise we'll end up in situation when last active TAA still have retirement date set in future)

Also, I'd like to discuss *3.1 route* (which is an API breaking change) because:
* it looks a bit cheaper than 3.3 on the node side
* it is more flexible than 3.3
* seems like I failed to describe it good enough (and we've got some more ideas after yet another discussion with [~sergey.minaev])

So, updated *3.1* proposal is following:
* change return value of GET_TAA(latest) from latest digest to list of digests with optional retirement dates
** this list is guaranteed to contain all currently active TAAs
** this list can contain some of TAAs that were already retired
** this IS a breaking change to Indy Node API
* add call ""parse_latest_taa"" to Indy SDK, which takes a reply from GET_TAA(latest) request
** if empty string or digest is passed that it is returned as is (which means that we're working with old version of node)
** if list is passed then digest which won't be retired the longest is returned (digests without retirement date will always win)
** if all digests are retired according to local clock then empty string is returned, meaning that TAA is no longer enforced on the pool
** applications will need to use that ""parse_latest_taa"" which IS a breaking change to Indy SDK API, but it is backward compatible and also shouldn't take too much work to update application (just add that parse call before using GET_TAA(latest) result)

Pros of this solution are:
* we can schedule TAA retirement as we wish, including scheduling stopping TAA enforcement
* we can still apply some safety restriction on how we retire TAAs, but we're not forced to
* it should be relatively easy to implement deferred activation date when TAA is posted to the ledger in case we need it
* and, again, it looks a bit cheaper than option 3.3 if just Indy Node effort is considered

UPD
I'm inclined to discard option *3.2* based on your comment
bq. We won't be able to tell whether the second or third behavior is better until we have some real-world experience. I suggest we do the one that is easiest, and we can evaluate if changes are necessary. We can discuss further if the two are similar in effort.

3.2 is the most complex of all options from Indy Node PoV
;;;","12/Dec/19 2:34 AM;esplinr;Responses:
* The latest TAA should not be allowed to have a retirement date in the future. This prevents having all the TAAs retire and accidentally leaving a ledger without a TAA when that wasn't intended. A new TAA should be added to the ledger before the previous TAA can be given a retirement date.
* There should be a separate transaction to disable the TAA, which will immediately ""retire"" all the TAAs on the ledger including the latest one. This does not need to be coordinated, because it will not cause client transactions to be rejected (see next point).
* If a transaction includes any hash for a TAA, and no TAA is enforced on the ledger, we should accept the transaction. We don't need to check what ""agreement"" the hash references. Drummond confirmed that users should not be denied write access due to any included TAA information if no TAA is enforced. I recognize that this is a change from the current behavior where the hash must be empty if no TAA is enforced, but will reduce user friction. If it takes extra effort to make this change, we should raise a separate issue.
* If a mistake requires that we immediately retire the latest TAA while still enforcing TAAs in general, Drummond confirmed that we can require the network administrator to resubmit the previous TAA so that there is an active TAA on the ledger. We acknowledge that the resubmitted TAA will have a different version number and valid acceptance time. This will require all clients to re-accept the ""reverted"" TAA.
* Simplifying the ""reverting"" of a TAA, by allowing the latest TAA to be retired if a previous TAA is not retired would be convenient because it would allow the reversion to be transparent to network clients (they would not have to reaccept the TAA). But I think it isn't worth the effort or complexity given that this scenario is unlikely to happen in the wild and we have an agreed process for dealing with it.;;;","12/Dec/19 2:58 AM;esplinr;After this conversation, I dislike ""option 3.1"" because:
* We want to encourage clients to use the latest TAA on the ledger (the one without the retirement date). Returning just the latest TAA should be the default behavior and we shouldn't require people to parse a list to find it.
* Filtering the list to remove retired TAAs that are included simply because the state-proof was not updated should be transparent to the user (though it could be handled internal to a LibIndy call or by implementing a state proof update mechanism as mentioned in ""option 3.2"").
* The TAA behavior will be the most simple use case for the foreseeable future: one TAA active on the ledger. I want to postpone additional work until we have a real world need to consider.
* We need a more complete solution to searching the ledger for all types of transactions: schemas, cred defs, and TAAs. I am suspicious of trying to solve the problem for just TAAs.

Because option 3.2 is the most expensive, I think we should proceed with option 3.3.;;;","12/Dec/19 3:57 AM;Toktar;[~esplinr] Excuse me, a few new questions.
1. Should a node reject or apply a request with signed TAA for a CONFIG or POOL ledgers (where TAA cannot be enabled by design)?
2. Can client enable old TAA with upgrade an old transaction and clean ""retired""?;;;","13/Dec/19 1:26 AM;Toktar;*New PoA:*
 * versioning mechanism
 ** old handler *- done*
 ** txn versioning for all txn types - CURRENT_TXN_VERSIONS *- done*
 ** add test for the old handler *- done*
 ** add an integration test: *- in progress*
 *** patch CURRENT_TXN_VERSIONS - ""1"" for TAA and use the old handler for a state updating
 *** write TAA
 *** unpatch all
 *** write a new TAA
 *** add a new node
 *** write a new TAA
 *** check that all nodes have same data
 * change TAA validation
 ** for ADDING:
 *** check that ""_retired"" is None_
 ** _for APDATING:_
 *** check that ""_text"" is the same with the TAA text from the ledger_
 *** check that ""digest"" is not the same with a digest from the last TAA
 * change update_state in the TAA handler in indy-plenum
 ** add _digest : str_ to value  *- done*
 ** add _ratified_date_ _: timestamp (if it's an adding txn_)  *- _in progress_*
 ** add _retired_ _: timestamp_ if it's _filled*-* *in progress***_
 ** use _text_ from previous value if it's not contains in the txn  *- done*
 ** add _taa:list_ with list of last active TAAs _*- in progress*_
 ** for ADDING:
 *** add a new TAA to the _taa:last_
 *** _add a new TAA to the_ _taa:list_
 *** _update_ _taa:list - remove_ _retired TAAs_
 ** _for APDATING:_
 *** _add a new TAA to the_ _taa:list_
 *** _update_ _taa:list - remove_ _retired TAAs_
 ** _update unit tests_  _*- in progress*_
 * change client request validation in _do_taa_validation()_  _*- in progress*_
 ** refactoring for the old method
 ** 
||TAA is required||There are active TAAs||Request contains valid TAA signature||Validation result||
|-|+/-|-| PASS|
|-|+/-|+|REJECT|
|+|-|-|PASS|
|+|-|+|PASS|
|+|+|-|REJECT|
|+|+|+|PASS|

 * 
 ** add a check that sent TAA is active
 ** add unit tests
 * add a new transaction TRANSACTION_AUTHOR_AGREEMENT_DISABLE to disable TAA on the pool
 ** create a new handler:
 *** set retired in the past for all TAAs from taa:list
 *** clean taa:list
 *** clean taa:last
 ** add unit and integration tests

 * Client TAA request
 ** add _retired_ to client request
 ** make _text_ optional
 * add integration test like a system test but without upgrades
 * update a documentation for TAA, add for TAA_DISABLE;;;","13/Dec/19 3:48 AM;esplinr;1. We should be consistent across all transactions in not rejecting transactions that are signed by a TAA unnecessarily. This makes it easier for clients.

2. Two cases:
* If a TAA has a retired date, but the date has not yet passed (the TAA is still active), it should be possible to update or clear the retired date.
* We expect it to be truly exceptional to ""unretire"" an old TAA, and the old TAA can always be submitted as a new TAA. However, if it was necessary, it would be nice to be able to bring a TAA out of retirement while preserving the hash (thus avoiding having all users reaccept it). As discussed previously, the decision on the right architecture depends on the engineering complexity to create the feature, test it, and maintain it. It is a nice-to-have, but won't be worth a significant cost.;;;","13/Dec/19 3:52 AM;esplinr;Other clarifications on TAA behavior:
* The TAA is not required for writing to the Pool or Config ledgers, so it isn't required for a transaction that disables enforcing TAA acceptance to include a TAA acceptance.
* Adding a retirement date to an existing TAA is the same transaction type as creating a new TAA, but you don't have to specify the text so the transaction will be rejected if there is no existing TAA that matches. In addition, a new TAA is not allowed to have a retirement date. For both of these reasons, it should not be possible to accidentally create a new TAA when updating the retirement date.
* When updating the retirement date, the existing TAA to which the transaction applies can be specified with the version number.
* Disabling enforcement of the TAA will retire all old active TAAs.
* We considered adding an ""active date"" so that a TAA could be added to the ledger but not immediately enforced. This would allow inspection to ensure that it was correctly encoded as markdown. However, updating the latest TAA in the ledger state after the active date had been triggered would require a state proof, which the discussion in previous comments shows would be problematic.;;;","13/Dec/19 7:30 AM;lbendixsen;[~esplinr] I have read the rest of the comments here but I still have a few remaining questions/comments about versions for the TAA.  
 # Will the TAA version number have a prescribed format and will it need to be incremental?  (i.e can the first version be 7.0.2, and the next version be 4 for example?) I would prefer it if you forced a format and also forced it to be incremental to guard against possible mistakeos on my part.  
 # Can you repeat a version number, or must each be unique?  (In case I forget to manually increment my version string when I do an update) Do you have tests that check for this?
 # If there is a document describing the expected behavior then please send me a link.;;;","13/Dec/19 6:30 PM;sergey.khoroshavin;[~lbendixsen] as of current implementation:
# TAA version number doesn't have any prescribed format. It is just string.
# Version number must be unique. If you try to send a new TAA with already existing version number it will be rejected. And it is covered by tests (https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/txn_author_agreement/test_txn_author_agreement.py#L62)
# There is a document describing TAA: https://github.com/hyperledger/indy-plenum/blob/master/docs/source/transaction_author_agreement.md;;;","20/Dec/19 4:31 PM;ashcherbakov;PR with implementation:
 * [https://github.com/hyperledger/indy-plenum/pull/1424]
 * https://github.com/hyperledger/indy-node/pull/1530

Documentation update:
 * [https://github.com/hyperledger/indy-plenum/pull/1432]
 * [https://github.com/hyperledger/indy-node/pull/1543]

Debug and testing will be continued in the scope of INDY-2316;;;","21/Dec/19 12:32 AM;esplinr;As a note, the behavior of the Acceptance Mechanism list has not changed. A transaction must include in the TAA acceptance an indication of which mechanism was used, and that must come from the current latest Acceptance Mechanism List. When a new list is added, the previous list is immediately retired.

This means that:

* TAA version 1 has a list with mechanism A, B, C
* When TAA version 2 is approved, it is decided that the mechanism list should be B, C, D
* Even though version 1 has not been retired, acceptance validation will be based off of the mechanism list B, C, D.

We believe this is acceptable because the mechanism list will change rarely. If we do decide that a change is necessary we will either need to immediately impact people using the deprecated acceptance mechanisms, or we will need to modify the ledger behavior to smooth the transition. We won't be doing that work unless we see that it is necessary.;;;"
Sovrin upgrade fails due to incorrect package dependencies,INDY-2303,43429,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,23/Nov/19 12:22 AM,04/Feb/20 6:16 PM,28/Oct/23 2:47 AM,10/Dec/19 6:31 PM,,,1.12.1,,,,,0,TShirt_M,,,,"Steps to Reproduce:
1. Install sovrin 1.1.50 stable.
2. Try to upgrade it using pool upgrade command to sovrin 1.1.52.

Actual Results:
There are 2 sovtoken entries in dependecy list in journalctl: one with proper version number and another without version. The upgrade is failed due to unmet dependencies - it tries to get the latest sovtoken:
{noformat}
apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovtoken indy-plenum=1.9.1 sovtokenfees=1.0.1 indy-node=1.9.1 sovtoken=1.0.1 libindy-crypto=0.4.5 sovrin=1.1.52
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/19 11:38 PM;VladimirWork;jctl.txt;https://jira.hyperledger.org/secure/attachment/18014/jctl.txt","04/Feb/20 6:16 PM;VladimirWork;jctl_stable.txt;https://jira.hyperledger.org/secure/attachment/18123/jctl_stable.txt","06/Dec/19 11:38 PM;VladimirWork;node_control.log;https://jira.hyperledger.org/secure/attachment/18013/node_control.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irq",,,,Unset,Unset,Ev-Node 19.24,Ev-Node 19.25,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"04/Dec/19 7:47 PM;VladimirWork;[~anikitinDSR] [~ashcherbakov] Can we test this fix without fixing https://jira.hyperledger.org/browse/INDY-2216 ?;;;","06/Dec/19 6:56 PM;VladimirWork;Upgrade still fails on 1.1.179 sovrin.;;;","10/Dec/19 12:25 AM;VladimirWork;Waiting for new builds with the final fix.;;;","10/Dec/19 11:45 PM;VladimirWork;Build Info:
sovrin 1.1.185

Steps to Validate:
1. Install 1.1.185 master sovrin and upgrade it to 1.1.186 master (1.1.187 also exists).
2. Install 1.1.185 master sovrin and upgrade it to 1.1.187 master.

Actual Results:
Both upgrades perfromed successfully.;;;",,,,,,,,,,,,,,,,,,
Auth_Rules documentation should explain how endorsers work,INDY-2304,43430,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,esplinr,esplinr,23/Nov/19 1:07 AM,09/Dec/19 11:16 PM,28/Oct/23 2:47 AM,09/Dec/19 11:16 PM,,,1.12.1,,,,,0,,,,,"This page should have a brief description of when endorsers are required to authorize a transaction:

[https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md]

Specifically:

* Unpriviledged users cannot submit any transaction (including administrative transactions like pool upgrade or restart) without a signature from a DID with the endorser role that is specified in the endorser field.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41huu",,,,Unset,Unset,Ev-Node 19.24,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add IP address range for outbound TCP connections from validator nodes,INDY-2305,43475,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,smithbk,smithbk,smithbk,27/Nov/19 6:09 AM,17/Dec/19 11:41 PM,28/Oct/23 2:47 AM,,,,,libsovrin,plenum,,,0,,,,,"The availability requirements for steward nodes are very high (99.9+% uptime).  In order to achieve this in cases of hardware failure, a steward node can run in a virtual environment such as kubernetes which provides failover from one failing node to another.

In order to provide greater HA for stewards, a steward node should be allowed to specify an IP address range which it will use for outbound TCP connections when connecting to other stewards.  This will allow a steward to failover to another piece of hardware which uses a different source IP address for outbound TCP connections.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwqr3:wu",,,,Unset,Unset,CommunityContribution,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,esplinr,sergey.khoroshavin,smithbk,,,,,,,,"27/Nov/19 9:54 PM;sergey.khoroshavin;Thanks for raising this. 

Indy node has config for IP address it binds to (located at /etc/indy/indy.env), which could be different from public IP address announced to other nodes through NODE transaction in pool ledger.  If my understanding is correct you want to be able to route traffic from a single public IP to some kubernetes pod running indy node which has arbitrary internal IP in some subnet, and that pod can be recreated (which would change its IP) at any time. In this case you have several options:
* just bind node to 0.0.0.0 - it will bind to all available interfaces and IP addresses (simplest, but probably not secure enough in some environments)
* when provisioning pod run some initial configuration script which writes correct internal IP to /etc/indy/indy.env
* modify indy node so that an IP range can be provided in /etc/indy/indy.env - you can look at INDY-1332 for some reference where to start implementation

However, if you want node to have several public IP addresses - this is a whole different story.;;;","05/Dec/19 12:46 AM;smithbk;Thanks Sergey, but let me clarify.  In our kubernetes cloud environment, our validator is always available at some fixed IP.  For example, say it is 1.2.3.4.   There is no problem there.

The problem is that when our validator makes a TCP connection to another validator, the source IP address that it uses is not 1.2.3.4.  In our case, it could be any of two IP addresses, for example: 1.2.3.5 or 1.2.3.6, depending on where the validator happens to be running at the time.  The problem is then that the other validator will block our inbound TCP connection because the source IP is not 1.2.3.4.

Therefore, I would like to add additional metadata which appears in the pool_transactions_genesis file.  For example, 

{code}
""node_ip"": ""1.2.3.4"",
""node_ip_outbound"": [""1.2.3.5"",""1.2.3.6""],
{code} 

I'm not suggesting that the validator code needs to change to enforce this by checking the source IP address for inbound connections; however, this data can be used to configure firewalls appropriately.

Does this use case make sense now?;;;","09/Dec/19 8:33 PM;sergey.khoroshavin;Thanks for clarifying. Probably it's my lack of production experience with kubernetes, but I'm still not sure whether I understand correctly how your case works. I'll try to describe it here, please correct me if I'm wrong:
* you have a kubernetes cluster with public IPs assigned to each node
* you have a pod running indy ode, probably managed by StatefulSet
* you have a Service bound to some external IP, which routes all traffic to indy node pod
* inside pod with Indy Node you have only one network interface bound to current kubernetes node, and all outbound traffic goes unmodified
* there is no solution (like SNAT) to make traffic going from nodes appear like it goes from service external IP
If this is the case - then yes, NODE transaction should be modified to include outbound node IPs. However this looks a bit hacky, that's why I wasn't considering this scenario in the beginning.
;;;","11/Dec/19 2:53 AM;smithbk;[~sergey.khoroshavin] Yes, your understanding is correct.  Also, according to the IBM Cloud experts I've been talking to in order to find a solution. they do not know of any cloud offerings which offer SNAT for outbound connections, or at least not out of the box.  It is definitely not part of kubernetes.  I agree that it would be nice and cleaner if cloud solutions provided this, but this use case is quite unique since the majority of cloud apps deal only with hostnames and even fewer would be concerned with the outbound source IP address.

That said, can you point me in the right direction for where to add this metadata?

Thanks;;;","12/Dec/19 1:48 AM;sergey.khoroshavin;[~smithbk] Node transactions (those which contain information about nodes) are handled here: https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/request_handlers/node_handler.py . Main entry points are *static_validation* (stateless check that request is properly formed), *dynamic_validation* (stateful check that request is valid, can be based on data already written to ledger) and *update_state* (update merkelized key-value store which is used for fast secure lookups). However it looks like nothing needs to be changed in this part of code. Also we have transaction schema validation, schemas which are relevant for this task are defined here: https://github.com/hyperledger/indy-plenum/blob/master/plenum/common/messages/client_request.py 

In my opinion minimal implementation would be:
* update ClientNodeOperationData schema to include optional field ""node_ip_outbound"", and for you use cases it looks like it should be either network subnet or list of network subnets, not individual IPs
* update schema tests to match updated scheme
* write integration test which sends NODE transaction with ""node_ip_outbound"" key added and check that it is written into ledger (it could be added here https://github.com/hyperledger/indy-plenum/tree/master/plenum/test/pool_transactions, with other tests used as a somewhat reference)
* update documentation so this new key becomes part of public API and other stewards can rely on it when setting up their whitelisting rules

I think it makes sense to raise WIP PR as soon as you start working on it so that:
* if you accidentally break any tests you get early notification
* more people become aware of this change as soon as possible and could do preliminary review;;;","12/Dec/19 2:30 AM;anikitinDSR;[~smithbk] If I understood correctly, the situation is (for example):
 # NodeA has several instances, on 3 different machines, on '1.2.3.4', '1.2.3.5' and '1.2.3.6'. If something goes wrong, you can automatically switch to another instance, '1.2.3.5' for example. Address for outbound connections will keep the same, '1.2.3.4', but inbound connections will be changed to '1.2.3.5'. Does it mean that all other nodes (NodeB, NodeC, etc) should change those connections to other one, '1.2.3.5' ?
 # We use ZMQ now and we expect, that all of connection-like problems will be resolved by ZMQ. I mean, if we have network problem, unplugged cable or something like that, ZMQ resolves all this problem, all of messages will be delivered/resent after reconnection. As I understood, if instance '1.2.3.4' will be broken then we need to ask ZMQ to reconnect to another address, '1.2.3.5', and resend all packages to other one.
 # Does this instance changing affect client side too? I mean, do we need keep the same logic on the client side?;;;","12/Dec/19 3:08 AM;esplinr;The suggested change appears odd because it adds an additional field to the definition of a node transaction on the pool ledger, and that field is not actually used by Indy Node. However, I think it is appropriate because the pool ledger exists to communicate node information to all the validators in the pool, and we have explicitly decided that managing the whitelist of validator nodes should be managed outside of Indy Node by the stewards firewall solution.

Discussing this with the team, it seems like the ideal solution would be to implement a type of service discovery mechanism so that validators can change their IP addresses without a network transaction, but that is a bigger set of changes than can be completed in this ticket. A service discovery mechanism could also help with dynamic node selection, reducing the size of genesis files, and the catchup mechanism for validators and clients.;;;","12/Dec/19 8:56 AM;esplinr;[~lbendixsen] says that in the last couple of months, at least three Sovrin stewards were unreachable from other validator nodes due to them using a different IP address and port number for inbound and outbound connections. These stewards say this is a common thing when hosting behind corporate firewalls. They suggest that we explicitly list an inbound and outbound IP address. They also requested that we have an automated way for Indy Node to update this information.

The list proposed by Keith probably meets the same need with more flexibility. We probably need to also allow for a list of port numbers in addition IP addresses.;;;","13/Dec/19 8:00 AM;esplinr;I spoke with [~danielhardman].

He thinks that we should have different IP addresses for ingress and egress.

However, he is very concerned about the idea of a range of IP addresses, where the validator node can dynamically select its IP address. Our consensus algorithm assumes that validator nodes will have a stable IP address.
* If a node does not respond to a message because the message was sent to the wrong IP address, its reputation will suffer. This will prompt unnecessary requests for View Changes.
* When a node changes its IP address, not all of the network will recognize the need to communicate on the new IP address at the same time. The transition from the old IP address to the new IP address makes the network non-deterministic and can easily cause a partition in the network that exceeds our threshold of F.
* Errors caused by nodes communicating on the wrong IP address are very difficult to diagnose (as Lynn discovered recently).
* Kubernetes can select any internal IP address, so long as the administrator of the validator node runs a firewall or load balancer in front of the machine to give it a stable IP address.

Daniel also pointed out that this would be a good time to make the port designation explicit instead of allowing ZMQ to use any port within a range of 100. This is a common request from administrators who want to reduce the number of open ports on their firewalls.;;;","14/Dec/19 12:58 AM;esplinr;Sergey pointed out that I misunderstood part of this issue.

The inbound IP address needs to be reliable to preserve consensus. It is not as dangerous to consensus if the outbound IP address changes dynamically.;;;","17/Dec/19 2:06 AM;esplinr;We discussed this on the Indy Contributors call.
* Allowing a large range of IP address could become an attack vector; any of those IP addresses could be used to disrupt validation.
* A small range would probably be appropriate, but that might not help with the Kubernetes use case.
* Could we limit it to a 32 or 64 subnet?;;;",,,,,,,,,,,
GET_CRED_DEF for a Schema with a lot of attributes may fail with Timeout,INDY-2306,43483,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,JeromeK,JeromeK,27/Nov/19 10:14 PM,19/Dec/19 7:56 PM,28/Oct/23 2:47 AM,13/Dec/19 11:03 PM,,,1.12.1,,,,,0,Needs-Review,node-SDK,quality,,"Hey Guys

https://chat.hyperledger.org/channel/indy-node?msg=kiDMpGmDcTKvrwvcX

After this discussion with [~ashcherbakov] he suggested creating a new Issue to talk about the 128 KB response limit of the Indy-Nodes. Because we think that the nodes have been only tested for the write-operations (where you don't have any problems with a maximum of 125 Attributes). But when you start reading the Data of the ledger you will get Metadata and some more crypto-stuff which ends up in a Timeout of the Nodes, because you can end up easily over that 128KB.",Tested on the SovrinStagingNet with the Indy-CL0 1.12.0 and the current python-wrapper,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41huv",,,,Unset,Unset,Ev-Node 19.25,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,donqui,JeromeK,VladimirWork,,,,,,,,"27/Nov/19 10:28 PM;JeromeK;Here another Chat that could verify this problem: https://chat.hyperledger.org/channel/indy?msg=8dy9zQpkwMZ3DnXFL;;;","04/Dec/19 4:52 PM;ashcherbakov;We checked how big is reply to GET_SCHEMA in this case:
{code}
16:08:26,383|DEBUG|client_message_provider.py|CONNECTION: Cannot transmit message. Error InvalidMessageExceedingSizeException('Message len 199615 exceeded allowed limit of 131072',)
{code}
So, the reply size is `199615` bytes which is more than the limit of 128K.
What we need to understand is why the state proof in this case is so big and whether this is expected. 

Possible options:
1) Reduce maximum number of attributes
- May be not so good from use cases point of view
- There are already big SCHEMA txns on prod nets
2) Increase message limit (from 128K to 256K for example)
3) Do not apply message limit restrictions for Replies to clients.

Option 3 may be the best one.;;;","04/Dec/19 5:28 PM;JeromeK;I would also recommend going with option 3.;;;","11/Dec/19 7:18 PM;donqui;Problem reason/description:
 - An outbound message could not be sent if it is over a certain size. This cause issues with schemas tha when written were under the limit, but when fetched went over the limit due to additional metadata and the size of the state proof.

Changes: 
 Removed check for the size of outbound messages
 * PR:
 [https://github.com/hyperledger/indy-plenum/pull/1429]

Version:
 - plenum: 1.12.1.dev977
 - node: 1.12.1~dev1162

Risk:
 - Low

Covered with tests:
 * [https://github.com/hyperledger/indy-plenum/blob/master/stp_zmq/test/test_no_size_limit_for_outbound_msgs.py]

Recommendations for QA
 - write a SCHEMA that is under the limit but close to it, and try to read it so that when read it is over the limit. This probably needs to be done on a build that does not have a fix, and the build with the fix.;;;","13/Dec/19 11:03 PM;VladimirWork;Build Info:
1.12.1~dev1162

Steps to Valdiate:
https://github.com/VladimirWork/indy-test-automation/blob/93f874abaaed4653f49a7a0f86583715f0d5f589/system/draft/test_misc.py#L2250

Actual Results:
Test fails against 1.12.1~dev1158 wiht PoolLedgerTimeout. Test passes against 1.12.1~dev1162.;;;","19/Dec/19 5:30 PM;ashcherbakov;[https://github.com/hyperledger/indy-node/pull/1542] - tests to check that GET_CRED_DEF for large Schemas works;;;",,,,,,,,,,,,,,,,
Support historical req handlers for non-config ledgers,INDY-2307,43484,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,27/Nov/19 10:20 PM,28/Nov/19 5:36 PM,28/Oct/23 2:47 AM,28/Nov/19 5:32 PM,,,1.12.0,,,,,0,,,,,"We supported pool-version-based pluggable req handlers for old version of txns in the scope of INDY-2292, but the fix there was working for config ledger only.
We need to support it for all ledgers.

One of the options how it can be implemented is to use timestamps",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i408i",,,,Unset,Unset,Ev-Node 19.24,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"27/Nov/19 10:59 PM;Toktar;*Problem reason:*
 - Versioned state restoring works only for Config ledger but this feature is also needed for other ledgers.

*Changes:*
 - Create a dict with version-timestamp in Config state restoring.
 - Use the dict to restore other ledgers.

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1419]
 * [https://github.com/hyperledger/indy-node/pull/1519]
 * [https://github.com/sovrin-foundation/token-plugin/pull/352] 

*Version:*
 * sovrin - #174 - master
 * token-plugin #122 - master
 * indy-node 1.12.0.dev1142 -master
 * indy-plenum 1.12.0.dev966 -master

*Risk factors:*
 - Problems with a state restoring

*Risk:*
 - Medium

*Tests:*
 * [test_leder_state_recovery_with_different_versions.py|https://github.com/sovrin-foundation/token-plugin/pull/352/files#diff-fb7b1c6a068592493b156958c4e143ca] 

*Recommendations for QA:*
 * Write a system test:
 ** UPGRADE_POOL to 1.9.0
 ** Send mint and xfer
 ** UPGRADE_POOL to latest version
 ** Promote a new node
 ** Restart an old node with a states removing
 ** Send mint and xfer
 ** Check that all nodes have a same data;;;","28/Nov/19 5:27 PM;VladimirWork;Build Info:
indy-node 1.9.0
plugins 1.0.0

Steps to Validate:
1. Install sovrin 1.1.50 pool.
2. Upgrade it to 1.1.50 with reinstall.
3. Mint tokens and sent a few payments.
4. Upgrade pool to the latest stable using pool upgrade command.
5. Upgrade pool to the latest master (1.1.174 sovrin / 1.12.0~dev1142 indy-node) using apt.
6. Add new node to the pool and wait for catchup.
7. Sent a few payments.

Actual Results:
All txns were written and catched up. Ledgers and states at all nodes are in sync.;;;",,,,,,,,,,,,,,,,,,,,
A node lagging behind may not be able to finish view change if nodes have been added/demoted,INDY-2308,43489,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,28/Nov/19 4:48 AM,20/Jan/20 7:06 PM,28/Oct/23 2:47 AM,17/Jan/20 10:11 PM,,,1.12.2,,,,,0,,,,,"*Issue*
* A node is lagging behind and doing a view change to viewNo=X. This may happen if a node joins a pool under heavy load.
* Other nodes in the pool are already on viewNo=X+Y, and *their node registry is different* (that is new Nodes have been demoted/added).
* The node finishes the view change, and stashes all 3PC messages for future view coming from other nodes
* The node doesn't unstash the messages until it performs re-ordering till last prepared certificate
* The node gets a quorum of instance changes for viewNo=X+1
* The same situation happens for viewNo=X+2, X+3, etc.
* At some view lagging Node expects different Primaries than other nodes did since it doesn't have up-to-date node registry
* As we don't process any checkopoints and don't start catchup by checkpoints during the view change, the node will be in the view change forever

*Workaround*
* Node restart

*Possible fix*
* Option 1: Process future checkpoints and allow to start catchup during view change by checkpoints
* Option 2: request NEW_VIEW from a quorum of Node and trust who is the Primary based on this NEW_VIEW",,,,,,,,,,,,,,,,,,,,,,,,INDY-2276,INDY-2320,INDY-2322,INDY-2326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irw",,,,Unset,Unset,Ev-Node 19.25,Ev-Node 19.26,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,donqui,Toktar,,,,,,,,,"11/Dec/19 9:35 PM;Toktar;PoA:
 * Write an integration test:

 * 
 ** make 2 view changes (current view_no=2)
 ** delay NewView messages on [Node1, 2, 3, 4] (nv_delay())
 ** promote new Node5 (send_and_check the transaction)
 ** start Node5
 ** delay commits on all nodes(Node1, 2, 3, 4, 5) (node.nodeIbStasher.delay(cDelay()))
 ** reset delay for NewView
 ** wait for a view change (2->3)
 ** promote new Node6  (send, not check the transaction)
 ** start Node6, wait for a catchup
 ** reset delays for commits
 ** wait for a view change (3->4) (Node5 a new primary)
 ** check that view change finished on all nodes
 ** check that all nodes can write
 * Add a fix. If a node receive NewView from non-primary node, ask all nodes for a NewView. If the lagging node have a quorum (f+1) of NewView messages,
 ** finish a view change 
 ** start a catchup ( ? );;;","17/Dec/19 5:07 AM;donqui;Fork that contains a test that potentially tests this exact issue:
 [https://github.com/donqui/indy-plenum/tree/feature/INDY_2308]
 plenum/test/view_change/test_nemanja.py;;;","17/Jan/20 10:10 PM;ashcherbakov;Validation will be done in the scope of INDY-2326;;;",,,,,,,,,,,,,,,,,,,
Do not verify individual BLS sigs in COMMITs,INDY-2309,43602,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,05/Dec/19 9:50 PM,08/Jan/20 5:17 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Option 1: Do not verify individual BLS sigs in commits:
- Verify aggregated BLS sig only when receive a quorum of commits (in assumption that in most of the cases it's valid)
- If aggregated BLS sig is invalid, then validate each individual COMMIT

Option 2: Validate BLS sigs on Primary only
- Do not validate BLS sig for Commits on non-Primary replicas
- Do not calculate aggregated BLS sig on non-Primary replicas while Ordering
- If a Primary doesn't have requests to be sent when it's time for the next Batch, but there are requests on the previous batch ordered (so that BLS aggregated sig is not calculated for them yet), then send a Freshness empty batch now (doesn't wait till the next Freshness check).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41isni",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Release 1.12.1,INDY-2310,43604,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,05/Dec/19 11:50 PM,28/Dec/19 5:32 PM,28/Oct/23 2:47 AM,28/Dec/19 5:26 PM,,,1.12.1,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irr",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"26/Dec/19 10:47 PM;VladimirWork;*Version Information*
indy-node 1.12.1
indy-plenum 1.12.1
sovrin 1.1.67

*Notices for Stewards*
Use forced simultaneous pool upgrade.
Please be careful with demoting/promoting/adding nodes (see Known Issues for details).
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.

*Major Changes*
- Multiple active TAAs implementation
- Stability fixes

*Detailed Changelog*

+Major Fixes+
INDY-2303 - Sovrin upgrade fails due to incorrect package dependencies
INDY-2306 - GET_CRED_DEF for a Schema with a lot of attributes may fail with Timeout
INDY-2024 - Only Trustee or Node owner can be the author of NODE demotion txn regardless of endorsement or auth constraint rules set

+Changes and Additions+
INDY-2302 - Allow multiple active TAAs
INDY-2316 - Allow multiple active TAAs: Debug
INDY-2313 - Improve TAA acceptance date validation
INDY-2297 - Get TAA should return the hash
INDY-2304 - Auth_Rules documentation should explain how endorsers work
INDY-2138 - Document PBFT view change protocol
INDY-2299 - Backups should start ordering in new view only after master instance ordered till prepared cert from NewView
INDY-2294 - Get rid of transport batches
INDY-2289 - Enable zeroMQ auto-reconnection
INDY-2286 - Improve simulation tests to include NODE txns
INDY-2280 - Improve BLS signature performance
INDY-2263 - Improve simulation tests to include processing of InstanceChanges

+Known Issues+
INDY-2308 - A node lagging behind may not be able to finish view change if nodes have been added/demoted
INDY-2319 - Up to F Nodes may not be able to finish View Change if there are uncommitted NODE txns
INDY-2318 - A node may start re-sending messages in a loop in case of connection issues
INDY-2320 - A lagging node may use wrong N and F quorum values and never finish view change if there are NODE txns being processed
;;;",,,,,,,,,,,,,,,,,,,,,
Replace Indy Crypto with Ursa,INDY-2311,43630,,Task,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,sergey.khoroshavin,esplinr,esplinr,07/Dec/19 12:48 AM,16/May/20 2:57 AM,28/Oct/23 2:47 AM,,,,1.13.0,indycrypto,plenum,,,0,,,,," 

[https://github.com/hyperledger/indy-plenum/pull/1369]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixk",,,,Unset,Unset,Ev-Node 20.2,Ev-Node 20.05,Ev-Node 20.06,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,"19/Mar/20 5:23 PM;ashcherbakov;[~cam-parra]
Do you have any progress with the task? We are considering to help with the task in this Sprint if needed. ;;;",,,,,,,,,,,,,,,,,,,,,
Incorrect audit ledger error during production load test,INDY-2312,43644,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,,VladimirWork,VladimirWork,10/Dec/19 12:34 AM,10/Dec/19 7:58 PM,28/Oct/23 2:47 AM,10/Dec/19 7:58 PM,,,,,,,,0,,,,,"Build Info:
indy-node 1.12.1~dev1154
plugins 1.0.6~dev128

Steps to Reproduce:
1. Set `ForceViewChangeFreq = 10800`.
2. Run production load test for 66 hours.

Actual Results:
We have one node stalled (8) and another one with *more* amount of txns than n-f nodes (15).
There is an *incorrect audit ledger* error in logs. ViewNo at all nodes ranges from 20 to 862. Pool was not writing at the last hours of load test.

Logs and metrics:
ev@evernymr33:logs/09_12_2019_summary.tar.gz
ev@evernymr33:logs/09_12_2019_full_data.tar.gz (edited) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irp",,,,Unset,Unset,Ev-Node 19.25,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,"10/Dec/19 7:56 PM;Toktar;The pool lost consensus due nodes had no space left on devices.
So it's expected behavior for case when no space left.;;;",,,,,,,,,,,,,,,,,,,,,
Improve TAA acceptance date validation,INDY-2313,43707,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,13/Dec/19 6:49 AM,10/Jan/20 6:33 AM,28/Oct/23 2:47 AM,26/Dec/19 10:33 PM,,,1.12.1,,,,,0,,,,,"*Story*
As a developer building solutions for a network that enforces a Transaction Author Agreement, I want my organization's legal team to be able to review and accept the TAA in advance of it being written to the ledger. Thus when we submit a transaction we can report the real date of meaningful acceptance, instead of an arbitrary date engineered to be newer than when the TAA is added. This will also allow us to stage the configuration change in advance of the TAA being added, which reduces our risk of disruption when the TAA is written.

The TAA could be legally accepted at any point after the TAA is approved by network governance. After approval, it takes some time for the TAA to be encoded in MarkDown and anchored on the ledger.

Validating that the date of TAA acceptance is after the TAA was written to the ledger causes organizations that accepted the TAA in advance to risk a disruption to the ability to write to the ledger because they can't finalize their configuration until after the date the TAA is added to the ledger. They should be able to deploy the configuration change earlier so that it is ready when the ledger is updated, and indicate a more accurate date of TAA acceptance.

*Acceptance Criteria*
* When a TAA is anchored to the ledger, it must include a property representing the date when the TAA was approved for use on the network. Let's call it taa_ratified_date.
* The taa_ratified_date must be in the past.
* The taa_ratified_date cannot be edited by the administrator.
* If a TAA is already on the ledger, then the date the transaction was written to the ledger should be used as the taa_ratified_date.
* When a TAA is enforced on a ledger, accept any date for the TAA acceptance that is equal to or earlier than the present time and newer than or equal to the ratified_date (preserving the current logic to account for a few minutes of clock skew).
* Update the documentation.

*Notes*
* The taa_ratified_date must be in the past to avoid the challenges that come with the latest TAA being invalid, or dynamically updating the value with the proper BLS signatures when it becomes valid.
* The taa_ratified_date cannot be modified in order to simplify development and testing (changing requirements are threatening to delay the release). If we need to be able to modify it, we can scope that as a separate story.
* Reason for the change:
** When we originally designed the TAA, we thought it would be important to validate that the date of TAA acceptance was realistic. We considered this important for the TAA acceptance to fulfill its role as evidence of meaningful acceptance.
** However, we erroneously had a mental model of individuals interacting with the ledger in real-time, whereas individual data should not be written to the public ledger so the majority of writes are made by organizations where the software operator is not authorized to bind the organizations to an agreement like the TAA. That must be done by the legal team and management.
** Further, we did not consider that there will be a lag between the TAA being published, and when it will actually be put on the ledger. In reality, the date of acceptance can predate the enforcement on the ledger.
** Finally, we did not appreciate the coordination difficulty caused by having to coordinate the date of TAA acceptance. We don't want to require every client to have to code around the problem.",,,,,,,,,,,,,,,,,,,,,INDY-2302,INDY-1942,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irqi",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,sergey.khoroshavin,VladimirWork,,,,,,,,,"14/Dec/19 3:37 AM;esplinr;The approved version of the TAA for the Sovrin Network incorporates this language:
{quote}
If Transaction Author continues to Author Transactions to the Sovrin Ledger Networks after the Amendment Cutover Date, such continued use will constitute acceptance of the amended Agreement.   
{quote}

This is interpreted to mean that once someone has accepted any version of the agreement, they do not need to explicitly accept future versions because continued usage is acceptance. [~drummondreed] confirmed that:
* Authors need to be notified that the new version exists.
* Authors should continue to sign each transaction with a TAA acceptance.
* And authors should continue to include the hash of the latest TAA in their acceptance.
* But that the acceptance date can be any date after the first TAA was enforced on the ledger and available for acceptance.

This approach raises questions like:
* How does this logic hold when we cannot enforce that users of a distributed ledger are notified of the latest agreement?
* Is it really meaningful acceptance to not review the latest agreement?
* Why would we include the hash of the latest agreement, but not the date the latest agreement was accepted?
* Is it really not important to know which version of the agreement the Transaction Author agreed to (because the acceptance date doesn't relate to the hash being used to sign the transaction).

Regardless, the approach described in this issue is compatible with these requirements because the ratified date for the TAA could be the date the first TAA is ratified. The same ratified date would continue to be used when future agreements are put on the ledger.;;;","17/Dec/19 7:36 AM;esplinr;We discussed this on the Indy Contributors call. We will proceed with the flexible approach defined in the body of this story, knowing that it can also be used the way that Drummond explained. Future work outside of the Indy project:
* When there is a new TAA for the Sovrin Network, they can decide whether to populate the ratified date with the same date as the first enforced version, or the date the new agreement is ratified.
* The Sovrin Governance Working Group will need to work with the Sovrin Foundation and other members of the community to specify the UX patterns they think should be used.;;;","20/Dec/19 8:41 PM;sergey.khoroshavin;*PoA*
* modify scheme of TRANSACTION_AUTHOR_AGREEMENT transaction
** add optional field *ratification_ts*
** change existing optional field *retired* to *retirement_ts* for consistency
* implement tests and logic according to requirements;;;","24/Dec/19 2:33 AM;sergey.khoroshavin;[~esplinr] Requirements question - when we send a new TAA should nodes allow for some clock skew when checking that ratification datetime is in the past or not? I expect answer to be no (with rationale described [here|https://github.com/hyperledger/indy-plenum/pull/1437/#discussion_r360933051]), but - again - this is a requirements question, so having your real answer would be much better.;;;","25/Dec/19 8:27 PM;sergey.khoroshavin;*Problem reason*
As described in requirements

*Changes made*
Added optional ratification_ts field to TXN_AUTHOR_AGREEMENT transaction, implemented logic described in requirements, update load script to use new workflow. In process had to update API of pluggable request handlers (most notable change - added optional req_pp_time parameter to WriteRequestHandler.dynamic_validation).

*Versions*
indy-plenum 1.12.1.dev988
indy-node 1.12.1~dev1171
token-plugin 1.0.6~dev138

*PR*
https://github.com/hyperledger/indy-plenum/pull/1434
https://github.com/hyperledger/indy-node/pull/1546
https://github.com/sovrin-foundation/token-plugin/pull/369
https://github.com/hyperledger/indy-plenum/pull/1437
https://github.com/hyperledger/indy-node/pull/1550
https://github.com/sovrin-foundation/token-plugin/pull/372

*Covered by tests*
test_create_txn_author_agreement_without_ratified_fails
test_create_txn_author_agreement_with_ratified_from_future_fails
test_txn_author_agreement_update_ratification_fails
+ quite a lot of existing tests were updated to use new workflow

*Risks*
Medium

*Risk factors*
TAA workflow was changed, as well as pluggable request handlers API.

*Recommendations for QA*
* Check that current behaviour adheres to new requirements (described in this ticket and documentation)
* Check that sending actions still work
* Check that sending forced write transactions still work;;;","26/Dec/19 10:33 PM;VladimirWork;All system tests are implemented and pass:

https://github.com/hyperledger/indy-test-automation/blob/3e6232c492b47d32d9721b93a608a00c0cb0c16e/system/draft/test_misc.py#L2287
https://github.com/hyperledger/indy-test-automation/blob/3e6232c492b47d32d9721b93a608a00c0cb0c16e/system/draft/test_misc.py#L2592
https://github.com/hyperledger/indy-test-automation/blob/3e6232c492b47d32d9721b93a608a00c0cb0c16e/system/draft/test_upgrade_docker_7_new_taa.py#L20;;;","10/Jan/20 6:33 AM;esplinr;Regarding accounting for potential clock skew in comparing a TAA with the ratification date, I agree with the logic explained by [~sergey.khoroshavin] in the PR. It is very unlikely to be needed.;;;",,,,,,,,,,,,,,,
REV_REG_DEF `tag` field is not validated,INDY-2314,43735,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,VladimirWork,VladimirWork,16/Dec/19 11:12 PM,09/Jan/20 11:42 PM,28/Oct/23 2:47 AM,09/Jan/20 11:36 PM,,,1.12.2,,,,,0,,,,,"Build Info:
indy-node 1.12.1~dev1162
libindy 1.13.0~111

Steps to Reproduce:
1. Try to write REV_REG_DEF txn with 100000 symbols `tag`.

Actual Results:
PoolLedgerTimeout is returned.

Expected Results:
REQNACK is expected the same as for CRED_DEF.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/19 11:12 PM;VladimirWork;rev_reg_def_tag.tar.gz;https://jira.hyperledger.org/secure/attachment/18037/rev_reg_def_tag.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismi",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,VladimirWork,,,,,,,,,,"09/Jan/20 11:42 PM;anikitinDSR;PRs with fixes:
 * indy-plenum: [https://github.com/hyperledger/indy-plenum/pull/1451]
 * indy-node: [https://github.com/hyperledger/indy-node/pull/1562]

Changes:
 * added limitation for REVOC_TYPE, TAG fields for REVOC_REG_DEF txn type and for REVOC_TYPE field for REVOC_REG_ENTRY txn type. The limitation is 256 symbols
 * added integration tests for checking that node will reply REQNACK if length of fields not applied.

Version:
 * indy-node: 1.12.2~dev1183;;;",,,,,,,,,,,,,,,,,,,,,
Debug move to Aardvark: Phase 2,INDY-2315,43751,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,18/Dec/19 12:48 AM,20/Dec/19 11:54 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2250,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ji",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow multiple active TAAs: Debug,INDY-2316,43779,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Dec/19 4:23 PM,27/Dec/19 5:23 PM,28/Oct/23 2:47 AM,26/Dec/19 10:33 PM,,,1.12.1,,,,,0,,,,,"*Acceptance criteria*
 * Make sure there is enough test coverage by unit and integration tests
 * Make sure that SDK is used in all integration tests
 * Make sure all unit and integration tests pass reliably
 * Make sure there is enough test coverage by system tests
 * Make sure all system tests pass
 * Make sure all changes in code are implemented:
 ** Need to validate TAA acceptance against uncommitted TAA
 ** Fix retirement time for disabling (set to the current pp_time instead of 0)
 ** Do not allow empty text for TAA
 * Make sure all comments in [https://github.com/hyperledger/indy-plenum/pull/1424] are processed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/19 7:42 PM;VladimirWork;new_retired_taa.tar.gz;https://jira.hyperledger.org/secure/attachment/18046/new_retired_taa.tar.gz","20/Dec/19 11:42 PM;VladimirWork;old_timestamp.tar.gz;https://jira.hyperledger.org/secure/attachment/18047/old_timestamp.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irq9",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,"20/Dec/19 7:19 PM;VladimirWork;Build Info:
1.12.1~dev1165

Steps to Reproduce:
1. Try to create a transaction author agreement with a 'retired' field.

Actual Results:
Pool returns REPLY.

Expected Results:
Pool must return REJECT.

{color:#57D9A3}RESOLVED{color} - fixed in indy-node 1.12.1~dev1167 / plugins 1.0.6~dev136;;;","20/Dec/19 10:27 PM;VladimirWork;Testing PoA:

1. Check TAA backward compatibility at pool side (system test).
2. Check functionality and its compatibility to requirements (INDY-2302) (system test + review).
3. Check TAA backward compatibility at client side (state proof reading using old client, manual + system test).
4. Validate indy-node and indy-sdk TAA documentation (review).;;;","20/Dec/19 11:42 PM;VladimirWork;Build Info:
1.12.1~dev1165

Steps to Reproduce:
1. Try to write nym with appended TAA _with timestamp before TAA creation data_.

Actual Results:
Pool returns REPLY.

Expected Results:
Pool must return REJECT.

System test fails here: https://github.com/VladimirWork/indy-test-automation/blob/a72a4a24e54bcf6f6fe84565f64134df27b8699f/system/draft/test_upgrade_docker_7_new_taa.py#L238

{color:#57D9A3}RESOLVED{color} - timestamp was rounded to the actual date;;;","21/Dec/19 12:29 AM;Toktar;Please, re-test this step, if it's possible.
""Try to write nym with appended TAA _with timestamp before TAA creation data_.""
With the older time of signing TAA because it will be rounded to a day.
Thank you!;;;","25/Dec/19 5:39 PM;Toktar;*Problem reason:*
 - After changing the logic of the TAA, several bugs and several opportunities for improvements were found.

*Changes:*
 - Need to validate TAA acceptance against uncommitted TAA
 - Fix retirement time for disabling (set to the current pp_time instead of 0), disable only active TAAs
 - And other changes.

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1436]
 * [https://github.com/hyperledger/indy-plenum/pull/1438]
 * [https://github.com/hyperledger/indy-node/pull/1548]
 * [https://github.com/hyperledger/indy-plenum/pull/1439]
 * [https://github.com/hyperledger/indy-node/pull/1549]
 * [https://github.com/hyperledger/indy-plenum/pull/1442]

*Version:*
 * sovrin - #195 - master
 * token-plugin #139 - master
 * indy-node 1.12.1.dev1171 -master
 * indy-plenum 1.121.dev989 -master

*Risk factors:*
 - TAA

*Risk:*
 - Medium

*Recommendations for QA:*
 * Check that the DISABLE txn disables only active TAAs (change retirement_ts to time of ) and doesn't change old retired TAAs.
 * Add a system tests, when we send NYM with a new TAA right after creating it. Without waiting for the TAA committing ** 
 * Check that update TAA without field ""text"" works correctly
 * Check that the sdk metadata parser works correctly;;;","25/Dec/19 6:19 PM;VladimirWork;We have a stacktrace disabling TAA written before the upgrade without ratification_ts:
{noformat}
E           Dec 25 09:09:04 0782b52ae74a env[1240]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/request_handlers/txn_author_agreement_disable_handler.py"", line 48, in update_state
E           Dec 25 09:09:04 0782b52ae74a env[1240]:     taa[TXN_AUTHOR_AGREEMENT_VERSION], taa[TXN_AUTHOR_AGREEMENT_RATIFICATION_TS],
E           Dec 25 09:09:04 0782b52ae74a env[1240]: KeyError: 'ratification_ts'
{noformat}

{color:#57D9A3}RESOLVED{color} - fixed in indy-node 1.12.1~dev1174 / plugins 1.0.6~dev141;;;","26/Dec/19 10:32 PM;VladimirWork;All system tests are implemented and pass:

https://github.com/hyperledger/indy-test-automation/blob/3e6232c492b47d32d9721b93a608a00c0cb0c16e/system/draft/test_misc.py#L2287
https://github.com/hyperledger/indy-test-automation/blob/3e6232c492b47d32d9721b93a608a00c0cb0c16e/system/draft/test_misc.py#L2592
https://github.com/hyperledger/indy-test-automation/blob/3e6232c492b47d32d9721b93a608a00c0cb0c16e/system/draft/test_upgrade_docker_7_new_taa.py#L20;;;",,,,,,,,,,,,,,,
Cannot upgrade sovrin with unbound package dependencies,INDY-2317,43790,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,23/Dec/19 6:40 PM,23/Dec/19 6:41 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Build Info:
indy-node 1.12.1~dev1163
sovrin 1.1.191

Steps to Reproduce:
1. Try to upgrade pool to 1.1.195 sovrin (the latest).

Actual Results:
Wrong plenum version (not the latest) in dependencies list: `install python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken indy-node=1.12.1~dev1167` and failed upgrade:
{noformat}
Dec 23 08:28:52 940c3aa13c3f env[80]: E: Version '1.6.53' for 'indy-plenum' was not found
Dec 23 08:28:52 940c3aa13c3f env[80]: E: Version '1.3.0' for 'distro' was not found
Dec 23 08:28:52 940c3aa13c3f env[80]: + deps='python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken indy-node=1.12.1~dev1167'
Dec 23 08:28:52 940c3aa13c3f env[80]: + '[' -z 'python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken indy-node=1.12.1~dev1167' ']'
Dec 23 08:28:52 940c3aa13c3f env[80]: + echo 'Try to donwload indy version python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken ind
Dec 23 08:28:52 940c3aa13c3f env[80]: Try to donwload indy version python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken indy-node=1
Dec 23 08:28:52 940c3aa13c3f env[80]: + apt-get -y update
Dec 23 08:28:53 940c3aa13c3f env[80]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Dec 23 08:28:53 940c3aa13c3f env[80]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Dec 23 08:28:53 940c3aa13c3f env[80]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Dec 23 08:28:53 940c3aa13c3f env[80]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Dec 23 08:28:56 940c3aa13c3f env[80]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Dec 23 08:29:01 940c3aa13c3f env[80]: Reading package lists...
Dec 23 08:29:01 940c3aa13c3f env[80]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout
Dec 23 08:29:05 940c3aa13c3f env[80]: Reading package lists...
Dec 23 08:29:06 940c3aa13c3f env[80]: Building dependency tree...
Dec 23 08:29:06 940c3aa13c3f env[80]: Reading state information...
Dec 23 08:29:06 940c3aa13c3f env[80]: E: Version '1.6.53' for 'indy-plenum' was not found
Dec 23 08:29:06 940c3aa13c3f env[80]: + ret=100
Dec 23 08:29:06 940c3aa13c3f env[80]: + '[' 100 -ne 0 ']'
Dec 23 08:29:06 940c3aa13c3f env[80]: + echo 'Failed to obtain python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken indy-node=1.12.
Dec 23 08:29:06 940c3aa13c3f env[80]: Failed to obtain python3-dateutil sovrin=1.1.195 sovtokenfees python3-distro=1.3.0 python3-timeout-decorator=0.4.0 indy-plenum=1.6.53 sovtoken indy-node=1.12.1~dev116
Dec 23 08:29:06 940c3aa13c3f env[80]: + exit 1
{noformat}

Expected Results:
Upgrade to the latest versions of packages should be performed successfully even with unbound sovrin package.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i016sz:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A node may re-send messages in a loop in case of connection issues,INDY-2318,43797,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,24/Dec/19 11:22 PM,08/Jan/20 9:30 PM,28/Oct/23 2:47 AM,08/Jan/20 9:30 PM,,,1.12.2,,,,,0,,,,,"*Issue*
 * Reproduced in [test_misc_node_and_vc_interleaved system test|https://build.sovrin.org/job/indy-node/job/indy-node-cd/job/master/1169/testReport/junit/system.indy-node-tests.TestViewChangeSuite/TestViewChangeSuite/System_tests__master____TestViewChangeSuite_py___Run_TestViewChangeSuite_py____1__Upload_test_report___test_misc_node_and_vc_interleaved_10_5_False_/]
 * Node3 lost connection to Node1 and started to re-send the same message in a loop until it connected back

It lead to the issue that Node 3 wasn't able to finish View Change like others (see TBD).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41irs",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"25/Dec/19 12:10 AM;ashcherbakov;Issue Cause:
 * Node 3 was re-sending some messages after it got Pong from Node1
 * While re-sending, it realized (on `isConnected` check) that connection is lost
 * So, the next message to be re-sent was added to the original list of messages
 * So, we started to try re-sending the same message over and over again

 

Fix:
 * Remove custom re-sending logic since ZMQ should take care about this for us

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1441]

Recommendation for QA:
 * make sure the issue is not reproduced in system tests dealing with reconnections, adding/promoting/demoting nodes.;;;","08/Jan/20 9:30 PM;ashcherbakov;The issue is not reproduced in system tests anymore (in about 20 nightly runs).;;;",,,,,,,,,,,,,,,,,,,,
Up to F Nodes may not be able to finish View Change if there are uncommitted NODE txns,INDY-2319,43799,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,25/Dec/19 9:35 PM,14/Jan/20 12:37 AM,28/Oct/23 2:47 AM,14/Jan/20 12:37 AM,,,1.12.2,,,,,0,,,,,"*Issue:*

Up to F Nodes may not be able to finish View Change as they select different Primaries during view change if there are uncommitted NODE txns.
As uncommitted state is reverted after we select new primaries in new view, it's possible that some Nodes will select incorrect Primaries.

*Case:*
 * 4 Nodes, viewNo=3, Node4 is a Primary
 * a NODE txn to add Node5  is sent and received by Nodes, but not yet proposed in a PrePrepare to any Node
 * A view change to viewNo=3 is triggered, Node4 is selected as a new Primary
 * View Change is finished on Node4 (next Primary) only
 * Node4 sent a PrePrepare with a NODE txn to add a new Node (Node5)
 * Only Node4 applied this PrePrepare, so its node registry is updated (uncommitted)
 * The PrePrepare can not be committed as only 1 node finished view change
 * View Change to viewNo=4 is triggered (because of View Change timeout).
 * Nodes select primaries against (uncommitted) node reg as it was at the beginning of last view.
 ** [Node1, Node2, Node3, Node4] for Nodes 1-3 (for viewNo=2)
 ** [Node1, Node2, Node3, Node4, Node5] for Node 4 (for viewNo=3)
 * So, for viewNo=4 Nodes 1-3 select Node1 as the next Primary, and Node 4 - Node5.
 * View Change will not finish on Node4.
 ** INDY-2308 will help in finishing

*Possible fix:*
 Either revert uncommitted state before selecting new Primaries in the new view, or keep `node_reg_at_beginning_of_view` for committed txns only.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41is",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"25/Dec/19 11:20 PM;sergey.khoroshavin;On the first glance keeping `node_reg_at_beginning_of_view` for committed state looks like more clear solution. I'd prefer to stick with it unless it causes some unexpected problems.;;;","26/Dec/19 9:02 PM;ashcherbakov;Changes:
 * Keep `node_reg_at_beginning_of_view` for committed txns only so that committed state is used when selecting primaries in a new view

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1447]

Tests:
 * test_view_change_add_one_node_uncommitted_by_next_primary
 * test_update_node_reg_at_beginning_of_view_updated_on_commit_only

 ;;;","14/Jan/20 12:37 AM;ashcherbakov;System tests are passing so far. Will keep an eye on this in the scope of INDY-2322 and INDY-2320.;;;",,,,,,,,,,,,,,,,,,,
A lagging node may use wrong N and F quorum values and never finish view change if there are NODE txns being processed,INDY-2320,43802,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,27/Dec/19 9:49 PM,17/Jan/20 10:11 PM,28/Oct/23 2:47 AM,17/Jan/20 10:11 PM,,,1.12.2,,,,,0,,,,,"*Issue*
 * Pool of 4 Nodes, viewNo=0
 * Adding Node5 and demoting Node4
 * Node 5 is committed on all nodes, Instance Change to view=1 is sent
 ** => N=5, F=1 on all nodes
 * Before view change is started, Node4 demotion is also committed, but on Nodes2, 3 and 5 only
 ** Node1: N=5, F=1
 ** Nodes2, 3 ,5: N=4, F=1
 * View Change to view=1 is started on all nodes
 * Nodes 2, 3, 5 finish View change using N-f=3 View Change messages in a NewView
 * Node1 receives NewView from the  Node2 and tries to verify it.
 As Node1's N=5, it expects N-F=4 ViewChange messages, while only 3 ViewChanges are there
 * So, Node1 can not finish View Change

*Possible Options for Fixes:*
 * Enhance changes done in the scope of INDY-2308 to finish View Change if a Node sees a quorum of equal NewView messages (not necessary verifiable).
 * Change the logic how and when we calculate N and F
 ** For example, update N and F not on every committed Node txn, but at the beginning of new View only.
 * Enhance Instance Change triggering logic to include N value into InstanceChanges, and start view change only if Nodes' N is equal to the one in the quorum of Instance Changes.
 * Do not allow 3PC batches to Pool Ledger with more than 1 txn
 * Allow only one NODE txn per view with adding/demoting
 * Start View Change immediately (without waiting for a quorum of Instance Changes) once a node is added or promoted.
 ** It may make sense to send Instance Change as well to propagate view change to lagged nodes.

 ",,,,,,,,,,,,,,,,,,,,,INDY-2308,,,INDY-2322,INDY-2326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41isk",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"10/Jan/20 1:49 AM;ashcherbakov;PoA:
 # Write integration test
 # Extend fixes in the scope of INDY-2308 to
 ** Request NEW_VIEW from other nodes if a NEW_VIEW received by a Primary is not valid (not valid because of lack of sufficient number of View Change messages)
 ** Accept any NEW_VIEW (even non-valid from this node point of view) if we have a quorum;;;","16/Jan/20 11:12 PM;ashcherbakov;See comments in INDY-2322;;;","17/Jan/20 10:11 PM;ashcherbakov;Validation will be done in the scope of INDY-2326;;;",,,,,,,,,,,,,,,,,,,
Persistent pool failed to upgrade to 1.1.65 sovrin RC,INDY-2321,43803,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Won't Do,sergey.khoroshavin,VladimirWork,VladimirWork,27/Dec/19 11:06 PM,28/Dec/19 12:07 AM,28/Oct/23 2:47 AM,28/Dec/19 12:06 AM,,,,,,,,0,,,,,"Build Info:
sovrin 1.1.62

Steps to Reproduce:
1. Upgrade pool using pool upgrade command *without* force.

Actual Results:
Pool upgrades but has no consensus after upgrade and different views on different nodes.

Expected Results:
Pool must have consensus after upgrade and the same view on all nodes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41isl",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,VladimirWork,,,,,,,,,,"28/Dec/19 12:00 AM;sergey.khoroshavin;It turned out that changes between indy-node 1.12.1 and 1.12.0 not entirely compatible. Optimizations made in scope INDY-2280 led incompatibility in COMMIT messages, so old nodes were discarding COMMIT messages from upgraded nodes. That was the reason why rolling upgrade from 1.12.0 to 1.12.1 failed. It means that *upgrade from 1.12.0 to 1.12.1 should be forced*.;;;",,,,,,,,,,,,,,,,,,,,,
A lagging node may be the only one who started view change in case of F Nodes added/promoted in 1 batch,INDY-2322,43805,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,28/Dec/19 1:20 AM,23/Jan/20 12:05 AM,28/Oct/23 2:47 AM,17/Jan/20 10:11 PM,,,1.12.2,,,,,0,,,,,"*Issue:*
 * Pool of 4 Nodes
 * Adding Node5
 * Node5 is committed on Nodes1-3 (so their N=5 now), but not committed yet on Node4 (so N is still 4 there)
 * Nodes 1-3 sending InstanceChange as they added a new Node
 ** No quorum yet for Nodes1-3 (they expect N-F=4 Instance Changes)
 ** Quorum for Node4 (N-F=3 for Node4)
 * So, Node4 only started the view change
 * Other nodes will not start it since Node4 hasn't ordered NODE txn for Node5 yet so it hasn't sent Instance Change

*Possible Options for Fixes:*
 * O1: Change the logic how and when we calculate N and F
 ** For example, update N and F not on every committed Node txn, but at the beginning of new View only.
 * O2: Enhance Instance Change triggering logic to include N value into InstanceChanges, and start view change only if Nodes' N is equal to the one in the quorum of Instance Changes.
 * O3: Do not allow 3PC batches to Pool Ledger with more than 1 txn
 * O4: Allow only one NODE txn per view with adding/demoting
 * O5: Start View Change immediately (without waiting for a quorum of Instance Changes) once a node is added or promoted.
 ** It may make sense to send Instance Change as well to propagate view change to lagged nodes.",,,,,,,,,,,,,,,,,,,,,INDY-2320,INDY-2308,,INDY-2326,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41isj",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"10/Jan/20 1:06 AM;ashcherbakov;PoA:

Phase 1:
 * Write integration tests
 * Change the logic of how and when we calculate N and F
** Send `MasterReorderedAfterVC` after the first batch in a new view is orderded
 ** Move the logic to change N and F and adjust replicas to `MasterReorderedAfterVC` processing 
 *** Reset monitor at this point as well 
 ** Select master primary only at the start of view change; backup primaries will be set when re-ordering phase is finished 
 ** Update `data.primaries` after re-ordering is finished and calculate primaries for backups 
 ** Introduce active node reg as the node reg at the beginning of the current view 
 ** Create Primaries Batch Handlers to be set for audit 
 ** Do a view change if selected master primary turned out to be demoted after re-ordering is finished 
 ** Recover N, F, number of replicas and primaries correctly after catchup using `primaries` and `node_reg_at_beginning_of_view[current_view]` 
 ** Stash (with a new stash code) all 3PC messages with ppSeqNo > prep_cert +1 until last_ordered <= prep_cert+1, so that we don't order anything in a new view with not yet updated N and F.
 Unstash these messages once N and F are updated in `MasterReorderedAfterVC` 
 ** [Optional] Introduce a timeout for the first batch after re-ordering so that malicious primary in a new view really starts ordering
 ** [Optional] Do not do view change in view change service for backup instances as they don't do real view change (exchange ViewChange messages), and backups start ordering after master finished re-ordering only. 
 * Update docs

Phase 2:
 * Re-factor to move the code related to view change triggering on nodes count change to a node-independent place (NodeRegHandler for example) to be able to run it in simulation tests
 ** This can be done in the scope of INDY-2324;;;","16/Jan/20 11:11 PM;ashcherbakov;[Link to slides|https://docs.google.com/presentation/d/1l7_D3iActhLBMwzoFAH3xWUEGmlH-2PRaLKrNH2_KKg/edit#slide=id.g76776eb639_0_98]

*PR*
- https://github.com/hyperledger/indy-plenum/pull/1456

*Issue*
- Issue 1: A lagging node may be the only one who started view change in case of F Nodes added/promoted in 1 batch
- Issue 2: A lagging node may use wrong N and F quorum values and never finish view change if there are NODE txns being processed

*Issue Reason*
- N and F are re-calculated right after a Node committed NODE txn changing F.
- As Commit is not synchronous operation, it’s not guaranteed that all nodes will have the same N and F values during view change
-- Not equal quorums for INSTANCE_CHANGE 
-- Not enough votes in VIEW_CHANGE and NEW_VIEW messages
-- Different number of replicas and primaries on replicas (as it depends on N and node registry) including primaries in audit ledger

*Fix*
- Do not change N and F right after NODE txn is committed.
- Change it at the end of the View Change instead (which is triggered every time number of nodes is changed)
- Calculate Primaries for backups at the end of the View Change only
- End of view change = when the first txn in a new view is committed (so that Primaries are written to the audit ledger correctly for catch-up)
- N, F and backup Primaries are calculated against the node registry as it was at the beginning of (new) view 
- Do not allow processing of 3PC messages in a view until the first txn in the view is committed

*Implementation*
- Fixes in NodeRegHandler (added active_node_reg)
- Fixes in PrimarySelector (correct backup primary selector)
- Added PrimaryBatchHandler (correctly update primaries in audit ledger)
- Fixes in Node (adjust N, F and select Backup Primaries when view change is finished or after catch-up)
- Fixes in OrderingService and OrderingServiceMsgValidator (do not order in a view until the first batch is committed)
- Fixes in ViewChangeService (do not select backup primaries at the beginning of view change)

*Tests*
- test_node_reg_handler.py
- test_round_robin_node_reg_primary_selector.py
- test_add_node_delay_commit_on_one.py
- test_order_after_demote_and_restart
- test_view_change_while_adding_new_node_1_slow_commit.py
- test_view_change_while_adding_new_node_1_slow_preprepare.py
- test_view_change_while_adding_new_node_2_slow_commit.py
- Other tests related to add/promote/demote of nodes

*Build*
TBD

*Risk*
- Medium

*Recommendation for QA:*
- Make sure all system tests pass
- Run load tests
-- restart nodes; 
-- demote/promote nodes
- Write more tests related to demotion/promotion/adding new nodes if needed
;;;","17/Jan/20 10:10 PM;ashcherbakov;Validation will be done in the scope of INDY-2326;;;",,,,,,,,,,,,,,,,,,,
Improve load testing with demotions and promotions,INDY-2323,43806,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,28/Dec/19 10:27 PM,24/Jan/20 1:40 AM,28/Oct/23 2:47 AM,24/Jan/20 1:40 AM,,,,,,,,0,,,,,We should extend load script with demotion and promotion operations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismni",,,,Unset,Unset,Ev-Node 20.01,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,"24/Jan/20 1:40 AM;VladimirWork;Implemented, verified and merged into master.;;;",,,,,,,,,,,,,,,,,,,,,
Improve node promotion/demotion simulation tests,INDY-2324,43860,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,08/Jan/20 10:45 PM,23/Jan/20 11:30 PM,28/Oct/23 2:47 AM,17/Jan/20 11:18 PM,,,,,,,,0,,,,,"*Rationale*
We discovered a bunch of edge cases related to promotion/demotion combined with view change (for example INDY-2308, INDY-2320, INDY-2322). Despite that we have a plan of attack for fixing these problems it is still quite alarming that these cases were caught by intermittent failures of system-level tests, and were completely uncaught by simulation tests. In order to be sure that these problems are really fixed (and improve general confidence in current implementation) we need to make sure these edge cases are reliably caught in simulation tests.

*Acceptance criteria*
Make sure these edge cases are captured in simulation tests. If improved simulation tests uncover more edge cases that cannot be fixed fast - create follow-up tickets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismk",,,,Unset,Unset,Ev-Node 19.26,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"08/Jan/20 10:52 PM;sergey.khoroshavin;*PoA*
* Promotion/demotion related logic should be available in simulation tests - Alex
** There should be single source of truth for nodeRegs (both current and at the start of view) and Quorums (N, F)
*** Most probably it should be in ConsensusSharedData
*** It should be updated by appropriate req or batch handlers or services, available in simulation tests
*** Legacy code containing copies of this data should be refactored to make it a read-only properties accessing single source of truth (if it is in ConsensusSharedData then one from master instance should be used)
** Code starting a view change should be placed in service available in simulation tests
* View Changer improvements - Sergey Kh.
* SimNetwork should be able to provide message delaying functionality similar to one available in integration tests - Sergey Kh.
** When ""sending"" message process it using list of processors
** Processors can added or removed from this list ""on the fly""
** Last added processors are used first
** Each processor consists of selector and action
** Selector is a function indicating whether an action should applied for this message (very much like current delayers)
** Action can be one of
*** Discard(probability) - discard message with some probability (if not discarded handle it by next processor)
*** Deliver(min_delay,max_delay) - schedule message for delivery
*** Stash - stash message into some storage, reapply all processors if stashing processor is removed from list
* Following generic test case should be implemented - Andrew N.
** Create pool with random number of nodes and random initial view_no (and hence random primary)
** Schedule some random transactions over period of time
** Schedule some promotion/demotion transaction during this period
** Schedule some view changes during this period
** Make sure all transactions are ordered and nodes are in the same state eventually
** Make sure that this test fails before fixes are merged
** *NOTE* We're not going to actually start and stop nodes in this test, and amount of promotion/demotion transaction should be limited so that pool won't become nonfunctional without new nodes. Real starting/stopping nodes should be done in scope of INDY-2148;;;","17/Jan/20 10:34 PM;ashcherbakov;The first item will be done in the scope of INDY-2329;;;","17/Jan/20 11:18 PM;ashcherbakov;Item1 - TBD in INDY-2329
Item 2 - https://github.com/hyperledger/indy-plenum/pull/1454, https://github.com/hyperledger/indy-plenum/pull/1457
Item 3 - https://github.com/hyperledger/indy-plenum/pull/1459
Item 4 - https://github.com/hyperledger/indy-plenum/pull/1453, https://github.com/hyperledger/indy-plenum/pull/1455

Item 1 and merging of PRs will be done in the scope of INDY-2329;;;",,,,,,,,,,,,,,,,,,,
Release 1.12.2,INDY-2325,43967,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Jan/20 10:09 PM,30/Jan/20 8:34 PM,28/Oct/23 2:47 AM,30/Jan/20 8:34 PM,,,1.12.2,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismn",,,,Unset,Unset,Ev-Node 20.01,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,"30/Jan/20 1:40 AM;VladimirWork;*Version Information*
indy-node 1.12.2
indy-plenum 1.12.2
sovrin 1.1.71

*Notices for Stewards*
Stop indy-node service on demoted nodes to avoid a minor issue with client's requests processing (see Known Issues for details).
There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.

*Major Changes*
- Stability fixes

*Detailed Changelog*

+Major Fixes+
INDY-2221 - WARNING messages incorrectly logged if tokens are not used
INDY-2314 - REV_REG_DEF `tag` field is not validated
INDY-2318 - A node may re-send messages in a loop in case of connection issues
INDY-2319 - Up to F Nodes may not be able to finish View Change if there are uncommitted NODE txns
INDY-2308 - A node lagging behind may not be able to finish view change if nodes have been added/demoted
INDY-2320 - A lagging node may use wrong N and F quorum values and never finish view change if there are NODE txns being processed
INDY-2322 - A lagging node may be the only one who started view change in case of F Nodes added/promoted in 1 batch
INDY-2326 - Debug View Change when nodes added/demoted/promoted

+Changes and Additions+
-

+Known Issues+
INDY-2334 - Demoted Node should not process client's requests;;;",,,,,,,,,,,,,,,,,,,,,
Debug View Change when nodes added/demoted/promoted,INDY-2326,43968,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Jan/20 10:11 PM,05/Feb/20 2:29 AM,28/Oct/23 2:47 AM,30/Jan/20 9:32 PM,,,1.12.2,,,,,0,,,,,"Follow-up of debug and stabilization for INDY-2308, INDY-2322, INDY-2320.

*Acceptance criteria*
- Make sure all PRs are merged
-- https://github.com/hyperledger/indy-plenum/pull/1456 DONE
-- PR for follow-up fixes in INDY-2308 DONE
-- PR with simulation tests https://github.com/hyperledger/indy-plenum/pull/1455 [~anikitinDSR]
- Make sure simulation tests pass (probably after INDY-2329 is done) [~anikitinDSR]
- Write more tests related to demotion, promotion and adding of nodes [~anikitinDSR]
- Make sure all system tests pass [~VladimirWork]
- Make sure load tests with random promotions, demotion and restarts pass [~VladimirWork]
",,,,,,,,,,,,,,,,,,,,,INDY-2308,INDY-2322,INDY-2320,,,,,,,,"28/Jan/20 5:49 PM;VladimirWork;figure1-1.png;https://jira.hyperledger.org/secure/attachment/18113/figure1-1.png","28/Jan/20 5:32 PM;VladimirWork;figure1.png;https://jira.hyperledger.org/secure/attachment/18109/figure1.png","28/Jan/20 5:49 PM;VladimirWork;figure10-1.png;https://jira.hyperledger.org/secure/attachment/18112/figure10-1.png","28/Jan/20 5:32 PM;VladimirWork;figure10.png;https://jira.hyperledger.org/secure/attachment/18108/figure10.png","29/Jan/20 9:29 PM;VladimirWork;figure15_29.png;https://jira.hyperledger.org/secure/attachment/18118/figure15_29.png","29/Jan/20 9:29 PM;VladimirWork;figure1_29.png;https://jira.hyperledger.org/secure/attachment/18119/figure1_29.png","28/Jan/20 5:49 PM;VladimirWork;figure20-1.png;https://jira.hyperledger.org/secure/attachment/18111/figure20-1.png","28/Jan/20 5:32 PM;VladimirWork;figure20.png;https://jira.hyperledger.org/secure/attachment/18107/figure20.png","29/Jan/20 9:29 PM;VladimirWork;figure25_29.png;https://jira.hyperledger.org/secure/attachment/18117/figure25_29.png","28/Jan/20 5:32 PM;VladimirWork;report_27_01_2020.png;https://jira.hyperledger.org/secure/attachment/18110/report_27_01_2020.png","28/Jan/20 5:49 PM;VladimirWork;report_28_01_2020.png;https://jira.hyperledger.org/secure/attachment/18114/report_28_01_2020.png","29/Jan/20 9:29 PM;VladimirWork;report_29_01_2020.png;https://jira.hyperledger.org/secure/attachment/18120/report_29_01_2020.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismm",,,,Unset,Unset,Ev-Node 20.01,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,"28/Jan/20 5:32 PM;VladimirWork;Load test results with demotions/promotions once in 400 seconds (200 seconds promotion shift): !figure20.png|thumbnail!  !figure10.png|thumbnail!  !figure1.png|thumbnail!  !report_27_01_2020.png|thumbnail! ;;;","28/Jan/20 5:49 PM;VladimirWork;Load test results with demotions/promotions once in 4000 seconds (2000 seconds promotion shift): !figure20-1.png|thumbnail!  !figure10-1.png|thumbnail!  !figure1-1.png|thumbnail!  !report_28_01_2020.png|thumbnail! ;;;","29/Jan/20 9:29 PM;VladimirWork;Load test results with demotions/promotions once in 1800 seconds (180 seconds promotion shift) with 1/10 of production load rate: !figure25_29.png|thumbnail!  !figure15_29.png|thumbnail!  !figure1_29.png|thumbnail!  !report_29_01_2020.png|thumbnail! ;;;","30/Jan/20 6:41 PM;anikitinDSR;Comments, regarding to 
{code:java}
Load test results with demotions/promotions once in 400 seconds (200 seconds promotion shift)`{code}
 * there weren't any problems with view change procedure. All of VC rounds were finished
 * problems with backup ordering were detected and it caused to failing by memory error
 * all nodes that had view no == 0 were not promoted because of test specific (random demotion and random promotion)
 * was created the ticket INDY-2334 regarding request processing on demoted node;;;","30/Jan/20 6:48 PM;anikitinDSR;Comment, regarding to
{code:java}
Load test results with demotions/promotions once in 4000 seconds (2000 seconds promotion shift{code}
 * there was the same problem with slowly ordering on backups that caused to request queue increasing and memory failure then
 * all nodes that had view no == 0 were not promoted because of test specific (random demotion and random promotion)
 * also, 2000 seconds under load means, that demoted node can not to finish catchup process after promotion because it needs to catchup a lot of requests. Some of nodes was in this state. They didn't order but always tried to catchup.

 ;;;","30/Jan/20 7:00 PM;anikitinDSR;Comments, regarding to
{code:java}
Load test results with demotions/promotions once in 1800 seconds (180 seconds promotion shift) with 1/10 of production load rate
{code}
 * this test looks much better. We had predictable circles of demotion/promotion
 * there was a problem with slow backup ordering, but it was not too critical as in previous tests
 * as in previous tests, nodes with view_no == 0 was demoted but not promoted back.

 ;;;",,,,,,,,,,,,,,,,
Support the W3C standard for Verifiable Credentials and Rich Schemas,INDY-2327,43969,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,14/Jan/20 11:23 PM,01/Oct/20 8:27 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"The W3C standard for Verifiable Credentials defines a richer approach to schemas than is currently supported in Indy Node. The approach is described in this RFC:
https://github.com/hyperledger/aries-rfcs/tree/master/features/0281-rich-schemas

This requires a number of new objects being added to the Indy ledger:
* Context: https://github.com/hyperledger/aries-rfcs/tree/master/features/0249-rich-schema-contexts
* Rich Schema
* Mapping
* Encoding
* Presentations

*Notes*
* The work is being led by Ken Ebert at the Sovrin Foundation and @Brent Zundel (Evernym).
* Overview slide deck: https://docs.google.com/presentation/d/1IYLWgAJM6pMYIuGLFyf7WmErNuBwGapOCs8ohLUhGD8/edit#slide=id.g4227fd63e5_0_225",,,,,,,,,,,,,,,,,,,,,IS-1402,IS-1545,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-9,,W3C Rich Schemas,To Do,No,,Unset,No,,,"1|i017mj:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Formalize the architectural plan for Rich Schemas,INDY-2328,43970,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,esplinr,esplinr,14/Jan/20 11:25 PM,30/Jan/20 1:49 AM,28/Oct/23 2:47 AM,30/Jan/20 1:49 AM,,,,,,,,0,,,,,"We need to formalize the architecture plan for Rich Schemas so that we understand how to partition the work so that additional people can accelerate the delivery.

Acceptance Criteria
* Investigate the W3C VC standard
* Review the existing the work in progress
* Create a Plan of Attack (POA) for the remaining work to bring our implementation inline with the standard.
** Define the relationships between the Rich Schema objects
** Should include the work necessary to implement in Indy SDK / LibIndy
* Create the relevant issues.

*Notes*
* The work necessary to support Rich Schemas in LibVCX will be tracked separately.
* The work necessary to support Rich Schemas in Aries Shared Libraries will be tracked separately.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismp",,,,Unset,Unset,Ev-Node 20.01,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,"27/Jan/20 6:24 PM;ashcherbakov;Existing work done / in progress:
 * 3 HIPEs are merged:
 ** [https://github.com/hyperledger/indy-hipe/tree/master/text/0119-rich-schemas]
 ** [https://github.com/hyperledger/indy-hipe/tree/master/text/0138-rich-schema-context]
 ** [https://github.com/hyperledger/indy-hipe/tree/master/text/0149-rich-schema-schema]
 * 2 PRs to update the HIPEs are sent:
 ** [https://github.com/hyperledger/indy-hipe/pull/151]
 ** [https://github.com/hyperledger/indy-hipe/pull/152]
 * 3 Aries RFCs are merged:
 ** [https://github.com/hyperledger/aries-rfcs/tree/master/concepts/0250-rich-schemas]
 ** [https://github.com/hyperledger/aries-rfcs/tree/master/features/0249-rich-schema-contexts]
 ** [https://github.com/hyperledger/aries-rfcs/tree/master/features/0281-rich-schemas]
 * 1 PR with implementation of Context object in indy-node is merged:
 ** [https://github.com/hyperledger/indy-node/pull/1430]
 * 1 PR with implementation of Rich Schema object is in progress:
 ** [https://github.com/hyperledger/indy-node/pull/1513]
 
What has been done in the scope of the this task:
 * [https://github.com/hyperledger/indy-hipe/pull/151] and [https://github.com/hyperledger/indy-hipe/pull/152] are reviewed, left comments
 * Created [https://github.com/hyperledger/aries-rfcs/issues/398] with questions and items to be defined regarding Rich Schema objects;;;","30/Jan/20 1:49 AM;ashcherbakov;Next Steps:
 # INDY-2336: Design identification and relationship between Rich Schema objects to resolve questions from [https://github.com/hyperledger/aries-rfcs/issues/398]
 # INDY-2337: Design all individual Rich Schema objects (HIPEs/RFCs)
 # INDY-2338: Finish implementation of Context and Rich Schema
 # INDY-2339: Implement Encoding 
 # INDY-2340: Implement Mapping
 # INDY-2341: Implement Credential Definition
 # INDY-2342: Implement Presentation Definition;;;",,,,,,,,,,,,,,,,,,,,
Allow view change triggering on node count changed in sim tests,INDY-2329,44016,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,17/Jan/20 10:32 PM,24/Jan/20 8:39 PM,28/Oct/23 2:47 AM,24/Jan/20 7:50 PM,,,1.12.2,,,,,0,,,,,"Acceptance criteria:
- Move logic of view change triggering from node.py to service- and bus-based places so that it can be triggered from sim tests
- Make sure https://github.com/hyperledger/indy-plenum/pull/1455 is merged and passes
- [Optionally] Re-factor PoolManager to get rid of multiple places for node reg
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismmi",,,,Unset,Unset,Ev-Node 20.01,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"24/Jan/20 8:39 PM;anikitinDSR;All the changes was merged in PR for INDY-2326.
PRs related to INDY-2326:
 * [https://github.com/hyperledger/indy-plenum/pull/1465]
 * https://github.com/hyperledger/indy-plenum/pull/1464;;;",,,,,,,,,,,,,,,,,,,,,
Create tool to test ZMQ connectivity to Validators,INDY-2330,43995,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,mgbailey,mgbailey,15/Jan/20 6:48 AM,27/Feb/20 7:44 AM,28/Oct/23 2:47 AM,17/Feb/20 11:43 PM,,,,,,,,0,EV-CS,,,,"Create a simple binary tool that we can put on an Ubuntu VM that will tell us if zmq traffic is being allowed to pass to a destination. More specifically, this would be for an administrator who is making a new agent or diagnosing an existing one, to be able to tell if a firewall is preventing contact with Validators due to deep-packet-inspection (DPI) blocking ZMQ traffic. By having control of both sides of the connection, we can pair the tool with other network diagnostic techniques to identify the source of connection errors.

*Acceptance Criteria*
* Command line tool exists that can be run to test that ZMQ traffic can be passed.
* The tool is run on both sides of a connection (a machine on the client network, and the support representative's machine).
* Network traffic generated by the tool mimics Indy Node traffic in order to diagnose behavior of DPI firewalls.
* Command line tool runs in Ubuntu 18.04.
* Installation and execution should be as simple as possible, ideally just copying the binary over and executing it from the command line. (No dependencies would be ideal.)

*Notes*
* It would be undesirable to have this baked into the indy-node library, since the diagnostic will be of use to agents as well as to new validators.",,,,,,,,,,,,,,,,,,,,,,,,INDY-2345,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1381,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqd",,,,Unset,Unset,Ev-Node 20.2,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,mgbailey,,,,,,,,,"12/Feb/20 6:42 PM;anikitinDSR;h1. PoA.

According to acceptance criteria we should create a tool which can provide client and server functionality as well. Also, for diagnostic purposes it should check tcp and zmq functionality.
h2. Client.

As a client it should be able to create connection through tcp and zmq sockets and send/receive diagnostic message which emulates Node traffic.
h2. Server.

As server we propose to use current ZStack implementation and add a simple tcp check. Current ZStack will be copied (it's enough independent) as is and combined with client's part.;;;","17/Feb/20 7:15 PM;anikitinDSR;PRs with script:
 * [https://github.com/hyperledger/indy-plenum/pull/1471]
 * [https://github.com/hyperledger/indy-plenum/pull/1472]

Documentation and HowToUse:
 * [https://github.com/hyperledger/indy-plenum/blob/master/scripts/test_zmq/README.md];;;","18/Feb/20 8:01 AM;mgbailey;[~anikitinDSR], will the client script work if tested against the IP address and port of a running validator as the server? Or must it be tested against the {{check_zmq}} server only?;;;","18/Feb/20 5:47 PM;anikitinDSR;For already run validator's nodes it's not useful, because it's not use genesis files. For checking you have to run this script on both side as client and server. I think, we can discuss support of currently run validators if it's needed.;;;","27/Feb/20 7:20 AM;ashcherbakov;- Client-side: fat binary; write in Rust for example
- A mode to run against a Validator node - main use case
- Server-side: requirement Ubuntu18.04 (not necessary a fat binary; Docker would be great); would be great to run as a servie
;;;",,,,,,,,,,,,,,,,,
Node troubleshooting checklist,INDY-2331,44017,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,esplinr,esplinr,18/Jan/20 12:08 AM,20/Feb/20 6:50 PM,28/Oct/23 2:47 AM,20/Feb/20 6:50 PM,,,,,,,,0,,,,,"We need documentation to help other people to maintain an Indy network.

*Acceptance Criteria*
Best practices for troubleshooting a node are documented.
* What logs are useful
* What tools exist for analyzing a node",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqe",,,,Unset,Unset,Ev-Node 20.2,Ev-Node 20.03,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,sergey.khoroshavin,,,,,,,,,,"12/Feb/20 7:12 PM;sergey.khoroshavin;PoA:
* Create a document in indy-node repo with following outline:
** General info
*** what types of failures are possible, and what actions they might require
*** where to get information about node state (validator info, journalctl, logs, metrics)
*** what tools and techniques can be used for analysis
** Checklist on diagnosing problems
** Checklist on fixing problems (probably can be merged with diagnosing checklist);;;","13/Feb/20 11:22 PM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-node/pull/1582;;;",,,,,,,,,,,,,,,,,,,,
ujson 1.3.3 does not handle large reqId's,INDY-2332,44109,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,lbendixsen,lbendixsen,23/Jan/20 8:28 AM,23/Jan/20 8:28 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"_Environment_:  Development environment using latest master for Indy-node and indy-plenum (~1.12.2)

_Steps to Reproduce_: 

Create a a custom txn with a long ReqId and send it to the ledger with indy-sdk ""signandsend"" method. 

_Expected Behavior_: 

ledger should handle it. 

_Observed Behavior_: 

ledger sometimes returns ""Value is too big"" as shown below but all the time the master node hangs and a timeout is returned.

_Notes_:

I recommend checking the ReqID length (even 1.3.5 will break with sufficiently long length) and moving to ujson 1.3.5 as indicated by the following output sequences.
 * 
 {{$ pip install ujson==1.33}}
{{Collecting ujson==1.33}}
{{  Downloading https://files.pythonhosted.org/packages/98/8c/47949a4e454b7ee27cf82a81735414187368fb94bae94de0291903c9881b/ujson-1.33.zip (197kB)}}
{{     100% |████████████████████████████████| 204kB 1.9MB/s}}
{{Installing collected packages: ujson}}
{{   Running setup.py install for ujson ... done}}
{{Successfully installed ujson-1.33}}
{{You are using pip version 9.0.1, however version 20.0.1 is available.}}
{{You should consider upgrading via the 'pip install --upgrade pip' command.}}
{{}}
{{$ python}}
{{Python 3.5.7 (default, Jan 22 2020, 12:19:18)}}
{{[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux}}
{{Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.}}
{{>>> msg = '\{""reqId"":9486768349997817835}'}}
{{>>> import ujson}}
{{>>> ujson.loads(msg)}}
{{Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>}}
{{ValueError: Value is too big}}
{{>>>}}
{{}}
 * 
 {{$ pip install ujson==1.35}}
{{Collecting ujson==1.35}}
{{   Using cached https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz}}
{{Installing collected packages: ujson}}
{{  Running setup.py install for ujson ... done}}
{{Successfully installed ujson-1.35}}
{{You are using pip version 9.0.1, however version 20.0.1 is available.}}
{{You should consider upgrading via the 'pip install --upgrade pip' command.}}
{{}}
{{$ python}}
{{Python 3.5.7 (default, Jan 22 2020, 12:19:18)
[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux}}
{{Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.}}
{{>>> import ujson
>>> msg = '\{""reqId"":9486768349997817835}'}}
{{>>> ujson.loads(msg)}}
{{{'reqId': 9486768349997817835}}}
{{>>>}}

{{}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i0183n:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),lbendixsen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reordering after view change caused by adding new node.,INDY-2333,44143,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,anikitinDSR,anikitinDSR,27/Jan/20 7:50 PM,27/Jan/20 7:50 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"There is edge-case (hard reproducible) then lagged behind node have to reorder txn with adding new node which would be selected as new primary.

In this case, `old view preprepares` will be requested and during this process other pool will finish reordering (because they don't need to real ordering) and first batch in new view will be ordered too. For now, ReAppliedInNewView internal message will not be sent and lagged node will not unstash all the 3PC message received while requesting `old_view_preprepares`.

Also, there is a problem, that lagged node doesn't know new added node and will reject all messages from it and first PrePrepare in new view too.

Possible fixes:
 # Make ReAppliedInNewView logic more clear. For example, this action can be added directly after applying `prev_view_prepare_certificate`.
 # We need to add requesting preprepare logic for case if node received not the first batch in new view (the second, the third etc..) and didn't have PrePrepare for the first batch (it would be STASH_WAITING_FIRST_BATCH_IN_VIEW reason)

There is a test that can be used for debugging (need to turn off 2 checkpoints requests sending):

plenum/test/view_change/test_vc_with_incorrect_primary_in_promote.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i0189n:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Demoted Node should not process client's requests.,INDY-2334,44152,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,anikitinDSR,anikitinDSR,28/Jan/20 5:42 PM,28/Jan/20 5:42 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Node that was demoted by NODE transaction should not process client's requests.

For now, after demotion node will receive client's requests and try to send PROPAGATE message to the rest of nodes. It's not critical, because it doesn't affect them. But in general, other nodes will log all of the WARNINGs like ""receive message from unknown node"". Of course, we can't guarantee that demoted malicious node will not send PROPOGATE. But for ""normal"" node it can be helpful.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i018bn:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Turn off preprepare time check while reordering.,INDY-2335,44159,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,anikitinDSR,anikitinDSR,28/Jan/20 11:20 PM,28/Jan/20 11:20 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"After view change complete node has to reorder all of requests before last stable checkpoint. If node will do real reordering, then it can try to apply old PrePrepare (with PrePrepare time more then 10 minutes for now) and will raise suspicious and as result will stop reordering and order process in current view too. To avoid this situation we should turn off time checking while reordering after view change.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i018cj:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Design identification and relationship between Rich Schema objects,INDY-2336,44171,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/20 12:35 AM,13/Feb/20 10:56 PM,28/Oct/23 2:47 AM,13/Feb/20 10:31 PM,,,,,,,,0,,,,,Write a HIPE and RFC answering the questions from https://github.com/hyperledger/aries-rfcs/issues/398,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqe4",,,,Unset,Unset,Ev-Node 20.2,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"11/Feb/20 11:44 PM;ashcherbakov;PRs:
 * Indy HIPE PR: [https://github.com/hyperledger/indy-hipe/pull/153]
 * Aries RFC PR: [https://github.com/hyperledger/aries-rfcs/pull/420]

The design defines and proposes the following:
 * Identification scheme (as DID)
 ** We don't have 100% consensus in community about this, but this looks not critical for the first phase of implementation.
 * Reference scheme and relationship diagram
 * Common pattern and structure

Next steps:
 * Continue implementation on the Ledger side (unified approach proposed in the HIPEs allows to finish it pretty quickly): INDY-2338
 ** Some specific validation may be skipped for the first phase
 * Wait until all Rich Schema objects are designed and examples are provided: INDY-2337 (it will be done by [~brentzundel] and [~kenebert] ).
 * Start implementation on Indy SDK side (aries-indy-vdri most probably)
 * Resolve unresolved questions, merge HIPEs and RFCs;;;",,,,,,,,,,,,,,,,,,,,,
"Design Schema, Context, Encoding, Mapping, CredDef",INDY-2337,44172,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/20 12:45 AM,17/Mar/20 1:38 AM,28/Oct/23 2:47 AM,13/Mar/20 9:25 PM,,,,,,,,0,,,,,"Make sure there are examples of all Rich Schema objects.
 *Acceptance criteria*
 - Update existing Indy HIPEs and Aries RFCs
 ** Mapping and CredDefs use a single Schema only
 ** Update Schema, Context and Encoding HIPEs/RFCs to match the Common format of rich schema objects

 - Create Indy HIPEs and Aries RFCs for the following objects:
 -- Mapping
 -- Credential Definition",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixr",,,,Unset,Unset,Ev-Node 20.04,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"13/Mar/20 9:24 PM;ashcherbakov;PRs:
- https://github.com/hyperledger/indy-hipe/pull/156
- https://github.com/hyperledger/aries-rfcs/pull/446;;;",,,,,,,,,,,,,,,,,,,,,
Common implementation of all Rich Schema objects,INDY-2338,44173,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/20 12:57 AM,13/Mar/20 9:32 PM,28/Oct/23 2:47 AM,24/Feb/20 11:57 PM,,,1.13.0,,,,,0,,,,,"According to https://github.com/hyperledger/indy-hipe/pull/153, it should be possible to implement all Rich Schema objects in a generic way.
It may exclude some specific validation dependent on object type, but should be enough to continue efficient implementation on the client side. 

*Acceptance criteria:*
- create common code for dealing with all rich schema transactions as described in https://github.com/hyperledger/indy-hipe/pull/153
- specific object-related validation may not be implemented on this phase
- re-factor existing Rich Schema code to match the generic approach",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeir",,,,Unset,Unset,Ev-Node 20.03,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"20/Feb/20 9:38 PM;ashcherbakov;- All rich schema objects are implemented  according to https://github.com/hyperledger/indy-hipe/pull/153
- tests have been added/edited
- PR: https://github.com/hyperledger/indy-node/pull/1586

Future work:
- support specific format and validation for all rich schema objects once it's stabilized;;;",,,,,,,,,,,,,,,,,,,,,
Implement Encoding specific,INDY-2339,44174,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/20 12:58 AM,27/Mar/20 5:22 PM,28/Oct/23 2:47 AM,20/Mar/20 9:10 PM,,,1.13.0,,,,,0,,,,,"Implement Encoding-specific validation logic which is not covered by INDY-2338.

- Static Validation: Make sure that all required fields are present (see [Encoding HIPE|https://github.com/hyperledger/indy-hipe/blob/30d1155090170f9d17844d8fbdb1b8df7b871687/text/0154-rich-schema-encoding/README.md]
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixod",,,,Unset,Unset,Ev-Node 20.05,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"19/Mar/20 5:18 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/1591;;;","20/Mar/20 9:09 PM;ashcherbakov;Integration and unit tests are written.
A full validation will be done after the correspondiong changes are supported by indy-sdk side.;;;",,,,,,,,,,,,,,,,,,,,
Implement Mapping specific,INDY-2340,44175,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/20 12:58 AM,27/Mar/20 5:22 PM,28/Oct/23 2:47 AM,20/Mar/20 9:10 PM,,,1.13.0,,,,,0,,,,,"Implement Mapping-specific validation logic which is not covered by INDY-2338.

- Static Validation: Make sure that all required fields are present (see [Mapping HIPE|https://github.com/hyperledger/indy-hipe/blob/30d1155090170f9d17844d8fbdb1b8df7b871687/text/0155-rich-schema-mapping/README.md]
- Dynamic validation: points to an existing Encoding

Note: we may disable or modify the dynamic validation rules if we are going to expect pointing to Encodings from other Indy ledgers or from other types of ledgers (compatible to Aries). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixoe",,,,Unset,Unset,Ev-Node 20.05,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"19/Mar/20 5:18 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/1591;;;","20/Mar/20 9:09 PM;ashcherbakov;Integration and unit tests are written.
A full validation will be done after the correspondiong changes are supported by indy-sdk side.;;;",,,,,,,,,,,,,,,,,,,,
Implement Credential Definition specific,INDY-2341,44176,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/20 1:00 AM,27/Mar/20 5:22 PM,28/Oct/23 2:47 AM,20/Mar/20 9:10 PM,,,1.13.0,,,,,0,,,,,"Implement Encoding-specific validation logic which is not covered by INDY-2338. 
- Static Validation: Make sure that all required fields are present (see [CredDef HIPE|https://github.com/hyperledger/indy-hipe/blob/30d1155090170f9d17844d8fbdb1b8df7b871687/text/0156-rich-schema-cred-def/README.md]
- Dynamic validation: CredDef points to an existing Schema on the current Ledger
- Dynamic validation: CredDef points to an existing Mapping on the current Ledger

Note: we may disable or modify the dynamic validation rules if we are going to expect pointing to Schema or Mapping from other Indy ledgers or from other types of ledgers (compatible to Aries). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixog",,,,Unset,Unset,Ev-Node 20.05,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"19/Mar/20 5:18 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/1591;;;","20/Mar/20 9:10 PM;ashcherbakov;Integration and unit tests are written.
A full validation will be done after the correspondiong changes are supported by indy-sdk side.;;;",,,,,,,,,,,,,,,,,,,,
Implement Presentation Definition specific,INDY-2342,44177,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,30/Jan/20 1:00 AM,27/Mar/20 5:22 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,Implement Encoding-specific validation logic which is not covered by INDY-2338.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwx",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate IBC and Cosmos,INDY-2343,44359,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,17/Feb/20 4:50 PM,27/Feb/20 10:09 AM,28/Oct/23 2:47 AM,27/Feb/20 2:05 AM,,,,,,,,0,,,,,"Acceptance criteria:
* Research IBC protocol and implementation
* Check whether there are any issues there
* Check if implementation exists and how stable/mature is it
* Check if it can be applied to our use cases
* Google doc with report should be the output of the ticket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeix",,,,Unset,Unset,Ev-Node 20.03,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"25/Feb/20 11:18 PM;sergey.khoroshavin;*What is IBC and what it is good for*
* IBC is a generic reliable transport protocol between state machines. *It is not limited to blockchains*
* IBC doesn't require any low-level transport protocol, instead it relies on:
** Relayer processes, that can read and communicate state between machines
** Participating machines performing verification themselves, in order to withstand arbitrary behavior (including byzantine) of relayers. It means that in case of blockchains *all participants must implement IBC-related transactions on-chain*.
** Easy verification and non-repudiability of state, so that machines can trust data forwarded by relayers without direct communication with other machines, and without replicating full state of other machines. It means that in case of blockchains *all participants must allow light clients*
* IBC can be used to implement arbitrary cross-machine logic, including (according to authors [claims|https://github.com/cosmos/ics/blob/master/ibc/2_IBC_ARCHITECTURE.md]) cross-chain token transfer and sharding between different blockchains.

 *IBC implementation state*
* There is a growing number of RFC-like [documents|https://github.com/cosmos/ics/tree/master/spec] (called ICS - InterChain Standards) describing IBC protocol:
** They range from general concepts and interfaces to concrete implementations of interfaces (like light clients for solo machines or tendermint blockchains), as well as example applications on top of tendermint (token transfer and interchain accounts)
** All ICSs are in draft state currently, although there is already quite a lot of work put into them (first PRs date back to March 2019)
** However these ICSs are also compiled into fancy PDF stating that this is [IBC spec 1.0.0-rc5|https://github.com/cosmos/ics/blob/master/spec.pdf] (as of Feb 25, 2020), so probably they are nearing some kind of release
* There is a [pinned issue|https://github.com/cosmos/ics/issues/42] to formalize IBC security properties in some proof assistant, targeting version 1.1. This is both bad (they don't have auto-verifiable formal proofs yet) and good (they are at least seriously planning to get that level of security).
* Cosmos SDK has separate actively developed branch [ics-alpha|https://github.com/cosmos/cosmos-sdk/tree/ibc-alpha] with IBC implementation
;;;","27/Feb/20 2:01 AM;sergey.khoroshavin;A bit more info.

Pretty good introduction to IBC can be found either here: [https://github.com/cosmos/ics/tree/master/ibc] (which is also a repository where IBC [specs|https://github.com/cosmos/ics/tree/master/spec] live) or here: [https://cosmos.network/intro] (actually this is a generic Cosmos ecosystem overview, but it contains a pretty good section on IBC).

State of IBC in Cosmos roadmap can be found here: [https://cosmos.network/roadmap]

*Main conclusions*
 * IBC spec is still in draft state, and implementation is not in Cosmos SDK master yet
 * IBC technology looks very promising for Indy use cases:
 ** can be used to implement cross-chain token transfer
 ** can be used to implement cross-chain protocols (including fee payments)
 ** can be used to implement blockchain sharding for better scalability
 ** is not limited to blockchains, and can be applied to any state-machine, given some prerequisites
 ** it looks like it may make sense to implement [Peer DIDs|https://openssi.github.io/peer-did-method-spec/index.html] on top of IBC
 * *As of now IBC cannot be used to connect Tendermint and Stellar pools directly* without heavy modification of Stellar
 ** Stellar needs to implement IBC-related low-level transactions
 ** Stellar needs to support light client for its own state
 * Some articles mention possibility of so-called ""peg-zones"" which are special adaptor blockchains built to communicate with incompatible blockchains, so *it might be possible to connect Tendermint and Stellar pools through custom-built peg zone*, however:
 ** there are no mentions of peg zones in current specifications
 ** peg zone validators must be reputable enough so that clients actually trust them enough to use it
 ** implementing peg zone for Stellar looks like quite a big undertaking:
 *** as of now only adaptor to Etherium has some implementation, and it looks quite complex
 *** there are some articles mentioning that Etherium adaptor is relatively easy, since Etherium already has light client support and even EVM to run some smart contracts on-chain, and that implementing similar adaptor for Bitcoin would be much more complex task
 *** Stellar doesn't have light client support and smart contracts, and on top of that its state is much more complex than Bitcoin (~15 different transaction types vs just UTXOs), so most probably implementing adaptor for Stellar it is even more complex;;;","27/Feb/20 10:09 AM;sergey.khoroshavin;Also I think it is worth separately mentioning possibility of using IBC as a basis for [Peer DIDs|https://openssi.github.io/peer-did-method-spec/index.html], or at least using at as some inspiration. Thoughts which led to that idea are following:
* Peer DIDs introduce idea of actors having some state, which is changed through a sequence of applied deltas, and even mention microledger term. At the same time IBC allow working with any state machine, and even have a specification for light client of a solo machine maintaining transaction log: https://github.com/cosmos/ics/tree/master/spec/ics-006-solo-machine-client 
* Peer DIDs expect different actors exchange deltas of their state and hope to reach eventual consistency using CRDTs. However conflicts are still possible in case of malicious behavior, and protocol itself looks quite complex. IBC has several pros here:
** it splits transport and application layer, so that application layer protocols can be much less complex thanks to encapsulated complexity in lower-level common transport layer
** by requiring signing ""outgoing datagrams"" from state machines IBC allows explicit detection of state machine misbehavior (if we get two different correctly signed deltas for same seq no we know for sure that something bad has happened) and handles it on transport layer (basically it freezes communication with that client and requires a new handshake to connect again)
* As of now Peer DIDs doesn't have any formal proofs of correctness to the best of my knowledge - and this is quite a big task. On the other hand in IBC there is already an ongoing work on formally proving transport layer safety using automatic proof assistance system (it looks like they went with Agda)
* I might be wrong, but for me it looks like Peer DIDs require their actors to actively communicate with each other. On the other hand IBC explicitely introduce relayer processes, which do actual relaying packets between state machines (this is possible because of signatures) and allow state machines to be just state machines. In my opinion this greatly increases flexibility, and allows more complex scenarios, like having connection to some blockchain acting as a witness to many individual actors without introducing additional methods of communication.
;;;",,,,,,,,,,,,,,,,,,,
Write a proposal of options,INDY-2344,44360,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,17/Feb/20 4:52 PM,29/Feb/20 12:30 AM,28/Oct/23 2:47 AM,29/Feb/20 12:30 AM,,,,,,,,0,,,,,"Acceptance criteria:
* List of all options
* Assumptions
* Cons and pros
* Rough WBS
* Rough estimate",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixi",,,,Unset,Unset,Ev-Node 20.03,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"29/Feb/20 12:30 AM;ashcherbakov;Results of research and Action Items can be found here: https://docs.google.com/document/d/1HaKE7WighX4z4SpQ8bFC4mVF4-1ij_vMXBKLPJNKf-w/edit#;;;",,,,,,,,,,,,,,,,,,,,,
Simplify use of ZMQ connection client,INDY-2345,44436,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,ashcherbakov,ashcherbakov,27/Feb/20 7:44 AM,27/Mar/20 4:25 PM,28/Oct/23 2:47 AM,27/Mar/20 4:25 PM,,,,,,,,0,,,,,"*Acceptance criteria:*
- The client should be a fat binary
  - consider writing in Rust with dependencies statically linked
  - provide manually built packages for Ubuntu18.04 and Windows 10
- The client binary needs to output whether there is a ZMQ connection 
- Client should work against a given Validator Node (through the client stack)

*Notes*
* Automated CI / CD is not required. Manual builds from a developer machine is sufficient.
** Builds for Centos 8 would be nice, but are not required.
** We do not expect to need Mac builds.
* We do not need a fat binary for the server-side tool.
** If we need additional work on the server-side tool, it will be to have the docker file setup the service.
* It is optional to work against the server-side script in addition to the validator node.
",,,,,,,,,,,,,,,,,,,,,INDY-2330,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixj",,,,Unset,Unset,Ev-Node 20.03,Ev-Node 20.04,Ev-Node 20.05,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,"17/Mar/20 10:22 PM;anikitinDSR;Documentation is placed in
[https://github.com/hyperledger/indy-sdk/tree/master/scripts/test_zmq]

Binaries divided on platforms are located on:
https://repo.sovrin.org/check/;;;","27/Mar/20 4:25 PM;ashcherbakov;[~mgbailey] 
Did you have a chance to look at the new version?
I'm going to close the ticket, and if you see any issues or have any requests, please create a new one.;;;",,,,,,,,,,,,,,,,,,,,
Design Presention Definition,INDY-2346,44437,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,brentzundel,ashcherbakov,ashcherbakov,27/Feb/20 7:47 AM,27/Mar/20 9:14 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Acceptance criteria*
 - Create Indy HIPE and Aries RFC with examples and description of
 Presentation Definition object",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixy",,,,Unset,Unset,Ev-Node 20.06,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Design W3C Credential,INDY-2347,44438,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,27/Feb/20 7:56 AM,27/Mar/20 4:24 PM,28/Oct/23 2:47 AM,27/Mar/20 4:24 PM,,,,,,,,0,,,,,"*Acceptance criteria*
 * Write Indy HIPE and Aries RFC for W3C credential format applying to Indy/Aries",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixo9",,,,Unset,Unset,Ev-Node 20.04,Ev-Node 20.05,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"25/Mar/20 12:26 AM;ashcherbakov;Done in https://github.com/hyperledger/indy-hipe/pull/157;;;",,,,,,,,,,,,,,,,,,,,,
Design W3C Presentation,INDY-2348,44439,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,brentzundel,ashcherbakov,ashcherbakov,27/Feb/20 7:58 AM,27/Mar/20 9:13 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"*Acceptance criteria*
 * Write Indy HIPE and Aries RFC for W3C presentation format applying to Indy/Aries",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixz",,,,Unset,Unset,Ev-Node 20.06,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Blog Post March,INDY-2349,44453,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,echo.harker,esplinr,esplinr,28/Feb/20 7:54 AM,20/Mar/20 9:11 PM,28/Oct/23 2:47 AM,20/Mar/20 9:11 PM,,,,,,,,0,,,,,"Draft at least one technical blog post. Possible options:
* Troubleshooting Indy
* Considering consensus protocols
* Why Rich Schemas will be awesome.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixji",,,,Unset,Unset,Ev-Node 20.04,Ev-Node 20.05,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,sergey.khoroshavin,,,,,,,,,"13/Mar/20 7:48 PM;sergey.khoroshavin;Decision was made to go with ""Troubleshooting Indy"" topic;;;","20/Mar/20 9:11 PM;ashcherbakov;The content is written and the feedback from Echo is received, so we can close the ticket. ;;;",,,,,,,,,,,,,,,,,,,,
Improvements in Rich Schema common code,INDY-2350,44454,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,28/Feb/20 11:06 AM,20/Mar/20 9:10 PM,28/Oct/23 2:47 AM,20/Mar/20 9:10 PM,,,1.13.0,,,,,0,,,,,"*Acceptance criteria:*
 * {color:#000000}Make sure that @id == id{color}
 * {color:#000000}json_ld validation for objects that must be json_lds{color} (presence of @id, @type)
 ** Schema
 ** Mapping
 ** Presentation Definition

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixo",,,,Unset,Unset,Ev-Node 20.04,Ev-Node 20.05,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"19/Mar/20 5:18 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/1591;;;","20/Mar/20 9:09 PM;ashcherbakov;Integration and unit tests are written.
A full validation will be done after the correspondiong changes are supported by indy-sdk side.;;;",,,,,,,,,,,,,,,,,,,,
Research Stellar: Part 2,INDY-2351,44462,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,29/Feb/20 12:28 AM,13/Mar/20 9:48 PM,28/Oct/23 2:47 AM,13/Mar/20 9:24 PM,,,,,,,,0,,,,,"*Acceptance criteria:*
 * whether it's possible to implement Indy txns in Stellar
 * whether it's possible to support light clients in Stellar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwr",,,,Unset,Unset,Ev-Node 20.04,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,"02/Mar/20 9:12 PM;sergey.khoroshavin;*PoA*
* Find options for Indy txns on top of Stellar
** Research how to create immutable records in Stellar
** Research off-ledger validation options for Indy txns
** Come up with ideas how to implement revocation without on-chain validation
* Light clients for Stellar
** Find out how block signing is implemented (or if it is implemented at all) in Stellar. Looking for ""catch up"" implementation should help.
** If there are no block signatures - research options for adding them in a federated network
** Research current implementation of Stellar state, find out whether it is possible to reduce it to simple key-value storage (or at least its part responsible for storing immutable records)
** Come up with a PoA and rough estimate for implementing light client in Stellar;;;","13/Mar/20 9:48 PM;sergey.khoroshavin;Research results can be found here:
* https://docs.google.com/spreadsheets/d/10CwUSqF2NG71BOU5soy5FancBS0iYf-YDvWDBhHnBKc
* https://docs.google.com/document/d/1eeK9sL8dGK4UrKZQkW-dRMWPrx9I8GyEVg99Jg8z34E
* https://docs.google.com/document/d/1hi6AYIr44w9SGZOCwa5egRfCHFMItTK2miJLR8k3uh4;;;",,,,,,,,,,,,,,,,,,,,
Future Ledger options: discuss and continue,INDY-2352,44559,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,13/Mar/20 9:26 PM,30/Mar/20 11:22 PM,28/Oct/23 2:47 AM,30/Mar/20 11:22 PM,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwr4",,,,Unset,Unset,Ev-Node 20.06,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"30/Mar/20 11:21 PM;ashcherbakov;A summary a links are in [https://docs.google.com/document/d/1Td8LNjscmdgUqAbZWfor9eG4Ye7eZVvNcJsZ7lF60jY/edit.] No more actions are expected for a while.;;;",,,,,,,,,,,,,,,,,,,,,
Rich Schema Design: process feedback,INDY-2353,44560,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,13/Mar/20 9:28 PM,27/Mar/20 8:49 PM,28/Oct/23 2:47 AM,27/Mar/20 8:49 PM,,,,,,,,0,,,,,Process feedback for HIPEs and RFCs created in the scope of INDY-2337 and INDY-2347.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2327,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixoc",,,,Unset,Unset,Ev-Node 20.05,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,"27/Mar/20 8:48 PM;ashcherbakov;All PRs to indy-hipe are merged; PR to aries-rfc is reviewed; all findings and comments are processed.;;;",,,,,,,,,,,,,,,,,,,,,
Release 1.13.0,INDY-2354,44561,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,13/Mar/20 9:30 PM,30/Mar/20 11:21 PM,28/Oct/23 2:47 AM,,,,1.13.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwr9",,,,Unset,Unset,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bad implementation of the BLS signature algorithm,INDY-2355,44635,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Duplicate,sergey.minaev,dhuseby,dhuseby,23/Mar/20 6:01 AM,14/Apr/20 5:09 AM,28/Oct/23 2:47 AM,14/Apr/20 5:09 AM,,,,indycrypto,,,,0,,,,,"I'm writing to inform you of a bad implementation of the BLS signature algorithm in Hyperledger Indy. Hyperledger Indy uses the BLS signature for their consensus algorithm for the ledger. These signatures are created by each node in the ledger pool and aggregated for verification, state proofs and proofs of possession to prevent rogue key attacks. Because the BLS signature was not implemented in any library, Indy implemented the BLS signature over the BN254 curve in its own library. The library code for this is in Indy-crypto in [https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/src/bls/mod.rs].
 This common code is used in Indy node and Indy-SDK.
 Here is the bug/security problem. All ECC arithmetic operations are supposed to be modulo the curve modulus. For example, if we add field elements A + B, the result is (A+B) mod p.
 Instead Indy is using the curve order which is a smaller prime usually denoted as n or q. This results in a reduction of security. BN254 with all the recent attacks, the security level has been reduced to about 100 symmetric bits or a little less. Using the curve order lowers this even more. By Hasse's theorem, the number of points on an elliptic curve is less than 2 \sqrt(p), where p is the order of the field that the elliptic curve is over. So the security level could be less than 80 which is in the range where brute force attacks become viable. Starting here [https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/src/pair/amcl.rs#L402], you can see further down the code they are using the CURVE_ORDER instead of MODULUS.  This is not according to the spec as defined at [https://eprint.iacr.org/2018/483] and [https://crypto.stanford.edu/~dabo/pubs/papers/BLSmultisig.html]. They are also not using an approved hash to curve algorithm which means an attacker could use timing attacks to extract private keys during signing operations. Here are the code spots where they are hashing [https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/src/bls/mod.rs#L496] and [https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/src/pair/amcl.rs#L191]. They are computing SHA256 of the message then looping while adding 1 to the hash until a valid curve point is found. While not entirely bad, it's not constant time and [https://datatracker.ietf.org/doc/draft-irtf-cfrg-hash-to-curve/?include_text=1] is now considered the preferred method.
  
 The fix cryptographically is quite simple, change CURVE_ORDER to MODULUS. Ursa has this fix in its amcl_wrapper crate and implemented at [https://github.com/hyperledger/ursa/blob/master/libursa/src/signatures/bls.rs]
  
 However, this fix will break compatibility going forward. Old signatures will still rely on being modulo CURVE_ORDER vs MODULUS. Indy consumers will have to decide if they want to replace all existing transactions with the fix or checkout the ledger and move forward with new ones while supporting the old ones. The latter case will probably be acceptable. I have reviewed this with 3 cryptographers (Hart Montgomery, Mike Hamburg, and Thomas Pornin) who all agree this is very bad and are surprised it even works at all.
  
 My recommendation is that Indy move as quickly as possible to adopt Ursa and replace its dependency on Indy-crypto for all new BLS signatures. Existing deployments should checkout their ledger once the fix is in place and only use the old code to verify old transactions if needed.

This also affects revocation registry signatures and revocation credential signatures",,,,,,,,,Security issue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Artemkaaas,ashcherbakov,dhuseby,esplinr,MikeLodder,nage,redmike7,sergey.khoroshavin,sergey.minaev,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i01a6b:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Artemkaaas,ashcherbakov,dhuseby,esplinr,MikeLodder,nage,redmike7,sergey.khoroshavin,sergey.minaev,,,"25/Mar/20 10:04 PM;sergey.minaev;[~dhuseby] [~MikeLodder] thank you for reporting the potential security problem. We (indy team) spent some time analyzing the report, links to the source code and articles. And we are in doubt do we understand the report (or the crypto) correctly. Please take a look at our findings below, and also please note, that we are not cryptographers so *any corrections from you are very appreciated*.

*In short, we can not see any problem neither in BLS nor Revocation (the only exception is probably the out-dated EC BN254) and we hope to receive more guidance from you.*
 *1) BLS: the only thing in BLS codebase of IndyCrypto depending on CURVE_ORDER is the key generation. And the private key is not a point on the ECC but a number from `Z_n` where n==CURVE_ORDER in amcl.*
 *2) Other helpers are not used in BLS and belong to CL logic. They are not about EC points operations, but about calculations of `g^x` where g is a generator (EC point), and `x` is a result of a mapping (a+b, a*b. a^b) from Z_n to Z_n, and n=CURVE_ORDER.*

Please find more details below.
h2. 2 versions of BLS

As you know indy-crypto is about to be finally archived and replaced in Indy stack. The URSA contains almost the same code for compatibility as an alternative to a new one. So instead of ""indy-crypto"" vs ""URSA"" let's define 2 shortcuts for 2 different implementations. I suggest using URSA's features for that.

`bls_bn254` - old implementation based on bn254 curve - single option in indy-crypto [https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/src/bls/mod.rs] - also present in URSA at [https://github.com/hyperledger/ursa/blob/master/libursa/src/bls/mod.rs]
 `bls_bls12381` - new one implementation in URSA [https://github.com/hyperledger/ursa/blob/master/libursa/src/signatures/bls.rs] (based on new curve). In my chat this Mike he mentioned that it contains correct implementation for EC operations and should be used as a replacement of `bls_bn254`.

`bls_bn254` uses helpers from pair/amcl submodule of URSA and `GroupOrderElement`, `PointG1`, and `PointG2`. The pair/amcl helpers and other amcl related code is based on `amcl` crate 0.2.0 [https://crates.io/crates/amcl/0.2.0]
 As mentioned in the ticket description helpers for the `GroupOrderElement` contains some math operations by module `CURVE_ORDER` (lines 403 and below).
 As far as I can see, only `new_from_seed` and `new` helpers are used for BLS, other helpers of `GroupOrderElement` are used in CL Revocation only. And `GroupOrderElement` used as a Private Key.
 All the rest of BLS logic is based on helpers for `PointG1` and `PointG2` structures from the same file. But they are not using CURVE_ORDER, the amcl calls are used directly. E.g. `g1mul` call from `pair` module of external `amcl` crate.

`bls_bls12381` uses amcl_wrapper and the wrapper is based on miracl_amcl 3.2.5 [https://crates.io/crates/miracl_amcl/3.2.5]
 It uses `FieldElement` structure from the `amcl_wrapper` as a Private Key. Also, it uses `G1` and `G2` from the wrapper. So private key is constructed as the following `PrivateKey::random();` there random is [https://github.com/lovesh/amcl_rust_wrapper/blob/master/src/field_elem.rs#L103] and it's based at `get_big_num_from_RAND` function in the same file. As you can see it also uses Curve Order. [https://github.com/lovesh/amcl_rust_wrapper/blob/master/src/field_elem.rs#L335].
 *So the key generation part seems the same in `bls_bn254` and `bls_bls12381`.*
 As far as I can see the multiplication there is implemented by defining the Mul trait of Rust to overload * operator. Internal are based on `scalar_mul_const_time` function [https://github.com/lovesh/amcl_rust_wrapper/blob/master/src/group_elem_g1.rs#L108] and it's miracl_amcl::bls381::ECP::mul [https://github.com/miracl/amcl/blob/master/version3/rust/src/ecp.rs#L1003]
h2. Helpers with `% CURVE_ORDER`

Let's define (please correct me in case of ambiguous or wrong terms):
 E - set of EC points over field F_p (with order p = MODULUS)
 N = |E| (curve order?)
 G - additive cyclic subgroup of EC (G1 or G2) there is g_0 - the generator for the subgroup
 n = |G|, (`CURVE_ORDER` constant in amcl?), so `n * g_0` = 0
 Z_n = 0..(n-1)
h3. Key generation

Then according to [https://crypto.stanford.edu/~dabo/pubs/papers/BLSmultisig.html] Private key is `a` from `Z_n`. And the public key is `h = g_0^a` from G
 So seems like it should be aligned to `CURVE_ORDER` constant.
h3. Other helpers in `GroupOrderElement` of old `bls_bn254`

Seem like the naming in the codebase is misleading. Let me describe my understanding of `pow_mod` helper.
 ```
 /// (GroupOrderElement ^ GroupOrderElement) mod GroupOrder
 pub fn pow_mod(&self, e: &GroupOrderElement) -> UrsaCryptoResult<GroupOrderElement> {
 ```

Actually `GroupOrderElement` is not from E or G. It's from Z_n. So these helpers are not about EC point operations. They are about the calculation of expressions like `g_0^x` where `x` is a number from Z_n. And `x` can be an expression like `a+b`, `a*b`, `a^b`, where `a`, `b` in `Z_n`, and the result is in `Z_n` as well. Seems like CURVE_ORDER modulo is the appropriate one here.
h2. Hashing

For Indy use cases it seems like not a problem. There is no way for an attacker to measure the signing time itself as it's hidden by consensus algorithm and network delays.
 By the way, I don't understand how timing attack is possible on that hash. There is a transformation H for an incoming message (M) to point from G. So H(M) = g1 from G. As mentioned in the ticket `H` does SHA256 on the message and checking is it on the EC. So it doesn't related to private key at all. In other words, the timing for the particular message depends on only the Group and the message itself. Could you provide more context, why you are considering timing attack here?;;;","25/Mar/20 10:21 PM;MikeLodder;Z_n is the curve modulus, not the curve order. Hart Montgomery and three other cryptographers have verified that if you ever you curve order, that is incorrect. You are misunderstanding the Stanford paper with that conclusion. I have already update amcl_rust_wrapper to use the correct modulus here - [https://github.com/mikelodder7/amcl_rust_wrapper/blob/master/src/field_elem.rs.]

The modulus is used anytime you do field element arithmetic. It doesn't pertain to points. The hashing of the message can be hidden away using the hash2curve crate at [https://crates.io/crates/hash2curve,] so using shouldn't be an issue.

BLS signatures are created by hashing the message to a curve point then multiplying that point by the private key. So two things can happen if the hash is done incorrectly:

 

1- If the point is invalid, then the signature will also be invalid and not verify

2- If the point is invalid, then it's theoretically possible to discover the private key.

Notice how neither of those has to do with timing attacks. Because of this alone, I would want to change the algorithms.

Now a timing attack happens when an attacker can submit data to be signed to an endpoint and received back a signature. Based on how much it takes to respond, certain bits of information is leaked. Nadia Heninger, for example, used this very idea to extract TPM keys [https://tpm.fail/tpmfail.pdf.] I hope that helps.

 

hash2curve crate has been verified by two cryptographers to meet the spec. The algorithms implemented there are constant time and thus not susceptible to timing attacks. The underlying microarchitecture might not have constant time multiplies but there isn't much we can do about that.;;;","25/Mar/20 10:57 PM;ashcherbakov;We understand that curve order |E(F_p)| (a number of elements) is not the same as the prime order of G1 and G2 cyclic groups (denoted by n) where the private key is in Z_n. We may be wrong, but it looks like amcl uses CURVE_ORDER constant as the prime order of G1 and G2 cyclic groups, and MODULUS as the order of a field F_p the elliptic curve is over (that is p=MODULUS)

If `CURVE_ORDER` is an order of EC (that is the number of elements), then it must be >= than the order n of a cyclic subgroup G1 or G2.
But according to https://github.com/miracl/amcl/blob/master/version3/rust/src/roms/rom_bn254_64.rs#L24, `MODULUS` is greater than `CURVE_ORDER`, which means that an order of a subgroup is greater than the order of the group => a contradiction. 


bq. Ursa has this fix in its amcl_wrapper crate and implemented at https://github.com/hyperledger/ursa/blob/master/libursa/src/signatures/bls.rs

As it was mentioned, this code (in Ursa master) uses CurveOrder as well:
{code}
n = BigNum::randomnum(&BigNum::new_big(&CurveOrder), r);
{code}

According to your last comment it looks like `bls_bls12381` implementation in the master Ursa needs to have a fix as well (in `amcl_rust_wrapper`)?;;;","25/Mar/20 11:06 PM;MikeLodder;Yes, I have had an open PR to lovesh's amcl_rust_wrapper crate for a long time but he hasn't responded so I published amcl_wrapper_ml which does have this fix.

 

Group elements are part of the cyclic group CURVE_ORDER. However, all field element operations MUST be done over the MODULUS. I'm making sure it should be that way everywhere.;;;","25/Mar/20 11:14 PM;ashcherbakov;{quote}Group elements are part of the cyclic group CURVE_ORDER. However, all field element operations MUST be done over the MODULUS. I'm making sure it should be that way everywhere.
{quote}
I believe it depends on the operation. If you have two EC (over F_p) points A and B, and perform A+B operation, then it must be over the p=MODULUS.

But if you have a cyclic group G1 of order q (q=CURVE_ORDER) with a generator g0, then a BLS primary key is a number from Z_q (over CURVE_ORDER), not Z_p (over MODULUS).
 That's how we read [https://crypto.stanford.edu/~dabo/pubs/papers/BLSmultisig.html:]
{quote}a bilinear pairing e:G0×G1→GT. The pairing is efficiently computable, non-degenerate, and all three groups have prime order q
{quote}
{quote}choose a random α← Z_q
{quote};;;","25/Mar/20 11:22 PM;ashcherbakov;And the same for the methods `pow_mod`, `add_mod`, `sub_mod`, `mul_mod` ([https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/src/pair/amcl.rs#L426).]
The input parameter `r` there is from Z_q where q is an order of groups G1, G2, please see [https://github.com/hyperledger/indy-crypto/blob/master/libindy-crypto/docs/AnonCred.pdf]
An example where `pow_mod` and `add_mod` is used is equation (17) on the page 5: sk+gamma^i is a mapping from Z_q -> Z_q where q=CURVE_ORDER.

 ;;;","25/Mar/20 11:23 PM;ashcherbakov;BTW we would be happy to have a call to discuss this in more details.;;;","26/Mar/20 12:59 AM;ashcherbakov;We had a meeting with Mike L. and Hart M. and came to the following conclusion:
 * All the mentioned functions are about the exponents only (from Z_q where q=CURVE_ORDER), so it's correct to use mod over CURVE_ORDER.
 * There is no timing attack related to hashing algorithm used in BLS
 * However, the current indy algorithm for BLS hashing may produce non-uniform distribution which can be considered as a security bug. However, this is not a critical bug, so that we can plan migration to a new hashing algorithm according to a normal schedule instead of a hot fix deprecating the old algorithm.

Action Item:
 * There is no serious security vulnerability, so no immediate fixes are needed
 * We need to improve the documentation for indy-crypto's and Ursa's  `pow_mod`, `add_mod`, `sub_mod`, `mul_mod` calls to explicitly mention that they are done over the exponents from Z_q, not F_p.
 * Still plan to migrate to a new hashing algorithm and a new Elliptic Curve for BLS in future
 * Communicate with Mike about the failures observed for some test vectors.

 ;;;","14/Apr/20 4:56 AM;dhuseby;This security issue is being handled as a Github Security Advisory as a test of that system. Please continue work on this issue over here: https://github.com/hyperledger/ursa/security/advisories/GHSA-c39v-9xw6-85hr;;;","14/Apr/20 5:09 AM;dhuseby;Please use the security advisory over on Github: https://github.com/hyperledger/ursa/security/advisories/GHSA-c39v-9xw6-85hr;;;",,,,,,,,,,,,
Release 1.14.0,INDY-2356,44684,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/20 5:21 PM,27/Mar/20 5:22 PM,28/Oct/23 2:47 AM,,,,1.14.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwy",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move indy-plenum CI pipeline to GitHub Actions,INDY-2357,44685,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/20 5:31 PM,03/Apr/20 9:58 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2363,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwri",,,,Unset,Unset,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move indy-node CI pipeline to GitHub Actions,INDY-2358,44686,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/20 5:31 PM,03/Apr/20 9:58 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2363,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwrr",,,,Unset,Unset,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate indy-plenum CD pipeline from Jenkins,INDY-2359,44688,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/20 9:48 PM,03/Apr/20 9:58 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2363,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixws",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate indy-node CD pipeline from Jenkins,INDY-2360,44689,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/20 9:49 PM,03/Apr/20 9:58 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2363,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixws4",,,,Unset,Unset,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate to new BLS in Ursa 0.4.0,INDY-2361,44690,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,27/Mar/20 10:13 PM,27/Mar/20 10:21 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"TODO
* How long  should we support the old hashing algorithm so that old clients don't slow down in their responses?",,,,,,,,,,,,,,,,,IS-1541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqeixwz",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate Indy Node from Jenkins to GitHub,INDY-2363,44728,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,03/Apr/20 9:58 AM,03/Apr/20 10:01 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-1546,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-7,,Node Jenkins to GitHub,To Do,No,,Unset,No,,,"1|i01alf:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
From different clients hyperledger Indy pool,INDY-2364,44762,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,plmplm55,plmplm55,09/Apr/20 7:42 PM,09/Apr/20 7:42 PM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"I have a question about indy pool

It seems that alice, faber, and acme used in the indy deme run in the same client.

However, I want to proceed with demo after creating as alice, faber, acme as different clients.

For example, alice is an android app, faber is a server with a db where student information is entered, and acme is a third party app.

In my opinion, do three clients need to connect to the same pool to do this?

So, how can different clients connect to the same pool?

If you simply make the pool configuration file and the domain configuration file the same and then create and access the pool, are they on the same indy network (ledger)?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i01asb:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Define the anoncreds 2 revocation ledger transactions, content and work required",INDY-2365,44831,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,swcurran,swcurran,swcurran,17/Apr/20 2:49 AM,17/Apr/20 2:49 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Discussions were held with some folks ([~redmike7] [~cywolf] and myself) about anoncreds revocation 2.0 (aka Merkle Tree-based revocation) based on the capabilities exposed in Ursa.  We propose that the following attributes and approaches be used in the Indy credential revocation 2.0 instance.
 * Tree leaves will be indexed bits (vs. bytes) that are either 0 (not revoked) or 1 (revoked).
 * The Merkle tree will be an 8-ary tree (vs. binary or 4-ary).
 * Nodes holding the tree leaves will be 64-bit unsigned integers,
 ** As such, a minimum revocation registry is one layer deep with a root hash and eight 64 bit ints, and hence would be capable of handling 512 credentials.
 * On creation of a revocation registry, a parameter will be the depth of the tree, with a value of 1 to 7, supporting up to ~134 million credentials in a single registry.
 ** The table below gives the number of credentials and MByte size of the leaves by the number of layers of the registry.
 * The two existing revocation-related ledger transactions will continue to be used - REVOC_REG_DEF and REVOC_REG_ENTRY
 ** REVOC_REG_DEF will contain at least a link to the cred def and the number of layers of the Merkle Tree. Since all the leaves are 0 (all credentials unrevoked), the Merkle Tree itself is not needed.
 ** REVOC_REG_ENTRY will contain at least a signed Merkle Tree root and a gzip compressed set of leaf nodes in a well-defined configuration.
 *** Since the distribution of revoked credentials is likely to be skewed one way or the other, the compression rate in the common case is likely to be high.
 *** For a 7-layer ledger with (unlikely) ineffective compression the worst ledger entry size would be ~16Mb.
 *** The interior nodes of the Merkle Tree must be calculated from the leaves and the calculated root verified against that in the ledger transaction.
 **** It may be possible to organize the leaves such that a prover can stream the compressed data, decompressing only the section necessary to construct the proof--the path from the credential leaf to the root.
 *** A ""domain separation"" value is needed as well, for reasons to be added here.
 * As is necessary today, the holder/prover and verifier must read from the ledger the REVOC_REG_DEF and appropriate REVOC_REG_ENTRY values in creating and verifying the proof.
 ** Unlike today, no tails file is needed by the prover.

To be determined is whether any change is needed to Indy Node to support the transaction content, or whether the current Indy Node code can support the same transaction with different content. Note that Indy Node does not have to do anything with the transaction content other than write it to the ledger and pass it back to any readers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i01b37:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),swcurran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move issue tracking for Indy Node and Indy Plenum to GitHub issues/Deprecate the use of JIRA,INDY-2366,44832,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,swcurran,swcurran,17/Apr/20 3:02 AM,17/Apr/20 3:02 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,"Now that [security advisories are supported in github|[https://help.github.com/en/github/managing-security-vulnerabilities/managing-security-vulnerabilities-in-your-project]], we would like to switch from Jira to GitHub issues for Indy.  For those with github maintainer capabilties, please update the settings in GitHub to activate issues.

As well, someone (I can take it on) needs to do a PR to update the ""Contributors"" guidelines to cover that github issues should be used for issues, that Jira has history and that the security features are used by maintainers to track vulnerabilities.

[~esplinr] - can you help with the activation step?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i01b3f:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),swcurran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
All Ci's,INDY-2367,46345,43109,Sub-task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Q1Blue,Q1Blue,25/Nov/20 9:55 AM,25/Nov/20 9:55 AM,28/Oct/23 2:47 AM,,,,1.13.0,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i01hdf:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Q1Blue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Journey,INDY-2368,46346,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Q1Blue,Q1Blue,25/Nov/20 9:58 AM,25/Nov/20 9:58 AM,28/Oct/23 2:47 AM,,,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i01hdn:",,,,Unset,Unset,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Q1Blue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
