Summary,Issue key,Issue id,Parent id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Affects Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Description,Environment,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Inward issue link (Blocks),Outward issue link (Blocks),Outward issue link (Blocks),Outward issue link (Blocks),Outward issue link (Blocks),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Relates),Inward issue link (Relates),Inward issue link (Relates),Inward issue link (Relates),Inward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Outward issue link (Relates),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Business Value),Custom field (Commit Levels),Custom field (Current Status),Custom field (Design),Custom field (Design Status),Custom field (Documentation Impact),Custom field (Documentation Status),Custom field (Epic Color),Custom field (Epic Link),Custom field (Epic Name),Custom field (Epic Status),Custom field (Executed),Custom field (Found in Commit),Custom field (Function Test Status),Custom field (Must Fix),Custom field (Original story points),Custom field (Parent Link),Custom field (Rank),Custom field (Release Note),Custom field (Release Note Required),Custom field (Root Cause Analysis),Custom field (SDK Impact),Custom field (Sample/Tutorial),Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Sprint,Custom field (Steps to Reproduce),Custom field (Story Points),Custom field (System Test Impact),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Team),Custom field (Test Plan),Custom field (Test Result),Custom field (Test Result Details),Custom field (Test Type),Custom field (Triaged),Custom field (Usage),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Watchers),Custom field (Workaround),Custom field (gitCommitsReferenced),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Cannot move plenum/test/instances tests to sdk,INDY-1001,24717,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,dsurnin,dsurnin,04/Dec/17 9:00 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,,,,,,,,,,,IS-445,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzyyhb:",,,,,,,,,,,,,,2.0,,,,,,,,,,,,Derashe,dsurnin,SeanBohan_Sovrin,,,,,,,,,,"18/Jan/18 6:58 AM;SeanBohan_Sovrin;[~dsurnin] - is this still an issue? Please advise;;;","18/Jan/18 1:29 PM;dsurnin;[~SeanBohan_Sovrin]
Not an issue anymore, sdk is ready so we can continue to move tests to sdk;;;","13/Mar/18 8:32 PM;Derashe;Problem reason:
 - We need to integrate all tests in plenum/test/instances folder with sdk

Changes:
 - Tests integrated, created additional fixtures and functions

PR:
- https://github.com/hyperledger/indy-plenum/pull/563

Version:
 - master, 270

Risk factors:
 - No

Risk:
 - Low

Covered with tests:
 - No

Recommendations for QA

-;;;",,,,,,,,,,,,,,,,,,,,,,
Sustain a 10 transaction per second write to an 25-node Validator pool,INDY-1002,24719,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,SeanBohan_Sovrin,SeanBohan_Sovrin,04/Dec/17 11:27 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,LibIndy,,,"Testing Node Performance and understanding the overall throughput and performance of the Validator Pool. This is 1 of 3 tickets regarding transactions per second (ticket 2 is for a 35-node validator pool, ticket 3 is for a 50-node validator pool). 

Spec sent last week to replicate test scripts and setup with libindy
 # Set up local environment (until Kelly hands over his - possibly next 2 sprints)
 # Run a pool with 25 Validator nodes
 # Document results, report to Engineering/Product

Success criteria: report detailing transaction per second at 25 validator nodes

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1607,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwywv:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,SeanBohan_Sovrin,,,,,,,,,,,"28/Sep/18 6:42 AM;esplinr;Completed as part of our testing in the series of Indy Node 1.6 releases.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Sustain a 10 transaction per second write to a 35-node Validator pool,INDY-1003,24720,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,SeanBohan_Sovrin,SeanBohan_Sovrin,04/Dec/17 11:28 PM,11/Oct/19 6:58 PM,28/Oct/23 2:47 AM,11/Oct/19 6:58 PM,,,,,,0,LibIndy,,,"Testing Node Performance and understanding the overall throughput and performance of the Validator Pool. This is 1 of 3 tickets regarding transactions per second (ticket 1 is for a 25-node validator pool, ticket 3 is for a 50-node validator pool). 

Spec sent last week to replicate test scripts and setup with libindy
 # Set up local environment (until Kelly hands over his - possibly next 2 sprints)
 # Run a pool with 35 Validator nodes
 # Document results, report to Engineering/Product

Success criteria: report detailing transaction per second at 35 validator nodes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwyx3:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,SeanBohan_Sovrin,,,,,,,,,,,"11/Oct/19 6:58 PM;esplinr;We have shown adequate performance with a 25 node pool, and have capped the pool at that size for now. We can create new stories for future testing when we have a demonstrated need for better performance or a bigger pool.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Sustain a 10 transaction per second write to a 50-node Validator pool,INDY-1004,24721,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,SeanBohan_Sovrin,SeanBohan_Sovrin,04/Dec/17 11:29 PM,11/Oct/19 6:58 PM,28/Oct/23 2:47 AM,11/Oct/19 6:58 PM,,,,,,0,LibIndy,,,"Testing Node Performance and understanding the overall throughput and performance of the Validator Pool. This is 1 of 3 tickets regarding transactions per second (ticket 1 is for a 25-node validator pool, ticket 2 is for a 35-node validator pool). 

Spec sent last week to replicate test scripts and setup with libindy
 # Set up local environment (until Kelly hands over his - possibly next 2 sprints)
 # Run a pool with 50 Validator nodes
 # Document results, report to Engineering/Product

Success criteria: report detailing transaction per second at 50 validator nodes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1433,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwyxb:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,SeanBohan_Sovrin,,,,,,,,,,,"11/Oct/19 6:58 PM;esplinr;We have shown adequate performance with a 25 node pool, and have capped the pool at that size for now. We can create new stories for future testing when we have a demonstrated need for better performance or a bigger pool.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As a Sovrin Steward, my node should be able to support a sustained 10,000 reads per second",INDY-1005,24722,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,SeanBohan_Sovrin,SeanBohan_Sovrin,04/Dec/17 11:40 PM,05/Dec/17 12:02 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"We need to know how many reads per second an individual Observer node can sustain

Dependent on:
 # Alex finishing PoA for Observer nodes
 # state-proofs

Success criteria: 

1. Report showing Reads Per Second per Node on an Observer pool (test)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwyxj:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change test_valid_message_request to using sdk,INDY-1008,24768,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,anikitinDSR,anikitinDSR,05/Dec/17 10:19 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Need to change test to using indy sdk.
For now we cannot get invalid request from node.

Test location:
plenum/test/node_request/message_request/test_valid_message_request.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzz11b:",,,,,,Sprint 18.05,,,,,,,,1.0,,,,,,,,,,,,anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clean up Getting Started Guide,INDY-1009,24806,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,danielhardman,danielhardman,danielhardman,06/Dec/17 9:42 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"A developer who encounters Indy should find the Getting Started Guide prominently linked from the readme.md in the repo of all indy-* repos, as well as on any wikis for the repos. All these links should point to the same place.

Links that are the master branch of the code should point to the version of the Guide that's on either master or stable, but never to any other branches. Links on the stable branch should always point to the Guide on the stable branch, never to master.

All old/stale versions of the Guide (in Google Docs, in PDFs on sovrin.org, in github.com/hyperledger/archive) should be marked as deprecated and/or updated to point to more recent content, or deleted/hidden so people can't be confused.

Testing of the Getting Started Guide should evaluate all branches of the flow described in the Guide (e.g., both Docker and Vagrant), and should begin by browsing to the github repo and following a link from the readme.md, to guarantee that QA folks are experiencing the guide the same way a developer would find it. Specifically, the QA workflow should not begin by browsing to a bookmarked URL that might be different from the one external developers would find on their own.

If a particular variant of the Guide isn't working (e.g., Docker doesn't work), the Guide should not be considered functional until the bug is fixed or the variant of the workflow is removed from the Guide.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/17 1:55 AM;ozheregelya;formatting.jpg;https://jira.hyperledger.org/secure/attachment/13909/formatting.jpg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzypvr:",,,,,,INDY 17.25,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,danielhardman,krw910,ozheregelya,TechWritingWhiz,,,,,,,,"06/Dec/17 9:46 AM;danielhardman;Chat transcript between Andy T and Daniel H:

 

Andy T
 @danielh Does the Getting Started guide work? Some clear and loud feedback from here in Finland that developers who are interested in Sovrin are getting turned off because Getting Started does not work. Feedback is that this ""hello world"" stuff must work to keep developers engaged. They try it, it doesn't work, they ditch it and never come back, but they don't say anything about it so we never found out that they try. To ensure we get developer support, we need a slick Getting Started that *just works*. So, could you please check and let me know if Getting Started works, and if not can we get it working reliably and consistently please Thanks!

 

danielh
 We test that Getting Started works, before every release. However, it is possible that the people who are reporting failure are trying to go through the tutorial differently from our testers. I am asking Kelly to do some fact-finding.

 

danielh
 Response from Kelly: In this next release of Stable we have rebranded from Sovrin to Indy so all the commands that were sovrin are now indy. The Guide has been updated to reflect the new commands, but the build is still in RC waiting for TGB approval before moving to the Stable repo. So we have a guide that does not match the current code.
 At least that is the working theory without knowing more.

 

danielh
 I recommend that you connect the people reporting trouble directly to Kelly Wilson, so he can compare their workflow to what we are testing. Do we have the names of the people who are failing?

Here is what sovrin support had to say about it:
 I think it will work for them if 1) they are on stable 2) the link to the vagrant setup is corrected to “[https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestSovrinClusterSetup.md]”
 GitHub
 evernym/sovrin-environments
 sovrin-environments - Environments for standing up Sovrin for different use cases.

 

Andy T
 Hi Daniel - thanks for the input. I will be with some of the guys that have tried this today here in Finland so will point them at Kelly. Ideally I’d like something more definitive than “I _think_ it will work”. If there are tweaks or workarounds that are needed, these should be clear in the instructions. Unfortunately I’m at a bit of a disadvantage as I don’t have the technical know-how these days to try it myself otherwise I’d be rather more helpful in my feedback!

 

Andy T
 Another observation. That link that support provided doesn’t shout “Getting Started Guide” all over it, and it’s in Evernym’s github. There was a getting started in the Indy github but I can’t seem to find it. Any other devs will have similar trouble I imagine. Is there a way to clearly signpost the getting started from the Indy repos?

Looking at the Hyperledger Indy Wiki, it points me to this page, which is different to the one that Mike Bailey pointed to above: [https://github.com/hyperledger/indy-node/blob/stable/getting-started.md]. If there are multiple versions it will be easy to get out of sync.;;;","13/Dec/17 5:07 PM;ashcherbakov;[~krw910] Who should start working on this task? Is it [~TechWritingWhiz]?;;;","14/Dec/17 3:30 AM;ozheregelya;INDY-1056: Document *Running a Simulation of a Indy Cluster and Agents* ([https://github.com/hyperledger/indy-node/blob/stable/docs/cluster-simulation.md]):
{code:java}
$ pip install -U --no-cache-dir indy-client{code}
{code:java}
>>> from indy_client.test.training.getting_started import * {code}
indy-client does not exist. It should be indy-node or sovrin instead of indy-client. 

INDY-1059: Document *Indy – Running the Getting Started tutorial locally* ([https://github.com/hyperledger/indy-node/blob/stable/docs/indy-running-locally.md] )
 Script _setupEnvironment.sh_ is not actual after file/folder changes (INDY-831 - INDY-833). Node data are placed in /var/lib/indy, not in ~/.indy.

INDY-1059: Document *Indy – Running the Getting Started tutorial locally* ([https://github.com/hyperledger/indy-node/blob/stable/docs/indy-running-locally.md] )
{code:java}
python /usr/lib/python3.5/site-packages/indy_client/test/agent/faber.py --port 5555 python /usr/lib/python3.5/site-packages/indy_client/test/agent/acme.py --port 6666 python /usr/lib/python3.5/site-packages/indy_client/test/agent/thrift.py --port 7777{code}
Paths to agent scripts are wrong, network parameter is absent. Correct commands:
{code:java}
python /usr/local/lib/python3.5/dist-packages/indy_client/test/agent/faber.py  --port 5555 --network <network_name>
python /usr/local/lib/python3.5/dist-packages/indy_client/test/agent/acme.py  --port 6666 --network <network_name>
python /usr/local/lib/python3.5/dist-packages/indy_client/test/agent/thrift.py  --port 7777 --network <network_name>{code}
INDY-1060: Document *Setting Up a Test Indy Network in VMs* (master, stable)
 Broken link [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/sandbox/DevelopmentEnvironment/Vagrantfile]
 here: ""...you may continue setting up an actual Developer Environment connected to a sandbox by following these {color:#d04437}instructions{color}.""

INDY-1060: Document *Setting Up a Test Indy Network in VMs* ( [https://github.com/evernym/sovrin-environments/blob/master/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md] )
 Wrong formatting:
 !formatting.jpg|thumbnail!
 As far as I understand, text marked with red arrows should be formatted as text, not as code.
  
INDY-1060: Document *Setting Up a Test Indy Network in VMs*
 Mixed up links. Master version of this document [https://github.com/evernym/sovrin-environments/blob/master/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md] links to stable version of GST [https://github.com/hyperledger/indy-node/blob/stable/getting-started.md]
  
INDY-1062: Document *Getting Started with Indy* 
 broken link [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/sandbox/DevelopmentEnvironment/Vagrantfile]
 here: Note: If you're looking to create an actual Developer Environment connected to a sandbox, please visit this _{color:#d04437}guide{color}_ instead.
  
INDY-1062: Document *Getting Started with Indy*
 Wrong output of command _load sample/faber-request.indy_ in example:
 ALICE> load sample/faber-request.indy
 {color:#d04437}New wallet Default created{color}
 {color:#d04437}Active wallet set to ""Default""{color}
 1 connection request found for Faber College.
 Creating Connection for Faber College.
  
 Try Next:
     show connection ""Faber College""
     accept request from ""Faber College""
  
 New wallet Default will not be created because wallet Alice was created manually on one of previous steps. 
  
INDY-1062: Document - *Getting Started with Indy*
 Links to vagrant instruction ([https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md]) are still broken for both of stable and master versions. These links also should accord to master/stable instruction version.

INDY-1060: Need to document that vagrant instruction does not work on host ubuntu system due to vagrant issue: [https://github.com/hashicorp/vagrant/issues/7155] (see comments below).;;;","18/Dec/17 2:02 AM;ozheregelya;[~krw910], [~danielhardman], 
One major problem was noticed during verification of documentation. Vagrant installation is impossible on host ubuntu system due to Vagrant issue: [https://github.com/hashicorp/vagrant/issues/7155]

Should this information be included to Vagrant instruction?

For host windows system all works well.;;;","18/Dec/17 2:09 AM;ozheregelya;Problem in Vagrant script for validator: INDY-1046 (actual only for master version, stable works well).;;;","18/Dec/17 8:43 PM;ozheregelya;Problem in Vagrant script for agent: INDY-1048 (actual only for master version, stable works well).;;;","19/Dec/17 1:14 AM;krw910;[~ozheregelya] On Vagrant and Ubuntu lets go ahead and make sure that gets documented and you can provide the link to the vagrant ticket from hashicorp.;;;","20/Dec/17 7:19 AM;ozheregelya;New tickets for documentation problems were created (INDY-1056, INDY-1059, INDY-1060, INDY-1062), so this one can be closed.;;;","05/Jan/18 8:01 AM;TechWritingWhiz;Tickets INDY-1059, INDY-1060, and INDY-1062 have all had pull requests submitted. Still waiting on clarification on INDY-1056. ;;;",,,,,,,,,,,,,,,,
We need to reduce the read load on Validators Pool,INDY-1010,24808,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,ashcherbakov,ashcherbakov,06/Dec/17 6:56 PM,09/Oct/19 5:30 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,See https://docs.google.com/document/d/1HcVp0V1RvgHanW_et84ttiga-eWkOaNyzhBzFwsWBIg/edit#heading=h.gzcglndoi3x9,,,,,,,,,,INDY-628,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-58,,,,,,,,,"1|hzwypr:",,,,,,,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support GATEKEEPER service in NODE txn,INDY-1011,24809,24808,Sub-task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,06/Dec/17 6:57 PM,06/Dec/17 6:57 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Support GATEKEEPER service in NODE txn, define auth rule on who can create gatekeepers Nodes. No special behaviour for Gatekeepers yet",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyppz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Abstarct Observers,INDY-1012,24810,19816,Sub-task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,06/Dec/17 6:58 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,Support adding abstract Observers (pub-sub) to Nodes and possibility of a custom policy for syncing (no real policies yet),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypq7:",,,,,,INDY 17.25,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Separate pool to Validators and gatekeepers ,INDY-1013,24811,24808,Sub-task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,06/Dec/17 6:58 PM,06/Dec/17 6:58 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Separate pool to Validators and gatekeepers with N_V, f_V and N_G, f_G and make them connect to each other",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypqf:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Register all Gatekeeper Nodes as Observers,INDY-1014,24812,24808,Sub-task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,06/Dec/17 7:01 PM,06/Dec/17 7:01 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Register all Gatekeeper Nodes as Observers with a default Policy to send each write Reply to all Gatekeepers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypqn:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Restrict Gatekeepers to process read requests only,INDY-1015,24813,24808,Sub-task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,06/Dec/17 7:01 PM,06/Dec/17 7:01 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Restrict Gatekeepers to process read requests only and exclude them from consensus. Gatekeepers reject write requests at this point,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypqv:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support processing observed data by Observers,INDY-1016,24814,19816,Sub-task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,ashcherbakov,ashcherbakov,ashcherbakov,06/Dec/17 7:02 PM,30/Mar/19 5:39 AM,28/Oct/23 2:47 AM,30/Mar/19 5:39 AM,,,,,,0,,,,Support processing observed data by Gatekeepers (or Observers in general) ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypr3:",,,,,,INDY 17.25,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch-up BLS store by all Nodes ,INDY-1017,24815,24808,Sub-task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,06/Dec/17 7:05 PM,06/Dec/17 7:05 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Catch-up BLS store by all Nodes (in fact the latest BLS store is needed),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyprb:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pool is unable to write txns after two nodes adding,INDY-1018,24816,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,06/Dec/17 8:30 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Build Info:
indy-node 1.2.50 (rc)

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~9000 txns.
3. Add the 5th node.
4. Send NYM txn.
5. Add the 6th node.
6. Send NYM.
7. Get NYM.

Actual Results:
Both nodes catches up successfully but NYM from Step 6 is not added (but GET_NYM command from Step 7 still works).

Expected Results:
Pool should work normally after both nodes adding.

Workaround:
Restart at least 4 nodes of 6 to return the pool to valid state.

Additional Info:
The issue is not reproducing with small ledger pool.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1033,,,,,INDY-1029,,,,,,,,,,,,,,,"06/Dec/17 8:26 PM;VladimirWork;Node5.log;https://jira.hyperledger.org/secure/attachment/13444/Node5.log","06/Dec/17 8:26 PM;VladimirWork;Node6.log;https://jira.hyperledger.org/secure/attachment/13443/Node6.log","22/Dec/17 1:15 AM;VladimirWork;_node1.txt;https://jira.hyperledger.org/secure/attachment/13936/_node1.txt","22/Dec/17 1:13 AM;VladimirWork;_node5.txt;https://jira.hyperledger.org/secure/attachment/13937/_node5.txt","22/Dec/17 1:13 AM;VladimirWork;_node6.txt;https://jira.hyperledger.org/secure/attachment/13938/_node6.txt","12/Dec/17 9:42 PM;VladimirWork;logs.7z;https://jira.hyperledger.org/secure/attachment/13603/logs.7z","15/Dec/17 9:19 PM;VladimirWork;logs_after_full_catchup.7z;https://jira.hyperledger.org/secure/attachment/13901/logs_after_full_catchup.7z","15/Dec/17 9:16 PM;VladimirWork;logs_before_full catchup.7z;https://jira.hyperledger.org/secure/attachment/13900/logs_before_full+catchup.7z","06/Dec/17 8:30 PM;VladimirWork;nodes_adding.PNG;https://jira.hyperledger.org/secure/attachment/13442/nodes_adding.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyptr:",,,,,,INDY 17.25,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,VladimirWork,,,,,,,,,,"15/Dec/17 9:17 PM;VladimirWork;Build Info:
indy-node 1.2.241

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~10000 txns.
3. Add the 5th node.
4. Send NYM txn (send NYM dest=TLYLkqxLmZS6zhyzDpUD21).
5. Add the 6th node.
6. Send NYM (send NYM dest=5nGBeNBbhvxzUxoGnUAc2v).
7. Send NYM (send NYM dest=JuWFpbXytMZxWEdH6oBwk1).
8. Wait about 30 minutes.
9. Send NYM (send NYM dest=Kw1qHdrAGDVWKKM6Yifx79).

Actual Results:
Step 4 NYM is added. Step 6 and 7 NYMs are not added. Node 5 catches up to 10211 txns but txn from Step 5 (adding Node 6 Steward) is not written to Node 5 domain ledger (so after Step 5 initial 4 nodes have 10212 txns but Node 5 has still 10211 and looks like not in consensus with the whole pool). Node 6 catches up to 8173 txns right after adding to the pool and up to 10212 after ~30 minutes only (NYM from Step 9 (sent after full Node 6 catch up) is not added too).

Logs before and after full Node 6 catchup are in attachment. [^logs_before_full catchup.7z]  [^logs_after_full_catchup.7z] ;;;","21/Dec/17 8:52 PM;dsurnin;node build 246 should contain the fix;;;","22/Dec/17 1:15 AM;VladimirWork;Build Info:
indy-node 1.2.246

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~10000 txns.
3. Add the 5th node.
4. Send NYM txn (send NYM dest=TLYLkqxLmZS6zhyzDpUD21).
5. Add the 6th node.

Actual Results:
Node 5 catches up successfully but doesn't reach consensus (NYMs from Steps 4, 6 aren't added to 5th node ledger). Node 6 doesn't catch up and doesn't reach consensus. [^_node1.txt]  [^_node5.txt]  [^_node6.txt] ;;;","11/Jan/18 9:41 PM;dsurnin;new node should use fixed plenum

fixed plenum version is 215;;;","12/Jan/18 12:16 AM;ashcherbakov;[~dsurnin] Please provide description of changes;;;","12/Jan/18 1:33 PM;dsurnin;Problem reason:
watermarks were not changed after catch up during the node were connected to pool

Changes:
add watermark changes after catch up
prevent reset of watermark on primary change in case of propagate

PR:
https://github.com/hyperledger/indy-plenum/pull/491

Version:
plenum 215

Risk factors:
view change
propagation

Risk:
Med

Covered with tests:
test_add_node_to_pool_with_large_ppseqno.py
test_recover_primary_no_view_change.py;;;","12/Jan/18 10:18 PM;VladimirWork;Build Info:
indy-node 1.2.265
indy-plenum 1.2.215

Steps to Validate:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~10000 txns.
3. Add the 5th node.
4. Send NYM txn.
5. Add the 6th node.
6. Send NYM txn.

Actual Results:
Both added nodes catch up and reach consensus successfully.;;;",,,,,,,,,,,,,,,,,,
Change fixture for 3pc and appropriative tests for node_request,INDY-1019,24817,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,anikitinDSR,anikitinDSR,06/Dec/17 9:16 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Need to change fixtures like propageted1, preprepared1, prepared1, etc, which used for 3pc states testing.

list of tests:
 plenum/test/node_request/test_commit/test_num_commit_with_2_of_6_faulty.py
 plenum/test/node_request/test_commit/test_num_commit_with_one_fault.py
 plenum/test/node_request/test_commit/test_num_of_commit_with_f_plus_one_faults.py
 plenum/test/node_request/test_commit/test_num_of_commit_with_zero_faulty_node.py
 plenum/test/node_request/test_commit/test_num_of_sufficient_commit.py

plenum/test/node_request/test_order/test_request_ordering_2.py

plenum/test/node_request/test_pre_prepare/test_non_primary_sends_a_pre_prepare.py
 plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_f_plus_one_faults.py
 plenum/test/node_request/test_pre_prepare/test_num_of_pre_prepare_with_one_fault.py
 plenum/test/node_request/test_pre_prepare/test_num_of_preprepare_with_zero_faulty_node.py
 plenum/test/node_request/test_pre_prepare/test_num_of_sufficient_preprepare.py
 plenum/test/node_request/test_pre_prepare/test_primary_sends_preprepare_of_high_num.py
 plenum/test/node_request/test_prepare/*

plenum/test/node_request/test_propagate/test_num_of_propagate_with_f_plus_one_faulty_nodes.py
 plenum/test/node_request/test_propagate/test_num_of_propagate_with_one_fault.py
 plenum/test/node_request/test_propagate/test_num_of_propagate_with_zero_faulty_node.py
 plenum/test/node_request/test_propagate/test_num_of_sufficient_propagate.py

plenum/test/node_request/node_request_helper.py
 plenum/test/node_request/test_already_processed_request.py
 plenum/test/node_request/test_different_ledger_request_interleave.py
 plenum/test/node_request/test_quorum_disconnected.py
 plenum/test/node_request/test_quorum_faulty.py
 plenum/test/node_request/test_split_non_3pc_messages_on_batches.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1172,,,,,INDY-1008,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzyra7:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,8.0,,,,,,,,,,,,anikitinDSR,Derashe,,,,,,,,,,,"13/Feb/18 7:02 PM;Derashe;+plenum/test/node_request/test_different_ledger_request_interleave.py+ blocked until pool_transaction tests sdk integration done. PR: [https://github.com/hyperledger/indy-plenum/pull/525];;;","26/Feb/18 2:06 PM;Derashe;Problem reason:

We need to integrate tests of node_request folder with libindy client


 Changes: 
 - tests were integrated and their behaviour stayed the same

little changes to pool_transaction folder: additional ""sdk_"" fixtures and functions which have been used in node_request


 PR:

[https://github.com/hyperledger/indy-plenum/pull/525]


 Version:

master, 257


 Risk factors:

No


 Risk:

Low


 Covered with tests:

No


 Recommendations for QA

the only noticed problem is versioning. Tests require master libindy.;;;",,,,,,,,,,,,,,,,,,,,,,,
Seed for BLS keys has less than 48 bytes of entropy when prepare_seed is used ,INDY-1020,24839,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,lovesh,lovesh,07/Dec/17 7:10 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"In {{crypto/bls/indy_crypto/bls_crypto_indy_crypto.py}} , the {{prepare_seed}} takes a seed and if the seed is not 48 bytes in length, it adds a padding with 0s. This method is used when node keys are generated using the scripts {{scripts/init_indy_keys}} or {{scripts/init_bls_keys}}. The method {{init_bls_keys}} is passed a 32 byte seed intended to be used for generating ed25519 keypair. This can be fixed in several ways, HKDF can be used on the 32 byte seed or rather than expecting/generating 32 bytes of seed, expect/generate 48 bytes of seed which are good for creating BLS keys but discard the first or last 16 bytes of the seed and use for ed25519 keypair generation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-61,,,,,,,,,"1|hzyq7r:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,lovesh,,,,,,,,,,,"21/Dec/17 6:28 PM;ashcherbakov;This is not the case anymore. The expected seed length with the current EC is 32 bytes.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermittently failing test: test_primary_recvs_3phase_message_outside_watermarks,INDY-1021,24855,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,dsurnin,dsurnin,07/Dec/17 9:37 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Total 1 runs 153 passed, 1 failed, 0 errors, 14 skipped_____________ test_primary_recvs_3phase_message_outside_watermarks _____________ perf_chk_patched = <module 'plenum_config.py' from '/tmp/pytest-of-indy/pytest-1/0/etc/indy/plenum_config.py'> chkFreqPatched = <module 'plenum_config.py' from '/tmp/pytest-of-indy/pytest-1/0/etc/indy/plenum_config.py'> looper = <stp_core.loop.looper.Looper object at 0x7f829c67a588> txnPoolNodeSet = [Alpha, Beta, Gamma, Delta], sdk_pool_handle = 137 sdk_wallet_client = (138, '6ouriXMZkLeHsuXrN1X1fd'), reqs_for_logsize = 40 def test_primary_recvs_3phase_message_outside_watermarks(perf_chk_patched, chkFreqPatched, looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, reqs_for_logsize): """""" One of the primary starts getting lot of requests, more than his log size and queues up requests since they will go beyond its watermarks. This happens since other nodes are slow in processing its PRE-PREPARE. Eventually this primary will send PRE-PREPARE for all requests and those requests will complete """""" tconf = perf_chk_patched delay = 5 instId = 0 reqs_to_send = 2 * reqs_for_logsize + 1 logger.debug('Will send \{} requests'.format(reqs_to_send)) npr = getNonPrimaryReplicas(txnPoolNodeSet, instId) pr = getPrimaryReplica(txnPoolNodeSet, instId) from plenum.server.replica import TPCStat orderedCount = pr.stats.get(TPCStat.OrderSent) for r in npr: r.node.nodeIbStasher.delay(ppDelay(delay, instId)) r.node.nodeIbStasher.delay(pDelay(delay, instId)) tm_exec_1_batch = waits.expectedTransactionExecutionTime(len(txnPoolNodeSet)) batch_count = math.ceil(reqs_to_send / tconf.Max3PCBatchSize) total_timeout = (tm_exec_1_batch + delay) * batch_count def chk(): assert orderedCount + batch_count == pr.stats.get(TPCStat.OrderSent) sdk_send_random_and_check(looper, txnPoolNodeSet, sdk_pool_handle, sdk_wallet_client, reqs_to_send) > looper.run(eventually(chk, retryWait=1, timeout=total_timeout)) plenum/test/checkpoints/test_message_outside_watermark1.py:53: _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ stp_core/loop/looper.py:259: in run return self.loop.run_until_complete(what) /usr/lib/python3.5/asyncio/base_events.py:387: in run_until_complete return future.result() /usr/lib/python3.5/asyncio/futures.py:274: in result raise self._exception /usr/lib/python3.5/asyncio/tasks.py:239: in _step result = coro.send(None) stp_core/loop/looper.py:250: in wrapper raise ex stp_core/loop/looper.py:237: in wrapper results.append(await coro) stp_core/loop/eventually.py:183: in eventually raise ex stp_core/loop/eventually.py:156: in eventually res = coroFunc(*args) _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ def chk(): > assert orderedCount + batch_count == pr.stats.get(TPCStat.OrderSent) E AssertionError: assert (0 + 9) == 8 E + where 8 = <bound method Stats.get of \{'CommitSent': 8, 'PrePrepareSent': 8, 'PrepareRcvd': 24, 'OrderSent': 8, 'CommitRcvd': 17, 'PrepareSent': 0, 'PrePrepareRcvd': 0, 'ReqDigestRcvd': 0}>(<TPCStat.OrderSent: 7>) E + where <bound method Stats.get of \{'CommitSent': 8, 'PrePrepareSent': 8, 'PrepareRcvd': 24, 'OrderSent': 8, 'CommitRcvd': 17, 'PrepareSent': 0, 'PrePrepareRcvd': 0, 'ReqDigestRcvd': 0}> = \{'CommitSent': 8, 'PrePrepareSent': 8, 'PrepareRcvd': 24, 'OrderSent': 8, 'CommitRcvd': 17, 'PrepareSent': 0, 'PrePrepareRcvd': 0, 'ReqDigestRcvd': 0}.get E + where \{'CommitSent': 8, 'PrePrepareSent': 8, 'PrepareRcvd': 24, 'OrderSent': 8, 'CommitRcvd': 17, 'PrepareSent': 0, 'PrePrepareRcvd': 0, 'ReqDigestRcvd': 0} = Alpha:0.stats E + and <TPCStat.OrderSent: 7> = <enum 'TPCStat'>.OrderSent plenum/test/checkpoints/test_message_outside_watermark1.py:50: AssertionError",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-462,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzyt6f:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,SeanBohan_Sovrin,Toktar,,,,,,,,,"16/Jan/18 7:23 AM;SeanBohan_Sovrin;[~ashcherbakov] is this at all related to the stability issues?;;;","16/Jan/18 5:08 PM;ashcherbakov;Not sure. I think it's more related to the test itself (we fixed a similar issuer recently). We need to have a look whether the test still fails intermittently (I believe it's not).;;;","10/Oct/18 7:29 PM;Toktar;It was fixed in PR: [https://github.com/hyperledger/indy-plenum/pull/638]

In the scope of INDY-1297, INDY-1298;;;",,,,,,,,,,,,,,,,,,,,,,
Anyone needs to have access to up-to-date Technical overview of plenum and indy,INDY-1022,24887,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,08/Dec/17 6:53 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"- We need to have enough technical documentation available in our Github repos 
- We need to describe how Plenum works, what are key concepts and features, why it was created that way, what are the differences from other blockchains
- We need to have at least one Markdown document in plenum/node repos (doc folder) describing all this
- We need to have a link to this doc from README.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1159,INDY-1158,INDY-1157,INDY-1156,INDY-1155,INDY-1154,INDY-1161,INDY-1160,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzyrcf:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,5.0,,,,,,,,,,,,ashcherbakov,lovesh,,,,,,,,,,,"11/Dec/17 8:06 PM;lovesh;The plan is to divide the documentation into several files each containing a separate topic and referenced from the index file, something that we can later host at readthedocs or something. The topics would be (intentionally skipped the CLI):
*Indy-plenum:*
	1. Communication layer covering curvezmq's use and the Stack (and it's children) abstraction
	2. Storage layer covering ledger, state, merkle tre/ie their supplemantary storage, client's storage and serialisations in ledger and state
	3. Node and its usage (init/destroy) of stacks and other storage
	4. Catchup process
	6. View change process
	7. Monitoring
	8. Kinds of messages and fields and their validation
	9. Request handling covering authentication (cover ClientAuthNr), validation and post ordering (cover RequestHandler class), intentionally skipping RBFT specific parts as the reader can be pointed to the paper, however the changes we have in the 3 phase messages compared to RBFT do need to be called out.
	10. Wallet and signing
	11. Client and request-reply cycle, retries, reply observers
	12. Addition and removal of nodes covered by PoolManager and clients learning about membership changes
	13. BLS, its use during consensus
	14. Config params
*Indy-node:*
	1. ATTRIB txn and its store
	2. NYM txn and IdrCache
	3. SCHEMA and CLAIM_DEF txn
	4. Upgrade process and config txns;;;","11/Dec/17 8:36 PM;lovesh;It is a huge task and i will need some help, like on the input validation and configuration sections;;;","11/Dec/17 9:02 PM;ashcherbakov;The plan looks good in general.
Some comments:
1) I don't see any big value to include client-related docs, since it will be deprecated quite soon (items 10, 11)
2) I think the main Index file should start with a general description of the project from technical point of view listing the main technologies we use (like 'This is a distributed ledger supporting Byzantine Failures where RBFT is used for consensus with ed25519 and BLS signatures support........'
3) I think we need a section about Auth rules and roles.

Also please note that RAET, Plenum CLI and Client code will be deprecated and removed soon.;;;","11/Dec/17 9:07 PM;lovesh;Got it, will not add section about client then, will put a placeholder to point to indy-sdk client doc which can be updated once we have docs for client and wallet in indy-sdk. Will add section about auth rules.;;;","18/Jan/18 9:15 PM;lovesh;I think i should add a doc explaining all quorum values.;;;","09/Feb/18 2:18 AM;ashcherbakov;The main docs were created:

[https://github.com/hyperledger/indy-plenum/tree/master/docs]
[https://github.com/hyperledger/indy-plenum/blob/master/docs/main.md]

A number of tasks for more detailed documentation are created:

INDY-1154, INDY-1155, INDY-1156, INDY-1157, INDY-1158, INDY-1159, INDY-1160, INDY-1161;;;",,,,,,,,,,,,,,,,,,,
Anyone needs access to a public Indy overview from the product point of view ,INDY-1023,24888,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,SeanBohan_Sovrin,ashcherbakov,ashcherbakov,08/Dec/17 6:58 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"- We need to have a documentation describing Indy project from the Product point of view
- It should point to the main goals of Indy, use cases and requirements. 
- It should mention what differs Indy from other Blockchains
- We need to have this doc public on GitHub  (in plenum and node repos).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwwif:",,,,,,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",Sprint 18.04,Sprint 18.05,18.06,18.07 Stability & Monitoring,EV 18.09 Stability-RocksDB,,5.0,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"20/Apr/18 5:05 PM;ashcherbakov;[~SeanBohan_Sovrin] Are we ready to close this?;;;","18/May/18 10:49 PM;esplinr;Things are in a much better state now, and we have other stories for making further improvements. I'm going to close this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,
Logging tests were not run on ci,INDY-1024,24890,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,dsurnin,dsurnin,08/Dec/17 10:18 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,During PR testing tests from logging directory were not run on ci,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypxz:",,,,,,INDY 17.25,,,,,,,,,,,,,,,,,,,,dsurnin,VladimirWork,,,,,,,,,,,"08/Dec/17 11:39 PM;dsurnin;It looks like the tests skip was caused by silent ignore of errors by runner.py during the tests collection phase;;;","11/Dec/17 5:58 PM;dsurnin;The changes were made to runner.py script only;;;","13/Dec/17 11:11 PM;VladimirWork;Build Info:
indy-plenum 1.2.198

Steps to Validate:
1. Make breaking changes in __init__.py in any plenum test directory
-> Run `python runner.py --pytest ""python -m pytest"" --dir plenum --output test-result-plenum-1.ubuntu-01.txt --test-only-slice 1/3`
2. Make breaking changes in conftest.py in any plenum test directory
-> Run `python runner.py --pytest ""python -m pytest"" --dir plenum --output test-result-plenum-1.ubuntu-01.txt --test-only-slice 1/3`
3. Make breaking changes in any `test_` file in any plenum test directory
-> Run `python runner.py --pytest ""python -m pytest"" --dir plenum --output test-result-plenum-1.ubuntu-01.txt --test-only-slice 1/3`

Actual Results:
runner.py throws `errors during collection` in all cases, so tests can be run without breaking changes in .py files only.;;;",,,,,,,,,,,,,,,,,,,,,,
Pool stopped working and lost consensus while new node was performing a catch-up,INDY-1025,24894,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,krw910,krw910,09/Dec/17 4:08 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,blocked,ViewChange,,"The pool lost it's ability to reach consensus while a new node was performing a catch-up.
I don't have detailed logs only at the info level.

*Setup*
I have pool of 13 nodes with 5,022 transactions. I was adding 3 more nodes to the pool (14, 15, 16)

*Steps*
# I added Node14 and let it perform a catch up before adding the next node
# I added Node15 to the pool after Node14 was at 5,022 transactions
# I then added Node16 after Node15 was at 5,022 transactions

*Other Info*
* The catch-up is pretty fast so I run ""read_ledger --type domain --count"" around every 20 - 30 seconds to see when it has completed.
* The ledger tool was displaying the incorrect ledger count (this is a different issue) while performing a catch-up. I jumped from 12 to 6,000 transactions (more than what the pool has) and then to 9,337. 
* The ledger on Node16 settled at 4,688 on Node16 and did not change.
* I sent a new transaction from the CLI on a different machine and the pool stopped taking transactions.

*{color:#d04437}Error{color}*
With only info level debugging this is all I captured
{code}
(  29) | discard | Node1 discarding message INSTANCE_CHANGE{'reason': 26, 'viewNo': 1} because Received instance change request with view no 1 which is not more than its view no 1
{code}

It appears that a view change might have been attempted while Node16 was performing a catch-up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1034,INDY-1054,INDY-1095,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzx0zz:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,ozheregelya,,,,,,,,,,"20/Dec/17 8:20 PM;ashcherbakov;[~krw910] Do you have logs? Can you please attach them or provide a link?;;;","12/Jan/18 12:16 AM;ashcherbakov;We hope it's fixed in the scope of INDY-1018;;;","19/Jan/18 2:31 AM;krw910;Blocked by INDY-1095
;;;","26/Jan/18 2:17 AM;ozheregelya;*Environment:*
indy-node 1.2.279
AWS QA live pool (25 nodes)

*Steps to Validate:*
1. Setup each node using generate_indy_pool_transactions script.
2. Start all nodes at once.
3. Run load test several times.

*Actual Results:*
Pool successfully wrote 25000 transactions. 

*Additional Information:*
Pool setup in steps differs from initial procedure in description. Initial case will be verified in scope of INDY-1095.;;;",,,,,,,,,,,,,,,,,,,,,
read_ledger tool gives incorrect information during catch up of a node,INDY-1026,24897,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,krw910,krw910,09/Dec/17 5:36 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"While a node is performing a catch up the read_ledger tool gives incorrect information. The ledger count can exceed the actual pool ledger count.

*Steps*
I had a pool with 5,022 transactions
Added a new node to the pool
While the node was performing a catch up I executed the read_ledger tool to get the ledger count. 

*{color:#d04437}Issue{color}*
The information returned while catching up was incorrect and exceed the actual size of the ledger. The actual ledger size is 5,022 but you will see counts over 6,000 while catching up.

{code}
ubuntu@irelandQALive16:~$ sudo su - sovrin -c""read_ledger --type domain --count""
12
ubuntu@irelandQALive16:~$ sudo su - sovrin -c""read_ledger --type domain --count""
6000
ubuntu@irelandQALive16:~$ sudo su - sovrin -c""read_ledger --type domain --count""
9337
ubuntu@irelandQALive16:~$ sudo su - sovrin -c""read_ledger --type domain --count""
4688
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzyt73:",,,,,,,,,,,,,,,,,,,,,,,,,,Derashe,krw910,,,,,,,,,,,"07/Nov/18 9:51 PM;Derashe;QA did not notice these things in actual code.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Unable to add BLS key to ledger through CLI,INDY-1027,24900,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,ashcherbakov,krw910,krw910,09/Dec/17 7:24 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"After upgrading a pool I was unable to send the node transaction through the CLI to add the new BLS key to the ledger.

{color:#d04437}Short Version I receive the following error through the CLI{color}
{code}
Node request failed with error: client request invalid: InvalidClientRequest('validation error [ClientNodeOperationData]: unknown field - bls_key=<full key shown here>
{code}

*Setup*
4 Node pool
1 Client Machine

*{color:#205081}1. Installed current Stable build and configured pool using the live pool settings in the sovrin_config.py file{color}*

*Starting Version*
indy-plenum=1.1.27
indy-anoncreds=1.0.10
indy-node=1.1.43
sovrin=1.1.6

*{color:#205081}2. Used the pool upgrade transaction without force=True to upgrade the pool to indy-node 1.2.50{color}*

*Upgrade Version*
libindy-crypto=0.1.6-10
indy-plenum=1.2.29
indy-anoncreds=1.0.11
indy-node=1.2.50
sovrin=1.1.7

*{color:#205081}3. Manually upgraded the CLI machine{color}*
sudo apt install indy-node -y
sudo apt install sovrin -y

*+Post Upgrade Steps to Enable BLS Keys+*

{code}
Sometime later each Steward will need to run the following command line to add their BLS Keys
    1. From the Validator Node
        a. Switch to the indy user
            sudo su - indy
	b. init_bls_keys --name NAME --seed SEED
            The --seed a 48 character input and is what is used to create the BLS key. NOTE: This is not your Steward or Node seed

Example with seed
init_bls_keys --name Node1 --seed 000000000000000000001111111111111111111111111111

        c. Capture the Stdout 
BLS Public key is 3AfkzUZVn2WT9mxW2zQXMgX39FXSY5qzohnMVpdvNS5KSath1YG5Ux4u9ubTFTaP6W55XX9Yx7xPWeYos489oyY53WzwNBG7X4o32ESnZ9xacLmNsQLBjqc6oqpWGTbEXv4edFTrZ88n93sEh4fjFhQMumaXxDfWJgd9aj7KCSpf38F

    2. From the CLI
        a. Manually upgrade CLI
            $ sudo apt install indy-node -y
            $ sudo apt install sovrin -y
	b. The first time running the upgraded CLI you will be prompted to migrated your previous settings.
	c. Now you will be sending a Node transaction like when you added the node to the pool. We will be adding the BLS key as a new parameter to the command to update the pool ledger with public key.

	d. Launch the CLI
            $ indy
	e. Connect to the pool
            $ connect live
	f. Set your Steward as the signer in the CLI
            $ indy@live> use DID <Steward DID>
            Example:
                $ indy@live> use DID Th7MpTaRZVRYnPiabds81Y
	g. Send the updated node information with the BLS key
            $ indy@live> send NODE dest=<node_dest> data={'alias':'<node name>', 'bls_key': 'key_generated_by_init_bls_keys'}

            Example:
                $ indy@live> send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data={'alias':'Node1','bls_key': '3AfkzUZVn2WT9mxW2zQXMgX39FXSY5qzohnMVpdvNS5KSath1YG5Ux4u9ubTFTaP6W55XX9Yx7xPWeYos489oyY53WzwNBG7X4o32ESnZ9xacLmNsQLBjqc6oqpWGTbEXv4edFTrZ88n93sEh4fjFhQMumaXxDfWJgd9aj7KCSpf38F'}
{code}

*{color:#d04437}This is where you will get the following error{color}*
{code}
Node request failed with error: client request invalid: InvalidClientRequest('validation error [ClientNodeOperationData]: unknown field - bls_key=3AfkzUZVn2WT9mxW2zQXMgX39FXSY5qzohnMVpdvNS5KSath1YG5Ux4u9ubTFTaP6W55XX9Yx7xPWeYos489oyY53WzwNBG7X4o32ESnZ9xacLmNsQLBjqc6oqpWGTbEXv4edFTrZ88n93sEh4fjFhQMumaXxDfWJgd9aj7KCSpf38F',)
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzx0uv:",,,,,,,,,,,,,,,,,,,,,,,,,,krw910,,,,,,,,,,,,"09/Dec/17 8:04 AM;krw910;[~ashcherbakov] I found that I was given incorrect steps instead of bls_key it is just blskey. So I am marking this as invalid.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support did:sov prefix for DIDs,INDY-1028,25709,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,13/Dec/17 5:22 PM,14/Jan/20 11:17 PM,28/Oct/23 2:47 AM,14/Jan/20 11:17 PM,,,,,,0,,,,"We need to support fully qualified DIDs in the form `did:sov:<value>`, not just `<value>`.
* If we see just <value>, we can assume the prefix is “did:sov:” 
* If we see a prefix that isn’t “did:sov:“, we can potentially raise an error about an unsupported DID type.

Note that some DID formats may make different assumptions than Sovrin does about the length of a DID, whether it is case-sensitive, etc. We need to get in the habit of thinking of DIDs as being fully qualified. Our sample files should show this. Our wallet APIs should handle it. Etc. This is the foundation for us being able to have a single wallet/agent/agency that supports DIDs from the production Sovrin ledger as well as a test network--and it is the foundation for eventual interop with other ledgers like Bitcoin and Ethereum.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-51,,,,,,,,,"1|hzwx4f:2m",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"14/Jan/20 11:17 PM;esplinr;We do not want to store the prefix on the ledger, rather we allow LibIndy to translate between ledger DIDs and fully qualified DIDs. This allows more flexibility should one decide to fork the ledger with a different prefix.;;;",,,,,,,,,,,,,,,,,,,,,,,,
One of added nodes doesn't catch up,INDY-1029,25714,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,13/Dec/17 10:04 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Overview:
One of added nodes doesn't catch up.

Build Info:
indy-node 1.2.234

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~20000 txns.
3. Add the 5th node.
4. Send NYM txn.
5. Add the 6th node.
6. Send NYM.

Actual Results:
Node 5 doesn't catch up (but Node 6 does and pool works normally).

Expected Results:
Node 5 should catch up.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1018,,,,,,,,,,,,,,,,,,,,"11/Jan/18 6:24 PM;VladimirWork;1and2.7z;https://jira.hyperledger.org/secure/attachment/14128/1and2.7z","11/Jan/18 6:24 PM;VladimirWork;3and4.7z;https://jira.hyperledger.org/secure/attachment/14129/3and4.7z","11/Jan/18 6:24 PM;VladimirWork;5and6.7z;https://jira.hyperledger.org/secure/attachment/14130/5and6.7z","11/Jan/18 6:20 PM;VladimirWork;INDY-1029.PNG;https://jira.hyperledger.org/secure/attachment/14127/INDY-1029.PNG","12/Jan/18 12:15 AM;VladimirWork;Node1.PNG;https://jira.hyperledger.org/secure/attachment/14131/Node1.PNG","18/Jan/18 1:06 AM;VladimirWork;Node1.log;https://jira.hyperledger.org/secure/attachment/14215/Node1.log","18/Jan/18 1:06 AM;VladimirWork;Node2.log;https://jira.hyperledger.org/secure/attachment/14216/Node2.log","18/Jan/18 1:06 AM;VladimirWork;Node3.log;https://jira.hyperledger.org/secure/attachment/14217/Node3.log","18/Jan/18 1:06 AM;VladimirWork;Node4.log;https://jira.hyperledger.org/secure/attachment/14218/Node4.log","12/Jan/18 12:15 AM;VladimirWork;Node5.PNG;https://jira.hyperledger.org/secure/attachment/14132/Node5.PNG","18/Jan/18 1:06 AM;VladimirWork;Node5.log;https://jira.hyperledger.org/secure/attachment/14219/Node5.log","18/Jan/18 1:11 AM;VladimirWork;Node6.7z;https://jira.hyperledger.org/secure/attachment/14220/Node6.7z","12/Jan/18 12:15 AM;VladimirWork;Node6.PNG;https://jira.hyperledger.org/secure/attachment/14133/Node6.PNG","13/Dec/17 10:03 PM;VladimirWork;logs.7z;https://jira.hyperledger.org/secure/attachment/13703/logs.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzytyv:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,anikitinDSR,VladimirWork,,,,,,,,,,,"11/Jan/18 6:24 PM;VladimirWork; !INDY-1029.PNG|thumbnail! [^1and2.7z]  [^3and4.7z]  [^5and6.7z] ;;;","12/Jan/18 12:16 AM;VladimirWork;This issue is not reproducing in 1.2.265 master. !Node1.PNG|thumbnail!  !Node5.PNG|thumbnail!  !Node6.PNG|thumbnail! ;;;","17/Jan/18 5:55 PM;anikitinDSR;Problem reason: 
- One of added nodes doesn't catch up on pool with big number of transactions

Changes: 
- Change timeout logic evaluating for resending CATCHUP_REQ messages if there is no qourum for CATCHUP_REP

PR:
- https://github.com/hyperledger/indy-plenum/pull/499


Version:
- master

Risk factors:
- frequently resending for very huge domain ledger

Risk:
- Low

Recommendations for QA
 * Install pool of 4 nodes.
 * Run load tests to fill ledger with ~20000 txns.
 * Add the 5th node.
 * Send NYM txn.
 * Add the 6th node.
 * Send NYM.;;;","18/Jan/18 1:06 AM;VladimirWork;Build Info:
indy-node 1.2.273

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~80000 txns.
3. Add the 5th node.
4. Send NYM txn.
5. Add the 6th node.

Actual Results:
Node 5 catches up and reaches consensus. Node 6 doesn't catch up. [^Node1.log]  [^Node2.log]  [^Node3.log]  [^Node4.log]  [^Node5.log] ;;;","18/Jan/18 10:37 PM;VladimirWork;Build Info:
indy-node 1.2.275

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load tests to fill ledger with ~80000 txns.
3. Add the 5th node.
4. Send NYM txn.
5. Add the 6th node.
6. Send NYM.

Actual Results:
Both nodes catch up and reach consensus successfully.;;;",,,,,,,,,,,,,,,,,,,,
"Issues with stability of pools, nodes, agents",INDY-1032,25727,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,SeanBohan_Sovrin,SeanBohan_Sovrin,14/Dec/17 6:45 AM,25/Dec/19 9:35 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-1,,Stability,To Do,,,,,,,"1|hzyvhz:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Problems with nodes demotion during load test,INDY-1033,25729,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ozheregelya,ozheregelya,14/Dec/17 7:20 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Case 1:
 Steps to Reproduce:

1. Setup the pool of 7 nodes.
 2. Write several transactions.
 3. Disconnect one of nodes (not primary) using following command in CLI:
 send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data=\{'alias': 'Node1', 'services': []}
 4. Send 1000 tnx using load test (based on indy-sdk).
 5. Connect disconnected node back:
 send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data=\{'alias': 'Node1', 'services': ['VALIDATOR']}
 6. Send 1000 tnx using load test.

Actual Results:
 Node which was disconnected stopped processed transactions (it wrote 3027 when the rest ones wrote 3046).

Expected Results:
 Node which was disconnected should work same as another ones.

There is a problem to collect extended logs for case 1 because of case 2, so there are only logs with info level for this case: [^case1_logs.7z]

Case 2:
 Steps to Reproduce:
 1. Setup the pool of 4 nodes.
 2. Demote one of nodes.
 3. Try to write 1000 tnx using load test.

Actual Results:
 Pool is broken after writing 300 tnx. Following messages appear in logs:
{code:java}
2017-12-14 11:12:08,017 | WARNING | replica.py (821) | dispatchThreePhaseMsg | Node3:0 stashing 3 phase message PREPREPARE{'reqIdr': [['Th7MpTaRZVRYnPiabds81Y', 1513249927978175705]], 'viewNo': 1, 'ledgerId': 1, 'instId': 0, 'digest': '41597a8d40cc9ae0505aaa50fceb3fd84d2d81b48a200af9b19461f732420a28', 'ppSeqNo': 301, 'txnRootHash': 'AVHPJs38TuQ3puSHBRrvCYz51fMUratYLYn9UHgkBnBV', 'ppTime': 1513249928, 'stateRootHash': 'FYhgkCWazT92EMzrzfRRwZHqh66tDiHfZgPmk1j1jKPF', 'blsMultiSig': ['RXUoQnE2XWcbke2ekzRyx2aaWmpVUzByNkDzMmHxVNuamw8MdYFgcsFPQqvmBNM1ck5YazZ16Kk4kHT5SJwmQFKMybZF5WYDhXZ1n6FKheSSccT48onVEW5WupBkHymAzUpWokoiN38miTKNP3VXVtkQz6o1oHPCcaWC9Be1oRJWaB', ['Node4', 'Node2', 'Node3'], [1, '3yg9fiR6inj6S9GTJbEpYmCKyynhw9CjBFAs4QzVk5Kb', 'GYrER3ijU71jQeiy1uW8ftfagXatTwPps9wMUVa4ZQyt', '8iA1MpeX34en2UVmMtESoMDEJZ2MdQHN7A7enh2yfdDm', 1513249927]], 'discarded': 1} since ppSeqNo 301 is not between 0 and 300{code}
Expected Results:
 Pool should work.

Extended logs for Case 2: [^case2_extended_part1.7z][^case2_extended_part2.7z][^case2_extended_part3.7z]

Logs for Case 2: [^Node1.log] [^Node2.log][^Node3.log] [^Node4.log]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-757,,,,,INDY-1018,INDY-1141,,,,,,,,,,,,,,"14/Dec/17 8:28 PM;ozheregelya;Node1.log;https://jira.hyperledger.org/secure/attachment/13705/Node1.log","14/Dec/17 8:28 PM;ozheregelya;Node2.log;https://jira.hyperledger.org/secure/attachment/13706/Node2.log","14/Dec/17 8:28 PM;ozheregelya;Node3.log;https://jira.hyperledger.org/secure/attachment/13707/Node3.log","14/Dec/17 8:28 PM;ozheregelya;Node4.log;https://jira.hyperledger.org/secure/attachment/13708/Node4.log","14/Dec/17 11:30 PM;ozheregelya;case1_logs.7z;https://jira.hyperledger.org/secure/attachment/13712/case1_logs.7z","14/Dec/17 11:12 PM;ozheregelya;case2_extended_part1.7z;https://jira.hyperledger.org/secure/attachment/13709/case2_extended_part1.7z","14/Dec/17 11:12 PM;ozheregelya;case2_extended_part2.7z;https://jira.hyperledger.org/secure/attachment/13710/case2_extended_part2.7z","14/Dec/17 11:12 PM;ozheregelya;case2_extended_part3.7z;https://jira.hyperledger.org/secure/attachment/13711/case2_extended_part3.7z","20/Jan/18 12:55 AM;VladimirWork;logs.tar.gz;https://jira.hyperledger.org/secure/attachment/14332/logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyvrj:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,dsurnin,ozheregelya,VladimirWork,,,,,,,,,,"12/Jan/18 8:50 PM;dsurnin;the issue might be fixed with 1018
so needs to be retested with latest master with plenum v215;;;","16/Jan/18 3:02 AM;ozheregelya;[~dsurnin], Case 1 is still reproducing on version 1.2.270.


Case 2 can't be rechecked now because of unclear problem with load scripts:
{code:java}
indy@de69716a8962:~/perf$ python3 Perf_Add_nyms.py -n 1000
Traceback (most recent call last):
 File ""Perf_Add_nyms.py"", line 9, in <module>
 from indy import ledger, signus, wallet, pool
ImportError: No module named 'indy'{code};;;","19/Jan/18 6:01 PM;dsurnin;Problem reason:
  quorum for checkpoint txn was 2f

Changes:
  change quorum to n-f-1

Versions:
  master plenum 224
  master node 277

Risk factors:
  watermarks, checkpoints, catch up

Risk:
 Med/

Covered with tests:
  plenum/test/primary_selection
  plenum/test/checkpoints;;;","20/Jan/18 12:57 AM;VladimirWork;Build Info:
indy-node 1.2.279

Steps to Reproduce:
1. Setup the pool of 7 nodes.
2. Write several transactions.
3. Disconnect one of nodes (not primary) using following command in CLI:
send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data={'alias': 'Node1', 'services': []}
4. Send 1000 tnx using load test (based on indy-sdk).
5. Connect disconnected node back:
send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data={'alias': 'Node1', 'services': ['VALIDATOR']}
6. Send 1000 tnx using load test.

Actual Results:
Node which was disconnected stopped processed transactions (it wrote 1631 when the rest ones wrote 2021). Logs from 1st (demoted/promoted back) and 2nd, 3rd nodes are in attachment. [^logs.tar.gz] ;;;","26/Jan/18 3:26 AM;ozheregelya;One more problem with lagging node.
(Case 3) On pool of 25 nodes Node1 was lagged without any demotions/promotions. Note that Node11 was not connected to the pool.
 Logs for this problem: [https://drive.google.com/file/d/1Lv6VrHJl4j64b44xnj__HdyEOBCxyOfZ/view?usp=sharing]

(Case 4) On pool of 7 nodes Node7 was lagged without any demotions/promotions.
 Logs for this case: [https://drive.google.com/open?id=1sFt8n-69h-8vdvT2kuXpXQsAev1zJ4W4]

In both of cases *steps for reproduce* are:
 1. Setup nodes using generate_indy_pool_transactions.
 2. Start nodes at once.
 3. Run load test.;;;","30/Jan/18 6:12 PM;VladimirWork;Issue is not reproducing on 1.2.287 master.;;;","30/Jan/18 10:21 PM;dsurnin;According to logs with 7 nodes it looks like that lagged nodes works correctly and most probably they just need some time to get all the missed txns;;;","31/Jan/18 3:45 PM;dsurnin;About logs for pool of 25 nodes - all the logs shows that all the nodes committed the same seqNo, so there are now any lagging according to logs;;;","31/Jan/18 4:01 PM;dsurnin;Probably it should be mentioned that read_ledger is not really a precise way to detect that ledgers are the same. Due to leveldb limitations to read from levelbd we first copy all files to different folder and then traverse all the records one by one. It is not the fastest way and actual ledger contents could be changed significantly to the end of read_ledger script.
So probably to check the ledgers equality read_ledger script should be called only in case of pool without any load and several times with some time span.;;;","31/Jan/18 7:18 PM;dsurnin;According to discussion with Alexandr and Andrey K it looks like better way to monitor ledgers equality is validator-info
but it also should be run without a load and several times;;;","01/Feb/18 5:13 AM;ozheregelya;Ticket for read_ledger problems: INDY-1117

Case 1: was retested by [~VladimirWork] on version 1.2.287 - ok.
Case 2: INDY-1095 - ok.
Case 3: this was not a problem on node side, it was a problem with read_ledger script (INDY-1117). Validator-info showed that all nodes successfully wrote 350356 transactions. - ok.

*Case 4 - not ok:*
It doesn't looks like problem from INDY-1117 - there are no old read_ledger data:
{code:java}
root@e2878f6f0526:/home/indy# sudo find / -type d -name ""*-read-copy""
root@e2878f6f0526:/home/indy# {code}
But results of read_ledger and validator-info are different:
{code:java}
root@e2878f6f0526:/home/indy# read_ledger --type domain --count
703
root@e2878f6f0526:/home/indy# validator-info 
Validator Node7 is running
Validator DID: BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW
Verification Key: 4u9hRgKH6daKcEWb6x9zhsM1G6X93VoXSJHppaeJhdivrHWpKvVtUj9
Node Port: 9713
Client Port: 9714
Metrics:
 Uptime: 26 minutes, 22 seconds
 Total Ledger Transactions: 569
 Total Pool Transactions: 7
 Read Transactions/Seconds: 0.00
 Write Transactions/Seconds: 0.35
Reachable Hosts: 7/7
Unreachable Hosts: 0/7{code}
Anyway, count of transactions on 7th node is less than count of transactions on another ones.

 

There is no load on this pool. Node is working since  {{Wed Jan 31 14:47:10 UTC 2018}} but it still have not wrote remaining transactions.

So, I can't close this ticket for now. It needs in additional discussion and exploration.;;;","02/Feb/18 10:24 PM;VladimirWork;The most of cases are done and it looks like in Case 4 we face another issue with watermarks so INDY-1141 is reported according to the last discussion.;;;",,,,,,,,,,,,,
View change issue stopped pool from accepting new transactions,INDY-1034,25732,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,krw910,krw910,14/Dec/17 3:08 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,blocked,ViewChange,,"I have a pool of 25 Nodes
I am running load testing scripts that call libindy
Across 5 machines I am running 200 threads making add nym calls to the pool.
I set the scripts to add a total of 100,000 nyms
After 20941 transactions the pool stopped accepting new transactions.

I did not have debug level turned on in the logs, but they are all showing the same message:
*{color:#d04437}Issue{color}*
{code}
2017-12-14 02:04:37,350 | INFO     | message_processor.py (  29) | discard | Node2 discarding message INSTANCE_CHANGE{'viewNo': 1, 'reason': 25} because Received instance change request with view no 1 which is not more than its view no 1
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1025,,,,,INDY-1054,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzx107:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,ozheregelya,,,,,,,,,,"20/Dec/17 7:04 AM;krw910;This may be the same issue as INDY-1054;;;","20/Dec/17 8:21 PM;ashcherbakov;[~krw910] Do you have logs? Can you please attach them or provide a link?;;;","12/Jan/18 12:16 AM;ashcherbakov;We hope it's fixed in the scope of INDY-1018;;;","19/Jan/18 2:32 AM;krw910;Blocked by INDY-1095;;;","26/Jan/18 2:26 AM;ozheregelya;*Environment:*
indy-node 1.2.279
AWS QA live pool (25 nodes)

*Steps to Validate:*
1. Setup each node using generate_indy_pool_transactions script.
2. Start all nodes at once.
3. Run load test several times.

*Actual Results:*
Pool successfully wrote 25000 transactions. 

*Additional Information:*
Steps to validate differ from initial procedure in description. Initial case will be verified in scope of INDY-1095.;;;",,,,,,,,,,,,,,,,,,,,
Do not allow update of existing Schema,INDY-1035,25735,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Dec/17 7:12 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"As Schema is identified by (NAME, VERSION, ORIGIN), but `schema_seq_no` is one of identifiers of CLAIM_DEF, we need to disallow any updates of existing Schemas.
If one needs to make changes in a Schema, he needs to create a new one with a new Version (or with a new name).",,,,,,,,,,,,,,,,,IS-454,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/17 10:03 PM;VladimirWork;INDY-1035.PNG;https://jira.hyperledger.org/secure/attachment/13946/INDY-1035.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzypuv:",,,,,,INDY 18.01: Stability+,,,,,,,,1.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"22/Dec/17 6:16 PM;ashcherbakov;*Changes*
Added static validation to not allow creation of a new Schema with the same NAME, VERSION and ORIGIN, so that a Schema with a given seq_no can be uniquely identified by a triple (NAME, VERSION, ORIGIN), and we can use seq_no or the triple for Schema identification.

*Risk*
Low

*PR*
https://github.com/hyperledger/indy-node/pull/502

*Build*
indy-node master 1.2.250

*Recommendation for QA*
check that it's not possible to create a Schema with the same NAME and VERSION from CLI (try both Python and libindy CLI);;;","22/Dec/17 10:02 PM;VladimirWork;Build Info:
indy-node 1.2.250

Steps to Validate:
1. Try to add a Schema with the same NAME and VERSION by a single user.
2. Check that Schema from Step 1 can be added by another user (but also one time only).

Actual Results:
Schema with the same NAME, VERSION and ORIGIN (issuer DID) parameters can't be added more than one time (both python and libindy CLi throws errors in this case).

Additional Info:
Python CLI also throws one stacktrace line in addition to readable error `_ensureReqCompleted failed; not trying any more because 20 seconds have passed; args were (('XhYtvJqezMUKfF6KVNaGmT', 1513944915910751), 9hcpLr7br9Nw7tMudKSbKtdWpfdfSqFVjkDdpjjrcFVb, <function _submitData at 0x7f7b65f61f28>)` but it is not an issue since we have new libindy CLI.;;;",,,,,,,,,,,,,,,,,,,,,,,
Configure zmq to limit number of simultaneous clients connections,INDY-1037,25743,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,andkononykhin,andkononykhin,15/Dec/17 12:02 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,Currently we don't limit number of clients connections and it already caused such issue as INDY-986. Need to review our logic of zmq configuration and improve it if possible. If not we should consider to review zmq socket type (ROUTER) that we use for node server,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-986,,,,,INDY-570,INDY-1087,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzytzr:",,,,,,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,,andkononykhin,ashcherbakov,SeanBohan_Sovrin,,,,,,,,,,"23/Jan/18 6:52 AM;SeanBohan_Sovrin;Can this get us to the replacement for Indy 1093? [~andkononykhin] ?;;;","24/Jan/18 12:36 AM;andkononykhin;Yes, I think. Actually INDY-1087 made this task done. But I'm not sure about ""agent-to-agent functions"" mentioned in INDY-1093. [~ashcherbakov] what do you think?;;;","24/Jan/18 12:54 AM;ashcherbakov;I believe it can be replaced by INDY-1093 (or https://jira.hyperledger.org/browse/INDY-1085 which is a design for this);;;",,,,,,,,,,,,,,,,,,,,,,
[Refactor] Move initialization of ledgers from Node class,INDY-1038,25745,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,15/Dec/17 1:50 AM,09/Oct/19 5:10 PM,28/Oct/23 2:47 AM,09/Oct/19 5:10 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzwyqv:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"09/Oct/19 5:10 PM;ashcherbakov;Done in the scope of other tasks;;;",,,,,,,,,,,,,,,,,,,,,,,,
Create Entity Relationship Diagram of current ledger objects including primary and secondary keys used to link each object together.,INDY-1039,25746,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,nage,nage,nage,15/Dec/17 2:02 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Create a diagram showing all the data objects that exist on the ledger, and are especially important for the identity protocol to use and reference the objects on the ledger (schema, claim definition, revocation registry, nym+attrs/did document, claim request, claim, proof request and proofs) so that we can demonstrate that the object retrieval calls on the ledger are properly formed and match the main use cases in libindy and elsewhere.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzypu7:",,,,,,INDY 18.01: Stability+,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,nage,,,,,,,,,,,"28/Dec/17 10:42 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/508

Added the following docs and diagrams (see docs folder):

* Relationship diagram (plantuml + png)
* Transactions format doc
* Requests format doc
* Code quality guideline
* Indy File folder structure
* Helper Scripts doc
* Updated README
;;;","10/Jan/18 7:45 PM;ashcherbakov;Looks good from Nathan's side.;;;",,,,,,,,,,,,,,,,,,,,,,,
Script clear_node.py throws permission error,INDY-1044,26102,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Won't Do,,VladimirWork,VladimirWork,16/Dec/17 12:34 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Overview:
Script clear_node.py throws permission error.

Build Info:
indy-node 1.2.241

Steps to Reproduce:
1. Run `clear_node.py --full FULL` on the node.

Actual Results:
Script *works* but throws error with traceback:
{quote}Traceback (most recent call last):
  File ""/usr/local/bin/clear_node.py"", line 30, in <module>
    clean(config, args.full, args.network)
  File ""/usr/local/bin/clear_node.py"", line 20, in clean
    shutil.rmtree(config_helper.ledger_base_dir)
  File ""/usr/lib/python3.5/shutil.py"", line 478, in rmtree
    onerror(os.rmdir, path, sys.exc_info())
  File ""/usr/lib/python3.5/shutil.py"", line 476, in rmtree
    os.rmdir(path)
PermissionError: [Errno 13] Permission denied: '/var/lib/indy'{quote}

Expected Results:
Script should work without any errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Dec/17 12:33 AM;VladimirWork;clear_node.PNG;https://jira.hyperledger.org/secure/attachment/13902/clear_node.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzypyv:",,,,,,,,,,,,,,,,,,,,,,,,,,Derashe,nage,VladimirWork,,,,,,,,,,"12/Jan/18 7:23 AM;nage;Please modify this script to work with the install packages' standard permissions;;;","07/Nov/18 10:01 PM;Derashe;Duplicated in INDY-1217;;;",,,,,,,,,,,,,,,,,,,,,,,
Transactions added to nodes in STN during system reboot.,INDY-1045,26111,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,VladimirWork,mgbailey,mgbailey,16/Dec/17 5:23 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"A new Ubuntu software update required a reboot of the nodes of the STN.  The nodes were rebooted one at a time.  Previous to the reboot, the validator-info utility was run and the nodes were in sync, with 255 transactions in the domain ledger on each.  After the reboot, the validator-info utility was run again.  To our surprise, additional transactions had appeared on each node.  Of the seven nodes we have access to, five have 327 transactions, and two have 299.  We need to determine why this occurred and correct it.

The 'korea' node, which is one of the nodes with 299 transactions, has trace-level logging enabled.  Its logs and transactions are attached.  The logs and transactions for 'virginia', a node with 327 transactions, are also attached. ","STN, running version 1.1.43",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-959,,,,,INDY-1047,INDY-1121,,,,,,,,,,,,,,"16/Dec/17 6:02 AM;mgbailey;Screen Shot 2017-12-15 at 12.31.08 PM.png;https://jira.hyperledger.org/secure/attachment/13905/Screen+Shot+2017-12-15+at+12.31.08+PM.png","16/Dec/17 6:02 AM;mgbailey;Screen Shot 2017-12-15 at 12.31.33 PM.png;https://jira.hyperledger.org/secure/attachment/13904/Screen+Shot+2017-12-15+at+12.31.33+PM.png","16/Dec/17 6:02 AM;mgbailey;Screen Shot 2017-12-15 at 12.32.05 PM.png;https://jira.hyperledger.org/secure/attachment/13903/Screen+Shot+2017-12-15+at+12.32.05+PM.png","16/Dec/17 6:04 AM;mgbailey;korea.tgz;https://jira.hyperledger.org/secure/attachment/13907/korea.tgz","16/Dec/17 6:02 AM;mgbailey;virginia.tgz;https://jira.hyperledger.org/secure/attachment/13906/virginia.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzypuf:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,mgbailey,spivachuk,VladimirWork,,,,,,,,,,"29/Dec/17 1:20 AM;spivachuk;*Problem reason:*
The issue is the same as one described in INDY-959. The only difference is that in INDY-959 there were pool transactions with duplicated client requests, while in this bug there are domain transactions with duplicated client requests.
For example, for the request {{('VjyDVzQ5s7rJ8HZTgEdvT5', 1510007293268391)}} in {{korea.log}} we can see the following events occurred on 2017-12-15:
- At 18:59:49,557 {{korea}} received PREPREPARE with this client request for the instance 0 from {{virginia}}.
- At 18:59:49,561 {{korea}} sent MESSAGE_REQUEST for PROPAGATE of this client request to all the others.
- At 18:59:50,390 {{korea}} received MESSAGE_RESPONSE with PROPAGATE of this client request from {{virginia}}.
- At 19:01:44,606 {{korea}} received MESSAGE_REQUEST for PROPAGATE of this client request from {{virginia}}.
- At 19:01:44,616 {{korea}} sent MESSAGE_RESPONSE with PROPAGATE of this client request to {{virginia}}.
- At 19:01:45,089 {{korea}} received PROPAGATE of this client request from {{virginia}}.

So we can see that {{virginia}} requested PROPAGATE of the client request which it propagated just now. If we look into {{virginia.log}} we can see the following events occurred on 2017-12-15:
- At 18:59:50,987 {{virginia}} committed this client request into the ledger.
- At 19:01:20,013 {{virginia}} was initiating after the restart.
- At 19:01:45,808 {{virginia}} committed this client request into the ledger once again.

So after the restart {{virginia}} committed once again the client request that it had already committed before the restart. This was so because {{virginia}} was not able to detect that it saw this client request earlier (as we have noticed in {{korea.log}} above). If there were the fix made in scope of INDY-959 then, already having the transaction in the ledger, {{virginia}} would process neither the request received directly from the client nor the propagated request (the request received directly from the client would be just replied basing on the current data in the ledger). Hence {{virginia}} would not commit this client request into the ledger again.

*Problem state:*
The bug with a lack of a check of the request presence in seqNoDB on processing of the belated PROPAGATE message was fixed in scope of INDY-959.;;;","10/Jan/18 6:24 PM;VladimirWork;Build Info:
indy-node 1.2.261

Steps to Validate:
1. Install pool of 4..7 nodes.
2. Send about 250 txns to the pool.
3. Check the pool/domain ledger count.
4. Shutdown and start all machines in the pool (docker container *stop and start* if docker pool is used).
5. Check the pool/domain ledger count.

Actual Results:
Pool and domain counts after machines' rebooting are the same as before.;;;",,,,,,,,,,,,,,,,,,,,,,,
Wrong config path is used in vagrant validator script,INDY-1046,26126,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ozheregelya,ozheregelya,18/Dec/17 2:06 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"[https://github.com/evernym/sovrin-environments/blob/master/vagrant/training/vb-multi-vm/scripts/validator.sh]

 
{code:java}
#--------------------------------------------------------
echo 'Fixing Bugs'
if grep -Fxq '[Install]' /etc/systemd/system/indy-node.service
then
#--------------------------------------------------------
echo 'Fixing Bugs'
if grep -Fxq '[Install]' /etc/systemd/system/indy-node.service
then
echo '[Install] section is present in indy-node target'
else
perl -p -i -e 's/\\n\\n/[Install]\\nWantedBy=multi-user.target\\n/' /etc/systemd/system/indy-node.service
fi
if grep -Fxq 'SendMonitorStats' /home/indy/.indy/indy_config.py
then
echo 'SendMonitorStats is configured in indy_config.py'
else
echo 'SendMonitorStats = False' >> /home/indy/.indy/indy_config.py
fi
chown indy:indy /home/indy/.indy/indy_config.py
#--------------------------------------------------------
_

 echo '[Install] section is present in indy-node target'
else
 perl -p -i -e 's/\\n\\n/[Install]\\nWantedBy=multi-user.target\\n/' /etc/systemd/system/indy-node.service
fi
if grep -Fxq 'SendMonitorStats' /home/indy/.indy/indy_config.py
then
 echo 'SendMonitorStats is configured in indy_config.py'
else
 echo 'SendMonitorStats = False' >> /home/indy/.indy/indy_config.py
fi
chown indy:indy /home/indy/.indy/indy_config.py
#--------------------------------------------------------
{code}
File /home/indy/.indy/indy_config.py doesn't exist after file/folder changes.

 ",,,,,,,,,,INDY-1064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-987,,,,,,,,,"1|hzypt3:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,VladimirWork,,,,,,,,,,"18/Dec/17 2:33 AM;ozheregelya;This problem actual only for master version.;;;","10/Jan/18 2:00 AM;ashcherbakov;Please note that it needs to be fixed (if needed) in https://github.com/hyperledger/indy-node/tree/master/environment;;;","10/Jan/18 2:04 AM;ashcherbakov;It's aleady fixed in the scope of INDY-1055, INDY-1060, INDY-1062;;;","11/Jan/18 11:02 PM;ashcherbakov;Fixed in https://github.com/hyperledger/indy-node/pull/517;;;","15/Jan/18 10:39 PM;VladimirWork;Build Info:
indy-node 1.2.270

Steps to Validate:
1. Build pool from stable branch with vagrant.
2. Build pool from master branch with vagrant.
3. Check shell output.

Actual Results:
Pools from both branches were built successfully. There is no error with `/home/indy/.indy/indy_config.py`.;;;",,,,,,,,,,,,,,,,,,,,
Nodes write duplicated txns in domain ledger,INDY-1047,26132,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,VladimirWork,VladimirWork,18/Dec/17 7:22 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Overview:
Nodes write duplicated txns in domain ledger due to primary node disconnection.

Build Info:
indy-node 1.2.242

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Stop the 4th node -> send two NYMs -> start the 4th node.
3. Stop the 3rd node -> send two NYMs -> start the 3rd node.
4. Stop the 2nd node -> send two NYMs -> start the 2nd node.
5. Send one more NYM to prove that pool works after all nodes are online.
6. Check the domain ledger (there are no duplicated entries now).
7. Stop the 1st node (primary) -> send two NYMs -> start the 1st node.
8. Check the domain ledger (there are 2 copies in ledger now).
9. Stop the 2nd node (primary) -> send two NYMs -> start the 2nd node.
10. Check the domain ledger (there are another 2 copies in ledger now, total incorrect entries count is 4).

Actual Results:
There are duplicated entries in domain ledger after Step 6 (17th and 18th entries) and Step 8 (21st and 22nd entries):

bq. [10,{""dest"":""542MVr22zcHbVyGzaXmbT1"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587806654444,""signature"":""XF8BLcAR8hyFHmKmuT83vCxnqx4LMVJiRibRmFfU66mssWSxTDW71RwnNGUzCRxmU9ztRbG5QVdwtyxw7t9d7xs"",""signatures"":null,""txnTime"":1513587806,""type"":""1""}]
bq. [11,{""dest"":""542MVr22zcHbVyGzaXmbT2"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587816791437,""signature"":""2dwAdkc1ZzLdvNa1ijnert7o4paPptxsb3itvxQ2B1XMHHYVhZrZQDgzSPi8czyUjAiv4FZwSGC41jCmzyi5YPWm"",""signatures"":null,""txnTime"":1513587816,""type"":""1""}]
bq. [12,{""dest"":""542MVr22zcHbVyGzaXmbT3"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587891520062,""signature"":""5NBJ2kakB94XNsHbvWr99tFNdrE32HgpEwq9Rm5NCCf8AqZPdb3eW8DUGnrYod1u4Px6hWaexCpJp3UQCSEnamgq"",""signatures"":null,""txnTime"":1513587891,""type"":""1""}]
bq. [13,{""dest"":""542MVr22zcHbVyGzaXmbT4"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587896717719,""signature"":""2juhEkJgSbEEVkUzgrjMoBC7D4AVbvuPp6xSTqJ9vX8TNWF17XFVHFpSoN5vTzaQZihNRNaa569fL847xXrSd2gy"",""signatures"":null,""txnTime"":1513587896,""type"":""1""}]
bq. [14,{""dest"":""542MVr22zcHbVyGzaXmbT5"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587961287460,""signature"":""55dkV1wX9BkNAERff1nGEnDRMVooXrP4wXhiMAv5LaJbUjgcMHMvff3DHVEz25tYL2RuAoShwprNbj3xLaAHiPhP"",""signatures"":null,""txnTime"":1513587961,""type"":""1""}]
bq. [15,{""dest"":""542MVr22zcHbVyGzaXmbT6"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587970341952,""signature"":""4zoVKp7rtQWp42ueNz13GsRkHewSPtpPKUZwz1hP9JSwDY86bez7LxmejAw595uncSBYAtCReiVTyUbnHTVEhKVV"",""signatures"":null,""txnTime"":1513587970,""type"":""1""}]
bq. [16,{""dest"":""542MVr22zcHbVyGzaXmbT7"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588002055730,""signature"":""5NDPnug9SeDKFZYZATi1YnmQnS1x74anyesm8oCuXBy5gCTzDi5aPPKv3NHZpseEUPxRHQ3C3QpDa6F6kZdxT1RV"",""signatures"":null,""txnTime"":1513588002,""type"":""1""}]
bq. [17,{""dest"":""542MVr22zcHbVyGzaXmbT6"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587970341952,""signature"":""4zoVKp7rtQWp42ueNz13GsRkHewSPtpPKUZwz1hP9JSwDY86bez7LxmejAw595uncSBYAtCReiVTyUbnHTVEhKVV"",""signatures"":null,""txnTime"":1513588246,""type"":""1""}]
bq. [18,{""dest"":""542MVr22zcHbVyGzaXmbT5"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587961287460,""signature"":""55dkV1wX9BkNAERff1nGEnDRMVooXrP4wXhiMAv5LaJbUjgcMHMvff3DHVEz25tYL2RuAoShwprNbj3xLaAHiPhP"",""signatures"":null,""txnTime"":1513588246,""type"":""1""}]
bq. [19,{""dest"":""542MVr22zcHbVyGzaXmbT8"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588275511311,""signature"":""4sSAy95wERSvD3MnrtcRDJPQMbZHU52QACCJjgw8eJzhu5LVgCtvTFKWcTWY2oUpmSW79uETDdzFmqowHS63a1BR"",""signatures"":null,""txnTime"":1513588275,""type"":""1""}]
bq. [20,{""dest"":""542MVr22zcHbVyGzaXmbT9"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588278446878,""signature"":""3pChMCk7CT91F19H68ci3n8jLq59UnzmFGGLdab3kWKrTAcw7TH7pTQoWVFCQU6Ch2dXvVxAqs9vHJckyhwprXLb"",""signatures"":null,""txnTime"":1513588278,""type"":""1""}]
bq. [21,{""dest"":""542MVr22zcHbVyGzaXmbT1"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587806654444,""signature"":""XF8BLcAR8hyFHmKmuT83vCxnqx4LMVJiRibRmFfU66mssWSxTDW71RwnNGUzCRxmU9ztRbG5QVdwtyxw7t9d7xs"",""signatures"":null,""txnTime"":1513588474,""type"":""1""}]
bq. [22,{""dest"":""542MVr22zcHbVyGzaXmbT2"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513587816791437,""signature"":""2dwAdkc1ZzLdvNa1ijnert7o4paPptxsb3itvxQ2B1XMHHYVhZrZQDgzSPi8czyUjAiv4FZwSGC41jCmzyi5YPWm"",""signatures"":null,""txnTime"":1513588474,""type"":""1""}]
bq. [23,{""dest"":""542MVr22zcHbVyGzaXmb88"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588500569414,""signature"":""2VdVRpGbNE5svubpwbF5mj7CvH1v3Q4756Gp8HJL4ksTMmVKxFKANm69EURrbQujt7xLaVqM6EtjKE4SSgY9jLy1"",""signatures"":null,""txnTime"":1513588500,""type"":""1""}]
bq. [24,{""dest"":""542MVr22zcHbVyGzaXmb99"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588502898016,""signature"":""3581rf1fq6fu1mHh6j1tQWbTV2bZocKhyioDHhoa1Kk4PwVkx4aGH1PvmBdWwv6zhjut7AuthDLhbz58uoH7dFxL"",""signatures"":null,""txnTime"":1513588502,""type"":""1""}]
bq. [25,{""dest"":""542MVr22zcHbVyGzaXm888"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588630115778,""signature"":""2ZPtUm6TcthAW1ymbFHh68HVcryQbEuyQQtbxPUDBYapqn9mVAhimNVcrYQza3LiDkMqTEYPXGZRi3r8wWy5cADU"",""signatures"":null,""txnTime"":1513588630,""type"":""1""}]
bq. [26,{""dest"":""542MVr22zcHbVyGzaXm999"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588648939221,""signature"":""2BsR5DvexXYtrVsxQgo5TQPBLHoNDzVz1GHYiB69QQF38B8F5patejpUPrGEHNqmbnbLLm2yfhtQSmQULdRexeVK"",""signatures"":null,""txnTime"":1513588648,""type"":""1""}]
bq. [27,{""dest"":""542MVr22zcHbVyGzaXm111"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1513588734955980,""signature"":""4eRMaYjULxXy836HTAbYJVzE5EcfrBmcGiidhLhHY5Piyd8J1kGopFemr8s44MkgNMFRuhofFF9vxZug3qn1wRvJ"",""signatures"":null,""txnTime"":1513588734,""type"":""1""}]

Expected Results:
There should be no spontaneous duplicated entries in ledger.",,,,,,,,,,,,,,,,,,,,,,,,INDY-1091,,,,,INDY-1045,,,,,,,,,,,,,,,,,,,,"18/Dec/17 7:21 PM;VladimirWork;Node1.log;https://jira.hyperledger.org/secure/attachment/13913/Node1.log","18/Dec/17 7:21 PM;VladimirWork;Node2.log;https://jira.hyperledger.org/secure/attachment/13912/Node2.log","18/Dec/17 7:21 PM;VladimirWork;Node3.log;https://jira.hyperledger.org/secure/attachment/13911/Node3.log","18/Dec/17 7:21 PM;VladimirWork;Node4.log;https://jira.hyperledger.org/secure/attachment/13910/Node4.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyt2v:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,dsurnin,ozheregelya,VladimirWork,,,,,,,,,,"15/Jan/18 6:08 PM;dsurnin;It seems fixed with INDY-1045
Does not reproduced on latest master;;;","16/Jan/18 3:21 AM;ozheregelya;Version info:
indy-node=1.2.270

Steps to Validate:
1. Install pool of 4 nodes.
2. Stop the 4th node -> send two NYMs -> start the 4th node.
3. Stop the 3rd node -> send two NYMs -> start the 3rd node.
4. Stop the 2nd node -> send two NYMs -> start the 2nd node.
5. Send one more NYM to prove that pool works after all nodes are online.
6. Check the domain ledger (there are no duplicated entries now).
7. Stop the 1st node (primary) -> send two NYMs -> start the 1st node.
8. Check the domain ledger (there are 2 copies in ledger now).
9. Stop the 2nd node (primary) -> send two NYMs -> start the 2nd node.
10. Check the domain ledger (there are another 2 copies in ledger now, total incorrect entries count is 4).

Actual Results:
Ledgers don't contain any duplicated entries:
{code:java}
[1,{""dest"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""0"",""type"":""1"",""verkey"":""~CoRER63DVYnWZtK8uAzNbx""}]
[2,{""dest"":""Th7MpTaRZVRYnPiabds81Y"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""2"",""type"":""1"",""verkey"":""~7TYfekw4GUagBnBVCqPjiC""}]
[3,{""dest"":""EbP4aYNeTHL6q385GuVpRV"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""2"",""type"":""1"",""verkey"":""~RHGNtfvkgPEUQzQNtNxLNu""}]
[4,{""dest"":""4cU41vWW82ArfxJxHkzXPG"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""2"",""type"":""1"",""verkey"":""~EMoPA6HrpiExVihsVfxD3H""}]
[5,{""dest"":""TWwCRQRZ2ZHMJFn9TzLp7W"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""2"",""type"":""1"",""verkey"":""~UhP7K35SAXbix1kCQV4Upx""}]
[6,{""dest"":""7JhapNNMLnwkbiC2ZmPZSE"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~LgpYPrzkB6awcHMTPZ9TVn""}]
[7,{""dest"":""MEPecrczs4Wh6FA12u519D"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~A4rMgHYboWYS1DXibCgo9W""}]
[8,{""dest"":""EAPtwgevBpzP8hkj9sxuzy"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~gmzSzu3feXC6g2djF7ar4""}]
[9,{""dest"":""LuL1HK1sDruwkfm68jrVfD"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~Nyv9BKUJuvjgMbfbwk8CFD""}]
[10,{""dest"":""462p8mtcX6jpa9ky565YEL"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~LCgq4hnSvMvB8nKd9vgsTD""}]
[11,{""dest"":""UVLJt5JgDbwD9UpmVHeqNz"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~FDehdDJinZ3JKrrW7jgeBx""}]
[12,{""dest"":""NoiFAetbXCk4neFqkzimKb"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~MDiarGFWGxXd4xww5GHb4T""}]
[13,{""dest"":""9KGhYEq4RwBNHmUY7wh62x"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~R4UeJHE3SytQ42kgKrmBSX""}]
[14,{""dest"":""DwS8R7jwsNqRuRABoqkCit"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~8uScWK5cv45stvttizAmAy""}]
[15,{""dest"":""CuNNCEQS3gN1o8fqoggZy9"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""type"":""1"",""verkey"":""~StdxSAKmgm9dEsAWb1pHEW""}]
[16,{""dest"":""V4SGRU86Z58d6TV7PBUe11"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516039870076406,""signature"":""o3AdQDbMPqjHzaymcuP5Y3mFMCcmeBn2PsSh99LiteGxKYXJ4uiZ9JxRAWS88YMFh4AxjF2BuqdnQ4C2zrQx5Jq"",""signatures"":null,""txnTime"":1516039870,""type"":""1""}]
[17,{""dest"":""V4SGRU86Z58d6TV7PBUe12"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516039872034800,""signature"":""4PJBcbu7mks9fYNo4MNP25h3NYqWKDMmR3gJkw2tR4sPZ152jFySESHawJzDG6sZn8Qc8pD6jgxTmx6gsxVPME2s"",""signatures"":null,""txnTime"":1516039872,""type"":""1""}]
[18,{""dest"":""V4SGRU86Z58d6TV7PBUe13"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516039897485869,""signature"":""vzUhNXSJQoThSA9tCL6XjT6DQGW3bgNHZpWiZYEUPX9uJQfcp8ALMEQc8ZvcXai4qZKnjikm2JxuCM9gZyyzSCW"",""signatures"":null,""txnTime"":1516039897,""type"":""1""}]
[19,{""dest"":""V4SGRU86Z58d6TV7PBUe14"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516039902423367,""signature"":""5nAGriFn2LsuwV5jErsVR5pYCnifUMZHaWRwkRw9REnqg815iFWjNqqJqFfRzffexo9MkjYJ8SAbPQR4RgHezqnP"",""signatures"":null,""txnTime"":1516039902,""type"":""1""}]
[20,{""dest"":""V4SGRU86Z58d6TV7PBUe15"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516039967389768,""signature"":""KvePEnNLZsDHDLuHFFuKz8Vd259WXSxuVbeQfPdyeACm5kRGW9Jh8na2R3eG1mq2QRenG7ifDZaaUv6FuVxicu2"",""signatures"":null,""txnTime"":1516039967,""type"":""1""}]
[21,{""dest"":""V4SGRU86Z58d6TV7PBUe16"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516039970491876,""signature"":""12A9SSdqe5XDPPCTixvsw6TkyrMszTH6eJAbPzD3uQao9dSUG3GerNFuaTzyAnQsSY6SiyVL3Df93U1k4HWEfWM8"",""signatures"":null,""txnTime"":1516039970,""type"":""1""}]
[22,{""dest"":""V4SGRU86Z58d6TV7PBUe21"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516040072739395,""signature"":""26Zq7Xoh2quNtoyjvmErCy6yA3GhoiMBafs9Su8FhTC6xFizh7gEHyZM2N6sSswdsAdPtKf3DEupi235cQbxxksg"",""signatures"":null,""txnTime"":1516040072,""type"":""1""}]
[23,{""dest"":""V4SGRU86Z58d6TV7PBUe22"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516040077665600,""signature"":""3j7RE4u8ppSbQPfrdFmKaJ6vhJCCqgsQHXqP3Vn2m11pzM12WWvUBPEbqRTHJ2S9aTYAjcDk1MSbHRaaE1Dk9w3J"",""signatures"":null,""txnTime"":1516040077,""type"":""1""}]
[24,{""dest"":""V4SGRU86Z58d6TV7PBUe23"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516040165532960,""signature"":""3tS4FzQGcZiWd9aKXKbNhhjabXRfztAsT4sMKypBz3pPtM2ZzPWLB37QVPRbwJJ7uC5LqrTTacGB9bLDxrQBFyhc"",""signatures"":null,""txnTime"":1516040165,""type"":""1""}]
[25,{""dest"":""V4SGRU86Z58d6TV7PBUe24"",""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1516040168218473,""signature"":""2DwWZUErKHnKoTPeg6JCbCexxLYG4x9L8iXNF3MrvDkSZwu8xh2xs4X45i3ftdyRd3AKauqnWwDao9ZZEAM14qRa"",""signatures"":null,""txnTime"":1516040168,""type"":""1""}]{code};;;",,,,,,,,,,,,,,,,,,,,,,,
generate_indy_pool_transactions can be run only by indy user,INDY-1048,26133,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ozheregelya,ozheregelya,ozheregelya,18/Dec/17 8:41 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Steps to Reproduce:
 1. Setup the machine with installed sovrin package.
 2. Set yourself as any user differ from _indy_.
 3. Try to generate transactions to connect to some test network:
{code:java}
generate_indy_pool_transactions --nodes 4 --clients 4 --ips '10.20.30.201,10.20.30.202,10.20.30.203,10.20.30.204'{code}
*Actual Results:*
 Permission denied error appear for file /etc/indy/indy_config.py, transaction file was not generated.

*Expected Result:*
 generate_indy_pool_transactions should work without errors.

*Additional Information:*
 Ticket priority is Highest because this problem appears in agent installation script for vagrant, so one performing GST is impossible on vagrant pool.

Problem is actual only for master version of vagrant scripts. Stable version of vagrant scripts work well.

Problem also reproduces on the latest version for not vagrant setup.",indy-node 1.2.242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-987,,,,,,,,,"1|hzypvb:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,dsurnin,ozheregelya,,,,,,,,,,,"18/Dec/17 8:42 PM;ozheregelya;[~krw910], [~danielhardman], FYI.;;;","19/Dec/17 1:52 AM;ozheregelya;Note that this problem also reproduces for current RC: indy-node 1.2.50.;;;","21/Dec/17 11:05 PM;dsurnin;fix should be included to node build version 247;;;","26/Dec/17 7:38 PM;ozheregelya;*Case 1:*
Steps to Validate:
1. Setup the machine with installed sovrin package.
2. Set yourself as any user differ from indy.
3. Try to generate genesis transactions for client:
{code:java}
generate_indy_pool_transactions --nodes 4 --clients 4 --ips '10.20.30.201,10.20.30.202,10.20.30.203,10.20.30.204'{code}

Actual Results:
Genesis files were successfully generated.

 

*Case 2:*
Steps to Validate:
1. Setup the machine with installed sovrin package.
2. Set yourself as any user differ from indy.
3. Try to generate transactions for node:

 
{code:java}
generate_indy_pool_transactions --nodes 4 --clients 4 --nodeNum 1 --ips '10.20.30.201,10.20.30.202,10.20.30.203,10.20.30.204' 
{code}
=> Permission denied error appear.
4. To workaround this permission denied error, add current user to _indy_ group and retry generation of genesis files.

Actual Results:
Genesis files were generated, but owner of indy configs was changed to current user. But we assume that only _indy_ user can configure node (confirmed with [~krw910]), so this case is not included in current workflow and this behavior is acceptable.;;;",,,,,,,,,,,,,,,,,,,,,
Static code analysis to find the cause of performance issues,INDY-1049,26136,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,18/Dec/17 10:38 PM,30/Jan/18 7:09 AM,28/Oct/23 2:47 AM,,,,,,,0,explore,,,"As a follow-up of INDY-974 and INDY-977,
we need to perform static code analysis to find out a possible reason of issues with performance.
We need to figure out some obvious implementation inaccuracy like unnecessary iteration over all records in ledgers and so on.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-974,INDY-977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwylz:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
 Single-node read/write ledger operations without hard disk (in-memory),INDY-1050,26137,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,ashcherbakov,ashcherbakov,18/Dec/17 10:40 PM,30/Jan/18 7:09 AM,28/Oct/23 2:47 AM,,,,,,,0,blocked,explore,,"As a follow-up of INDY-974 and INDY-977,
we need to perform single-node read/write ledger operations without hard disk (in-memory).
At this stage we can determine problems in implementation of tree algorithms by analysis of decreasing of number of read/write operations per second caused by growing number of written records.",,,,,,,,,,INDY-1073,,,,,,,,,,,,,,,,,,,INDY-974,INDY-977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwym7:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Single-node read/write ledger operations with hard disk (including levelDB),INDY-1051,26138,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,ashcherbakov,ashcherbakov,18/Dec/17 10:41 PM,30/Jan/18 7:09 AM,28/Oct/23 2:47 AM,,,,,,,0,explore,,,"As a follow-up of INDY-974 and INDY-977,
we need to perform single-node read/write ledger operations with hard disk (including levelDB).
At this stage we can analyse levelDB settings and hard disk usage, try to find the ways to minimise disk I/O operations (use caches, I/O batching etc.).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-974,INDY-977,,,,,,,,,,,,,,,,,,,"30/Dec/17 12:10 AM;VladimirWork;Untitled spreadsheet - Sheet2.pdf;https://jira.hyperledger.org/secure/attachment/13956/Untitled+spreadsheet+-+Sheet2.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwymf:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"27/Dec/17 5:52 PM;VladimirWork;Direct NYM txns adding in leveldb ledger was performed via `add_json_txns_to_ledger.py`:

| 5 minutes | 25k txns total added | 1st 25k part was added in 5 minutes |
| 20 minutes | 50k txns total added | 2nd 25k was part was added in 15 minutes |
| 45 minutes | 75k txns total added | 3rd 25k part was added in 25 minutes |
| 1 hour 20 minutes | 100k txns total added | 4th 25k part was added in 35 minutes |

So we can see that direct adding json requests to ledger (without consensus reaching and network delays) degrades due to increasing amount of ledger txns.;;;","30/Dec/17 12:10 AM;VladimirWork;HDD/SSD check:  [^Untitled spreadsheet - Sheet2.pdf] ;;;",,,,,,,,,,,,,,,,,,,,,,,
 Multi-node read/write ledger operations using isolated local network,INDY-1052,26139,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,ashcherbakov,ashcherbakov,18/Dec/17 10:53 PM,30/Jan/18 7:09 AM,28/Oct/23 2:47 AM,,,,,,,0,explore,,,"As a follow-up of INDY-974 and INDY-977,
we need to investigate multi-node read/write ledger operations using isolated local network.

We can investigate functionality of isolated pool with different number of nodes without influence of global routing and check implementation of requests processing and RBFT influence.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-974,INDY-977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwymn:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"29/Dec/17 10:34 PM;VladimirWork;Build Info:
indy-node 1.2.255
4 node pool setup

Actual Results:
100k NYM txns are added in ~3.9 hours (14000 seconds average, 4 client machines with 25 threads on each sending 1000 NYMs by each thread).;;;",,,,,,,,,,,,,,,,,,,,,,,,
 Multi-node read/write ledger operations using global network,INDY-1053,26140,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,ashcherbakov,ashcherbakov,18/Dec/17 11:01 PM,30/Jan/18 7:09 AM,28/Oct/23 2:47 AM,,,,,,,0,explore,,,"As a follow-up of INDY-974 and INDY-977,
we need to analyse multi-node read/write ledger operations using global network.
We need to investigate influence of global routing and AWS infrastructure, comparing throughput degradation with isolated pool.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-974,INDY-977,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwymv:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"28/Dec/17 1:09 AM;VladimirWork;Build Info:
indy-node 1.2.252
7 node pool setup

Actual Results:
100k NYM txns are added in ~3.5 hours (12600 seconds average, 4 client machines with 25 threads on each sending 1000 NYMs by each thread).;;;","29/Dec/17 7:16 PM;VladimirWork;Build Info:
indy-node 1.2.253
4 node pool setup

Actual Results:
100k NYM txns are added in ~4 hours (14200 seconds average, 4 client machines with 25 threads on each sending 1000 NYMs by each thread).;;;",,,,,,,,,,,,,,,,,,,,,,,
View Change on large pools of 19 or more nodes can cause pool to stop functioning,INDY-1054,26144,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,krw910,krw910,19/Dec/17 6:06 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,blocked,ViewChange,,"It appears that view change on pools of 19 nodes or more can cause the pool to stop functioning.

*Setup*
I have a pool with 25 nodes
After adding all the nodes to the pool from a genesis of 7 nodes I had 30 total transactions in the ledger.

*Test*
I set the load scripts to run transactions every 6 - 30 seconds for an hour. 
I have 25 clients across 5 machines each sending 10 transaction bursts every 6 - 30 seconds. Each will send 1400 total transactions which should create activity on the pool for about an hour. 

*{color:#d04437}Issue{color}*
After 524 transaction had been sent the pool stopped taking new transactions. Each node match the other nodes and the log files did not have any errors. I did see in some logs a view change was being requested so I believe this has to do with view change on a large pool.

*{color:#205081}Logs and Screenshots{color}*
# Attached are the smaller logs and the current ones when the nodes failed.
# Also attached is a screenshot of a search I did in the logs. I searched for ""proposed_view_change"". Only 12 of the 25 nodes have a propose view change and it looks like Node 7 was going to run a different view change ID 991 not ID 992 that the other show.

Some of what I see in the logs around the same time stamp are the following
*Node 12*
{code}
| has_action_queue.py  (  36) | _schedule | Node12 scheduling action propose_view_change with id 992 to run in 2 seconds
| checkInstances | Node12 choosing to start election on the basis of count 19 and nodes {'Node20', 'Node1', 'Node5', 'Node19', 'Node24', 'Node7', 'Node14', 'Node6', 'Node9', 'Node21', 'Node4', 'Node16', 'Node17', 'Node18', 'Node25', 'Node3', 'Node8', 'Node22'}
{code}

*Node 14*
{code}
| onConnsChanged | Node14 lost connection to primary of master
| lost_master_primary | Node14 scheduling a view change in 2 sec
{code}

*Node 15*
{code}
| set_status | Node15 changing status from started to started_hungry
| checkInstances | Node15 choosing to start election on the basis of count 24 and nodes {'Node23', 'Node5', 'Node7', 'Node6', 'Node14', 'Node24', 'Node18', 'Node12', 'Node2', 'Node20', 'Node10', 'Node22', 'Node4', 'Node21', 'Node8', 'Node25', 'Node9', 'Node17', 'Node19', 'Node16', 'Node3', 'Node11', 'Node1'}
{code}

*Node 17*
{code}
| set_status | Node17 changing status from started to started_hungry
| checkInstances | Node17 choosing to start election on the basis of count 24 and nodes {'Node4', 'Node1', 'Node9', 'Node5', 'Node10', 'Node14', 'Node15', 'Node3', 'Node8', 'Node6', 'Node23', 'Node7', 'Node2', 'Node20', 'Node16', 'Node11', 'Node18', 'Node24', 'Node25', 'Node19', 'Node21', 'Node22', 'Node12'}
{code}

*Node 18*
{code}
| onConnsChanged | Node18 lost connection to primary of master
| lost_master_primary | Node18 scheduling a view change in 2 sec
{code}

*Node 22*
{code}
| onConnsChanged | Node22 lost connection to primary of master
| lost_master_primary | Node22 scheduling a view change in 2 sec
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1034,INDY-1025,,,,,,,,,,,,,,,,,,,"16/Jan/18 6:07 PM;VladimirWork;AWS_logs_1_2_272_master.7z;https://jira.hyperledger.org/secure/attachment/14201/AWS_logs_1_2_272_master.7z","19/Dec/17 6:03 AM;krw910;DeadPool Logs.7z;https://jira.hyperledger.org/secure/attachment/13921/DeadPool+Logs.7z","19/Dec/17 6:03 AM;krw910;ViewChange25Node.JPG;https://jira.hyperledger.org/secure/attachment/13920/ViewChange25Node.JPG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzx0zj:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,krw910,ozheregelya,spivachuk,VladimirWork,,,,,,,"20/Dec/17 7:04 AM;krw910;This may be the same issue as INDY-1034;;;","28/Dec/17 9:32 PM;dsurnin;[~krw910]

Could you please describe the machine each node run on? RAM, cpu cores, etc.

I would like to have journalctl logs from each machine.

Also could you please reproduce it once again and monitor memory usage on nodes with ps or top?;;;","30/Dec/17 12:22 AM;spivachuk;[~krw910], as to different IDs of {{propose_view_change}} actions on different nodes - specifically this is not an issue. Such an ID is not a view number. It is an identifier of an action added to the action queue of a node. So these IDs are not to be the same on different nodes.;;;","11/Jan/18 9:58 PM;dsurnin;according to logs the issue could be connected with INDY-1018 - adding new node to the pool with big ppseqno could lead to situation when node can catch up but cannot take part in consensus

the fix is ready for test, so could you please retest it with latest master;;;","12/Jan/18 3:29 AM;spivachuk;3PC-batches stopped to be ordered by the master protocol instance since the batch (1, 301) because this batch and the following batches in the view 1 were considered as laying outside the watermarks by master replicas of more than a third of the nodes. On these replicas the watermarks were staying at (0, 300] despite the previous batches including (1, 300) were ordered.;;;","16/Jan/18 6:00 PM;ashcherbakov;The issue reproduced again during adding of more nodes;;;","16/Jan/18 6:07 PM;VladimirWork;AWS QA Live pool logs from 1st, 2nd and 19th nodes. [^AWS_logs_1_2_272_master.7z] ;;;","18/Jan/18 1:37 AM;ashcherbakov;*Problem reason*
 * Once a Node started, it starts a timer (60 sec by default) to check it it's connected to a Primary. This is needed to be able to select a Primary when all nodes except the first one are started (since we use round robin for Primary selection, 1st node is the first primary, and it may be not started).
 * If a primary for a new node still don't have a Primary after 60 sec, it sends INSTANCE_CHANGE. It used to send it unconditionally, not checking if the node is actually participating and ready, that is connected to all nodes in the Pool. But it doesn't make sense to send it, if it hasn't yet connected (the Primary is there, this is the new node which is not connected yet).
 * In the test scenario above, all new Nodes are started one by one, but NODE txn is not sent for a while.
 So, each node sent INSTANCE_CHANGE because of primary 'disconnection' (it wasn't able to connect to the Primary since other nodes don't accept such a connection without NODE txn sent).
 Each node sent INSTANCE_CHANGE once is started participating.
 After a number of new nodes, there was a quorum of INSTANCE_CHANGE messages (actually old and incorrect messages), so ViewChange happened.
 But the quorum was only for the first 7 nodes in the pool (not added manually). For new nodes it was not a real ViewChange, but propagate primary logic (since they didn't receive enough INSTANCE_CHANGE msgs, but received f+1 ViewChangeDone messages from the first 7 Nodes with a view greater than their current one).
 So, ViewChange finished for the new nodes (they got a quorum of ViewChangeDone), and, since it's Propagate Primary ViewChange, they didn't send ViewChangeDone to others.
 First 7 nodes had only 7 ViewChangeDone, which is not enough for their ViewChange quorum (n-f) since they experience a real View Change based on INSTANCE_CHANGE messages, not primary propagation as new nodes.
 As a result, pool becomes stalled.

*Changes*
 * do not send INSTANCE_CHANGE because of Primary disconnection if node is not ready yet (that is doesn't have enough connections).

*PR*
 * [https://github.com/hyperledger/indy-plenum/pull/502]

*New Tests*
 * `[test_no_instance_change_before_node_is_ready.py|https://github.com/hyperledger/indy-plenum/pull/502/files#diff-22101ab3b6b3508dd4f6c9d3f87aa3cd]`

*Risk*
 * low

*Recommendations for QA*
 * add nodes one by one to the pool (add sufficient number of nodes, ~15)
 * do not send NODE txn immediately for each node

*Build*

- master indy-node 1.2.275

 

 ;;;","19/Jan/18 2:31 AM;krw910;Blocked by INDY-1095;;;","26/Jan/18 2:24 AM;ozheregelya;*Environment:*
indy-node 1.2.279
AWS QA live pool (25 nodes)

*Case 1:*
*Steps to Validate:*
1. Setup each node using generate_indy_pool_transactions script.
2. Start all nodes at once.
3. Run load test several times.

*Actual Results:*
Pool successfully wrote 25000 transactions. 

*Case 2:*
*Steps to Validate:*
1. Setup 7 nodes using generate_indy_pool_transactions script.
2. Add 18 nodes to the pool one by one.
3. Initiate view change.
4. Check that pool works.

*Actual Results:*
Pool works after adding 18 nodes and view change. 

*Additional Information:*
Steps to validate differ from initial procedure in description. Initial case will be verified in scope of INDY-1095.;;;",,,,,,,,,,,,,,,
Move scripts from sovrin-environment to one of Indy repo,INDY-1055,26156,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Dec/17 12:38 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"We need to move Docker, Vagrant and other scirpts from sovrin-environment to one of Hyperledger Indy's team (need to decide to which one).
Update the documentation correspondingly.
",,,,,,,,,,,,,,,,,INDY-1064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-987,,,,,,,,,"1|hzyptz:",,,,,,INDY 18.01: Stability+,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,krw910,mgbailey,VladimirWork,,,,,,,,,"20/Dec/17 9:37 PM;ashcherbakov;[~nage] [~danielhardman] [~krw910] [~tharmon] [~SeanBohan_Sovrin]
Some of the current scripts in sovrin-environment rely on `sovrin` packages, not indy-node (the only difference is in initial genesis transactions).
Can we copy all scripts as is, and then re-factor them in a separate task to get rid of Sovrin dependency?;;;","22/Dec/17 11:14 PM;krw910;[~ashcherbakov] Check with [~mgbailey] because he was just working on fixing those scripts. I don't know if he put in a PR already or if he was waiting until things moved.;;;","23/Dec/17 1:31 AM;mgbailey;[~ashcherbakov], if it is desirable to change to the indy-node package instead of sovrin, that is fine. It will require changes to documentation to add instructions to get the sovrin package in cases where the STN or live network genesis files are needed.;;;","29/Dec/17 12:37 AM;ashcherbakov;Changes:
- Moved scripts for sovrin-environment to https://github.com/hyperledger/indy-node/tree/master/environment as it is (just merged master and stable in sovrin-environment)
- Docker points to stable packages by default.

PR: 
- https://github.com/hyperledger/indy-node/pull/510

Version:
-  1.2.255

Recommendations for QA:
- check that everything works as before (in sovrin-environment), at least Docker and Vagrant.;;;","11/Jan/18 5:19 PM;VladimirWork;Build Info:
indy-node master branch 1.2.261

Steps to Validate:
1. Git clone/pull indy-node master.
2. Set up docker environment from `~/indy-node/environment/docker/pool`.
3. Set up vagrant environment from `~/indy-node/environment/vagrant/training/vb-multi-vm` (works for Windows only).

Actual Results:
Both environments are set up successfully. Minor issues found in vagrant environment will be reported as separate tickets.;;;",,,,,,,,,,,,,,,,,,,,
"""Running a Simulation of a Indy Cluster and Agents"" instruction does not work",INDY-1056,26157,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,ozheregelya,ozheregelya,20/Dec/17 12:41 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,documentation,,,"*Case 1:*

*Steps to Reproduce:*
 1. {{$ pip install -U --no-cache-dir indy-client}}

=> Most probably this is mistake in instruction because indy-client does not exist. If this command runs with indy-node instead of indy-client, following error appear:
{code:java}
Command ""/usr/bin/python3 -u -c ""import setuptools, tokenize;__file__='/tmp/pip-build-zsono534/Charm-Crypto/setup.py';exec(compile(getattr(tokenize, 'open', open)(__file__).read().replace('\r\n', '\n'), __file__, 'exec'))"" install --record /tmp/pip-uyz7neml-record/install-record.txt --single-version-externally-managed --compile --user --prefix="" failed with error code 1 in /tmp/pip-build-zsono534/Charm-Crypto/{code}
To workaround this error, need to install Charm-Crypto
 [https://github.com/hyperledger/indy-anoncreds#prerequisites-for-debian-based-systems]

 

So, need to fix installation of indy-node, or add step with Charm-Crypto installation to instruction.

2. After installation Charm-Crypto, and indy-node, do following:
 {{python3}}
 {{from indy_client.test.training.getting_started import *}}
 => following error appear:
{code:java}
>>> from indy_client.test.training.getting_started import *
Loading module /usr/local/lib/python3.5/dist-packages/config/config-crypto-example1.py
libpbc.so.1: cannot open shared object file: No such file or directory{code}
*Actual Results:*

Instruction does not work out of the box.

*Expected Results:*
 Instruction should work. Need to fix commands in instruction or scripts.

*Case 2:*

Broken link: https://github.com/hyperledger/indy/wiki/Roadmap

(Indy also has a programmatic API, but it is not yet fully formalized, and this version of the guide doesn’t document it. See the {color:#d04437}Indy roadmap{color}.)

*Link to the Document:*
 [https://github.com/hyperledger/indy-node/blob/stable/docs/cluster-simulation.md]",indy-node 1.2.50 (stable),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwypj:",,,,,,,,,,,,,,,,,,,,,,,,,,anikitinDSR,ozheregelya,,,,,,,,,,,"12/Oct/18 6:47 PM;anikitinDSR;indy-client is deprecated for now;;;",,,,,,,,,,,,,,,,,,,,,,,,
[Refactor] Get rid of RAET code,INDY-1057,26158,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,20/Dec/17 12:49 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,We can remove all RAET-specific code,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 6:25 PM;VladimirWork;INDY-1057.PNG;https://jira.hyperledger.org/secure/attachment/14659/INDY-1057.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-784,,,,,,,,,"1|hzyraf:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,Derashe,VladimirWork,,,,,,,,,,"20/Feb/18 9:19 PM;Derashe;Problem reason: 
- We need to remove raet folder

Changes: 
- Raet folder and all of it dependencies removed

PR:
- https://github.com/hyperledger/indy-node/pull/541
- https://github.com/hyperledger/indy-plenum/pull/512

Version:
- 1.3.312-master

Risk factors:
- No

Risk:
- Low

Covered with tests:
- No

Recommendations for QA
- Check that there is no 'raet' string and everything connected with raet in plenum and node. Check that build is ok;;;","21/Feb/18 6:25 PM;VladimirWork;Build Info:
indy-node 1.3.312
indy-plenum 1.2.251

Steps to Validate:
1. Check for `raet` in indy-node and indy-plenum.
2. Install pool with version 1.3.312+.
3. Install client nd connect it to pool.
4. Check that pool works.
5. Run write and read load test against this pool.

Actual Results:
Pool without RAET code works normally. !INDY-1057.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,,
[Refactor] Get rid of Registry-based pools,INDY-1058,26159,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,20/Dec/17 12:52 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"As of now, we have two ways to initialize the Pool: transaction-based (Pool ledger txns) and Registry-based (explicit).
Registry-based approach is not used anywhere except tests.

We need to get rid of Registry-based approach:
- simplify abstractions
- make all tests to use txnPoolNodeSet.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1209,INDY-1237,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-784,,,,,,,,,"1|hzz0vj:",,,,,,Sprint 18.05,18.06,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,Derashe,VladimirWork,,,,,,,,,,"23/Mar/18 12:37 AM;Derashe;Problem reason:
 - We need to make all tests in plenum/tests folder to work with txnPoolNodeSet

Changes:
 - Tests changed. nodeSet fixture removed.

PR:
- https://github.com/hyperledger/indy-plenum/pull/568

Version:
 - master, 281

Risk factors:
 - No

Risk:
 - Low

Covered with tests:
 - No

Recommendations for QA

-;;;","23/Mar/18 12:53 AM;ashcherbakov;Can we also remove `RegistryPoolManager` from the code? It can be done in a separate ticket probably.;;;","26/Mar/18 6:45 PM;VladimirWork;[~Derashe] > Can we also remove `RegistryPoolManager` from the code? It can be done in a separate ticket probably.
Can we move this ticket to DONE?;;;","26/Mar/18 7:15 PM;ashcherbakov;[~VladimirWork]
I believe we can, we just need to create a separate ticket for removing `RegistryPoolManager` class.;;;",,,,,,,,,,,,,,,,,,,,,
"Paths were not changed in ""Indy – Running the Getting Started tutorial locally""",INDY-1059,26160,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ashcherbakov,ozheregelya,ozheregelya,20/Dec/17 12:53 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,documentation,,,"*Case 1:*
([https://github.com/hyperledger/indy-node/blob/stable/docs/indy-running-locally.md] )
Script _setupEnvironment.sh_ is not actual after file/folder changes (INDY-831 - INDY-833). Node data are placed in /var/lib/indy, not in ~/.indy.

*Case 2:*
{code:java}
python /usr/lib/python3.5/site-packages/indy_client/test/agent/faber.py --port 5555 python /usr/lib/python3.5/site-packages/indy_client/test/agent/acme.py --port 6666 python /usr/lib/python3.5/site-packages/indy_client/test/agent/thrift.py --port 7777{code}
Paths to agent scripts are wrong, network parameter is absent. Correct commands:
{code:java}
python /usr/local/lib/python3.5/dist-packages/indy_client/test/agent/faber.py  --port 5555 --network <network_name>
python /usr/local/lib/python3.5/dist-packages/indy_client/test/agent/acme.py  --port 6666 --network <network_name>
python /usr/local/lib/python3.5/dist-packages/indy_client/test/agent/thrift.py  --port 7777 --network <network_name>{code}",indy-node 1.2.50 (stable),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzyps7:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,TechWritingWhiz,,,,,,,,,,"22/Dec/17 7:15 AM;TechWritingWhiz;These items have been fixed. The pull request is here: https://github.com/hyperledger/indy-node/pull/504;;;","11/Jan/18 1:37 AM;ozheregelya;[~TechWritingWhiz], [~krw910], there is one minor thing:
{code:java}
# Remove .indy folder
rm -rf /var/lib/indy{code}
Script will work correctly, but _.indy folder_ in comment may confuse users. May be it will be better to replace it by something like ""Remove _node data_""?;;;","16/Jan/18 5:55 PM;ashcherbakov;Fixed in https://github.com/hyperledger/indy-node/pull/522;;;",,,,,,,,,,,,,,,,,,,,,,
"Some mistakes and broken links in ""Setting Up a Test Indy Network in VMs""",INDY-1060,26161,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,20/Dec/17 12:57 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,CommunityWishlist,documentation,,"[https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestSovrinClusterSetup.md]

*Case 1:*
 Broken link [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/sandbox/DevelopmentEnvironment/Vagrantfile]
 here: ""...you may continue setting up an actual Developer Environment connected to a sandbox by following these {color:#d04437}instructions{color}.""

*Case 2:*
 Wrong formatting:
!formatting.jpg|thumbnail!
 As far as I understand, text marked with red arrows should be formatted as text, not as code.
  
 *Case 3:*
 Mixed up links. Master version of this document [https://github.com/evernym/sovrin-environments/blob/master/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md] references to stable version of GST [https://github.com/hyperledger/indy-node/blob/stable/getting-started.md]

*Case 4:*
 Need to document that vagrant instruction does not work on host ubuntu system due to vagrant issue: [https://github.com/hashicorp/vagrant/issues/7155] 
 See comments in INDY-1009:
 https://jira.hyperledger.org/browse/INDY-1009?focusedCommentId=37554&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-37554
 https://jira.hyperledger.org/browse/INDY-1009?focusedCommentId=37590&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-37590",indy-node 1.2.50 (stable),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/17 1:35 AM;ozheregelya;formatting.jpg;https://jira.hyperledger.org/secure/attachment/13926/formatting.jpg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzyptb:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,TechWritingWhiz,tharmon,,,,,,,,,"23/Dec/17 8:12 AM;TechWritingWhiz;The first issue has already been resolved. 

The third issue is resolved when master is moved to stable. 

All of the other items  listed in this ticket have been corrected. The pull request is here: [https://github.com/ever]nym/sovrin-environments/pull/55

Please note, this is my local branch to be pulled in the MASTER branch on this item, not the stable.;;;","28/Dec/17 1:23 AM;TechWritingWhiz;Please don't do anything with this until I get my pull request corrected.;;;","28/Dec/17 1:48 AM;tharmon;Regarding *Case 3*:
There was a decision made a while back to have all hyperlinks use the {{stable}} branch. This was done because there were issues with {{master}}-based links being merged into {{stable}}, which then made it very difficult for anyone to actually use the documentation, as the community should be using the {{stable}} branch, as that is targeted to what they can easily download.

Relative links will work in some cases, but not all, especially if they are cross-repository.

So, I don't think *Case 3* is actually a bug.

^^ [~krw910];;;","05/Jan/18 6:47 AM;TechWritingWhiz;These items have been completed. The pull request is here: [https://github.com/hyperl]edger/indy-node/pull/512

Note that  case #3 was ignored. ;;;","09/Jan/18 8:09 PM;ashcherbakov;Merged. Please test against scripts moved to indy-node repo (INDY-1055).;;;","11/Jan/18 2:02 AM;ozheregelya;Cases 1, 2 and 4 were fixed and verified in *master* version, indy-node=1.2.261.;;;",,,,,,,,,,,,,,,,,,,
Replica.lastPrePrepareSeqNo may not be reset on view change,INDY-1061,26162,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,spivachuk,spivachuk,20/Dec/17 1:01 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,ViewChange,,,"During investigation of the bug INDY-959, on the master replica of the node ""korea"" a 3PC-batch was observed with a key with {{ppSeqNo}} generated using {{Replica.lastPrePrepareSeqNo}} counter that was not reset after the previous view, while it must be reset. The corresponding 3PC-batches on the backup replicas of this node had keys with expected {{ppSeqNo}} (generated using the counter reset after the previous view). The {{PrePrepare}} (initial message for 3PC-batch) on the master replica was postponed to {{Replica.prePreparesPendingPrevPP}} due to unavailability of previous {{PrePrepares}} while the 3PC-batches on all the backup replicas were successfully ordered by them.

This 3PC-batch on the master replica and the corresponding 3PC-batches on the backup replicas were the 3PC-batches with {{viewNo}} 2 containing the single request with {{reqId}} 1509109895321705 from {{identifier}} Fvy1AsYgrpkjmTE7KsD4CY. On the master replica {{ppSeqNo}} was 76 and on the backup replicas {{ppSeqNo}} was 1.

Also it's worth to note that later on the master replica there was a 3PC-batch with the same single request and with {{viewNo}} 3. It had {{ppSeqNo}} 1 and was successfully ordered by the replica and eventually by the node.

The node log has been attached. Also this log and other logs from the pool can be found in INDY-959.","STN network, running 1.1.43.",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-959,,,,,,,,,,,,,,,,,,,,"20/Dec/17 12:53 AM;spivachuk;low_level_logs.tgz;https://jira.hyperledger.org/secure/attachment/13925/low_level_logs.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyty7:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,dsurnin,ozheregelya,spivachuk,,,,,,,,,,"12/Jan/18 8:42 PM;dsurnin;the issue might be fixed with 1018
so needs to be retested with latest master with plenum v215;;;","16/Jan/18 4:42 AM;ozheregelya;[~spivachuk], [~dsurnin], could you please provide approximate steps how to reproduce described behavior?;;;","26/Jan/18 2:47 AM;ozheregelya;As it was discussed with developers, it's enough to run load test to verify this issue.

*Environment:*
indy-node 1.2.279
AWS QA live pool (25 nodes)

*Steps to Validate:*
1. Setup the pool.
2. Run load test several times.

*Actual Results:*
Pool successfully wrote 25000 transactions.;;;",,,,,,,,,,,,,,,,,,,,,,
"Some mistakes and broken links in ""Getting Started with Indy""",INDY-1062,26163,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,20/Dec/17 1:15 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,04/Jan/18 12:00 AM,0,documentation,,,"[https://github.com/hyperledger/indy-node/blob/master/getting-started.md]

Case 1:
broken link [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/sandbox/DevelopmentEnvironment/Vagrantfile]
here: Note: If you're looking to create an actual Developer Environment connected to a sandbox, please visit this _{color:#d04437}guide{color}_ instead.
 
Case 2:
Wrong output of command _load sample/faber-request.indy_ in example:
ALICE> load sample/faber-request.indy
{color:#d04437}New wallet Default created{color}
{color:#d04437}Active wallet set to ""Default""{color}
1 connection request found for Faber College.
Creating Connection for Faber College.
 
Try Next:
    show connection ""Faber College""
    accept request from ""Faber College""
 
New wallet Default will not be created because wallet Alice was created manually on one of previous steps. 
 
Case 3:
Links to vagrant instruction ([https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md]) are still broken for both of stable and master versions. These links should accord to master/stable instruction version.",indy-node 1.2.50 (stable),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzyptj:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,ozheregelya,TechWritingWhiz,,,,,,,,,"03/Jan/18 3:35 AM;krw910;[~TechWritingWhiz] If you check the document ""Document Changes from INDY-1009"" I have the information that will address this ticket. Look for the section ""INDY-1062"".
Let me know if you have any questions.;;;","05/Jan/18 2:38 AM;TechWritingWhiz;This item has been completed. The pull request is here: [https://github.com/hyperledger/indy-node/pull/511]

 ;;;","09/Jan/18 8:08 PM;ashcherbakov;Merged. Please test against scripts moved to indy-node repo (INDY-1055).;;;","11/Jan/18 1:52 AM;ozheregelya;All cases were fixed and verified in *master* version, indy-node=1.2.261.;;;",,,,,,,,,,,,,,,,,,,,,
Bad links in Stable GSG,INDY-1063,26164,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,,SeanBohan_Sovrin,SeanBohan_Sovrin,20/Dec/17 2:08 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"In the Stable GSG I found the following bad links and am unsure where they should link to. 

1. ""*Note:* If you're looking to create an actual Developer Environment connected to a sandbox, please visit this [guide|https://github.com/evernym/sovrin-environments/blob/stable/vagrant/sandbox/DevelopmentEnvironment/Vagrantfile] instead.""

links to: [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/sandbox/DevelopmentEnvironment/Vagrantfile]

404 not found

 

2. *Automated VM Creation with Vagrant* [Create virtual machines|https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md] using VirtualBox and Vagrant.

links to: [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md]

 

404 not found

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzypsf:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,krw910,ozheregelya,SeanBohan_Sovrin,,,,,,,,,,"12/Jan/18 7:22 AM;krw910;[~ozheregelya] I think this was already done can you verify that it was completed.;;;","16/Jan/18 1:32 AM;ozheregelya;[~krw910], [~SeanBohan_Sovrin], Yes these cases were fixed with another findings from INDY-1009.;;;",,,,,,,,,,,,,,,,,,,,,,,
Get rid of Sovrin dependency in environment scripts,INDY-1064,26215,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,21/Dec/17 12:04 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,get rid of all Sovrin dependencies after moving sovrin-environment scripts to Indy.,,,,,,,,,,INDY-1055,,,,,,,INDY-1046,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Jan/18 12:24 AM;VladimirWork;INDY-1064.PNG;https://jira.hyperledger.org/secure/attachment/14134/INDY-1064.PNG","12/Jan/18 1:02 AM;VladimirWork;vagrant_setup_errors.PNG;https://jira.hyperledger.org/secure/attachment/14135/vagrant_setup_errors.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-987,,,,,,,,,"1|hzypsv:",,,,,,INDY 18.01: Stability+,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"12/Jan/18 12:24 AM;VladimirWork;There is an error on each node of the pool if *master branch* pool is built with docker (but stable branch pool is built successfully):

Step 9/13 : RUN init_indy_node $nodename $nport $cport
 ---> Running in 7e03ae70d850
Node-stack name is Node4
Client-stack name is Node4C
Traceback (most recent call last):
  File ""/usr/local/bin/init_indy_keys"", line 31, in <module>
    os.makedirs(config_helper.keys_dir, exist_ok=True)
  File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_helper.py"", line 21, in keys_dir
    os.path.join(self.config.KEYS_DIR, self.config.NETWORK_NAME, 'keys'))
  File ""/usr/lib/python3.5/posixpath.py"", line 89, in join
    genericpath._check_arg_types('join', a, *p)
  File ""/usr/lib/python3.5/genericpath.py"", line 143, in _check_arg_types
    (funcname, s.__class__.__name__)) from None
TypeError: join() argument must be str or bytes, not 'NoneType'
The command '/bin/sh -c init_indy_node $nodename $nport $cport' returned a non-zero code: 1
Node Node4 9707 9708 created

So nodes' containers are not created. !INDY-1064.PNG|thumbnail! ;;;","12/Jan/18 1:05 AM;VladimirWork;There is an error on each node of the pool with vagrant (vagrantfile and scripts are not changed after `git pull` so this is the master branch too):

    validator04: Created symlink from /etc/systemd/system/multi-user.target.wants/indy-node.service to /etc/systemd/system/indy-node.service.
    validator04: ● indy-node.service - Indy Node
    validator04:    Loaded: loaded (/etc/systemd/system/indy-node.service; enabled; vendor preset: enabled)
    validator04:    Active: active (running) since Thu 2018-01-11 08:41:47 MST; 123ms ago
    validator04:  Main PID: 14304 (python3)
    validator04:    CGroup: /system.slice/indy-node.service
    validator04:            └─14304 python3 -O /usr/local/bin/start_indy_node Node4 9707 9708
    validator04:
    validator04: Jan 11 08:41:47 validator04 systemd[1]: Started Indy Node.
    validator04: Fixing Bugs
    validator04: [Install] section is present in indy-node target
    validator04: Setting Up Indy Node Number 4
    validator04: Node-stack name is Node4
    validator04: Client-stack name is Node4C
    validator04: Traceback (most recent call last):
    validator04:   File ""/usr/local/bin/init_indy_keys"", line 28, in <module>
    validator04:     config = getConfig()
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_util.py"", line 26, in getConfig
    validator04:     CONFIG = _getConfig(PlenumConfig, general_config_dir, user_config_dir)
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_util.py"", line 19, in _getConfig
    validator04:     user_config_dir=user_config_dir)
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 47, in extend_with_default_external_config
    validator04:     extend_with_external_config(extendee, (extendee.GENERAL_CONFIG_DIR, extendee.GENERAL_CONFIG_FILE))
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 32, in extend_with_external_config
    validator04:     config = getInstalledConfig(*extender)
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 26, in getInstalledConfig
    validator04:     spec.loader.exec_module(config)
    validator04:   File ""<frozen importlib._bootstrap_external>"", line 661, in exec_module
    validator04:   File ""<frozen importlib._bootstrap_external>"", line 767, in get_code
    validator04:   File ""<frozen importlib._bootstrap_external>"", line 727, in source_to_code
    validator04:   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
    validator04:   File ""/etc/indy/indy_config.py"", line 22
    validator04:     NETWORK_NAME = 'sandbox'SendMonitorStats = False
    validator04:                                            ^
    validator04: SyntaxError: invalid syntax
    validator04: Traceback (most recent call last):
    validator04:   File ""/usr/local/bin/generate_indy_pool_transactions"", line 15, in <module>
    validator04:     TestNetworkSetup.bootstrapTestNodes(getConfig(), portsStart, nodeParamsFileName,
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_util.py"", line 26, in getConfig
    validator04:     CONFIG = _getConfig(PlenumConfig, general_config_dir, user_config_dir)
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_util.py"", line 19, in _getConfig
    validator04:     user_config_dir=user_config_dir)
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 47, in extend_with_default_external_config
    validator04:     extend_with_external_config(extendee, (extendee.GENERAL_CONFIG_DIR, extendee.GENERAL_CONFIG_FILE))
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 32, in extend_with_external_config
    validator04:     config = getInstalledConfig(*extender)
    validator04:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 26, in getInstalledConfig
    validator04:     spec.loader.exec_module(config)
    validator04:   File ""<frozen importlib._bootstrap_external>"", line 661, in exec_module
    validator04:   File ""<frozen importlib._bootstrap_external>"", line 767, in get_code
    validator04:   File ""<frozen importlib._bootstrap_external>"", line 727, in source_to_code
    validator04:   File ""<frozen importlib._bootstrap>"", line 222, in _call_with_frames_removed
    validator04:   File ""/etc/indy/indy_config.py"", line 22
    validator04:     NETWORK_NAME = 'sandbox'SendMonitorStats = False
    validator04:                                            ^
    validator04: SyntaxError: invalid syntax
 !vagrant_setup_errors.PNG|thumbnail! ;;;","15/Jan/18 11:05 PM;VladimirWork;Build Info:
indy-node 1.2.270

Steps to Validate:
1. Build pool from stable/master branch with vagrant/docker.
2. Check nodes status.
3. Check NYMs' adding/getting.

Actual Results:
Pools are built successfully.;;;",,,,,,,,,,,,,,,,,,,,,,
Return from GET_TXN for schema and claim_def is a little confusing,INDY-1065,25762,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,ashcherbakov,devin-fisher,devin-fisher,15/Dec/17 7:10 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"There are repeating fields and some have a rather generic field name (data).

 

Examples:
{code}
\{
""op"": ""REPLY"",
""result"": \{
    ""{color:#ff0000}seqNo{color}"": 15,
    ""type"": ""3"",
    ""identifier"": ""GGBDg1j8bsKmr4h5T9XqYf"",
    ""reqId"": 1513288914009792259,
    ""{color:#ff0000}data{color}"": \{
        ""rootHash"": ""Gnrip4cJgwJ3HE1fbrTBAPcuJ9RejAhX12PAUaF5HMij"",
        ""type"": ""101"",
        ""{color:#ff0000}seqNo{color}"": 15,
        ""signature"": ""2paGvrWEfsCAYFAD47Qh7hedinymLy8VsbfatUrjWW7tpcryFtTsikJjWhKkD5QA3PLr7dLTmBFteNr4LWRHhrEn"",
        ""auditPath"": [
            ""ERHXC95c5GkeGN1Cn8AsFL8ruU65Mmc5948ey4FybZMk"",
            ""8RPu6xcwmSaEgVohv83GtZu2hjJm5ghWQ6UEvSdjYCg4"",
            ""FUUbzChmnGjrGChBv3LZoKunodBPrVuMcg2vUrhkndmz""
        ],
        ""txnTime"": 1510246647,
        ""{color:#ff0000}data{color}"": \{
            ""name"": ""Home Address"",
            ""version"": ""0.1"",
            ""attr_names"": [
                ""address1"",
                ""address2"",
                ""city"",
                ""state"",
               ""zip""
            ]
        },
        ""identifier"": ""4fUDR9R7fjwELRvH9JT6HH"",
        ""reqId"": 1510246647859168767
    }
}
}
{code}
\{
""op"":""REPLY"",
""result"":\{
""seqNo"":16,
""type"":""3"",
""identifier"":""GGBDg1j8bsKmr4h5T9XqYf"",
""reqId"":1513289190174242862,
""data"":\{
""rootHash"":""t6ZRMjScyzm1Xcv9ZY16NoJQd71PVnpviFvrZRFadbf"",
""signature_type"":""CL"",
""seqNo"":16,
""signature"":""3o68ednKE3bRSfuK3f4bmzPprHAPJbBJPCAuURbs5phsua74SG2qtmp1YuVymrALSYXmQw3PigrxdXTYnwX4Uizn"",
""auditPath"":[
""9Bch8LSBBQwGY4CbL9V7aK7Jt2XQiUHWStNiDnRTS6yv"",
""ERHXC95c5GkeGN1Cn8AsFL8ruU65Mmc5948ey4FybZMk"",
""8RPu6xcwmSaEgVohv83GtZu2hjJm5ghWQ6UEvSdjYCg4"",
""FUUbzChmnGjrGChBv3LZoKunodBPrVuMcg2vUrhkndmz""
],
""type"":""102"",
""txnTime"":1510257461,
""ref"":48,
""data"":\{
""revocation"":\{

},
""primary"":\{
""z"":""205"",
""s"":""199"",
""rctxt"":""5355"",
""n"":""104"",
""rms"":""140"",
""r"":

\{ ""address2"":""9266"", ""city"":""567"", ""address1"":""164"", ""zip"":""572"", ""state"":""634"" }

}
},
""identifier"":""4fUDR9R7fjwELRvH9JT6HH"",
""reqId"":1510257461473195033
}
}
}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-886,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-784,,,,,,,,,"1|hzyavz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,devin-fisher,,,,,,,,,,,"21/Dec/17 6:25 PM;ashcherbakov;Any GET_TXN result has the following form:
{code}
{
  ""identifier"": <request_identifier>                        # GET_TXN req identifier (DID who sent it)
  ""reqId"": <request_req_id>                                 # GET_TXN req id
  ""type"": <req_type>,                                           # GET_TXN type (=3)
  ""data"": <txn_from_ledger_with_metadata>,       # txn as is on the ledger plus ""rootHash"" and ""auditPath"" metadata to make sure that txn really belongs to the ledger
  ""seqNo"": <txn_seq_no>                                     # txn seq_no as on the ledger
}
{code}
So, 
1) 'seqNo' is duplicated (it's already present in 'data' as part of the transaction on the ledger. This should be fixed.
2) there is just one 'data' in the template meaning the transaction data. But some types of transactions (SCHEMA and CLAIM_DEF in particular) may have their own 'data' field. It would be nice to fix it, but I don't think it's so critical.

So, the PoA:
1) Fix duplicate 'seqNo' by having it only in txn 'data'
- we should take into account backward-compatiblity problem, so that old clients may expect seqNo outside data. However, I'm not sure that we have any existing clients that rely on this, so probably this is ok just to remove it in this particular case.
2) Fix duplicate data fields (Optional)
- Rename Replies' 'data' to 'transaction' or 'transaction_data'
- Support backward-compatibility with old clients using PROTOCOL VERSION (see INDY-1066)
;;;","02/Feb/18 3:24 AM;ashcherbakov;This is will be done as part of INDY-1108.;;;",,,,,,,,,,,,,,,,,,,,,,,
inconsistent case in indy tranactions (camel case and snake case),INDY-1066,25749,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,ashcherbakov,devin-fisher,devin-fisher,15/Dec/17 2:56 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Examples:
{quote}\{
 ""reqId"":1510246647994082549,
 ""{color:#ff0000}seqNo{color}"":15,
 ""data"":\{
 ""{color:#ff0000}attr_names{color}"":[
 ""address1"",
 ""address2"",
 ""city"",
 ""state"",
 ""zip""
 ],
 ""name"":""Home Address"",
 ""version"":""0.1""
 },
 ""{color:#ff0000}txnTime{color}"":1510246647,
 ""type"":""107"",
 ""identifier"":""4fUDR9R7fjwELRvH9JT6HH"",
 ""dest"":""4fUDR9R7fjwELRvH9JT6HH""
}{quote}
{quote}\{
 ""{color:#ff0000}reqId{color}"": 1510257732123766311,
 ""identifier"": ""4fUDR9R7fjwELRvH9JT6HH"",
 ""operation"": \{
 ""type"": ""108"",
 ""ref"": 48,
 ""{color:#ff0000}signature_type{color}"": ""CL"",
 ""origin"": ""4fUDR9R7fjwELRvH9JT6HH""
 }
}{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-886,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-784,,,,,,,,,"1|hzyavr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,devin-fisher,,,,,,,,,,,"21/Dec/17 6:01 PM;ashcherbakov;There are multiple ways on how we can fix it. The main problem is that any minor change in txns structure is non trivial because we need to be backward-compatible, and don't have versioning support for txns yet. 

*Option1*
- Do not change field names on the ledger, but just transform them into the same case (snake or camel) when returning Reply to client.
- The new clients (expecting the same case) will send requests with protocolVersion=2 (new one). We already have this protocol version support. This guarantees backward-compatibility for old clients (they will get replies as it's now) while new clients can get replies in the same case.
- Support consistent field names in libindy

This option doesn't require any versioning in transaction (which we don't have now), but doesn't solve the inconsistency problem on ledger level.

*Option 2*
- Support 'version' field in txns (version=0 if it's absent)
- support consistent field names and increment the version.
- support old clients (with inconsistent fields) as described in Option1.

So, this Option is next step of Option1 introducing versioning in ledger. It requires more work.;;;","02/Feb/18 3:24 AM;ashcherbakov;This is will be done as part of INDY-1108.;;;",,,,,,,,,,,,,,,,,,,,,,,
Should get txn require a sender DID,INDY-1067,25748,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,dsurnin,devin-fisher,devin-fisher,15/Dec/17 2:53 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"GET TXN require ""submitter_did"" that translates to ""identifier"" in the transaction.

For example:
{quote}{{\{}}
{{ ""op"": ""REPLY"",}}
{{ ""result"": \{}}
{{ ""reqId"": 1513272264245012814,}}
{{ ""seqNo"": 14,}}
{{ ""data"": ""\{\""dest\"":\""4fUDR9R7fjwELRvH9JT6HH\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""101\"",\""seqNo\"":14,\""txnTime\"":1510246587,\""verkey\"":\""2zoa6G7aMfX8GnUEpDxxunFHE7fZktRiiHk1vgMRH2tm\""}"",}}
{{ ""txnTime"": 1510246587,}}
{{ ""type"": ""105"",}}
{{ ""{color:#ff0000}identifier{color}"": ""V4SGRU86Z58d6TV7PBUe6f"",}}
{{ ""dest"": ""4fUDR9R7fjwELRvH9JT6HH""}}
{{ }}}
{{}}}
{quote}
This seems consistent with the GET TXN that I've tried. But this identifier does not seem to be enforced. (I've created a random DID not on the ledger and used it. Additionally, I can send the TXN without a signature). Seems like this identifier could be logged and could provide correlating data. So I'm questioning it need.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-886,IS-863,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-784,,,,,,,,,"1|hzytzb:",,,,,,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,,ashcherbakov,devin-fisher,lovesh,,,,,,,,,,"18/Dec/17 7:12 PM;lovesh;[~devin-fisher] The DID or reqId in a query (GET_*) is immaterial from the ledger's perspective but the client side logic uniquely identifies each request using a compound key, i.e did+reqId. Thus, the client side interface for queries should be made such that a default identifier (DID, not needed on the ledger) be used in the sent request but not required from the caller;;;","21/Dec/17 6:02 PM;ashcherbakov;[~devin-fisher] [~lovesh] Ok, so can we then close this ticket as Won't Fix, or we still have any concerns about this behaviour?;;;","25/Jan/18 5:09 AM;ashcherbakov;I'm going to close the ticket as Won't Fix according to the comments above.;;;",,,,,,,,,,,,,,,,,,,,,,
We need to have BLS signed Pool State,INDY-1068,26244,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,21/Dec/17 10:17 PM,11/Jun/19 5:21 PM,28/Oct/23 2:47 AM,11/Jun/19 5:21 PM,,,,,,0,GA-0,,,"Currently we have BLS signature over Domain State (domain ledger transactions) only, that is over NYM, CLAIM_DEF, SCHEMA, etc.
We need to have BLS muli-signature support over Pool State (pool ledger transactions such as NODE) as well to be able to perform catch-up more efficiently. With BLS multi-signature for Pool state, we could catch-up from on node and make sure that this is a valid state.
Find more details in [Observer Architecture|https://docs.google.com/document/d/1HcVp0V1RvgHanW_et84ttiga-eWkOaNyzhBzFwsWBIg/edit#].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzyrwf:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,lovesh,SeanBohan_Sovrin,,,,,,,,,,"16/Jan/18 7:26 AM;SeanBohan_Sovrin;[~ashcherbakov] please add a description to the ticket;;;","11/Jun/19 5:21 PM;ashcherbakov;This is already done.;;;",,,,,,,,,,,,,,,,,,,,,,,
Nym request to STN results in inconsistent responses,INDY-1069,26262,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.minaev,sergey.minaev,22/Dec/17 7:36 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Steps to reproduce:
Send NYM transaction to the STN ledger through indy sdk with enabled logs.

Expected result:
Receive ACK/REPLAY from all nodes (except icenode may be)

Actual result:
Attached files:
1) log_stn_nym_4_m is filtered copy of log_stn_nym_4. The most strange is last responses: some nodes reply as in situation on duplicate request
2) sendNymLog.txt - last section in the file - all nodes respond with ACK, but there is no one REPLY

Versions: 
for the pool, apt is pointed at the rc branch, and the indy-node version is 1.2.44.  For the client, the apt is pointed at the stable branch, and the indy-node version is 1.1.0.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-407,,,,,IS-457,,,,,,,,,,,,,,,"22/Dec/17 7:38 PM;sergey.minaev;log_stn_nym_4;https://jira.hyperledger.org/secure/attachment/13942/log_stn_nym_4","22/Dec/17 7:38 PM;sergey.minaev;log_stn_nym_4_m;https://jira.hyperledger.org/secure/attachment/13943/log_stn_nym_4_m","22/Dec/17 7:49 PM;sergey.minaev;sendNymLog.txt;https://jira.hyperledger.org/secure/attachment/13944/sendNymLog.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzx0zb:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,sergey.minaev,sergey-shilov,VladimirWork,,,,,,,,,,"29/Jan/18 6:31 PM;sergey-shilov;*Problem state / reason:*

Test case:
 # An indy-node receives NYM transaction sent by non-trustee and starts ordering;
 # The indy-node receives the same NYM transaction with the same request ID (this may occur in case of pre-prepare came before client request, client re-send etc.);
 # The indy-node answers with REQNACK instead of sending answer from cache.

Such behaviour caused by NYM access rights check (state-related check) done during static validation.

*Changes:*

Moved NYM access rights check from static validation to dynamic validation routine.

*Committed into:*

[https://github.com/hyperledger/indy-node/pull/531]

indy-node 1.2.284-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Medium

*Recommendations for QA:*

Do the following test using indy-cli as a client:
 # Setup pool, run indy-cli client with enabled debug logging (RUST_LOG=trace);
 # As a Steward send NYM request, indy-cli will log raw request to console;
 # Copy this request and send it as a raw request (to emulate re-sending);
 # The indy-cli client should receive the same answer as after the first request.

The following test may be done just to check incorrect client behaviour (not tested yet): do the steps described above but:
 # Change verkey;
 # Change NYM.;;;","31/Jan/18 10:34 PM;VladimirWork;Build Info:
indy-node 1.2.288

Steps to Validate:
1. Setup pool, run indy-cli client with enabled debug logging (RUST_LOG=trace).
2. As a Steward send NYM request, indy-cli will log raw request to console.
3. Copy this request and send it as a raw request using `ledger custom {...}` (to emulate re-sending).

Actual Results:
There are ACK and REPLY messages after first NYM sending and REPLY messages only after all next NYM resendings.;;;",,,,,,,,,,,,,,,,,,,,,,,
test_checkpoint_created fails intermittently,INDY-1070,26263,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,andkononykhin,andkononykhin,22/Dec/17 8:24 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,tests,,,"Test: test_checkpoint_created
 Path: plenum/test/checkpoints/test_basic_checkpointing.py
Logs: attached
 [Jenkins build|https://ci.evernym.com/job/Indy-Plenum/job/indy-plenum-verify-x86_64/339/] (temporarily available)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/17 8:26 PM;andkononykhin;test_checkpoint_created.test-result-plenum-1.ubuntu-02.txt.tar.gz;https://jira.hyperledger.org/secure/attachment/13945/test_checkpoint_created.test-result-plenum-1.ubuntu-02.txt.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzyt2n:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,andkononykhin,ashcherbakov,VladimirWork,,,,,,,,,,"11/Jan/18 11:24 PM;ashcherbakov;Already fixed in https://github.com/hyperledger/indy-plenum/pull/491;;;","12/Jan/18 8:43 PM;VladimirWork;Build Info:
indy-plenum 1.2.215

Steps to Validate:
1. Run single test `pytest test_basic_checkpointing.py` several times.
2. Run all plenum tests via runner.py `python3 runner.py --pytest ""python3 -m pytest"" --dir plenum --output test-result-plenum-1.ubuntu-05.txt` several times.

Actual Results:
plenum/test/checkpoints/test_basic_checkpointing.py is not failed during test runs.;;;",,,,,,,,,,,,,,,,,,,,,,,
test_node_request_propagates[no_client_requests] fails intermittently,INDY-1071,26264,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,22/Dec/17 10:23 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,tests,,,"Test: test_node_request_propagates[no_client_requests]
Path: plenum/test/node_request/test_propagate/test_node_lacks_finalised_requests.py
Logs: attached
 [Jenkins build|https://ci.evernym.com/job/Indy-Plenum/job/indy-plenum-verify-x86_64/342/] (temporarily available)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Dec/17 10:24 PM;andkononykhin;test_node_request_propagates.test-result-plenum-1.ubuntu-06.txt;https://jira.hyperledger.org/secure/attachment/13947/test_node_request_propagates.test-result-plenum-1.ubuntu-06.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzwyon:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,ashcherbakov,spivachuk,,,,,,,,,,"17/Jan/18 6:26 PM;ashcherbakov;I haven't seen it failing recently., Let's keep it in the backlog for a while and monitor.;;;","12/Oct/18 2:37 AM;spivachuk;This test stably passes now. So close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,
Remove DEBUG output from init_indy_node,INDY-1072,26269,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,mgbailey,mgbailey,23/Dec/17 3:03 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"When init_indy_node is run, it dumps dozens of lines of debug log messages to the screen.  This command is run by our new stewards, and they are instructed to save the output. Debug lines here are completely unacceptable.

 
{code:java}
indy@validator05:~$ init_indy_node Node5 9701 9702 2>/dev/null
Node-stack name is Node5
Client-stack name is Node5C
Generating keys for random seed b'FA7b1cc42Da11B8F4BC83990cECF63aD'
Public key is e3b5bd1cc81958c5d0fd64c4418b5ff8809e2b116c6de001b7d8e023efbc9437
Verification key is bfede8c4581f03d16eb053450d103477c6e840e5682adc67dc948a177ab8bc9b
Public key is e3b5bd1cc81958c5d0fd64c4418b5ff8809e2b116c6de001b7d8e023efbc9437
Verification key is bfede8c4581f03d16eb053450d103477c6e840e5682adc67dc948a177ab8bc9b
2017-12-22 17:56:31,779 | DEBUG | bls.py (31) | from_bytes | BlsEntity::from_bytes: >>>
2017-12-22 17:56:31,780 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_generator_from_bytes', args: (b'\x16\xcbn\x1f\x1bx\x03\xf3\n\xb2\xc6a\x19o\xe1\x99\xaf\x17\xd8\xed\x19=\x98\xa3\xd0\xfa\x17c\x8a:\x1b\x83\x1d\xf5A\x91\x8f\x0eZ\xcd\x05v\x99\x8b\xfd\xb891\x83I\xb8\xac\xbbA\x06\xfe\x93\xe6\xa3\xd3Z?\x00\x81\x07\xe2\xc4\xa7\xc9\xa5\x04\x9f,\xc9\xf9\xd7\xce\xd5\x04\x9fC6\xf6xC\xc5\xdc2\xad\x94\x0e9~%-\xf7\x17j\x8fv\xfd\x15\xd56\xbc\x8d)J\xc7\x04\x0fl\xc8\xd5`\xda\xd1=\xe8\x8c=\xfar`\xec64R', 128, <cparam 'P' (0x7ff8e77a8780)>)
2017-12-22 17:56:31,780 | DEBUG | lib.py (29) | _load_cdll | _load_cdll: >>>
2017-12-22 17:56:31,781 | DEBUG | lib.py (35) | _load_cdll | _load_cdll: Detected OS name: linux
2017-12-22 17:56:31,782 | DEBUG | lib.py (45) | _load_cdll | _load_cdll: Resolved libindy name is: libindy_crypto.so
2017-12-22 17:56:31,783 | DEBUG | lib.py (50) | _load_cdll | _load_cdll: Init Indy Crypto logger
2017-12-22 17:56:31,783 | DEBUG | lib.py (53) | _load_cdll | _load_cdll: <<< res: <CDLL 'libindy_crypto.so', handle 16da1c0 at 0x7ff8e9cfaf98>
2017-12-22 17:56:31,784 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_generator_from_bytes' returned err: 0
2017-12-22 17:56:31,785 | DEBUG | bls.py (19) | __init__ | BlsEntity.__init__: >>> self: <indy_crypto.bls.Generator object at 0x7ff8eeef5d68>, instance: c_void_p(23730448)
2017-12-22 17:56:31,785 | DEBUG | bls.py (38) | from_bytes | BlsEntity::from_bytes: <<< res: <indy_crypto.bls.Generator object at 0x7ff8eeef5d68>
2017-12-22 17:56:31,785 | DEBUG | bls.py (111) | new | SignKey::new: >>>
2017-12-22 17:56:31,786 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_sign_key_new', args: (b'FA7b1cc42Da11B8F4BC83990cECF63aD0000000000000000', 48, <cparam 'P' (0x7ff8ec0b1a28)>)
2017-12-22 17:56:31,787 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_sign_key_new' returned err: 0
2017-12-22 17:56:31,788 | DEBUG | bls.py (19) | __init__ | BlsEntity.__init__: >>> self: <indy_crypto.bls.SignKey object at 0x7ff8ed9c0978>, instance: c_void_p(23731488)
2017-12-22 17:56:31,789 | DEBUG | bls.py (118) | new | SignKey::new: <<< res: <indy_crypto.bls.SignKey object at 0x7ff8ed9c0978>
2017-12-22 17:56:31,790 | DEBUG | bls.py (140) | new | VerKey::new: >>>
2017-12-22 17:56:31,790 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_ver_key_new', args: (c_void_p(23730448), c_void_p(23731488), <cparam 'P' (0x7ff8e77a89a0)>)
2017-12-22 17:56:31,792 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_ver_key_new' returned err: 0
2017-12-22 17:56:31,793 | DEBUG | bls.py (19) | __init__ | BlsEntity.__init__: >>> self: <indy_crypto.bls.VerKey object at 0x7ff8ec0c45f8>, instance: c_void_p(23963392)
2017-12-22 17:56:31,794 | DEBUG | bls.py (147) | new | VerKey::new: <<< res: <indy_crypto.bls.VerKey object at 0x7ff8ec0c45f8>
2017-12-22 17:56:31,795 | DEBUG | bls.py (47) | as_bytes | BlsEntity.as_bytes: >>> self: <indy_crypto.bls.SignKey object at 0x7ff8ed9c0978>
2017-12-22 17:56:31,795 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_sign_key_as_bytes', args: (c_void_p(23731488), <cparam 'P' (0x7ff8e9ce0098)>, <cparam 'P' (0x7ff8e9ce0120)>)
2017-12-22 17:56:31,796 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_sign_key_as_bytes' returned err: 0
2017-12-22 17:56:31,796 | DEBUG | bls.py (55) | as_bytes | BlsEntity.as_bytes: <<< res: b""\x02\x961\xda\x18\xe11^N\\\xcd\x07\x9b|4'0\xb6\x1d\xec*\xea<\x11n\x93\xcd\xa5t\x02\xba\xec""
2017-12-22 17:56:31,797 | DEBUG | bls.py (47) | as_bytes | BlsEntity.as_bytes: >>> self: <indy_crypto.bls.VerKey object at 0x7ff8ec0c45f8>
2017-12-22 17:56:31,798 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_ver_key_as_bytes', args: (c_void_p(23963392), <cparam 'P' (0x7ff8e9ce0120)>, <cparam 'P' (0x7ff8e9ce0098)>)
2017-12-22 17:56:31,798 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_ver_key_as_bytes' returned err: 0
2017-12-22 17:56:31,799 | DEBUG | bls.py (55) | as_bytes | BlsEntity.as_bytes: <<< res: b'$\x98^>\x06_\x0ed)@\x7f\xc0\xa9;8\x13+\xe5c|\xa7\xba\xca0\x08e\xc6\x12\xc4\x8d=\xc8\x0ePm)|I\xffPh+\x88F\xfe\xce\xa7\x7f_\t\xc0\xd0\x07\xebe/<\xddE\x17q\xda\xc9\xf7\x00v\xf7;\xbc\xa5\xdd\xa4\xa8v\x1d\x81\xabK\xcb\xa9@\x96o\xd4\x01\xd1f\x92tm\xe3\xc8\xd1\xcev\xfb\x15\x0f\x08(\xca\x08\xbfx\x9e\x83UwI\xbb\xba\xa8\xe6\xa2\xedZp\x98+\xdej\xa2^Vk\x8e\x90\x9e'
2017-12-22 17:56:31,799 | DEBUG | bls.py (60) | __del__ | BlsEntity.__del__: >>> self: <indy_crypto.bls.Generator object at 0x7ff8eeef5d68>
2017-12-22 17:56:31,800 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_generator_free', args: (c_void_p(23730448),)
2017-12-22 17:56:31,801 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_generator_free' returned err: 0
2017-12-22 17:56:31,801 | DEBUG | bls.py (60) | __del__ | BlsEntity.__del__: >>> self: <indy_crypto.bls.SignKey object at 0x7ff8ed9c0978>
2017-12-22 17:56:31,802 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_sign_key_free', args: (c_void_p(23731488),)
2017-12-22 17:56:31,803 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_sign_key_free' returned err: 0
2017-12-22 17:56:31,803 | DEBUG | bls.py (60) | __del__ | BlsEntity.__del__: >>> self: <indy_crypto.bls.VerKey object at 0x7ff8ec0c45f8>
2017-12-22 17:56:31,804 | DEBUG | lib.py (10) | do_call | do_call: >>> name: 'indy_crypto_bls_ver_key_free', args: (c_void_p(23963392),)
2017-12-22 17:56:31,805 | DEBUG | lib.py (14) | do_call | do_call: Function 'indy_crypto_bls_ver_key_free' returned err: 0
BLS Public key is 4kCWXzcEEzdh93rf3zhhDEeybLij7AwcE4NDewTf3LRdn8eoKBwufFcUyyvSJ4GfPpTQLuX6iHjQwnCCQx4sSpfnptCWzvFEdJnhNSt4tJMQ2EzjcL9ewRWi24QxAaCnwbm2BBGJXF7JjqFgMzGfuFXXHhGPX3UtdfAphrojk3A1sgq
{code}

In addition, the ""Public key is..."" and ""Verification key is..."" lines are duplicated. The duplicates should be removed.

The same issue occurs with generate_indy_pool_transactions.  Debug lines should be removed from that command's output as well.",Indy 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/18 7:48 PM;VladimirWork;INDY-1072.PNG;https://jira.hyperledger.org/secure/attachment/14521/INDY-1072.PNG","30/Jan/18 7:42 PM;VladimirWork;INDY-1072.PNG;https://jira.hyperledger.org/secure/attachment/14513/INDY-1072.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzysxj:",,,,,,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,,anikitinDSR,mgbailey,VladimirWork,,,,,,,,,,"29/Jan/18 5:55 PM;anikitinDSR;Problem reason: 
- Dublicate strings Public key and Verification key

Changes: 
- Added additional info about public and verification key owner. This pairs are the same but created for client and node stacks

PR:
- https://github.com/hyperledger/indy-plenum/pull/507


Version:
- master, ;;;","30/Jan/18 7:43 PM;VladimirWork;`init_indy_node` is ok but we still have an issue with `generate_indy_pool_transactions`.;;;","31/Jan/18 7:48 PM;VladimirWork;Build Info:
indy-node 1.2.289

Steps to Validate:
1. Run `init_indy_node` with various parameters.
2. Run `generate_indy_pool_transactions` with various parameters.

Actual Results:
We can see scripts' outputs only in shell (without any level log messages). Keys for node and client stacks are separated. !INDY-1072.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,
Pool doesn't work in in-memory mode,INDY-1073,26283,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,anikitinDSR,VladimirWork,VladimirWork,27/Dec/17 12:54 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Build Info:
indy-node 1.2.251

Steps to Reproduce:
1. Install pool.
2. Stop all nodes.
3. Clear all nodes by `clear_node.py --full FULL`.
4. Execute `generate_indy_pool_transactions` on all nodes.
5. Add to /etc/indy/indy_config.py:
{noformat}
hashStore = {
  ""type"": ""memory""
}

domainStateStorage = 2
poolStateStorage = 2
reqIdToTxnStorage = 2
stateSignatureStorage = 2
{noformat}
6. Start all nodes.

Actual Results:
Nodes disconnect from each other spontaneously after start in Step 6. There is a tracelog in journalctl and indy-node status:
{noformat}
Dec 26 11:37:19 c0f20025ee61 env[189]: Traceback (most recent call last):
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/bin/start_indy_node"", line 17, in <module>
Dec 26 11:37:19 c0f20025ee61 env[189]:     run_node(config, self_name, int(sys.argv[2]), int(sys.argv[3]))
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 34, in run_node
Dec 26 11:37:19 c0f20025ee61 env[189]:     looper.run()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 290, in __exit__
Dec 26 11:37:19 c0f20025ee61 env[189]:     self.shutdownSync()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 286, in shutdownSync
Dec 26 11:37:19 c0f20025ee61 env[189]:     self.loop.run_until_complete(self.shutdown())
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Dec 26 11:37:19 c0f20025ee61 env[189]:     return future.result()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Dec 26 11:37:19 c0f20025ee61 env[189]:     raise self._exception
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Dec 26 11:37:19 c0f20025ee61 env[189]:     result = coro.send(None)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 276, in shutdown
Dec 26 11:37:19 c0f20025ee61 env[189]:     await self.runFut
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 363, in __iter__
Dec 26 11:37:19 c0f20025ee61 env[189]:     return self.result()  # May raise too.
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Dec 26 11:37:19 c0f20025ee61 env[189]:     raise self._exception
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 34, in run_node
Dec 26 11:37:19 c0f20025ee61 env[189]:     looper.run()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 260, in run
Dec 26 11:37:19 c0f20025ee61 env[189]:     return self.loop.run_until_complete(what)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Dec 26 11:37:19 c0f20025ee61 env[189]:     return future.result()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Dec 26 11:37:19 c0f20025ee61 env[189]:     raise self._exception
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Dec 26 11:37:19 c0f20025ee61 env[189]:     result = coro.send(None)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 223, in runForever
Dec 26 11:37:19 c0f20025ee61 env[189]:     await self.runOnceNicely()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 206, in runOnceNicely
Dec 26 11:37:19 c0f20025ee61 env[189]:     msgsProcessed = await self.prodAllOnce()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 151, in prodAllOnce
Dec 26 11:37:19 c0f20025ee61 env[189]:     s += await n.prod(limit)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 351, in prod
Dec 26 11:37:19 c0f20025ee61 env[189]:     c = await super().prod(limit)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 910, in prod
Dec 26 11:37:19 c0f20025ee61 env[189]:     c += await self.serviceNodeMsgs(limit)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 944, in serviceNodeMsgs
Dec 26 11:37:19 c0f20025ee61 env[189]:     await self.processNodeInBox()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1503, in processNodeInBox
Dec 26 11:37:19 c0f20025ee61 env[189]:     await self.nodeMsgRouter.handle(m)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
Dec 26 11:37:19 c0f20025ee61 env[189]:     res = self.handleSync(msg)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
Dec 26 11:37:19 c0f20025ee61 env[189]:     return self.getFunc(msg[0])(*msg)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_req_processor.py"", line 50, in process_message_rep
Dec 26 11:37:19 c0f20025ee61 env[189]:     return handler.process(msg, frm)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 63, in process
Dec 26 11:37:19 c0f20025ee61 env[189]:     return self.processor(valid_msg, params, frm)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 81, in processor
Dec 26 11:37:19 c0f20025ee61 env[189]:     self.node.ledgerManager.processLedgerStatus(validated_msg, frm=frm)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 329, in processLedgerStatus
Dec 26 11:37:19 c0f20025ee61 env[189]:     self.mark_ledger_synced(ledgerId)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 876, in mark_ledger_synced
Dec 26 11:37:19 c0f20025ee61 env[189]:     self.postAllLedgersCaughtUp()
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1737, in allLedgersCaughtUp
Dec 26 11:37:19 c0f20025ee61 env[189]:     if self.is_catchup_needed():
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1753, in is_catchup_needed
Dec 26 11:37:19 c0f20025ee61 env[189]:     if self.caught_up_for_current_view():
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1789, in caught_up_for_current_view
Dec 26 11:37:19 c0f20025ee61 env[189]:     ledger.tree.merkle_tree_hash(0, size)) != root_hash:
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 211, in merkle_tree_hash
Dec 26 11:37:19 c0f20025ee61 env[189]:     foldedHash = self.__hasher._hash_fold(hashes[::-1])
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/ledger/tree_hasher.py"", line 77, in _hash_fold
Dec 26 11:37:19 c0f20025ee61 env[189]:     accum = self.hash_children(cur, accum)
Dec 26 11:37:19 c0f20025ee61 env[189]:   File ""/usr/local/lib/python3.5/dist-packages/ledger/tree_hasher.py"", line 29, in hash_children
Dec 26 11:37:19 c0f20025ee61 env[189]:     hasher.update(b""\x01"" + left + right)
Dec 26 11:37:19 c0f20025ee61 env[189]: TypeError: can't concat bytes to tuple
{noformat}

Expected Results:
Pool should work normally in in-memory mode.",,,,,,,,,,,,,,,,,INDY-1050,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzwylj:",,,,,,,,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,krw910,SeanBohan_Sovrin,VladimirWork,,,,,,,,"12/Jan/18 7:00 AM;krw910;[~VladimirWork] Are we sure this was a clean environment after running the clear_node.py. Also, what are we trying to accomplish with this test? What are we simulating?;;;","12/Jan/18 7:13 PM;VladimirWork;[~krw910]
bq. Are we sure this was a clean environment after running the clear_node.py.
Yes, /var/lib/indy folder is empty after running the script.

bq. Also, what are we trying to accomplish with this test? What are we simulating?
I ran this test to determine possible problems in implementation of tree algorithms without influence of interaction with hard drive/ssd (see INDY-1050 for additional info) and to check the performance loss between in-memory ledger and leveldb ledger node's modes (so maybe some leveldb optimizations will be needed if this loss will be significant).;;;","17/Jan/18 7:22 AM;SeanBohan_Sovrin;[~ashcherbakov] - is this creating issues we should care about Alex? if not we can deprecate;;;","17/Jan/18 5:12 PM;ashcherbakov;No, this is not an issue for real pools, as they will always run in leveldb, not in memory, mode.

This should be fixed just to have some performance testing (we would like to have performance testing with InMemory vs. Leveldb ledgers to compare).

So, this is not a critical ticket. I will change the label from Stability to Node Performance.;;;","12/Oct/18 4:20 PM;anikitinDSR;I think, that this issue need to recheck for actual version of indy-node.;;;",,,,,,,,,,,,,,,,,,,,
ATTRIB transaction with ENC and HASH doesn't work,INDY-1074,26289,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,27/Dec/17 8:18 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Make sure that ATTRIB and GET_ATTRIB works correctly.
Enable corresponding tests in indy-node.
Update documentation in requests.md",,,,,,,,,,,,,,,,,IS-523,,,,,,,,,,,,,,,,,IS-560,,,,,,,,,,,,,,,"06/Feb/18 6:43 PM;VladimirWork;image (1).png;https://jira.hyperledger.org/secure/attachment/14549/image+%281%29.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyrd3:",,,,,,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,ashcherbakov,sergey-shilov,VladimirWork,,,,,,,,,,"03/Feb/18 12:39 AM;sergey-shilov;*Problem state / reason:*

ATTRIB and GET_ATTR for hash and enc attributes were not implemented on client and server sides.

*Changes:*

Client side/CLI:
  - Added command regexps for ATTRIB/GET_ATTR operations for hash and enc attributes.
  - Added support of hash and enc attributes to the code that builds requests and parses replies.

Server side:
  - Added validation schemas for ATTRIB/GET_ATTR operations for hash and enc attributes.
  - Support hash and enc attributes for storing and looking up.

Also implemented several related tests for client and server sides.

*Committed into:*

     [https://github.com/hyperledger/indy-plenum/pull/515]
     [https://github.com/hyperledger/indy-node/pull/548]
     indy-node 1.2.294-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

Send ATTRIB/GET_ATTR operations (correct and incorrect) for RAW, HASH and ENC attributes using old and new CLIs. Note that GET_ATTR for HASH and ENC is something like ""proof of existence"".

NOTE: HASH attribute should be SHA256 hash value.;;;","06/Feb/18 6:41 PM;VladimirWork;Build Info:
indy-node 1.2.295

Steps to Validate:
1. Install pool.
2. Send NYM to ledger.
3. Add all types of attributes (RAW, HASH, ENC) to this NYM via `send ATTRIB`.
4. Get this attributes via `send GET_ATTRIB`.

Actual Results:
Node can add and return new attributes (it's checked via old CLI, new CLI doesn't support this types of attributes now so IS-560 is reported).;;;",,,,,,,,,,,,,,,,,,,,,,,
CLI: Wallet open command works without key for encrypted wallet,INDY-1075,26291,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,,ozheregelya,ozheregelya,28/Dec/17 3:49 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Steps to Reproduce:
1. Open the indy-cli.
2. Create encrypted wallet.
3. Open this wallet with wrong key (no key or/and rekey parameter, wrong key, rekey differs from key, etc).
=> wallet has been opened
4. Try to create did in this wallet.

Actual Results:
Error ""Indy SDK error occurred CommonInvalidState"" after attempt to create did.

Expected Results:
_wallet open_ command parameters should be validated. User-friendly message should appear after _wallet open_ with wrong keys.","indy-node 1.2.251
indy-cli 1.1.1~297",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzys33:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,,,,,,,,,,,,"28/Dec/17 3:50 AM;ozheregelya;Issue was created by mistake in INDY project instead of IS.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Unable to send transactions in STN,INDY-1076,26308,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,mgbailey,mgbailey,29/Dec/17 8:22 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"At 22:43:49,551 in the attached logs, a ""send NYM"" command was posted to the STN, with DID=BcMZWTPLANTNHj2eRqJzwM. This transaction, as with others attempted earlier, was not posted to the ledger. We have direct access to 8 of 11 nodes on this ledger, and all 8 report that they are up and connected to all the others, as shown:
{code:java}
mikebailey@sao-stn-p001:~$ sudo -i -u sovrin validator-info -v
[sudo] password for mikebailey:
Validator brazil is running
Current time:     Thursday, December 28, 2017 11:13:06 PM
Validator DID:    2MHGDD2XpRJohQzsXu4FAANcmdypfNdpcqRbqnhkQsCq
Verification Key: 3hNHV1PbFpyibZZkj8bt7foMCzZ1xBXUc5wWYsuA1ZkgQufHGfp1HoS
Node Port:        9701/tcp on 0.0.0.0/0
Client Port:      9702/tcp on 0.0.0.0/0
Metrics:
  Uptime: 32 minutes, 0 seconds
  Total Config Transactions:  0
  Total Ledger Transactions:  379
  Total Pool Transactions:    25
  Read Transactions/Seconds:  0.00
  Write Transactions/Seconds: 0.00
Reachable Hosts:   11/11
  RFCU
  atbsovrin
  australia
  brazil
  canada
  england
  korea
  mapleleaf
  ricFlair
  singapore
  virginia
Unreachable Hosts: 0/11
Software Versions:
  indy-node: 1.1.43
  sovrin: 1.1.6
{code}

We need to determine and repair what is preventing us from posting transactions to this network.

The attached logs are at the TRACE level on the primary node. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1079,,,,,,,,,,,,,,,,,,,,"29/Dec/17 8:26 AM;mgbailey;brazil.log.tgz;https://jira.hyperledger.org/secure/attachment/13952/brazil.log.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzytxz:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,dsurnin,krw910,mgbailey,ozheregelya,,,,,,,,,"29/Dec/17 8:25 AM;mgbailey;Questions on the STN may be addressed to [~tharmon] over the next few days, while I am away.;;;","29/Dec/17 8:34 AM;krw910;May be similar to INDY-1034 and INDY-1054;;;","12/Jan/18 5:24 AM;mgbailey;The problem appears to occur more frequently now that we have upgraded to 1.2.50.  I had to restart nodes on the STN again this morning.;;;","15/Jan/18 10:02 PM;dsurnin;it seems similar to INDY-1079
needs retest;;;","26/Jan/18 8:40 AM;ozheregelya;As it was discussed with developers on stand-up meeting, it's enough to run load test to verify this issue.

*Environment:*
indy-node 1.2.279
AWS QA live pool (25 nodes)

*Steps to Validate:*
1. Setup the pool.
2. Run load test several times.

*Actual Results:*
Pool successfully wrote 25000 transactions.;;;",,,,,,,,,,,,,,,,,,,,
The huge amount of calls and a lot of execution time in kv_store.py,INDY-1077,26317,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,30/Dec/17 1:01 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Build Info:
indy-node 1.2.255

Steps to Reproduce:
1. Install node and stop it.
2. Place file with 100k json NYMs to node named `txns`.
3. Run `python3 -m cProfile -s time add_json_txns_to_ledger.py txns`.
4. Check the cprofile results.

Actual Results:
~5400 seconds from ~6500 are spent in kv_store.py at lines 64 and 69.
99% of all functions' calls are performed in kv_store.py at line 69.
{noformat}
10091189092 function calls (10086844765 primitive calls) in 6563.480 seconds

Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
10003200163 3039.453    0.000 3039.453    0.000 kv_store.py:69(<genexpr>)
   200016 2371.693    0.012 6481.807    0.032 kv_store.py:64(size)
   200026 1064.881    0.005 4104.334    0.021 {built-in method builtins.sum}
   915129   17.407    0.000   25.512    0.000 base58.py:30(b58encode)
   299994    9.021    0.000   10.000    0.000 {method 'Put' of 'leveldb.LevelDB' objects}
 40228222    7.379    0.000    7.379    0.000 {built-in method builtins.divmod}
   400037    4.235    0.000    4.235    0.000 {method 'RangeIter' of 'leveldb.LevelDB' objects}
1600164/1600034    2.716    0.000    6.400    0.000 typing.py:1043(__subclasscheck__)
   200000    2.160    0.000    8.971    0.000 msgpack_serializer.py:45(__sort_dict)
7832836/4832307    1.993    0.000    8.831    0.000 {built-in method builtins.isinstance}
1600227/1600059    1.711    0.000    3.060    0.000 abc.py:194(__subclasscheck__)
   100000    1.695    0.000 6501.976    0.065 compact_merkle_tree.py:95(_push_subtree)
   100000    1.669    0.000 6488.565    0.065 leveldb_hash_store.py:21(writeLeaf)
   400037    1.545    0.000    5.780    0.000 kv_store_leveldb.py:33(iterator)
1330300/99998    1.463    0.000    3.362    0.000 hash_store.py:87(getNodePosition)
199994/100000    1.416    0.000    4.084    0.000 compact_merkle_tree.py:139(__push_subtree_hash)
        1    1.386    1.386 6563.483 6563.483 add_json_txns_to_ledger.py:3(<module>)
   100000    1.351    0.000   31.568    0.000 ledger.py:138(_build_merkle_proof)
  3000559    1.347    0.000    1.347    0.000 _weakrefset.py:70(__contains__)
  1630306    1.331    0.000    1.331    0.000 util.py:24(highest_bit_set)
   100000    1.309    0.000    1.309    0.000 decoder.py:345(raw_decode)
   200000    1.134    0.000    2.177    0.000 __init__.py:41(packb)
   299994    1.103    0.000   11.728    0.000 kv_store_leveldb.py:45(put)
   815122    1.075    0.000    2.752    0.000 tree_hasher.py:27(hash_children)
   200000    1.043    0.000    1.043    0.000 {method 'pack' of 'msgpack._packer.Packer' objects}
  1630298    0.992    0.000    0.992    0.000 util.py:5(count_bits_set)
     3415    0.990    0.000    2.251    0.001 inspect.py:690(getmodule)
  1788618    0.978    0.000    0.978    0.000 kv_store_leveldb_int_keys.py:13(compare)
   199998    0.944    0.000    1.734    0.000 compact_merkle_tree.py:35(_update)
  1600000    0.863    0.000    7.262    0.000 typing.py:1035(__instancecheck__)
   200032    0.814    0.000    0.849    0.000 {built-in method builtins.sorted}
   200012    0.746    0.000 6482.553    0.032 leveldb_hash_store.py:59(leafCount)
   915122    0.724    0.000    0.724    0.000 {method 'update' of '_hashlib.HASH' objects}
   915123    0.690    0.000    0.690    0.000 {built-in method _hashlib.openssl_sha256}
   100000    0.688    0.000 6502.664    0.065 compact_merkle_tree.py:156(append)
   915122    0.685    0.000    0.685    0.000 {method 'digest' of '_hashlib.HASH' objects}
   915114    0.679    0.000   26.190    0.000 ledger.py:228(hashToStr)
   100004    0.649    0.000    2.714    0.000 tree_hasher.py:73(_hash_fold)
   200000    0.637    0.000   14.049    0.000 msgpack_serializer.py:21(serialize)
   200979    0.595    0.000    0.595    0.000 {method 'match' of '_sre.SRE_Pattern' objects}
   100000    0.530    0.000 6534.762    0.065 ledger.py:133(_addToTreeSerialized)
   100000    0.521    0.000   22.225    0.000 ledger.py:142(<listcomp>)
{noformat}

Expected Results:
Investigation of actual results is needed for some optimization fixes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Dec/17 1:00 AM;VladimirWork;add_json_txns_to_ledger.py;https://jira.hyperledger.org/secure/attachment/13957/add_json_txns_to_ledger.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzyprr:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,dsurnin,sergey.minaev,VladimirWork,,,,,,,,,,"10/Jan/18 12:31 AM;sergey.minaev;There is problem while using leveldb for hash storage: we need check size for some operation and current code iterate over all records on each write to check count.
AFAIK leveldb doesn't support any efficient call for this operation, so we should implement optimization on higher level. [PR with possible fix|https://github.com/hyperledger/indy-plenum/pull/495/files].;;;","12/Jan/18 10:19 PM;dsurnin;After further discussion and analysis it looks like this fix is complete and does not need any modifications for the moment.
ready for performance test;;;","17/Jan/18 1:07 AM;VladimirWork;Build Info:
indy-node 1.2.273

Steps to Validate:
1. Install node and stop it.
2. Place file with 100k json NYMs to node named `txns`.
3. Run `python3 -m cProfile -s time add_json_txns_to_ledger.py txns`.
4. Check the cprofile results.

Actual Results:
{noformat}
87754442 function calls (83407144 primitive calls) in 61.532 seconds

   Ordered by: internal time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   915180   10.358    0.000   17.030    0.000 base58.py:30(b58encode_int)
 40230462    6.672    0.000    6.672    0.000 {built-in method builtins.divmod}
   915180    6.450    0.000   24.074    0.000 base58.py:41(b58encode)
   300153    4.567    0.000    5.493    0.000 {method 'Put' of 'leveldb.LevelDB' objects}
1600433/1600296    2.276    0.000    5.214    0.000 typing.py:1043(__subclasscheck__)
200046/200038    1.772    0.000    7.839    0.000 msgpack_serializer.py:45(__sort_dict)
7844401/4843364    1.477    0.000    7.025    0.000 {built-in method builtins.isinstance}
1330318/100008    1.391    0.000    3.360    0.000 hash_store.py:87(getNodePosition)
1600498/1600322    1.356    0.000    2.397    0.000 abc.py:194(__subclasscheck__)
  1630368    1.349    0.000    1.349    0.000 util.py:24(highest_bit_set)
  3003483    1.040    0.000    1.040    0.000 _weakrefset.py:70(__contains__)
{noformat}

Additional Info:
Load test run against AWS acceptance pool (4 clients x 25 threads x 1000 NYMs to write) takes a bit more than 3 hours (vs 4.5-5 hours before fix).;;;",,,,,,,,,,,,,,,,,,,,,,
False cancel message during upgrade,INDY-1078,26406,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,mgbailey,mgbailey,05/Jan/18 3:13 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"While upgrading the STN from 1.1.43 to 1.2.50 the notifier service sent emails with the following message for each validator:
{code:java}
Upgrade of node 'korea' to version 1.2.50 has been cancelled due to some reason
{code}
The cancelation message also appears in the indy logs.  No mention of any issue appears anywhere else, including in journalctl. In spite of the cancelation message, the upgrade occurred at the scheduled time, without issue.

The cancelation message was replicated in a local test pool, which also upgraded correctly.

The insufficient nature of the message has been addressed in INDY-801, but the fact that a cancelation message was issued erroneously, when no cancelation had occurred, is a new bug.

Logs and config ledger are attached.  At the time that the logs and ledger were obtained, not all nodes had completed their upgrades, but all were proceeding normally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-801,,,,,INDY-1101,,,,,,,,,,,,,,,"05/Jan/18 3:13 AM;mgbailey;korea_config_ledger;https://jira.hyperledger.org/secure/attachment/14000/korea_config_ledger","05/Jan/18 3:13 AM;mgbailey;korea_log.tgz;https://jira.hyperledger.org/secure/attachment/14001/korea_log.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-738,,,,,,,,,"1|hzyvr3:",,,,,,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,ozheregelya,,,,,,,,,,"11/Jan/18 11:23 PM;ashcherbakov;I believe it's already fixed in the latest master.;;;","26/Jan/18 2:54 AM;ozheregelya;Testing is blocked by INDY-1101.;;;","06/Feb/18 12:01 AM;ozheregelya;Environment:
indy-node 1.2.214 -> 1.2.294
QA-live pool (7+18 nodes)

Steps to Validate:
1. Setup the pool of 7 nodes.
2. Add 18 nodes manually.
3. Schedule upgrade like 
{code:java}
send POOL_UPGRADE name=upgrade12294 version=1.2.294 sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv':'2018-02-05T05:00:00.000000+00:00', '8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-02-05T05:05:00.000000+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya':'2018-02-05T05:10:00.000000+00:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA':'2018-02-05T05:15:00.000000+00:00', '4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe':'2018-02-05T05:20:00.000000+00:00', 'Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8':'2018-02-05T05:25:00.000000+00:00', 'BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW':'2018-02-05T05:30:00.000000+00:00', '4EG9n9ErTVqcZ7xSnhHH8PQVtYDfugw5SwqcezvYVrGg':'2018-02-05T05:35:00.000000+00:00', 'FR454t4km7JJvaZc6ttzFU9Lor5ib9YB3wao1kJeoEZb':'2018-02-05T05:40:00.000000+00:00', 'F9abmLwUQy5svguBPNi2GQTdKsiVGDAyBfaFpwcpLJDm':'2018-02-05T05:45:00.000000+00:00', '72vCSKVDbcbpnCXekEd9vhagse75uXnjJDjDGYDxeXPG':'2018-02-05T05:50:00.000000+00:00', '8oE3bGvZTzGxVQRHxfLZHD8Uyx2cbX6wnF1aQAiJe5sm':'2018-02-05T05:55:00.000000+00:00', 'A9hZwUqe62MNQXoswU47271CyRf7g1G5Gat3aqeQ6DeN':'2018-02-05T06:00:00.000000+00:00', '2LssMb56SBibrQYFhRnnMvZHhEiaorMuNkmW8QsZfVwA':'2018-02-05T06:05:00.000000+00:00', '8A5NzusREw844wQmttwBqhhWkTNby5o4UvYq6T1FsByb':'2018-02-05T06:10:00.000000+00:00', 'Bh492xBFGYKS7Z57EQkP5cQtJp6jDvypHoSpXHh259q5':'2018-02-05T06:15:00.000000+00:00', '2MbQjn7ij9DFKT7rt425SaeDvgzyjycv72FEiPVdacEb':'2018-02-05T06:20:00.000000+00:00', 'EwZyzG8HBvjWvxmWVgreTVWYQJScpWMVKeqUa7Rk1Pr5':'2018-02-05T06:25:00.000000+00:00', '58b3Fy45qjcBfVtEt2Zi1MgiRzX9PPmj68FwD143SuWQ':'2018-02-05T06:30:00.000000+00:00', '2FGgKVcp2heyWiGTDLVEyF6AJrfaQBvrhkUzhYTjiHA6':'2018-02-05T06:35:00.000000+00:00', '6CRQcKzeRMCstErDT2Pso4he3rWWu1m16CRyp1fjYCFx':'2018-02-05T06:40:00.000000+00:00', '53skV1LWLCbcxxdvoxY3pKDx2MAvszA27hA6cBZxLbnf':'2018-02-05T06:45:00.000000+00:00', 'CbW92yCBgTMKquvsSRzDn5aA5uHzWZfP85bcW6RUK4hk':'2018-02-05T06:50:00.000000+00:00', 'H5cW9eWhcBSEHfaAVkqP5QNa11m6kZ9zDyRXQZDBoSpq':'2018-02-05T06:55:00.000000+00:00', 'DE8JMTgA7DaieF9iGKAyy5yvsZovroHr3SMEoDnbgFcp':'2018-02-05T07:00:00.000000+00:00'} timeout=10 force=False{code}

Actual Results:
There are no cancel messages in upgrade log and node logs.

Additional Information:
Another problems with upgrade will be described in INDY-1101.;;;",,,,,,,,,,,,,,,,,,,,,,
Unable to post transactions in STN,INDY-1079,26635,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,mgbailey,mgbailey,06/Jan/18 9:10 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"This week we upgraded STN to 1.2.50. Following the upgrade, we have been unable to post transactions to the network.  

The STN currently has 11 nodes, 8 of which I have visibility into, and 3 of which are managed by other stewards.  As of this writing, init_bls_keys has been run on my 8 nodes, but not on the other 3.  The other stewards have been asked to run this command on their nodes as well.  Since we cannot post transactions, none of the bls keys have been added to the ledger.

We have changed the log level on our 8 nodes to TRACE, and logs are attached for these nodes, as well as for the cli.

For reference, a transaction was attempted to be sent after Fri Jan  5 23:40:57 UTC 2018:
{code:java}
indy@sandbox> send NYM dest=YWfdj3CUEoU7QQdVGdZJQ6 verkey=~3dLg4PLxgcmp5JgT2MQbmj
Adding nym YWfdj3CUEoU7QQdVGdZJQ6
indy@sandbox>
{code}

The only error seen is this, which may or may not be relevant:
{code:java}
/var/log/indy/sandbox/mapleleaf.log:2018-01-05 23:28:26,579 | ERROR    | ledger_manager.py    ( 956) | _buildConsistencyProof | mapleleaf cannot build consistency proof till 18 since its ledger size is 12{code}

One of the external stewards reports that he has 18 transactions on his config ledger, where mine just have 12.  I will be pursuing this with him, meanwhile.","STN, running 1.2.50",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1103,,,,,INDY-1076,,,,,,,,,,,,,,,"06/Jan/18 9:09 AM;mgbailey;can-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14105/can-stn-p001.tgz","06/Jan/18 9:10 AM;mgbailey;cli.tgz;https://jira.hyperledger.org/secure/attachment/14101/cli.tgz","06/Jan/18 9:09 AM;mgbailey;lon-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14106/lon-stn-p001.tgz","06/Jan/18 9:09 AM;mgbailey;nva-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14102/nva-stn-p001.tgz","06/Jan/18 9:09 AM;mgbailey;ohi-dvn-e001.evernym.lab.tgz;https://jira.hyperledger.org/secure/attachment/14104/ohi-dvn-e001.evernym.lab.tgz","06/Jan/18 9:09 AM;mgbailey;sao-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14103/sao-stn-p001.tgz","06/Jan/18 9:09 AM;mgbailey;seo-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14109/seo-stn-p001.tgz","06/Jan/18 9:09 AM;mgbailey;sgp-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14107/sgp-stn-p001.tgz","06/Jan/18 9:09 AM;mgbailey;syd-stn-p001.tgz;https://jira.hyperledger.org/secure/attachment/14108/syd-stn-p001.tgz","09/Jan/18 10:30 PM;mgbailey;transactions.tgz;https://jira.hyperledger.org/secure/attachment/14117/transactions.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzytxr:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,mgbailey,ozheregelya,spivachuk,,,,,,,,,,"09/Jan/18 2:17 PM;mgbailey;Update: The steward for the node with 18 transactions in its config ledger shut down his node.  The error messages are no longer being written to the logs of the other nodes, but transactions are still not being posted.;;;","09/Jan/18 10:28 PM;mgbailey;[^transactions.tgz] contains pool, domain, and config transactions for the mapleleaf node.;;;","12/Jan/18 1:47 AM;spivachuk;As we can see for now in the attached logs, 3PC-batches stopped to be ordered by the master protocol instance and so transactions stopped to be committed into ledgers on January 4 at 17:50 (for about half an hour period) because replicas in the master protocol instance received a PREPREPARE message with a wrong state trie root hash from the master's primary. The state trie root hash of a received PREPREPARE message is verified for the master protocol instance only, so only the master protocol instance stopped to order transactions. This PREPREPARE message was sent by {{virginia:0}} just after some next view change from the series of view changes caused by sequential nodes restarts performed during the pool upgrade procedure. This PREPREPARE message most likely contained the client request {{('NVji6YsYLTK6ozNDzBHyWB', 1514923761877253)}}. We are not able to state this for sure because INFO-logs do not contain the corresponding information, but we can guess this because we see that the backup protocol instances ordered 3PC-batches with this client request at the same time. Also we see that this client request appeared for the second time - it had been already ordered earlier, two days ago. This might be the reason why the state trie root hash of the PREPREPARE message was wrong.

As to repeated processing of client requests or their PROPAGATEs, earlier we already saw such cases in INDY-959, INDY-1045 and we added a check of the request presence in {{seqNoDB}} on processing of the belated / repeated PROPAGATE message in scope of INDY-959.;;;","12/Jan/18 3:38 AM;spivachuk;*Problem reason:*
- Please see the previous comment.

*Problem state:*
- The issue with repeated processing of client requests or their PROPAGATEs was fixed in scope of INDY-959.;;;","26/Jan/18 2:51 AM;ozheregelya;As it was discussed with developers on stand-up meeting, it's enough to run load test to verify this issue.

*Environment:*
indy-node 1.2.279
AWS QA live pool (25 nodes)

*Steps to Validate:*
1. Setup the pool.
2. Run load test several times.

*Actual Results:*
Pool successfully wrote 25000 transactions.;;;",,,,,,,,,,,,,,,,,,,,
[Design] Design a better approach for View Change,INDY-1080,26683,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,09/Jan/18 6:31 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"We use a simple Catch-up based approach for View Change (Option 4 in [View Change Protocol|https://docs.google.com/document/d/1oftK70I00wPGXFvFqiA2tmpwU7-kIjgNNieYkZcFkic/edit#]).
Have a design for better approach.

- Send a PR with the proposed Design to the repo
- take into account desired re-factoring in the scope of INDY-970 and INDY-971",,,,,,,,,,,,,,,,,INDY-970,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzz70f:",,,,,,EV 18.09 Stability-RocksDB,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,lovesh,,,,,,,,,,,"13/Jan/18 1:22 AM;ashcherbakov;We need to separate ViewChange protocol (how all instances and replicas move to the next view and become in sync) and next Primary selection (how next primary is selected).
 - I believe we should use ViewChange protocol as described in PBFT paper (with application to RBFT as mentioned in Option2 of the doc, in particular apply it to all instances). We should do it the same way as described in PBFT (using stable checkpoints).

 - I propose not changing Primary selection logic, so continue using round robin. We can move to selection based on Performance as suggested in Option 1 of the doc later.

 

Why I prefer approach 2 (PBFT) for View Change:
 - It's defined in PBFT paper => contains lots of thinking from authors, BFT experts and community
 - Looks like it solves the main problem we were facing with view change: how to get all replicas in sync in the new view. Using checkpoints and re-applying requests look like relatively easy and stable way to do it. 
- Option 2 (PBFT) is easier to implement than Option 1. Moreover, we already have checkpoints logic.
- Our main goal now is stability, not performance. Option 1 may be better from performance point of view (BTW it needs to be measured), but I think it may be worse from stability point of view since it's a custom algorithm, not reviewed by BFT experts.;;;","13/Jan/18 1:29 AM;ashcherbakov;PoA for PBFT View Change Protocol:
 * All view change protocol  related logic should be implemented in separate classes following Actor model approach and using State machine (see INDY-970 and [State refactoring|https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.eybc53im5j6s]). 
We should do it the same way as described in PBFT (using stable checkpoints).
 * Primary selection logic should be independent (may still be the same round robin as of now)
 * Write a doc describing View Change (in fact it will be a short doc referencing PBFT paper and mentioning RBFT and multiple instances specific);;;","16/Jan/18 10:39 PM;lovesh;My suggestion is not change the overall algorithm but refactor the code since we have not seen a flaw in the algorithm but rather than the implementation where there are so many state variables to be updated and some of them are not

Some inefficiencies for PBFT style view change
VIEW-CHANGE message requires each node to store received CHECKPOINT messages from each node for the last stable checkpoint, it is not needed.
VIEW-CHANGE-ACK is sent to primary for each replica's VIEW-CHANGE, so a node receiving 3 VIEW-CHANGE sends VIEW-CHANGE-ACKs to the primary. This is chatty.
The primary has to sends null requests for pre-prepared requests and primary starts the requests again for already prepared requests.
Moreover it will take considerable time


Refactoring required:
	Checkpoints are not needed so get rid of watermarks and checkpoints. We can use multi-sig rounds to remove processed requests.
	Use state machine for view change
;;;","16/Jan/18 10:52 PM;ashcherbakov;So, in fact we consider the following two Options:

*Option1*: re-factor existing View Change code as suggested in [State Machine Refactoring|https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit#heading=h.myn1swcdi79r]
 - cons:
 ** even if we have clean implementation of this approach, we are not sure for 100% that this ViewChange approach works in all cases.
 This is our custom algorithm not reviewed by BFT experts and community.
So, there is chance that we spent time for re-factoring and will still come to the conclusion that we need to implement PBFT or something else.
 - pros:
 ** may be less work (although not significantly less)
 ** will improve stability
 ** will improve code quality

*Option2*: write View Change logic from scratch using PBFT algorithm
 * cons:
 ** May require more time for implementation (although not significantly more)
 ** May require some time for debugging and testing the new approach
 * pros:
 ** a well defined approach by BFT experts and community, so more confidence that it will work
 ** stable view change
 ** can have good code quality if we implement it properly from scratch;;;","18/Jan/18 1:45 AM;ashcherbakov;The current problems with View Change code:

1. There are multiple files dealing with View Change - related logic:
 * node.py
 * view_changer.py
 * replica.py
 * ledger_manager.py
 * primary_selector.py

Please note that `view_changer.py` was created recently as one of the refactorings (see INDY-480 and [Plenum States Refactoring|https://docs.google.com/document/d/1qDfyb6ALqvf7Cwrnmk0RUmyVbM2Znd7urey-8B21j6o/edit]. 
 I believe our intention is that all view-change related logic should be in ViewChanger class, and this class doesn't call/use any variables/state from other classes directly. 
 It should communicate with other entities via messages as a step towards Actor model approach.

2. Many of the files below are really huge (node.py contains 3000 lines; replica.py contains 2500 lines), so it's quite hard to find out what is related to view change and what is not.

3. View Change process affects the state of replicas and 3PC. But because we don't have clear states and state machine, it's done quite implicitly and can be a source of issues (see [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L466])

4. View Change logic is very tangled with catch-up logic. Some of catch-up conditions are mixed with the View change ones (see [link|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L1746] as example)

5. View Change logic is also quite tangled with Primary selection logic (although primary selection now is just a round robin) (see [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/view_change/view_changer.py#L484]).

6. View Change logic is used in two scenarios: real view change (when participating replicas change a primary), and propagation of the current state (and current view in particular) to a newly joined node. They have some common, but also have differences. It leads to an ugly code with special variables and conditions (see [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/view_change/view_changer.py#L49] and [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/view_change/view_changer.py#L92] and [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/replica.py#L492]) and a source of bugs.

7. There are many different variables in many classes. It's quite often that classes use variables from other classes (see [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/replica.py#L262] and [this |https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L398]for example).

8. Many methods do not what is expected from them. For example, methods like `is_something_done` or `can_do_something` don't just check conditions, but perform some actions which is not expected and makes workflow not so clear (see [this|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/node.py#L398] as example).;;;","18/Jan/18 1:47 AM;ashcherbakov;I believe INDY-1092 can help us to check the stability of the current implementation and the protocol better.;;;","31/Jan/18 12:55 AM;ashcherbakov;[~danielhardman] [~nage] [~lovesh] [~gudkov]
So, what is our final decision here, especially taken into account INDY-970.
We can do refactoring of existing code and protocol, or implement checkpoint-based one.

 ;;;","26/Apr/18 1:02 AM;ashcherbakov;The final decision was as follows:
* Continue analysis of the current protocol and write more tests around it (these tests will help to prove that the new implementation is good): INDY-1296
* Start implementing PBFT in parallel. Take the algorithm from PBFT as a basis, modify it for Plenum needs (take into account 3PC in particular), and have a look at the Fabric's PBFT experience and Sawtooth's RBFT one: INDY-1290;;;",,,,,,,,,,,,,,,,,
Pool can't reach consensus after stopping part of nodes without restart of all nodes,INDY-1081,26693,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,10/Jan/18 2:44 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Steps to Reproduce:
 1. Setup pool of 5 node.
 2. Send NYM transaction to check that pool works.
 3. Stop indy-node on node 1.
 4. Send NYM transaction to check that pool still works.
 5. Stop indy-node on node 2.
 6. Send NYM transaction to check that there is no consensus.
 7. Stop indy-node on node 3, send NYM transaction to check that there is no consensus.
 8. One by one start services on nodes 1 - 3 (order doesn't matter), send NYM transaction after each node connection.
 => indy-node is running on each node, but transactions are not written.
 9. Restart indy-node on each node and send NYM transaction.

Actual Results:
 Pool can reach consensus only after restarting of all nodes.

Expected Results:
 Pool should reach consensus when n-f nodes are working.

Additional Information:
 After stopping of 2 nodes instead of 3, all works correctly.
 It doesn't matter, which nodes are stopped. In case of stopping nodes 3 - 5, behavior is the same.",indy-node 1.2.255,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1166,,,,,,,,,,,,,,,"22/Jan/18 7:31 PM;VladimirWork;INDY-1081.PNG;https://jira.hyperledger.org/secure/attachment/14361/INDY-1081.PNG","22/Jan/18 7:31 PM;VladimirWork;INDY-1081_bug.PNG;https://jira.hyperledger.org/secure/attachment/14362/INDY-1081_bug.PNG","10/Jan/18 2:45 AM;ozheregelya;Node1.log;https://jira.hyperledger.org/secure/attachment/14118/Node1.log","10/Jan/18 2:45 AM;ozheregelya;Node2.log;https://jira.hyperledger.org/secure/attachment/14119/Node2.log","10/Jan/18 2:45 AM;ozheregelya;Node3.log;https://jira.hyperledger.org/secure/attachment/14120/Node3.log","10/Jan/18 2:45 AM;ozheregelya;Node4.log;https://jira.hyperledger.org/secure/attachment/14121/Node4.log","10/Jan/18 2:45 AM;ozheregelya;Node5.log;https://jira.hyperledger.org/secure/attachment/14122/Node5.log","22/Jan/18 7:33 PM;VladimirWork;logs.tar.gz;https://jira.hyperledger.org/secure/attachment/14363/logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyrbr:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,ozheregelya,spivachuk,VladimirWork,,,,,,,,,,"10/Jan/18 10:58 PM;ozheregelya;The issue reproduces on docker pool on previous RC versions (1.2.49 and 1.2.50), but during previous acceptance tests this case was passed on AWS pool.;;;","20/Jan/18 3:49 AM;spivachuk;*Problem reason:*

The node might not update {{LedgerManager.last_caught_up_3PC}} when synchronizing a ledger in case actual catch-up was not needed. {{LedgerManager.last_caught_up_3PC}} is used for updating {{last_ordered_3pc}} in the node's replicas on completion of catch-up phase. On the master's primary {{last_ordered_3pc}} is used for updating {{lastPrePrepareSeqNo}} on completion of propagate primary process. So miss of updating {{LedgerManager.last_caught_up_3PC}} on the master's primary resulted in creation of 3PC-batches with duplicated keys (with {{ppSeqNo}} starting from 1 again) and this stopped ordering of 3PC-batches.

If actual catch-up was not needed, {{LedgerManager.last_caught_up_3PC}} was updated using the optional properties {{viewNo}} and {{ppSeqNo}} of the last received {{LedgerStatus}} needed for quorum of {{LedgerStatuses}} not newer than own ledger status. Actually this update was performed only in case this last received {{LedgerStatus}} matches own ledger status (not older than it) and contained {{viewNo}} and {{ppSeqNo}}. {{LedgerStatus}} being sending by a node contains {{viewNo}} and {{ppSeqNo}} only if the node was not stopped since its master replica had ordered the batch which contained the last transaction in the ledger (because the map from transactions seq nos to 3PC-keys is stored in memory only).

In the case with stopping 3 nodes from this ticket, the pool came back to consensus by turning on {{Node2}} which was the master's primary. During catch-up phase in scope of propagate primary process {{Node2}} received the last {{LedgerStatus}} needed for quorum from {{Node3}} which had been also stopped and started previously, so this {{LedgerStatus}} did not contain {{viewNo}} and {{ppSeqNo}} and thus the master replica of {{Node2}} which was primary did not recover its {{lastPrePrepareSeqNo}} and then created batches with wrong {{ppSeqNos}} (starting from 1 again) and they were not ordered. When all the nodes in pool were simultaneously restarted, all the protocol instances were terminated and then started from scratch. Each protocol instance started 3PC-batching from the initial key ({{viewNo}}=0, {{ppSeqNo}}=1). So ordering of 3PC-batches was started again.

In the case with stopping 2 nodes from this ticket, the pool came back to consensus by turning on {{Node2}} which was the master's primary. During catch-up phase in scope of propagate primary process {{Node2}} received the last {{LedgerStatus}} needed for quorum from one of {{Node3}}, {{Node4}} or {{Node5}} (it doesn't matter from which one namely). None of these three nodes had been stopped previously, so this {{LedgerStatus}} contained {{viewNo}} and {{ppSeqNo}} and thus the master replica of {{Node2}} which was primary successfully recovered its {{lastPrePrepareSeqNo}} and then created 3PC-batches with correct {{ppSeqNos}} and they were successfully ordered.

*Changes:*
- Fixed the bug with possible miss of updating {{LedgerManager.last_caught_up_3PC}} when synchronizing a ledger in case actual catch-up is not needed. Now the node considers {{viewNo}} and {{ppSeqNo}} of all the received {{LedgerStatuses}} that match own ledger.
- Added a test verifying the fix specified above.
- Fixed a bug in Python wrapper of libindy with trying to set a result or an exception on a cancelled future.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/505
- https://github.com/hyperledger/indy-node/pull/529
- https://github.com/hyperledger/indy-sdk/pull/482

*Version:*
- indy-node 1.2.278 master
- indy-plenum 1.2.225 master
- python3-indy 1.3.0-dev-321

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_pool_reaches_quorum_after_f_plus_2_nodes_turned_off_and_later_on}}

*Recommendations for QA*:

To ensure that the bug has been fixed please stop / start nodes in the following order (for the pool of 5 nodes):
- Stop {{Node1}}.
- Stop {{Node2}}. _(Pool must lose consensus here.)_
- Stop {{Node3}}.
- Start {{Node3}}.
- Start {{Node2}}. _(Pool must come back to consensus here.)_
- Start {{Node1}}.;;;","22/Jan/18 7:33 PM;VladimirWork;Build Info:
indy-node 1.2.279

Steps to Reproduce:
1. Install pool of 5 nodes.
2. Stop nodes from 1st to 3rd.
3. Start nodes from *3rd to 1st* sending NYMs after each node starting to check consensus (pool reaches consensus at 2nd node starting).
4. Stop nodes from 1st to 3rd.
5. Start nodes from *1st to 3rd* sending NYMs after each node starting to check consensus (pool reaches consensus at 2nd node starting). !INDY-1081.PNG|thumbnail! 
6. Stop nodes from 5th to 3rd.
7. Start nodes from 5th to 3rd sending NYMs after each node starting to check consensus. !INDY-1081_bug.PNG|thumbnail! 

Actual Results:
Pool reaches consensus in Step 7 at *n*, not at *n-f*. Debug logs from all nodes are in attachment. [^logs.tar.gz] ;;;","09/Feb/18 4:50 AM;spivachuk;*Problem reason:*
- If there are less than {{n-f}} alive nodes in a pool but the master's primary is alive then it still can create new 3PC-batches. However, these batches are not ordered by the nodes because the nodes do not gather the quorums of PREPAREs (due to there are not enough alive nodes in the pool for consesus). When the number of alive nodes in the pool becomes {{n-f}} and the primary creates some new 3PC-batch and sends the PREPREPARE, the just started nodes request the missing PREPREPAREs and PREPAREs (that were sent while they were being off). However, the just started nodes cannot gather the quorums of the missing PREPAREs because MESSAGE_REPONSEs with PREPAREs already seen by the replica are discarded. Such the criteria of discarding is incorrect for MESSAGE_REPONSEs with PREPAREs. Thus the missing batches are not ordered. Next batches are not ordered due to the previous ones have not been ordered.

*Changes:*
- Fixed a bug with handling of MESSAGE_REQUEST for PREPARE: now the request is replied by a replica only if the requested PREPARE was sent by this replica earlier.
- Fixed a bug with ignoring MESSAGE_RESPONSE with PREPARE in case the requested PREPARE has already been received from any replica: now the response from a replica is ignored only if the requested PREPARE has already been received from this replica.
- Fixed a bug with a wrong set of 3PC-messages being requested by Replica.processPrePrepare method in case of PP_CHECK_NOT_NEXT error.
- Fixed some bugs in test helpers.
- Corrected the test verifying the pool consensus after stopping and starting {{f+2}} nodes including the master's primary.
- Added a test verifying the pool consensus after stopping and starting {{f+2}} nodes but not the master's primary.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/523
- https://github.com/hyperledger/indy-node/pull/557

*Version:*
- indy-node 1.2.299-master
- indy-plenum 1.2.241-master

*Risk factors:*
- The changes affect the fix from INDY-849.

*Risk:*
- Low

*Covered with tests:*
- {{test_quorum_after_f_plus_2_nodes_but_not_primary_turned_off_and_later_on}}

*Recommendations for QA:*
- Please re-test INDY-849 since the fix from it has been affected the changes made in scope of the current ticket.;;;","09/Feb/18 8:02 AM;ozheregelya;Environment:
indy-node 1.2.299
indy-plenum 1.2.241

Steps to Validate:
1. Setup the pool of 5 nodes.
2. Stop 3 nodes in following order: 1 -> 2 -> 3, send transaction after each stopping.
3. Start 3 nodes in following order: 3 -> 2 -> 1, send transaction after each starting.

Actual Results:
Pool returned to consensus after starting of 2nd node. 

Following behavior was noticed during testing of this issue: transaction which was send after stopping 2nd node (which become primary after stopping of node 1) was not written. Need to discuss this case with developers.

Additional Information:
INDY-849 was retested on pool of 7 nodes. Pool returned to consensus when 3 nodes was started in 2 hours after stopping.
Case with stopping of 5 -> 4 -> 3 and starting 3 -> 4 -> 5 was also retested.;;;","09/Feb/18 8:38 PM;ozheregelya;{quote}Following behavior was noticed during testing of this issue: transaction which was send after stopping 2nd node (which become primary after stopping of node 1) was not written. Need to discuss this case with developers.
{quote}
This is expected behavior because we can't guarantee that transaction will be written after reaching consensus if primary was stopped when transaction has been send. So, this ticker can be closed.;;;",,,,,,,,,,,,,,,,,,,
"Nodes crash with ""No space left on device"" error",INDY-1082,26696,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,,krw910,krw910,10/Jan/18 5:29 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"libindy-crypto=0.2.0
indy-plenum=1.2.212
indy-anoncreds=1.0.32
indy-node=1.2.255
sovrin=1.1.42

I have 7 nodes setup in a global pool.
I was running the load scripts that use libindy to get to 100,000+ NYMs
I reached 118,000 on most of the nodes, but at least two are only at about 87,000
The scripts stopped working so I checked the nodes with both the read_ledger tool and the validator-info tool.
In both cases an error was thrown that there was no space left on the device.
I created a new file on the system and when I open it with VIM I am unable to save it with text. I get an error message about the swap file.

*{color:#d04437}Errors{color}*

The logs do not show any errors just that they stopped in the middle of different operations.

*Journalctl -xe*
*See attachment for full log output*
{code}
error: maximum authentication attempts exceeded for root from 89.151.159.158 port 60477 ssh2 [preauth]
Disconnecting: Too many authentication failures [preauth]
{code}

*Opening a node log file*
{code}
E297: Write error in swap file
""Node1.log"" [noeol] 59162L, 55095296C
{code}

*read_ledger --type domain --count*
{code}
ubuntu@tokyoQALive5:~$ sudo su - indy -c""read_ledger --type domain --count""
Traceback (most recent call last):
  File ""/usr/local/bin/read_ledger"", line 161, in <module>
    read_copy_ledger_data_dir = make_copy_of_ledger(ledger_data_dir)
  File ""/usr/local/bin/read_ledger"", line 150, in make_copy_of_ledger
    shutil.copytree(data_dir, read_copy_data_dir)
  File ""/usr/lib/python3.5/shutil.py"", line 309, in copytree
    os.makedirs(dst)
  File ""/usr/lib/python3.5/os.py"", line 241, in makedirs
    mkdir(name, mode)
OSError: [Errno 28] No space left on device: '/var/lib/indy/live/data/Node5-read-copy'
{code}

validator-info -v
{code}
ubuntu@tokyoQALive5:~$ sudo su - indy -c""validator-info -v""
--- Logging error ---
Traceback (most recent call last):
  File ""/usr/lib/python3.5/logging/__init__.py"", line 984, in emit
    self.flush()
  File ""/usr/lib/python3.5/logging/__init__.py"", line 964, in flush
    self.stream.flush()
OSError: [Errno 28] No space left on device
Call stack:
  File ""/usr/local/bin/validator-info"", line 651, in <module>
    sys.exit(main())
  File ""/usr/local/bin/validator-info"", line 606, in main
    logger.debug(""Cmd line arguments: {}"".format(args))
Message: ""Cmd line arguments: Namespace(basedir='/var/lib/indy/live', json=False, log='/var/log/indy/validator-info.log', stdlog=False, verbose=True)""
Arguments: ()
--- Logging error ---
Traceback (most recent call last):
  File ""/usr/lib/python3.5/logging/handlers.py"", line 71, in emit
    if self.shouldRollover(record):
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/common/logging/TimeAndSizeRotatingFileHandler.py"", line 19, in shouldRollover
    bool(RotatingFileHandler.shouldRollover(self, record))
  File ""/usr/lib/python3.5/logging/handlers.py"", line 188, in shouldRollover
    self.stream.seek(0, 2)  #due to non-posix-compliant Windows feature
OSError: [Errno 28] No space left on device
Call stack:
  File ""/usr/local/bin/validator-info"", line 651, in <module>
    sys.exit(main())
  File ""/usr/local/bin/validator-info"", line 643, in main
    logger.info(""Reading file {} ..."".format(file_path))
Message: 'Reading file /var/lib/indy/live/node5_info.json ...'
Arguments: ()
Traceback (most recent call last):
  File ""/usr/local/bin/validator-info"", line 651, in <module>
    sys.exit(main())
  File ""/usr/local/bin/validator-info"", line 644, in main
    print(get_stats_from_file(file_path, args.verbose, args.json))
  File ""/usr/local/bin/validator-info"", line 490, in get_stats_from_file
    stats = json.loads(f.read())
  File ""/usr/lib/python3.5/json/__init__.py"", line 319, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python3.5/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python3.5/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/18 5:28 AM;krw910;No Space Left Issue;https://jira.hyperledger.org/secure/attachment/14124/No+Space+Left+Issue",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzysv3:",,,,,,,,,,,,,,,,,,,,,,,,,,krw910,,,,,,,,,,,,"11/Jan/18 2:43 AM;krw910;Looks like this was due to having my SSH connection open while running a load test. Marking this invalid.;;;",,,,,,,,,,,,,,,,,,,,,,,,
It should not be possible to override CLAIM_DEF for existing schema-did pair,INDY-1083,26707,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,10/Jan/18 7:49 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"We should have an assumption to be able to have just one CLAIM_DEF (that is a key) for an ISSUER_DID.
If one needs to rotate a key, a new DID must be used. It means that ISSUER_DID is rather a CLAIM_DEF's DID (or key's DID).
So, we must check if there is already a CLAIM_DEF for a Schema-DID pair, and not allow to override it on ledger. We now do the same for Schema (that is we can not override Schema with the existent name-version-origin triple).

Tasks:
1) do not allow to override CLAIM_DEF on ledger for already existent schem-did pair;
2) update docs and diagrams so that it's clear it's not possible to create multiple CLAIM_DEFS for the same schema-did pair.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyprz:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"16/Jan/18 5:58 PM;ashcherbakov;Changes:
- added dynamic validation for ClaimDef transaction to not allow update of a ClaimDef's data (public key) for the same (issuer-did; scheam-ref; signature-type).
- moved validation for Schema from static to dynamic
- added more tests
- re-factored `test_public_repo` (split into two files; use parameters to avoid duplicate code)

PR:
- https://github.com/hyperledger/indy-node/pull/522

Build
- master 1.2.273

Tests:
- `test_schema`
- `test_claim_def`
- `test_send_claim_def`

Risk:
- Low

Recommendations for QA:
- add ClaimDef txn
- try to add ClaimDef by the same user for the same Schema (should not be possible)
- try to add ClaimDef by a different user for the same Schema (should be possible)
- try to add ClaimDef by the same user for a different Schema (should be possible)
;;;","16/Jan/18 11:46 PM;VladimirWork;Build Info:
indy-node 1.2.273

Steps to Validate:
1. Send Schema as Trustee.
2. Add ClaimDef as Trustee for Step 1 Schema.
3. Try to add another one ClaimDef as Trustee for Step 1 Schema.
4. Send another Schema as Steward.
5. Add ClaimDef as Steward for Step 1 Schema.
6. Add ClaimDef as Trustee for Step 4 Schema.

Actual Results:
All steps are passed except Step 3 (as expected).;;;",,,,,,,,,,,,,,,,,,,,,,,
test_6th_node_join_after_view_change_by_master_restart fails intermittently,INDY-1084,26710,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,dsurnin,dsurnin,10/Jan/18 11:29 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyprj:",,,,,,INDY 18.01: Stability+,,,,,,,,,,,,,,,,,,,,anikitinDSR,dsurnin,VladimirWork,,,,,,,,,,"16/Jan/18 10:24 PM;anikitinDSR;Problem reason: 
- fails intermittently

Changes: 
- checkViewNoForNodes now checks that current viewNo >= expected. This fix include situation, when first view_change was failed.

PR:
- https://github.com/hyperledger/indy-plenum/pull/498


Version:
- master

Risk factors:
- view_change may occur several times with time=vc_count * view_change_timeout

Risk:
- Low

Recommendations for QA
- run test plenum/test/view_change/test_6th_node_join_after_view_change_by_master_restart ;;;","17/Jan/18 8:13 PM;VladimirWork;Build Info:
indy-plenum 1.2.218

Steps to Validate:
1. Run single test `pytest test_6th_node_join_after_view_change_by_master_restart` several times.
2. Run all plenum tests via runner.py `python3 runner.py --pytest ""python3 -m pytest"" --dir plenum --output test-result-plenum-1.ubuntu-05.txt` several times.

Actual Results:
plenum/test/view_change/test_6th_node_join_after_view_change_by_master_restart.py is not failed during test runs.;;;",,,,,,,,,,,,,,,,,,,,,,,
[Design] We need to design a new Node-to-Client communicaion approach as ZMQ replacement,INDY-1085,26733,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,ashcherbakov,ashcherbakov,11/Jan/18 5:24 PM,21/Jan/20 9:07 AM,28/Oct/23 2:47 AM,,,2.0,,,,0,GA-0,,,"We were working on a problem with incoming client connections (which may cause too many file opens exception due to limited number of file descriptors), see INDY-570.
And it turned out that ZMQ doesn't provide any handles to control incoming connections. The answer from ZMQ developers was to use firewall and authentication.

Although ZMQ works good for Node-to-Node communication, it looks like it's not the best choice for Node-to-Client communication because of mentioned ZMQ limitations.

So, we need to get rid of ZMQ as Node-to-Client solution and use something else.
- As an option, we can consider a similar approach as for Agent-to-Agent communication in libindy (use `authcrypt` and any transport, for instance pure http).
- We can implement Node-to-Client communication fully separately from Plenum (consensus) process using any highload frameworks and any languages.",,,,,,,,,,,,,,,,,IS-1326,,,,,,,,,,,,IS-1471,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1381,,,,,,,,,"1|hzwx4f:2rzlw",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,benjsmi,danielhardman,esplinr,,,,,,,,,"14/Apr/18 8:10 AM;danielhardman;I don't agree with the premise of this ticket. That is, I'm not convinced that ZMQ is a bad technology. It is entirely possible that I'm wrong, but I want to see some data to that effect, not just an assertion that it's necessary to make such a profound change.;;;","29/May/18 5:30 PM;ashcherbakov;Proofs and links on why we need it can be found here: [https://docs.google.com/document/d/1XghaJtCrBmql-HdhNtKyof-MTaZUVJ5akJ-Tjlc2bfg/edit#|https://docs.google.com/document/d/1XghaJtCrBmql-HdhNtKyof-MTaZUVJ5akJ-Tjlc2bfg/edit]
 * ZMQ (and hence Indy-Node) can not drop client connections from the server side if the client doesn’t close it on her side.
 ** https://jira.hyperledger.org/browse/INDY-1087
 ** https://jira.hyperledger.org/browse/INDY-1252
 ** [https://github.com/zeromq/libzmq/issues/2877]
 * So, If there are a lot of open connections from clients to pool, then the pool will not be able to accept any new clients/connections.;;;","07/Nov/18 8:15 AM;benjsmi;I'd like to add my +1 to this point.

One of the biggest things that influences the adoption of any technology is the ease with which a developer can get started with it.  Not how *powerful* or *performant* it is, but how easy it is *to get started*. People have short attention spans.

Most legit services use REST APIs these days, and given that we know security is concern, I don't see why anybody would have problems with either client-key authentication or maybe even something like JWTs or similar. Eventually, you'd probably want to manage a command and control list of apps/services that have access to your node, and that doesn't work well when they all share the same key ;)

I have used the Evernym (soon to be Indy) agent, and it's great.  It helps out a lot with the various procedures you need to do to issue and verify credentials, but I think the idea behind this post is that even when you've got the agent, you _still_ have to talk to it using the evernym-sdk or libindy or... all ZMQ under the covers. That's fine, but it decreases the number of places/ways in which a developer can deploy and use the agent, or the network.

The key thing in my opinion, is the reliance on an SDK and its dependencies. I give Indy major points for at least having language bindings such that it's usable for potentially any language, but what's wrong with well-secured REST?;;;","22/Jul/19 10:53 PM;esplinr;Additional requirements:
* Given that users of Indy have moved to production with their solutions, this transition needs to be done in a backwards compatible way.
** Indy SDK can drop support for ZeroMQ once networks have been upgraded.
** Indy Node will need to retain support for ZeroMQ client communication for an extended period of time to allow all clients the ability to upgrade.
* Indy Node will need to be extended to listen on more than one port in order to allow connections over ZeroMQ and HTTP transport.
* System tests, integration tests, and load tests need to be migrated to the new communication transport. The old communication transport does not need to be extensively tested once it is deprecated.;;;","23/Jul/19 1:39 AM;danielhardman;I strongly agree with the suggestion in the original post for the issue, that we use agent-to-agent (now ""didcomm"") as the governing paradigm for the solution.

This means we need to design and implement what Aries RFCs would call a ""protocol""; see Aries RFC 0003.

We can implement the protocol with HTTP being the only supported transport, if we like. This would address some of the sentiments expressed by [~benjsmi] above. However, we have to wrestle one important impedance, which is the intersection with certificates. Ben suggested a ""properly secured"" REST endpoint. That is usually code for TLS, and that means certificates.

The reason this gives me heartburn is that Indy is meant to replace certificates and CAs, because the trust of such mechanisms is fundamentally centralized. Having its major API surface depend on certs therefore feels problematic. It also means that every steward has to get a cert and keep it from expiring–and that every client has to automatically accept the certs from dozens, maybe hundreds, of validator nodes.

An alternative is to use plain HTTP, not HTTPS. If we are using DIDComm, plain HTTP can be secure, depending on how the client encrypts messages. This would eliminate cert management from steward concerns and cert acceptance from client code. ([~kdenhartog] has recently had a conversation about whether plain HTTP with DIDComm is ever a good idea and may want to say something here.);;;",,,,,,,,,,,,,,,,,,,,
test_request_not_accepted_if_agent_was_added_using_hex_as_pubkey fails intermittently,INDY-1086,26734,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,dsurnin,dsurnin,11/Jan/18 6:08 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyt27:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,,,,,,,,,,,"11/Jan/18 11:21 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-node/pull/515, https://github.com/hyperledger/indy-node/pull/516;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add iptables rules to limit the number of clients connections,INDY-1087,26736,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,sergey-shilov,sergey-shilov,11/Jan/18 6:45 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"The investigation done in scope of ticket [INDY-570|https://jira.hyperledger.org/browse/INDY-570] showed that there is no way to limit the number of clients connections using ZMQ API. So we need external firewall (iptables) to do it.

Corresponding iptables rule may be added manually by steward or automatically by install script. The questions here is what max number of sumultaneous connections should be specified? Just to remind: the main problem of non-limited number of clients connections is situation when we can not open some file as the limit of opened file descriptors is reached. The main point here is that we always should have ability to open files that are necessary for node functionality. So I propose the following solution:
   1. calculate approximate number of file descriptors needed to open local files, DBs etc. (F)
   2. calculate approximate number of file descriptors needed for communication with other nodes (N)
   3. define some window, i.e. some number of spare file descriptors as two steps above calculate file descriptors approximately (W)
   4. now we can calculate max number of clients connections (X): X = LimitNOFILE - (F + N + W)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-570,INDY-1037,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzytz3:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,3.0,,,,,,,,,,,,sergey-shilov,VladimirWork,,,,,,,,,,,"16/Jan/18 7:49 PM;sergey-shilov;*Problem state / reason:*

There is no way to limit the number of simultaneous clients connections using ZMQ API, so some external firewall is needed.

*Changes:*
 * Added iptables dependency to indy-node package
 * Added a script _setup_iptables_ that adds an iptables rule that limits the number of simultaneous connections per client port
 * Added a wrapper script _setup_indy_node_iptables_ for the script described above, this script uses indy-node environment file to get client port and recommended clients connections limit

*Committed into:*
 * [https://github.com/hyperledger/indy-node/pull/520]
 * [https://github.com/hyperledger/indy-node/pull/523]
 * indy-node 1.2.272-master

*Risk factors:*
 - Nothing is expected.

*Risk:*
 - Low

*Recommendations for QA:*

Check installation of indy-node deb package using apt, as a result of the installation process the iptables should be installed as a dependency,

Then run the _setup_indy_node_iptables_ script as a non-root, the following error message should appear:

_""Warning: iptables is not installed or permission denied, clients connections limit is not set.""_

Then run the _setup_indy_node_iptables_ script as a root, no any messages are expected. Then as a root check that a rule has been added using command below (example);

======================================================================
 # iptables -L

Chain INPUT (policy ACCEPT)
 target     prot opt source               destination         
 REJECT     tcp  --  anywhere             anywhere             tcp dpt:9702 flags:FIN,SYN,RST,ACK/SYN #conn src/0 > 15360 reject-with tcp-reset

Chain FORWARD (policy ACCEPT)
 target     prot opt source               destination         

Chain OUTPUT (policy ACCEPT)
 target     prot opt source               destination

======================================================================

*NOTE:* if the indy-node package is installed under the docker image and the docker container is run without _--privileged_ option (default for us now) then iptables should be installed as a dependency, but all operations on it are denied due to lack of privileges even if you are a root.

Also check upgrade (migration), expected:
 * clients connections limit is appended to /etc/indy/indy.env
 * setup_indy_node_iptables works;;;","19/Jan/18 1:11 AM;VladimirWork;Build Info:
indy-node 1.2.275

Steps to Validate:
0. Install pool.
1. Set connections limit using `setup_indy_node_iptables`.
2. Set connections limit using `setup_iptables` (e.g. `setup_iptables 9702 1`).
3. Install pool of version without iptables changes.
4. Upgrade it to 1.2.275+ version.
5. Retest Steps 1, 2 on upgraded pool.

Actual Results:
Script sets iptables policies as expected. CLIENT_CONNECTIONS_LIMIT is written into indy.env during migration.;;;",,,,,,,,,,,,,,,,,,,,,,,
Knowledge transfer on Indy build processes,INDY-1088,26760,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,andkononykhin,mgbailey,mgbailey,12/Jan/18 6:38 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"To provide for steward support, we need to disseminate understanding of the Indy node build processes more widely. 

Please provide time and documentation to educate [~mgbailey] and others as directed on the processes used to build the various parts of indy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzytyn:",,,,,,Sprint 18.02 Stability,,,,,,,,2.0,,,,,,,,,,,,andkononykhin,ashcherbakov,mgbailey,,,,,,,,,,"24/Jan/18 4:14 PM;ashcherbakov;I suggest to put the documentation right into the doc folder in indy-node.;;;","26/Jan/18 9:58 PM;ashcherbakov;Done in https://github.com/hyperledger/indy-node/pull/537;;;",,,,,,,,,,,,,,,,,,,,,,,
Sovrin package should be upgraded when doing ledger upgrades,INDY-1089,26761,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,mgbailey,mgbailey,12/Jan/18 6:46 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"When a ledger upgrade transaction is posted, the resulting upgrade does not upgrade the sovrin package.  While it is true that the sovrin package is actually changed infrequently, a new version to the package is created every time we do a release. The result to our stewards is that even though indy was upgraded, there remains a package, sovrin, that was not. This plays havoc with the automated tools that enterprises use to maintain the state of their systems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-738,,,,,,,,,"1|hzyt6v:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,,,,,,,,,,,"04/Sep/18 10:52 PM;ashcherbakov;Done in INDY-1491;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Planning for migration/upgrade to Ubuntu 18.04 (April 26, 2018)",INDY-1090,26804,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,,SeanBohan_Sovrin,SeanBohan_Sovrin,15/Jan/18 10:29 PM,11/Oct/19 9:03 PM,28/Oct/23 2:47 AM,11/Oct/19 9:03 PM,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1701,,,,,INDY-2186,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwxpj:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,SeanBohan_Sovrin,,,,,,,,,,,"15/Jan/18 10:36 PM;SeanBohan_Sovrin;We need to have a plan for migrating the pool and ensure our strategy on the roadmap for gaining some additional platform diversity. 
 # Need to define the impact this update will have on our dev and Stewards
 # Scheduled for April 26, Alpha1 delivered 1/11/18;;;","11/Oct/19 9:03 PM;ashcherbakov;Duplicates https://jira.hyperledger.org/browse/INDY-2186;;;",,,,,,,,,,,,,,,,,,,,,,,
Duplicate transactions posted to live ledger,INDY-1091,26812,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Won't Do,,mgbailey,mgbailey,16/Jan/18 5:38 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Two duplicate transactions appeared on the live ledger over the weekend.  These are transactions that I posted a few days earlier.  I was not logged into any computer at the time that the duplicate transactions were posted.
{code:java}
[33,{""dest"":""MxZqWmuLHXPNzXpho32pus"",""identifier"":""NMjQb59rKTJXKNqVYfcZFi"",""reqId"":1510865743895378,""role"":""2"",""signature"":""2h4BP2w23xymsqVFpWNpJSKafmywBw2SBuSgipx2KJLF7aFeE652KpxG8gWWf2fanojfqMLmEbnUN4UcHCc1ryXQ"",""txnTime"":1510865743,""type"":""1"",""verkey"":""~R9T1TRJM3Smzm1AdZW6Kq7""}]
[34,{""dest"":""7SeddhvXz8NXMuQ3Y2Wda3"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515166029574854,""signature"":""55VnBQhXMUNfuzbVfKDHPdwVghSXD8kWsitsguH2GmWtRYLPvd4zbxNckKE8uPHtFEH2gPtMUizKuhS5jGeCuRks"",""txnTime"":1515166031,""type"":""1"",""verkey"":""~PMiFnDt9fmNnSYzeEBY7YQ""}]
[35,{""dest"":""7SeddhvXz8NXMuQ3Y2Wda3"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515167900196091,""signature"":""4cMRxUHoS57AoevpV8gRaosLS5k3ssjDYCx9F2STjcDHfHvxQkkFeRmgKWgPKTWHrm1CWjPGsawUpidY66E8MxDi"",""txnTime"":1515168633,""type"":""1"",""verkey"":""~PMiFnDt9fmNnSYzeEBY7YQ""}]
[36,{""dest"":""7SeddhvXz8NXMuQ3Y2Wda3"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515169081791625,""signature"":""ja114sURP9uQ38jujR2a1VvJEzSwf975BaTpuRqN3e3JbMSztU22yggo6sCwgygFsYbxX2fjQDhC8GUYcTQETiY"",""txnTime"":1515169814,""type"":""1"",""verkey"":""~PMiFnDt9fmNnSYzeEBY7YQ""}]
[37,{""dest"":""7RpGhzS7pdQAiyLAUcwBYL"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515715477356351,""signature"":""2n2iUCWsBkegD5oZjTZDgM9Ru6E5jK9p2nFPywEHhfQwVX5MKdyAMXuyoFFgUyy549gASdnAwZqydnVRQSSimFQA"",""txnTime"":1515716595,""type"":""1"",""verkey"":""~HBVK9kG4tnjGzLnnN4id6z""}]
[38,{""dest"":""Devind6md7x7PVnMxUwTvx"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515790849028036,""signature"":""2cmrjckq4gEBk3VZw6w7CDSv2XnX5UMtmYoaikuXqNazYvjCRqNQQTeeHtTEMnFuvNrZ62RfdWEDsYdYXrhJjisH"",""txnTime"":1515794841,""type"":""1"",""verkey"":""7u1rZYdUac2M18xBpjMapKrAcbt39uc9pBxg9aDvNr1b""}]
[39,{""dest"":""7SeddhvXz8NXMuQ3Y2Wda3"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515167900196091,""signature"":""4cMRxUHoS57AoevpV8gRaosLS5k3ssjDYCx9F2STjcDHfHvxQkkFeRmgKWgPKTWHrm1CWjPGsawUpidY66E8MxDi"",""txnTime"":1515963033,""type"":""1"",""verkey"":""~PMiFnDt9fmNnSYzeEBY7YQ""}]
[40,{""dest"":""7SeddhvXz8NXMuQ3Y2Wda3"",""identifier"":""J4N1K1SEB8uY2muwmecY5q"",""reqId"":1515169081791625,""signature"":""ja114sURP9uQ38jujR2a1VvJEzSwf975BaTpuRqN3e3JbMSztU22yggo6sCwgygFsYbxX2fjQDhC8GUYcTQETiY"",""txnTime"":1515963033,""type"":""1"",""verkey"":""~PMiFnDt9fmNnSYzeEBY7YQ""}]
{code}
In the snippet of the domain ledger shown, transactions 39 and 40 are duplicates of 35 and 36, even having the identical reqId.

The node that I have access to did not restart services or anything similar over the weekend.

Attached are logs and ledgers from ev1 (the Evernym node).","Live ledger pool, running 1.1.43.",,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1047,,,,,,,,,,,,,,,,,,,,,,"16/Jan/18 5:37 AM;mgbailey;ev1Files.tgz;https://jira.hyperledger.org/secure/attachment/14200/ev1Files.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzytzj:",,,,,,INDY 18.01: Stability+,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,krw910,mgbailey,ozheregelya,,,,,,,,,,"16/Jan/18 5:49 AM;mgbailey;These ledger transactions also show something erroneous that I thought was the subject of another ticket, but which I cannot locate.  It is that the steward wrote a NYM with DID and verkey.  He then attempted twice more to write it, with the same DID but a different verkey.  The message in the CLI showed that it was rejected, but an entry wound up in the ledger anyway.  This is where transactions 35 & 36 came from.;;;","16/Jan/18 6:48 AM;krw910;[~ozheregelya] We need you to try and duplicate this issue using version 1.1.43 of Stable. The main issue in this ticket is that the pool wrote a transaction days later which is a duplicate of earlier transactions. If you can duplicate it with 1.1.43 try with 1.2.50.;;;","16/Jan/18 6:53 AM;ozheregelya;[~krw910], description looks like INDY-1047. I'll compare logs and try to reproduce this problem on old stable tomorrow.;;;","17/Jan/18 11:28 PM;ozheregelya;[~krw910], [~mgbailey],
As I can see in logs, duplicate transactions were written right after view change. So, steps are the same as in INDY-1047. INDY-1047 was reproduced on indy-node 1.2.242 version. It means that this problem will reproduce on both previous stables: 1.1.43 and 1.2.50. But this problem was fixed in the latest master.
I think we should close this ticket because it was re-tested during re-testing of INDY-1047 and wasn't reproduced.;;;","18/Jan/18 8:17 PM;ozheregelya;If nobody objects, I'll move it to 'won't fix'.;;;",,,,,,,,,,,,,,,,,,,,
Test current View Change protocol,INDY-1092,26877,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/Jan/18 11:31 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"We need to have more intensive testing of the current view change protocol:
 * that view is changed on performance degradation;
 * that view is changed on disconnections of primary;
 * that view change works properly with random delays;
 * that view change works properly when some nodes in the pool are down;
 * that view change happens when primary behaves maliciously (sends incorrect data for example);
 * we need to test it also in a quite large and distributed pool

We need to design more scenarios and test them.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1112,INDY-1113,INDY-1114,,,,,,,,,,,,,"25/Jan/18 11:57 PM;VladimirWork;AWS_the_same_primary_for_both_instances_on_not_demoted_nodes.7z;https://jira.hyperledger.org/secure/attachment/14408/AWS_the_same_primary_for_both_instances_on_not_demoted_nodes.7z","25/Jan/18 11:56 PM;VladimirWork;docker_no_view_change_due_to_degradation.tar.gz;https://jira.hyperledger.org/secure/attachment/14407/docker_no_view_change_due_to_degradation.tar.gz","30/Jan/18 12:32 AM;VladimirWork;issue_with_view_changer_timeout.tar.gz;https://jira.hyperledger.org/secure/attachment/14500/issue_with_view_changer_timeout.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwuy7:",,,,,,Sprint 18.02 Stability,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"25/Jan/18 11:57 PM;VladimirWork;that view is changed on performance degradation;
 - run load script on primary node (in docker for faster node degradation): 1 node of 4 tries to  make view change due to primary performance degradation only so unable to change primary this way (docker logs [^docker_no_view_change_due_to_degradation.tar.gz])
 - run load script on separate client (for continuous equal load for the whole pool): unable to force view change this way
 - run cpuburn on primary: unable to force view change this way
 
that view is changed on disconnections of primary;
- works properly after many view changes done on long lived pool: ok
- the next possible primaries (1, 2) are stopped/demoted (more than 4 nodes are needed): ok
- add/demote/promote nodes of the pool (AWS logs [^AWS_the_same_primary_for_both_instances_on_not_demoted_nodes.7z]):

{noformat}
2018-01-24 15:34:59,386 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node1 changed to view 1, will start catchup now
2018-01-24 15:34:59,459 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node1:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]
2018-01-24 15:34:59,460 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node1:1 declares view change 1 as completed for instance 1, new primary is Node4:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 14:33:43,092 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node2 changed to view 1, will start catchup now
2018-01-24 14:33:43,165 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node2:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')]
2018-01-24 15:34:59,340 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node2:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 14:33:43,085 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node3 changed to view 1, will start catchup now
2018-01-24 14:33:43,174 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node3:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')]
2018-01-24 14:33:45,094 | INFO     | view_changer.py      (380) | sendInstanceChange | VIEW CHANGE: Node3 sending an instance change with view_no 2 since Primary of master protocol instance disconnected
2018-01-24 15:34:59,338 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node3:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 14:33:43,088 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node4 changed to view 1, will start catchup now
2018-01-24 14:33:43,177 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node4:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')]
2018-01-24 15:34:59,313 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node4:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 15:35:54,923 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node5 changed to view 1, will start catchup now
2018-01-24 15:35:55,048 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node5:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 9, '8MMN1HuP2XrkiCubNXk9Cqw685UUCJL93SGFksaq1gEd'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]
2018-01-24 15:35:55,049 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node5:1 declares view change 1 as completed for instance 1, new primary is Node4:1, ledger info is [(0, 9, '8MMN1HuP2XrkiCubNXk9Cqw685UUCJL93SGFksaq1gEd'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

{noformat}

that view change works properly with random delays;
- added `sleep` in  process_vchd_msg of ~/plenum/server/view_change/view_changer.py: 1st node can't reselect primary when 4th (previous primary) is restarting/disconnecting but 2nd and 3rd select 3rd as primary (and pool can'r write NYMs (can read them only) in this case, see logs [^issue_with_view_changer_timeout.tar.gz])

that view change works properly when some nodes in the pool are down;
- node 1(2) of 4(7) is shutted down: unable to rotate primary between the remaining at n-f: clarification is needed
- nodes 2(3) of 4(7) is shutted down: unable to elect primary at less than n-f: ok

that view change happens when primary behaves maliciously (sends incorrect data for example);
- added malformed data in create3PCBatch of ~/plenum/server/replica.py: nodes reselect primary with ""since Primary of master protocol instance degraded the performance"" message during NYMs sending, ok

we need to test it also in a quite large and distributed pool
- cases with view change caused by primary disconnecions (with some switched off nodes) on QA Live Pool: ok;;;","31/Jan/18 10:52 PM;ashcherbakov;As for `that view is changed on performance degradation;` items:

we have INDY-34 which may be a cause of the issues.;;;","31/Jan/18 11:45 PM;VladimirWork;Separate tickets for found issues are created and linked to this ticket. Also exploratory INDY-1115 is created.;;;",,,,,,,,,,,,,,,,,,,,,,
Research replacement of ZMQ for client-to-node communication,INDY-1093,26920,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,,SeanBohan_Sovrin,SeanBohan_Sovrin,18/Jan/18 6:48 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,We should default to using the same functions as agent-to-agent communications,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-41,,,,,,,,,"1|hzytu7:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,SeanBohan_Sovrin,,,,,,,,,,,"24/Jan/18 12:55 AM;ashcherbakov;Looks like a duplicate of https://jira.hyperledger.org/browse/INDY-1085;;;",,,,,,,,,,,,,,,,,,,,,,,,
Need to configure Jenkins to store logs or/and artifacts on external storage,INDY-1094,26930,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,18/Jan/18 5:10 PM,11/Oct/19 8:50 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"Number of logs and size of artifacts lead to lack of free space on jenkins server. We have set some limits for number of days and number of jobs to keep but it's a temporary solution and we will have to decrease the limits very soon. Moreover it's not a good idea at all because for some branches/jobs it's better to keep as long history as possible.

Currently we have the following sizes for jenkins jobs:

{code:java}
/var/lib/jenkins/jobs$ du -sb * | sort -nr 
45829779230     Evernym Agency 
17070585160     agency 
12410592367     Indy SDK CI 
5526072100      Indy SDK CD 
4502267867      agency-integrationtests 
710663519       Indy-Plenum 
531781791       Indy-Node 
183536093       Indy Crypto CD 
64074962        Indy-Anoncreds 
49378018        Indy Crypto CI 
47050675        indy-node-verify-x86_64_copy 
45924950        Sovrin 
18217297        Test 
...
{code}

 

Options:
 # extrenal storage for all or some pipelines
 ** explore options how and where to store logs and especially artifacts outside the jenkins server machine
 ** re-configure Jenkins or/and pipelines
 # review and fix pipelines that produce most heavy logs/artifacts",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-766,,,,,,,,,"1|hzytvr:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,SeanBohan_Sovrin,,,,,,,,,,,"23/Jan/18 6:47 AM;SeanBohan_Sovrin;[~andkononykhin] can you check in with [~ckochenower] and coordinate;;;","24/Jan/18 12:17 AM;andkononykhin;[~SeanBohan_Sovrin] sure. Will do that today.;;;",,,,,,,,,,,,,,,,,,,,,,,
Blocking Issue: Large pool stopped working during load test,INDY-1095,26936,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,ozheregelya,ozheregelya,19/Jan/18 12:45 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Steps to Reproduce:
1. Setup the pool of 7 nodes and add 18 nodes manually.
2. Send 1 transaction manually.
3. Run python3 Perf_Add_nyms.py -n 10
=> transactions successfully written.
4. Run python3 Perf_Add_nyms.py -s 5 -n 100
=> transactions successfully written.
5. Run python3 Perf_Add_nyms.py -s 5 -n 10000

Actual Results:
Pool stopped working after writing of ~300 transactions.
Count of domain transactions: 291 on 1-7 nodes, 310 on 8 node, and 311 on remaining nodes.

Expected Results:
Pool should work.",indy-node 1.2.275,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1025,,,,,,,,,,,,,,,,,,,,"19/Jan/18 12:46 AM;ozheregelya;Node1.log;https://jira.hyperledger.org/secure/attachment/14302/Node1.log","19/Jan/18 12:46 AM;ozheregelya;Node10.log;https://jira.hyperledger.org/secure/attachment/14311/Node10.log","19/Jan/18 12:46 AM;ozheregelya;Node11.log;https://jira.hyperledger.org/secure/attachment/14312/Node11.log","19/Jan/18 12:46 AM;ozheregelya;Node12.log;https://jira.hyperledger.org/secure/attachment/14313/Node12.log","19/Jan/18 12:46 AM;ozheregelya;Node13.log;https://jira.hyperledger.org/secure/attachment/14314/Node13.log","19/Jan/18 12:46 AM;ozheregelya;Node14.log;https://jira.hyperledger.org/secure/attachment/14315/Node14.log","19/Jan/18 12:46 AM;ozheregelya;Node15.log;https://jira.hyperledger.org/secure/attachment/14316/Node15.log","19/Jan/18 12:46 AM;ozheregelya;Node16.log;https://jira.hyperledger.org/secure/attachment/14317/Node16.log","19/Jan/18 12:46 AM;ozheregelya;Node17.log;https://jira.hyperledger.org/secure/attachment/14318/Node17.log","19/Jan/18 12:46 AM;ozheregelya;Node18.log;https://jira.hyperledger.org/secure/attachment/14319/Node18.log","19/Jan/18 12:46 AM;ozheregelya;Node19.log;https://jira.hyperledger.org/secure/attachment/14320/Node19.log","19/Jan/18 12:46 AM;ozheregelya;Node2.log;https://jira.hyperledger.org/secure/attachment/14303/Node2.log","19/Jan/18 12:46 AM;ozheregelya;Node20.log;https://jira.hyperledger.org/secure/attachment/14321/Node20.log","19/Jan/18 12:46 AM;ozheregelya;Node21.log;https://jira.hyperledger.org/secure/attachment/14322/Node21.log","19/Jan/18 12:46 AM;ozheregelya;Node22.log;https://jira.hyperledger.org/secure/attachment/14323/Node22.log","19/Jan/18 12:46 AM;ozheregelya;Node23.log;https://jira.hyperledger.org/secure/attachment/14324/Node23.log","19/Jan/18 12:46 AM;ozheregelya;Node24.log;https://jira.hyperledger.org/secure/attachment/14325/Node24.log","19/Jan/18 12:46 AM;ozheregelya;Node25.log;https://jira.hyperledger.org/secure/attachment/14326/Node25.log","19/Jan/18 12:46 AM;ozheregelya;Node3.log;https://jira.hyperledger.org/secure/attachment/14304/Node3.log","19/Jan/18 12:46 AM;ozheregelya;Node4.log;https://jira.hyperledger.org/secure/attachment/14305/Node4.log","19/Jan/18 12:46 AM;ozheregelya;Node5.log;https://jira.hyperledger.org/secure/attachment/14306/Node5.log","19/Jan/18 12:46 AM;ozheregelya;Node6.log;https://jira.hyperledger.org/secure/attachment/14307/Node6.log","19/Jan/18 12:46 AM;ozheregelya;Node7.log;https://jira.hyperledger.org/secure/attachment/14308/Node7.log","19/Jan/18 12:46 AM;ozheregelya;Node8.log;https://jira.hyperledger.org/secure/attachment/14309/Node8.log","19/Jan/18 12:46 AM;ozheregelya;Node9.log;https://jira.hyperledger.org/secure/attachment/14310/Node9.log","20/Jan/18 4:55 AM;ozheregelya;debug_logs1.7z;https://jira.hyperledger.org/secure/attachment/14333/debug_logs1.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs10.7z;https://jira.hyperledger.org/secure/attachment/14342/debug_logs10.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs11.7z;https://jira.hyperledger.org/secure/attachment/14343/debug_logs11.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs12.7z;https://jira.hyperledger.org/secure/attachment/14344/debug_logs12.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs13.7z;https://jira.hyperledger.org/secure/attachment/14345/debug_logs13.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs14.7z;https://jira.hyperledger.org/secure/attachment/14346/debug_logs14.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs15.7z;https://jira.hyperledger.org/secure/attachment/14347/debug_logs15.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs16.7z;https://jira.hyperledger.org/secure/attachment/14348/debug_logs16.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs17.7z;https://jira.hyperledger.org/secure/attachment/14349/debug_logs17.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs18.7z;https://jira.hyperledger.org/secure/attachment/14350/debug_logs18.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs19.7z;https://jira.hyperledger.org/secure/attachment/14351/debug_logs19.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs2.7z;https://jira.hyperledger.org/secure/attachment/14334/debug_logs2.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs20.7z;https://jira.hyperledger.org/secure/attachment/14352/debug_logs20.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs21.7z;https://jira.hyperledger.org/secure/attachment/14353/debug_logs21.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs22.7z;https://jira.hyperledger.org/secure/attachment/14354/debug_logs22.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs23.7z;https://jira.hyperledger.org/secure/attachment/14355/debug_logs23.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs24.7z;https://jira.hyperledger.org/secure/attachment/14356/debug_logs24.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs25.7z;https://jira.hyperledger.org/secure/attachment/14357/debug_logs25.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs3.7z;https://jira.hyperledger.org/secure/attachment/14335/debug_logs3.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs4.7z;https://jira.hyperledger.org/secure/attachment/14336/debug_logs4.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs5.7z;https://jira.hyperledger.org/secure/attachment/14337/debug_logs5.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs6.7z;https://jira.hyperledger.org/secure/attachment/14338/debug_logs6.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs7.7z;https://jira.hyperledger.org/secure/attachment/14339/debug_logs7.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs8.7z;https://jira.hyperledger.org/secure/attachment/14340/debug_logs8.7z","20/Jan/18 4:55 AM;ozheregelya;debug_logs9.7z;https://jira.hyperledger.org/secure/attachment/14341/debug_logs9.7z","19/Jan/18 1:03 AM;ozheregelya;jctl20.txt;https://jira.hyperledger.org/secure/attachment/14329/jctl20.txt","19/Jan/18 1:03 AM;ozheregelya;jctl7.txt;https://jira.hyperledger.org/secure/attachment/14327/jctl7.txt","19/Jan/18 1:03 AM;ozheregelya;jctl8.txt;https://jira.hyperledger.org/secure/attachment/14328/jctl8.txt",,,,,,,,,INDY-1032,,,,,,,,,"1|hzyvrb:",,,,,,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,krw910,ozheregelya,spivachuk,zhigunenko.dsr,,,,,,,,,"19/Jan/18 2:35 AM;krw910;This is blocking the following tickets:
INDY-1054
INDY-1025
INDY-1034;;;","20/Jan/18 4:56 AM;ozheregelya;The issue is reproducing on the latest master 1.2.279.
 DEBUG logs and journalctl are attached in archives.;;;","20/Jan/18 4:58 AM;ozheregelya;[~krw910], 
After restart of whole pool load test works fine, so this problem doesn't block testing with load test fully.;;;","23/Jan/18 12:25 AM;krw910;[~ozheregelya] If we can not get past 400 transactions then it does block the other tickets I have. Restarting the pool has always seemed to fix the issues, but we have to be able to run thousands not hundreds of transactions.;;;","23/Jan/18 2:33 AM;ozheregelya;[~krw910], 
Last week I was able to send ~4500 transactions after restart the pool. I had stopped on this value only because of ""No space left on device"" error which was caused by too big syslog file.;;;","01/Feb/18 4:03 AM;spivachuk;*Problem reason:*

When a replica orders a 3PC-batch, it checks whether it has an own checkpoint with the bounds including {{ppSeqNo}} of this batch. If it does not have such one, it creates a new checkpoint with the lower bound equal to {{ppSeqNo}} of the batch and the upper bound equal to {{ppSeqNo + CHK_FREQ - 1}}. So while a replica orders every next batch in a view, bounds of checkpoints which it creates are predictable. Under these conditions an upper bound of any checkpoint is divisible by {{CHK_FREQ}}.

Bounds of checkpoints may be shifted if a replica did not participate in ordering of some 3PC-batches in a view, the node got the transactions contained in these 3PC-batches via catch-up and after that the replica orders next 3PC-batches in the same view. Shifted bounds of a checkpoint prevent to stabilize it because the replica does not receive checkpoints with the same bounds from other replicas in the instance. Other replicas, in turn, may not stabilize their checkpoints with regular bounds since they may not gather a quorum of checkpoints with the same bounds from other replicas in the instance because some replicas send checkpoints with shifted bounds.

In the case described in the ticket more than {{f}} nodes in the pool caught up in the the same view. So checkpoints on replicas of these nodes were created with shifted bounds. Replicas did not gather quorum of checkpoints to stabilize them, so did not move the watermarks forward and eventually stopped to process 3PC-messages when their {{ppSeqNos}} exceeded the high watermark.

*Changes:*
- Fixed an issue with possible shift of checkpoint bounds that had resulted in inability to stabilize the current and all the following checkpoints in the current view. Now the upper bound of a checkpoint is always set to a value divisible by {{CHK_FREQ}}. So the second checkpoint after a catch-up can always be stabilized and so the watermarks will be moved forward. (With the default configuration settings, ordering of 3PC-batches can survive 2 not stabilized checkpoints because the watermarks window size - {{LOG_SIZE}} - is {{3 * CHK_FREQ}} by default.)
- Added a test verifying that the upper bound of the checkpoint after a catch-up is divisible by {{CHK_FREQ}} config parameter.
- Added a test verifying that the second checkpoint after a catch-up can be stabilized.
- Added a test from [~anikitinDSR] which verifies that more than {{LOG_SIZE}} batches can be ordered in one view after more than {{f}} nodes caught up in this view when some 3PC-batches had already been ordered in this view.
- Made minor corrections in existing tests of checkpoints.

*PRs:*
- https://github.com/hyperledger/indy-node/pull/544
- https://github.com/hyperledger/indy-plenum/pull/510

*Version:*
- indy-node 1.2.290-master
- indy-plenum 1.2.231-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_upper_bound_of_checkpoint_after_catchup_is_divisible_by_chk_freq}}
- {{test_second_checkpoint_after_catchup_can_be_stabilized}}
- {{test_ordering_after_more_than_f_nodes_caught_up}}

*Recommendations for QA:*
- To verify the fix it is sufficient to deploy a pool of 4 nodes, add 2 new nodes to the pool and then send more than 300 domain transactions one by one with one second interval. In case if all the transactions have been successfully committed into the ledger, it makes sense to ensure that all the nodes in the pool are in the view 0.;;;","02/Feb/18 5:05 PM;zhigunenko.dsr;*Environment:*
* indy-node 1.2.291
* indy-plenum 1.2.231

*Steps to Reproduce:*
CASE 1
1. Setup the pool of 4 nodes
2. Send 1 transaction manually.
3. Run python3 Perf_Add_nyms.py -n 10
=> transactions successfully written.
4. Run python3 Perf_Add_nyms.py -s 5 -n 100
=> transactions successfully written.
5. Run python3 Perf_Add_nyms.py -s 5 -n 10000
6. add node5 and node6
7. Run another python3 Perf_Add_nyms.py -s 5 -n 10000
CASE 2
1. Setup the pool of 4 nodes
2. Add 2 nodes manually one by one
3. Run python3 Perf_Add_nyms.py -s 5 -n 10000

*Actual results:*
All transactions (before and after nodes addition) successfully recordered in all nodes
_read_ledger --type domain --count_ show the same values in all nodes
_validator-info_ show the same values in all nodes

*Additional info:*
Suspicious differences in the logs of nodes will allocated into INDY-1146;;;",,,,,,,,,,,,,,,,,,
It does not make sense to have the number of validators to be not a multiple of 3f+1,INDY-1097,27010,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,23/Jan/18 4:26 PM,24/Jan/18 7:07 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"It doesn't make any sense to have a number of nodes in the pool which is not a multiple of 3f+1. We don't have any benefit in it, moreover, things are getting more fragile and more chatty. (see, for example, [http://pmg.csail.mit.edu/papers/osdi99.pdf]). That's why all papers assume consensus of 2f+1, and not n-f.

 

Probably we can not force this restriction, but we can consider the following:

1) Update documentation with a suggestion about a proper number of nodes in the pool

2) Consider making new stewards observers and then promote them in sets of three",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyubr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,SeanBohan_Sovrin,,,,,,,,,,,"24/Jan/18 7:07 AM;SeanBohan_Sovrin;Political and trust framework reasons go beyond tech requirements stated here. This is low priority and we feel this should stay in the backlog and addressed thru documentation. 

 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Wrong message on cancellation of not existing upgrade,INDY-1098,27011,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,ozheregelya,ozheregelya,23/Jan/18 7:04 PM,24/Jan/18 7:36 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*Steps to Reproduce:*
1. Open any CLI.
2. Set yourself as TRUSTEE.
3. Send POOL_UPGRADE:
  * Old CLI:
{code:java}
send POOL_UPGRADE name=upgrade333 version=1.2.350 sha256=f6f2ea8f45d8a057c9566a33f99474da2e5c6a6604d736121650e2730c6fb0a3 action=cancel schedule={'Gw6pDLhcBcoQ
esN72qfotTgFa7cbuqZpkX3Xo6pLhPhv':'2020-08-25T09:50:00.000000+00:00','8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb':'2020-08-25T09:55:00.000000+00:00','DKVxG2fXXTU8yT5N7hGEb
XB3dfdAnYv1JczDUHpmDxya':'2020-08-25T10:00:00.000000+00:00','4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA':'2020-08-25T10:05:00.000000+00:00','4SWokCJWJc69Tn74VvLS6t2G2ucvXq
M9FDMsWJjmsUxe':'2020-08-25T10:15:00.000000+00:00','Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8':'2000-08-25T10:25:00.000000+00:00'} timeout=10 reinstall=True{code}
  * New CLI:
{code:java}
ledger pool-upgrade name=upgrade222 version=1.2.350 sha256=f6f2ea8f45d8a057c9566a33f99474da2e5c6a6604d736121650e2730c6fb0a3 action=cancel schedule={""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2020-08-25T09:50:00.000000+00:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2020-08-25T09:55:00.000000+00:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2020-08-25T10:00:00.000000+00:00"",""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2020-08-25T10:05:00.000000+00:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2000-08-25T10:15:00.000000+00:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2000-08-25T10:25:00.000000+00:00""} timeout=10 reinstall=true{code}
*Actual Result:*
  * Old CLI:
{code:java}
Pool upgrade failed: client request invalid: UnauthorizedClientRequest('TRUSTEE cannot do POOL_UPGRADE',){code}
  * New CLI:
{code:java}
Transaction has been rejected: ""TRUSTEE cannot do POOL_UPGRADE\'""{code}
*Expected Result:*
Message should be ""Not existing upgrade can't be cancelled.""","indy-node 1.2.271
indy-cli 1.3.0~323
libindy 1.3.0~323",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyubz:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,SeanBohan_Sovrin,,,,,,,,,,,"24/Jan/18 7:05 AM;SeanBohan_Sovrin;[~ozheregelya]

Did user have permissions to submit transaction?
Did this version exist (guessing it doesn't)

Are you trying to cancel the upgrade or submit a new one?;;;","24/Jan/18 7:36 PM;ozheregelya;[~SeanBohan_Sovrin],
Yes, my user is default TRUSTEE.
Yes, you are correct about the version, but we don't validate it for existence. The only validation for version is that it is greater than or equal to current one. 
I tried cancel of upgrade. If I will change action=cancel to action=start, upgrade will be successfully scheduled.

I just copied wrong transaction with action=cancel instead of action=start and I was not able to find my mistake for a long time because of wrong message. It says that TRUSTEE can't schedule upgrade (but actually he can) when the problem is that I'm trying to cancel not existing upgrade.;;;",,,,,,,,,,,,,,,,,,,,,,,
New transactions wipe out old ones in bad data ledger,INDY-1099,27016,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,VladimirWork,VladimirWork,23/Jan/18 11:09 PM,09/Oct/18 6:59 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Build Info:
indy-node 1.2.279

Steps to Reproduce:
1. Install pool of 5 nodes.
2. Stop all nodes.
3. Delete value of ""dest"" attribute (""V4SGRU86Z58d6TV7PBUe6f"") in default Trustee's entry of domain ledger transactions .ldb file on all nodes.
4. Delete all tree and state folders (with merkle leaves/nodes and state affixes) of all ledgers.
5. Start all nodes.
6. Check nodes' status and domain ledger by `read_ledger` tool (some of transactions in ledger are lost and current seqNo counter is not ""the last entry seqNo + 1"").
7. Send NYMs to reach old transactions' seqNo in domain ledger and increase it up (so new added NYM have the same seqNo as old one).

Actual Results:
New transactions (NYMs) wipe out old ones if their seqNos intersect.

Additional Info:
We have explicit data loss (there are 10 transactions in domain ledger with seqNos from 19 to 29 after Step 5, so the first 18 transactions were lost) due to 1 value in 1 transaction was removed and implicit data loss due to broken seqNo counter that was set to `13` instead of `30`.

The priority of this ticket is low now (case with bad data ledger is not common) but it can be discussed additionally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwyg7:",,,,,,,,,,,,,,,,,,,,,,,,,,krw910,VladimirWork,,,,,,,,,,,"06/Mar/18 2:24 AM;krw910;[~VladimirWork] Is there a way to create a bad ledger through a normal use case without having to manually delete entries?;;;","06/Mar/18 5:08 PM;VladimirWork;[~krw910] >Is there a way to create a bad ledger through a normal use case without having to manually delete entries?
No, because of several levels of txn validation at client's and node's sides.;;;",,,,,,,,,,,,,,,,,,,,,,,
Malformed default Trustee can send transactions to ledger,INDY-1100,27017,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,VladimirWork,VladimirWork,23/Jan/18 11:09 PM,09/Oct/18 6:59 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Build Info:
indy-node 1.2.279

Steps to Reproduce:
1. Install pool of 5 nodes.
2. Stop all nodes.
3. Delete value of ""dest"" attribute (""V4SGRU86Z58d6TV7PBUe6f"") in default Trustee's entry of domain ledger transactions .ldb file on all nodes.
4. Delete all tree and state folders (with merkle leaves/nodes and state affixes) of all ledgers.
5. Start all nodes and client for this pool.
6. Send NYMs using default Trustee's key (`new key with seed 000000000000000000000000Trustee1`).

Actual Results:
NYMs are added successfully and the signer of this transactions in ledger is default Trustee (""identifier"" is default Trustee's DID) but his DID (""dest"") is absent in ledger (Step 3) so the currently absent in ledger entity can add transactions to it (we suppose that Trustee's DID is still present in some cache storage like idr_cache_db).

The priority of this ticket is low now (case with bad data ledger is not common) but it can be discussed additionally.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-286,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwygn:",,,,,,,,,,,,,,,,,,,,,,,,,,krw910,VladimirWork,,,,,,,,,,,"06/Mar/18 2:24 AM;krw910;[~VladimirWork] Is there a way to create a bad ledger through a normal use case without having to manually delete entries?;;;","06/Mar/18 5:08 PM;VladimirWork;[~krw910] >Is there a way to create a bad ledger through a normal use case without having to manually delete entries?
No, because of several levels of txn validation at client's and node's sides.;;;",,,,,,,,,,,,,,,,,,,,,,,
Upgrade from 1.2.214 (~1.2.50stable) to 1.2.279 don't work,INDY-1101,27020,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,ozheregelya,ozheregelya,24/Jan/18 1:33 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Steps to Reproduce:*
1. Setup the pool using 1.2.214 version (master analog of latest stable).
2. Schedule valid upgrade like
{code:java}
send POOL_UPGRADE name=upgrade12279 version=1.2.279 sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv': '2018-01-23T11:30:00.000000+00:00', '8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-01-23T11:35:00.000000+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya': '2018-01-23T11:40:00.000000+00:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA': '2018-01-23T11:45:00.000000+00:00', '4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe': '2018-01-23T11:50:00.000000+00:00', 'Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8': '2018-01-23T11:55:00.000000+00:00', 'BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW': '2018-01-23T12:00:00.000000+00:00', '98VysG35LxrutKTNXvhaztPFHnx5u9kHtT7PnUGqDa8x': '2018-01-23T12:05:00.000000+00:00', '6pfbFuX5tx7u3XKz8MNK4BJiHxvEcnGRBs1AQyNaiEQL': '2018-01-23T12:10:00.000000+00:00', 'HaNW78ayPK4b8vTggD4smURBZw7icxJpjZvCMLdUueiN': '2018-01-23T12:15:00.000000+00:00', '2zUsJuF9suBy2iKkcgmm8uoMB6u5Dq2oHoRuchrZbj2N': '2018-01-23T12:20:00.000000+00:00', 'BXV4SXKEJeYQ8XCRHgpw1Xume5ntqALsRhbUYcF85Mse': '2018-01-23T12:25:00.000000+00:00', '71WAtEevzz8aZr8baNJhQCUDLwRhM7LeaErSKNWWKxzn': '2018-01-23T12:30:00.000000+00:00', 'FEUGMFWCSAM725vyH8JZnsitiNUy31NPhugVKb8zDpng': '2018-01-23T12:35:00.000000+00:00', 'DPZ8GJ1NyNZGJMU6qQZVuBsumY1aVzvcV4FqQK9Y215x': '2018-01-23T12:40:00.000000+00:00', 'FYDoBrDhfGuSwt39Sgd3DZETihpnXy6SzZBggyD9HMrD': '2018-01-23T12:45:00.000000+00:00', 'EMNhsHNsEpuffxCmgC3fpwVj7LgwtSm3riSizCMN6MBo': '2018-01-23T12:50:00.000000+00:00', 'HD1XnVG6jXqGdmFMDTdJk3AoChxaqTfa6zGLkyXTtHwH': '2018-01-23T12:55:00.000000+00:00', 'DUGXi5vxRZcrDC8VPZFU6bpiHDMhnWic9tDaoDJv3Bj6': '2018-01-23T13:00:00.000000+00:00', 'D7jphMASPQAD6UFvT2ULjEfYybCJVDzwvfG5ZWJoXa69': '2018-01-23T13:05:00.000000+00:00', '7vcRBffPvKuGQz4F1ThYAo3Ucq3rXgU62enf6d23u8KX': '2018-01-23T13:10:00.000000+00:00', 'DfSoxVHbbdZrAmwTJcRqM2arwUSvK3L6PXjqWHGo58xD': '2018-01-23T13:15:00.000000+00:00', 'FTBmYnhxVd8zXZFRzca5WFKh7taW9J573T8pXEWL8Wbb': '2018-01-23T13:20:00.000000+00:00', 'EjZrHfLTBR38d67HasBxpyKRBvrPBJ5RiAMubPWXLxWr': '2018-01-23T13:25:00.000000+00:00', 'koKn32jREPYR642DQsFftPoCkTf3XCPcfvc3x9RhRK7': '2018-01-23T13:30:00.000000+00:00'} timeout=10{code}
3. Wait for upgrade.

*Actual Results:*
1) Pool upgrade had failed
{code:java}
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,659 | WARNING | node_control_tool.py (172) | _restore_from_backup | Copying last_version failed due to [Errno 2] No such file or directory: '/root/.indy/sandbox/last_version'
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,659 | WARNING | node_control_tool.py (172) | _restore_from_backup | Copying next_version failed due to [Errno 2] No such file or directory: '/root/.indy/sandbox/next_version'
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,659 | WARNING | node_control_tool.py (172) | _restore_from_backup | Copying upgrade_log failed due to [Errno 2] No such file or directory: '/root/.indy/sandbox/upgrade_log'
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,660 | WARNING | node_control_tool.py (172) | _restore_from_backup | Copying last_version_file failed due to [Errno 2] No such file or directory: '/root/.indy/sandbox/last_version_file'
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,660 | DEBUG | node_control_tool.py (192) | _remove_old_backups | Removing old backups
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,660 | ERROR | node_control_tool.py (250) | _declare_upgrade_failed | Upgrade from 1.2.213 to 1.2.279 failed: [Errno 2] No such file or directory: '/var/lib/indy/backup'
Jan 23 11:30:16 californiaQALive1.qatest.evernym.com env[16689]: 2018-01-23 11:30:16,660 | ERROR | node_control_tool.py (226) | _upgrade | Trying to rollback to the previous version [Errno 2] No such file or directory: '/var/lib/indy/backup'{code}
2) Pool can't restore previous version:
{code:java}
Jan 23 11:30:23 californiaQALive1.qatest.evernym.com env[16689]: Reading package lists...
Jan 23 11:30:23 californiaQALive1.qatest.evernym.com env[16689]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-anoncreds=1.0.32 indy-plenum=1.2.173 indy-node=1.2.213
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: Reading package lists...
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: Building dependency tree...
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: Reading state information...
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: indy-anoncreds is already the newest version (1.0.32).
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: Some packages could not be installed. This may mean that you have
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: requested an impossible situation or if you are using the unstable
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: distribution that some required packages have not yet been created
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: or been moved out of Incoming.
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: The following information may help to resolve the situation:
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: The following packages have unmet dependencies:
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: indy-plenum : Depends: python3-indy-crypto (= 0.1.6) but 0.2.0 is to be installed
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: E: Unable to correct problems, you have held broken packages.
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: + ret=100
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: + '[' 100 -ne 0 ']'
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: + echo 'Failed to obtain indy-anoncreds=1.0.32 indy-plenum=1.2.173 indy-node=1.2.213'
Jan 23 11:30:24 californiaQALive1.qatest.evernym.com env[16689]: Failed to obtain indy-anoncreds=1.0.32 indy-plenum=1.2.173 indy-node=1.2.213{code}
As result, pool upgraded to 1.2.279 version, indy-node was not started, pool is not able to restore 1.2.214 version.

*Expected Results:*
Upgrade should work.","indy-node 1.2.214->1.2.279
pool: 25 AWS nodes (QA Live pool)",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1078,,,,,,,,,,,,,,,,,,,,"24/Jan/18 1:53 AM;ozheregelya;jctl_upgrade.txt;https://jira.hyperledger.org/secure/attachment/14368/jctl_upgrade.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyrcv:",,,,,,Sprint 18.02 Stability,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,anikitinDSR,ozheregelya,,,,,,,,,,,"31/Jan/18 10:54 PM;anikitinDSR;Problem reason: 
- Failed upgrade procedure

Changes: 
- added python3-indy-crypto as dependency into node_control_tool
- move remove_backups procedure from try except clause

PR:
- https://github.com/hyperledger/indy-node/pull/539


Version:
- 1.2.291-master

Risk:
- Med

Recommendations for QA
- upgrade indy-node from 1.2.213 to current master;;;","06/Feb/18 12:12 AM;ozheregelya;Environment:
QA live pool (7+18 nodes)
indy-node 1.2.214 -> 1.2.294

Steps to Reproduce:
1. Setup the pool of 7 nodes.
2. Add 18 nodes manually.
3. Schedule upgrade:
{code:java}
send POOL_UPGRADE name=upgrade12294 version=1.2.294 sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv':'2018-02-05T05:00:00.000000+00:00', '8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-02-05T05:05:00.000000+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya':'2018-02-05T05:10:00.000000+00:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA':'2018-02-05T05:15:00.000000+00:00', '4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe':'2018-02-05T05:20:00.000000+00:00', 'Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8':'2018-02-05T05:25:00.000000+00:00', 'BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW':'2018-02-05T05:30:00.000000+00:00', '4EG9n9ErTVqcZ7xSnhHH8PQVtYDfugw5SwqcezvYVrGg':'2018-02-05T05:35:00.000000+00:00', 'FR454t4km7JJvaZc6ttzFU9Lor5ib9YB3wao1kJeoEZb':'2018-02-05T05:40:00.000000+00:00', 'F9abmLwUQy5svguBPNi2GQTdKsiVGDAyBfaFpwcpLJDm':'2018-02-05T05:45:00.000000+00:00', '72vCSKVDbcbpnCXekEd9vhagse75uXnjJDjDGYDxeXPG':'2018-02-05T05:50:00.000000+00:00', '8oE3bGvZTzGxVQRHxfLZHD8Uyx2cbX6wnF1aQAiJe5sm':'2018-02-05T05:55:00.000000+00:00', 'A9hZwUqe62MNQXoswU47271CyRf7g1G5Gat3aqeQ6DeN':'2018-02-05T06:00:00.000000+00:00', '2LssMb56SBibrQYFhRnnMvZHhEiaorMuNkmW8QsZfVwA':'2018-02-05T06:05:00.000000+00:00', '8A5NzusREw844wQmttwBqhhWkTNby5o4UvYq6T1FsByb':'2018-02-05T06:10:00.000000+00:00', 'Bh492xBFGYKS7Z57EQkP5cQtJp6jDvypHoSpXHh259q5':'2018-02-05T06:15:00.000000+00:00', '2MbQjn7ij9DFKT7rt425SaeDvgzyjycv72FEiPVdacEb':'2018-02-05T06:20:00.000000+00:00', 'EwZyzG8HBvjWvxmWVgreTVWYQJScpWMVKeqUa7Rk1Pr5':'2018-02-05T06:25:00.000000+00:00', '58b3Fy45qjcBfVtEt2Zi1MgiRzX9PPmj68FwD143SuWQ':'2018-02-05T06:30:00.000000+00:00', '2FGgKVcp2heyWiGTDLVEyF6AJrfaQBvrhkUzhYTjiHA6':'2018-02-05T06:35:00.000000+00:00', '6CRQcKzeRMCstErDT2Pso4he3rWWu1m16CRyp1fjYCFx':'2018-02-05T06:40:00.000000+00:00', '53skV1LWLCbcxxdvoxY3pKDx2MAvszA27hA6cBZxLbnf':'2018-02-05T06:45:00.000000+00:00', 'CbW92yCBgTMKquvsSRzDn5aA5uHzWZfP85bcW6RUK4hk':'2018-02-05T06:50:00.000000+00:00', 'H5cW9eWhcBSEHfaAVkqP5QNa11m6kZ9zDyRXQZDBoSpq':'2018-02-05T06:55:00.000000+00:00', 'DE8JMTgA7DaieF9iGKAyy5yvsZovroHr3SMEoDnbgFcp':'2018-02-05T07:00:00.000000+00:00'} timeout=10 force=False{code}
Actual Results:
6 of 15 nodes were not upgraded.

Expected Results:
Pool should be successfully upgraded.;;;","07/Feb/18 11:12 PM;ozheregelya;In general pool upgrade works, so this ticket can be closed as invalid. 
 Regarding problem from previous comment, it can be workarounded by restarting of not upgraded nodes. It will be moved to separated ticket INDY-1150.;;;",,,,,,,,,,,,,,,,,,,,,,
Node logs are duplicated in syslog,INDY-1102,27021,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,ozheregelya,ozheregelya,24/Jan/18 1:48 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Steps to Reproduce:

1. Setup the pool with logLevel=0.
2. Run load test on this pool.

Actual Results:
On nodes with ~7Gb memory, syslog grows up to 6Gb after writing about 4000 transactions, which results  ""No space left on device"" error.

Expected Results:
All node logs should not be duplicated in syslog.

Additional Information:
As workaround, add StandardOutput=null to /etc/systemd/system/indy-node.service","indy-node 1.2.279
 pool: 25 AWS nodes (QA Live pool)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1221,,,,,,,,,,,,,,,"24/Jan/18 1:47 AM;ozheregelya;syslog;https://jira.hyperledger.org/secure/attachment/14367/syslog",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0y7:",,,,,,Sprint 18.05,,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,ozheregelya,,,,,,,,,,"08/Mar/18 12:12 AM;anikitinDSR;Problem reason: 
- dublicate logs into journalctl

Changes: 
- if config (indy_config) has ""enableStdOutLogging=False"" parameter, then logs must be not dublicated in journalctl

PR:
- https://github.com/hyperledger/indy-node/pull/595
- https://github.com/hyperledger/indy-plenum/pull/555


Version:
- indy-node: master 1.3.331
- indy-plenum: master 1.2.267

Risk:
- Low

Recommendations for QA
- Prepare node for running
- add enableStdOutLogging = False into /etc/indy/indy_config.py
- run node by ""systemctl start indy-node""
- check that there is no any log string from indy-node into journalctl;;;","13/Mar/18 1:54 AM;ozheregelya;*Environment:*
 indy-node 1.3.333
 25 nodes AWS pool, 4 nodes docker pool

*Steps to Validate:*
 1. Setup the pool with logLevel=0 and enableStdOutLogging = False in /etc/indy/indy_config.py.
 2. Check that debug logs are still written to /var/log/indy/sandbox/Node*.log.
 3. Check that journalctl and syslog don't contain indy-node debug logs.
 4. Check that start/stop of indy-node services, indy-node upgrade and indy-node exceptions are still written to journalctl.

Actual Results:
 indy-node logs are not duplicated to journalctl. 

*Reason for Reopen:*
 Need to add writing enableStdOutLogging = False to /etc/indy/indy_config.py to dockerfiles and to vagrant scripts.

*Additional Information:*
Ticket to do the same during node installation: INDY-1221;;;","13/Mar/18 9:12 PM;ashcherbakov;I suggest to do remaining work in the scope of INDY-1221 and close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,,
Unable to send transactions to Live Sovrin,INDY-1103,27052,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,mgbailey,mgbailey,25/Jan/18 6:51 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Following the upgrade to to 1.2.50, the live Sovrin network is no longer processing transactions.  In particular, we are not able to do ""send NODE"" to configure the BLS keys.

For example, shortly before Wed Jan 24 21:18, I sent the following transaction: 
{code:java}
indy@live> send NODE dest=GWgp6huggos5HrzHVDy5xeBkYHxPvrRZzjPNAyJAqpjA data={'alias':'ev1','blskey': '2zg4ufG8iyn5BQfh1YsutZqEB4QdZt8NUvef9CX7VrqCKHsfxY2muPY3LSGbBshqB6sfvXa7zXVPFUjM1ZcZmc2RFpSKsRNQC1shGA6yLhtkG5UUkNCRfXMd2UsuLu8x1pi7fHQaD
ioPQ8WxUf4YiJT8BuPx6RyfYRtyvuvJxr2UwBL'}
Sending node request for node DID GWgp6huggos5HrzHVDy5xeBkYHxPvrRZzjPNAyJAqpjA by J4N1K1SEB8uY2muwmecY5q (request id: 1516830370402348)
indy@live>
{code}
No response was received at the cli.

It appears that there are enough connected nodes for consensus:
{code:java}
mikebailey@sao-sov-p001:~$ sudo validator-info -v
[sudo] password for mikebailey:
Validator ev1 is running
Current time: Wednesday, January 24, 2018 9:15:39 PM
Validator DID: GWgp6huggos5HrzHVDy5xeBkYHxPvrRZzjPNAyJAqpjA
Verification Key: 46HnJNoJCRdFasFSBakSyqSgNGE93STk7wZiddJnsCfLgMKWjhWEJNc
Node Port: 9701/tcp on 0.0.0.0/0
Client Port: 9702/tcp on 0.0.0.0/0
Metrics:
Uptime: 8 hours, 44 minutes, 12 seconds
Total Config Transactions: 114
Total Ledger Transactions: 42
Total Pool Transactions: 15
Read Transactions/Seconds: 0.00
Write Transactions/Seconds: 0.00
Reachable Hosts: 12/14
BIGAWSUSEAST1-001
DustStorm
OASFCU
ServerVS
danube
digitalbazaar
esatus_AG
ev1
prosovitor
royal_sovrin
thoth
zaValidator
Unreachable Hosts: 2/14
icenode
iRespond
Software Versions:
indy-node: 1.2.50
sovrin: 1.1.6{code}
Looking at the logs they are rolling over very rapidly.  The ten logs retained on ev1 only go back about 3 hours.  They are attached.

Also, ev1 is the first node in the genesis file, so I would expect it to be the primary.  This is not the case, and in fact the logs show that the primary has changed several times in the last few hours.

 ",1.1.43 upgraded to 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1106,INDY-1079,INDY-1119,,,,,,,,,,,,,"25/Jan/18 11:25 PM;mgbailey;BIGAWSUSEAST1-001_ledgers.tgz;https://jira.hyperledger.org/secure/attachment/14404/BIGAWSUSEAST1-001_ledgers.tgz","27/Jan/18 1:51 AM;mgbailey;BIG_Node.tar;https://jira.hyperledger.org/secure/attachment/14420/BIG_Node.tar","26/Jan/18 1:08 AM;mgbailey;OASFCU_config_ledger;https://jira.hyperledger.org/secure/attachment/14412/OASFCU_config_ledger","25/Jan/18 11:59 PM;mgbailey;OASFCU_ledgers.tgz;https://jira.hyperledger.org/secure/attachment/14409/OASFCU_ledgers.tgz","26/Jan/18 1:57 AM;mgbailey;ServerVS_ledgers_with_config.tgz;https://jira.hyperledger.org/secure/attachment/14415/ServerVS_ledgers_with_config.tgz","26/Jan/18 2:39 AM;mgbailey;danube_ledgers_with_config.tgz;https://jira.hyperledger.org/secure/attachment/14417/danube_ledgers_with_config.tgz","25/Jan/18 11:40 PM;mgbailey;esatus_AG_ledgers.tgz;https://jira.hyperledger.org/secure/attachment/14405/esatus_AG_ledgers.tgz","26/Jan/18 1:57 AM;mgbailey;esatus_config_ledger;https://jira.hyperledger.org/secure/attachment/14416/esatus_config_ledger","26/Jan/18 1:09 AM;mgbailey;ev1_ledgers_with_config.tgz;https://jira.hyperledger.org/secure/attachment/14413/ev1_ledgers_with_config.tgz","25/Jan/18 6:51 AM;mgbailey;ev1_logs_1.tgz;https://jira.hyperledger.org/secure/attachment/14399/ev1_logs_1.tgz","25/Jan/18 6:51 AM;mgbailey;ev1_logs_2.tgz;https://jira.hyperledger.org/secure/attachment/14400/ev1_logs_2.tgz","25/Jan/18 6:51 AM;mgbailey;ev1_logs_3.tgz;https://jira.hyperledger.org/secure/attachment/14401/ev1_logs_3.tgz","25/Jan/18 6:51 AM;mgbailey;ev1_logs_4.tgz;https://jira.hyperledger.org/secure/attachment/14402/ev1_logs_4.tgz","26/Jan/18 3:15 AM;mgbailey;prosovitor_ledgers_with_config.tgz;https://jira.hyperledger.org/secure/attachment/14418/prosovitor_ledgers_with_config.tgz","26/Jan/18 1:11 AM;mgbailey;royal_sovrin_ledgers_with_config.tgz;https://jira.hyperledger.org/secure/attachment/14414/royal_sovrin_ledgers_with_config.tgz","25/Jan/18 11:40 PM;mgbailey;thoth_ledgers.tgz;https://jira.hyperledger.org/secure/attachment/14406/thoth_ledgers.tgz","26/Jan/18 7:02 AM;mgbailey;zaValidator_ledgers_with_config.tgz;https://jira.hyperledger.org/secure/attachment/14419/zaValidator_ledgers_with_config.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzytwv:",,,,,,Sprint 18.02 Stability,,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,ozheregelya,SeanBohan_Sovrin,spivachuk,,,,,,,,"25/Jan/18 7:11 AM;SeanBohan_Sovrin;[~ashcherbakov] - this one is a high priority per my last discussion with MikeB, Kelly and Nathan

 ;;;","25/Jan/18 9:56 PM;spivachuk;[~mgbailey], could you please provide config ledgers from all the nodes and also pool and domain ledgers from {{ev1}}?;;;","25/Jan/18 11:17 PM;mgbailey;[~spivachuk] I have requested ledgers from stewards.  It would probably take days to get them from all nodes.  Attached are the ledgers from ev1.[^ev1_ledgers.tgz];;;","26/Jan/18 11:16 PM;spivachuk;Thank you, [~mgbailey]. Could you please provide logs from {{BIGAWSUSEAST1-001}}?;;;","27/Jan/18 1:51 AM;mgbailey;[~spivachuk], Attached logs from BIG: [^BIG_Node.tar];;;","30/Jan/18 8:38 PM;spivachuk;*Problem reason:*
- The master replica of {{ev1}} stopped to order 3PC-batches on January, 24 at 13:21:06 when it received a PREPREPARE message with a wrong state trie root hash from the master's primary ({{BIGAWSUSEAST1-001:0}}). As we can see in the attached ledgers, the whole master protocol instance stopped to order 3PC-batches from this PREPREPARE. This PREPREPARE contained repeated and belated requests. _(We saw the behavior with sending PREPREPAREs with repeated requests earlier in INDY-959, INDY-1045 and INDY-1079 and we made a related fix in scope of INDY-959 with adding a check of the request presence in {{seqNoDB}} on processing PROPAGATE.)_ Also this PREPREPARE was the first one for the domain ledger during the pool upgrade process. The format of a nym representation in the domain state was changed in the version {{1.2.50-stable}} in comparison with the version {{1.1.43-stable}}: {{txnTime}} field was added to the set of fields being stored. This results in difference between the states trie root hashes for the same domain transaction logs in these versions. At the moment when this PREPREPARE was sent, 5 nodes (including {{ev1}}) of 14 were already upgraded to the version {{1.2.50-stable}}, 1 node was in the process of upgrading and 8 nodes (including the master's primary {{BIGAWSUSEAST1-001}}) were still on the version {{1.1.43-stable}}. Each upgraded node re-created all the states because they had been removed by the migration script {{1_2_44_to_1_2_45.py}}. So each upgraded node got the domain state in the new format. Also when applying NYM requests from the PREPREPARE, each upgraded node updated the domain state using the new format. So only 7 not upgraded non-primary nodes could agree with the state trie root hash of this PREPREPARE from the not upgraded primary and send PREPARE. But that was insufficient for the quorum of PREPAREs which was 9. Thus the master instance did not order this 3PC-batch.

*Problem state:*
- The change of the domain state format (the addition of {{txnTime}} field to the nym representation) was appeared in the version {{1.2.50-stable}}. Each upgraded node rebuilt all the states because they had been removed by the migration script {{1_2_44_to_1_2_45.py}}. Thus each upgraded node has the domain state in the new format. So this change in the domain state format will not harm upgrades from {{1.2.50-stable}} or higher version, i.e. further upgrades of the live pool.

*Recommendations for QA:*
- When testing upgrade to a new version, it makes sense to test pool upgrade under load of domain requests.;;;","30/Jan/18 8:46 PM;spivachuk;The cause of the issue observed in INDY-1079 with incorrect state trie root hash of PREPREPARE message was the same as in this ticket.;;;","01/Feb/18 12:37 AM;ashcherbakov;So, nothing to fix now, the issues as it is will not be reproduced on master.

Nevertheless it's worth creating some exploratory tasks to investigate about duplicates (they will not have any harm because of INDY-959, but it makes sense to check whether they still may occur and why).;;;","01/Feb/18 11:09 PM;ozheregelya;Ticket for QA exploration: INDY-1120.;;;",,,,,,,,,,,,,,,,
Live pool config content should be edited,INDY-1104,27055,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,dsurnin,dsurnin,25/Jan/18 4:52 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,FixMe,,,"During the live pool upgrade to v 1.2.50 we checked config of one node and there are some issues with it

Config send by [~mgbailey]


{code:java}
enableStdOutLogging=False

baseDir = '/var/lib/indy'

NODE_BASE_DATA_DIR = baseDir

LOG_DIR = '/var/log/indy'

BACKUP_DIR = '/var/lib/indy/backup'

CLI_BASE_DIR = '~/.indy-cli/'

CLI_NETWORK_DIR = '~/.indy-cli/networks'

NODE_BASE_DATA_DIR = baseDir

enableStdOutLogging=False

logLevel = 0

enableStdOutLogging=False

enableStdOutLogging=False

~~~~~~~~

About 500 lines repeating

~~~~~~~~

enableStdOutLogging=False

enableStdOutLogging=False

NETWORK_NAME = 'live'

{code}


1 - live pool has debug trace level ""logLevel = 0"". it will generate lots of logs.
2 - lots of ""enableStdOutLogging=False"" strings.

these issues are not critical, but we should be watching main config file to prevent it to become unreadable garbage",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-253,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyuin:",,,,,,,,,,,,,,,,,,,,,,,,,,Derashe,dsurnin,,,,,,,,,,,"07/Nov/18 8:21 PM;Derashe;seems to be similar to https://jira.hyperledger.org/browse/INDY-253, which did not reproduced;;;",,,,,,,,,,,,,,,,,,,,,,,,
Revisite node count (quorum) logic,INDY-1105,27124,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,dsurnin,dsurnin,29/Jan/18 10:22 PM,04/May/18 10:29 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"New node affects pool's F value right after node transaction written. As a result we cannot add several nodes during a small period of time. Also in case of stepping though the border values where F value should be changed (from 6 nodes to 7, from 9 to 10, etc.) we require higher consensus value.
It would be better to recalculate F values only after new node finished catchup and started to participate.
There are at least two options
1 - new NODE transaction, something like NODE_PARTICIPATING
2 - recheck node status",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwyfz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,SeanBohan_Sovrin,,,,,,,,,,"30/Jan/18 6:47 AM;SeanBohan_Sovrin;Like idea of working on synch status before it participate in consensus. 

[~ashcherbakov] - how is this different than an observer node state? There should be a new state in the pool state that is a staging area for new nodes. Pls advise.;;;","30/Jan/18 5:14 PM;ashcherbakov;it's not about Observers, this is a general issue that nodes re-calculate n and f once a new node is connected. Bute the newly connected node may not yet finished catching up, so it is not yet a full member of the pool. So, we need to re-calculate n and f for new nodes only after they finish initial catch-up.;;;",,,,,,,,,,,,,,,,,,,,,,,
Live network node does not sync following BLS upgrade,INDY-1106,27223,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,mgbailey,mgbailey,mgbailey,30/Jan/18 7:04 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"The steward made a mistake and did ""init_bls_keys"" as root instead of as the indy user. This resulted in a misconfiguration, with no bls data in the /var/lib/indy/live/keys/<ALIAS>/bls_keys folder. He then put the bls key data for that node into the ledger.

His pool ledger is now out of sync, lacking the last 2 transactions, including his own. I later got online with him to debug this. I discovered the error, and had him re-run the init_bls_keys as the indy user. The correct values are now in the /var/lib/indy/live/keys/<ALIAS>/bls_keys folder, matching what is on the ledger. However, his node will still not sync. I do not see responses to his pings in his logs, although in the ev1 logs it looks like our node is responding.

Logs are attached.",1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1103,,,,,,,,,,,,,,,,,,,,"30/Jan/18 6:58 AM;mgbailey;DustStorm.log;https://jira.hyperledger.org/secure/attachment/14502/DustStorm.log","30/Jan/18 7:03 AM;mgbailey;ev1.log.tgz;https://jira.hyperledger.org/secure/attachment/14501/ev1.log.tgz","30/Jan/18 7:42 AM;mgbailey;ev1_ledger.txt;https://jira.hyperledger.org/secure/attachment/14503/ev1_ledger.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0wn:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,mgbailey,,,,,,,,,,"30/Jan/18 7:42 AM;mgbailey;Also attaching the pool ledger from ev1.  DustStorm does not have transactions 26 & 27.[^ev1_ledger.txt];;;","01/Feb/18 8:32 PM;dsurnin;It looks like DustStorm node has some network issues - it does not receive any messages from other nodes neither ping/pong nor node-to-node messages.
as a result it didn't received node txn that changes its bls key

according to ev1 there are lots of ping and INSTANCE_CHANGE requests from DustStorm node
it looks like some firewall issue - all incoming are blocked;;;","02/Feb/18 7:30 AM;mgbailey;[~dsurnin], the steward is checking to see if there have been firewall changes in his datacenter.;;;","27/Mar/18 10:48 PM;ashcherbakov;We can reopen any that reappear, but all is good on the live pool now.;;;",,,,,,,,,,,,,,,,,,,,,
"Ensure that new load script, based on sdk, sends requests and wait for a responce in background",INDY-1107,27236,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,dsurnin,dsurnin,30/Jan/18 10:08 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/18 5:46 PM;VladimirWork;Perf_Add_nyms.py;https://jira.hyperledger.org/secure/attachment/14724/Perf_Add_nyms.py","06/Mar/18 5:46 PM;VladimirWork;Perf_get_nyms.py;https://jira.hyperledger.org/secure/attachment/14725/Perf_get_nyms.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzz0xz:",,,,,,Sprint 18.05,18.06,,,,,,,3.0,,,,,,,,,,,,dsurnin,VladimirWork,,,,,,,,,,,"06/Mar/18 5:46 PM;VladimirWork;Version with synchronous NYM adding/getting and asynchronous responce waiting. [^Perf_Add_nyms.py]  [^Perf_get_nyms.py] ;;;","06/Mar/18 11:36 PM;VladimirWork;There is a new version of load scripts fixed by Logigear team. Need to review this scripts and make fixes in them if needed.;;;","15/Mar/18 7:52 PM;VladimirWork;New scripts prepare adding/getting requests in files using request_builder.py and then run synchronous sign_and_submit_several_reqs_from_files/submit_several_reqs_from_files methods using request_sender.py so we don't wait for new DIDs to send txns like in old scripts.;;;",,,,,,,,,,,,,,,,,,,,,,
[REFACTOR] Refactoring of Request-Reply json structure,INDY-1108,27238,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,ashcherbakov,ashcherbakov,ashcherbakov,30/Jan/18 11:49 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,We need to perform re-factoring as described in INDY-886.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-784,,,,,,,,,"1|hzyavj:",,,,,,,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"02/Feb/18 9:55 PM;ashcherbakov;Replaced by a number of subtasks.;;;",,,,,,,,,,,,,,,,,,,,,,,,
NETWORK_NAME isn't set while deploy Vagrant cluster,INDY-1109,27243,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,zhigunenko.dsr,zhigunenko.dsr,31/Jan/18 12:59 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Steps to Reproduce:*
 1. start _vagrant up_
 2. start _indy_ in cli01 node

*Actual Results:*
 vagrant@cli01:~$ indy
{code:java}
Traceback (most recent call last):
 File ""/usr/local/bin/indy"", line 30, in <module>
 config = getConfig()
 File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_util.py"", line 26, in getConfig
 CONFIG = _getConfig(PlenumConfig, general_config_dir, user_config_dir)
 File ""/usr/local/lib/python3.5/dist-packages/indy_common/config_util.py"", line 19, in _getConfig
 user_config_dir=user_config_dir)
 File ""/usr/local/lib/python3.5/dist-packages/plenum/common/config_util.py"", line 52, in extend_with_default_external_config
 os.path.join(extendee.GENERAL_CONFIG_DIR, extendee.GENERAL_CONFIG_FILE)))
Exception: NETWORK_NAME must be set in /etc/indy/indy_config.py
{code}
Similar troubles were found in _vagrant up_ logs

indy_config.py
{code:java}
#Current network
 NETWORK_NAME = None
 #Directory to store ledger.
 LEDGER_DIR = '/var/lib/indy'

#Directory to store logs.
 LOG_DIR = '/var/log/indy'
{code}
*Expected Results:*
 Old CLI is running

*Additional information:*

Simple insertion from dockerfile 
{code:java}
awk '\{if (index($1, ""NETWORK_NAME"") != 0) \{print(""NETWORK_NAME = \""sandbox\"""")} else print($0)}' /etc/indy/indy_config.py> /tmp/indy_config.py
 mv /tmp/indy_config.py /etc/indy/indy_config.py
{code}
into validator.sh did not help.

Attached *patchfile* resolved the problem",indy-node 1.2.284-master(git),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/18 5:14 PM;zhigunenko.dsr;patchfile;https://jira.hyperledger.org/secure/attachment/14532/patchfile","31/Jan/18 12:44 AM;zhigunenko.dsr;vagrant_up_log.txt;https://jira.hyperledger.org/secure/attachment/14519/vagrant_up_log.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzysxz:",,,,,,Sprint 18.04,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,"05/Feb/18 6:21 PM;zhigunenko.dsr;*Environment:*
 * indy-node 1.2.284-master(git)

*Actual Results:*
vagrant@cli01:~$ indy - successful start
vagrant@validator01:~$ indy - successful start;;;",,,,,,,,,,,,,,,,,,,,,,,,
Can't set up demo or other things,INDY-1110,27247,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,RichLevinson,RichLevinson,RichLevinson,31/Jan/18 1:46 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,libsovrin,,,0,,,,"I have been trying to set up a demo for about a week now using the indy getting started and setup dev pages, and have had no luck.

The demo says pip install indy-client

Can't do that.

Tried the whole dev setup thing and got lots of errors.

Running on redhat Linux

Looking to set up demo to get company interested in this product.

Would appreciate some help

Thanks,

Rich Levinson

Oracle

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyv53:",,,,,,,,,,,,,,,,,,,,,,,,,,benjsmi,RichLevinson,,,,,,,,,,,"06/Feb/18 10:04 AM;RichLevinson;I have resolved the problem by giving up on Linux, and instead doing the whole thing: VirtualBox, Vagrant, Git, etc. on Windows 10 and got thru it in the better part of a day.

Using the link to the ""Setting Up a Test Indy Network in VMs""

  [https://github.com/evernym/sovrin-environments/blob/stable/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md]

and doing everything on W10 enabled me to get thru the whole demo.

The demo was worth the effort.

 ;;;","06/Feb/18 10:11 AM;RichLevinson;I'd like to close this bug and mark it as resolved, but don't see any obvious way to do it.;;;","30/Oct/18 12:38 AM;benjsmi;To [~RichLevinson]'s point: is there a quick-start or ""production"" or ""client"" build anywhere?  It seems to me like it's possible to build the SDK and/or the agent and/or the CLI without installing Rust, Rubygems, etc, but I don't see any Dockerfiles or similar that would allow a user to just install Indy without a boatload of dependencies that you have to install through non-apt means.;;;",,,,,,,,,,,,,,,,,,,,,,
Need to prevent an Identity Owner from creating a schema or claimDef,INDY-1111,27252,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,krw910,krw910,31/Jan/18 6:36 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"For now we need to remove the ability for an identity owner to create a new schema or a new claimDef.

We may later add this functionality back in.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/18 7:39 PM;VladimirWork;claim-def.PNG;https://jira.hyperledger.org/secure/attachment/14771/claim-def.PNG","14/Mar/18 7:39 PM;VladimirWork;schema.PNG;https://jira.hyperledger.org/secure/attachment/14770/schema.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz0yv:",,,,,,Sprint 18.05,,,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,Toktar,VladimirWork,,,,,,,,,"28/Feb/18 10:00 PM;ashcherbakov;[~krw910] [~SeanBohan_Sovrin]
 Who should be able to create Schemas and Claim Defs then?

Trust Anchors? Stewards? Trustees?;;;","01/Mar/18 7:34 PM;ashcherbakov;[https://docs.google.com/spreadsheets/d/1TWXF7NtBjSOaUIBeIH77SyZnawfo91cJ_ns4TR-wsq4/edit#gid=0]
According to this spreadsheet, SCHEMA and CLAIM_DEF can be created by TRUSTEE, STEWARD and TRUST_ANCHOR. 
So, let's follow this assumption.;;;","13/Mar/18 9:22 PM;Toktar;Problem reason:
 -  Have not validation of client permission for create schema and claim_def

Changes:
 -  Added validation of client permission for create schema and create/update claim_def

PR:
 - [https://github.com/hyperledger/indy-node/pull/598]

Version:
 - indy-node 1.3.335-master

Risk factors:
 - Nothing is expected

Risk:
 - Low

Covered with tests:
 - [test_auth_claim_def.py|https://github.com/hyperledger/indy-node/pull/598/files#diff-f228add6528963939ae2b26efc75c977]
 - [test_auth_schema.py|https://github.com/hyperledger/indy-node/pull/598/files#diff-2b93f5c29dddfa84dd0ac0e0bff25481]
 - [test_schema.py|https://github.com/hyperledger/indy-node/pull/598/files#diff-5360cb7144c3efe27f10566ecf3e1802]
 ** test_update_claim_def_for_same_schema_and_signature_type
 - [test_claim_def.py|https://github.com/hyperledger/indy-node/pull/598/files#diff-a2e74cead670f8cbb27857bbcb9a6d12]  -
 ** test_can_not_submit_claim_def_by_identity_owner
 ** test_submit_claim_def_same_schema_and_signature_type;;;","14/Mar/18 7:39 PM;VladimirWork;Build Info:
indy-node 1.3.336

Steps to Validate:
1. Try to add schemas/claim-defs by Identity Owner/TGB.
2. Try to add/update schemas/claim-defs by Trust Anchor/Steward/Trustee.

Actual Results:
Trust Anchor/Steward/Trustee roles can add schemas/claim-defs only.  !schema.PNG|thumbnail!  !claim-def.PNG|thumbnail! 

Additional Info:
None/TGB roles cannot add schemas/claim-defs via old CLI with inconsistent errors but it's not an issue since old CLI will be obsolete soon.;;;",,,,,,,,,,,,,,,,,,,,,
The same primary for instances 0 and 1,INDY-1112,27262,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,31/Jan/18 11:05 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Overview:
There is an issue with the same primary elected for both instances due to nodes demoting/promoting/switching off/switching on.

Build Info:
indy-node 1.2.282

Steps to Reporduce:
It's unclear, see attached logs from all nodes for more info.

Actual Results:
There is the same primary for instances 0 and 1 at nodes *that was never demoted during test run*:

{noformat}
2018-01-24 15:34:59,386 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node1 changed to view 1, will start catchup now
2018-01-24 15:34:59,459 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node1:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]
2018-01-24 15:34:59,460 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node1:1 declares view change 1 as completed for instance 1, new primary is Node4:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 14:33:43,092 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node2 changed to view 1, will start catchup now
2018-01-24 14:33:43,165 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node2:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')]
2018-01-24 15:34:59,340 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node2:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 14:33:43,085 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node3 changed to view 1, will start catchup now
2018-01-24 14:33:43,174 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node3:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')]
2018-01-24 14:33:45,094 | INFO     | view_changer.py      (380) | sendInstanceChange | VIEW CHANGE: Node3 sending an instance change with view_no 2 since Primary of master protocol instance disconnected
2018-01-24 15:34:59,338 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node3:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 14:33:43,088 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node4 changed to view 1, will start catchup now
2018-01-24 14:33:43,177 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node4:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 7, 'D2qVBkrRoKU78vLQdsEnrydHxuwCS15s8jyzPTqFodB2'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 1, '1fxbz7Tb68KqRhJyE87yaymZ3kQWFMwX6AvVzYa2Qew')]
2018-01-24 15:34:59,313 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node4:1 declares view change 1 as completed for instance 1, new primary is Node3:1, ledger info is [(0, 8, 'CazoXBw6BYkxkXM6FXfQZnCwwwqNbAYK6NZyTsr8R2CC'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]

2018-01-24 15:35:54,923 | INFO     | node.py              (485) | on_view_change_start | VIEW CHANGE: Node5 changed to view 1, will start catchup now
2018-01-24 15:35:55,048 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node5:0 declares view change 1 as completed for instance 0, new primary is Node3:0, ledger info is [(0, 9, '8MMN1HuP2XrkiCubNXk9Cqw685UUCJL93SGFksaq1gEd'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]
2018-01-24 15:35:55,049 | DISPLAY  | node.py              (2314) | select_primaries | VIEW CHANGE: Node5:1 declares view change 1 as completed for instance 1, new primary is Node4:1, ledger info is [(0, 9, '8MMN1HuP2XrkiCubNXk9Cqw685UUCJL93SGFksaq1gEd'), (1, 13, 'DXNrGGWmZWwd72zmwMFXutRu7qMs92FWfztmeTRiS8iJ'), (2, 2, 'CV9Dq7nStEqe5hJP6kbvT2UqHxKoyVdQ6dYg6ReGXP4Y')]
{noformat}

Expected Results:
Primaries for different instances should be different.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1092,,,,,INDY-1179,,,,,,,,,,,,,,,"31/Jan/18 11:04 PM;VladimirWork;AWS_the_same_primary_for_both_instances_on_not_demoted_nodes.7z;https://jira.hyperledger.org/secure/attachment/14522/AWS_the_same_primary_for_both_instances_on_not_demoted_nodes.7z","08/Feb/18 12:19 AM;zhigunenko.dsr;Logs.7z;https://jira.hyperledger.org/secure/attachment/14553/Logs.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0zb:",,,,,,Sprint 18.04,Sprint 18.05,,,,,,,,,,,,,,,,,,,sergey-shilov,VladimirWork,zhigunenko.dsr,,,,,,,,,,"08/Feb/18 12:19 AM;zhigunenko.dsr;Case 2
Promotion of node require one more instance of primary
*Steps to reproduce:*
1. Create pool with 7 nodes
>> _Primary: node2 for instance 0,  node3 for instance1, node4 for instance2_
2. Demote primary node2
>> _Primary: node5 for instance 0,  node6 for instance1 (view9)_
3. Promote node2
_Primary: {color:red}node5 for instance 0{color},  node6 for instance1, {color:red}node5 for instance2{color} (view9)_;;;","16/Feb/18 1:28 AM;sergey-shilov;There is much more simple case to get the same primary for master and replica instances: just subsequent demote and promote initial primary. This is caused by current math that does not consider current primaries and relies on viewNo and instanceId only despite N (module) may be changed.;;;","28/Feb/18 6:45 PM;sergey-shilov;*Problem state / reason:*

The formula that is used for selection of primary node for master replica can not be applied for selection of primary nodes for backup replicas since N is variable.

*Changes:*

Now primaries for backup instances are chosen in a round-robin manner always starting from primary. If the next node is a primary for some instance then this node is skipped. So the first non-primary node is chosen as primary for current instance. Such approach allows to avoid election of instances of the same node as a primaries for different instances.
 The election procedure of the primary for master instance is not changed.
Also working with node registry was changed. Now a newly added node does not add itself to owned node registry during initialisation. It waits for pool ledger catch-up instead. So now node registry is consistent with pool txns.


*Committed into:*

    https://github.com/hyperledger/indy-plenum/pull/539
    https://github.com/hyperledger/indy-node/pull/585
    indy-node 1.2.320-master

*Risk factors:*

    Nodes demotion and promotion, growing number of instances.

*Risk:*

    Medium

*Recommendations for QA:*

 - Check demotion and promotion of primary of master replica when number of instances is changed.
 - Check demotion and promotion of random nodes.;;;","01/Mar/18 7:33 PM;VladimirWork;Build Info:
indy-node 1.3.322

Steps To Validate:
1. Force multiple view changes by primary demotion.
2. Force multiple view changes by primary restart.
3. Demote and promote nodes (primary and not) arbitrarily.
4. Check previous steps with several instances (pool of 7+ nodes) and big viewNo (20+).

Actual Results:
New primary for master instance (`instId`==0) is always selected according to *`(viewNo+instId)%NumberOfNodes`* formula. All backup primaries are selected as next nodes *that are not primaries for another instance already*.;;;",,,,,,,,,,,,,,,,,,,,,
It's unable to reselect primary if view change timeout expired more than 2 times,INDY-1113,27264,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,31/Jan/18 11:22 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Build Info:
indy-node 1.2.282

Steps to Reproduce:
1. Emulate slow view change processing (slow network or sleep() in ~/plenum/server/view_change/view_changer.py).
2. Force multiple view changes by primary restarting until there are 2 view change timeouts expire.

Actual Results:
When first view change (forced by primary restarting) falls into timeout error node starts another view change and if this view change also falls into timeout error it looks like there are no any additional attempts to select another primary (you can even shut down current primary after this and other nodes will count it as primary as well).

Expected Results:
Pool should retry the primary selection (more attempts or endless loop).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1092,,,,,,,,,,,,,,,,,,,,"31/Jan/18 11:20 PM;VladimirWork;issue_with_view_changer_timeout.tar.gz;https://jira.hyperledger.org/secure/attachment/14523/issue_with_view_changer_timeout.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0zj:",,,,,,Sprint 18.04,Sprint 18.05,,,,,,,,,,,,,,,,,,,spivachuk,VladimirWork,,,,,,,,,,,"05/Mar/18 11:26 PM;spivachuk;*Problem state:*

The behavior was correct. After setting delay on VIEW_CHANGE_DONE messages processing to 60 seconds at 11:12 {{Node1}} failed to complete any view change. It started view change to {{view_no=5}} after reception the propagate primary quorum number of VIEW_CHANGE_DONE with {{view_no=5}} from other nodes. It failed to complete view change to {{view_no=5}} in time, so it sent INSTANCE_CHANGE with {{view_no=6}}. Then {{Node1}} gathered quorum number of INSTANCE_CHANGEs with {{view_no=6}} and started view change to {{view_no=6}}. It failed to complete view change to {{view_no=6}} in time too, so it sent INSTANCE_CHANGE with {{view_no=7}}. However, {{Node1}} did not gather the quorum number of INSTANCE_CHANGE with {{view_no=7}}. So it did not even start view change to {{view_no=7}}. Correspondingly, no timer for view change completion was started.

After integration tests related to this case had been written, faced ​intermittent failures of the new test verifying that a disconnected node with a lagged view pulls up its view on re-connection ({{test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection}}). Found that these intermittent failures were caused by incorrectness of the procedures of node disconnection and reconnection that are used in tests. After these procedures had been corrected, the new test specified above became stably passing but one of the existing tests ({{test_6_nodes_pool_cannot_reach_quorum_with_2_disconnected}}) became stably hanging up. During investigation of the last test hang-ups, faced an error with a not retrieved exception of a future related to Indy SDK. However, the fix of this error did not resolve the issue with the test hang-ups. Made a workaround in this test to avoid hang-ups.

*Changes:*
- Added a test verifying that a view change is restarted each time when the previous one is timed out.
- Added a test verifying that a disconnected node with a lagged view accepts the current view from the other nodes on re-connection.
- Corrected the procedures of node disconnection and reconnection in tests.
- Removed a workaround patch which had become unnecessary from {{start_stopped_node}} function in tests.
- Corrected existing tests according to the fixes specified above.
- Fixed the cause of a potential error with a not retrieved exception of a future in {{sdk_get_replies}} function.
- Corrected test helpers for Indy SDK.
- Added a workaround to avoid hang-ups of {{test_6_nodes_pool_cannot_reach_quorum_with_2_disconnected}}.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/551
- https://github.com/hyperledger/indy-node/pull/591

*Version:*
- indy-node 1.3.325-master
- indy-plenum 1.2.260-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_multiple_view_change_retries_by_timeouts}}
- {{test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection}};;;","06/Mar/18 6:58 PM;VladimirWork;Integration tests' changes are added. The pool behavior was correct in this case.;;;",,,,,,,,,,,,,,,,,,,,,,,
Incorrect message for view change cause if primary behaves maliciously,INDY-1114,27265,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,VladimirWork,VladimirWork,31/Jan/18 11:32 PM,27/Mar/18 7:29 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Build Info:
indy-node 1.2.282

Steps to Reproduce:
1. Send malformed requests from primary node (by adding malformed data in create3PCBatch function of ~/plenum/server/replica.py for example).
2. Send some NYMs to force view change.
2. Check logs after view change.

Actual Results:
There is ""since Primary of master protocol instance degraded the performance"" message.

Expected Results:
There shuold be message about bad primary requests/mailicious primary or something like this.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1092,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwyfj:",,,,,,,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
How does pool deal with random delays in node-to-node messages,INDY-1115,27267,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,zhigunenko.dsr,VladimirWork,VladimirWork,31/Jan/18 11:40 PM,08/Oct/19 9:19 PM,28/Oct/23 2:47 AM,08/Oct/19 9:19 PM,,,,,,0,explore,,,We need to explore delayed node-to-node messages handling because it can help to find more bugs (like it was found with view change delayed messages).,,,,,,,,,,INDY-1280,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwy8n:",,,,,,18.07 Stability & Monitoring,,,,,,,,3.0,,,,,,,,,,,,esplinr,VladimirWork,zhigunenko.dsr,,,,,,,,,,"05/Apr/18 11:35 PM;zhigunenko.dsr;*Possible ways to reproduce:*
 [tc|http://man7.org/linux/man-pages/man8/tc.8.html] - for usage on certain node
 [Pumba|https://github.com/alexei-led/pumba] - for usage on whole docker pool
 [Comcast|https://github.com/tylertreat/Comcast]

*Environment*
Case1-3: 15-node pool in Docker, direct sleep() code injection, old load script (not Logigear), 10threads*1000txns
Case4-9: 15-node pool in Docker, direct sleep() code injection, logigear load script, 10threads

*Results*
Case1: delay = 0.5s
186 txns were written during 54min

Case2: delay = 0.4s
load script was stopped with 307 error, only 2 txns were written

Case3: delay = 0.1s
590 txns were written during 41min

Case4: no delays
average = 5,3210sec

Case 5: delay = 0.05s
average = 22.6sec

Case 6: delay = 0.1s
average = 36.37sec

Case 7: delay = 0.2s
average = 67.37sec

Case 8: delay = 0.3s
average = 105.06sec

Case 9: delay = 0.4s
failed with timeout;;;","23/Apr/18 10:39 PM;zhigunenko.dsr;Further research requires usage of tool from INDY-1280 on AWS pool;;;","08/Oct/19 9:19 PM;esplinr;Vladimir has tested with random delays, and the ledger is performing well.;;;",,,,,,,,,,,,,,,,,,,,,,
Live pool lost consensus after BLS keys setup,INDY-1116,27270,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,mgbailey,ashcherbakov,ashcherbakov,01/Feb/18 1:47 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"We achieved 10 of 14 nodes active, with BLS keys configured on the live pool. However, when one of the remaining stewards attempted to add his BLS key to the ledger last night, the transaction would not post.  When [~mgbailey] attempted to post a NYM or an update to his NODE, these would not post either.

I don't know if this is the same problem as INDY-1079, but it may be worth checking that first.

Since the live pool is once again down, this needs highest priority. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Feb/18 1:47 AM;ashcherbakov;ev1_20180131.tgz;https://jira.hyperledger.org/secure/attachment/14524/ev1_20180131.tgz","02/Feb/18 10:50 PM;mgbailey;ev1_20180202.tgz;https://jira.hyperledger.org/secure/attachment/14536/ev1_20180202.tgz","02/Feb/18 1:53 AM;mgbailey;ev1_logs_all.tgz;https://jira.hyperledger.org/secure/attachment/14528/ev1_logs_all.tgz","02/Feb/18 9:19 PM;mgbailey;zaValidator_logs.tgz;https://jira.hyperledger.org/secure/attachment/14534/zaValidator_logs.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0wv:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,mgbailey,,,,,,,,,,"02/Feb/18 6:53 PM;anikitinDSR;The fateful events that led to the broken pool was:
 # At 2018-01-26 15:19:51 node ev1 was started.
 # After timeout (1 minute) it sent INSTANCE_CHANGE from 0 to 1 viewNo message with reason ""Primary of master protocol instance disconnected"". It's ""viewNo 0"" logic.
 # Node was connected with 13 other nodes, than ""number of nodes"" was 14, ""f""  4. Therefore, quorum for instance change was 14 - 4 = 10.
 # ""ev1"" received 8 instance change messages from other nodes and ""ev1"" as self. Now, we have 9 instance change messages.
 # ""ev1"" received quorum of view change done messages and approved viewNo=0. When new viewNo was applied, queue with instance change message wasn't cleaned. Therefore, viewNo=0 and 9 instance_change messages for the viewNo=1 in queue.
 # At 2018-01-26 19:17:18 node ev1 selected ev1:0 as primary for the instance 0.
 # Than pool worked fine, transaction was ordered.
 # At 2018-01-31 07:17:07 was received 10th instance change from zaValidator and initiated view change from 0 to 1. primaryName set to None.
 # Therefore was not received view change done messages for view 1, ev1 sent instance change message for change view from 1 to 2. This message was not replied.
 # Now, ev1 has viewNo = 1 and not participating mode and other node use viewNo=0 and expect that primary node is ev1.
 # Client request, which was sent to ev1 was propagated but PREPREPARE wasn't sent because ev1 not choose primary for it current viewNo = 1.

What we can do for restore consensus:
We propose fast restart for ev1 node (like ""systemctl restart indy-node""). When ev1 would restarted, he will ask for CURRENT_STATE from other nodes and when received quorum of VIEW_CHANGE_DONE message set viewNo as 0 and choose ev1 as primary (reset viewNo state);;;","02/Feb/18 9:39 PM;mgbailey;[~anikitinDSR] After restarting indy-node on ev1, zaValidator is now the primary.  Since this node's status is that it has BLS keys configured locally, but it does not have its BLS keys on the ledger, this is probably a problem.
{code:java}
indy@sao-sov-p001:/var/log/indy/live$ grep 'selected primary' *
ev1.log:2018-02-02 12:30:23,729 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:0 selected primary zaValidator:0 for instance 0 (view 1)
ev1.log:2018-02-02 12:30:23,753 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:1 selected primary danube:1 for instance 1 (view 1)
ev1.log:2018-02-02 12:30:23,763 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:2 selected primary royal_sovrin:2 for instance 2 (view 1)
ev1.log:2018-02-02 12:30:23,776 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:3 selected primary digitalbazaar:3 for instance 3 (view 1)
ev1.log:2018-02-02 12:30:23,788 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: ev1:4 selected primary OASFCU:4 for instance 4 (view 1)
{code};;;","02/Feb/18 9:43 PM;ashcherbakov;[~mgbailey]
This is not a problem until a `NYM` txn is sent. So, the pool should be able to process txns from the Pool ledger (NODE txns), that is continue initializing BLS. But can not process any NYM txns, and may get broken once NYM txn is sent.;;;","02/Feb/18 10:51 PM;mgbailey;After ev1 restarted, the zaValidator BLS transactions that were previously sent appeared on the ledger.  However, we then tried sending a NYM transaction to the ledger since we think that there are now 11 properly configured validators.  The send NYM failed. Logs of ev1 are attached. ;;;","03/Feb/18 1:31 AM;mgbailey;I also attempted to re-send the ev1 NODE transaction, and that was also not posted.

 ;;;","05/Feb/18 4:57 PM;anikitinDSR;Current situation on live pool.
It's look like problem, described in INDY-1081. Node ev1 got PREPREPARE with ppseqno 4 for view 1 (1,4) from zaValidator and then requested missing PREPREPARE and PREPARE for previous ppseqno 2, 3 and for 4. All requests from (1, 1) was ordered, but when we get PREPREPARE from zaValidator again, this node was suspected.
This issue will be fixed in INDY-1081. For now, restart zaValidator and change view number can help with the live pool.;;;","27/Mar/18 10:48 PM;ashcherbakov;We can reopen any that reappear, but all is good on the live pool now.;;;",,,,,,,,,,,,,,,,,,
Sometimes read_ledger script shows wrong data,INDY-1117,27283,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,anikitinDSR,ozheregelya,ozheregelya,01/Feb/18 4:57 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Sometimes read_ledger script shows wrong data. Possible way how to reproduce this behavior:
1. Connect to node with large ledger by ssh.
2. Run read_ledger script.
3. Ssh session is terminated by timeout while read_ledger script was hanging because of large ledger.

Actual Results:
{code:java}
ubuntu@californiaQALive1:~$ sudo read_ledger --type domain --count
4760
ubuntu@californiaQALive1:~$ sudo validator-info 
Validator Node1 is running
Validator DID: Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv
Verification Key: 33nHHYKnqmtGAVfZZGoP8hpeExeH45Fo8cKmd5mcnKYk7XgWNBxkkKJ
Node Port: 9701
Client Port: 9702
Metrics:
 Uptime: 5 days, 0 hours, 21 minutes, 24 seconds
 Total Ledger Transactions: 350356
 Total Pool Transactions: 25
 Read Transactions/Seconds: 0.00
 Write Transactions/Seconds: 0.68
Reachable Hosts: 25/25
Unreachable Hosts: 0/25{code}
read_ledger and validator-info show different data. Old read_ledger data was not removed.
{code:java}
ubuntu@californiaQALive1:~$ sudo find / -type d -name ""*-read-copy""
/var/lib/indy/sandbox/data/Node1-read-copy{code}
Expected Results:
read_ledger should show correct data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzwyl3:",,,,,,,,,,,,,,,,,,,,,,,,,,anikitinDSR,ozheregelya,SeanBohan_Sovrin,,,,,,,,,,"01/Feb/18 6:51 AM;SeanBohan_Sovrin;""only works well with small ledgers"";;;","01/Feb/18 8:00 AM;ozheregelya;[~SeanBohan_Sovrin], I'm not sure about it, because I get this problem first time with less than 5000 transactions in ledger.;;;","12/Oct/18 3:56 PM;anikitinDSR;As of now, all of our storages are Rocksdb by default and read_ledger does not making copy of requested ledger but only open storage in reqd_only mode. Therefore, i think this issue is not actual for now.;;;",,,,,,,,,,,,,,,,,,,,,,
exploratory tasks...,INDY-1118,27293,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,SeanBohan_Sovrin,SeanBohan_Sovrin,01/Feb/18 6:53 AM,09/Oct/19 6:17 PM,28/Oct/23 2:47 AM,09/Oct/19 6:16 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-2,,ExploratoryTasks,Done,,,,,,,"1|hzxx1j:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamic validation may behave differently depending on unrelated aspects,INDY-1119,27297,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,anikitinDSR,spivachuk,spivachuk,01/Feb/18 7:37 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"It has been found that the dynamic validation may behave differently depending on unrelated aspects such as whether the request is duplicate of another one previously committed into the ledger or not.

Below there is a fragment of {{ev1.log.2018-01-24}} from INDY-1103:
{quote}
2018-01-24 13:21:06,174 | DEBUG | replica.py ( 882) | processPrePrepare | ev1:0 received PRE-PREPARE(6, 2) from BIGAWSUSEAST1-001:0

2018-01-24 13:21:06,174 | DEBUG | replica.py (1147) | _apply_pre_prepare | ev1:0 state root before processing PREPREPARE\{'ppSeqNo': 2, 'discarded': 5, 'stateRootHash': 'GGtQgZh5r9oydsoavWnvMjL8YNo7Z32Q2xGsWpbvbJC', 'ppTime': 1516800066, 'instId': 0, 'ledgerId': 1, 'txnRootHash': '9RierSCibg6PkQNgHcTiJm9V1xGGaVvur2hAg3utLnXr', 'reqIdr': [('J4N1K1SEB8uY2muwmecY5q', 1515790849028036), ('J4N1K1SEB8uY2muwmecY5q', 1515166029574854), ('J4N1K1SEB8uY2muwmecY5q', 1515715477356351), ('J4N1K1SEB8uY2muwmecY5q', 1515167900196091), ('J4N1K1SEB8uY2muwmecY5q', 1515169081791625), ('J4N1K1SEB8uY2muwmecY5q', 1515169124908830)], 'digest': '4657fe4649366a3e6d9dc0a7773e20e375b2334936a16772fb5875e32f2cded6', 'viewNo': 6} is b'9gQ\xe9\xf5\t\xc5\x17\x0e\xfb\xf6\x93*\xd4$W\x11pp\xeaK\x98\x89\xc7\xd7\n\x17\xc8\xf3\xdc\xcc\xa8', D896UvYCu32N19Xj2ZPVNCQsQgDm4duHDhoQQQM99ozL

2018-01-24 13:21:06,181 | WARNING | replica.py ( 682) | processReqDuringBatch | ev1:0 encountered exception UnauthorizedClientRequest('STEWARD cannot update verkey',) while processing Request: \{'identifier': 'J4N1K1SEB8uY2muwmecY5q', 'reqId': 1515169124908830, 'operation': \{'dest': '7SeddhvXz8NXMuQ3Y2Wda3', 'type': '1', 'verkey': '~PMiFnDt9fmNnSYzeEBY7ZQ'}, 'signature': '5XZeadMpaYjXuaiMJnoH2et6LTp8m37x7s9tTBmRdcaysc27jCLS9AUUUgsbjkTP4jdxXMp7p3aAJxZ6LLm5PJKf'}, will reject
{quote}

Here we see that the dynamic validation behaved differently for requests-duplicates and regular requests. The requests {{('J4N1K1SEB8uY2muwmecY5q', 1515166029574854)}}, {{('J4N1K1SEB8uY2muwmecY5q', 1515167900196091)}}, {{('J4N1K1SEB8uY2muwmecY5q', 1515169081791625)}}, {{('J4N1K1SEB8uY2muwmecY5q', 1515169124908830)}} were NYM requests for the same existing nym. All these requests have the same values of the domain fields and the same sender. The values of the domain fields of these requests were the same as the values of these fields of this nym in the domain state at that moment. The difference is that the requests {{('J4N1K1SEB8uY2muwmecY5q', 1515166029574854)}}, {{('J4N1K1SEB8uY2muwmecY5q', 1515167900196091)}}, {{('J4N1K1SEB8uY2muwmecY5q', 1515169081791625)}} are duplicates of the requests which had already been committed into the ledger (see the domain ledger of the node {{ev1}} in the attachments to INDY-1103) while the request {{('J4N1K1SEB8uY2muwmecY5q', 1515169124908830)}} had not been committed into the ledger yet. We can see that {{Replica.processReqDuringBatch}} called from {{Replica._apply_pre_prepare}} multiple times (one time for each request from the PREPREPARE message) invalidated only the request {{('J4N1K1SEB8uY2muwmecY5q', 1515169124908830)}} but treated all the other requests in the PREPREPARE message as valid. (There are no warnings from {{Replica.processReqDuringBatch }} called from this very invocation of {{Replica._apply_pre_prepare}} below this fragment.) Also we can see that the primary which sent this PREPREPARE message also invalidated only the request {{('J4N1K1SEB8uY2muwmecY5q', 1515169124908830)}} - {{discarded}} field value of PREPREPARE contains the index starting from which invalid requests are located.

In scope of this ticket it must be determined which behavior of the dynamic validation is correct and then the correct behavior must be ensured for all the cases.

_*NOTE:* As to processing of requests-duplicates, we made a fix preventing processing of a PROPAGATE message with a request-duplicate. The version 1.2.50-stable of indy-node does not contain this fix, so we could observe the case with processing of requests-duplicates described above. However, the described case revealed non-uniform behavior of the dynamic validation that we should investigate._",1.2.50-stable,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1103,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyvs7:",,,,,,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,SeanBohan_Sovrin,spivachuk,,,,,,,,,"03/Feb/18 6:43 AM;SeanBohan_Sovrin;if this is related to other issues of STN and live pool, this should be in current sprint [~ashcherbakov];;;","05/Feb/18 4:34 PM;ashcherbakov;I think it's worse investigation.;;;","07/Feb/18 7:47 PM;anikitinDSR;Request  ('J4N1K1SEB8uY2muwmecY5q', 1515169124908830) was a request for changing verkey, therefore it's expected behavior it's not dubliate.
Ledger include txn with verkey ""~PMiFnDt9fmNnSYzeEBY7{color:#d04437}Y{color}Q"" but in request ""~{color:#333333}PMiFnDt9fmNnSYzeEBY7{color:#d04437}Z{color}Q{color}"".
In current master logic for validating request for existing was changed. Now we check that owner of request is 'authorized'.;;;",,,,,,,,,,,,,,,,,,,,,,
QA: test pool upgrade under load of domain requests,INDY-1120,27303,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,01/Feb/18 9:44 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,explore,,,"From INDY-1103:
{quote}When testing upgrade to a new version, it makes sense to test pool upgrade under load of domain requests.
{quote}
See more detailed comment from Nikita: https://jira.hyperledger.org/browse/INDY-1103?focusedCommentId=39358&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-39358",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzyvlb:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,zhigunenko.dsr,,,,,,,,,,,"06/Feb/18 12:24 AM;zhigunenko.dsr;*Environment:*
AWS-cluster with 6 nodes (4 + 2)
indy-node 1.2.294
libindy 1.3.0~353

*Steps:*
1) Use existing pool with 4 original nodes and 2 additional nodes
2) Schedule upgrade to version=1.2.294 for all 6 nodes with 5-minutes delay between nodes
3) Before, during and after nodes upgrade Perf_add_nyms.py (Node2) and Perf_get_nyms.py (Agent01) have been running

*Actual results:*
* All nodes have the same amount of NYMs (_read_ledger --type domain --count_)
* There were no occasional problems during the execution of scripts
* All nodes services are active and with new version;;;",,,,,,,,,,,,,,,,,,,,,,,,
Primary replica may send PREPREPARE with duplicates of requests already committed to a ledger,INDY-1121,27318,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,spivachuk,spivachuk,02/Feb/18 1:29 AM,11/Oct/19 9:12 PM,28/Oct/23 2:47 AM,11/Oct/19 9:12 PM,,,,,,0,GA-0,,,"We saw cases when a primary replica sends a PREPREPARE message with duplicates of requests already committed to a ledger. The cases from INDY-959 and INDY-1045 are examples of this incorrect behavior.

The general scenario in these cases was as follows:
# The primary replica in some instance sends PREPREPARE with already ordered requests to all the other replicas in the instance.
# The nodes containing these other replicas send MESSAGE_REQUESTs for PROPAGATEs of these requests to all the others.
# The node containing the specified primary replica responds to received MESSAGE_REQUESTs by MESSAGE_RESPONSEs with requested PROPAGATEs.
# Having seen these requests as if for the first time, the rest of the nodes send PROPAGATEs to all the others (but actually the nodes just do not detect that these old already processed requests were received earlier).
# All the nodes reach quorums for PROPAGATEs and forward these requests to their replicas. 3PC-process for these requests proceeds in the specified instance and also starts in all the other instances.
# Eventually these requests are ordered for the second time.

In scope of INDY-959 we made a fix preventing processing of a PROPAGATE message with a request-duplicate. The version 1.1.43-stable of indy-node does not contain this fix, so we could observe the described behavior with committing requests-duplicates to a ledger in INDY-959 and INDY-1045. However, we still have not found the cause why a primary replica may send a PREPREPARE message with duplicates of requests already committed to a ledger. In scope of this ticket we must investigate and fix this issue.",1.1.43-stable,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-959,INDY-1045,,,,INDY-978,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwy67:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,Sprint 18.05,18.06,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,andkononykhin,ashcherbakov,spivachuk,,,,,,,,,,"22/Feb/18 12:54 AM;andkononykhin;PoA:
 * add checks for duplicate requests in PrePrepare 3PC batches
 ** against already received in previous PrePrepares
 ** against already ordered;;;","03/Aug/18 9:14 PM;ashcherbakov;Please have a look at the tests `test_preprepare_not_processed_if_any_request_is_already_ordered` and `test_preprepare_not_processed_if_any_request_is_already_in_3pc_process` in https://github.com/hyperledger/indy-plenum/pull/544;;;","11/Oct/19 9:12 PM;ashcherbakov;This is already implemented;;;",,,,,,,,,,,,,,,,,,,,,,
Validator-info shows log in shell output,INDY-1122,27320,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,VladimirWork,VladimirWork,02/Feb/18 1:37 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,validator-info,,,1,help-wanted,,,"Build Info:
indy-node 1.2.291

Steps to Reproduce:
1. Install pool.
2. Run `validator-info`.

Actual Results:
There are log entries (of all levels) in shell output.

Expected Results:
There should be no log entries (of all levels) in shell output.
",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1175,,,,,,,,,,,,,,,,,,,,,,"02/Feb/18 1:38 AM;VladimirWork;image.png;https://jira.hyperledger.org/secure/attachment/14527/image.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzzawn:",,,,,,,,,,,,,,,,,,,,,,,,,,arjanvaneersel,esplinr,mgbailey,SeanBohan_Sovrin,VladimirWork,,,,,,,,"08/Feb/18 6:43 AM;SeanBohan_Sovrin;[~VladimirWork]

are we using the logging module?

what sort of log setup is it missing?
where should the log entries go?

how was the log object configured?;;;","08/Feb/18 8:17 PM;VladimirWork;[~SeanBohan_Sovrin]

bq. are we using the logging module?
bq. what sort of log setup is it missing?
bq. how was the log object configured?
Validator-info tool uses the same logging module as indy-node. There is no separate log setup and configuring for this tool.

bq. where should the log entries go?
They go into /var/log/indy/validator-info.log (and it's ok) but the issue is that they double into shell output (there should be node statistics only without any logs).;;;","27/Mar/18 12:15 AM;mgbailey;[~SeanBohan_Sovrin] +1 for this ticket.  Our stewards use this feature all the time.  There should not be this gibberish surrounding the desired output.  In addition, this has broken everyones tools that use the --json flag, since this results in invalid json output.;;;","13/Apr/18 11:06 PM;mgbailey;[~SeanBohan_Sovrin] this should be a minimal effort ticket.  Can we get it into a sprint?;;;","18/May/18 11:56 AM;arjanvaneersel;I'd like to help out with this ticket. Should I assign it to myself?;;;","21/May/18 10:44 PM;esplinr;Resolved as part of INDY-1175.;;;",,,,,,,,,,,,,,,,,,,
Refactor common transactions structure,INDY-1123,27333,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,02/Feb/18 3:22 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"[Common Txn Structure|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md#common-structure]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1374,,,,,,,,,"1|hzz6y7:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.05,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,5.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"03/May/18 6:53 PM;ashcherbakov;We need to support the new format in SDK to be able to run tests in Plenum/Node with the changed format: https://jira.hyperledger.org/browse/IS-674;;;","10/May/18 5:50 PM;ashcherbakov;The main implementation is finished in `txn-refactoring` branches in plenum and node.

We need to make sure that all tests pass with a new libindy supporting the new format. It will be done in the scope of INDY-1319.;;;",,,,,,,,,,,,,,,,,,,,,,,
Refactor common Request structure,INDY-1124,27334,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,02/Feb/18 3:22 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,GA-0,,,"[Common Request format|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#common-request-structure]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzyavb:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,Sprint 18.05,18.06,,,,,5.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"28/Mar/18 12:59 AM;ashcherbakov;The work is done in [https://github.com/hyperledger/indy-node/tree/indy-886] and https://github.com/hyperledger/indy-plenum/tree/transactions-refactor;;;",,,,,,,,,,,,,,,,,,,,,,,,
Refactor common Reply structure,INDY-1125,27335,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,ashcherbakov,ashcherbakov,02/Feb/18 3:22 AM,30/May/18 7:28 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"[Common Reply format|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#common-reply-structure]",,,,,,,,,,,,,,,,,IS-658,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwybz:",,,,,,"Sprint 18.03 Stability, DKMS",,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor NYM txn and request,INDY-1126,27336,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,dsurnin,ashcherbakov,ashcherbakov,02/Feb/18 3:23 AM,30/May/18 7:28 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"[NYM req|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#nym]

[GET_NYM req|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#get_nym]

[NYM txn|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md#nym]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwybr:",,,,,,,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor ATTRIB txn and request,INDY-1127,27337,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,02/Feb/18 3:23 AM,30/May/18 7:28 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"[ATTRIB txn|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md#attrib]

[GET_ATTRIB req|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#get_attrib]

[ATTRIB req|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md#attrib]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwyc7:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor POOL txn and request,INDY-1128,27338,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,02/Feb/18 3:23 AM,30/May/18 7:29 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"See [requests|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md] and [txns|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwycf:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Refactor SCHEMA/CLAIM_DEF txn and request,INDY-1129,27339,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,02/Feb/18 3:26 AM,30/May/18 7:59 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"See [requests|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md] and [txns|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwycn:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Refactor POOL_UPGRADE, NODE_UPGRADE and POOL_CONFIG txn and request",INDY-1130,27340,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,02/Feb/18 3:26 AM,30/May/18 7:59 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"See [requests|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/requests.md] and [txns|https://github.com/hyperledger/indy-node/blob/c964f5c55bf5853799449aef1b6faad21d7ccbe6/docs/transactions.md]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwycv:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Anoncreds revocation design,INDY-1131,27341,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,ashcherbakov,ashcherbakov,ashcherbakov,02/Feb/18 3:29 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzyvnz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"02/Feb/18 8:44 PM;ashcherbakov;The design can be found on https://github.com/hyperledger/indy-node/pull/547;;;","02/Feb/18 9:31 PM;ashcherbakov;Duplicates INDY-680;;;",,,,,,,,,,,,,,,,,,,,,,,
Add indy docker base images for CI into Hyperledger Jenkins agents,INDY-1132,27359,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,andkononykhin,andkononykhin,02/Feb/18 6:08 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,We need to optimize docker build routine by adding our base images for CI into local docker cache inside OpenStack VMs (minions) that are used as on-demand one time agents on Hyperledger Jenkins server,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzywnj:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,3.0,,,,,,,,,,,,andkononykhin,VladimirWork,,,,,,,,,,,"16/Feb/18 1:04 AM;andkononykhin;[~ryjones] created new CloudStack template which includes indy docker image:  [https://github.com/hyperledger/ci-management/blob/master/packer/provision/docker.sh#L116]

All indy projects now require new Jenkins agents labels ""hyp-x-indy"" ([https://github.com/hyperledger/ci-management/blob/master/jjb/indy/indy-macros.yaml#L19]) which is pointed to mentioned new CloudStack template (https://jenkins.hyperledger.org/configure-readonly/).

 ;;;","16/Feb/18 1:27 AM;andkononykhin;Problem reason:
 - ci pipelines spent a lot of time to build docker images because docker images layers not included ones needed by indy ci docker containers

Changes:
 - baseimage for indy docker containers was added to docker cache on the CloudStack machines



Risk factors:
 - Nothing is expected.

Risk:
 - Low

Recommendations for QA: do the following sequence of steps
 * trigger a build for any indy project PR adding ""test this please"" comment
 * check on [https://jenkins.hyperledger.org/view/indy/] that a job was triggered and docker builds based on ""hyperledger/indy-core-baseci:0.0.1"" use local docker cache;;;","19/Feb/18 7:32 PM;VladimirWork;Steps to Validate:
1. Trigger a build for any indy project PR adding ""test this please"" comment.
2. Check on https://jenkins.hyperledger.org/view/indy/ that a job was triggered and docker builds based on ""hyperledger/indy-core-baseci:0.0.1"" use local docker cache.

Actual Results:
Docker builds based on ""hyperledger/indy-core-baseci:0.0.1"" use local docker cache (there are no ""Pulling fs layer..."" entries for indy-core-baseci in new ci jobs).;;;",,,,,,,,,,,,,,,,,,,,,,
Optimize multiple agents per build usage by indy CI on Hyperledger Jenkins,INDY-1133,27360,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,andkononykhin,andkononykhin,02/Feb/18 6:16 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Our pipelines for indy core have several stages each processed on Jenkins agents which are one-time on-demand OpenStack minions on Hyperledger Jenkins.

As long as new agent launching is quite slow operation need to:
 * investigate how it actually happens on Hyperledger Jenkins for our case
 * optimize

Optimization options:
 * optimize pipelines according to Hyperledger Jenkins specific
 * ask Hyperledger Jenkins admins to make number of hyp-x agents always available
 * ask Hyperledger Jenkins admins to allow reuse of launched agents",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzywnr:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,5.0,,,,,,,,,,,,andkononykhin,VladimirWork,,,,,,,,,,,"16/Feb/18 1:30 AM;andkononykhin;All indy projects now require new Jenkins agents labels ""hyp-x-indy"" ([https://github.com/hyperledger/ci-management/blob/master/jjb/indy/indy-macros.yaml#L19]) which is pointed to new CloudStack template ([https://jenkins.hyperledger.org/configure-readonly/]): ubuntu1604-indy-x86_64-2c-8g. That template configured to stay in idle stay for half an hour. Thus any new node('hyp-x-indy') during this time will get that idle minion.;;;","16/Feb/18 1:33 AM;andkononykhin;Problem reason:
 - ci pipelines performs a lot of separate stages each required new CloudStack minion

Changes:
 - indy jobs now use specific CloudStack template which is terminated only after half an hour idle state, thus it can be re-used

Risk factors:
 - Nothing is expected.

Risk:
 - Low

Recommendations for QA: do the following sequence of steps
 * trigger a build for any indy project PR adding ""test this please"" comment
 * check on [https://jenkins.hyperledger.org/view/indy/] that a job was triggered and Jenkins agents are re-used (check for 'running on' phrase in console log);;;","19/Feb/18 7:32 PM;VladimirWork;Steps to Validate:
1. Trigger a build for any indy project PR adding ""test this please"" comment.
2. Check on https://jenkins.hyperledger.org/view/indy/ that a job was triggered and Jenkins agents are re-used (check for 'running on' phrase in console log).

Actual Results:
Jenkins agents are re-used in ci jobs.;;;",,,,,,,,,,,,,,,,,,,,,,
Support REVOC_REG_DEF txn,INDY-1134,27366,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,02/Feb/18 8:33 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"[REVOC_REG_DEF txn|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#revoc_reg_def]",,,,,,,,,,,,,,,,,IS-464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzyr9j:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,,,,,,,,,,,"22/Feb/18 12:37 AM;anikitinDSR;Support for REVOC_REG_DEF transactions was added.
PR: https://github.com/hyperledger/indy-node/pull/567;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support GET_REVOC_REG_DEF request,INDY-1135,27367,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,02/Feb/18 8:33 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"[REVOC_REG_DEF txn|https://github.com/hyperledger/indy-node/blob/b790ab1155128e7e8ba05ac6d0d66fb5bf57bac9/design/anoncreds.md#revoc_reg_def]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzz107:",,,,,,Sprint 18.05,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support REVOC_REG_ENTRY txn,INDY-1136,27368,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,02/Feb/18 8:34 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"[REVOC_REG_ENTRY|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#revoc_reg_entry]",,,,,,,,,,,,,,,,,IS-464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzyr8n:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,,,,,,,,,,,"28/Feb/18 9:48 PM;anikitinDSR;Implemented support for REVOC_REG_ENTRY.
PR: https://github.com/hyperledger/indy-node/pull/576;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support GET_REVOC_REG request,INDY-1137,27369,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,02/Feb/18 8:36 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"[GET_REVOC_REG|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#get_revoc_reg]
[GET_REVOC_REG_DELTA|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#get_revoc_reg_delta]",,,,,,,,,,,,,,,,,IS-464,,,,,,,,,,,,INDY-933,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzz0xb:",,,,,,Sprint 18.05,18.06,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,sergey.minaev,,,,,,,,,,"12/Mar/18 7:10 PM;sergey.minaev;[~ashcherbakov] will GET_REV_REG_DELTA be implemented in this ticket scope?;;;","12/Mar/18 7:22 PM;ashcherbakov;[~sergey.minaev] Yes, this is for both GET_REVOC_REG and GET_REVOC_REG_DELTA;;;","14/Mar/18 10:51 PM;ashcherbakov;We need to have regular updates (with latest timestamp) to reduce the risk for malicious nodes to send very old data for GET_REVOC_REG. So, we need to implement INDY-933.;;;","21/Mar/18 5:27 PM;anikitinDSR;  PR: https://github.com/hyperledger/indy-node/pull/610
  Version of indy-node is 1.3.343-master
  All changes in reply structure was added in desing (see Desctiprion);;;",,,,,,,,,,,,,,,,,,,,,
[Revocation] Support getting state root by timestamp,INDY-1138,27370,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,02/Feb/18 8:37 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"[Timestamp Support in State|https://github.com/hyperledger/indy-node/blob/b790ab1155128e7e8ba05ac6d0d66fb5bf57bac9/design/anoncreds.md#timestamp-support-in-state]",,,,,,,,,,INDY-1205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzyav3:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,Sprint 18.05,,,,,,5.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,sergey-shilov,,,,,,,,,,"03/Mar/18 12:54 AM;sergey-shilov;The main question here is a persistent storage for storing of _(timestamp, root)_ pairs with ability to efficiently get the closest lower key. Our quick investigation shown that the best solution here is to use RocksDB as LevelDB has not required functionality and relational DBs look as too expensive solution for key-value storage. RocksDB is a fast, lightweight, high performance key-value persistent storage.
From the official RocksDB doc:

""Start from *4.13*, Rocksdb added _{{Iterator::SeekForPrev()}}_. This new API will seek to the last key that is less than or equal to the target key, in contrast with _{{Seek()}}_.""

[https://github.com/facebook/rocksdb/wiki/SeekForPrev]

This is exact what we need for current task. And also it fully correlates with INDY-1205, so I'm switching to this blocker task.;;;","15/Mar/18 6:35 PM;anikitinDSR;PR: https://github.com/hyperledger/indy-node/pull/602
Changes:
* created state-timestampe storage, key - timestamp, value - root_hash of state for txn timestamp.
* added method get_equal_or_prev into parent KeyValueStorage class and implementation for LevelDB;;;",,,,,,,,,,,,,,,,,,,,,,,
Support UUID to state trie key cache,INDY-1139,27371,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,02/Feb/18 8:37 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"See `State` section in, for example, [REVOC_REG_DEF|https://github.com/hyperledger/indy-node/blob/b790ab1155128e7e8ba05ac6d0d66fb5bf57bac9/design/anoncreds.md#revoc_reg_def]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-59,,,,,,,,,"1|hzywnb:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"13/Feb/18 11:57 PM;ashcherbakov;We don't need it, as State Trie key will be used as ID instead of UUID.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Publish indy pipelines' logs to Nexus,INDY-1140,27373,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,andkononykhin,andkononykhin,02/Feb/18 9:35 PM,09/Oct/19 6:19 PM,28/Oct/23 2:47 AM,09/Oct/19 6:19 PM,,,,,,0,devops,,,"As [~ry] pointed in out indy jobs we need to use the standard macros (used by all jenkins jobs)  to publish the logs to nexus because logs are not kept for a long on Hyperledger Jenkins server.

Examples and snippets that Ry suggest to start with:
{code:none}
 publishers:
 - lf-infra-publish
{code}
{code:none}
 builders:
 - verify-commit-message
 {code}
{code:none}
 publishers:
 - lf-infra-publish
 - archive-artifacts:
 artifacts: '.tox/**/*.log'
 {code}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzwylb:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,esplinr,,,,,,,,,,,"09/Oct/19 6:19 PM;esplinr;We are moving from Jenkins to GitLab, and will need a different method of publishing logs.;;;",,,,,,,,,,,,,,,,,,,,,,,,
One node falls behind others,INDY-1141,27374,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,02/Feb/18 10:22 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Build Info:
indy-node 1.2.291

Steps to Reproduce:
1. Install pool of 7 nodes.
2. Run load test with 20 threads by 100 NYMs to write *from 1st node*.
3. Check the domain ledger count by validator-info or read_ledger.

Actual Results:
1st node falls behind others (1851 vs 2018).

Expected Results:
Each node should have the same domain ledger count.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1033,INDY-757,,,,INDY-1188,,,,,,,,,,,,,,,"02/Feb/18 10:21 PM;VladimirWork;Node1.log;https://jira.hyperledger.org/secure/attachment/14535/Node1.log","05/Feb/18 8:09 PM;VladimirWork;debug.tar.gz;https://jira.hyperledger.org/secure/attachment/14545/debug.tar.gz","02/Feb/18 11:41 PM;VladimirWork;load_from_client_case.tar.gz;https://jira.hyperledger.org/secure/attachment/14537/load_from_client_case.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzywnz:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,,,,,,,,,,,,,dsurnin,VladimirWork,,,,,,,,,,,"02/Feb/18 11:43 PM;VladimirWork;There is the same issue when I run load test from client machine (logs from 1st (primary), 2nd and 5th (lagged) node are in attachment).;;;","07/Feb/18 12:20 AM;dsurnin;it looks like lagged node is able to restore valid state after load script is finished;;;","07/Feb/18 7:48 PM;VladimirWork;The issue is still reproducing in AWS pool with 4 nodes during adding NYMs via 80 threads. Logs are too big to add them to Jira, will be sent in Slack.;;;","19/Feb/18 10:25 PM;dsurnin;lagged node cannot catchup because batching logic cannot generate batch from messages almost reached the size limit
fixed in
node v 310
plenum v 250;;;","20/Feb/18 8:29 PM;VladimirWork;Build Info:
indy-node 1.3.310
indy-plenum 1.2.250

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run 80 threads x 100 write requests load test (from 1st node) and check domain ledger count.
3. Run 100 threads x 100 write requests load test (from 1st node) and check domain ledger count.
4. Run 120 threads x 100 write requests load test (from 1st node) and check domain ledger count.
5. *Run 200 threads x 50 write requests load test (from 1st node) and check domain ledger count.*
6. *Run 1 thread x 100 write requests load test (from 1st node) and check domain ledger count.*
7. *Run 1 thread x 1 write request load test (from 1st node) and check domain ledger count.*

Actual Results:
There are 29531 txns written to 1st node vs 29533 txns written to all other nodes at Step 5 (2 lost txns).
There are 29627 txns written to 1st node vs 29633 txns written to all other nodes at Step 6 (6 lost txns).
There are 29627 txns written to 1st node vs 29634 txns written to all other nodes at Step 7 (7 lost txns).
;;;","22/Feb/18 11:55 PM;dsurnin;looks like issue is not reproduced with small load.

for QA:
could you please test basic case:
  25 nodes, 20 threads, >= 100000 reqs, run load test from the client machine;;;","27/Feb/18 9:26 PM;VladimirWork;The issue is not reproducing with small load (less than 200 threads) and pools up to 11 nodes.
Case with 25 nodes pool will be tested in scope of INDY-1180.;;;",,,,,,,,,,,,,,,,,,
"ACK, NACK, Reject refactoring",INDY-1142,27382,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,ashcherbakov,ashcherbakov,03/Feb/18 12:37 AM,30/May/18 7:59 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,See https://github.com/hyperledger/indy-node/pull/536,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1375,,,,,,,,,"1|hzwyd3:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Needs to be obvious when a node is blacklisted (in logs, here's why)",INDY-1143,27392,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,SeanBohan_Sovrin,SeanBohan_Sovrin,03/Feb/18 6:57 AM,05/Feb/18 4:36 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyvvj:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,SeanBohan_Sovrin,,,,,,,,,,,"05/Feb/18 4:36 PM;ashcherbakov;[~SeanBohan_Sovrin] Can you please provide more details?
is it about OTHER nodes being blacklisted, or THIS node being blacklisted?
Also as of now there is only one case (catch-up-related) when the node is really blacklisted.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Need a way to re-enroll a node after it has been blacklisted without having to reboot all nodes in pool,INDY-1144,27393,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,SeanBohan_Sovrin,SeanBohan_Sovrin,03/Feb/18 6:59 AM,08/Feb/18 6:36 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyvvr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,SeanBohan_Sovrin,,,,,,,,,,,"05/Feb/18 4:38 PM;ashcherbakov;There is only one case when the node is really blacklisted (if catch-up failed). So, I don't think we can do anything here.;;;","08/Feb/18 6:36 AM;SeanBohan_Sovrin;in the worst case we have to manually repair node, what do we have to do to get it BACK into the pool without rebooting the pool?;;;",,,,,,,,,,,,,,,,,,,,,,,
In order to troubleshoot the pool there needs to be better access to the logs,INDY-1145,27394,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,SeanBohan_Sovrin,SeanBohan_Sovrin,03/Feb/18 7:01 AM,03/Feb/18 7:01 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyvvz:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inconsistencies in the node logs in the cluster,INDY-1146,27409,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,SeanBohan_Sovrin,zhigunenko.dsr,zhigunenko.dsr,05/Feb/18 5:37 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Case 1*
*Steps to reproduce:*
1. Create pool with 4 nodes
2. Add another two nodes sequentially
3. Run load script which adds NYMs

*Expected results*
Similar entries in the logs of each node

*Actual results*
The logs of the original nodes are different from the logs for the added nodes

*Case 2*
Some of the log entries do not correspond to the logging level. E.g. syslog contains debugging records of a python script.","AWS acceptance poool
indy-anoncreds 1.0.32
indy-node 1.2.291
indy-plenum 1.2.231 
libindy-crypto 0.2.0
python3-indy-crypto 0.2.0
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/18 5:37 PM;zhigunenko.dsr;node1-4.log;https://jira.hyperledger.org/secure/attachment/14543/node1-4.log","05/Feb/18 5:37 PM;zhigunenko.dsr;node5-6.log;https://jira.hyperledger.org/secure/attachment/14542/node5-6.log","05/Feb/18 5:37 PM;zhigunenko.dsr;syslog.log;https://jira.hyperledger.org/secure/attachment/14541/syslog.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-779,,,,,,,,,"1|hzwykv:",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,"07/Sep/18 10:32 PM;zhigunenko.dsr;*Reason to Close:*
Issue is outdated.;;;",,,,,,,,,,,,,,,,,,,,,,,,
To support s390 platform.,INDY-1147,27456,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,MichaelWang,MichaelWang,07/Feb/18 12:48 AM,11/Oct/19 10:08 PM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,To resolve all the incompatible  dependencies which is necessary to build the package for s390.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-9,,s390 platform support,Done,,,,,,,"1|hzyvgn:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,MichaelWang,,,,,,,,,,,"28/Sep/18 7:19 AM;esplinr;Cleaning up empty epics.

We will not be prioritizing support for s390 at this time. We can reopen this epic of we get multiple requests.

If someone wants to do this work, we would be interested in receiving pull requests. If someone identifies specific issues, they can reopen this epic and create specific issues that we would consider.;;;",,,,,,,,,,,,,,,,,,,,,,,,
It's possible to create several nodes with the same alias,INDY-1148,27457,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ozheregelya,ozheregelya,07/Feb/18 12:50 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Steps to Reproduce:
1. Setup the pool and several clear nodes for adding.
2. Add first node.
3. Add second node with the same alias.

Actual Results:
Node is successfully added with not unique alias. After adding alias can't be changed.

Expected Results:
Alias should be validated for uniqueness.",indy-node 1.2.294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Mar/18 8:06 PM;VladimirWork;INDY-1148.PNG;https://jira.hyperledger.org/secure/attachment/14735/INDY-1148.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0zz:",,,,,,Sprint 18.05,,,,,,,,,,,,,,,,,,,,ozheregelya,Toktar,VladimirWork,,,,,,,,,,"06/Mar/18 5:28 PM;Toktar;Problem reason:
 -  Have not validation of uniqueness for alias.

Changes:
 *  Added validation of uniqueness for alias.

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/554]
 - [https://github.com/hyperledger/indy-node/pull/592]

Version:
 - indy-node 1.3.329-master
 - indy-plenum 1.2.264-master

Risk factors:
 - Nothing is expected

Risk:
 - Low

Covered with tests:
 - [*test_nodes_with_pool_txns.py* - test_add_node_with_not_unique_alias|https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/pool_transactions/test_nodes_with_pool_txns.py];;;","07/Mar/18 8:06 PM;VladimirWork;Build Info:
indy-node 1.3.330

Steps to Validate:
1. Setup the pool and several clear nodes for adding.
2. Add first node.
3. Add second node with the same alias.

Actual Results:
New CLI: there is `existing data has conflicts with request data` validation error. !INDY-1148.PNG|thumbnail! 
Old CLI: there is an error that STEWARD has no verkey (but it actually has and can add this node with another alias).

So nodes with not unique aliases cannot be added in both CLIs (wrong error in old CLI doesn't matters since this CLI will be outdated soon).;;;",,,,,,,,,,,,,,,,,,,,,,,
Non-zero instance of primary node is same as instance 0,INDY-1149,27494,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,zhigunenko.dsr,zhigunenko.dsr,07/Feb/18 11:41 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Steps to reproduce:*
1. Create pool with 7 nodes
>> _Primary: node2 for instance 0,  node3 for instance1, node4 for instance2_
2. Demote node2
>> _Primary: node5 for instance 0,  node6 for instance1 (view9)_
3. Promote node2
_Primary: {color:red}node5 for instance 0{color},  node6 for instance1, {color:red}node5 for instance2{color} (view9)_

*Expected results:*
All instances link to different nodes

*Additional info:*
Revealed during [INDY-794|https://jira.hyperledger.org/browse/INDY-794]","indy-anoncreds 1.0.32
indy-cli 1.3.0~363
indy-node 1.2.296
indy-plenum 1.2.235
libindy 1.3.0~363
libindy-crypto 0.2.0
python3-indy-crypto 0.2.0                ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzywef:",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,"07/Feb/18 11:54 PM;zhigunenko.dsr;Duplication of INDY-1112;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool upgrade was scheduled but was not happened on part of nodes,INDY-1150,27496,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,08/Feb/18 12:15 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Case 1:

Steps to Reproduce:
 1. Setup the pool of 7 nodes.
 2. Add 18 nodes manually.
 3. Send some count of transactions (in my cases first time it was 300 txns, second time - 170).
 4. Schedule upgrade:
{code:java}
send POOL_UPGRADE name=upgrade12294 version=1.2.294 sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv':'2018-02-05T05:00:00.000000+00:00', '8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-02-05T05:05:00.000000+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya':'2018-02-05T05:10:00.000000+00:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA':'2018-02-05T05:15:00.000000+00:00', '4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe':'2018-02-05T05:20:00.000000+00:00', 'Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8':'2018-02-05T05:25:00.000000+00:00', 'BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW':'2018-02-05T05:30:00.000000+00:00', '4EG9n9ErTVqcZ7xSnhHH8PQVtYDfugw5SwqcezvYVrGg':'2018-02-05T05:35:00.000000+00:00', 'FR454t4km7JJvaZc6ttzFU9Lor5ib9YB3wao1kJeoEZb':'2018-02-05T05:40:00.000000+00:00', 'F9abmLwUQy5svguBPNi2GQTdKsiVGDAyBfaFpwcpLJDm':'2018-02-05T05:45:00.000000+00:00', '72vCSKVDbcbpnCXekEd9vhagse75uXnjJDjDGYDxeXPG':'2018-02-05T05:50:00.000000+00:00', '8oE3bGvZTzGxVQRHxfLZHD8Uyx2cbX6wnF1aQAiJe5sm':'2018-02-05T05:55:00.000000+00:00', 'A9hZwUqe62MNQXoswU47271CyRf7g1G5Gat3aqeQ6DeN':'2018-02-05T06:00:00.000000+00:00', '2LssMb56SBibrQYFhRnnMvZHhEiaorMuNkmW8QsZfVwA':'2018-02-05T06:05:00.000000+00:00', '8A5NzusREw844wQmttwBqhhWkTNby5o4UvYq6T1FsByb':'2018-02-05T06:10:00.000000+00:00', 'Bh492xBFGYKS7Z57EQkP5cQtJp6jDvypHoSpXHh259q5':'2018-02-05T06:15:00.000000+00:00', '2MbQjn7ij9DFKT7rt425SaeDvgzyjycv72FEiPVdacEb':'2018-02-05T06:20:00.000000+00:00', 'EwZyzG8HBvjWvxmWVgreTVWYQJScpWMVKeqUa7Rk1Pr5':'2018-02-05T06:25:00.000000+00:00', '58b3Fy45qjcBfVtEt2Zi1MgiRzX9PPmj68FwD143SuWQ':'2018-02-05T06:30:00.000000+00:00', '2FGgKVcp2heyWiGTDLVEyF6AJrfaQBvrhkUzhYTjiHA6':'2018-02-05T06:35:00.000000+00:00', '6CRQcKzeRMCstErDT2Pso4he3rWWu1m16CRyp1fjYCFx':'2018-02-05T06:40:00.000000+00:00', '53skV1LWLCbcxxdvoxY3pKDx2MAvszA27hA6cBZxLbnf':'2018-02-05T06:45:00.000000+00:00', 'CbW92yCBgTMKquvsSRzDn5aA5uHzWZfP85bcW6RUK4hk':'2018-02-05T06:50:00.000000+00:00', 'H5cW9eWhcBSEHfaAVkqP5QNa11m6kZ9zDyRXQZDBoSpq':'2018-02-05T06:55:00.000000+00:00', 'DE8JMTgA7DaieF9iGKAyy5yvsZovroHr3SMEoDnbgFcp':'2018-02-05T07:00:00.000000+00:00'} timeout=10 force=False{code}
Actual Results:
 After 300 txns: 6 of 25 nodes were not upgraded (12, 15, 19, 22, 23, 24).
 After 170 txns: 5 of 25 nodes were not upgraded (14, 16, 23, 24, 25). After restart of node14, it was upgraded. 

Expected Results:
 Pool should be successfully upgraded.

Logs and data:
 After 300 txns: [https://drive.google.com/open?id=1xUok5VxjQShQP29XD85h16BWk6bRSLH4]
 Unfortunately, data of not upgraded nodes was not copied for not upgraded nodes.

After 170 txns: [https://drive.google.com/file/d/1RcIam23g7xCCSWD_Av4J493LbgSq-7Bd/view?usp=sharing]
 journalctl: [https://drive.google.com/file/d/1uG0AGwxb8Tk5KUaGO67HBjt5b-ZzBTYU/view?usp=sharing]
 Config ledger: [https://drive.google.com/file/d/17AXPe6sOYaXIKV0Hk6WbiaxDEwHg7lZN/view?usp=sharing]

Case 2 (in progress):
 Need to try the same case for upgrade with force=true.
UPD: POOL_UPGRADE with force=True at the same time works without any problems. All nodes were upgraded and worked after upgrade.","Environment:
QA live pool (7+18 nodes)
indy-node 1.2.214 -> 1.2.294",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1152,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyrcn:",,,,,,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,,anikitinDSR,ozheregelya,,,,,,,,,,,"09/Feb/18 9:07 PM;anikitinDSR;Need to test upgrade from 1.2.301 to 1.2.302 (Simple upgrade, without forcing);;;","12/Feb/18 10:30 PM;ozheregelya;Environment:
indy-node 1.2.301 -> 1.2.302,
QALive pool (7+18 nodes)

Steps to Validate:
1. Setup the pool of 7 nodes with 1.2.301 version.
2. Add 18 nodes to the pool.
3. Schedule upgrade to version 1.2.302 with force=False.

Actual Results:
Upgrade successfully completed on all nodes.

Additional Information:
Upgrade from previous stable works only with force=true, when all nodes upgraded at the same time.;;;",,,,,,,,,,,,,,,,,,,,,,,
"When returning N-F nodes to the pool, ""View change"" does not occur if Primary node is stopped",INDY-1151,27528,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,zhigunenko.dsr,zhigunenko.dsr,08/Feb/18 10:15 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Steps to Reproduce:*
1) Create pool from 4 nodes, where node2 is primary
2) Stop indy-node service on node4
3) Stop indy-node service on node2
4) Send some NYM from client
=> There is no feedback from pool (consensus is lost)
5) Start indy-node service on node4
=> ""view change"" was not happened after node start, consensus was not restored
6) Send some NYM from client
=> No ""view change"" by timeout or after request
7) Start indy-node service on node2
8) Send some NYM from client
=> ""view change"" was made after request, requests were written

*Actual Results:*
View change was not happened when the pool comes back to consensus with stopped primary node.

*Expected Results:*
View change should happen when the pool comes back to consensus with stopped primary node.","indy-anoncreds             1.0.32
indy-node                  1.2.296
indy-plenum                1.2.235
libindy                    1.3.0~363
libindy-crypto             0.2.0
python3-indy-crypto        0.2.0",,,,,,,,,,,,,,,,,,,,,,,INDY-1166,,,,,,,,,,,,,,,,,,,,,,,,,"09/Feb/18 10:27 PM;zhigunenko.dsr;1151_clean.7z;https://jira.hyperledger.org/secure/attachment/14564/1151_clean.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyran:",,,,,,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,,ozheregelya,sergey-shilov,zhigunenko.dsr,,,,,,,,,,"15/Feb/18 1:05 AM;sergey-shilov;*Problem state / reason:*

When returning N-F nodes to a pool, ""View change"" does not occur if Primary node is stopped. This happens due to the fact that a node currently participating in the pool does not schedule a view change if the pool has come back to quorum when a primary is still down.

*Changes:*

Now a node schedules a view change if it transits its state from _starting_ to _started_hungry_ and a primary is set but it is still down. It means that a pool has come back to quorum while primary is still down, so initiating of view change process makes sense.

*Committed into:*

    [https://github.com/hyperledger/indy-plenum/pull/531]
    https://github.com/hyperledger/indy-plenum/pull/533
    https://github.com/hyperledger/indy-node/pull/565
    indy-node 1.2.308-master

*Risk factors:*

    Some cases of view change may be influences by committed changes, but this is not expected.

*Risk:*

    Medium

*Recommendations for QA:*

Do full testing of view change.
Also the test case mentioned in description of this ticket should be repeated and expected results should be got.;;;","15/Feb/18 7:08 AM;ozheregelya;*Environment:*
 indy-node 1.3.308
 docker pool of 4 and 7 nodes

*Steps to Validate:*
 1. Setup the pool of 4 nodes, check the primary, make sure that all nodes working.
 => primary is node2.
 2. Stop Node4, send transaction.
 => transaction was successfully written.
 3. Stop Node2, send transaction.
 => transaction was not written.
 4. Start Node4, check primary, write transaction.

*Actual Results:*
 Consensus restored after starting Node4. 

*Following cases also were verified:*
 - The same case with 7 nodes.
 - View change in case of stopping primary.
 - View change in case of primary performance decreasing (load test).
 - View change in case of primary demotion.
 Unclear behavior was noticed after primary demotion: one of nodes stopped writing transactions after primary demotion and re-promotion. Problem was reproduced only once. Logs are collected for this problem and it will be moved to separated ticket after discussion with developers.
UPD: ticket number for this problem is INDY-1179.;;;",,,,,,,,,,,,,,,,,,,,,,,
Failed restart after getting unhandled exception (KeyError),INDY-1152,27532,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,anikitinDSR,anikitinDSR,09/Feb/18 12:43 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"While testing pool upgrade was noted, that nodes which failed the upgrade procedure had broken indy-node processes. In systemd service we have option like 'Restart=on-failure' that meens that service must be restarted when running process will broke (by KeyError unhandled exception for example). All of nodes with faulty upgrade procedure had unhandled exception and systemctl did not restart indy-node service. 
 The main assumption is that main python process had unhandled exception, was stopped but child processes was not stopped and systemd not restart service.
 Ways to investigate:
 # There is several type of service - 'forked' and 'sample'. What of type do we use dy default?
 # How many subprocesses we use for indy-node service

Also, set 'SIGTERM' into list of handled signal.",indy-node 1.2.50-stable,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1150,,,,,,,,,,,,,,,,,,,,"15/Feb/18 6:47 PM;VladimirWork;INDY-1152.PNG;https://jira.hyperledger.org/secure/attachment/14600/INDY-1152.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyrb3:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,,,,,,,,,,,,,anikitinDSR,VladimirWork,,,,,,,,,,,"14/Feb/18 6:00 PM;anikitinDSR;Problem reason: 
- When raised unhandled exception, node's looper killed unexpectedly

- When run 'systemctl stop indy-node' main process killed unexpectedly too

Changes: 
- Added SIGTERM signal into handled signal

- Don't wait runFut job if unhandled exception was raised

PR:
- https://github.com/hyperledger/indy-plenum/pull/530


Version:
- master

Risk:
- Low

Recommendations for QA
- run 'systemctl stop indy-node'. In logs excpected record like ""Looper shut down in <seconds num> seconds"";;;","15/Feb/18 6:50 PM;VladimirWork;Build Info:
indy-node 1.3.308
indy-plenum 1.2.248

Steps to Validate:
1. Shut down node by `systemctl stop indy-node`.
2. Check logs for ""Looper shut down in <seconds num> seconds"".
3. Start node back.
4. Change node code to raise KeyError serially.
5. Check logs and journalctl for node restarting due to raised KeyError.

Actual Results:
There is ""Looper shut down in <seconds num> seconds"" entry in logs on node shutdown.
Node starts after KeyError-based shutdown successfully. !INDY-1152.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,,
Ordering of 3PC-batches on a backup replica may stop after a catch-up,INDY-1153,27536,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,spivachuk,spivachuk,09/Feb/18 1:17 AM,10/Oct/19 10:51 PM,28/Oct/23 2:47 AM,10/Oct/19 10:51 PM,,,,,,0,GA-0,,,"Ordering of 3PC-batches on a backup replica may stop after a catch-up. It is so because the backup replica tries to adjust {{last_ordered_3pc}} according to the lowest probable prepared certificate (in order to be able to order next batches) only when it receives and stashes an out-of-order PREPREPARE. But normally the PREPREPARE is received before the PREPAREs (which forms together the prepared certificate).

However, usually the backup replica succeeds with adjusting {{last_ordered_3pc}} because when after a catch-up the replica receives and stashes the next PREPREPARE which turns out to be out-of-order, it requests missing PREPREPAREs and PREPAREs. This step seems to be incorrect because the backup replica will discard most (if not all) previous 3PC-messages as laying out of the watermarks (which were updated after a catch-up using the master's last ordered 3PC-key). But the range of messages being requested as missed includes {{pp_seq_no}} of the out-of-turn PREPREPARE. So this PREPREPARE is re-received from other nodes in MESSAGE_RESPONSEs. Due to the last two MESSAGE_RESPONSEs with this PREPREPARE are usually received after the quorum number of the PREPAREs have been received in the normal way, the backup replica adjusts its {{last_ordered_3pc}} when receives the former and commits the prepared certificate when receives the latter. After this the backup replica is able to process new batches in the normal way.

In scope of this ticket the specified logic must be reworked. The range of messages being requested as missed must never include {{pp_seq_no}} of the out-of-order PREPREPARE. When after a catch-up a backup replica receives the next PREPREPARE, it must not request the previous messages. But at the same time, a backup replica must set up its state after a catch-up in a way to be able to order next 3PC-batches.",,,,,,,,,,INDY-1301,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1382,,,,,,,,,"1|hzwy6n:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,,,,,,,,,,,,,ashcherbakov,spivachuk,,,,,,,,,,,"06/Mar/18 2:31 AM;spivachuk;*PoA:*
The backup replicas must be able to restore their state in a redundant way as well as the master replica is. For this {{viewNo}} and {{ppSeqNo}} fields in LEDGER_STATUS and CONSISTENCY_PROOF messages must be replaced with a list of pairs ({{viewNo}}, {{ppSeqNo}}) for each replica. This, in turn, will require to track 3PC-keys for {{txnSeqNos}} not only for the master replica but for all the replicas.;;;","07/Mar/18 10:20 PM;spivachuk;In a detailed discussion with [~ashcherbakov] of the approach proposed in the previous comment we agreed that the 3PC-keys for backup replicas being reported by LEDGER_STATUS and CONSISTENCY_PROOF messages must correspond to some synchronization point as well as the 3PC-key for the master replica now corresponds to the last transaction in a ledger. We cannot just report the last ordered 3PC-keys for replicas because, for example, in case of significant load, lacking CONSISTENCY_PROOFs are requested for a specific end transaction {{seqNo}} in order to gather the quorum of the same messages. However, for a backup instance we cannot use the last transaction {{seqNo}} as a synchronization point for the 3PC-keys in LEDGER_STATUSes or CONSISTENCY_PROOFs. This is so because the backup instance may not have ordered this transaction for the moment or even may not be going to order it (if the primary of the instance is offline). This means that we should use an individual synchronization point for each instance. To make this possible, we can emulate commitment of transactions to a ledger on backup replicas. With this we will be able to use {{seqNo}} of such a virtual transaction on a backup instance as a synchronization point for the 3PC-keys being reported by LEDGER_STATUSes or CONSISTENCY_PROOFs. However, in such the case each replica of a node will have its own target state when the node is performing catch-up. These states will not be synchronized between the replicas of the node. This makes it reasonable to divide the whole node catch-up procedure into separate procedures for each of the replicas. This work must be included into the scope of INDY-971.;;;","26/Apr/18 6:10 PM;spivachuk;Created the separate task INDY-1301 for a design of the division of the whole node catch-up procedure into separate procedures for each of the replicas.;;;","10/Oct/19 10:51 PM;ashcherbakov;We implemented fixes for this. Also we are going to get rid of Backup Instances after moving to Aardvark.;;;",,,,,,,,,,,,,,,,,,,,,
[TechDoc]  Communication layer covering curvezmq's use and the Stack,INDY-1154,27542,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,09/Feb/18 2:12 AM,13/Mar/18 10:53 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyhr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] View change process,INDY-1155,27543,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Duplicate,,ashcherbakov,ashcherbakov,09/Feb/18 2:12 AM,09/Oct/19 5:43 PM,28/Oct/23 2:47 AM,09/Oct/19 5:43 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2138,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyhz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] Monitoring,INDY-1156,27544,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,09/Feb/18 2:13 AM,09/Oct/19 5:46 PM,28/Oct/23 2:47 AM,09/Oct/19 5:46 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyi7:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"09/Oct/19 5:46 PM;esplinr;Each administrator deploying an Indy network will need to provide their own monitoring solution.

If we need to implement some common approaches in Indy, then we will document them as separate issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"[TechDoc] BLS, its use during consensus",INDY-1157,27545,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,09/Feb/18 2:14 AM,11/Jun/19 5:19 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyfo:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] Config params,INDY-1158,27546,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,09/Feb/18 2:14 AM,13/Mar/18 10:53 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyin:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] Addition and removal of nodes covered by PoolManager,INDY-1159,27547,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,09/Feb/18 2:15 AM,13/Mar/18 10:53 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyiv:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] IdrCache,INDY-1160,27548,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,09/Feb/18 2:15 AM,09/Oct/19 5:49 PM,28/Oct/23 2:47 AM,09/Oct/19 5:49 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyj3:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"09/Oct/19 5:49 PM;ashcherbakov;We have this in https://github.com/hyperledger/indy-plenum/blob/master/docs/source/storage.md;;;",,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] Quorum values,INDY-1161,27549,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,09/Feb/18 2:16 AM,09/Oct/19 6:00 PM,28/Oct/23 2:47 AM,09/Oct/19 6:00 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1022,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyjb:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"09/Oct/19 6:00 PM;esplinr;This does not need to be a separate epic.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Exception: NETWORK_NAME must be set in /etc/indy/indy_config.py,INDY-1162,27629,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,MichaelWang,MichaelWang,12/Feb/18 5:46 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"I install all the packages with pip install.

When I try to start indy CLI, I got this error.

I wonder if this is a bug.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-987,,,,,,,,,"1|hzwyfb:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,MichaelWang,,,,,,,,,,,"13/Sep/18 12:27 AM;esplinr;We are not seeing this error in recent releases of Indy Node. If you are able to reproduce the problem on the current release, please create a new issue.

Issues should contain:
* The exact steps to reproduce
* The expected behavior
* The observed behavior
* And the environment;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool has lost consensus after primary demotion,INDY-1163,27638,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,,VladimirWork,VladimirWork,13/Feb/18 12:38 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"Build Info:
indy-node 1.3.51

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Force view change by primary disconnecting (2nd node).
3. Write and read NYM.
4. Connect node back.
5. Force view change by primary demoting (3rd node).
6. Write and read NYM.

Actual Results:
Pool has lost write consensus in Step 5. Step 6 NYM is not written.

Expected Results:
Pool should have write\read consensus at n=3 f=0.

Additional Info:
Pool reaches consensus after all nodes' restart.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1583,,,,,,,,,,,,,,,"14/Feb/18 10:42 PM;VladimirWork;INDY-1163-new.tar.gz;https://jira.hyperledger.org/secure/attachment/14592/INDY-1163-new.tar.gz","14/Feb/18 10:41 PM;VladimirWork;INDY-1163.PNG;https://jira.hyperledger.org/secure/attachment/14591/INDY-1163.PNG","13/Feb/18 12:43 AM;VladimirWork;INDY-1163.tar.gz;https://jira.hyperledger.org/secure/attachment/14572/INDY-1163.tar.gz","13/Feb/18 12:38 AM;VladimirWork;bad_primary_demote.PNG;https://jira.hyperledger.org/secure/attachment/14571/bad_primary_demote.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwyfr:",,,,,,"Sprint 18.03 Stability, DKMS",EV 18.17 Service Pack,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,VladimirWork,,,,,,,,,,"13/Feb/18 5:42 PM;VladimirWork;Issue reproduces with 4 nodes setup only (can't reproduce with 6 nodes setup with the same steps).;;;","14/Feb/18 5:41 PM;ashcherbakov;Added an integration test to reproduce the scenario:
PR: [https://github.com/hyperledger/indy-plenum/pull/532]

The tests passes.

 

Also There is something really weird in the log files:
Node2:0 discards PREPREPARE with a message `PRE-PREPARE being sent to primary; suspicion code is 1`.
 * Node2:0 declares view change 3 as completed for instance 0, new primary is Node1:0
 * Node2 receives PREPREPARE  from Node1
 * According to code, `_can_process_pre_prepare`, it checks first that PREPREPARE   comes from the Primary. The check passes, so Node2:0 has Node1 as a Primary.
 * The next check is that the current replica is not a Primary (so that the PREPREPARE    is not sent TO Primary). And this check fails. So, Node2:0 started to think that Node2 is the Primary (not Node1).
 * The code is sequential, this is just two subsequent checks. Looks really weird.

I suggest to re-test it.

 ;;;","14/Feb/18 10:42 PM;VladimirWork;Issue is reproducing with the same steps on 1.3.51. !INDY-1163.PNG|thumbnail!  [^INDY-1163-new.tar.gz] ;;;","23/Feb/18 12:51 AM;ashcherbakov;The issue wasn't reproduced on my machine (and added integration test passes). However, it's reproduced on Vladimir's machine.;;;","20/Aug/18 8:39 PM;ashcherbakov;Needs to be fixed together with INDY-1580;;;","27/Aug/18 7:53 PM;ozheregelya;Will be re-tested in scope of INDY-1580 (according comment was added to INDY-1580).;;;",,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.3.55,INDY-1164,27644,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,kelly.wilson,VladimirWork,VladimirWork,13/Feb/18 2:12 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,19/Feb/18 12:00 AM,0,Documentation,,,"*Version Information*
 indy-node 1.3.55
 indy-anoncreds 1.0.11
 indy-plenum 1.2.34
 sovrin 1.1.8

*Upgrade to this version should be performed simultaneously for all nodes (with `force=True`).*

*Major Fixes*
 INDY-799 - Transactions missing from config ledger after upgrade
 INDY-960 - Node is broken after load_test.py run
 INDY-911 - Pool stopped taking transactions after sending 1,000 simultaneous transactions
 INDY-986 - Pool stops working: Node services stop with 1,000 simultaneous clients doing GET_NYM reads
 INDY-948 - Node is broken after adding it to pool
 INDY-1048 - generate_indy_pool_transactions can be run only by indy user
 INDY-1035 - Do not allow update of existing Schema
 INDY-1018 - Pool is unable to write txns after two nodes adding
 INDY-1083 - It should not be possible to override CLAIM_DEF for existing schema-did pair
 INDY-1077 - The huge amount of calls and a lot of execution time in kv_store.py
 INDY-1029 - One of added nodes doesn't catch up
 INDY-1025 - Pool stopped working and lost consensus while new node was performing a catch-up
 INDY-1054 - View Change on large pools of 19 or more nodes can cause pool to stop functioning
 INDY-1034 - View change issue stopped pool from accepting new transactions
 INDY-1076, INDY-1079 - Unable to send transactions in STN
 INDY-1061 - Replica.lastPrePrepareSeqNo may not be reset on view change
 INDY-897 - Unable to send an upgrade transaction without including demoted nodes
 INDY-1069 - Nym request to STN results in inconsistent responses
 INDY-959 - Validator node is re-promoted during view change
 INDY-1078 - False cancel message during upgrade
 INDY-1045 - Transactions added to nodes in STN during system reboot
 INDY-1033 - Problems with nodes demotion during load test
 INDY-995 - Node monitoring tool (email plugin) doesn't work
 INDY-1074 - ATTRIB transaction with ENC and HASH doesn't work
 INDY-1151 - When returning N-F nodes to the pool, ""View change"" does not occur if Primary node is stopped
 INDY-1166 - Unable to recover write consensus at n-f after f+1 descent
 INDY-1183 - Newly upgraded STN fails to accept transactions (pool has been broken after upgrade because of one not upgraded node)
INDY-1190 - Unable to submit upgrade transaction to STN

*Changes and Additions*
 INDY-900, INDY-901 - Add indy-sdk test dependency to plenum and use indy-sdk for plenum tests
 INDY-962 - Publish docker images to dockerhub
 INDY-480 - Simplify view change code
 INDY-878 - We need to re-factor config.py to reflect file folder re-factoring for Incubation
 INDY-628 - Abstract Observers Support
 INDY-1055 - Move scripts from sovrin-environment to one of Indy repo
 INDY-1064 - Get rid of Sovrin dependency in environment scripts
 INDY-1062 - Some mistakes and broken links in ""Getting Started with Indy""
 INDY-1060 - Some mistakes and broken links in ""Setting Up a Test Indy Network in VMs""
 INDY-1087 - Add iptables rules to limit the number of clients connections
 INDY-1088 - Knowledge transfer on Indy build processes
 INDY-837 - Incubation: Move CI part of pipelines to Hyperledger infrastructure
 INDY-582 - As a User, I should revoke a connection by rotating my new key to nothing
 INDY-1022 - Anyone needs to have access to up-to-date Technical overview of plenum and indy

*Known Issues*
 INDY-1163 - Pool has lost consensus after primary demotion (with 4 nodes setup only)
 INDY-1179 - Ambiguous behavior after node demotion
 INDY-1180 - One of the nodes does not respond to libindy after several running load test
INDY-1197 - Pool does not work after not simultaneous manual pool upgrade
INDY-1198 - Pool stopped working if primary node was not included to schedule in upgrade transaction

*Node promoting is not recommended for 1.3.55 version according to known issues because backup protocol instances may work incorrectly until next view change.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwfr:",,,,,,"Sprint 18.03 Stability, DKMS",Sprint 18.04,,,,,,,1.0,,,,,,,,,,,,krw910,ozheregelya,TechWritingWhiz,VladimirWork,,,,,,,,,"17/Feb/18 2:24 AM;krw910;[~VladimirWork] Remember that when this is all completed assign it to [~TechWritingWhiz] and add a due date. ;;;","17/Feb/18 6:27 AM;ozheregelya;[~krw910], which due date should we add?;;;","20/Feb/18 10:02 AM;TechWritingWhiz;The pull request for the updated release notes is here: [https://github.com/sovrin-foundation/sovrin/pull/50]

 ;;;","24/Feb/18 1:37 AM;ozheregelya;[~TechWritingWhiz], 
There was a problem with upgrade STN pool to indy-node=1.3.52 - INDY-1183. The issue was fixed in version 1.3.54. QA have re-tested and approved this RC. INDY-1183 should be added to release notes, so I'll turn this ticket back to you.

[~krw910], [~VladimirWork], [~zhigunenko.dsr], FYI.;;;","01/Mar/18 4:15 AM;ozheregelya;[~TechWritingWhiz],
One more problem with STN upgrade INDY-1190 was fixed in RC 1.3.55. After testing of this RC two more known issues were added: INDY-1197, INDY-1198.

[~krw910], [~VladimirWork], [~zhigunenko.dsr], FYI.;;;","01/Mar/18 8:10 AM;TechWritingWhiz;These things have been updated. The pull request is here: https://github.com/sovrin-foundation/sovrin/pull/52;;;",,,,,,,,,,,,,,,,,,,
test_6th_node_join_after_view_change_by_master_restart failed,INDY-1165,27659,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,13/Feb/18 5:01 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/18 5:01 PM;ashcherbakov;test-result-plenum-3.ubuntu-12.txt.zip;https://jira.hyperledger.org/secure/attachment/14575/test-result-plenum-3.ubuntu-12.txt.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0zr:",,,,,,Sprint 18.04,Sprint 18.05,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to recover write consensus at n-f after f+1 descent,INDY-1166,27662,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,13/Feb/18 6:15 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Build Info:
indy-node 1.3.51

Steps to Reproduce:
1. Install pool of 6 nodes and send NYM to make sure that pool works.
2. Stop 6th node >> read NYM >> write NYM.
3. Stop 5th node >> read NYM >> write NYM.
4. Stop 4th node >> read NYM >> write NYM.
5. Stop 3rd node >> read NYM >> write NYM.
6. Start nodes from 3rd to 6th successively.
7. Try to write NYM.

Actual Results:
Step 7 NYM is not written to ledger so the pool has lost write consensus after descent to f+1 and recovery to >n-f.

Expected Results:
Pool should work write NYMs normally after >n-f.

Workaround:
Restart the whole pool to write NYMs.",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1151,,INDY-1081,,,,,,,,,,,,,,,,,,,,"15/Feb/18 1:55 AM;VladimirWork;INDY-1166-fixed.PNG;https://jira.hyperledger.org/secure/attachment/14594/INDY-1166-fixed.PNG","13/Feb/18 6:32 PM;VladimirWork;INDY-1166.tar.gz;https://jira.hyperledger.org/secure/attachment/14577/INDY-1166.tar.gz","13/Feb/18 6:15 PM;VladimirWork;bad_recovery_after_f+1.PNG;https://jira.hyperledger.org/secure/attachment/14576/bad_recovery_after_f%2B1.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyrbb:",,,,,,"Sprint 18.03 Stability, DKMS",,,,,,,,,,,,,,,,,,,,sergey-shilov,VladimirWork,,,,,,,,,,,"15/Feb/18 1:05 AM;sergey-shilov;*Problem state / reason:*

When returning N-F nodes to a pool, ""View change"" does not occur if Primary node is stopped. This happens due to the fact that a node currently participating in the pool does not schedule a view change if the pool has come back to quorum when a primary is still down.

*Changes:*

Now a node schedules a view change if it transits its state from _starting_ to _started_hungry_ and a primary is set but it is still down. It means that a pool has come back to quorum while primary is still down, so initiating of view change process makes sense.

*Committed into:*

    [https://github.com/hyperledger/indy-plenum/pull/531]
     [https://github.com/hyperledger/indy-plenum/pull/533]
     [https://github.com/hyperledger/indy-node/pull/565]
     indy-node 1.2.308-master

*Risk factors:*

    Some cases of view change may be influences by committed changes, but this is not expected.

*Risk:*

    Medium

*Recommendations for QA:*

Do full testing of view change.
 Also the test case mentioned in description of this ticket should be repeated and expected results should be got.;;;","15/Feb/18 1:55 AM;VladimirWork;Build Info:
indy-node 1.3.308

Steps to Validate:
1. Install pool of 6 nodes and send NYM to make sure that pool works.
2. Stop any 4 nodes of 6 sending read and write requests and then start them in any succession.
3. Try to read\write NYM.

Actual Results:
Pool reaches write consensus at >n-f and doesn't lose read consensus during test run. !INDY-1166-fixed.PNG|thumbnail! 
;;;","15/Feb/18 2:06 AM;VladimirWork;Full testing of view change will be performed in scope of INDY-1151.;;;",,,,,,,,,,,,,,,,,,,,,,
Install indy-crypto independently?,INDY-1167,27668,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,MichaelWang,MichaelWang,13/Feb/18 10:58 PM,13/Feb/18 10:58 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"I install all the dependencies with pip. When I start the node, error arise:

libindy-crypto.so not found. 

As I found this file is not in the right place. 

So I install it manually. And it works. I this is not  good. Why not compile and install this file automatically with this command? Of course install the dependencies firstly, like rust .etc.

 

Is this good?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyx9z:",,,,,,,,,,,,,,,,,,,,,,,,,,MichaelWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
STN not accepting transactions,INDY-1168,27698,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,gudkov,mgbailey,mgbailey,14/Feb/18 10:08 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"The STN is unable to accept new transactions. This has been happening regularly (about every 3 days) for the several weeks, but this time it is different. In the past I have been able to fix this by restarting the 8 nodes that I have access to. This is not restoring the network this time. 
It is possible that this is due to a new validator node that was added to the network on Friday evening by a steward that is testing his node prior to adding it to the live network.
There are 4 of 12 nodes that I do not have access to.  I have attached logs for the nodes I have access to.

One node, ricFlair, is turned off.  Another node, pcValidator01, was added to the network on Friday evening.  All nodes but recFlair show 'connected' using validator-info.

Shortly before capturing the logs I attempted to post a NYM transaction to to the ledger, to provide debugging data:
{code:java}
indy@sandbox> send NYM dest=Fuey3pEhVbhMxgZfk7NkYM verkey=~QwZJsuUN7oGxdEby5H6b78
Adding nym Fuey3pEhVbhMxgZfk7NkYM
{code}
Please determine if this is a known, or a new, unknown failure.",STN running 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/18 9:56 PM;mgbailey;STN_pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/14875/STN_pool_ledger.txt","14/Feb/18 10:08 AM;mgbailey;can-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14583/can-stn-p001.tar","14/Feb/18 10:08 AM;mgbailey;lon-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14588/lon-stn-p001.tar","14/Feb/18 10:08 AM;mgbailey;nva-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14584/nva-stn-p001.tar","14/Feb/18 10:08 AM;mgbailey;ohi-dvn-e001.evernym.lab.tar;https://jira.hyperledger.org/secure/attachment/14582/ohi-dvn-e001.evernym.lab.tar","14/Feb/18 10:08 AM;mgbailey;sao-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14589/sao-stn-p001.tar","14/Feb/18 10:08 AM;mgbailey;seo-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14586/seo-stn-p001.tar","14/Feb/18 10:08 AM;mgbailey;sgp-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14585/sgp-stn-p001.tar","14/Feb/18 10:08 AM;mgbailey;syd-stn-p001.tar;https://jira.hyperledger.org/secure/attachment/14587/syd-stn-p001.tar",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz633:",,,,,,Sprint 18.04,Sprint 18.05,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,EvelynEvergreene,mgbailey,sergey.khoroshavin,,,,,,,,"27/Mar/18 10:18 PM;ashcherbakov;Please find out whether this is a new or a known issue;;;","09/Apr/18 6:30 PM;sergey.khoroshavin;Looking through logs I found that:
- all nodes except ricFlair (which is offline) did send PROPAGATE, which means they are functional in some way
- transaction was not accepted because only 7 nodes sent PREPARE message (at least 8 needed)
- nodes that did not sent PREPARE were australia (which is normal since it's primary), ricFlair (which is offline), brazil, atbsovrin and pcValidator01
- among above mentioned nodes with problems only brazil had logs, and according to them all request messages were pended
- this situation can happen only if there is view change in progress, but its start was not captured in these logs
- atbsovrin and pcValidator01 behaved very similar to brazil, so most probably they are in the same state as brazil

From these logs there is no further analysis possible since start of view change was not captured.;;;","09/Apr/18 11:23 PM;mgbailey;[~sergey.khoroshavin], please share this information with the team working on INDY-1261 to see if the behavior is the same.  In that case, I believe that the logs posted will have included the view change on March 30.;;;","10/Apr/18 1:19 AM;mgbailey;[~sergey.khoroshavin], do you see in the logs that it is trying to contact ricFlair, atbsovrin, and pcValidator01?  It should not be.  These are not currently part of the network.  Here is the output of validator-info -v, which gives the correct list of validators in the network:
{code:java}
alidator australia is running
Current time: Monday, April 9, 2018 4:15:15 PM
Validator DID: UZH61eLH3JokEwjMWQoCMwB3PMD6zRBvG6NCv5yVwXz
Verification Key: 3WKhgs5gCoHv4GP2SDAi4Q7bcFx8R7cEjWX8tnjjKRxJaU3eksm77wP
Node Port: 9701/tcp on 0.0.0.0/0
Client Port: 9702/tcp on 0.0.0.0/0
Metrics:
Uptime: 3 days, 23 hours, 34 minutes, 38 seconds
Total Config Transactions: 28
Total Ledger Transactions: 441
Total Pool Transactions: 29
Read Transactions/Seconds: 0.00
Write Transactions/Seconds: 0.00
Reachable Hosts: 10/10
NewtonD
RFCU
australia
brazil
canada
england
ibm
korea
singapore
virginia
Unreachable Hosts: 0/10
Software Versions:
indy-node: 1.3.55
sovrin: 1.1.7
{code}
The other nodes have all been demoted.

 ;;;","10/Apr/18 1:43 AM;sergey.khoroshavin;[~mgbailey]

{quote}please share this information with the team working on INDY-1261 to see if the behavior is the same{quote}
Dmitry already knows about this

{quote}do you see in the logs that it is trying to contact ricFlair, atbsovrin, and pcValidator01?  It should not be{quote}
Which logs are you referring to? Ones attached to INDY-1261?;;;","10/Apr/18 5:14 AM;mgbailey;[~sergey.khoroshavin], no, the ones for the STN (this ticket).  These nodes are not active as validators on the STN. So I don't understand your comments above about PROPAGATE / PREPARE messages from these nodes.  These nodes should not be sending messages to the STN.;;;","10/Apr/18 6:27 AM;sergey.khoroshavin;[~mgbailey] Very interesting. According to logs attached here these nodes did send PROPAGATE on request and were included as recipients for PREPARE messages, this is why I concluded they are functional. Is it possible that these nodes were active validators at time when these logs were taken and were demoted at some time later?;;;","10/Apr/18 9:57 PM;mgbailey;I'm attaching the pool ledger: [^STN_pool_ledger.txt]

Look for the transactions where ""services"":[], which is where the nodes are removed from the network, and you can check the txnTime when this occurred.;;;","10/Apr/18 10:06 PM;sergey.khoroshavin;[~mgbailey] Looked through transactions, pcValidator01 was demoted at 1521825598, but I don't see anything about ricFlair and atbsovrin, how that could be?;;;","11/Apr/18 4:49 AM;mgbailey;Oh, sorry.  This is an older ticket, I thought it was current.  Back then these nodes were part of the STN.  On Feb 15th the ledger on all nodes was reset and rebuilt from genesis files.  These nodes were dropped from the STN at that point.;;;",,,,,,,,,,,,,,,
"Adding new schema, field 'attr_names' of schema json can be an empty list.",INDY-1169,27227,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,nhan.trong.nguyen,nhan.trong.nguyen,30/Jan/18 11:33 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"When I use libindy to build and submit schema request to ledger, I realize that ledger can accept the schema whose 'attr_names' is an empty list.

Reproduce steps when using libindy.
| |Step|Data|
|1|Create a pool ledger config.
 pool.create_pool_ledger_config(pool_name, pool_config).|pool_name = 'test_pool'
 pool_config = \{'genesis_txn': pool_transactions_sandbox_genesis}|
|2|Open pool ledger and get 'pool_handle'.
 pool.open_pool_ledger(pool_name).|pool_name = 'test_pool'|
|3|Create a wallet.
 wallet.create_wallet(pool_name, wallet_name, None, None, None).|pool_name = 'test_pool'
 wallet_name = 'test_wallet'|
|4|Open wallet and get 'wallet_handle'.
 wallet.open_wallet(wallet_name, None, None).|wallet_name = 'test_wallet'|
|5|Create and store did of default trustee as 'did_default_trustee'.
 signus.create_and_store_my_did(wallet_handle, did_json).|wallet_handle in step 4.
 did_json = '\{'seed': '000000000000000000000000Trustee1'}'.|
|6|Build schema request and store created request as 'schema_req'.
 ledger.build_schema_request(submitter_did, schema_json).|submitter_did = did_default_trustee.
 schema_json = '\{'name': 'test', 'version': '1.0', 'attr_names': []}'|
|7|Submit 'schema_req' to ledger by role steward.
 ledger.sign_and_submit_request(pool_handle, wallet_handle, submitter_did, schema_req).|pool_handle in step 2.
 wallet_handle in step 4.
 submitter_did = did_default_trustee.
 schema_req in step 9.|

Expected result: SDK throws an error when the parameters it receives are incorrect.
 Actual result: ledger accepts the request. 

 

However, when I execute command manually in CLI to submit schema request, the program works well with 'keys' parameter that is not assigned to any value.

Reproduce steps when executing manually:
 # Open command prompt.
 # Execute command 'indy'.
 # Execute command 'connect sandbox'.
 # Execute command 'new key with seed 000000000000000000000000Trustee1'.
 # Execute command 'send SCHEMA name=test version=1.0 keys='.

Expected result: the schema request cannot be sent.

Actual result: 'invalid syntax' message displays and user cannot submit schema request.","OS: ubuntu16.04

Libindy: 1.3.0

Wrapper: python

indy-node: 1.2.50

indy-plenum: 1.2.29

indy-anoncred: 1.0.11

sovrin: 1.1.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/18 11:08 AM;nhan.trong.nguyen;log_automation.log;https://jira.hyperledger.org/secure/attachment/14507/log_automation.log","30/Jan/18 11:08 AM;nhan.trong.nguyen;manual.png;https://jira.hyperledger.org/secure/attachment/14506/manual.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz0yn:",,,,,,Sprint 18.05,,,,,,,,,,,,,,,,,,,,ashcherbakov,gudkov,nhan.trong.nguyen,Toktar,zhigunenko.dsr,,,,,,,,"14/Feb/18 4:30 PM;gudkov;[~nhan.trong.nguyen] [~SeanBohan_Sovrin] [~ashcherbakov]

libindy doesn't perform any complex values validation in sign_and_submit_request. It is responsibility of Indy Node.

This validation seems reasonable, but can be added only on Indy Node level.

[~ashcherbakov] What do you think?

According to second case error message seems correct to me. You just passed incorrect command line and got correct result.;;;","14/Feb/18 7:39 PM;ashcherbakov;Yes, I think it doesn't make sense to send empty attrs list.;;;","14/Feb/18 7:40 PM;ashcherbakov;Moved to INDY;;;","13/Mar/18 8:36 PM;Toktar;Problem reason:
 -  Have not validation of new schema for detect empty field 'attr_names'

Changes:
 *  Added validation of new schema for detect empty field 'attr_names'.

PR:
 - https://github.com/hyperledger/indy-node/pull/599

Version:
 - indy-node 1.3.334-master

Risk factors:
 - Nothing is expected

Risk:
 - Low

Covered with tests:
 - [test_schema.py|https://github.com/hyperledger/indy-node/blob/master/indy_client/test/anon_creds/test_schema.py] - test_can_not_submit_schema_with_empty_attr_names;;;","14/Mar/18 6:13 PM;zhigunenko.dsr;*Environment:*
indy-anoncreds 1.0.32
indy-cli 1.3.1~409
indy-node 1.3.335
indy-plenum 1.2.267
libindy 1.3.1~409

*Steps to reproduce:*
1) Open command prompt.
2) Execute command 'indy'.
3) Execute command 'connect sandbox'.
4) Execute command 'new key with seed 000000000000000000000000Trustee1'.
5) Execute command 'send SCHEMA name=test version=1.0 keys='
6) Execute command 'send SCHEMA name=test version=1.0 keys=,'
*Actual results:*
 - The schema request cannot be sent
 - There is error message about empty list

*Additional information:*
Cases were tested with old and new CLI;;;",,,,,,,,,,,,,,,,,,,,
Move to master libindy,INDY-1170,27706,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,Derashe,Derashe,14/Feb/18 9:23 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,Need to move to master libindy for plenum. At least for tests which use libindy client.,,,,,,,,,,,,,,,,,INDY-1171,INDY-1172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzyr9r:",,,,,,Sprint 18.04,,,,,,,,1.0,,,,,,,,,,,,andkononykhin,ashcherbakov,Derashe,SeanBohan_Sovrin,,,,,,,,,"15/Feb/18 6:50 AM;SeanBohan_Sovrin;hey [~ashcherbakov]  - can you check in with [~Derashe] on what needs to happen here;;;","15/Feb/18 5:18 PM;ashcherbakov;Currently we use a stable version of libindy in Plenum's tests. We need to move to a master version, since master has some features required by Plenum that stable doesn't have, and the next stable is not going to be released soon.;;;","22/Feb/18 3:06 AM;andkononykhin;PoA:
 * set explicitly the same version for libindy python wrapper (python3-indy package) and libindy debian package;;;","22/Feb/18 11:09 PM;andkononykhin;Problem reason:
 - need to use master libindy for both python wrapper and binary debian package

Changes:
 - added explicitly the same version for both libindy packages (pypi and debian)

Committed into:
 - https://github.com/hyperledger/indy-plenum/pull/545
 - https://github.com/hyperledger/indy-node/pull/578

Risk factors:
 - broken CI testing

Risk:
 - Low

Covered with tests:
 - tested manually

Recommendations for QA: do the following sequence of steps
 * for indy-plenum and indy-node
 ** clone  repo
 ** create docker ci image *docker build -t indy-test -f ci/ubuntu.dockerfile ci*
 ** check that all libindy debian package version inside docker container: *docker run --rm -it indy-plenum-test bash -c ""dpkg -l | grep 'libindy '""*
 ** install pypi packages (using virtualenv is recommended): *pip install .[tests]*
 ** check python wrapper version *pip show python3-indy | grep Version* and ensure it's the same as for debian package;;;",,,,,,,,,,,,,,,,,,,,,
Integrate pool_transactions with libindy,INDY-1171,27707,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Derashe,Derashe,14/Feb/18 9:26 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,Integrate pool_transactions with libindy. ,,,,,,,,,,INDY-1170,,,,,,,INDY-1172,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzz10f:",,,,,,Sprint 18.04,Sprint 18.05,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,Derashe,SeanBohan_Sovrin,VladimirWork,,,,,,,,,"15/Feb/18 6:51 AM;SeanBohan_Sovrin;hey [~ashcherbakov]  - can you check in with [~Derashe] on what needs to happen here;;;","15/Feb/18 5:20 PM;ashcherbakov;This is one of the tasks to move Plenum's tests to libindy (instead of Python client).;;;","06/Mar/18 9:46 PM;Derashe;Problem reason:
 - We need to integrate all tests in pool_transaction folder with sdk

Changes:
 - Tests integrated, created additional fixtures and functions

PR:
- https://github.com/hyperledger/indy-plenum/pull/541

Version:
 - master, 262

Risk factors:
 - No

Risk:
 - Low

Covered with tests:
 - No

Recommendations for QA
- ;;;","06/Mar/18 11:43 PM;VladimirWork;Build Info:
indy-plenum master

Steps to Validate:
1. Run `plenum/test/pool_transactions/` locally via pytest.
2. Run `plenum/test/pool_transactions/` tests locally via runner.py.

Actual Results:
Tests are switched to sdk and passes.;;;",,,,,,,,,,,,,,,,,,,,,
Sdk integarion of one tests undone in node_request,INDY-1172,27708,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,Derashe,Derashe,14/Feb/18 9:30 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Sdk integarion for `test_different_ledger_request_interleave.py` test in plenum/test/node_request:

 ",,,,,,,,,,INDY-1170,INDY-1171,,,,,,,,,,,,,,,,,,,,,,,INDY-1019,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzyr9z:",,,,,,Sprint 18.04,,,,,,,,2.0,,,,,,,,,,,,Derashe,SeanBohan_Sovrin,,,,,,,,,,,"15/Feb/18 6:51 AM;SeanBohan_Sovrin;hey [~ashcherbakov]  - can you check in with [~Derashe] on what needs to happen here;;;","26/Feb/18 2:08 PM;Derashe;Problem reason:

We need to integrate tests of node_request folder with libindy client

Changes: 
- tests were integrated and their behaviour stayed the same

little changes to pool_transaction folder: additional ""sdk_"" fixtures and functions which have been used in node_request

PR:

[https://github.com/hyperledger/indy-plenum/pull/525]

Version:

master, 257

Risk factors:

No

Risk:

Low

Covered with tests:

No

Recommendations for QA

the only noticed problem is versioning. Tests require master libindy.;;;",,,,,,,,,,,,,,,,,,,,,,,
A Trustee needs to be able to restart the pool in critical situations,INDY-1173,27709,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Feb/18 9:36 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,GA-0,,,A Trustee needs to be able to send a Request (not transaction) to restart the whole pool at the given time.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-587,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz6xr:",,,,,,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,3.0,,,,,,,,,,,,ashcherbakov,ozheregelya,Toktar,VladimirWork,zhigunenko.dsr,,,,,,,,"19/Mar/18 6:57 PM;Toktar;Input transaction format:
{code:java}
{'reqId': 98262,
'signature': 'cNAkmqSySHTckJg5rhtdyda3z1fQcV6ZVo1rvcd8mKmm7Fn4hnRChebts1ur7rGPrXeF1Q3B9N7PATYzwQNzdZZ',
'protocolVersion': 1,
'identifier': 'M9BJDuS24bqbJNvBRsoGg3',
'operation': {
        'datetime': '2018-03-29T15:38:34.464106+00:00',
        'action': 'start',
        'type': '118'
        }
}{code}
Input transaction example:
{code:java}
{'op': 'REPLY',
'result': {
        'reqId': 98262,
        'isSuccess': True,
        'type': '118',
        'identifier': 'M9BJDuS24bqbJNvBRsoGg3',
        'msg': None
        }
}{code};;;","13/Apr/18 8:29 PM;Toktar;PR: [https://github.com/hyperledger/indy-node/pull/624]
 * Added mechanism of restart command
 * 
Changed the architecture of handler classes. An abstraction has been added for separating handlers for action transactions and writing and reading transactions
 * 
Above the classes restarter and upgrader is created an abstract class to avoid duplication of code.;;;","19/Apr/18 11:56 PM;Toktar;Problem reason:
Trustee can't restart all nodes in the some time.

Changes:

Added mechanism of restart command
Changed the architecture of handler classes. An abstraction has been added for separating handlers for action transactions and writing and reading transactions
Above the classes restarter and upgrader is created an abstract class to avoid duplication of code.
PR:
 * [https://github.com/hyperledger/indy-plenum/pull/582]
 * [https://github.com/hyperledger/indy-node/pull/624]

Version:
 * indy-plenum 1.2.319-master
 * indy-node 1.3.380-master

Risk factors:
 * Problems with upgrade and handling message can be found.

Risk:
 * Medium

Covered with tests:
 * [test_node_control_tool_for_restart.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/pool_restart/test_node_control_tool_for_restart.py]
 * [test_pool_restart.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/pool_restart/test_pool_restart.py]
 * [test_restart_log.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/pool_restart/test_restart_log.py]

Recommendations for QA:
 * POOL_UPGRADE and receiving may not work correctly. They need testing.
 * POOL_RESTART can have a small delay between nodes due to network features.
The documentation for the restart command - https://github.com/hyperledger/indy-node/blob/master/design/pool_restart_txn.md
 * A restart is not a transaction and is not written to the ladder.;;;","25/Apr/18 10:48 PM;zhigunenko.dsr;*Environment:*
indy-plenum=1.2.323
indy-node=1.3.382

*Reason for rejection*
Steps to reproduce:
1) run pool with 4 nodes
2) execute 
{code}
ledger pool-upgrade name=upgrade-383 version=1.3.383 action=start sha256=f284bdc3c1c9e24a494e285cb387c69510f28de51c15bb93179d9c7f28705388 schedule={""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-04-25T13:31:00+00:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-04-25T13:31:00+00:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-04-25T13:31:00+00:00"",""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-04-25T13:31:00+00:00""} force=true
{code}
Actual results:
Nodes are rebooted but still have 1.3.382 version

[Logs|https://drive.google.com/open?id=1ZCgDm-SbBVixxxPhHr1ELU8HfvUmwtjR]

*Additional info:*
These cases are succesful:
* action=start with timestamp in future
* action=start with timestamp in past
* double-sending action=start (pool have restarted in the last sent timestamp)
* action=cancel with same timestamp (restart cancelled)
* action=cancel with wrong timestamp (restart cancelled);;;","26/Apr/18 6:59 AM;Toktar;[~zhigunenko.dsr], It's the old problem with base 58, could be fixed in the last versions. Please, retry this  case again with last version.
  
{code:java}
Apr 25 13:26:23 40b7012e39aa env[720]: indy-plenum : Depends: python3-base58 (= 0.2.5) but 0.2.4 is to be installed {code}
 ;;;","03/May/18 3:57 AM;ozheregelya;TGB member is able to restart the pool. Is it expected behavior?
{code:java}
pool(p1):wallet(wa):did(V4S...e6f):indy> ledger nym did=M2wWfkx2WFEFQgzTSnv9cm role=TGB verkey=~RtCDJNb6qrihuAmSvJyn8w
Nym request has been sent to Ledger.
Metadata:
+---------------------+---------------------+
| Request ID | Transaction time |
+---------------------+---------------------+
| 1525287221943132979 | 2018-05-02 18:53:41 |
+---------------------+---------------------+
Data:
+------------------------+-------------------------+------+
| Did | Verkey | Role |
+------------------------+-------------------------+------+
| M2wWfkx2WFEFQgzTSnv9cm | ~RtCDJNb6qrihuAmSvJyn8w | TGB |
+------------------------+-------------------------+------+
pool(p1):wallet(wa):did(V4S...e6f):indy> did use M2wWfkx2WFEFQgzTSnv9cm
Did ""M2wWfkx2WFEFQgzTSnv9cm"" has been set as active
pool(p1):wallet(wa):did(M2w...9cm):indy> ledger pool-restart action=start datetime=0
Restart pool request has been sent to Ledger.
Metadata:
+------------------------+---------------------+
| Identifier | Request ID |
+------------------------+---------------------+
| M2wWfkx2WFEFQgzTSnv9cm | 1525287235665722374 |
+------------------------+---------------------+
Data:
+--------+----------+
| Action | Datetime |
+--------+----------+
| start | 0 |
+--------+----------+
pool(p1):wallet(wa):did(M2w...9cm):indy> ledger pool-restart action=cancel datetime=0
Restart pool request has been sent to Ledger.
Metadata:
+------------------------+---------------------+
| Identifier | Request ID |
+------------------------+---------------------+
| M2wWfkx2WFEFQgzTSnv9cm | 1525287377625071365 |
+------------------------+---------------------+
Data:
+--------+----------+
| Action | Datetime |
+--------+----------+
| cancel | 0 |
+--------+----------+
{code};;;","03/May/18 5:02 AM;ozheregelya;*Environment:*
 indy-node=1.3.396
 AWS pool of 4 nodes

*Reasons for Rejection:*
 *Case 1:*
 1. Send cancellation of pool-restart with datetime=0 or datetime=2007-05-02T19:35:00.000000+00:00
{code:java}
ledger pool-restart action=cancel datetime=0{code}
 

*Actual Results:*
 Pool is restarted right after sending the command, `cancel` record was not written to the restart_log.

*Expected Results:*
 Pool should not restart if the action is `cancel`, `cancel` record should be written to the restart_log.

*Case 2* (may be it's ok):
 1. Send pool-restart to the future.
 2. Send pool-restart with datetime=0 or to the past.
 3. Send pool-restart to the future.

*Actual Result:*
 `Cancel` was not written for pool-upgrade from step 1.

*Expected Result:*
 First pool-upgrade should be cancelled. ( ? )
 (At least steward will be confused looking into restart_log.);;;","04/May/18 6:25 PM;Toktar;*Problem reason:*
 * `Cancel` was not written for sequence: future restart, now restart, future restart
 * restart node with command for cancel without time
 * TGB should not have permission to restart pool
 * SDK send commands with datetime=""""

*Changes:*
 * removed permission to restart pool by TGB
 * fixed restart with cancel command
 * added opportunity to send restart command with datetime=""""
 * added cancel of previous restart action to restart now

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/684]

*Version:*
 * indy-node 1.3.404-master

*Risk factors:*
 * Problems with restart now can be found.

*Risk:*
 * Low

*Covered with tests:*
 * [test_pool_restart.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/pool_restart/test_pool_restart.py];;;","07/May/18 11:37 PM;zhigunenko.dsr;*Environment:*
indy-anoncreds 1.0.32
indy-cli 1.4.0~21
indy-node 1.3.404
indy-plenum 1.2.347
libindy 1.3.1~505
libindy-crypto 0.4.0
python3-indy-crypto 0.4.0

*Reason for rejection:*
There is problem if _datetime_ omitted

*Additional info:*
These cases are valid:
1) action=start with timestamp in future (also with timezone shifting)
2) action=start with timestamp in past (also with datetime = 0)
3) double-sending action=start (pool have restarted in the last sent timestamp)
4) action=cancel with same timestamp (restart cancelled)
5) action=cancel with wrong timestamp or datetime = 0 (restart cancelled)
6) future restart, now restart, future restart - all cancellations have logged
7) Steward and TGB should not have permission to restart pool;;;","10/May/18 6:42 PM;Toktar;*Problem reason:*
 * SDK send commands with datetime="""", it's invalid message for node

*Changes:*
 * added opportunity to send restart command with datetime="""". Added field type for datetime and its validation.

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/670]
 * [https://github.com/hyperledger/indy-node/pull/688]

*Version:*
 * indy-node 1.3.407-master
 * indy-plenum 1.2.353-master

*Risk factors:*
 * Problems with client messages validation

*Risk:*
 * Low

*Covered with tests:*
 * [test_pool_restart.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/pool_restart/test_pool_restart.py]
 * [test_fail_pool_restart.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/pool_restart/test_fail_pool_restart.py];;;","10/May/18 11:48 PM;VladimirWork;Build Info:
indy-node 1.3.407

Steps to Validate:
1. Send restart command with datetime omitted.

Actual Results:
Pool restarts right after command is sent.

Additional Info:
All cases from https://jira.hyperledger.org/browse/INDY-1173?focusedCommentId=43998&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-43998 are rechecked to avoid regression issues.;;;",,,,,,,,,,,,,,
Design extension of Validator Info tool to provide more information about the current state of the pool,INDY-1174,27710,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,ashcherbakov,ashcherbakov,14/Feb/18 9:40 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,validator-info,,,0,GA-0,,,"We need to think how Validator Info can be extended to provide more info about the health of the pool:
 * Whether we have consensus or not
 * Who is a Primary on each Instance
 * Whether some nodes are blacklisted
 * Whether some nodes are considered as Suspicious
 * What are uncommitted root hashes
 * etc.",,,,,,,,,,,,,,,,,INDY-1175,,,,,,,,,,,,INDY-1184,,,,,INDY-967,INDY-1814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwwhj:",,,,,,Sprint 18.04,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,dsurnin,,,,,,,,,,,"27/Feb/18 11:15 PM;dsurnin;PR with design
https://github.com/hyperledger/indy-node/pull/584;;;",,,,,,,,,,,,,,,,,,,,,,,,
Extend of Validator Info tool to provide more information about the current state of the pool,INDY-1175,27711,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,14/Feb/18 9:41 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,validator-info,,,0,GA-0,,,Extend Validator Info tool to provide more information according to the Design created in the scope of INDY-1174.,,,,,,,,,,INDY-1174,,,,,,,,,,,,,,INDY-1122,,,,,,,,,,INDY-1361,INDY-1362,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz9tb:",,,,,,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,5.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,dsurnin,mgbailey,ozheregelya,zhigunenko.dsr,,,,,,,"27/Mar/18 11:50 PM;ashcherbakov;Let's populate validator-info results from the code, not from the logs.;;;","29/Mar/18 3:51 PM;anikitinDSR;PoA:
Host machine info

Hardware
* HDD free space
* RAM available
* MBs used by node process
* Node's data folder size
Software
* OS version
* pip freeze output
* all indy related packets versions                  {color:#d04437} (Done) {color}
Current datetime and timezone
Configuration

Config
* Content of main config                                {color:#d04437} (Done) (In Additional info) {color}
* Content of network config                            {color:#d04437} (Done)(In Additional info) {color}
* Content of user config                                  {color:#d04437} (Done)(In Additional info) {color}
Content of Genesis Transaction Files
* indy.env                                                      {color:#d04437} (Done)(In Additional info) {color}
* node_control.conf                                        {color:#d04437} (Done)(In Additional info) {color}
* indy-node.service                                        {color:#d04437} (Done)(In Additional info) {color}
* indy-node-control.service                            {color:#d04437} (Done)(In Additional info) {color}
Extraction from logs and ledgers

* grep exception from journalctl                      {color:#d04437}(Done)  (In Additional info){color}
* grep view change, catch up, shutdown, instance change from the last log file    {color:#d04437} (later){color}
* systemctl status indy-node                            {color:#d04437} (Done)(In Additional info) 
* systemctl status indy-node-control                  {color:#d04437} (Done)(In Additional info) 
* Uptime                                                           
* upgrade_log                                                    {color:#d04437} (Done)(In Additional info)  {color}
* last N txns from pool ledger                             {color:#d04437} (Later)(In Additional info) {color}
* last N txns from config ledger                           {color:#d04437} (Later)(In Additional info){color} 
* last N txns from domain ledger                         {color:#d04437} (Later)(In Additional info){color} 
* pool ledger size                                               {color:#d04437} (Done)(moved into Node info) {color}
* config ledger size                                             {color:#d04437} (Done)(moved into Node info) {color}
* domain ledger size                                           {color:#d04437} (Done)(moved into Node info) {color}
Pool info 

* Global pool settings
* read-only mode
* Nodes' connect/disconnect events for the last N min           {color:#d04437} (later){color}
* Ping time to other nodes
* Current quorum values
* N value
* f value
* per action values
* Reachable nodes
* Unreachable nodes
* Blacklisted nodes
* List of Suspicious
* Protocol
* 
* supported client versions
* supported protocol versions
* supported requests versions
Node info

* name
* last N txns from the pool ledger that belong to the current node only
* current mode
* metrics
* view change status
* instance_change msgs
* waiting view_change_done msgs
* catchup status
* each ledger catchup status
* last ledger_status msgs
* waiting consistency_proof msgs
* number of replicas
* foreach replica
* name
* primary
* root hashes
* uncommitted txns count
* uncommitted root hashes
* watermarks
* last_ordered
* last_3pc key
* info about stashed txns
* total number of stashed txns
* number of stashed checkpoints
* min stashed PrePrepare
* min stashed Prepare
* min stashed Commit;;;","23/Apr/18 9:54 PM;dsurnin;Also it would be good to have the number of node restarts according to journalctl;;;","25/Apr/18 6:12 PM;anikitinDSR;link to design:
[validator_info|https://github.com/hyperledger/indy-node/blob/master/design/validator_info.md];;;","03/May/18 10:11 PM;anikitinDSR;Reason:
 - need to extend validator info

Changes:
 - extend validator info with appropriatted design:
https://github.com/hyperledger/indy-node/blob/master/design/validator_info.md

Versions:
indy-node: 1.3.398-master
indy-plenum: 1.2.345-master

For QA;
  Previous comment describe all of implemented parameters which must be dumped in two ways:
    - additional info will dump while node starting. It consist of configs information and etc.
    - general info for usual case.
  Now, if validator-info tools were run without any parameters, than fully information would shown. If run ""validator-info --short"", than information would be in ""old style""
   ;;;","05/May/18 1:55 AM;mgbailey;Please make sure that there are no log-style lines written to the output: i.e. no ""INFO"" or ""DEBUG"" lines.

The --short output should be the default.  It should include all the information that is in the -v output in the current stable.;;;","06/May/18 8:23 PM;ozheregelya;*Environment:*
indy-node 1.3.403

*Reasons for Rejection:*
Case 1:
Following traceback appears at the end of running validator-info:
{code:java}
Traceback (most recent call last):
 File ""/usr/local/bin/validator-info"", line 731, in <module>
 sys.exit(main())
 File ""/usr/local/bin/validator-info"", line 721, in main
 print(get_stats_from_file(file_path, args.verbose, args.json))
 File ""/usr/local/bin/validator-info"", line 533, in get_stats_from_file
 with open(fpath) as f:
TypeError: invalid file: ['/var/lib/indy/sandbox/node7_additional_info.json']{code}
 

Case 2:
Part of data is duplicated. For short version:
{code:java}
Validator Node7 is running
Validator DID: BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW
Verification Key: 4u9hRgKH6daKcEWb6x9zhsM1G6X93VoXSJHppaeJhdivrHWpKvVtUj9
Node Port: None
Client Port: None
Metrics:
 Uptime: 52 minutes, 4 seconds
 Total Ledger Transactions: 4107
 Total Pool Transactions: 25
 Read Transactions/Seconds: 0.00
 Write Transactions/Seconds: 1.30
Reachable Hosts: 25/25
Unreachable Hosts: 0/25{code}
...
{code:java}
Validator unknown is running
Validator DID: unknown
Verification Key: unknown
Node Port: None
Client Port: None
Metrics:
 Uptime: unknown
 Total Ledger Transactions: unknown
 Total Pool Transactions: unknown
 Read Transactions/Seconds: unknown
 Write Transactions/Seconds: unknown
Reachable Hosts: unknown/unknown
Unreachable Hosts: unknown/unknown{code}
Similar for extended version (Software, Pool_info and Node_info sections).

 

Case 3:
Parts of logs are still appear:
{code:java}
2018-05-06 11:00:41,155 | DEBUG | validator-info ( 681) | main | Cmd line arguments: Namespace(basedir='/var/lib/indy/sandbox', json=False, log='/var/log/indy/validator-info.log', short=True, stdlog=False, verbose=False)
2018-05-06 11:00:41,156 | INFO | validator-info ( 719) | main | Reading file /var/lib/indy/sandbox/node7_info.json ...
2018-05-06 11:00:41,157 | DEBUG | validator-info ( 536) | get_stats_from_file | Data {'Hardware': {'HDD_all': '2637 Mbs', 'HDD_used_by_node': '2637 MBs', 'RAM_used_by_node': '514 Mbs', 'RAM_all_free': '7062 Mbs'}, 'Pool_info': {'Reachable_nodes': ['Node1', 'Node10', 'Node11', 'Node12', 'Node13', 'Node14', 'Node15', 'Node16', 'Node17', 'Node18', 'Node19', 'Node2', 'Node20', 'Node21', 'Node22', 'Node23', 'Node24', 'Node25', 'Node3', 'Node4', 'Node5', 'Node6', 'Node7', 'Node8', 'Node9'], 'Unreachable_nodes_count': 0, 'Unreachable_nodes': [], 'f_value': 8, 'Total_nodes_count': 25, 'Reachable_nodes_count': 25, 'Quorums': ""{'checkpoint': Quorum(16), 'propagate_primary': Quorum(9), 'reply': Quorum(9), 'prepare': Quorum(16), 'view_change': Quorum(17), 'bls_signatures': Quorum(17), 'ledger_status_last_3PC': Quorum(9), 'ledger_status': Quorum(16), 'view_change_done': Quorum(17), 'election': Quorum(17), 'f': 8, 'commit': Quorum(17), 'timestamp': Quorum(9), 'observer_data': Quorum(9), 'consistency_proof': Quorum(9), 'propagate': Quorum(9), 'same_consistency_proof': Quorum(9)}"", 'Blacklisted_nodes': [], 'Suspicious_nodes': '', 'Read_only': False}, 'Software': {'indy-node': '1.3.403', 'sovrin': '1.1.51', 'Indy_packages': ['hi indy-anoncreds 1.0.32 amd64 Anonymous credentials', 'hi indy-node 1.3.403 amd64 Indy node', 'hi indy-plenum 1.2.347 amd64 Plenum Byzantine Fault Tolerant Protocol', 'ii libindy-crypto 0.4.0 amd64 This is the shared crypto libirary for Hyperledger Indy components.', 'hi python3-indy-crypto 0.4.0 amd64 This is the official wrapper for Hyperledger Indy Crypto library (https://www.hyperledger.org/projects).', ''], 'Installed_packages': ['semver 2.7.9', 'sovrin 1.1.51', 'rlp 0.5.1', 'indy-node 1.3.403', 'Pygments 2.2.0', 'base58 1.0.0', 'ioflo 1.5.4', 'orderedset 2.0', 'sha3 0.2.1', 'setuptools 38.5.2', 'sortedcontainers 1.5.7', 'indy-plenum 1.2.347', 'timeout-decorator 0.4.0', 'python-dateutil 2.6.1', 'python-rocksdb 0.6.9', 'Charm-Crypto 0.0.0', 'pyzmq 17.0.0', 'indy-crypto 0.4.0-13', 'six 1.11.0', 'indy-anoncreds 1.0.32', 'jsonpickle 0.9.6', 'portalocker 0.5.7', 'psutil 5.4.3', 'libnacl 1.6.1', 'intervaltree 2.1.0'], 'OS_version': 'Linux-4.4.0-1055-aws-x86_64-with-Ubuntu-16.04-xenial'}, 'Protocol': {}, 'timestamp': 1525604416, 'response-version': '0.0.1', 'Node_info': {'Uncommitted_root_hashes': {}, 'Client_protocol': 'tcp', 'did': 'BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW', 'Metrics': {'Delta': 0.4, 'avg backup throughput': 2.1788303154986712, 'instances started': [1028187.47344777, 1028187.47366069, 1028187.473844688, 1028187.474016361, 1028187.474250274, 1028187.474429558, 1028187.47460209, 1028187.474768239, 1028187.474990024], 'master throughput': 2.2095107651995254, 'master throughput ratio': 1.0140811560600267, 'throughput': {'6': 2.1025905671761995, '8': 2.104306227919157, '3': 2.1063077814353015, '0': 2.2095107651995254, '2': 2.1165528660512036, '7': 2.122879826456821, '5': 2.114060890417075, '1': 2.646670703327902, '4': 2.2137136440574436}, 'average-per-second': {'write-transactions': 1.2979785090990041, 'read-transactions': 0.0}, 'ordered request durations': {'6': 1928.5732863559388, '8': 1927.000902340049, '3': 1925.1697381266858, '0': 1835.2479036841542, '2': 1915.8510354458122, '7': 1910.1410967609845, '5': 1918.109368741978, '1': 1532.1135322581977, '4': 1831.7635665684938}, 'Lambda': 60, 'master request latencies': {}, 'ordered request counts': {'6': 4055, '8': 4055, '3': 4055, '0': 4055, '2': 4055, '7': 4055, '5': 4055, '1': 4055, '4': 4055}, 'Node7 Monitor metrics:': None, 'client avg request latencies': [{'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.45258887883702936]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.3778331768824158]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.4724663465957613]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.47476442370571786]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.45172960951134244]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.4730232721928429]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.4756037697548553]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.47105822361553257]}, {'V4SGRU86Z58d6TV7PBUe6f': [4055, 0.47521600550925985]}], 'total requests': 4055, 'uptime': 3124, 'transaction-count': {'config': 0, 'pool': 25, 'ledger': 4107}, 'Omega': 5}, 'Count_of_replicas': 9, 'Name': 'Node7', 'Config_ledger_size': 0, 'Uncommitted_txns': {'2': [], '0': [], '1': []}, 'View_change_status': {'VCDone_queue': {'Node24': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node15': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node14': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node11': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node5': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node10': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node7': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node16': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node12': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node4': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node21': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node3': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node6': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node8': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node1': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node22': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node19': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node23': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node25': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node13': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node2': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node17': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node9': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node20': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]], 'Node18': ['Node1', [[0, 25, '6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'], [1, 52, 'GLQfx465ri7ynv6vkbenQpvDD97kzPeC7aAnNcrFnsGQ'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]]}, 'IC_queue': {}, 'View_No': 0, 'VC_in_progress': False}, 'Node_port': 9713, 'Catchup_status': {'Received_LedgerStatus': '', 'Last_txn_3PC_keys': {'2': {'Node24': [None, None], 'Node6': [None, None], 'Node15': [None, None], 'Node11': [None, None], 'Node10': [None, None], 'Node25': [None, None], 'Node13': [None, None], 'Node2': [None, None], 'Node23': [None, None], 'Node17': [None, None], 'Node16': [None, None], 'Node9': [None, None], 'Node4': [None, None], 'Node21': [None, None], 'Node3': [None, None]}, '0': {'Node13': [None, None], 'Node17': [None, None], 'Node16': [None, None], 'Node9': [None, None], 'Node4': [None, None], 'Node1': [None, None], 'Node3': [None, None], 'Node25': [None, None]}, '1': {'Node13': [None, None], 'Node17': [None, None], 'Node16': [None, None], 'Node9': [None, None], 'Node4': [None, None], 'Node3': [None, None], 'Node25': [None, None]}}, 'Number_txns_in_catchup': {'2': 0, '0': 0, '1': 0}, 'Waiting_consistency_proof_msgs': {'2': None, '0': None, '1': None}, 'Ledger_statuses': {'2': 'synced', '0': 'synced', '1': 'synced'}}, 'Root_hashes': {'2': ""b'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn'"", '0': ""b'6YgcxFrcQpEXLdUHjNU7id36SMUCid4XmyjJXm3aJkSy'"", '1': ""b'7M1gNibf2gYPVLxBNsPvaBF1rAzB3j4mMK1CjFYx6eRm'""}, 'Pool_ledger_size': 25, 'Client_port': 9714, 'verkey': '4u9hRgKH6daKcEWb6x9zhsM1G6X93VoXSJHppaeJhdivrHWpKvVtUj9', 'Domain_ledger_size': 4107, 'Replicas_status': {'Node7:0': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node1:0'}, 'Node7:5': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node6:5'}, 'Node7:2': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node3:2'}, 'Node7:7': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node8:7'}, 'Node7:8': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node9:8'}, 'Node7:1': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node2:1'}, 'Node7:4': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node5:4'}, 'Node7:3': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node4:3'}, 'Node7:6': {'Watermarks': '4000:4300', 'Last_ordered_3PC': [0, 4055], 'Stashed_txns': {'Stashed_checkoints': 0}, 'Primary': 'Node7:6'}}, 'Mode': 'participating', 'Node_protocol': 'tcp'}}
{code}
[~mgbailey], we have separated ticket for this problem: INDY-1122. Should we put it to current or next sprint, or fix the problem in scope of this ticket?

 

*Enhancements:*
1. Need to add something like 'n/a' if the section is empty. Example:
{code:java}
""Total_nodes_count"": 25 
""Reachable_nodes_count"": 25 
""Unreachable_nodes"": 
    ""f_value"": 8 
""Suspicious_nodes"": 
  ""Read_only"": False 
""Unreachable_nodes_count"": 0 
{code}
It's not obvious that there is no ""Unreachable_nodes"", also, hierarchy (""f_value"" inside the ""Unreachable_nodes"") looks strange.

2. Some information, like configs and genesis txns looks excess. 
May be it make sense to remove it from default validator-info output and put into the --extended mod, or to separated parameters (–configs, --genesis)?

 ;;;","09/May/18 1:20 AM;ozheregelya;Case 4:

Following errors appear every minute in node logs:
{code:java}
2018-05-08 15:56:31,670 | ERROR | validator_info_tool.py ( 565) | dump_general_info | Error while dumping into json: TypeError('string indices must be integers',){code}

Steps to reproduce are unclear.;;;","09/May/18 10:08 PM;ozheregelya;Case 5:
On some nodes validator-info doesn't work at all with following error:
{code:java}
2018-05-09 13:02:15,974 | DEBUG | validator-info ( 681) | main | Cmd line arguments: Namespace(basedir='/var/lib/indy/sandbox', json=False, log='/var/log/indy/validator-info.log', short=False, stdlog=False, verbose=False)
2018-05-09 13:02:15,975 | INFO | validator-info ( 719) | main | Reading file /var/lib/indy/sandbox/node15_info.json ...
Traceback (most recent call last):
 File ""/usr/local/bin/validator-info"", line 731, in <module>
 sys.exit(main())
 File ""/usr/local/bin/validator-info"", line 724, in main
 print(""{}"".format(os.linesep).join(create_print_tree(json.load(f_stats))))
 File ""/usr/lib/python3.5/json/__init__.py"", line 268, in load
 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)
 File ""/usr/lib/python3.5/json/__init__.py"", line 319, in loads
 return _default_decoder.decode(s)
 File ""/usr/lib/python3.5/json/decoder.py"", line 339, in decode
 obj, end = self.raw_decode(s, idx=_w(s, 0).end())
 File ""/usr/lib/python3.5/json/decoder.py"", line 357, in raw_decode
 raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0){code}
Sometimes it shows only configs content, without Pool Info section.;;;","14/May/18 7:42 PM;anikitinDSR;Fixed issue in previous comment.
New version is:
indy-node 1.3.412
indy-plenum 1.2.357;;;","15/May/18 12:05 AM;zhigunenko.dsr;*Environment:*
indy-node 1.3.412
indy-plenum 1.2.357

Actual results:
1) NodeX.log ( LogLevel=0)
{code}
2018-05-14 11:47:26,613 | DEBUG    | validator_info_tool.py (  31) | wrap | Validator info tool fails to execute __sovrin_pkg_version because ImportError(""No module named 'sovrin'"",)
{code}

2) _validator-info_ without options shows 
{code}
Node Port:        None
Client Port:      None
{code}
despite content of _indy.env_

3) some empty fields in the end
{code}
""Extractions"": 
     ""node-control status"": 
                     
     ""indy-node_status"": 
                     
     ""upgrade_log"": 
                     
     ""journalctl_exceptions"": 
                     
     ""stops_stat"":   None 
{code}

4) _""Pool_ledger_size"":  25_ shown twice
;;;","15/May/18 2:16 AM;mgbailey;Please also check all flags for this command for proper functionality, especially --json, which should produce valid JSON for the output.;;;","22/May/18 3:58 PM;anikitinDSR;Versions:
indy-node: 1.3.421-master
indy-plenum: 1.2.367;;;","23/May/18 9:24 PM;anikitinDSR;Versions:
indy-node: 1.3.424
indy-plenum: 1.2.368;;;","24/May/18 12:15 AM;zhigunenko.dsr;*Environment:*
indy-node: 1.3.424
indy-plenum: 1.2.368

*Actial results:*
* no parameters - correct output
* --json - output is valid JSON
* -v - correct output (except mock ""protocol"" section and some notices below)

*Additional info:*
* ""node-control status"": and ""indy-node_status"": may be empty while using Docker
* ""stops_stat"": and ""journalctl_exceptions"": are unavailable now, it's requires some changes in installation process (INDY-1361)
* --log option isn't working (INDY-1362);;;",,,,,,,,,,
Node logs must have no sensentive information,INDY-1176,27712,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Feb/18 9:43 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Node logs need to be shared and public.

So, we need to make sure that they don't have any sensitive information (private keys, seeds, etc.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzyr8v:",,,,,,Sprint 18.04,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,dsurnin,VladimirWork,,,,,,,,,,"21/Feb/18 11:41 PM;dsurnin;I think the best way to test it is to analyze logs from the pool after acceptance testing (with debug level logging);;;","28/Feb/18 11:04 PM;VladimirWork;Build Info:
indy-node 1.3.51~1.3.55 rc

Steps to Validate:
1. Run acceptance scenarios.
2. Check logs for sensitive information.

Actual Results:
There are no client/node private keys or seeds in DEBUG logs.;;;",,,,,,,,,,,,,,,,,,,,,,,
[Design] Dev team needs to be able to get logs from live pool without communication with Stewards,INDY-1177,27713,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,ashcherbakov,ashcherbakov,14/Feb/18 9:51 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"Once INDY-1176 is done, we should be able to get Node Log files without communication with Stewards.

Let's design and implement how we can do it easily without sophisticated Sovrin Dashboard and UI.",,,,,,,,,,,,,,,,,INDY-1240,,,,,,,,,,,,INDY-484,INDY-1526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz62f:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,danielhardman,devin-fisher,dsurnin,gudkov,mgbailey,nage,SeanBohan_Sovrin,sergey.khoroshavin,tharmon,,,"30/Mar/18 4:37 AM;mgbailey;Who should have access to these logs? I don't think that the general public should, because it might inadvertently provide data that could be used by a bad actor to find an exploit, but I think that stewards should all have access.  When it comes down to it, all stewards are peers, and we have no more right to this information than any other steward;;;","30/Mar/18 7:37 PM;sergey.khoroshavin;We have several problems now which need to be solved, and hardest one is that devs from indy core team don't have access to logs from live pool. Without this information it's impossible to debug issues that may arise in production. I understand concerns about exposing logs to general public, so I believe that there should be some form of authorization. The main point is that authorized team members should be able to get these logs as soon as possible, without human intervention.

Other problems are that there's a lot of information in logs (for example a few hours of load on 25 nodes currently generates ~100Gb of raw text data with some spikes of almost 1 Gb/minute), which needs to be stored and indexed somehow, so that queries on this data don't run for ages. I've already made a small tool which tries to solve problem of extracting useful data from logs, but as it grows in capabilities it already starts to resemble some of ELK tools, and I think that moving to ELK stack is the most cost-efficient solution for our monitoring and debugging problems.

I have three propositions on architecture to choose from:
1. One central ELK pool hosted by Sovrin, where logs from all Stewards are gathered. 
Pros:
  - one data source to run queries, simplifying debugging node communication issues
  - trivial to setup dashboards showing health of entire pool
  - guaranteed isolation from validator nodes
  - relatively simple to setup from technical point of view
  - minimal intervention needed from Stewards (just install filebeat and point it to ELK)

Cons:
  - needs to be hosted by Sovrin
  - is centralized (but I don't think this is a real problem as it just for logs and health monitoring and doesn't affect consensus process)

2. ELK pools hosted by Stewards, containing their own logs.
Pros:
  - decentralized

Cons:
  - devs need to get authorization from each steward to get logs
  - more complex for Stewards to setup (and more chances for mishaps)
  - no easy way to run queries concerning multiple nodes
  - no easy way to setup a dashboard showing health of entire pool

3. ELK pools hosted by Stewards, each containing logs from all Stewards
Pros:
  - decentralized
  - queries concerning multiple nodes possible
  - trivial to setup dashboards showing health of entire pool (from point of view of each Steward!)
  - devs can get all logs from any Steward reducing problems with authorization

Cons:
  - even more complex for Stewards to setup
  - massive data duplication

I want to stress that none of the proposed solutions require changes to existing plenum code nor removes simple logs files as currently written by nodes, so even if we go with centralized solution and something bad happens it won't affect request processing, and no valuable data (including logs) will be lost.;;;","31/Mar/18 6:01 AM;tharmon;I'm having a hard time with this ticket/requirement in general. Here are a few thoughts around it. First off, there are some statements that are being taken to be givens that I don't necessarily agree with.

{quote}We have several problems [...] one is that devs from indy core team don't have access to logs from live pool.{quote}

This was a design tradeoff from the very beginning. My belief is that no dev should believe they will have access to any or all of the logs from the live pool. That is a centralized administration mindset. Even if you wanted to try and enforce this, Stewards are free agents and under existing trust frameworks are not compelled to participate, so getting all logs is unlikely.

{quote}Without this information it's impossible to debug issues that may arise in production.{quote}

Difficult, yes. Impossible, no. That's a very strong word. I would like to see if we can come up with some more ingenious ways to handle this particular problem. Don't get me wrong, I fully understand the difficulty of what I'm suggesting here.

{quote}The main point is that authorized team members should be able to get these logs as soon as possible, without human intervention.{quote}

I believe there is a strong desire for this from the development point of view, but I'm not convinced it's okay. There may very well need to be some human intervention. In short, I question the very premise of this ticket.

I also have a problem with the apparent amount of outbound traffic these suggestions would create from any given node. Under load, the transmission pipes are already going to be quite full. I'm not particularly interested in filling them even more with what appears to be quite a bit of data (1Gb/minute uncompressed was mentioned), especially if it's just a convenience thing.

I suspect this particular view will likely cause some consternation and some follow-on conversations.

[~sergey.khoroshavin] : [~mgbailey] : [~danielhardman];;;","31/Mar/18 8:37 AM;mgbailey;[~tharmon] These are valid concerns that caused us to mothball the system for log aggregation that we had in place when we promoted the network from 'alpha' to 'provisional'.  In view of our experiences in debugging the provisional network, maybe we should revisit this.  First I will describe what we did then:

We set up a VM (aggregator) whose only purpose was to store logs from the nodes.  We then provided to the stewards a script that uses rsync to push the logs and the transaction count to the aggregator. This was configured with a ssl cert with narrowly defined write access to a config directory on the aggregator.  We also provided instructions for setting up a cron job to trigger the script periodically.  If we were to do this today, I would have it give the output of validator-info instead of just the transaction count.

Advantages of this approach is that it is entirely separate from indy & sovrin.  It is something that stewards can opt to do so that we don't have to bug them for logs, or they can choose not to and we will continue to harass as needed.  We can also provide read-only credentials to all the stewards who want to see the logs themselves.

It is simple and provides the autonomy that the stewards need, and we have used it successfully in the past.  The same reasoning regarding the cleanliness of the logs can be used to justify this approach, as with the other approach that is discussed here.;;;","01/Apr/18 8:06 AM;danielhardman;It is not acceptable to build any solution to this problem that violates the following principle:

...Developers should have access to no more data about the running system than the general public, UNLESS the stewards give them that data or the developers are stewards themselves...

Period. This is not negotiable; it is built into the trust framework for the network itself. Doing otherwise makes the system centralized and gives developers–who haven't signed any legal agreements with anybody in the trust framework--a back door. Ain't gonna happen.

I don't think this means that the title of this ticket is invalid, but it is close to crossing the line--we have to be very, very careful how we accomplish the goal.

It might be acceptable for stewards to publish some log data that they are willing to share with the whole world. Such data could be published and indexed automatically, and it could be useful for developer debugging. But we cannot put authorization around it. It must be world visible, and not dangerous in the hands of hackers. If we think we can accomplish our goal by doing this, by all means let's do it.

We can publish metrics and alerts to the world (but not just to developers). For example, there is nothing that prevents us from writing code in the network that puts a message in SQS announcing a view change or a troubled catchup. But anybody in the world would have to be allowed to subscribe to it.

We can add commands to the network that anybody in the world, not just developers, can run, to find out more about what might be going wrong.

We can do elaborate things to capture specific kinds of error conditions and dump them to disk–and then beg stewards to send such things to us to help us debug. But we can't get private data without going through stewards.

We could become stewards ourselves. Whatever happened to the conversation that Anatoli and Alex started, about DSR becoming a steward? There is no reason why stewards can't be developers. They'd then have access to their own box.

If we can't figure out a way to understand the behavior of the network without logs, then I claim we have a more fundamental problem than just lack of monitoring. We have relied on a log crutch when we should be building visibility features in, and testing in such a way, that we truly understand what's going on without needing privileged access.

 

 ;;;","02/Apr/18 2:04 AM;sergey.khoroshavin;Okay, I see this is both political and technical issue, and I was mostly thinking about technical side.
 First of all, thanks for making this clear
{quote}Developers should have access to no more data about the running system than the general public, UNLESS the stewards give them that data or the developers are stewards themselves
{quote}
So, as I see there are just two options from political point of view:
 1. Make logs public, so everyone can see what happens on live pool. Since https://jira.hyperledger.org/browse/INDY-1176 is done I assume that there is no sensitive information in logs, and the only concern is that too much information increases surface of possible attack. While this is a valid concern there is also another one: hiding what happens in the pool looks like security through obscurity, which is often considered not good in the long run. Also, one possible option is to have realtime logging solution, but for general public (including devs) make it lag for an hour or even a day. That way attacks exploiting timings will be much harder, but developers will still get access to full logs eventually. 
 2. Make developers stewards themselves, but I'm afraid this is completely out of scope of my competence, we need Alex response here.

There is also technical side of this issue. Let me try to explain what is happening now and why I think ELK (or something like ELK) is the way to go:
 1. During stress testing our test pools generate about 100 Gb of logs per session, and they could generate more if there was enough space. As part of https://jira.hyperledger.org/browse/INDY-1225 I'm also implementing log compression during rotation, so when this work will be done there will be even more data.
 2. In order to process these logs I proposed to write a simple log processor, now it's already in indy-plenum master, sort of grew and helped to find some issues. The only problem is that it's quite slow to process these amounts of data, that's why after a talk with Alex I asked Trev for CPU-packed spot instace.
 3. In order to accelerate processing one possible thing to do is to reduce amount of data logged, and there was already some work done in that area ([https://github.com/hyperledger/indy-plenum/pull/592]), but reduction is just 1.5-2-fold. Further reduction could be done in future after thorough analysis, but not now. For example, logging this much information helped us to identify that during one load test session one of nodes started effectively DoSing other nodes for no particular reason which decreased processing throughput to a halt (I'm talking about that 1 Gb of logs per minute episode, and it's clearly not a normal situation we expect in healthy pool).
 4. Other option to accelerate processing is indexing. It can be implemented as part of this log processor utility (making it even more complex), or some existing solution could be used leaving log processor relatively simple and suitable for simpler cases. Both me and Alex think that using existing solution (like ELK stack) is preferable.
 5. Now if we start using ELK stack for testing why not make this solution suitable for live pool? Filebeat doesn't listen on some network port, it just watches files and sends data over mutually authenticated and encrypted channel to logstash, so I believe this is no more complex or dangerous than rsync script mentioned by Mike. Also while bringing up central (publicly available) elasticsearch pool is more complex than simple VM for storing logs semantically it's not that different and can provide additional benefits.

So, what I'm proposing now is to make PoC of ELK-based logging solution, then apply it to testing pools, and after resolving political issues one or another way apply it to live pool.;;;","02/Apr/18 2:44 AM;sergey.khoroshavin;Also, I want to clarify some more things:
{quote}Difficult, yes. Impossible, no. That's a very strong word. I would like to see if we can come up with some more ingenious ways to handle this particular problem. Don't get me wrong, I fully understand the difficulty of what I'm suggesting here.
{quote}
Probably I've used wrong words. I mean that when we don't have any logs we can only try to make guess what happened, try to reproduce situation in tests, fix that and only hope that it was indeed the same situation that happened in live pool. Almost no way to prove that issue is really fixed.
{quote}I also have a problem with the apparent amount of outbound traffic these suggestions would create from any given node. Under load, the transmission pipes are already going to be quite full. I'm not particularly interested in filling them even more with what appears to be quite a bit of data (1Gb/minute uncompressed was mentioned), especially if it's just a convenience thing.
{quote}
As I stated above this was clearly not a normal situation, and amount of logs written was not the biggest problem. Also, there was some work to reduce amount of things to log, and I believe it could continued in the future.
{quote}We set up a VM (aggregator) whose only purpose was to store logs from the nodes.  ...

Advantages of this approach is that it is entirely separate from indy & sovrin.
{quote}
Who hosted that VM? I thought it was Sovrin, so it's hard to understand how it can be separate from it. If it wasn't Sovrin and that was ok next question is can this enitity host elasticsearch pool?
{quote}It must be world visible, and not dangerous in the hands of hackers. If we think we can accomplish our goal by doing this, by all means let's do it.
{quote}
To be honest personally I like this solution the most.
{quote}If we can't figure out a way to understand the behavior of the network without logs, then I claim we have a more fundamental problem than just lack of monitoring.
{quote}
It's only one month passed since I joined this project so my opinion could be totally wrong, but I have a feeling that we have too many integration tests and too little truly unit ones. This has led to sometimes quite convoluted code that's hard to reason about (and hard to write unit tests). But again - this is just my feeling.

 ;;;","03/Apr/18 7:51 PM;gudkov;[~danielhardman] [~tharmon] [~ashcherbakov] [~nage] [~SeanBohan_Sovrin]

What is about the following approach:

- Node can support custom logger configuration. It can write logs locally and also optionally support uploading logs to ELK instance provided in config
- Logs will contain public data only. At least logs that will be transferred to ELK
- Sovrin will host ELK instance for logging
- Stewards can optionally configure Nodes to post logs to Sovrin's ELK instance by some support agreement
- Access to Sovrin's ELK instance will be limited to Sovrin's developers team

This solution doesn't break any decentralization assumptions (as logging is optional) and for me it corresponds to what we do now: We ask stewards for logs and some stewards send logs to us. But with automation of this process.

Implementation of really public decentralized logging infrastructure doesn't look reasonable for now for the following reasons:
- It will be hard (impossible) to implement before TDE
- It simplifies discovery of approaches to hack our system in a short perspective
- It will reduce our stability and hacking-resistance in a short perspective

I understand that it looks as moving a bit out of ""ideal"" way, but we need some solution to our real problem:
- We have limited time and resources before TDE
- Our stability and performance require to be better
- Our reaction time to pool problems is too big now for TDE needs

For now this solution looks like the only feasible way to solve this. It significantly decreases risk of big pool problems after TDE. So it is a more question to Product team.;;;","03/Apr/18 11:59 PM;tharmon;At this time, I'm opposed to live streaming logs to an ELK instance because of the overhead it creates and my feeling that we are too close to the line with the trust framework. I'm opposed to all forms of enforced automation of this for similar reassons. As noted, we are trying to address some real problems.

{quote}We have limited time and resources before TDE{quote}

That certainly needs to be discussed.

{quote}Our stability and performance require to be better{quote}

That's independent of this ticket, to some extent. I know we _want_ this information to help with this, and that it would be helpful. But, stability and performance improvements does not have this as a prerequisite.

{quote}Our reaction time to pool problems is too big now for TDE needs{quote}

This needs to be addressed both from a technical and a political point of view. This ticket feels like we are trying to create a purely technical solution.

{quote}So it is a more question to Product team.{quote}

No, this is a question for the Technical Governance Board.
^^ [~danielhardman];;;","04/Apr/18 12:09 AM;gudkov;>  I'm opposed to live streaming logs to an ELK instance because of the overhead it creates

I am not sure that this overhead will be really visible especially if we limit log level;;;","10/Apr/18 10:11 PM;SeanBohan_Sovrin;[~nage] [~danielhardman] - can you weigh in on this;;;","11/Apr/18 12:59 AM;danielhardman;Slava's proposal could be entertained except for his last bullet, ""Access to Sovrin's ELK instance will be limited to Sovrin's developers team"". This is unacceptable, even if logging is optional. Let me explain why.

Sovrin's solution for identity is supposed to be decentralized, permissioned, and public. There are other blockchain-based solutions to identity, such as uPort and Blockstack. We have differentiated from them by saying that we can get better scale/performance/security by permissioning which nodes are running. Their critique of us is, ""Why should the world trust a few Sovrin stewards who claim they are being honest and noble with blockchain data?"" Our response is, ""Everything that the stewards do is transparent. They conduct their meetings in public, the upgrades that they schedule are done in public, the code they run is public...""

What Slava is proposing here would undermine that story. Stewards would have some specially privileged friends - Sovrin developers - that get to see more about what's going on in the system than the public itself. It doesn't make any difference that this is an optional system; we'd be trying hard to convince stewards to use it, and we'd be depending on that system to help us troubleshoot. And we'd be telling the world that this isn't a security risk, because Sovrin developers are trustworthy and wouldn't manipulate the system for their own purposes.

This is essentially centralization. We are centralizing the coding and troubleshooting of the system, and the knowledge about what is happening inside the system. And we are explicitly not being transparent; that's what ""access would be limited"" means. Critics of Sovrin from the permissionless world would be right to doubt the trustworthiness and transparency of such a network. There is a reason that a role with special access to the system internals isn't a construct supported by the Trust Framework; it's because it's not trustable. Any solutions that have this characteristic will be vetoed by the TGB if they get that far.

I think the reason we keep proposing variations on this idea is that we don't believe, in our heart of hearts, that it's going to be safe for the general public to know what the system is doing internally. Even if logs are scrubbed of especially sensitive items, it will be too easy for hackers to use such knowledge to refine their attempts at a DDoS attack.

So let's make it so we can troubleshoot without logs...

The amount of struggle we're having on this ticket has convinced me that we have a more fundamental problem than how to handle logs intelligently. The problem is this:

        Logging is our only visibility mechanism.

The majority of questions we need to resolve during troubleshooting center on state transitions: when did a view change happen, and why? Is catchup happening as we expected? And so forth. We observe these events in logs that contain thousands or millions of other events. So now we're dreaming up a mechanism to stream these logs somewhere, and index them so we can find the few key pieces of info that are most relevant to troubleshooting.

Maybe what we need instead is alerting (NOT logging) for the 0.01% of events that are especially helpful for troubleshooting. Maybe we should build a system where any steward can do specialized troubleshooting, but developers cannot–and then try much harder to make DSR a steward. Maybe we should build a system where we can ask the system at any time when its last view change or catchup happened, and what its status is, and what triggered it–but only if you're a steward or trustee? 

If that isn't something we can afford to build right now, then we can entertain more pragmatic ideas, such as Slava's idea minus special access for developers. But those pragmatic workarounds should not be confused with the right answer, and they cannot sacrifice trust. There is no point in building the system in the first place, if we give up trust.;;;","11/Apr/18 1:57 AM;devin-fisher;Do we need logs from multiple nodes in the pool for them to be useful to Indy development or Sovrin DevOps?  If not, evernym is a steward and as a steward, we should own and control the logs from our node. Surely we could send them to ELK stack for our own operational needs without breaking trust.

Of course, having only one node logs will not be as helpful and all the node's logs. But using just Evernym's logs could it be a middle ground that is pragmatic and fits the trust framework.

So is one node's log good enough?;;;","11/Apr/18 12:37 PM;danielhardman;What if we supported troubleshooting via transactions, including stuff like the following:
 * A new param to any existing transaction, called ""enable_debug"", that would cause the transaction to be returned with a lot of debug info that would help diagnose how it was handled by the pool.
 * A new read-only transaction, DEBUG_CONSENSUS, that would specifically test consensus and report what was learned.

We could make these things usable by anybody, or we could make them usable by stewards only. I don't think that would violate our principles of decentralization.

We could also change the behavior of the nodes such that, any time any of them requests a view change or begin a catchup, they automatically go into debug mode for 5 minutes, then revert back.;;;","11/Apr/18 11:52 PM;gudkov;[~danielhardman]

> A new param to any existing transaction, called ""enable_debug"", that would cause the transaction to be returned with a lot of debug info that would help diagnose how it was handled by the pool.

The main our problems don't directly related to transactions. They are related to ""view change"" and ""catch up"" processes corner cases. Not sure that info we got during transaction execution can help a lot.

> A new read-only transaction, DEBUG_CONSENSUS, that would specifically test consensus and report what was learned.

We plan to implement VALIDATOR_INFO transaction that behaves very similar. Take a look to https://jira.hyperledger.org/browse/INDY-1184 Also interesting ticket is https://jira.hyperledger.org/browse/INDY-1175

> If that isn't something we can afford to build right now, then we can entertain more pragmatic ideas, such as Slava's idea minus special access for developers. But those pragmatic workarounds should not be confused with the right answer, and they cannot sacrifice trust. There is no point in building the system in the first place, if we give up trust.

I believe we need to do this. It can help a lot in stability goal right now. In the future we can replace this with more elegant solution.;;;","12/Apr/18 10:19 PM;nage;There are a few steps here to avoid a centralization issue:
1) add an API for retrieving logs with the right permission settings
2) create the ELK or similar process to aggregate the data and report it on Sovrin.org
3) publish that code so that any entity with the right permissions can build their own versions of that site;;;","13/Apr/18 2:11 AM;sergey.khoroshavin;[~nage]
{quote}1) add an API for retrieving logs with the right permission settings{quote}
Do you require this to be some transaction implemented by validator node or it can be arbitrary solution like some (possibly replicated) service with REST API?

All in all I think that approach you propose will solve our problem, but I have concerns that service exposing API for retrieving logs can experience quite a high load due to possible amount of logs requested, and it will require some indexing implementation so that logs can be retrieved at least by time range. This in turn makes me think that this service better be separate from node, and it should be based on some existing tool. Probably this could end up with each steward hosting their own ElasticSearch+LogStash instance, and Sovrin.org hosting some lightweight front-end service.;;;","16/Apr/18 10:10 PM;ashcherbakov;There is a quick fix of a simple publishing of all log files to a server (using cron job). This quick-fix matches our first thinking and the first option to achieve the requirement of INDY-1177.
 But we started to analyse it further, and came to the conclusion that in addition to just getting logs, it would be great to have some build-in *analyses* of the logs (not re-inventing a wheel). You can find more details in Sergey Kh.'s comments in the ticket.
 Actually we already have some custom scripts (written by Sergey Kh.) that can analyze the logs. The script works pretty good for common cases, but they don't work well with a huge amount of data, and making them do so looks like re-inventing a wheel. The ELK approach allows to perform log analysis more efficiently which means that *efficiency of stability bug fixing will be increased.*

I believe we need to do the following:
 # Propose a short-term solution as a hot-fix we already have:

 * Use cron job and publish the logs to a public server if Steward wants
 * Use custom script to analyze the logs
 * No dev work is required
 * [~mgbailey] / [~tharmon] need to work with Stewards to configure such publishing of logs

 # Propose a long-term solution to use ELK (Sergey Kh. can provide more details):

 * Each Steward may have (optionally) ELK instance
 * Each instance is mapped (optionally) to the main Sovrin ELK instance with Dashboard
 * Use ELK for log analyses (not custom scripts)
 * Allows to perform log analysis more efficiently which means that *efficiency of stability bug fixing will be increased*
 * May require about *1 Sprint for 1 engineer for PoC* of this approach.

I would prefer to implement Option 2 (ELK) since it has an advantage of bug fixing efficiency. However, we may consider using a short-term solution for now, and implement long-term a bit latter taking into account a lot of other (quite critical) issues.

 

[~sergey.khoroshavin] [~danielhardman] [~nage] [~tharmon] [~mgbailey]
 Do you agree with proposed short- and long-term solutions? If so, I believe we can close the ticket.

Would you prefer to implement short solution first, or start with a long-term solution right now (taking into account the advatnage of long-term solution for log analysis and efficiency of stability fixing)?;;;","16/Apr/18 11:00 PM;ashcherbakov;[~sergey.khoroshavin]
Please provide more details regarding Long-term option if needed (in particular, the difference between push from Stewards to their local ELK and pull from global ELK to local ELKs).;;;","17/Apr/18 12:33 AM;sergey.khoroshavin;[~ashcherbakov] Probably not much new details here, but a sum up of architecture I propose given latest input:
 1. Each node (willing to provide logs) will contain filebeat service which will watch node log file and send data to logstash instance hosted by steward. This can be done over local network, and compression can be applied if needed (filebeat supports it out of the box). Also, this channel could be authenticated and encrypted which is also supported by filebeat.
 2. Logstash instance hosted by steward will process logs adding useful metadata and store them in elasticsearch also hosted by steward. Actual processing rules will be ported from current log processing utility and provided in some open source repository (to be decided). Since logstash instance is separate from validator node and even can be denied from connecting to it (filebeat connects to logstash, not vice versa) it should be relatively safe to update these rules automatically. If this is still a problem it's possible to define only minimal set of rules which won't change much, in this case further processing will be mostly based on full text search rather than metadata.
 3. Elasticsearch instance hosted by steward will store logs and provide external (possibly permissioned) API for ther retrieval. This API can be used by steward to implement some local dashboard/monitoring/alerting solution, or by some external entity including Sovrin and developers.
 4. Sovrin can implement global dashboard by hosting kibana and elasticsearch instance with cross cluster search feature enabled which will pull data from stewards as needed.

This way following goals should be achieved:
 1. Minimal interference and security risks for validator node
 2. Log processing burden is spread across stewards
 3. Stewards are in full control of what information they give away
 4. Stewards have almost ready-to-use solution for monitoring and alerting
 5. Any entity can have access to indexed logs for fast analysis;;;","18/Apr/18 7:13 AM;danielhardman;Some feedback:
 # I think that an ELK solution has many good characteristics, and that Sergey K's summary of achieved goals (1-5) in the preceding comment is nice. I also believe that the proposal would alleviate some pressure for us for a while. I recognize that a lot of effort has gone into the current proposal, and I appreciate it.
 # I do not believe that it will scale. My belief is that it will be adequate until late 2018 at most. This is based on the observation that A) ledger traffic is likely to spike by mid year due to increasing interest and adoption; B) ledger traffic will also grow substantially due to the way token features use the ledger; C) the number of stewards we are wanting to work with is growing linearly, which will cause the number of logged events to grow in an N-squared fashion; D) we are likely to increase the verbosity of logs as we drill into events that might be causing problems we want to troubleshoot.
 # I believe that the heavy reliance on logging introduces new failure modes for the network, where disk space for logs gets exhausted, or the network pipe over which logs are streamed elsewhere has brownouts or disconnections.
 # I do not believe it accomplishes the overarching goal for developers, because in order for developers to get the insight they are hoping for, they need stewards to run with log level = DEBUG. I don't consider this reasonable. Production software should run at INFO or WARN levels.
 # I do not like the way this turns a project about a ledger into a project with deep investment in dashboards and monitoring technology. Dashboards are a legitimate feature for Sovrin, but not for indy.
 # I don't believe a deep archive of log events is particularly useful. 99% of the time, if we experience an interruption of service, we should be interested in what the last hour of the logs tells us. If we are not noticing the problem for more than an hour, then we have other problems and should not confuse them with logging. Thus, building and maintaining an index over a vast corpus of log data is overkill.
 # I am fairly certain that the Ops and OpsSec departments of large stewards will reject this approach because it requires greater visibility into their infrastructure than they prefer.
 # I do not think we are leveraging the monitoring plugin feature that we already wrote. Farooq gave us a sample monitoring plugin that would raise alerts on SQS. It feels like we are ignoring it.
 # I believe the current proposal is fundamentally centralized. We are saying that we want to think about this system as a centralized system with a common view across all of it, and we want to invest in maintaining that common, centralized view as a permanent feature. I don't think it's tenable. I admit that the proposal gets past some of my objections about conflicts with the trust framework, but it assumes that the way to diagnose and troubleshoot a failure is to create and maintain a centralized view.

What I would recommend is a fundamentally different approach to visibility for developers. Let's do the quick-and-dirty fix that doesn't involve ELK, and close this ticket. Then let's rethink our whole approach and write a new ticket or set of tickets that contemplates things like this:
 * How can we detect a looming crisis BEFORE it happens, and put the system into a proactive ""capture mode"" that will help developers diagnose and troubleshoot when the inevitable failure actually happens? For example, can we detect a DDoS and start logging differently, assuming we might go down? Can we detect nagging patterns of near failure in consensus? Instead of noticing that the network is no longer achieving consensus, can we start troubleshooting as soon as any node routinely fails to participate in consensus? Could we agree that when a node isn't achieving consensus, we automatically switch into a debug mode where logging gets more aggressive?
 * Can we develop a dashboard that is NOT populated from logs, but rather is populated from measures like network connection quality, average time to process a transaction, etc?
 * Can we develop some tools that will quickly eliminate common failure cases as explanations for a problem?
 * Can we work on scheduled downtime for nodes, so we promote routine maintenance instead of accidental interruptions?
 * Doctors take people's temperature during a visit, because it tells them a lot about the health of the system. What is the equivalent for us? If we don't know how to take the system's ""temperature"", why not, and how can we fix it?
 * Can we instrument node state machines, such that just the sequence of state transitions is reported to developers? 
 * Can we develop a node selection algorithm that lets us identify problem nodes and remove them from the consensus pool?
 * Can we ask nodes to maintain a more verbose debug log for developers in a circular queue of, say, 1 hour–plus a command that would pull this data on demand? This would give us an hour to notice and pull data. The data could be pulled by a pre-authorized person, but only distributed to developers on request. In this way, we could avoid having massive logs to analyze.;;;","18/Apr/18 4:05 PM;ashcherbakov;Ok, then let's close this ticket and go with short-term solution for now.

[~mgbailey] [~tharmon]
Can you please make sure that we configure and apply short-term solution (cron job to upload logs to a server) to STN and Live Pools?;;;","18/Apr/18 4:22 PM;ashcherbakov;[~danielhardman]

I believe we need to create a set of Stories based on your comments and proposals ([~esplinr] [~SeanBohan_Sovrin] [~resplin]).

 

Some comments on your items:

{quote}I don't believe a deep archive of log events is particularly useful. 99% of the time, if we experience an interruption of service, we should be interested in what the last hour of the logs tells us. If we are not noticing the problem for more than an hour, then we have other problems and should not confuse them with logging. Thus, building and maintaining an index over a vast corpus of log data is overkill.
{quote}
I agree that this is true for most of the cases, and that we need some mechanisms to receive notifications about events in the system (other than logs).
However, I think that some history of events or logs may still be needed in some cases. For example, an event that something is wrong with the Node may be lost, or not processed by a Steward (or someone else who monitors it), or it just occurred in the middle of the night, so we will need more info than just the last hour.

{quote}
I do not think we are leveraging the monitoring plugin feature that we already wrote. Farooq gave us a sample monitoring plugin that would raise alerts on SQS. It feels like we are ignoring it.
{quote}
I'm not aware of such a plugin at all. Can you please provide more info? There are some plugins that can help gathering statistics, but I'm not aware of SQS one done by Farooq.

{quote}
I believe the current proposal is fundamentally centralized.
{quote}
I don't agree with this assumption. The short-term solution is centralized, but the architecture of the long-term solution is that each steward individually and independently configures ELK instance, and there is an API/way to pull logs from there (not necessary centralized).
Centralization there appears only when Sovrin also creates a global ELK instance pulling data from Stewards (in fact from Steward's ELK instances). But this is rather optional and not essential part.

;;;","18/Apr/18 4:46 PM;ashcherbakov;[~danielhardman] [~esplinr] [~tharmon] [~mgbailey]
I created a new Monitoring story (we were discussing it before + it matches some of your proposals): https://jira.hyperledger.org/browse/INDY-1278;;;","19/Apr/18 2:32 AM;mgbailey;Regarding the comment about scheduled downtime.  Some of the stewards have been asking about this, for example, for a kernel patch that is released that requires a reboot. This could be as sophisticated as a steward writing a transaction to the config ledger to schedule a down time for his node, along with a mechanism for a human-readable mechanism to display this.  It could be as simple as a shared google spreadsheet where stewards enter their downtimes.;;;"
improve handling of demotion and promotion,INDY-1178,27755,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,SeanBohan_Sovrin,danielhardman,danielhardman,15/Feb/18 6:46 AM,15/Feb/18 4:57 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Today, it is possible to prevent a known node from participating in consensus by submitting a transaction that changes its services list (in its config) to an empty list. We call this ""demotion"". Later, if we reactivate the node, we submit a new transaction that does ""promotion"".

We need some enhancements:
 # We need to split our current genesis txn handling into two parts, such that a network has a set of genesis txns, plus a set of subsequent txns that evolve the nodes–where BOTH of these txn sets are in external files that could be distributed with an install image. This will allow a .deb file to lay down genesis txns plus any subsequent txns that change the node pool, short-circuiting catchup for the node pool and allowing a new node or client to tolerate a case where the original set of validators, and the set of validators today, are totally disjoint.
 # We need a node state that is terminal, so we can retire a node and prevent it from being re-promoted. This eliminates an attack surface.
 # We need a way for demoted nodes to periodically catch up in some limited fashion, so a node that has been demoted for a long time can have minimal catchup time once it is promoted again. Perhaps this could be done with a rudimentary rsync running as a cron job, or by listening for ledger evolution from a single validator that replicates to it using state proofs to prevent tampering. (The latter approach would give us part of the observer feature, though it is not an observer feature by itself, since the idled/demoted node would not necessarily need to support read-only queries, and might not replicate very aggressively.) 

Since the Sovrin Board of Trustees wants to accept many steward applications, but doesn't want to run a giant validator pool, this feature is fairly urgent from Sovrin's point of view.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyxlz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,danielhardman,kelly.wilson,mgbailey,nage,,,,,,,,"15/Feb/18 4:57 PM;ashcherbakov;[~SeanBohan_Sovrin] Can you please prioritize this task?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Ambiguous behavior after node demotion,INDY-1179,27761,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,zhigunenko.dsr,zhigunenko.dsr,15/Feb/18 5:44 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Case 1:*
 ""selected primary for instance 2"" are different for different nodes after demotion of primary node
 !11111.png|thumbnail!

*Steps to Reproduce:*
 1. Setup the pool of 7 nodes.
 2. Demote primary node (node 2 in this case).
 => View change happened on the rest nodes. ""selected primary Node5 for instance 1"" and ""selected primary Node5 for instance 2"" on all of them. (like in INDY-1112)
 3. Promote demoted node back.

*Actual Result:*
 On Node2 ""selected primary Node6 for instance 2"". It differs from the rest nodes.

*Expected Results:*
 Selected primary for instance 2 should be the same on all of nodes.

*Case 2:*
 After demote and promote primary (node2) node1 has stopped to writing NYMs. Logs are attached.
 *Steps to Reproduce:*
 The same as in previous case, but selected primaries are different:
!image-2018-02-15-11-26-09-005.png|thumbnail!",indy-node 1.3.308 (master),,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1112,,,,,INDY-1256,,,,,,,,,,,,,,,"15/Feb/18 5:36 PM;zhigunenko.dsr;11111.png;https://jira.hyperledger.org/secure/attachment/14598/11111.png","15/Feb/18 8:25 PM;ozheregelya;image-2018-02-15-11-26-09-005.png;https://jira.hyperledger.org/secure/attachment/14602/image-2018-02-15-11-26-09-005.png","15/Feb/18 5:36 PM;zhigunenko.dsr;logs_rc.7z;https://jira.hyperledger.org/secure/attachment/14599/logs_rc.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyr8f:",,,,,,Sprint 18.04,,,,,,,,,,,,,,,,,,,,sergey-shilov,zhigunenko.dsr,,,,,,,,,,,"28/Feb/18 6:46 PM;sergey-shilov;*Problem state / reason:*

The formula that is used for selection of primary node for master replica can not be applied for selection of primary nodes for backup replicas since N is variable.

*Changes:*

Now primaries for backup instances are chosen in a round-robin manner always starting from primary. If the next node is a primary for some instance then this node is skipped. So the first non-primary node is chosen as primary for current instance. Such approach allows to avoid election of instances of the same node as a primaries for different instances.
 The election procedure of the primary for master instance is not changed.
 Also working with node registry was changed. Now a newly added node does not add itself to owned node registry during initialisation. It waits for pool ledger catch-up instead. So now node registry is consistent with pool txns.

*Committed into:*

    [https://github.com/hyperledger/indy-plenum/pull/539]
     [https://github.com/hyperledger/indy-node/pull/585]
     indy-node 1.2.320-master

*Risk factors:*

    Nodes demotion and promotion, growing number of instances.

*Risk:*

    Medium

*Recommendations for QA:*

 - Check demotion and promotion of primary of master replica when number of instances is changed.
  - Check demotion and promotion of random nodes.;;;","28/Feb/18 11:43 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.3.321
indy-plenum                1.2.259

*Steps to Reproduce:*
1. Setup the pool of 7 nodes.
2. Demote primary node
3. Promote demoted node back
4. Demote nodes for instance 1 and instance 0, promote instance 0
5. Demote nodes for instance 2 and instance 0, promote instance 0
6. Demote node which follow to the instance 2 and node for instance 0, promote instance 0

*Actual results:*
- No duplicates in primary instances
- Sending NYM is successful;;;",,,,,,,,,,,,,,,,,,,,,,,
One of the nodes does not respond to libindy after several running load test,INDY-1180,27763,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,dsurnin,ozheregelya,ozheregelya,15/Feb/18 9:22 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,Stability,,,"From IS-566:
{quote}In the attached log I see that libindy try to catchup pool transactions from 7 to 25 and one of the nodes (4th) do not response. And there are no logs after it, but libindy should after timeout detect and blacklist not responding nodes and resume catchup from others.
{quote}
Logs from Node4: [^Desktop.7z]","indy-node 1.3.51
libindy 1.3.1~371",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1191,,,,,INDY-1216,IS-600,INDY-1238,,,,,,,,,,,,,"15/Feb/18 9:20 PM;ozheregelya;Desktop.7z;https://jira.hyperledger.org/secure/attachment/14603/Desktop.7z","15/Feb/18 10:37 PM;ozheregelya;Node1.7z;https://jira.hyperledger.org/secure/attachment/14621/Node1.7z","15/Feb/18 10:12 PM;ozheregelya;Node1.7z;https://jira.hyperledger.org/secure/attachment/14604/Node1.7z","15/Feb/18 10:37 PM;ozheregelya;Node10.7z;https://jira.hyperledger.org/secure/attachment/14629/Node10.7z","15/Feb/18 10:37 PM;ozheregelya;Node11.7z;https://jira.hyperledger.org/secure/attachment/14606/Node11.7z","15/Feb/18 10:37 PM;ozheregelya;Node12.7z;https://jira.hyperledger.org/secure/attachment/14607/Node12.7z","15/Feb/18 10:37 PM;ozheregelya;Node13.7z;https://jira.hyperledger.org/secure/attachment/14608/Node13.7z","15/Feb/18 10:37 PM;ozheregelya;Node14.7z;https://jira.hyperledger.org/secure/attachment/14609/Node14.7z","15/Feb/18 10:37 PM;ozheregelya;Node15.7z;https://jira.hyperledger.org/secure/attachment/14610/Node15.7z","15/Feb/18 10:37 PM;ozheregelya;Node16.7z;https://jira.hyperledger.org/secure/attachment/14611/Node16.7z","15/Feb/18 10:37 PM;ozheregelya;Node17.7z;https://jira.hyperledger.org/secure/attachment/14612/Node17.7z","15/Feb/18 10:37 PM;ozheregelya;Node18.7z;https://jira.hyperledger.org/secure/attachment/14613/Node18.7z","15/Feb/18 10:37 PM;ozheregelya;Node19.7z;https://jira.hyperledger.org/secure/attachment/14614/Node19.7z","15/Feb/18 10:37 PM;ozheregelya;Node2.7z;https://jira.hyperledger.org/secure/attachment/14622/Node2.7z","15/Feb/18 10:37 PM;ozheregelya;Node20.7z;https://jira.hyperledger.org/secure/attachment/14615/Node20.7z","15/Feb/18 10:37 PM;ozheregelya;Node21.7z;https://jira.hyperledger.org/secure/attachment/14616/Node21.7z","15/Feb/18 10:37 PM;ozheregelya;Node22.7z;https://jira.hyperledger.org/secure/attachment/14617/Node22.7z","15/Feb/18 10:37 PM;ozheregelya;Node23.7z;https://jira.hyperledger.org/secure/attachment/14618/Node23.7z","15/Feb/18 10:37 PM;ozheregelya;Node24.7z;https://jira.hyperledger.org/secure/attachment/14619/Node24.7z","15/Feb/18 10:37 PM;ozheregelya;Node25.7z;https://jira.hyperledger.org/secure/attachment/14620/Node25.7z","15/Feb/18 10:37 PM;ozheregelya;Node3.7z;https://jira.hyperledger.org/secure/attachment/14623/Node3.7z","15/Feb/18 10:37 PM;ozheregelya;Node4.7z;https://jira.hyperledger.org/secure/attachment/14624/Node4.7z","15/Feb/18 10:37 PM;ozheregelya;Node5.7z;https://jira.hyperledger.org/secure/attachment/14625/Node5.7z","15/Feb/18 10:37 PM;ozheregelya;Node6.7z;https://jira.hyperledger.org/secure/attachment/14626/Node6.7z","15/Feb/18 10:37 PM;ozheregelya;Node7.7z;https://jira.hyperledger.org/secure/attachment/14627/Node7.7z","15/Feb/18 10:37 PM;ozheregelya;Node8.7z;https://jira.hyperledger.org/secure/attachment/14628/Node8.7z","15/Feb/18 10:37 PM;ozheregelya;Node9.7z;https://jira.hyperledger.org/secure/attachment/14605/Node9.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyzzr:",,,,,,Sprint 18.04,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,dsurnin,ozheregelya,,,,,,,,,"16/Feb/18 10:11 PM;ozheregelya;The issue was reproduced on the latest RC (indy-node 1.3.52). 
 Debug logs and journalctl: [https://drive.google.com/file/d/1wK5kj8yHLZ-SDiUP8Ip_OuIMjegnD6XA/view?usp=sharing]

Node 9 data: [https://drive.google.com/file/d/1QO-ZEPq8vwpoOIrjRQWZM5Ax8XZnFvwg/view?usp=sharing];;;","19/Feb/18 5:13 PM;ashcherbakov;*Issue 1: Not updated watermarks on backup instances*

_Problem reason:_
 When a master replica receives a quorum of stable checkpoints, and realizes that it's behind, it start catch-up and updates the watermarks. 
 Backup replicas should also update watermarks in this situation.

_Fix:_

Update the watermarks for delayed replicas regardless whether it's a master or backup.

_PR:_

[https://github.com/hyperledger/indy-plenum/pull/536]

*Issue 2: It looks like view change is not finished on some of the nodes*

_Problem reason:_

A node didn't finish view change for some reasons (not enough logs to say why).

_Fix:_

A Node needs to check whether it received a quorum of ViewChangeDone (Current State) messages, and request them if needed.

_PR:_

TBD

*Issue 3: One of the Backup Primaries is disconnected*

_Problem reason:_
We initiate a View Change only if master Primary is disconnected.
If a backup replica's Primary is disconnected, then the requests are just stashed.

It leads to increasing of queues size.

_PR:_

TBD

*Issue 4: Two Nodes couldn't restart*

Two Nodes hanged up on 'Recovering tree from hash store of size'. Recovering didn't finish.

Stop/restart of indy-node service didn't work.

The ledger data seems to be correct, it was able to recover on another machine.

Also the Node started properly after restart (killing indy-node service).

_PR:_

TBD

 ;;;","26/Feb/18 9:17 PM;anikitinDSR;Recomendation for QA:
 - If this issue will not reproduced while testing INDY-1141 then ticket may be closed.;;;","26/Feb/18 9:53 PM;ashcherbakov;* INDY-1191 is created for Issue 2
 * [~anikitinDSR]Please create a ticket for Issue 3

 * Let's see if Issue 4 is reproduced again, it looks like some problems with systemd.;;;","05/Mar/18 9:01 PM;ozheregelya;Environment:
 25-nodes AWS pool
 indy-node 1.3.324

Steps to Validate:
 1. Check basic load: {{for s in `seq 1 2000` ; do python3 Perf_Add_nyms.py -n 500 -s 20 ; done}} - {color:#d04437}{color:#14892c}failed{color}{color}.
 2. Check load with greater count of threads (like in INDY-1141).;;;","12/Mar/18 9:39 PM;ozheregelya;Exploratory ticket INDY-1216 was created to check how many transactions can be written with different count of threads. This ticket will be partially tested by exploration of INDY-1216.;;;","13/Mar/18 10:59 PM;ozheregelya;Environment:
 indy-node 1.3.331
 libindy 1.3.1~371
 AWS 25 nodes pool

Reason for Reopen:
 Problem with nodes lagging reproduces with 1 thread in load test.

Steps to Reproduce:
 1. Run load test with 1 thread:
 for s in `seq 1 2000` ; do python3 Perf_Add_nyms.py -n 500 -s 1 ; done

Actual Results:
 Part of nodes lagged. Most of nodes wrote 33509 transactions.
 Node5: 29576
 Node7: 29369
 Node12: 32648
UPD: Node14: 24074
 Node17: 29697
 Node21: 26812

Expected Results:
 Nodes should not lag.

Logs:
 [https://drive.google.com/file/d/1ZAoULbXf-P_Y-umXJkKJDimO0vgebarT/view?usp=sharing];;;","20/Mar/18 12:40 AM;ozheregelya;Logs after first [~dsurnin]'s fix (indy-node 1.3.339): [https://drive.google.com/file/d/16kD3pWpkg3tTowcWaNfMgk5QVC_u5TYU/view?usp=sharing];;;","20/Mar/18 11:21 PM;ozheregelya;Additional logs with memory monitoring (empty statistic_NodeX.log were added by mistake):
https://drive.google.com/file/d/1SiEzVZACd-Dmdh74QqrIKoci0sE3qjXT/view?usp=sharing

Environment:
AWS pool of 25 nodes (t2.medium)
indy-node 1.3.339

Steps to Reproduce:
1. Run load test with 5 threads for ~16-20 hours.

Actual Results:
One of nodes was killed because of out of memory after ~6 hours.
Most part of nodes wrote 46268 txns.
Node 6: 43931;
Node 7: 44903;
Node 8: 46230;
Node 14: 45842;
Node 16: 38420;
Node 17: 46230;
Node 18: 40315.
Note that there were problems with connection to node6 and it was restarted before logs collecting.;;;","22/Mar/18 10:54 PM;ozheregelya;More logs:
https://drive.google.com/file/d/1Aqop3BzlbPuTQlvJ-aHgoK46C1dL8JxR/view?usp=sharing;;;","26/Mar/18 10:08 PM;dsurnin;There were 3 fixes In scope of this issue.
1 - Fix of invalid batch splitting
2 - Cleaning of queue of stashed_during_chatchup commits
3 - Fix processing of non-finalized requests

At the moment pool is able to restore its state after the load is gone.
For the moment several nodes are being killed during the test with Out of mem error. Separate issue for memory issue is created [INDY-1238|https://jira.hyperledger.org/browse/INDY-1238];;;",,,,,,,,,,,,,,
fix packer instructions to suggest supported architecture amd64,INDY-1181,27827,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,beatchristen,beatchristen,19/Feb/18 12:30 AM,09/Oct/19 5:40 PM,28/Oct/23 2:47 AM,09/Oct/19 5:40 PM,,,,,,0,,,,"The packer instructions in {{environment/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md}} suggest to create a 32bit ubuntu box, but this architecture is not supported in the sovrin repo at https://repo.sovrin.org/deb/pool/xenial/stable/s/sovrin/

Encountered errors during creation of the virtual machines:

1. The box name used by the Vagrantfile is {{bento/ubuntu-16.04}}, not {{bento/ubuntu1604}}

2. Unable to locate package sovrin

==> cli01: Get:6 https://repo.sovrin.org/deb xenial Release [46.4 kB]
==> cli01: Get:7 https://repo.sovrin.org/deb xenial Release.gpg [819 B]
==> cli01: Fetched 149 kB in 2s (72.6 kB/s)
==> cli01: Reading package lists...
==> cli01: Reading package lists...
==> cli01: Building dependency tree...
==> cli01: Reading state information...
==> cli01: E
==> cli01: : 
==> cli01: Unable to locate package sovrin
h2. Links

[https://github.com/hyperledger/indy-node/blame/stable/environment/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md#L57]

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzyxzr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,beatchristen,,,,,,,,,,,"09/Oct/19 5:40 PM;ashcherbakov;We don't support Vagrant anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,
read_ledger exception on NFS mounted file system,INDY-1182,27864,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,smithbk,smithbk,20/Feb/18 11:42 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,libsovrin,,,0,,,,"T-Shirt: S

When the ledger is on an NFS mounted file system. the read_ledger script successfully reads the ledger but then fails to delete the copy of the ledger with the following exception:
{code}
Traceback (most recent call last):
  File ""/usr/local/bin/read_ledger"", line 169, in <module>
    shutil.rmtree(read_copy_ledger_data_dir)
  File ""/usr/lib/python3.5/shutil.py"", line 474, in rmtree
    _rmtree_safe_fd(fd, path, onerror)
  File ""/usr/lib/python3.5/shutil.py"", line 416, in _rmtree_safe_fd
    onerror(os.rmdir, fullname, sys.exc_info())
  File ""/usr/lib/python3.5/shutil.py"", line 414, in _rmtree_safe_fd
    os.rmdir(name, dir_fd=topfd)
OSError: [Errno 39] Directory not empty: 'domain_merkleNodes'
{code}

See https://uisapp2.iu.edu/confluence-prd/pages/viewpage.action?pageId=123962105 and https://github.com/hashdist/hashdist/issues/113 for related info.

",,,,,,,,,,INDY-1205,,,,,,,,,,,,,,,,,,,,,,,,INDY-1189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzz9vj:",,,,,,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,SeanBohan_Sovrin,smithbk,,,,,,,,,"28/Feb/18 7:31 AM;SeanBohan_Sovrin;[~smithbk] - right now no one else is working with NFS within the Sovrin network. Is this something where you have the bandwidth to submit a pull-request or support a developer for reproducing and testing a fix?;;;","02/Mar/18 4:52 PM;ashcherbakov;I believe a proper way to fix it is to migrate to RocksDB from Leveldb. We've already started this work, just need to finish and make sure that everything works. 
We've started making a refactoring with breaking changes, which will require migration of ledgers (changes in txns format). So, it would be convenient to migrate to RocksDB as well.;;;","08/May/18 11:37 PM;smithbk;There are still issues on RocksDB.  It looks to me that read_ledger is not passing read_only=true when opening the DB.   See the following stack entry below.
   File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 26, in _defaultStore
    dataDir, logName, open)

Here is the full stack trace.

$ read_ledger --type pool
Traceback (most recent call last):
  File ""/usr/local/bin/read_ledger"", line 177, in <module>
    ledger = get_ledger(args.type, ledger_data_dir)
  File ""/usr/local/bin/read_ledger"", line 99, in get_ledger
    return Ledger(CompactMerkleTree(hashStore=hash_store), dataDir=ledger_data_dir, fileName=ledger_name)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger.py"", line 13, in __init__
    super().__init__(*args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 60, in __init__
    self.start()
  File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 214, in start
    config=self.config)
  File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 26, in _defaultStore
    dataDir, logName, open)
  File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 36, in initKeyValueStorageIntKeys
    return KeyValueStorageRocksdbIntKeys(dataLocation, keyValueStorageName, open, read_only)
  File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb_int_keys.py"", line 21, in __init__
    super().__init__(db_dir, db_name, open, read_only)
  File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 23, in __init__
    self.open()
  File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb_int_keys.py"", line 27, in open
    self._db = rocksdb.DB(self._db_path, opts)
  File ""rocksdb/_rocksdb.pyx"", line 1437, in rocksdb._rocksdb.DB.__cinit__
  File ""rocksdb/_rocksdb.pyx"", line 84, in rocksdb._rocksdb.check_status
rocksdb.errors.RocksIOError: b'IO error: While lock file: /var/lib/indy/sandbox/data/ibmTest/pool_transactions/LOCK: Resource temporarily unavailable'

When indy-node is running, it creates the following NFS locks:

$ find /var/lib/indy/sandbox/data/ibmTest -name LOCK
/var/lib/indy/sandbox/data/ibmTest/domain_merkleNodes/LOCK
/var/lib/indy/sandbox/data/ibmTest/domain_merkleLeaves/LOCK
/var/lib/indy/sandbox/data/ibmTest/domain_transactions/LOCK
/var/lib/indy/sandbox/data/ibmTest/domain_state/LOCK
/var/lib/indy/sandbox/data/ibmTest/pool_state/LOCK
/var/lib/indy/sandbox/data/ibmTest/pool_merkleNodes/LOCK
/var/lib/indy/sandbox/data/ibmTest/pool_merkleLeaves/LOCK
/var/lib/indy/sandbox/data/ibmTest/pool_transactions/LOCK
/var/lib/indy/sandbox/data/ibmTest/idr_cache_db/LOCK
/var/lib/indy/sandbox/data/ibmTest/state_signature/LOCK
/var/lib/indy/sandbox/data/ibmTest/attr_db/LOCK
/var/lib/indy/sandbox/data/ibmTest/state_ts_db/LOCK
/var/lib/indy/sandbox/data/ibmTest/seq_no_db/LOCK
/var/lib/indy/sandbox/data/ibmTest/config_merkleNodes/LOCK
/var/lib/indy/sandbox/data/ibmTest/config_merkleLeaves/LOCK
/var/lib/indy/sandbox/data/ibmTest/config_transactions/LOCK
/var/lib/indy/sandbox/data/ibmTest/config_state/LOCK

;;;","08/May/18 11:44 PM;smithbk;Ah ... this is now a dup of https://jira.hyperledger.org/browse/INDY-1289, but note that https://jira.hyperledger.org/browse/INDY-1289 is different from https://jira.hyperledger.org/browse/INDY-1317;;;","14/May/18 8:51 PM;ashcherbakov;[~ozheregelya] Please check it as https://jira.hyperledger.org/browse/INDY-1289 is resolved.;;;","17/May/18 8:13 PM;ozheregelya;Environment:
indy-node 1.3.412
AWS acceptance pool

Steps to Validate:
1. Setup NFS server.
2. Mount NFS to /var/lib/indy on one of nodes.
3. Run the node, write several txns, run read_ledger.
=> Read ledger works.
4. Stop the node with NFS.
5. Unmount NFS and mount it to another folder.
6. In /etc/indy/indy_config.py change /var/lib/indy to new mount point.
7. Run the node, write several txns, run read_ledger.
=> Node works, read_ledger works.

Actual Results:
Read_ledger works fine with NFS.;;;",,,,,,,,,,,,,,,,,,,
Newly upgraded STN fails to accept transactions,INDY-1183,27873,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,mgbailey,mgbailey,21/Feb/18 5:00 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Today we upgraded the STN from 1.2.50 to 1.3.52. We were posting transactions to it successfully yesterday before the upgrade, but now we are unable to successfully post transactions.  For example, at 19:17:17 I attempted to post a DID:
{code:java}
indy@sandbox> send NYM dest=SYLcHfW7PEU74ZWHkonbqx verkey=~7Xz2uHvQ8swWbchnsj1aAN
Adding nym SYLcHfW7PEU74ZWHkonbqx
{code}
But it did not get written to the ledger.

The upgrade was successful for all nodes except for ibm, where we performed a manual upgrade.  I have access to 7 of 10 nodes, whose logs are attached.  All of our nodes (plus ibm) show that the same primaries have been selected on them.  validator-info shows connectivity between all nodes.

One thing that I am seeing are errors regarding consistency proofs:
{code:java}
2018-02-20 19:15:21,880 | ERROR    | ledger_manager.py    (1001) | _buildConsistencyProof | virginia cannot build consistency proof till 35 since its ledger size is 32
{code}
It appears that one of the nodes, pcValidator01, has 3 more transactions than the rest?  I will be contacting that steward for logs and ledger data, which I will post here.

If my supposition that there are 3 additional transactions in one validator's ledger is correct, then there are 2 things to look at: 
 # How did the transactions get in there?
 # Why does one node's ledger being out of sync prevent transactions in the whole pool?",STN running 1.3.52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Feb/18 8:40 AM;mgbailey;can-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14658/can-stn-p001_20180220.tgz","21/Feb/18 5:00 AM;mgbailey;config_ledger;https://jira.hyperledger.org/secure/attachment/14641/config_ledger","21/Feb/18 5:00 AM;mgbailey;domain_ledger;https://jira.hyperledger.org/secure/attachment/14642/domain_ledger","21/Feb/18 8:40 AM;mgbailey;lon-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14656/lon-stn-p001_20180220.tgz","21/Feb/18 8:40 AM;mgbailey;nva-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14655/nva-stn-p001_20180220.tgz","21/Feb/18 7:40 AM;mgbailey;pcvalidator01_logs_2018_02_20.zip;https://jira.hyperledger.org/secure/attachment/14651/pcvalidator01_logs_2018_02_20.zip","21/Feb/18 5:00 AM;mgbailey;pool_ledger;https://jira.hyperledger.org/secure/attachment/14643/pool_ledger","21/Feb/18 8:40 AM;mgbailey;sao-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14654/sao-stn-p001_20180220.tgz","21/Feb/18 8:40 AM;mgbailey;seo-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14657/seo-stn-p001_20180220.tgz","21/Feb/18 8:40 AM;mgbailey;sgp-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14652/sgp-stn-p001_20180220.tgz","21/Feb/18 8:40 AM;mgbailey;syd-stn-p001_20180220.tgz;https://jira.hyperledger.org/secure/attachment/14653/syd-stn-p001_20180220.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyr93:",,,,,,Sprint 18.04,,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,ozheregelya,,,,,,,,,,"21/Feb/18 7:40 AM;mgbailey;Last Thursday (2/15), we reset the ledger back to the genesis data.  As a part of this, we sent instructions to the stewards on how to backup and delete the ledger data on their nodes.  It appears that these instructions were not followed properly by the stewards of pcValidator01, since there is pre-reset ledger data on the copy of the ledger on their node.  It is not complete (i.e., not all of the old ledger data is there), which is confusing.  Interestingly enough, this mismatch did not prevent consensus until the upgrade occurred. 

My current theory is that when the nodes restarted their services following the upgrade, the ledger mismatch was discovered, and the nodes are unable to recover from it.

Logs and ledger data from pcValidator01 are attached. [^pcvalidator01_logs_2018_02_20.zip];;;","21/Feb/18 8:41 AM;mgbailey;*New update.*  I was able to work with the steward of pcValidator01 to get his problem corrected.  He restarted the service on his node.  I then restarted the service on my 7 nodes as well (at 23:24).  At 23:26 I sent a new transaction to the ledger, as shown.  Still no transaction was written.
{code:java}
indy@sandbox> send NYM dest=2iha2ZoMLJuxvaAU4DPgE5 verkey=~DqZ4ej4PTC6ZPMf6zBY7Es
Adding nym 2iha2ZoMLJuxvaAU4DPgE5
indy@sandbox>
{code}
I am uploading refreshed logs to include this last attempt. 

 

 ;;;","21/Feb/18 11:00 PM;ashcherbakov;*Problem reason*
 * One node didn't perform upgrade, and hence didn't restart. So, the last ordered 3PC key for this node was (2,1) (viewNo=2, ppSeqNo=1).
 * The other nodes restarted and started first catch-up after Upgrade.
 * All nodes except the not-restarted one send LedgerStatus with viewNo=None and ppSeqNo=None since a view change is not happened yet (no primary is selected yet).
 * The not-restarted node sent LedgerStatus with viewNo= 2and ppSeqNo=1
 * Since all ledgers are the same, the viewNo and ppSeqNo from LedgerStatus are used for setting of last_ordered_3PC for master replica.
 * There was a bug, that used the first non-None 3PC key from LedgerStatus without checking for quorum.
 * So, all restarted nodes set (2,1) as last_ordered_3PC for master
 * But the viewNo was set as 0.
 * So, when a request came, the primary sent a PREPREPARE with viewNo=0, ppSeqNo=1: (0,1)
 * All Nodes rejected it with `already ordered`, since (0,1) < (2,1).

=> One malicious Node broke the whole Pool.

*Changes* 
 Require a f+1 quorum for 3PC keys in LedgerStatus when using it for master replica's last_ordered_3PC.

*PR*

Hotfix to Stable branch:
 [https://github.com/hyperledger/indy-plenum/pull/542]

*Version:*
- RC 1.3.53

*Risk factors:*
 - CatchUp
 - Recovering from f+1 Nodes
 - Upgrade

*Risk*
 Low/Med

*Covered with tests:*
 * test_same_ledger_initial_catchup
 * test_get_last_txn_3PC_key

*Recommendations for QA*
 * Do multiple View Changes and perform a forced simultaneous Upgrade for all Nodes except the one (the one should be at viewNo>0).
 * Check that catch-up works
 * Check that recovering from f+1 works.;;;","22/Feb/18 11:30 PM;ashcherbakov;*Problem Description***

After simultaneous forced Upgrade, the first Node doesn't write any new txns.

*Problem reason*
 * One node (the first one) started Upgrade a bit earlier than the others, finished initial cathc-up and set `last_ordered_3PC= (1,111)` (viewNo=1, ppSeqNo=111). This is fine, since it was the real state of the pool at that time, and there was consensus for this value. 
 * Then the rest of the pool started restart at the same time. The rest of the pool set `last_ordered_3PC= (0,0)` (as a new fresh state).
 * => Node1's master replica last_ordered_3PC= (1,111), and all other nodes' master replica last_ordered_3PC= (0,0)
 * Initial viewNo=0 is set, and the first Node is selected as a primary.
 * First Node sends PREPREPARE for a new request, and it's correctly ordered on all nodes except the first one
 * It's not ordered on the first Node, since all 3PC msgs (PREPARE, COMMIT) from other nodes are discarded as `already ordered` (since (0,5) < (1,111)).

*Changes* 
Reset `last_ordered_3PC` when starting a view change if the current last_ordered_3PC's viewNo is equal or greater than the new proposed view (because it means this is a last_ordered_3PC from an old state of the pool).

*PR*

Hotfix to Stable branch:
https://github.com/hyperledger/indy-plenum/pull/546

*Version:*
 - RC 1.3.54

*Risk factors:*
 - CatchUp
 - Recovering from f+1 Nodes
 - Upgrade
 - ViewChange

*Risk*
Med

*Covered with tests:*
 * http://test_last_ordered_reset_for_new_view

*Recommendations for QA*
 * Do multiple View Changes and perform a forced simultaneous Upgrade for all Nodes except the one (the one should be at viewNo>0).
Do simultaneous Upgrade of all Nodes
 * Check that catch-up works
 * Check that recovering from f+1 works.;;;","24/Feb/18 12:57 AM;ozheregelya;*Environment:*
 indy-node=1.3.54 (RC)

*Steps to Validate:*
 1. Setup the pool of 11 nodes with indy-node=1.2.50.
 2. On one of the nodes change repos in /etc/apt/sources.list from RC to master (it is necessary to get upgrade on this node failed because there is no 1.3.52 version in master repos).
 3. Schedule upgrade to version 1.3.52 for all nodes.
 => Node with changed sources.list was not upgraded. Pool is broken.
 4. Upgrade not upgraded node to version 1.3.52 manually and restart it.
 5. Schedule upgrade of all nodes to version 1.2.54.

*Actual Results:*
 Pool works after upgrade to 1.2.54 version.

*Additional Information:*
 Note that all upgrades were performed with force=True and with the same dates in upgrade schedule for all nodes.

*Following cases were verified in scope of regression testing:*
Upgrade 1.2.50 -> 1.3.54:
\- Simple upgrade on analog of Live pool;
\- Upgrade without one node;
\- Manual upgrade;
Upgrade 1.3.52 -> 1.3.54:
\- Simple upgrade on analog of STN pool;
\- Upgrade without one node;
\- Manual upgrade;
 Adding new node to the pool;
 View change;
 Catch up;
 Restore after losing consensus;
 Upgrade on large pool.;;;",,,,,,,,,,,,,,,,,,,,
A Steward needs to be able to get validator-info from all nodes,INDY-1184,27907,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,dsurnin,dsurnin,22/Feb/18 1:00 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,validator-info,,,0,GA-0,,,"To be able to monitor pool state we should implement new request - read validator_info from all nodes. It should not require consensus, should be sent to all connected nodes",,,,,,,,,,IS-588,,,,,,,,,,,,,,,,,,,,,,,,INDY-1174,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz9tj:",,,,,,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,3.0,,,,,,,,,,,,dsurnin,mgbailey,ozheregelya,Toktar,zhigunenko.dsr,,,,,,,,"30/Mar/18 12:16 AM;mgbailey;This would result in very large (probably json) data, and would probably require additional scripts to parse and interpret it for human consumption.  This is not necessarily a big problem, but perhaps the command should take an optional parameter of a specific node, in order to limit the data quantity.;;;","13/Apr/18 8:32 PM;Toktar;PR: [https://github.com/hyperledger/indy-node/pull/644] - design;;;","13/Apr/18 8:46 PM;Toktar;[~mgbailey] We discussed this problem. At the moment, there should not be a lot of data, but we will put a check on the size so that it doesn't exactly become a problem. We are  developing parameter that determines whether we get a complete full data or a short one.
If the size of the data will a problem, then we will think about compression of the validator-info data.;;;","24/Apr/18 9:47 PM;Toktar;Problem reason:
 -  Have not easy way to getting validator info data from all nodes.

Changes:
 -  Added command VALIDATOR_INFO for getting node info.

PR:
 - [https://github.com/hyperledger/indy-node/pull/644]

Version:
 - indy-node 1.3.384-master

Risk factors:
 - Nothing is expected

Risk:
 - Low

Covered with tests:
 * [test_validator_info_command.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/validator_info/test_validator_info_command.py]
 * [test_validator_info_handler.py|https://github.com/hyperledger/indy-node/blob/master/indy_node/test/validator_info/test_validator_info_handler.py]

Recommendations for QA:
 * command description [https://github.com/hyperledger/indy-node/blob/master/design/validator_info.md#new-command]
 * VALIDATOR_INFO request example [https://github.com/hyperledger/indy-node/blob/master/docs/requests.md#validator_info]
 * With the testing of the command there will be some problems until the SDK prepares its part.;;;","26/Apr/18 8:16 PM;zhigunenko.dsr;[~gudkov]

unable to test before IS-588 merging;;;","23/May/18 6:20 PM;ozheregelya;Required functionality was implemented on node side and covered by auto tests.
This task will be tested manually in scope of IS-588.;;;",,,,,,,,,,,,,,,,,,,
Make sure that all tests pass on Hyperledger Jenkins,INDY-1185,27922,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,22/Feb/18 8:06 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"We have a number of failing tests (both plenum and node) on Hyperledger Jenkins.

We need to make sure that all the tests pass in order to fully migrate to Hyperledger Jenkins.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1224,,,,,,,,,,,,,,,"08/Mar/18 1:51 AM;sergey.khoroshavin;test_6th_node_join_after_view_change_by_master_restart.txt.gz;https://jira.hyperledger.org/secure/attachment/14739/test_6th_node_join_after_view_change_by_master_restart.txt.gz","08/Mar/18 1:51 AM;sergey.khoroshavin;test_new_node_accepts_chosen_primary.txt.gz;https://jira.hyperledger.org/secure/attachment/14740/test_new_node_accepts_chosen_primary.txt.gz","14/Mar/18 10:59 PM;sergey.khoroshavin;test_old_non_primary_restart_after_view_change.txt;https://jira.hyperledger.org/secure/attachment/14772/test_old_non_primary_restart_after_view_change.txt","13/Mar/18 11:13 PM;sergey.khoroshavin;test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on.txt.gz;https://jira.hyperledger.org/secure/attachment/14760/test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on.txt.gz","08/Mar/18 1:51 AM;sergey.khoroshavin;test_successive_batch_do_no_change_state.txt.gz;https://jira.hyperledger.org/secure/attachment/14741/test_successive_batch_do_no_change_state.txt.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzz0yf:",,,,,,Sprint 18.05,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"06/Mar/18 9:39 PM;sergey.khoroshavin;This issue is being fixed in https://github.com/hyperledger/indy-plenum/pull/553;;;","07/Mar/18 9:36 PM;sergey.khoroshavin;Investigated and fixed tests:
- test_new_primary_has_wrong_clock:
     problem: view change was sometimes happening in unexpected place
     solution: delayed instance change messages until safe moment
- test_node_catchup_after_restart_with_txns
     problem: nodes became synchronized too fast
     solution: increased delay of catchup reply messages
- testProtocolInstanceCannotBecomeActiveWithLessThanFourServers
     problem: not enough time for nodes to synchronize when added back to pool
     solution: increased allowed time to synchronize
- test_successive_batch_do_no_change_state
     problem: unexpected view change was sometimes happening
     solution: disabled all instance change message

Remaining suspicious tests:
- test_6th_node_join_after_view_change_by_master_restart
- test_new_node_accepts_chosen_primary
- test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on
- test_old_non_primary_restart_after_view_change (failed once in Evernym CI);;;","14/Mar/18 11:20 PM;sergey.khoroshavin;As for remaining tests - they fail much more rarely than fixed ones, so hyperledger jenkins builds are almost stable now. Maybe it's better to mark them as intermittent and create another issue for tracking them.;;;",,,,,,,,,,,,,,,,,,,,,,
A developer needs to be able to distinguish logs of each replica,INDY-1186,27923,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,22/Feb/18 8:08 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"As of now we have just 1 big log file for base Node messages, plus all replicas.

It's better to split the log into the following files:

1) The main Node log for information related to all replicas

2) Each replica individual log files.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-484,,,,,,,,,,,,,,,,,,,,"14/Mar/18 5:56 PM;VladimirWork;INDY-1186.PNG;https://jira.hyperledger.org/secure/attachment/14768/INDY-1186.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz0z3:",,,,,,Sprint 18.05,,,,,,,,5.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"08/Mar/18 12:20 AM;anikitinDSR;Problem reason: 
- Need to help developers in log investigatting procedure

Changes: 
- Added prefix REPLICA (<replica_name>) into logs

PR:
- https://github.com/hyperledger/indy-plenum/pull/565


Version:
- indy-plenum, 1.2.268

Recommendations for QA
- run indy-node
- in log file must be string by regexp:
| ******| replica.py           ( 339) | h | REPLICA:(Alpha:0) ****;;;","14/Mar/18 5:56 PM;VladimirWork;Build Info:
indy-node 1.3.336

Steps to Validate:
1. Run indy-node.
2. Find a string in log files by regexp:
{noformat}
******	replica.py ( 339)	h	REPLICA:(NodeX:X) ****
{noformat}

Actual Results:
Prefix is added and displayed at all log levels. !INDY-1186.PNG|thumbnail! 
;;;",,,,,,,,,,,,,,,,,,,,,,,
"As a developer, I need to be able to track the path of each request",INDY-1187,27924,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,22/Feb/18 10:46 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,GA-0,,,"It's quite hard to read the log files now.

We need a tool to track each request: what stages it passed, what was the status, etc.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwwhb:",,,,,,18.06,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"27/Mar/18 11:08 PM;sergey.khoroshavin;There is ongoing work on configurable log processor, and now it contains simple rules to track received and ordered events, example of use can be found here: https://github.com/hyperledger/indy-plenum/pull/593;;;","29/Mar/18 1:49 AM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/596

Python libraries to install: matplotlib, yaml

How testing can be done:
- get log files from nodes and place them somewhere locally
- get process_logs and process_logs.yml from indy-plenum (they don't have any dependencies on other indy code, so it's not necessary to clone whole repository)
- copy example_track_req.yml where appropriate, edit it so that it points to previously obtained log files
- run ""process_logs example_track_req.yml""
- during processing tool should report which files it's processing, in the end it should report summary request statistics, dump request received/ordered messages to output.log and show graphs depicting request received/ordered events per second
- read documentation, experiment with configs, try example_track_single_req.yml
;;;",,,,,,,,,,,,,,,,,,,,,,,
One node fails behind others during the load_test with high load,INDY-1188,27927,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,dsurnin,dsurnin,dsurnin,23/Feb/18 12:00 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,GA-0,,,"under some circumstances one node could lag during the load_test

load_test is run with >= 200 threads
number of requests >= 100
load_test is run from the node machine",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1288,,INDY-1141,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwwgf:",,,,,,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,SeanBohan_Sovrin,,,,,,,,,,"27/Feb/18 6:39 AM;SeanBohan_Sovrin;[~dsurnin] - should this get tested with 1141?;;;","23/Apr/18 10:27 PM;ashcherbakov;Duplicates INDY-1188;;;",,,,,,,,,,,,,,,,,,,,,,,
New Node Catchup for Long Ledgers,INDY-1189,27956,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,benjsmi,benjsmi,24/Feb/18 1:22 AM,09/Oct/19 6:36 PM,28/Oct/23 2:47 AM,,,,,,,0,ledger-2,,,"The documentation for Indy, in particular for running a Sovrin Steward Node, says:
{noformat}
High availability in Sovrin is achieved via the consensus algorithm; hot-swapped machines in a cluster configuration may actually make the network less reliable{noformat}
This makes sense intuitively to any students of distributed ledger technologies (DLTs) in that when a new node joins the network, it will ask its neighbors for the consensus-verified copy of the ledger such that it is now ""caught up"" with the events so it can validate the next transaction (otherwise it doesn't have the relevant hashes, etc, to do the crypto work required).

In the Bitcoin DLT for example, [we can see that the size of the blockchain/ledger is ever increasing|https://blockchain.info/charts/blocks-size].  Indeed, there are concerns about this unbounded growth [on reddit|https://www.reddit.com/r/Bitcoin/comments/1fgj4n/the_blockchains_neverending_increase_in_size/] and users [have started to complain|https://bitcoin.stackexchange.com/questions/8923/where-do-i-find-the-actual-blockchain-size] about the size that the chain takes up on their systems.

I've heard various solutions proposed for other DLTs such as only requiring a node to verify the past X transactions to be considered ""caught up"" vs. the whole history of the chain as one potential solution. What is Indy/Plenum's approach to this problem? 

In particular, I've linked  issue 1182 because in debugging this issue, we've discovered that the read_ledger script distributed with Indy makes a copy of the entire tree that contains the chain, reads from the copy, and then deletes the copy.  This could get to be a very expensive approach/implementation for larger Indy ledgers.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1182,,,,,INDY-2015,INDY-1242,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzyxzz:",,,,,,,,,,,,,,,,,,,,,,,,,,benjsmi,brycecurtis,esplinr,SeanBohan_Sovrin,,,,,,,,,"28/Feb/18 7:23 AM;SeanBohan_Sovrin;Ben - this is great. Can we bring this into a discussion on the Indy mailing list - 
[https://lists.hyperledger.org/mailman/listinfo/hyperledger-indy];;;","30/Nov/18 5:18 AM;benjsmi;I have had a recent experience with this, where my validator node took about 10-15 minutes to get caught up to the current state of the ledger. At what point does 10-15 minutes turn into several days?  Is there a cutoff point before which we just accept that the ledger is truth?;;;","11/May/19 11:48 AM;esplinr;Recent releases contain a lot of improvements to the catch-up. INDY-2053 is a good example (see the explanation video https://www.youtube.com/watch?v=EtmcRwgFXV0&list=PLRp0viTDxBWGLdZk0aamtahB9cpJGV7ZF&t=1400)

However, the fundamental concern is not addressed. We will include it in our conversations about a future ledger architecture.;;;",,,,,,,,,,,,,,,,,,,,,,
Unable to submit upgrade transaction to STN,INDY-1190,27974,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,mgbailey,mgbailey,24/Feb/18 7:17 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"While submitting an upgrade transaction to the STN (1.3.52 to 1.3.54) the following error occurred: 
{code:java}
indy@sandbox> send POOL_UPGRADE name=STNUpgrade20180223 version=1.3.54 sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 action=start schedule={'UZH61eLH3JokEwjMWQoCMwB3PMD6zRBvG6NCv5yVwXz':'2018-02-23T21:50:05.555
000-00:00','2MHGDD2XpRJohQzsXu4FAANcmdypfNdpcqRbqnhkQsCq':'2018-02-23T21:50:05.555000-00:00','8NZ6tbcPN2NVvf2fVhZWqU11XModNudhbe15JSctCXab':'2018-02-23T21:50:05.555000-00:00','DNuLANU7f1QvW1esN3Sv9Eap9j14QuLiPeYzf28Nub4W':'2018-02-23T21
:50:05.555000-00:00','HCNuqUoXuK9GXGd2EULPaiMso2pJnxR6fCZpmRYbc7vM':'2018-02-23T21:50:05.555000-00:00','Dh99uW8jSNRBiRQ4JEMpGmJYvzmF35E6ibnmAAf7tbk8':'2018-02-23T21:50:05.555000-00:00','EoGRm7eRADtHJRThMCrBXMUM2FpPRML19tNxDAG8YTP8':'201
8-02-23T21:50:05.555000-00:00','2B8bkZX3SvcBq3amP7aeATsSPz82RyyCJQbEjZpLgZLh':'2018-02-23T21:50:05.555000-00:00','5fKwygs8KEGoUPGa65qz1oCm7h6Fb7HrML9r4jmZ9cic':'2018-02-23T21:50:05.555000-00:00'} timeout=15 force=True
Sending pool upgrade STNUpgrade20180223 for version 1.3.54
Pool upgrade failed: client request invalid: 'services'
{code}
Lovesh agreed to take a look at it, and found that there is improper validation occurring as a part of posting this transaction.  On the pool ledger in the STN (as well as on the live network) there are several transactions that were required as part of the BLS key upgrade which do not have a services field.  These are valid transactions, but the are flagged as an error by the code to post the upgrade transaction.

A log and the pool ledger are attached.",STN running 1.3.52,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Feb/18 7:17 AM;mgbailey;pool_ledger_20180223;https://jira.hyperledger.org/secure/attachment/14680/pool_ledger_20180223","24/Feb/18 7:17 AM;mgbailey;seo-stn-p001_20180223.tgz;https://jira.hyperledger.org/secure/attachment/14679/seo-stn-p001_20180223.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyr9b:",,,,,,Sprint 18.04,,,,,,,,,,,,,,,,,,,,ashcherbakov,lovesh,mgbailey,ozheregelya,,,,,,,,,"24/Feb/18 7:58 AM;lovesh;Fix resides here 
https://github.com/hyperledger/indy-plenum/pull/549
https://github.com/hyperledger/indy-node/commit/83837b4ce0c1e7e0a37c584b164505d5f69f34de;;;","26/Feb/18 6:32 PM;ashcherbakov;*Problem:*

There is a validation for POOL_UPGRADE txn that checks whether a node in `schedule` is demoted. In order to do this, pool ledger is analysed and `service` field is checked. 

There was a bug in this validation assuming that `services` field is always present.

If there is BLS key rotation txn in the pool ledger which doesn't have `services` field (we omit it, and specify `blskey` field only), then the validation failes and upgrade doesn't happen.

*Fix*
 * Added a check if `service` is present.
 * Added a test that pool upgrade works after BLS key rotation

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/549]
 * https://github.com/hyperledger/indy-node/pull/582

*Build*

1.3.319 (master)

1.3.55 (RC)

 

*Risk*

Low

 

*Recommendation for QA*

Rotate BLS keys (omit `service` field!) and perform Upgrade

 

 ;;;","28/Feb/18 2:18 AM;ozheregelya;Environment:
indy-node 1.3.55

Steps to Validate:
1. Setup the pool with indy-node 1.2.50.
2. Send NODE transaction with empty 'sevrices' field:
{code:java}
new key with seed 000000000000000000000000Steward1
send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data={'alias':'Node1','blskey': 'G9CyMZepT2ktvLuXvUDF1trMiUPxiShfEJxQq3bQJwHxCjwFn9bwGfWzQcixAomTm3r6j7UGnwFAA5hb9y2oQEL62id1BiKEUmsr7YFvqG3RmzF53fcDGAy2TYKhS6BQkDSWxNHMU471A11C51PxHbMMkUpHnw7HPUR3pgd43op1N1'}
send NODE dest=Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv data={'alias':'Node1','blskey': 'G9CyMZepT2ktvLuXvUDF1trMiUPxiShfEJxQq3bQJwHxCjwFn9bwGfWzQcixAomTm3r6j7UGnwFAA5hb9y2oQEL62id1BiKEUmsr7YFvqG3RmzF53fcDGAy2TYKhS6BQkDSWxNHMU471A11C51PxHbMMkUpHnw7HPUR3pgd43op1N6'}{code}
3. Send POOL_UPGRADE transaction to 1.3.55 version.
=> Pool successfully upgraded.
4. Send one more POOL_UPGRADE to the same version with reinstall=True to make sure that transaction will be written.

Actual Result:
POOL_UPGRADE is successfully written after upgrade to the latest version.;;;",,,,,,,,,,,,,,,,,,,,,,
Request view_change_done messages,INDY-1191,28018,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,dsurnin,anikitinDSR,anikitinDSR,26/Feb/18 8:30 PM,09/Oct/19 6:33 PM,28/Oct/23 2:47 AM,09/Oct/19 6:33 PM,,,,,,0,,,,If node send view_change_done messages after catchup completed and did not get quorum then it should send message request for not received view_change_done messages.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1180,INDY-1198,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwx4f:2rzlv",,,,,,,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,,,,,,,,,,,"09/Oct/19 6:33 PM;ashcherbakov;We don't have VCD messages anymore because of PBFT View Change;;;",,,,,,,,,,,,,,,,,,,,,,,,
Change NYM request processing in DomainRequestHandler IN PLENUM code,INDY-1192,28034,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,Derashe,Derashe,Derashe,27/Feb/18 1:42 AM,11/Oct/19 6:35 PM,28/Oct/23 2:47 AM,11/Oct/19 6:35 PM,,,,,,0,,,,"Now NYM request can be correctly processed only by steward. Change this behaviour according to public: [Sovrin network roles and permissions, Aries release|https://docs.google.com/spreadsheets/d/1TWXF7NtBjSOaUIBeIH77SyZnawfo91cJ_ns4TR-wsq4/edit#gid=0]. Some tests will fail after this change, be careful.

!Снимок.PNG!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/18 1:41 AM;Derashe;Снимок.PNG;https://jira.hyperledger.org/secure/attachment/14685/%D0%A1%D0%BD%D0%B8%D0%BC%D0%BE%D0%BA.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzyyzr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,Derashe,krw910,,,,,,,,,,"01/Mar/18 10:37 PM;Derashe;[Sean|https://jira.hyperledger.org/secure/ViewProfile.jspa?name=SeanBohan_Sovrin], sorry, I have probably misinformed you. This issue is only about plenum code base and it influence only on plenum tests. The node's DomainRequestHandler is working ok. ;;;","06/Mar/18 2:22 AM;krw910;[~Derashe] So what needs to happen with this ticket?;;;","06/Mar/18 2:20 PM;Derashe;[Kelly Wilson|https://jira.hyperledger.org/secure/ViewProfile.jspa?name=krw910] Well, in plenum code, particulary in plenum tests, we have wrong behaviour of DomainRequestHandler. For example, at this moment nym transaction in plenum tests can be performed only by steward. Trustee cannot do that. This task is about to change the code in plenum's DomainRequestHandler in such a way that it will satisfy Sovrin network roles file.;;;","11/Oct/19 6:35 PM;ashcherbakov;We don't use Plenum outside Node for now;;;",,,,,,,,,,,,,,,,,,,,,
send ATTRIB help includes an invalid example,INDY-1193,28035,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ckochenower,ckochenower,27/Feb/18 1:51 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Version:*

indy-node 1.3.54

*Sample output:*

 
{code:java}
indy@dkms> send ATTRIB help

Invalid syntax: 'send ATTRIB help'



send ATTRIB

-----------

     title: Adds attributes to existing DID



     usage: send ATTRIB dest=<target DID> [raw={<json-data>}] [hash=<hashed-data>] [enc=<encrypted-data>]



     example(s):

        send ATTRIB dest=33A18XMqWqTzDpLHXLR5nT raw={""endpoint"": ""127.0.0.1:5555""}{code}
Note the example of adding endpoint <IP>:<PORT> is a string instead of a dictionary containing an 'ha' element:
{code:java}
send ATTRIB dest=33A18XMqWqTzDpLHXLR5nT raw={""endpoint"": ""127.0.0.1:5555""}{code}

The following test on master helped me discover the correct raw JSON that must be sent:
https://github.com/hyperledger/indy-node/blob/master/indy_client/test/cli/test_send_attrib_validation.py#L235-L252

The error I get when the ""<IP>:<PORT>"" is not included in the ""ha"" element is as follows:
{code}
indy@dkms> send ATTRIB dest=JdRkpwWwCBqjfcrsjAH1GT raw={""endpoint"": ""dkmseas.pdev.evernym.com:443""}
Adding attributes {""endpoint"": ""dkmseas.pdev.evernym.com:443""} for JdRkpwWwCBqjfcrsjAH1GT
Error: client request invalid: InvalidClientRequest() [caused by 'str' object has no attribute 'get']
{code}

The same command succeeds when wrapping ""<IP>:<PORT>"" in an 'ha' element:
{code}
indy@dkms> send ATTRIB dest=JdRkpwWwCBqjfcrsjAH1GT raw={""endpoint"": {""ha"": ""34.217.164.50:443""}}
Adding attributes {""endpoint"": {""ha"": ""34.217.164.50:443""}} for JdRkpwWwCBqjfcrsjAH1GT
Attribute added for nym JdRkpwWwCBqjfcrsjAH1GT
{code}

If the example is correct, the code needs to change. If the code is correct, the example needs to change.","{panel:title=OS}


root@dkmsa1:~/.indy-cli/networks/dkms# cat /etc/*release*

DISTRIB_ID=Ubuntu

DISTRIB_RELEASE=16.04

DISTRIB_CODENAME=xenial

DISTRIB_DESCRIPTION=""Ubuntu 16.04.4 LTS""

NAME=""Ubuntu""

VERSION=""16.04.4 LTS (Xenial Xerus)""

ID=ubuntu

ID_LIKE=debian

PRETTY_NAME=""Ubuntu 16.04.4 LTS""

VERSION_ID=""16.04""

HOME_URL=""http://www.ubuntu.com/""

SUPPORT_URL=""http://help.ubuntu.com/""

BUG_REPORT_URL=""http://bugs.launchpad.net/ubuntu/""

VERSION_CODENAME=xenial

UBUNTU_CODENAME=xenial
{panel}
{panel:title=apt-cache policy indy-node}
root@dkmsa1:~/.indy-cli/networks/dkms# apt-cache policy indy-node

indy-node:

  Installed: 1.3.54

  Candidate: 1.3.54

  Version table:

 *** 1.3.54 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

        100 /var/lib/dpkg/status

     1.3.52 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.2.50 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.1.43 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.1.37 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.1.37 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.1.35 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.1.33 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.1.31 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     1.0.28 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages

     0.4.27 500

        500 https://repo.sovrin.org/deb xenial/stable amd64 Packages
{panel}
 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyyzz:",,,,,,,,,,,,,,,,,,,,,,,,,,ckochenower,,,,,,,,,,,,"27/Feb/18 3:16 AM;ckochenower;Closed won't fix, because indy-cli replaces indy and indy-cli has a valid example of adding endpoint information.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Use new shared library for indy-anoncreds CI/CD pipelines,INDY-1194,28052,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,27/Feb/18 4:57 PM,09/Oct/19 6:17 PM,28/Oct/23 2:47 AM,09/Oct/19 6:17 PM,,,,,,0,devops,,,Need to switch indy-anoncreds CI/CD pipelines to use new shared library created in scope of INDY-997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzwydj:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,esplinr,,,,,,,,,,,"09/Oct/19 6:17 PM;esplinr;The pipeline was sufficient to get us out of Hyperledger Incubation.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Use new shared library for indy-plenum CI/CD pipelines,INDY-1195,28053,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,27/Feb/18 4:58 PM,09/Oct/19 6:18 PM,28/Oct/23 2:47 AM,09/Oct/19 6:18 PM,,,,,,0,devops,,,Need to switch indy-plenum CI/CD pipelines to use new shared library created in scope of INDY-997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzwydr:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,esplinr,,,,,,,,,,,"09/Oct/19 6:18 PM;esplinr;The pipeline was sufficient to get us out of Hyperledger Incubation.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Use new shared library for indy-node CI/CD pipelines,INDY-1196,28054,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,27/Feb/18 5:00 PM,09/Oct/19 6:19 PM,28/Oct/23 2:47 AM,09/Oct/19 6:19 PM,,,,,,0,devops,,,Need to switch indy-node CI/CD pipelines to use new shared library created in scope of INDY-997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-997,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-828,,,,,,,,,"1|hzwydz:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,esplinr,,,,,,,,,,,"09/Oct/19 6:19 PM;esplinr;The pipeline was sufficient to get us out of Hyperledger Incubation.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool does not work after not simultaneous manual pool upgrade,INDY-1197,28058,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ozheregelya,ozheregelya,27/Feb/18 10:33 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Steps to Reproduce:
 1. Setup the pool with 1.2.50 version.
 2. One one of the nodes change _deb [https://repo.sovrin.org/deb] xenial rc_ to _master_ in /etc/apt/sources.list (it is necessary to get not upgraded node like in INDY-1183).
 3. Send simultaneous POOL_UPGRADE to 1.3.52 version for all nodes.
 => One of the nodes was not upgraded, pool is broken.
 4. Upgrade not upgraded node manually to the version 1.3.52, send several transactions.
 => Pool still doesn't work, transactions were not written.
 5. One-by-one upgrade nodes manually (due to problem with upgrade INDY-1190).
 => Pool wrote transactions after upgrade of two nodes. After upgrade of the rest nodes pool stopped writing transactions.
 6. Restart the pool.
 7. Try to write some transactions.

Actual Results:
 After manual upgrade and restart the pool only one transaction was written. After that pool stopped writing.

Expected Results:
 Pool should work.

Possible Workaround:
Pool works if manual upgrade (at least nodes stopping and starting) will be simultaneous for all nodes.",indy-node 1.2.50 -> 1.3.52 -> 1.3.55 (like STN pool),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1198,INDY-1256,,,,,,,,,,,,,,"17/Mar/18 1:20 AM;VladimirWork;1197.tar.gz;https://jira.hyperledger.org/secure/attachment/14793/1197.tar.gz","17/Mar/18 1:15 AM;VladimirWork;INDY-1197.PNG;https://jira.hyperledger.org/secure/attachment/14792/INDY-1197.PNG","16/Mar/18 5:45 PM;VladimirWork;INDY-1197.PNG;https://jira.hyperledger.org/secure/attachment/14785/INDY-1197.PNG","26/Mar/18 6:38 PM;VladimirWork;INDY-1197_ok1.PNG;https://jira.hyperledger.org/secure/attachment/14811/INDY-1197_ok1.PNG","26/Mar/18 6:38 PM;VladimirWork;INDY-1197_ok2.PNG;https://jira.hyperledger.org/secure/attachment/14812/INDY-1197_ok2.PNG","27/Feb/18 10:38 PM;ozheregelya;manual_upgrade_52_55_jctl.7z;https://jira.hyperledger.org/secure/attachment/14698/manual_upgrade_52_55_jctl.7z","27/Feb/18 10:39 PM;ozheregelya;manual_upgrade_52_55_logs.7z;https://jira.hyperledger.org/secure/attachment/14697/manual_upgrade_52_55_logs.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzx10n:",,,,,,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,ozheregelya,VladimirWork,,,,,,,,,"14/Mar/18 11:59 PM;dsurnin;Problem reason:
node sent ledger status to newly connected nodes only in case of 2f + 1 nodes in pool; in other case it sent only current state;

plenum version >= 273

automated tests
plenum/tests/reset

Risk:
 Low
;;;","15/Mar/18 5:55 PM;ashcherbakov;Indy-node 1.3.338;;;","17/Mar/18 1:20 AM;VladimirWork;Build Info: 
indy-node 1.2.188 -> indy-node 1.3.338 

Steps to Reproduce: 
1. Install 1.2.188 pool (12 nodes). 
2. Set wrong repo on one node. 
3. Run simultaneous 1.3.338 upgrade for all nodes. 
4. Try to send and *get* some NYMs.

Actual Results:
NYMs are added but *not got* after the upgrade (see screenshot). !INDY-1197.PNG|thumbnail! 
Debug logs are in attachment. [^1197.tar.gz] 

Expected Results:
Pool should work normally.

;;;","26/Mar/18 5:30 PM;ashcherbakov;[~VladimirWork]
 As I can see, all nodes send correct GET Replies (with state proofs)

Can we get CLI logs? Did we try to send txns with new CLI?;;;","26/Mar/18 6:38 PM;VladimirWork;Build Info: 
indy-node 1.2.188 -> indy-node 1.3.338

Steps to Validate: 
1. Install 1.2.188 pool (12 nodes). 
2. Set wrong repo on one node. 
3. Run simultaneous 1.3.338 upgrade for all nodes. 
4. Try to send and get some NYMs via:
4.1. cli 1.2.188
4.2. cli 1.3.338
4.3. indy-cli master

Actual Results: cli 1.3.338 and indy-cli master send and get NYMs successfully. cli 1.2.188 send NYMs only (it's ok, since any version of old cli will be obsolete soon).
 !INDY-1197_ok1.PNG|thumbnail!  !INDY-1197_ok2.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,
Pool stopped working if primary node was not included to schedule in upgrade transaction,INDY-1198,28059,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,ozheregelya,ozheregelya,27/Feb/18 10:55 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"Steps to Reproduce:
 1. Setup the pool with indy-node 1.2.50.
 2. Send POOL_UPGRADE transaction for simultaneous upgrade to version 1.3.55 with force=True without the first node.

Actual Results:
 Pool stopped writing after this upgrade.

Expected Result:
 Pool should work.

Possible Workaround:
 Pool starts to work after restart first node.",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1199,,INDY-1197,INDY-1191,INDY-1199,,,,,,,,,,,,,,,,,,"29/Mar/18 6:49 PM;zhigunenko.dsr;1198.7z;https://jira.hyperledger.org/secure/attachment/14828/1198.7z","16/Mar/18 7:01 PM;VladimirWork;1198.tar.gz;https://jira.hyperledger.org/secure/attachment/14787/1198.tar.gz","16/Mar/18 6:56 PM;VladimirWork;INDY-1198.PNG;https://jira.hyperledger.org/secure/attachment/14786/INDY-1198.PNG","26/Mar/18 7:17 PM;VladimirWork;INDY-1198_12_nodes.tar.gz;https://jira.hyperledger.org/secure/attachment/14813/INDY-1198_12_nodes.tar.gz","26/Mar/18 7:17 PM;VladimirWork;INDY-1198_journal_from_2_and_10.tar.gz;https://jira.hyperledger.org/secure/attachment/14814/INDY-1198_journal_from_2_and_10.tar.gz","27/Feb/18 10:54 PM;ozheregelya;upgrade50_55_primary_failure.7z;https://jira.hyperledger.org/secure/attachment/14699/upgrade50_55_primary_failure.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz3jr:",,,,,,Sprint 18.05,18.06,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,sergey.khoroshavin,sergey-shilov,VladimirWork,zhigunenko.dsr,,,,,,,"15/Mar/18 1:33 AM;sergey.khoroshavin;Now there is an integration test for this: https://github.com/hyperledger/indy-plenum/pull/571/files#diff-c3c40a7bce827f52d3bb9eeed77089cf
It passes after this fix: https://github.com/hyperledger/indy-plenum/pull/567, although it consistently failed before.;;;","15/Mar/18 5:55 PM;ashcherbakov;Indy-node 1.3.338;;;","16/Mar/18 7:02 PM;VladimirWork;Build Info:
indy-node 1.2.188 -> indy-node 1.3.338

Steps to Reproduce:
1. Install 1.2.188 pool.
2. Send POOL_UPGRADE transaction for simultaneous upgrade to version 1.3.338 with force=True without the first node.

Actual Results:
Pool stopped writing after this upgrade. NYMs sending forces to nodes' disconnections (see screenshot). !INDY-1198.PNG|thumbnail! 
Debug logs from all nodes are in attachment. [^1198.tar.gz] 

Expected Result:
Pool should work.;;;","26/Mar/18 5:52 PM;ashcherbakov;Looks like the situation is the following:
 * A number of txns was ordered (at view=0, Node1 is the Primary) before Upgrade
 * Upgrade is performed on Nodes2-4
 * After initial catch-up, Nodes 2-4 has last ordered  (for master) = (0,0).
 * They all selected Node1 as a Primary (which wasn't restarted).
 * Last ordered on Primary (Node1) is (0, 4).
 * Primary (Node1) sends PRE-PREPARE (0,5)
 * Nodes2-4 request missing PRE-PREPARES and PREPARES from (0,1) till (0,4).
 * Only Node1 has them, but this is not enough (moreover, received missing PREPAREs are not processed since they are from the Primary).


Moreover, it looks like there were some crashes, so we need logs from journalctl.

 ;;;","26/Mar/18 5:53 PM;ashcherbakov;[~VladimirWork]
Can you please provide logs from journalctl?;;;","26/Mar/18 7:17 PM;VladimirWork;We have the same results during the same steps at pool of 12 nodes.

Debug logs: [^INDY-1198_12_nodes.tar.gz] 
Journalctl: [^INDY-1198_journal_from_2_and_10.tar.gz] ;;;","28/Mar/18 7:06 PM;sergey-shilov;[~VladimirWork]
Could you please repeat the steps at pool of 12 nodes? Seem like bls storages were broken.;;;","29/Mar/18 6:49 PM;zhigunenko.dsr;[~sergey-shilov] 
Successfully reproduced by these steps:
1. Install 1.2.188 pool (12 nodes)
2. Send POOL_UPGRADE transaction for simultaneous upgrade to version 1.3.338 with force=True without the first node.
 [^1198.7z] ;;;","04/Apr/18 2:39 AM;sergey-shilov;[~ozheregelya] [~zhigunenko.dsr] [~VladimirWork]

Guys, *1.2.188-master* is not an analogue of *1.2.50-stable*. It points to*1.2.156-master* of indy-plenum, but in *1.2.158-master* of indy-plenum were added changes that make MultiSignatures store non backward compatible. Moreover, there is no migration scripts for that as early implementation of MultiSignatures was not released, so upgrade from *1.2.188* to *1.3.338* fails in any case as *1.3.338* can not work with MultiSignatures storage that *1.2.188* creates.

Much more closer analogue of *1.2.50-stable* is *1.2.223-master*, so the test upgrade should be repeated with this version.;;;","04/Apr/18 10:33 PM;zhigunenko.dsr;Pool successfully upgraded by these steps:
1. Install pool with 12 nodes
* python3-indy-crypto=0.1.6
* indy-anoncreds=1.0.32
* indy-plenum=1.2.180
* indy-node=1.2.223

2. Send POOL_UPGRADE transaction for simultaneous upgrade to version 1.3.338 with force=True without the first node.

*Results:*
Node1 stops to write new txns.

[Logs|https://drive.google.com/file/d/15Ct0fM7nX0GY9BNo2R8p_E4ap51mPSXK/view?usp=sharing];;;","05/Apr/18 7:48 PM;sergey-shilov;This ticket relates to [lower vewno/ppseqno|https://jira.hyperledger.org/browse/INDY-1199] issue, so mark it as duplicate.;;;",,,,,,,,,,,,,,
A node need to hook up to a lower viewChange,INDY-1199,28060,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,zhigunenko.dsr,zhigunenko.dsr,27/Feb/18 11:00 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,GA-0,TShirt_M,,"*Step to reproduce:*
 # make some viewChanges
 # demote node1 with current viewNo X
 # Simultaneously stop indy-node on all nodes (exclude demoted node)
 # Simultaneously start indy-node on all nodes (exclude demoted node)
 # Promote node1

*Actual results:*
 node1 cannot link with any viewChange from pool, until it reach #(X+1) (more than current viewChange on node1).

*Expected results:*
 Re-promoted node accept viewNo from other nodes
{code:java}
2018-02-27 13:19:33,584 | INFO     | message_processor.py (29) | discard | Node1 discarding message INSTANCE_CHANGE{'reason': 26, 'viewNo': 1} because Received instance change request with view no 1 which is not more than its view no 4
{code}

*Acceptance Criteria:*
Create a plan of attack, and raise appropriate stories and epics that can be scheduled.","indy-anoncreds 1.0.32
indy-node 1.3.319
indy-plenum 1.2.258
libindy 1.3.1~404
libindy-crypto 0.2.0
",,,,,,,,,,,,,,,,,,,,,,,INDY-1198,,,,,INDY-794,,,,,INDY-1198,INDY-1256,INDY-1422,INDY-1465,,,,,,,,,,,,"27/Feb/18 10:54 PM;zhigunenko.dsr;case3_.7z;https://jira.hyperledger.org/secure/attachment/14700/case3_.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwxwn:",,,,,,18.07 Stability & Monitoring,EV 18.14 Monitoring/Stability,EV 18.15 Stability/Availabilit,,,,,,3.0,,,,,,,,,,,,ashcherbakov,ozheregelya,sergey.khoroshavin,sergey-shilov,zhigunenko.dsr,,,,,,,,"17/Apr/18 5:18 PM;ashcherbakov;[~sergey-shilov]
Please write a PoA for the issue.

We can consider two more or less simple fixes: just notification of Stewards that the node has lower view change and hence restart is needed, or auto-restart.;;;","26/Apr/18 10:49 PM;sergey-shilov;*PoA:*

Current implementation of ""view change"" logic does not accept moving to lower than current view number. Moreover, for now we process in a different ways INSTANCE_CHANGE messages which contain _viewno = currrent_viewno - 1_ and _viewno < currrent_viewno - 1_ as the first described messages treated as 'rudiments' of the previous view change process (as I understand). Finally, for now all these messages are discarded. Various tests shows that we can achieve quorum of nodes that have lower viewno relative to particular participating nodes (group restarts, demote/promote etc.).

The main complexity here is the fact that for now whole our system is designed in a manner when we expect *monotonically growing of everything* since the service is started. It relates to view numbers, ppseqno's, timestamps etc. So that switching to lower viewno on fly, a.g. making step back without service restart, does not correspond to current architecture, has so many explicit and implicit side effects and seems like really hard to implement and debug.

So that I propose the following solution:
 * collect INSTANCE_CHANGE messages for lower viewno's as well as for higher viewno's
 * if a quorum for some lower viewno is reached than call auto-restart (instead of starting of whole view change process)

I think that now it is the most optimal solution that does not require very much time.

*NOTE:* current code base contain at least two places with conditions that check for lower viewno, be careful modifying or removing them.;;;","19/Jul/18 5:36 PM;sergey.khoroshavin;*Problem reason:*
When n-f or more nodes restart they have clear 3PC state viewNo 0 ppSeqNo 0, while f or less remaining nodes retain their current state and cannot communicate it back to restarted nodes. This leads to inability to order requests by minority nodes until either view change happens with viewNo greater than their current state, or these nodes are restarted as well.

*Changes:*
Added option ENABLE_INCONSISTENCY_WATCHER_NETWORK (disabled by default). When enabled it will restart nodes if they suspect they are in minority with inconsistent 3PC state. Detection is done using network events (connections/disconnections).

*PR:*
https://github.com/hyperledger/indy-plenum/pull/811
https://github.com/hyperledger/indy-node/pull/821

*Version:*
indy-plenum: 1.4.461-master
indy-node: 1.4.509-master

*Risk:*
Low/Medium

*Risk factors:*
None when ENABLE_INCONSISTENCY_WATCHER_NETWORK is disabled. When this option is enabled there is risk that some sequence of connection/disconnection events can lead to false positive restart, although probability of this is low.

*Covered with tests:*
Unit tests: https://github.com/hyperledger/indy-plenum/pull/811/files#diff-8a7a254ad0901148c601c3b13e6cc79e
Plenum integration tests: https://github.com/hyperledger/indy-plenum/pull/811/files#diff-8c3427188e75361797749306d6ed0f4f
Node integration tests: https://github.com/hyperledger/indy-node/pull/821/files#diff-f381ea2de93ed34e6167e29aa3c25824

*Recommendations for QA:*
In order to validate this issue you can:
- put ENABLE_INCONSISTENCY_WATCHER_NETWORK=True in /etc/indy/indy_config.py on all nodes
- start pool along with mild load test
- restart n-f or more nodes
- check that after that remaining nodes automatically restart as well
- check that all nodes have increasing number of transactions in their ledger

Also it's recommended to enable this option during other tests and watch out for sporadic restarts. If they happen this might be a bug.;;;","20/Jul/18 10:11 PM;ozheregelya;*Environment:*
 AWS pool of 25 nodes
 indy-node 1.5.515

*Steps to Reproduce:*
 1. Setup the pool with ENABLE_INCONSISTENCY_WATCHER_NETWORK=True in /etc/indy/indy_config.py
 2. Run the load test, grep logs for the message ""Suspecting inconsistent 3PC state, going to restart"".
 => No ""Suspecting inconsistent 3PC state, going to restart"" in logs.
 3. Restart 1 node manually, check the logs.
 => No ""Suspecting inconsistent 3PC state, going to restart"" in logs.
 4. Restart F nodes manually.
 => No ""Suspecting inconsistent 3PC state, going to restart"" in logs.
 5. Restart N - (F+1) nodes manually.
 => No ""Suspecting inconsistent 3PC state, going to restart"" in logs.
 6. Restart N - F nodes manually.
 => ""Suspecting inconsistent 3PC state, going to restart"" appears in logs of not restarted manually nodes. Not restarted nodes also were restarted.
 7. Restart N nodes manually.
 => Part of nodes were restarted second time because of ""Suspecting inconsistent 3PC state"".
 8. Restart N nodes by _ledger pool-restart_.
 => Part of nodes were restarted second time because of ""Suspecting inconsistent 3PC state"".

*Actual Results:*
 After simultaneous restart of all nodes part of the nodes were restarted twice.

*Expected Results:*
 Nodes should restart once.;;;","23/Jul/18 6:17 PM;sergey.khoroshavin;*PR:*
https://github.com/hyperledger/indy-node/pull/838

*Version:*
indy-node 1.5.519-master

*Changes:*
Instead of performing restart immediately on suspecting inconsistent 3PC state it is scheduled some time later to avoid double restarts when all pool is manually restarted. 
Added INCONSISTENCY_WATCHER_NETWORK_TIMEOUT (defaulting to 90 seconds).

*Risk:*
Medium

*Risk factors:*
Possible interference with POOL_RESTART transactions which schedule restarts.

*Recommendations for QA:*
- validate [original case|https://jira.hyperledger.org/browse/INDY-1199?focusedCommentId=47515&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-47515]
- check that during full pool restart inconsistency 3PC restarts are still scheduled, but not actually triggered
- check that inconsistency restarts don't interfere with scheduled POOL_RESTART in some unexpected ways;;;","01/Aug/18 7:57 PM;ozheregelya;Environment:
indy-node 1.5.529
libindy 1.6.1~659

Steps to Validate:
1. Restart all nodes exclude one of them.
=> Not restarted node restarted in $(INCONSISTENCY_WATCHER_NETWORK_TIMEOUT) seconds because of 'suspecting inconsistent 3PC state'.
2. Send pool restart without specified date.
=> All nodes were restarted once. On part of nodes 'suspecting inconsistent 3PC state' appears, but it is cancelled by pool-restart.
3. Restart all nodes exclude one of them.
4. Before the end of INCONSISTENCY_WATCHER_NETWORK_TIMEOUT, send pool-restart.
=> Restart because of 'suspecting inconsistent 3PC state' was cancelled by pool-restart.
5. Schedule pool-restart.
6. Restart all nodes exclude one of them.
=> Pool restart was not happened on restarted nodes. It was not happened on not restarted node as well because it cancelled pool restart when scheduled restart because of 'suspecting inconsistent 3PC state'.
7. Restart whole the pool.
=> Part of nodes scheduled restart because of 'suspecting inconsistent 3PC state', but it was not happened after actual restart.

Actual Results:
Pool restarts only once independently on type of restart.;;;",,,,,,,,,,,,,,,,,,,
Each next 3PC batch should have a timestamp not less than the previous one,INDY-1200,28062,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,28/Feb/18 12:22 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"We need to make sure that txns timestamps is a monotonically increasing sequence, so that each new transaction (3PC batch) has a timestamp greater than the previous one.

It can be done as part of dynamic validation of a timestamp proposed by the Primary.",,,,,,,,,,,,,,,,,IS-602,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-63,,,,,,,,,"1|hzz6yn:",,,,,,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,2.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,lovesh,ozheregelya,sergey.khoroshavin,,,,,,,,"12/Apr/18 12:32 AM;sergey.khoroshavin;[~lovesh] [~sergey-shilov] [~anikitinDSR] [~ashcherbakov]

After some research I have the following basic proposal:
{code}
    def create3PCBatch(self, ledger_id):
        ...
        tm = self.utc_epoch
+       if tm < self.last_accepted_pre_prepare_time:
+            tm = self.last_accepted_pre_preapre_time
        ...
{code}

Also, [~sergey-shilov] proposed to add additional check if current system time is not less than last accepted preprepare time minus some tolerance so that if master primary has wrong clock it will stop processing requests, which in turn should trigger view change. It doesn't seem to break anything, but we want to be sure that I didn't miss anything, so please review this.;;;","12/Apr/18 5:03 AM;lovesh;[~sergey.khoroshavin] The ticket says ""so that each new transaction (3PC batch)"", since a batch can have more than 1 txn, do you mean each transaction in the batch should have a different timestamp? If yes then why? What does it give us and its technically incorrect too to give each txn of the same batch a different timestamp.;;;","12/Apr/18 4:41 PM;sergey.khoroshavin;[~lovesh] Probably this ticket have imprecise wording. We don't want to change logic behind transaction timestamps inside batch. The main problem that we are trying to address is that in some cases system time can go back and in this case it's possible that new batch will get older timestamp. So main change that we propose is to enforce monotonic timestamps for batches. ;;;","17/Apr/18 5:10 PM;ashcherbakov;[~sergey.khoroshavin] [~sergey-shilov]

The proposed solution can solve the problem of assigning a correct tm for each next 3PC batch by Primary. But what if the primary is malicious? I think we also need to add validation of tm for non-primaries such that they make sure the new 3PC batch's tm is not less than the previous one.;;;","17/Apr/18 5:55 PM;sergey.khoroshavin;[~ashcherbakov]

This validation is already [implemented|https://github.com/hyperledger/indy-plenum/blob/master/plenum/server/replica.py#L2280];;;","18/Apr/18 9:23 PM;ashcherbakov;Please note, that we need to consider the case when `self.last_accepted_pre_prepare_time is None`. We need to go to the storage with our timestamps and take the latest one.;;;","28/Apr/18 10:42 PM;anikitinDSR;PRs:
- indy-node: https://github.com/hyperledger/indy-node/pull/674
- indy-plenum: https://github.com/hyperledger/indy-plenum/pull/646

Versions:
- indy-node: 1.3.395
- indy-plenum: 1.2.342

Reasons:
- need to provide, that each next 3PC batch will have a timestamp not less than the previous one

Changes:
- move tsDbStorage from indy-node into indy-plenum
- save each transaction's time into storage
- check, that preprepare's timestamp greater than last ordered

Steps to check:
- send txn
- for primary node, set system time less than previous ordered.
- send new txn
- check, that transaction's time for new txn greater or equal previous ordered txn;;;","06/May/18 2:48 AM;ozheregelya;*Environment:*
indy-node 1.3.403
indy-cli 1.3.1~505

*Steps to Validate:*
1. Setup the pool.
2. Write several txns.
3. Change time on primary to ~5 min earlier.
4. Write one more transaction.

*Actual Results:*
New txn was written with the same time as previous one.

Need to clarify expected behavior for time changed on more than 10 minutes.;;;","07/May/18 9:55 PM;ozheregelya;UPD for changing time on more than 10 minutes: in case of changing time to the past, txns are successfully written. Case of changing time to the future is not affected by this ticket and will be tested separately.;;;",,,,,,,,,,,,,,,,
"Adding new schema, field 'attr_names' of schema json can contain duplicate attribute name.",INDY-1201,27354,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,nhan.trong.nguyen,nhan.trong.nguyen,02/Feb/18 1:06 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"When I use libindy to build and submit schema request to ledger, I realize that ledger can accept the schema whose 'attr_names' contains duplicate attribute name.

Reproduce steps when using libindy:
| |Step|Data|
|1|Create a pool ledger config.
pool.create_pool_ledger_config(pool_name, pool_config).|pool_name = 'test_pool'
pool_config = \{'genesis_txn': pool_transactions_sandbox_genesis}|
|2|Open pool ledger and get 'pool_handle'.
pool.open_pool_ledger(pool_name).|pool_name = 'test_pool'|
|3|Create a wallet.
wallet.create_wallet(pool_name, wallet_name, None, None, None).|pool_name = 'test_pool'
wallet_name = 'test_wallet'|
|4|Open wallet and get 'wallet_handle'.
wallet.open_wallet(wallet_name, None, None).|wallet_name = 'test_wallet'|
|5|Create and store did of default trustee as 'did_default_trustee'.
signus.create_and_store_my_did(wallet_handle, did_json).|wallet_handle in step 4.
did_json = '\{'seed': '000000000000000000000000Trustee1'}'.|
|6|Build schema request store created request as 'schema_req'.
ledger.build_schema_request(submitter_did, schema_json).|submitter_did = did_default_trustee.
schema_json = '\{'name': 'test', 'version': '1.0', 'attr_names': ['name', 'name']}'|
|7|Submit 'schema_req' to ledger by role steward.
ledger.sign_and_submit_request(pool_handle, wallet_handle, submitter_did, schema_req).|pool_handle in step 2.
wallet_handle in step 4.
submitter_did = did_default_trustee.
schema_req in step 9.|

Expected result: SDK throws an error when the parameters it receives are incorrect.
Actual result: ledger accepts the request. 

 

This issue also happen when I execute command manually in CLI to submit schema request.

Reproduce steps when executing manually:
 # Open command prompt.
 # Execute command 'indy'.
 # Execute command 'connect sandbox'.
 # Execute command 'new key with seed 000000000000000000000000Trustee1'.
 # Execute command 'send SCHEMA name=test version=1.0 keys=name,name'.

Expected result: the schema request cannot be sent, 'invalid syntax' message displays.

Actual result: ledger accept schema request.","OS: ubuntu16.04

Libindy: 1.3.0

Wrapper: python

indy-node: 1.2.50

indy-plenum: 1.2.29

indy-anoncred: 1.0.11

sovrin: 1.1.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Feb/18 1:02 PM;nhan.trong.nguyen;automation_log.log;https://jira.hyperledger.org/secure/attachment/14530/automation_log.log","02/Feb/18 12:45 PM;nhan.trong.nguyen;manual.png;https://jira.hyperledger.org/secure/attachment/14531/manual.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyvpz:",,,,,,,,,,,,,,,,,,,,,,,,,,nhan.trong.nguyen,Toktar,,,,,,,,,,,"10/Oct/18 9:55 PM;Toktar;Indy-crypto will consider such request 
{code:java}
 '{'name': 'test', 'version': '1.0', 'attr_names': ['name', 'name']}'{code}
equal to this request
{code:java}
 '{'name': 'test', 'version': '1.0', 'attr_names': ['name']}'
{code}
It is transformed in indy_issuer_create_schema();;;",,,,,,,,,,,,,,,,,,,,,,,,
[TechDoc] We need to have more diagrams to understad the system from the technical point of view,INDY-1202,28087,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,01/Mar/18 12:47 AM,09/Oct/19 5:05 PM,28/Oct/23 2:47 AM,09/Oct/19 5:05 PM,,,,,,0,,,,"We have some technical overview and documentation:
 * [https://github.com/hyperledger/indy-plenum/tree/master/docs]
 * [https://github.com/hyperledger/indy-plenum/blob/master/docs/main.md]
 * [https://github.com/hyperledger/indy-node/tree/master/docs]

Most of the docs there are just markdown documents describing the concepts and workflows in text.

It would be great to also have some diagrams (sequence, relationship, use cases, etc.).

Examples of diagrams that we may have:
 * Request ordering by a Node and replicas (validation, 3PC batch creation, 3PC protocol, BLS multi-sigs), see [https://github.com/hyperledger/indy-plenum/blob/master/docs/request_handling.md]
 ** write requests (txns)
 ** read requests (queries)
 * View Change
 * CatchUp, see https://github.com/hyperledger/indy-plenum/blob/master/docs/catchup.md
 * Checkpoints
 * Monitoring
 * State Proofs and BLS multi-sigs

 

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Diagrams,Done,,,,,,,"1|hzwxa7:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,hawkmauk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We need to separate node service shutdown cases from node disconnection cases,INDY-1203,28141,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,VladimirWork,VladimirWork,01/Mar/18 8:05 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,explore,,,View change can be forced by both shutdown and disconnection for example and there will be two different cases to check. It should be added to Acceptance scenario. Also some exploratory testing of nodes' *disconnection* should be performed (`docker network disconnect pool-network nodeX`).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1210,,,,,,,,,,,,,,,"05/Mar/18 10:30 PM;zhigunenko.dsr;disc-2.7z;https://jira.hyperledger.org/secure/attachment/14719/disc-2.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyzh3:",,,,,,,,,,,,,,,,,,,,,,,,,,krw910,VladimirWork,zhigunenko.dsr,,,,,,,,,,"05/Mar/18 10:29 PM;zhigunenko.dsr;*Environment:*
indy-anoncreds 1.0.32
indy-node                  1.3.324
indy-plenum                1.2.259
libindy                    1.3.1~406

All cases have been reproduced in Docker with 7 nodes

*Case1:*
*Steps to reproduce:*
1. Write some NYMs
2. Disconnect current primary (nodeX)
3. Write some NYMs
4. Connect nodeX back
*Actual results:*
- viewChange completed successfully
- nodeX returned back to pool successfully

*Case2:*
*Steps to reproduce:*
1. Write some NYMs
2. Disconnect node which follows the primary instance 2 (nodeY)
3. Disconnect current primary (nodeX)
4. Try to write some NYMs
5. Connect nodeY back
6. Connect nodeX back
*Expected results:*
- nodeY returned back to pool successfully
- viewChange completed successfully when (n-f+1) node returned back
- nodeX returned back to pool successfully

*Actual results:*
{color:red}- pool lost its quorum and cannot restore it
- all nodes (exclude X and Y) assume that X and Y are malicious
- nodeX cannot connect to all the others{color}
*Logs:* disc-2.7z (X = node3, Y = node6)
Allocated in INDY-1210

*Case3:*
*Steps to reproduce:*
1. Write some NYMs
2. Disconnect current primary (nodeX)
3. Demote disconnected node
4. Write some NYMs
5. Connect nodeX back
6. Promote nodeX
*Actual results:*
NodeX comes back to pool and catch-up successfully

*Case4:*
*Steps to reproduce:*
1. Write some NYMs
2. Demote current primary nodeX
3. Disconnect nodeX
4. Write some NYMs
5. Promote nodeX
6. Connect nodeX back
*Actual results:*
NodeX comes back to pool and catch-up successfully

*Case5:*
*Steps to reproduce:*
1. Write some NYMs
2. Disconnect current primary (nodeX)
3. Stop indy-node on disconnected node
4. Write some NYMs
5. Connect nodeX back
6. Restart nodeX
*Actual results:*
NodeX comes back to pool and catch-up successfully

*Case6:*
*Steps to reproduce:*
1. Write some NYMs
2. Stop indy-node on current primary nodeX
3. Disconnect nodeX
4. Write some NYMs
5. Restart nodeX
6. Connect nodeX back
*Actual results:*
NodeX comes back to pool and catch-up successfully

*Acceptance scenarios:*
Case 13 - updated;;;","09/Mar/18 1:30 AM;krw910;[~zhigunenko.dsr] Great work and good findings.;;;",,,,,,,,,,,,,,,,,,,,,,,
[QA] Design traceability matrix for indy project,INDY-1204,28142,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,01/Mar/18 8:11 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"We need to map all *requirements and features* that we have with all *test cases* that we have (integration, system, acceptance; manual, automated).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyyhr:",,,,,,Sprint 18.05,,,,,,,,5.0,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"06/Mar/18 12:35 AM;VladimirWork;https://docs.google.com/spreadsheets/d/15xNisqjpSDs8A8bYvwbnmm4l-nmcPF7VQBqrDLu6A0c/edit#gid=0;;;",,,,,,,,,,,,,,,,,,,,,,,,
Use RocksDB as a key-value storage,INDY-1205,28184,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,02/Mar/18 5:10 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Taken into account our agreement about refactoring with breaking changes, I believe we have a good time for migration from LevelDB to RocksDB because of the following:
 # Migration to RocksDB requires migration (modification) of the ledger. We are going to migrate the ledger in any case (hopefully the last time) for refactoring, so we can combine it with RocksDB support, so no more migrations are needed in future.
 # I thing migration to RocksDB should not take much time, since Dmitry did a PoC, and everything worked. We just need to merge the old branch [rocksdb-integration|https://github.com/hyperledger/indy-plenum/tree/rocksdb-integration].
 # We have new resources for this work.
 # It will fix bugs like INDY-1182, since it's possible to access RocksDB from other process in read only mode (it's not possible for Leveldb, that's why read_ledger script has to create a tmp copy of the ledger which is ugly, will not work for big ledgers, and has some problems for NFS mounted systems).
 # RocksDB has snapshots support, which can help us in more efficient ledger catch-up.
 # RocksDB works well on Windows

What needs to be done:
 * Implement RocksDB implementation of key-value storage
 * Integrate RocksDB into CI and CD
 * Create migration script from leveldb to rocksdb
 * Adapt `read_ledger` script to work with RocksDB and avoid creation of the ledger's copy",,,,,,,,,,,,,,,,,INDY-1138,INDY-1182,INDY-1219,,,,,,,,,,,,,,,INDY-1243,INDY-1244,INDY-1245,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzyyhj:",,,,,,Sprint 18.05,18.06,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey-shilov,VladimirWork,,,,,,,,,,"27/Mar/18 7:04 PM;sergey-shilov;*Problem state / reason:*

See ticket description and ticket [INDY-1138|https://jira.hyperledger.org/browse/INDY-1138].

*Changes:*

Added support of rocksdb. Now each key/value storage and hash storages are configurable.

Currently we use:
 * *Database library: rocksdb-5.8.8*
 * *Wrapper: python-rocksdb-0.6.9*

NOTE: according to comment in INDY-1138 we should use version of rocksdb higher than or equal to *4.13*, but canonical repos for Ubuntu 16.04 contain version *4.1*, so for now we build rocksdb deb package as 3rd party package. As we plan to migrate to Ubuntu 18.04 in future, we decided to use rocksdb version *5.8.8* as this version currently presented in Ubuntu 18.04 canonical repos. Current version of rocksdb wrapper (that is not a wrapper in fact as it is linked statically with rocksdb library) python-rocksdb is *0.6.9* that is fully compatible with *rocksdb-5.8.8*.
Currently rocksdb build procedure gets rocksdb sources from sovrin's rocksdb fork:
    [https://github.com/evernym/rocksdb.git]
For now sovrin's fork's master and original rocksdb master branches are fully synchronised (including git tags). Currently we build our package on *tag rocksdb-5.8.8*, used build flags:
 * _{color:#333333}PORTABLE=1{color}_
 * _{color:#333333}EXTRA_CFLAGS=""-fPIC""{color}_
 * _{color:#333333}EXTRA_CXXFLAGS=""-fPIC""{color}_

{color:#333333}Currently the only storage that uses rocksdb by default is state-timestamp storage. It is a new storage, so these changes are fully backward compatible and do not need the migration steps. *So now we don't migrate from leveldb to rocksdb, this step has been taken out of scope of this ticket.*{color}

*Committed into:*

    https://github.com/hyperledger/indy-plenum/pull/561
    https://github.com/hyperledger/indy-plenum/pull/580
    https://github.com/hyperledger/indy-plenum/pull/583
    https://github.com/hyperledger/indy-plenum/pull/584
    [https://github.com/hyperledger/indy-plenum/pull/586]
    [https://github.com/hyperledger/indy-plenum/pull/589]
    [https://github.com/hyperledger/indy-plenum/pull/590]
    [https://github.com/hyperledger/indy-node/pull/619]
    https://github.com/hyperledger/indy-node/pull/623
    indy-node 1.2.353-master

*Risk factors:*

Instability of {color:#333333}state-timestamp storage{color}, installation issues related to rocksdb library and wrapper, changing of configuration parameters related to storages if leveldb storages are already presented on disk,

*Risk:*

    Medium.

*Recommendations for QA:*

Check installation of indy-node deb package, Change storages type to rocksdb, example from config.py:

===============================

_{color:#333333}hashStore = \{{color}_
 _{color:#333333}""type"": HS_ROCKSDB{color}_
_{color:#333333}}{color}_

domainStateStorage {color:#f92672}= {color}KeyValueStorageType.Rocksdb

# etc.

===============================

But don't change parameters to rocksdb if indy-node have already been started with leveldb, it may cause broken storages.;;;","28/Mar/18 10:11 PM;VladimirWork;Build Info:
indy-node 1.3.353

Things tried during regression testing:

1. With leveldb:
1.1. Installation.
1.2. Upgrade.
1.3. Txns of various type sending.
1.4. Load testing.

2. With rocksdb:
2.1. Switching to rocksdb.
2.2. Txns of various type sending.
2.3. Logs/journalctl/data files checking.

Full acceptance testing of rocksdb build will be done with new RC.;;;",,,,,,,,,,,,,,,,,,,,,,,
Wrong count of backup replicas on part of nodes after nodes adding,INDY-1206,28186,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,02/Mar/18 7:46 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"Steps to Validate:
1. Set up the pool of 7 nodes.
2. Write about 40 transactions to the pool.
3. One by one, add nodes 8 - 16. (After each node adding check ledger size to make sure that catch up completed before adding next node.)

Actual Results:
16 node can't be added, pool don't write transactions.
Primaries on nodes 11 and 12:
{code:java}
ubuntu@ohioQALarge11:~$ grep ""selected primary"" /var/log/indy/sandbox/Node11.log 
2018-03-01 18:28:07,012 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node11:0 selected primary Node1:0 for instance 0 (view 0)
2018-03-01 18:28:22,292 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node11:0 selected primary Node1:0 for instance 0 (view 0)
2018-03-01 18:28:22,293 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node11:1 selected primary Node2:1 for instance 1 (view 0)
2018-03-01 18:28:22,294 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node11:2 selected primary Node3:2 for instance 2 (view 0)
2018-03-01 18:28:22,295 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node11:3 selected primary Node4:3 for instance 3 (view 0){code}

Primary on the rest nodes:
{code:java}
ubuntu@seoulQALarge8:~$ grep ""selected primary"" /var/log/indy/sandbox/Node8.log 
2018-03-01 18:26:08,137 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node8:0 selected primary Node1:0 for instance 0 (view 0)
2018-03-01 18:26:08,138 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node8:1 selected primary Node2:1 for instance 1 (view 0)
2018-03-01 18:26:08,139 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node8:2 selected primary Node3:2 for instance 2 (view 0)
2018-03-01 18:27:58,483 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node8:3 selected primary Node4:3 for instance 3 (view 0)
2018-03-01 18:28:51,229 | DISPLAY | node.py (2396) | select_primaries | PRIMARY SELECTION: Node8:4 selected primary Node5:4 for instance 4 (view 0){code}

Expected Results:
Pool should work.

Logs: https://drive.google.com/file/d/1R_9v-nfeh6-bgS8HX8q_4eUYrMjFhmH5/view?usp=sharing",indy-node 1.3.324,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz3uf:",,,,,,18.06,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,anikitinDSR,ozheregelya,zhigunenko.dsr,,,,,,,,,,"27/Mar/18 12:23 AM;anikitinDSR;Problem reason:
- wrong count of backup replicas

Changes:
- now, node would be joined only if node transaction was ordered (pool ledger is synced) or after completed catchup.

PR:
https://github.com/hyperledger/indy-node/pull/627

Version:
1.3.353-master

Covered with test:
https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/primary_selection/test_add_node_with_f_changed.py

Recommendations for QA:
 1. Set up pool of 7 nodes
 2. Write about 40 transactions
 3. One by one, add nodes 8-16
 4. Check, that all nodes have expected count of replicas
;;;","02/Apr/18 6:36 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.357
indy-plenum 1.2.299
libindy 1.3.1~441

*Steps to Validate:*
1. Set up the pool of 7 nodes.
2. Write about 40 transactions to the pool.
3. One by one, add nodes 8 - 16. (After each node adding check ledger size to make sure that catch up completed before adding next node.)

*Actual Results:*
Pool is working. NYMs are written. All nodes have the same list of backup replicas;;;",,,,,,,,,,,,,,,,,,,,,,,
Add Docker getting started configuration,INDY-1207,28196,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,brycecurtis,brycecurtis,brycecurtis,03/Mar/18 3:19 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,Documentation,,,"Add a turnkey, Docker-based sandbox that enables quick and easy exploration of Hyperledger Indy concepts. This devops repo can be used to gather hands-on experience of Indy basics using the scenarios outlined in the [Sovrin's Getting Started Guide|https://github.com/hyperledger/indy-node/blob/stable/getting-started.md].

This is the implementation described by placeholder [getting_started_turnkey|https://github.com/hyperledger/indy-node/tree/master/environment/docker/getting_started_turnkey]
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwxcv:",,,,,,,,,,,,,,,,,,,,,,,,,,brycecurtis,esplinr,,,,,,,,,,,"13/Sep/18 12:23 AM;esplinr;[~brycecurtis] You assigned this issue to yourself. Are you still working on it?;;;","13/Sep/18 12:32 AM;brycecurtis;This is completed and can be closed.;;;","20/Sep/18 10:08 PM;esplinr;Thank you for the contribution [~brycecurtis]!;;;",,,,,,,,,,,,,,,,,,,,,,
[QA] Load and performance testing,INDY-1208,28213,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,05/Mar/18 8:21 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,explore,GA-0,,"From conversation with [~krw910]:

I had a discussion last week with Nathan and Steve Tolman around scale and performance. When you can get back to scale testing we need the following questions answered:
1 - We need to find our failure point. What I mean by that is how many transactions can we write to the ledger before it just stops taking transactions due to its size. We don't have to write transactions as fast as we can we just need to know we stop functioning if the ledger hits a certain size.
2 - How much can you front load through the genesis file? This is what was tried before by having a domain_transactions_genesis file with 10,000 NYMs in it. If we can front load the pool by loading up the genesis file it would make it faster to get a large ledger (in theory)
3 - Can we get to 10 million transactions on the ledger? (We have logigear working on some scripts that will help in this area. I will get more details to you on those scripts early next week.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1235,INDY-1216,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz6yf:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,5.0,,,,,,,,,,,,ozheregelya,,,,,,,,,,,,"10/May/18 4:01 AM;ozheregelya;These test cases were moved to Performance list of Acceptance spreadsheet (""Things to be tried"" section): 
https://docs.google.com/spreadsheets/d/1OVjua8JMwW7RhBWsdd9vSfGvJkgxWxKhEjB3T0yil1U/edit?pli=1#gid=1727037395

All load and performance testing is tracked in this spreadsheet, so this ticket is unnecessary.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Raise to main node_request and pool_transactions fixtures and helper functions,INDY-1209,28241,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,Derashe,Derashe,06/Mar/18 6:00 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,Raise to main node_request and pool_transactions fixtures and helper functions.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-875,,,,,,,,,"1|hzz61j:",,,,,,Sprint 18.05,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,5.0,,,,,,,,,,,,ashcherbakov,Derashe,,,,,,,,,,,"25/Apr/18 9:49 PM;ashcherbakov;PR: https://github.com/hyperledger/indy-plenum/pull/643;;;","26/Apr/18 1:19 AM;Derashe;Problem reason:
We have old client's code and old functions

Changes:
 * Deletion of old client's code
 * Raising up some of functions/fixtures in helper/conftest hierarchy

PR:
 [https://github.com/hyperledger/indy-plenum/pull/643]

Risk factors:
 No

Risk:
 Low;;;",,,,,,,,,,,,,,,,,,,,,,,
Pool lose quorum after numerous connection problem,INDY-1210,28264,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,zhigunenko.dsr,zhigunenko.dsr,07/Mar/18 10:52 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,GA-0,,,"All cases have been reproduced in Docker with 7 nodes
X = node3, Y = node6

*Steps to reproduce:*
 1. Write some NYMs
 2. Disconnect node which follows the primary instance 2 (nodeY)
 3. Disconnect current primary (nodeX)
 4. Try to write some NYMs
 5. Connect nodeY back
 6. Connect nodeX back

*Expected results:*
 - nodeY returned back to pool successfully
 - viewChange completed successfully when (n-f+1) node returned back
 - nodeX returned back to pool successfully

*Actual results:*
 - pool lost its quorum and cannot restore it
 - all nodes (exclude X and Y) assume that X and Y are malicious
 - nodeX cannot connect to all the others","indy-anoncreds 1.0.32
indy-node 1.3.324
indy-plenum 1.2.259
libindy 1.3.1~406",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1203,,,,,INDY-1258,,,,,,,,,,,,,,,"30/Mar/18 5:09 PM;zhigunenko.dsr;1210-2.7z;https://jira.hyperledger.org/secure/attachment/14832/1210-2.7z","07/Mar/18 10:50 PM;zhigunenko.dsr;disc-2.7z;https://jira.hyperledger.org/secure/attachment/14736/disc-2.7z","02/Apr/18 11:25 PM;zhigunenko.dsr;suppressor.sh;https://jira.hyperledger.org/secure/attachment/14845/suppressor.sh","29/Mar/18 10:44 PM;zhigunenko.dsr;suppressor.sh;https://jira.hyperledger.org/secure/attachment/14830/suppressor.sh",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz3un:",,,,,,18.06,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,anikitinDSR,zhigunenko.dsr,,,,,,,,,,,"28/Mar/18 8:26 PM;anikitinDSR;Problem reason: 
- When n-f node start view change (on primary loss) pool can lose quorum

Changes: 
- added resend InstanceChange messages

PR:
- https://github.com/hyperledger/indy-plenum/pull/594

Version:
- 1.3.354-master

Risk:
- Low

Covered with tests:
- https://github.com/hyperledger/indy-plenum/blob/1.2.299-master/plenum/test/view_change/test_resend_instance_change_messages.py

Recommendations for QA
- Set up pool of 7 nodes
- disconnect not primary for all replica node
- disconnect primary
- check, that view change completed
- check that pool works normally;;;","29/Mar/18 10:47 PM;zhigunenko.dsr;*Environment:*
 indy-node 1.3.354
 indy-plenum 1.2.299
 libindy 1.3.1~439

*Case 1:*
 Disconnect primary node only (7 nodes in pool)
 *Actual results:*
 view change completed, pool works normally, node reconnected successfully

*Case 2:*
 Disconnect the last of backup instance and then disconnect primary node (7 nodes in pool)
 *Actual results:*
 view change completed, pool works normally, node reconnected successfully
 *Additional info:*

*Case 3:*
 Disconnect node which following up the last of backup instance and then disconnect primary node (7 nodes in pool)
 *Actual results:*
 view change completed, pool works normally, node reconnected successfully

*Case 4:*
 1) create pool with 15 nodes
 2) apply [^suppressor.sh] for node1, node2, node3, node7, node13 (started with $S2=6sec delay, not in the single moment)
 3) start load test 100threads*100txns
 *Actual results:*
 Pool collapsed after 1164 txns, load test failed with 307 error. Consensus was not reached after suppressed nodes reconnection

[Logs|https://drive.google.com/open?id=13ifBNFC0F3pgl6jLNxKHqCTKp_e4yW4S];;;","30/Mar/18 5:09 PM;zhigunenko.dsr;*Case 5:*
Disconnect node which following up the last of backup instance and then disconnect primary node (15 nodes in pool)
*Actual results:*
view change completed, pool works normally, but reconnected nodes are both unreachable[^1210-2.7z];;;","30/Mar/18 8:17 PM;anikitinDSR;Recheck case 4 and case 5 without ""docker network disconnect"".
""docker network disconnect"" will recreate network interface, that's not ""cleanly"".
Instead of docker way, try to use ""ifconfig <network name> down"" for disconnect emulating.
Steps:
- run containers in ""privileged"" mode (--privileged)
- inside selected node's containers (which should be disconnected) run :
    - install ""net-utils"" package
    - ifconfig <network name> down (usually network name is ""eth0"");;;","02/Apr/18 11:34 PM;zhigunenko.dsr;*Steps to reproduce (case4):*
 1) create pool with 15 nodes (privileged mode)
 2) apply [^suppressor.sh] (updated) for node1, node2, node3, node7, node13 (started with $S2=1.97..2.02 sec delay, not in the single moment)
 3) start load test 100threads*100txns
 *Actual results:*
 Pool finished with 5072 txns, but node1, node2, node3 have only 592 txns. All nodes shows each of others as reachable.
 After connection reestablishing these nodes couldn't catch up with the pool (without or with node restarting)

[Logs|https://drive.google.com/file/d/1lGtyB4X3tub3STMhjFGwY055RwYVDotT/view?usp=sharing];;;","04/Apr/18 7:26 PM;zhigunenko.dsr;*Reason for closing:*
Basic case has been fixed. Additional case has been moved in INDY-1258;;;",,,,,,,,,,,,,,,,,,,
Docs Folder Restructure/Clean-up,INDY-1211,28318,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,TechWritingWhiz,TechWritingWhiz,09/Mar/18 5:49 AM,23/Jun/18 12:47 AM,28/Oct/23 2:47 AM,,,,,,,0,Documentation,,,"It was discussed with more than one engineer to come up with a strategy to clean up our documents folders in our indy repos.

The folder structure for all of the indy repos should look like: 

indy-sdk

  /docs

     /design

     /tutorials

        /getting-started

        /other document....

        /other document...

     /how-tos

   /samples


Within the *docs* folder, there should be a folder for: 
 * design
 * tutorials
 * how-tos

The Getting Started Guides should be moved to the ""tutorials"" folder.

Each document within each folder should have it's OWN FOLDER. Inside of that document folder, should be the actual document along with any other supporting files that document contains.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwyjj:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,TechWritingWhiz,,,,,,,,,,,"21/Mar/18 5:22 AM;TechWritingWhiz;Second time around: Part of this has now been done to indy-sdk repo: https://github.com/hyperledger/indy-sdk/pull/583;;;","21/Apr/18 6:20 AM;esplinr;As I have been digging into this project, I see some challenges with keeping the consumer documentation in the same repository as the code:
 * It makes it hard to release the documentation on a different cycle than the code (the documentation often trails the release of the product it documents, and is often corrected and updated later).
 * It makes it hard to publish the documentation to a documentation platform like Read-the-Docs, which is used by most Hyperledger projects.
 * It makes it hard for people to find the information in the documentation via a Google Search, whereas publishing to Read-the-Docs makes this easier.

By keeping documentation in a separate repository, we address these concerns. Consequences:
 * This could be a singly indy-docs repo, or a docs repo for each Indy repo (indy-sdk-repo).
 * The docs repo would also contain the release notes, which allows them to be updated out-of-band with the release.
 * The docs repo would also contain RFCs, which would replace the currently unused indy-rfc repo.

In this discussion, I am differentiating the documentation referenced by those using the project from the documentation used by the people working to improve the project (even though both groups are ""developers""). Design and architecture documents and diagrams that are targeted to assist those improving the project should remain in the same repository as the code.

Let's make sure to have a conversation about this ticket before doing the work.;;;",,,,,,,,,,,,,,,,,,,,,,,
indy-node process crashed without terminating the indy-node service,INDY-1212,28325,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,09/Mar/18 8:32 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"A steward reports that his node, esatus_AG, says that indy-node is up (as reported by validator-info and systemctl status), but it is actually crashed.  The other nodes are unable to connect to it.  Based on the symptoms, this may have occurred on other nodes as well. This is what he has provided us:


{code:java}
$ sudo systemctl status indy-node

indy-node.service - Indy Node
   Loaded: loaded (/etc/systemd/system/indy-node.service; enabled; vendor preset: enabled)

   Active: active (running) since Do 2018-02-08 07:24:46 CET; 3 weeks 5 days ago

Main PID: 1011 (python3)

    Tasks: 4

   Memory: 67.7M

      CPU: 13h 52min 48.816s

   CGroup: /system.slice/indy-node.service

           └─1011 python3 -O /usr/local/bin/start_indy_node esatus_AG 9700 9710

 

Feb 10 02:02:46 esatusValidatorNode env[1011]:     sum(replica.serviceQueues(limit) for replica in self._replicas)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 768, in serviceQueues

Feb 10 02:02:46 esatusValidatorNode env[1011]:     r += self._serviceActions()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 100, in _serviceActions

Feb 10 02:02:46 esatusValidatorNode env[1011]:     action()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 110, in wrapper

Feb 10 02:02:46 esatusValidatorNode env[1011]:     action()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1535, in process_stashed_out_of_order_commits

Feb 10 02:02:46 esatusValidatorNode env[1011]:     self, self.stashed_out_of_order_commits[v], v, lastOrdered))

Feb 10 02:02:46 esatusValidatorNode env[1011]: RuntimeError: esatus_AG:0 found commits {2: COMMIT{'viewNo': 0, 'ppSeqNo': 2, 'instId': 0}} from previous view 0 that were not ordered but last ordered is (114, 2)
{code}
I asked for the relevant part of journalctl and he provided: 
{code:java}
Feb 10 02:02:46 esatusValidatorNode env[1011]: Traceback (most recent call last):

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/bin/start_indy_node"", line 18, in <module>

Feb 10 02:02:46 esatusValidatorNode env[1011]:     run_node(config, self_name, int(sys.argv[2]), int(sys.argv[3]))

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 33, in run_node

Feb 10 02:02:46 esatusValidatorNode env[1011]:     looper.run()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 289, in __exit__

Feb 10 02:02:46 esatusValidatorNode env[1011]:     self.shutdownSync()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 285, in shutdownSync

Feb 10 02:02:46 esatusValidatorNode env[1011]:     self.loop.run_until_complete(self.shutdown())

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete

Feb 10 02:02:46 esatusValidatorNode env[1011]:     return future.result()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result

Feb 10 02:02:46 esatusValidatorNode env[1011]:     raise self._exception

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step

Feb 10 02:02:46 esatusValidatorNode env[1011]:     result = coro.send(None)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 275, in shutdown

Feb 10 02:02:46 esatusValidatorNode env[1011]:     await self.runFut

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 363, in __iter__

Feb 10 02:02:46 esatusValidatorNode env[1011]:     return self.result()  # May raise too.

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result

Feb 10 02:02:46 esatusValidatorNode env[1011]:     raise self._exception

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 33, in run_node

Feb 10 02:02:46 esatusValidatorNode env[1011]:     looper.run()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 259, in run

Feb 10 02:02:46 esatusValidatorNode env[1011]:     return self.loop.run_until_complete(what)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete

Feb 10 02:02:46 esatusValidatorNode env[1011]:     return future.result()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result

Feb 10 02:02:46 esatusValidatorNode env[1011]:     raise self._exception

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step

Feb 10 02:02:46 esatusValidatorNode env[1011]:     result = coro.send(None)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 222, in runForever

Feb 10 02:02:46 esatusValidatorNode env[1011]:     await self.runOnceNicely()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 205, in runOnceNicely

Feb 10 02:02:46 esatusValidatorNode env[1011]:     msgsProcessed = await self.prodAllOnce()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 150, in prodAllOnce

Feb 10 02:02:46 esatusValidatorNode env[1011]:     s += await n.prod(limit)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 373, in prod

Feb 10 02:02:46 esatusValidatorNode env[1011]:     c = await super().prod(limit)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 800, in prod

Feb 10 02:02:46 esatusValidatorNode env[1011]:     c += await self.serviceReplicas(limit)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 821, in serviceReplicas

Feb 10 02:02:46 esatusValidatorNode env[1011]:     inbox_processed = self.replicas.service_inboxes(limit)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 70, in service_inboxes

Feb 10 02:02:46 esatusValidatorNode env[1011]:     sum(replica.serviceQueues(limit) for replica in self._replicas)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 70, in <genexpr>

Feb 10 02:02:46 esatusValidatorNode env[1011]:     sum(replica.serviceQueues(limit) for replica in self._replicas)

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 768, in serviceQueues

Feb 10 02:02:46 esatusValidatorNode env[1011]:     r += self._serviceActions()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 100, in _serviceActions

Feb 10 02:02:46 esatusValidatorNode env[1011]:     action()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 110, in wrapper

Feb 10 02:02:46 esatusValidatorNode env[1011]:     action()

Feb 10 02:02:46 esatusValidatorNode env[1011]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1535, in process_stashed_out_of_order_commits

Feb 10 02:02:46 esatusValidatorNode env[1011]:     self, self.stashed_out_of_order_commits[v], v, lastOrdered))

Feb 10 02:02:46 esatusValidatorNode env[1011]: RuntimeError: esatus_AG:0 found commits {2: COMMIT{'viewNo': 0, 'ppSeqNo': 2, 'instId': 0}} from previous view 0 that were not ordered but last ordered is (114, 2)
{code}
He says there is nothing more that is relevant.  The corresponding part of the indy log is:
{code:java}
2018-02-10 02:02:45,643 | INFO     | node.py              ( 532) | start_participating | esatus_AG started participating

2018-02-10 02:02:45,644 | DISPLAY  | primary_selector.py  ( 351) | _start_selection | VIEW CHANGE: esatus_AG:0 declares view change 114 as completed for instance 0, new primary is danube:0, ledger info is [(0, 32, '61KhLvvLcfu1XRbuWLABt$

2018-02-10 02:02:45,644 | DISPLAY  | primary_selector.py  ( 327) | _start_selection | PRIMARY SELECTION: esatus_AG:1 selected primary royal_sovrin:1 for instance 1 (view 114)

2018-02-10 02:02:45,645 | DISPLAY  | primary_selector.py  ( 351) | _start_selection | VIEW CHANGE: esatus_AG:1 declares view change 114 as completed for instance 1, new primary is royal_sovrin:1, ledger info is [(0, 32, '61KhLvvLcfu1XRb$

2018-02-10 02:02:45,645 | DISPLAY  | primary_selector.py  ( 327) | _start_selection | PRIMARY SELECTION: esatus_AG:2 selected primary digitalbazaar:2 for instance 2 (view 114)

2018-02-10 02:02:45,646 | DISPLAY  | primary_selector.py  ( 351) | _start_selection | VIEW CHANGE: esatus_AG:2 declares view change 114 as completed for instance 2, new primary is digitalbazaar:2, ledger info is [(0, 32, '61KhLvvLcfu1XR$

2018-02-10 02:02:45,646 | DISPLAY  | primary_selector.py  ( 327) | _start_selection | PRIMARY SELECTION: esatus_AG:3 selected primary OASFCU:3 for instance 3 (view 114)

2018-02-10 02:02:45,647 | DISPLAY  | primary_selector.py  ( 351) | _start_selection | VIEW CHANGE: esatus_AG:3 declares view change 114 as completed for instance 3, new primary is OASFCU:3, ledger info is [(0, 32, '61KhLvvLcfu1XRbuWLABt$

2018-02-10 02:02:45,647 | DISPLAY  | primary_selector.py  ( 327) | _start_selection | PRIMARY SELECTION: esatus_AG:4 selected primary BIGAWSUSEAST1-001:4 for instance 4 (view 114)

2018-02-10 02:02:45,647 | DISPLAY  | primary_selector.py  ( 351) | _start_selection | VIEW CHANGE: esatus_AG:4 declares view change 114 as completed for instance 4, new primary is BIGAWSUSEAST1-001:4, ledger info is [(0, 32, '61KhLvvLcf$

2018-02-10 02:02:45,928 | WARNING  | node.py              (2591) | reportSuspiciousNode | esatus_AG raised suspicion on node ServerVS for Commit message has invalid BLS signature; suspicion code is 31

2018-02-10 02:02:46,037 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:2 ordered batch request, view no 114, ppSeqNo 1, ledger 0, state root None, txn root None, requests ordered [('J4N1K1SEB8uY2muwmecY5q', 1518208$

2018-02-10 02:02:46,038 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:2 ordered batch request, view no 114, ppSeqNo 2, ledger 1, state root None, txn root None, requests ordered [('J4N1K1SEB8uY2muwmecY5q', 1518102$

2018-02-10 02:02:46,038 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:3 ordered batch request, view no 114, ppSeqNo 1, ledger 0, state root None, txn root None, requests ordered [('J4N1K1SEB8uY2muwmecY5q', 1518208$

2018-02-10 02:02:46,039 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:3 ordered batch request, view no 114, ppSeqNo 2, ledger 1, state root None, txn root None, requests ordered [('J4N1K1SEB8uY2muwmecY5q', 1518102$

2018-02-10 02:02:46,050 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:0 ordered batch request, view no 114, ppSeqNo 1, ledger 0, state root 8YY6MQEPTEovN3YPuihAhvdCga4eXiwFGP8p1Skh5RvU, txn root BafagSgueFyWwzgUYo$

2018-02-10 02:02:46,073 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:0 ordered batch request, view no 114, ppSeqNo 2, ledger 1, state root 2ErAr1SDd8K3qREkCWMKCzdsRwZp7aCeCoQMFs8MrzsT, txn root 2b61e2wdgufCSwyzB9$

2018-02-10 02:02:46,076 | INFO     | node.py              (2374) | executeBatch | esatus_AG committed batch request, view no 114, ppSeqNo 1, ledger 0, state root 8YY6MQEPTEovN3YPuihAhvdCga4eXiwFGP8p1Skh5RvU, txn root BafagSgueFyWwzgUYoc$

2018-02-10 02:02:46,078 | INFO     | node.py              (2374) | executeBatch | esatus_AG committed batch request, view no 114, ppSeqNo 2, ledger 1, state root 2ErAr1SDd8K3qREkCWMKCzdsRwZp7aCeCoQMFs8MrzsT, txn root 2b61e2wdgufCSwyzB9C$

2018-02-10 02:02:46,148 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:4 ordered batch request, view no 114, ppSeqNo 1, ledger 0, state root None, txn root None, requests ordered [('J4N1K1SEB8uY2muwmecY5q', 1518208$

2018-02-10 02:02:46,148 | INFO     | replica.py           (1619) | order_3pc_key | esatus_AG:4 ordered batch request, view no 114, ppSeqNo 2, ledger 1, state root None, txn root None, requests ordered [('J4N1K1SEB8uY2muwmecY5q', 1518102$

2018-02-10 02:02:46,262 | INFO     | looper.py            ( 272) | shutdown | Looper shutting down now...
{code}",Provisional network running 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0wf:",,,,,,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,,,,,,,,,,,"13/Mar/18 4:57 PM;ashcherbakov;This looks like already fixed in INDY-1152;;;","13/Mar/18 5:19 PM;ashcherbakov;We need to Upgrade the pool and validate whether the issue has gone.;;;","14/Mar/18 3:25 AM;mgbailey;[~ashcherbakov], I don't see INDY-1152 in the release notes for 1.3.55, so I don't think it will be fixed in this upgrade, will it?;;;","14/Mar/18 4:59 PM;ashcherbakov;[~mgbailey] It's fixed in the latest stable 1.3.55. Probably it was forgotten to add it into the release notes...;;;","27/Mar/18 11:38 PM;ashcherbakov;We can reopen any that reappear, but all is good on the live pool now.;;;",,,,,,,,,,,,,,,,,,,,
Duplicate transactions written to a node's ledger on provisional network,INDY-1213,28326,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,09/Mar/18 8:41 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"One validator had additional transactions written to its ledger, which were not on the ledgers of the other validators in the provisional pool.  These transactions appear to be duplicates of transactions that were previously on the pool.  The time stamps on the excess transactions are all the same, at a time on Feb. 2nd.  Unfortunately this is long enough ago that there are no logs available to give additional context.  However, a problem like this was seen with the metis node some time ago on the STN.  We need to confirm a the fix was correctly implemented.

Attached are ledgers for zaValidator, the node that experienced the issue.  In addition, ledgers for ev1, which is in sync with the rest of the pool, are included for reference.

Note that in this case, the ledger upgraded several days before the time stamp on the duplicate transactions.",Provisional network running 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-893,,,,,,,,,,,,,,,,,,,,"09/Mar/18 8:40 AM;mgbailey;ev1_domain_ledger.txt;https://jira.hyperledger.org/secure/attachment/14746/ev1_domain_ledger.txt","09/Mar/18 8:40 AM;mgbailey;ev1_pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/14747/ev1_pool_ledger.txt","09/Mar/18 8:40 AM;mgbailey;zaValidator_domain_ledger.txt;https://jira.hyperledger.org/secure/attachment/14749/zaValidator_domain_ledger.txt","09/Mar/18 8:40 AM;mgbailey;zaValidator_pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/14748/zaValidator_pool_ledger.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0w7:",,,,,,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,,,,,,,,,,,"13/Mar/18 5:18 PM;ashcherbakov;I believe the problem with duplicated transactions was fixed in INDY-1045 and INDY-1047.
Some related fixes are going to be also implemented in the scope of INDY-1121;;;","13/Mar/18 5:18 PM;ashcherbakov;We need to Upgrade the pool and validate whether the issue has gone.;;;","27/Mar/18 10:48 PM;ashcherbakov;We can reopen any that reappear, but all is good on the live pool now.;;;",,,,,,,,,,,,,,,,,,,,,,
"Indy-node process is running, but the node and client ports have no bindings",INDY-1214,28359,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,mgbailey,mgbailey,10/Mar/18 3:00 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"In the provisional network, a steward had a node that other nodes were unable to connect to, in spite of validator-info indicating that the node was operating normally.  There were no signs of problems in his indy logs, other than there being no new logs being written at all. He restarted the indy-node service with no improvement.  The only sign of the problem was in journalctl, which is attached.

After a lot of searching and not finding the problem, it was resolved when we again stopped the service to increase the log level verbosity.  When we brought the service back up, it resumed normal operation.

Please examine the attached journalctl stack trace, and see if there is an exception that could have been caught or other remediation that can be done to prevent this in the future.",Provisional network running 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Mar/18 2:59 AM;mgbailey;ServerVS_stack_trace.txt;https://jira.hyperledger.org/secure/attachment/14751/ServerVS_stack_trace.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0vz:",,,,,,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,,,,,,,,,,,"13/Mar/18 4:59 PM;ashcherbakov;This looks like already fixed in INDY-1152;;;","13/Mar/18 5:19 PM;ashcherbakov;We need to Upgrade the pool and validate whether the issue has gone.;;;","27/Mar/18 11:37 PM;ashcherbakov;We can reopen any that reappear, but all is good on the live pool now.;;;",,,,,,,,,,,,,,,,,,,,,,
Explore indy-cli `ledger custom` command,INDY-1215,28414,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,12/Mar/18 6:13 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Explore indy-cli `ledger custom` command:
- non-UTF/special characters (/)
- fields' size constraints (/)
- invalid json structure (/)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/18 7:23 PM;VladimirWork;cli-errors.PNG;https://jira.hyperledger.org/secure/attachment/14801/cli-errors.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz0xr:",,,,,,18.06,,,,,,,,3.0,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"20/Mar/18 7:27 PM;VladimirWork;Build Info:
indy-node 1.3.336
indy-cli 1.3.1~410

Things tried:
1. Non-UTF/special characters in JSON attribute names\values.
2. Fields' size constraints.
3. Invalid json structure and attributes.

Actual Results:
Indy-cli validation blocks `ledger custom` with bad data and throws readable errors. !cli-errors.PNG|thumbnail! 

FYI [~krw910];;;",,,,,,,,,,,,,,,,,,,,,,,,
[QA] Find out Stability Boundaries (number of clients and nodes for stable work),INDY-1216,28415,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,12/Mar/18 9:35 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,explore,Stability,,"The goals are

1) To find out the 'stability boundaries', that is how many treads (clients) and nodes in the pool we may have for a correct and stable work and when we start facing stability issues.

2) Measure performance based on the number of clients and nodes in the pool.

 

*Things to be tested:*
 1. Set up the large pool.
 2. Run load test with 1 thread writing.
 3. Reset the pool.
 4. Run load test with greater amount of threads.
 5. Repeat steps 3-4 until problems like INDY-1180 will reproduce.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1180,INDY-1208,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz3uv:",,,,,,Sprint 18.05,18.06,18.07 Stability & Monitoring,,,,,,5.0,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"29/Mar/18 6:23 PM;ashcherbakov;A first run of tests is run, multiple issues were found and fixed in the scope of INDY-1180.;;;","29/Mar/18 7:31 PM;ashcherbakov;Further testing will be done in the scope of INDY-1208.;;;",,,,,,,,,,,,,,,,,,,,,,,
Clear_node script issues ,INDY-1217,28421,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,ozheregelya,ozheregelya,12/Mar/18 10:53 PM,24/Apr/18 11:43 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*Case 1:*
Backup can't be removed.
*Steps to Reproduce:*
1. Setup the pool with 1.2.50 version.
2. Upgrade it to 1.3.55 version.
3. Run the clear_node as indy user:
sudo su - indy -c ""clear_node.py --full FULL""

*Actual Results:*
{code:java}
ubuntu@canadaQALive2:~$ sudo su - indy -c ""clear_node.py --full FULL""
Traceback (most recent call last):
 File ""/usr/local/bin/clear_node.py"", line 30, in <module>
 clean(config, args.full, args.network)
 File ""/usr/local/bin/clear_node.py"", line 20, in clean
 shutil.rmtree(config_helper.ledger_base_dir)
 File ""/usr/lib/python3.5/shutil.py"", line 474, in rmtree
 _rmtree_safe_fd(fd, path, onerror)
 File ""/usr/lib/python3.5/shutil.py"", line 412, in _rmtree_safe_fd
 _rmtree_safe_fd(dirfd, fullname, onerror)
 File ""/usr/lib/python3.5/shutil.py"", line 432, in _rmtree_safe_fd
 onerror(os.unlink, fullname, sys.exc_info())
 File ""/usr/lib/python3.5/shutil.py"", line 430, in _rmtree_safe_fd
 os.unlink(name, dir_fd=topfd)
PermissionError: [Errno 13] Permission denied: 'sandbox_backup_1.2.50.zip'{code}
Expected Results:
Backup should be removed.

*Case 2:*
Script fails if any directory doesn't exist.
Steps to Reproduce:
1. After Case 1 change owner of sandbox_backup_1.2.50.zip to indy.
2. Run clear_node again.

Actual Results:
{code:java}
root@77785ea1d587:/home/indy# sudo su - indy -c ""clear_node.py --full FULL""
Traceback (most recent call last):
 File ""/usr/local/bin/clear_node.py"", line 30, in <module>
 clean(config, args.full, args.network)
 File ""/usr/local/bin/clear_node.py"", line 15, in clean
 shutil.rmtree(config_helper.log_dir)
 File ""/usr/lib/python3.5/shutil.py"", line 465, in rmtree
 onerror(os.lstat, path, sys.exc_info())
 File ""/usr/lib/python3.5/shutil.py"", line 463, in rmtree
 orig_st = os.lstat(path)
FileNotFoundError: [Errno 2] No such file or directory: '/var/log/indy/sandbox'{code}
 

Expected Result:
Not existing directories should be ignored. Ideally, if some files/directories can't be removed, need to show this information to user and try to remove the rest files/directories.

*Case 3 (need to discuss):*
Configs remove.

From help:

 
{code:java}
root@77785ea1d587:/home/indy# sudo su - indy -c ""clear_node.py --help""
usage: clear_node.py [-h] [--full FULL] [--network NETWORK]
Removes node data and configuration
optional arguments:
 -h, --help show this help message and exit
 --full FULL remove configs and logs too
 --network NETWORK Network to clean 
{code}
> remove configs and logs too
In fact, we don't remove configs in /etc/indy. 
Should we remove general configs in scope of node cleaning?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzz0s7:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Generate_indy_pool transactions failed because of absence some packages (vagrant installation),INDY-1218,28436,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,13/Mar/18 2:10 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"From [https://forum.sovrin.org/t/error-when-running-vagrant-up/582] :
!1111.png|thumbnail!",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/18 2:09 AM;ozheregelya;1111.png;https://jira.hyperledger.org/secure/attachment/14752/1111.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzyzzj:",,,,,,18.06,,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,ozheregelya,,,,,,,,,,"21/Mar/18 9:57 PM;ashcherbakov;It should beed fixed once [https://github.com/hyperledger/indy-node/pull/621] is merged.;;;","21/Mar/18 11:52 PM;ashcherbakov;We need to fix it at the node dependency level, not in vagrant files only.;;;","26/Mar/18 11:34 PM;anikitinDSR;Problem reason: 
- need to add libsodium18 as indy-node dependency

Changes: 
- added to indy-node dependency

PR:
- https://github.com/hyperledger/indy-node/pull/625

Version:
- 1.3.352-master

Recommendations for QA
- on ""clean machine without libsodium18 installed"" install indy-node package and check, that libsodium installed;;;","28/Mar/18 12:09 AM;ozheregelya;Environment:
 indy-node 1.3.353
 Ubuntu VM, Vagrant VMs, Docker.

Case 1:
 Steps to Validate:
 0. Make sure that libsodium was not installed on your environment before.
{code:java}
me@me-VB:~/perf$ dpkg -l | grep libsodium{code}
1. Install the indy-node to the clear environment.
 2. Check that libsodium was installed.
 3. Check that libsodium was installed ac dependency of indy-node.

Actual Results:
{code:java}
me@me-VB:~/perf$ dpkg -l | grep libsodium
ii libsodium18:amd64 1.0.8-5 amd64 Network communication, cryptography and signaturing library
ii python3-libnacl 1.6.1 amd64 Python bindings for libsodium based on ctypes
me@me-VB:~/perf$ aptitude why libsodium18
i indy-node Depends libsodium18{code}
Case 2:
 Steps to Validate:
 1. Setup vagrant environment.
 2. Make sure that vagrant pool works.

Actual Results:
Pool basically works.

Case 3:
 Steps to Validate:
 1. Setup docker environment.
 2. Make sure that docker pool works.

Actual Results:
 Pool basically works.;;;",,,,,,,,,,,,,,,,,,,,,
validator-info and read_ledger give inconsistent responses in node on provisional,INDY-1219,28437,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ozheregelya,mgbailey,mgbailey,13/Mar/18 2:19 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,validator-info,,,0,GA-0,,,"T-Shirt: S

In trying to repair the provisional network, we have a node where the validator-info tool gives a different response for the state of the pool and the domain ledgers than the read_ledger tool does. Because of this, we are unable to determine if the ledger is in sync with the network, and we have no confidence in the state of the validator.

We urgently need to get the network into an operational state, so we need effort to get this node operational. We have restarted the service, and the results have not changed. We have deleted the validator-info.log file and verified that a fresh one was written after the restart.

Attached are files containing the outputs of validator-info, read_ledger for the domain and pool ledgers, and the indy log.  ","Steward node on the Provisional network, running 1.2.50",,,,,,,,,INDY-1205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/18 2:18 AM;mgbailey;log.tar;https://jira.hyperledger.org/secure/attachment/14756/log.tar","13/Mar/18 2:18 AM;mgbailey;read_ledger_config.out;https://jira.hyperledger.org/secure/attachment/14753/read_ledger_config.out","13/Mar/18 2:18 AM;mgbailey;read_ledger_domain.out;https://jira.hyperledger.org/secure/attachment/14754/read_ledger_domain.out","13/Mar/18 2:18 AM;mgbailey;read_ledger_pool.out;https://jira.hyperledger.org/secure/attachment/14755/read_ledger_pool.out","13/Mar/18 2:18 AM;mgbailey;validator_info.out;https://jira.hyperledger.org/secure/attachment/14757/validator_info.out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz9vr:",,,,,,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,,ashcherbakov,mgbailey,ozheregelya,Toktar,,,,,,,,,"13/Mar/18 9:30 PM;Toktar;The reason of the problem is that _read_ledger_ script creates a tmp copy of the ledger, since leveldb can not work with the db from multiple processes (even in read-only mode).
 So, if the copy is not deleted for some reasons, the results of _read_ledger_ may be outdated.
 This problem will be fixed when task INDY-1205 (Use RocksDB as a key-value storage) is merged, since RocksDB can work (in read-only mode) with a db from a different process. So, the new version of read_ledger script will not create a copy of the ledger.;;;","14/Mar/18 3:01 AM;mgbailey;[~Toktar], what is the work-around?  Do I delete the temporary copy before running read_ledger?  Where is the copy?;;;","16/Mar/18 1:54 AM;mgbailey;From [~Toktar]:
""Cache are saved in '/var/lib/indy/data' in file with postfix '-read-copy'. Deleting this file should fix this problem."";;;","14/May/18 8:51 PM;ashcherbakov;[~ozheregelya] Please check it as https://jira.hyperledger.org/browse/INDY-1289 is resolved.;;;","18/May/18 12:25 AM;ozheregelya;indy-node 1.3.412

New read_ledger doesn't create -read-copy directories. Validator-info and read_ledger data accord to each other. 

 
{code:java}
root@605382d6b0cd:/home/indy# validator-info 
Validator Node1 is running
Validator DID: Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv
Verification Key: 33nHHYKnqmtGAVfZZGoP8hpeExeH45Fo8cKmd5mcnKYk7XgWNBxkkKJ
Node Port: None
Client Port: None
Metrics:
 Uptime: 2 hours, 6 minutes, 25 seconds
 Total Ledger Transactions: 5027
 Total Pool Transactions: 6
 Read Transactions/Seconds: 0.00
 Write Transactions/Seconds: 0.66
Reachable Hosts: 6/6
Unreachable Hosts: 0/6

root@605382d6b0cd:/home/indy# read_ledger --type domain --count
5027
{code}
Validator-info and read_ledger will be additionally tested on larger pools and with larger ledger during load testing.

 ;;;",,,,,,,,,,,,,,,,,,,,
Unable to send NYM to provisional with 12/14 nodes connected,INDY-1220,28439,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,mgbailey,mgbailey,mgbailey,13/Mar/18 2:28 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Provisional network shows 12/14 nodes are connected, and the ledgers are synced on at least 11 of them, and maybe on 12 (see INDY-1219).  Still, NYM transactions sent to the ledger are not being posted.  Why is this happening?  Which nodes are not participating in consensus? Logs are attached. The CLI output where a send NYM was attempted is also attached.
{code:java}
mikebailey@sao-sov-p001:~$ sudo validator-info -v
[sudo] password for mikebailey:
Validator ev1 is running
Current time: Monday, March 12, 2018 5:26:42 PM
Validator DID: GWgp6huggos5HrzHVDy5xeBkYHxPvrRZzjPNAyJAqpjA
Verification Key: 46HnJNoJCRdFasFSBakSyqSgNGE93STk7wZiddJnsCfLgMKWjhWEJNc
Node Port: 9701/tcp on 0.0.0.0/0
Client Port: 9702/tcp on 0.0.0.0/0
Metrics:
Uptime: 6 days, 20 hours, 7 minutes, 7 seconds
Total Config Transactions: 114
Total Ledger Transactions: 43
Total Pool Transactions: 33
Read Transactions/Seconds: 0.00
Write Transactions/Seconds: 0.00
Reachable Hosts: 12/14
BIGAWSUSEAST1-001
DustStorm
OASFCU
ServerVS
danube
digitalbazaar
esatus_AG
ev1
iRespond
prosovitor
royal_sovrin
zaValidator
Unreachable Hosts: 2/14
thoth
icenode
Software Versions:
indy-node: 1.2.50
sovrin: 1.1.6
{code}",Provisional network running 1.2.50,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/18 3:14 AM;mgbailey;BIGAWSUSEAST1-001_logs.tgz;https://jira.hyperledger.org/secure/attachment/14764/BIGAWSUSEAST1-001_logs.tgz","14/Mar/18 3:14 AM;mgbailey;BIGAWSUSEAST1-001_logs2.tgz;https://jira.hyperledger.org/secure/attachment/14763/BIGAWSUSEAST1-001_logs2.tgz","13/Mar/18 2:43 AM;mgbailey;CLI.out;https://jira.hyperledger.org/secure/attachment/14759/CLI.out","14/Mar/18 3:14 AM;mgbailey;digitalbazaar_logs.tgz;https://jira.hyperledger.org/secure/attachment/14762/digitalbazaar_logs.tgz","13/Mar/18 2:28 AM;mgbailey;ev1_20180312.tgz;https://jira.hyperledger.org/secure/attachment/14758/ev1_20180312.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz0vb:",,,,,,Sprint 18.05,18.06,,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,mgbailey,Toktar,,,,,,,,,"13/Mar/18 5:30 PM;ashcherbakov;[~mgbailey] I think it should not be a blocker for the live pool Upgrade.;;;","14/Mar/18 3:14 AM;mgbailey;[~Toktar] I am attaching logs provided by stewards of digitalbazaar and BIGAWSUSEAST1-001. They are quite small, actually.;;;","14/Mar/18 6:18 PM;ashcherbakov;[~mgbailey]
What we can see from this logs is that ev1's *viewNo=0*, while Digitalbazaar's *viewNo=114.*
It looks like all PRE-PREPARES sent by the Primary (ev1) are discarded by other nodes because they have a higher viewNo.

We need more logs (ideally logs from all nodes) to say something more clear.

Actually I suggest sending forced NODE_UPGRADE which can update the code and restart the pool at the same time. Hopefully restart will help in recovering of inconsistent views.

 ;;;","14/Mar/18 7:07 PM;Toktar;The problem is in the incorrect data in the following nodes:
 * zaValidator
 * danube
 * digitalbazaar
 * OASFCU
 * BIGAWSUSEAST1
 * prosovito

The problem can be fixed by making the live pool Upgrade or by stopping nodes from the list and then running them one at a time.;;;","15/Mar/18 4:29 AM;mgbailey;[~ashcherbakov], [~Toktar], do we know how it got into this state and how to prevent it?  Is it possibly a known problem that is fixed in 1.3.55?;;;","15/Mar/18 4:40 AM;krw910;[~Toktar] Can you explain what you mean by ""incorrect data""? Mike meantime something about viewNo=114. Did we get into a state where we have two primary nodes and the nodes are not sure who they should be talking to?
;;;","15/Mar/18 5:43 PM;ashcherbakov;[~mgbailey] [~krw910]
We think it got into that state because of individual restarts of nodes. 

Unfortunately, we can't say too much from the current logs. We logs from 3 nodes only. And some of these logs rotated, so we don't have the full history...

I think we need to change how logs are rotated. I created INDY-1225 for this.;;;","15/Mar/18 5:44 PM;ashcherbakov;If we want to continue debug of the issue, we need to have log files from all nodes.;;;","16/Mar/18 4:54 AM;mgbailey;[~ashcherbakov] [~Toktar] I had BIGAWSUSEAST1-001 and digitalbazaar restart their services, and they sent me new logs:
{code:java}
2018-03-15 19:38:18,757 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: BIGAWSUSEAST1-001:0 selected primary ev1:0 for instance 0 (view 0)
2018-03-15 19:38:18,757 | INFO | node.py ( 532) | start_participating | BIGAWSUSEAST1-001 started participating
2018-03-15 19:38:18,757 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: BIGAWSUSEAST1-001:0 declares view change 0 as completed for instance 0, new primary is ev1:0, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-15 19:38:18,757 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: BIGAWSUSEAST1-001:1 selected primary zaValidator:1 for instance 1 (view 0)
2018-03-15 19:38:18,758 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: BIGAWSUSEAST1-001:1 declares view change 0 as completed for instance 1, new primary is zaValidator:1, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-15 19:38:18,758 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: BIGAWSUSEAST1-001:2 selected primary danube:2 for instance 2 (view 0)
2018-03-15 19:38:18,758 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: BIGAWSUSEAST1-001:2 declares view change 0 as completed for instance 2, new primary is danube:2, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-15 19:38:18,758 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: BIGAWSUSEAST1-001:3 selected primary royal_sovrin:3 for instance 3 (view 0)
2018-03-15 19:38:18,759 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: BIGAWSUSEAST1-001:3 declares view change 0 as completed for instance 3, new primary is royal_sovrin:3, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-15 19:38:18,759 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: BIGAWSUSEAST1-001:4 selected primary digitalbazaar:4 for instance 4 (view 0)
2018-03-15 19:38:18,759 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: BIGAWSUSEAST1-001:4 declares view change 0 as completed for instance 4, new primary is digitalbazaar:4, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-15 19:38:18,759 | INFO | ledger_manager.py ( 865) | catchupCompleted | CATCH-UP: BIGAWSUSEAST1-001 completed catching up ledger 2, caught up 0 in total
2018-03-15 19:38:18,759 | INFO | node.py (1598) | allLedgersCaughtUp | CATCH-UP: BIGAWSUSEAST1-001 does not need any more catchups
2018-03-15 19:38:18,760 | INFO | node.py (1685) | no_more_catchups_needed | BIGAWSUSEAST1-001 starting to participate since catchup is done, primaries are selected but mode was not set to participating
2018-03-15 19:38:18,760 | INFO | node.py ( 532) | start_participating | BIGAWSUSEAST1-001 started participating
{code}
and,
{code:java}
2018-03-14 19:48:04,568 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: digitalbazaar:0 selected primary ev1:0 for instance 0 (view 0)
2018-03-14 19:48:04,568 | INFO | node.py ( 532) | start_participating | digitalbazaar started participating
2018-03-14 19:48:04,569 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: digitalbazaar:0 declares view change 0 as completed for instance 0, new primary is ev1:0, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-14 19:48:04,569 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: digitalbazaar:1 selected primary zaValidator:1 for instance 1 (view 0)
2018-03-14 19:48:04,569 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: digitalbazaar:1 declares view change 0 as completed for instance 1, new primary is zaValidator:1, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-14 19:48:04,570 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: digitalbazaar:2 selected primary danube:2 for instance 2 (view 0)
2018-03-14 19:48:04,570 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: digitalbazaar:2 declares view change 0 as completed for instance 2, new primary is danube:2, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-14 19:48:04,570 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: digitalbazaar:3 selected primary royal_sovrin:3 for instance 3 (view 0)
2018-03-14 19:48:04,570 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: digitalbazaar:3 declares view change 0 as completed for instance 3, new primary is royal_sovrin:3, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-14 19:48:04,570 | DISPLAY | primary_selector.py ( 327) | _start_selection | PRIMARY SELECTION: digitalbazaar:4 selected primary digitalbazaar:4 for instance 4 (view 0)
2018-03-14 19:48:04,571 | DISPLAY | primary_selector.py ( 351) | _start_selection | VIEW CHANGE: digitalbazaar:4 declares view change 0 as completed for instance 4, new primary is digitalbazaar:4, ledger info is [(0, 33, 'BafagSgueFyWwzgUYocoeefS7qgGVFzS3ihPb57Rr6jS'), (1, 43, '2b61e2wdgufCSwyzB9CoGjiPKVkD5R7NbLmV3jHxigHL'), (2, 114, 'EFEMkAaNCQMGenatx4M11qDVLwABCLCkjmaWd1P2XDo6')]
2018-03-14 19:48:04,571 | INFO | ledger_manager.py ( 865) | catchupCompleted | CATCH-UP: digitalbazaar completed catching up ledger 2, caught up 0 in total
2018-03-14 19:48:04,571 | INFO | node.py (1598) | allLedgersCaughtUp | CATCH-UP: digitalbazaar does not need any more catchups
2018-03-14 19:48:04,572 | INFO | node.py (1685) | no_more_catchups_needed | digitalbazaar starting to participate since catchup is done, primaries are selected but mode was not set to participating
2018-03-14 19:48:04,572 | INFO | node.py ( 532) | start_participating | digitalbazaar started participating
{code}
It looks like these nodes are set to view 0 with primary ev1, right?  Does this mean that these nodes are now operating properly?

 ;;;","16/Mar/18 3:40 PM;ashcherbakov;Most probably yes, they should operate properly now.;;;","20/Mar/18 1:04 AM;ashcherbakov;Let's see whether live pool Upgrade helps.;;;","27/Mar/18 10:48 PM;ashcherbakov;We can reopen any that reappear, but all is good on the live pool now.;;;",,,,,,,,,,,,,
Need to set enableStdOutLogging = False in /etc/indy/indy_config.py during installation of indy-node,INDY-1221,28440,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ozheregelya,ozheregelya,13/Mar/18 2:29 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,Need to set enableStdOutLogging = False in /etc/indy/indy_config.py during installation of indy-node to don't duplicate node logs to syslog.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1102,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-987,,,,,,,,,"1|hzz0vr:",,,,,,18.06,,,,,,,,1.0,,,,,,,,,,,,anikitinDSR,ozheregelya,,,,,,,,,,,"19/Mar/18 9:55 PM;anikitinDSR;Problem reason: 
- Need to disable std out logging after install indy-node distributive

Changes: 
- added ""enableStdOutLogging = False"" into general_config. After install, if there is no indy_config.py into /etc/indy directory, it will create from general_config.py and added distributive specific parameters, such a path to log files etc.

PR:
- https://github.com/hyperledger/indy-node/pull/609


Version:
- 1.3.340-master 


Recommendations for QA
- Install indy-node on ""clean"" system, run and  check, that journalctl don't include any logs from node. If exist any indy-node distributive (/etc/indy/indy_config.py is exist), then delete them and install new indy-node distr.;;;","21/Mar/18 1:32 AM;anikitinDSR;Now, if ""enableStdOutLogging "" is not set in indy_config.py, then was added  ""enableStdOutLogging = False"";;;","21/Mar/18 4:06 PM;anikitinDSR;Changes are in 1.3.343-master in ""indy-node"";;;","27/Mar/18 2:06 AM;ozheregelya;*Environment:*
 indy-node 1.3.352
 docker pool + virtualbox VM

*Steps to Validate:*
 1. Install the indy node in following cases:
 * сlear machine (no config) - manual installation => 'enableStdOutLogging = False' added to indy_node.py config;
 * machine with 'enableStdOutLogging = True' - manual installation => 'enableStdOutLogging = True' is still in indy_node.py config;
 * machine with 'enableStdOutLogging = False' - manual installation => 'enableStdOutLogging = False' is still in indy_node.py config;
 * docker installation => 'enableStdOutLogging = False' is in indy_node.py config.

*Actual Results:*
 'enableStdOutLogging = False' is added to the indy_node.py during installation if there were no such parameter before.

*Additional Information:*
 Vagrant installation will be verified in scope of INDY-1218.

Journalctl and syslog size were verified earlier. All works fine.;;;",,,,,,,,,,,,,,,,,,,,,
All images should contain new CLI,INDY-1222,28472,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,,zhigunenko.dsr,zhigunenko.dsr,13/Mar/18 10:16 PM,16/Aug/19 11:58 PM,28/Oct/23 2:47 AM,16/Aug/19 11:58 PM,,,,,,0,,,,"_indy-cli_ package should be included in all environment images (Vagrant, Docker, etc)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz14v:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,zhigunenko.dsr,,,,,,,,,,,"14/Mar/18 5:42 AM;SeanBohan_Sovrin;can't put it in Docker until we have a stable package built;;;",,,,,,,,,,,,,,,,,,,,,,,,
A Steward needs to be able to get validator-info from all nodes,INDY-1223,28517,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,gudkov,ashcherbakov,ashcherbakov,14/Mar/18 11:09 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,validator-info,,,0,,,,We need to support GET_VALIDATOR_INFO request in libindy and CLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwyhj:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"06/Sep/18 2:10 AM;ozheregelya;Implemented in scope of IS-588.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Track and fix intermittent tests,INDY-1224,28519,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey.khoroshavin,sergey.khoroshavin,15/Mar/18 12:40 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"There are still some rare failures in random tests, this issue is for tracking them
 * -test_primary_recvs_3phase_message_outside_watermarks-     (Done, [~anikitinDSR])
 * -testInstChangeWithMoreReqLat-                                                 (Done, [~anikitinDSR])
 * test_quorum_after_f_plus_2_nodes_but_not_primary_turned_off_and_later_on (WIP, [~andkononykhin])
 * test_slow_node_reverts_unordered_state_during_catchup        (WIP,  [~anikitinDSR])
 * test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection
 * testPropagateRecvdAfterReques
 * test_restart_groups_wp
 * testPropagateRecvdAfterRequest",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1185,,,,,,,,,,,,,,,,,,,,"06/Jun/18 9:30 PM;anikitinDSR;test-result-plenum-1.prd-ubuntu1604-indy-x86_64-4c-16g-4309.txt;https://jira.hyperledger.org/secure/attachment/15054/test-result-plenum-1.prd-ubuntu1604-indy-x86_64-4c-16g-4309.txt","06/Jun/18 9:29 PM;anikitinDSR;test-result-plenum-1.prd-ubuntu1604-indy-x86_64-4c-16g-4309.txt;https://jira.hyperledger.org/secure/attachment/15053/test-result-plenum-1.prd-ubuntu1604-indy-x86_64-4c-16g-4309.txt","28/Mar/18 6:56 PM;anikitinDSR;test-result-plenum-1.ubuntu1604-indy-x86_64-4c-16g-35.txt;https://jira.hyperledger.org/secure/attachment/14826/test-result-plenum-1.ubuntu1604-indy-x86_64-4c-16g-35.txt","28/Mar/18 1:13 AM;anikitinDSR;test-result-plenum-3.ubuntu1604-indy-x86_64-4c-16g-372.txt;https://jira.hyperledger.org/secure/attachment/14825/test-result-plenum-3.ubuntu1604-indy-x86_64-4c-16g-372.txt","26/May/18 1:05 AM;ashcherbakov;testInstChangeWithMoreReqLat;https://jira.hyperledger.org/secure/attachment/15019/testInstChangeWithMoreReqLat","18/Jun/18 4:16 PM;andkononykhin;testPropagateRecvdAfterRequest.txt;https://jira.hyperledger.org/secure/attachment/15116/testPropagateRecvdAfterRequest.txt","04/Apr/18 7:47 PM;sergey.khoroshavin;testPropagateRecvdAfterRequest.txt;https://jira.hyperledger.org/secure/attachment/14848/testPropagateRecvdAfterRequest.txt","15/Mar/18 12:46 AM;sergey.khoroshavin;test_6th_node_join_after_view_change_by_master_restart.txt.gz;https://jira.hyperledger.org/secure/attachment/14773/test_6th_node_join_after_view_change_by_master_restart.txt.gz","19/Mar/18 10:07 PM;Derashe;test_add_node_to_pool_with_large_ppseqno_diff_views.txt;https://jira.hyperledger.org/secure/attachment/14799/test_add_node_to_pool_with_large_ppseqno_diff_views.txt","15/Mar/18 12:46 AM;sergey.khoroshavin;test_checkpoint_across_views.txt.gz;https://jira.hyperledger.org/secure/attachment/14774/test_checkpoint_across_views.txt.gz","11/Apr/18 7:53 PM;Toktar;test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection.ubuntu-11.zip;https://jira.hyperledger.org/secure/attachment/14877/test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection.ubuntu-11.zip","15/Mar/18 12:46 AM;sergey.khoroshavin;test_new_node_accepts_chosen_primary.txt.gz;https://jira.hyperledger.org/secure/attachment/14775/test_new_node_accepts_chosen_primary.txt.gz","27/Mar/18 2:07 AM;sergey.khoroshavin;test_node_request_propagates.txt.gz;https://jira.hyperledger.org/secure/attachment/14823/test_node_request_propagates.txt.gz","15/Mar/18 12:46 AM;sergey.khoroshavin;test_old_non_primary_restart_after_view_change.txt.gz;https://jira.hyperledger.org/secure/attachment/14776/test_old_non_primary_restart_after_view_change.txt.gz","15/Mar/18 12:46 AM;sergey.khoroshavin;test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on.txt.gz;https://jira.hyperledger.org/secure/attachment/14777/test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on.txt.gz","29/Mar/18 12:49 AM;sergey.khoroshavin;test_view_change_complex.html.gz;https://jira.hyperledger.org/secure/attachment/14827/test_view_change_complex.html.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzz19j:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,anikitinDSR,ashcherbakov,sergey.khoroshavin,spivachuk,Toktar,,,,,,,"28/Mar/18 1:15 AM;anikitinDSR;Failed tests: test_restart_groups_wp
log files:  [^test-result-plenum-3.ubuntu1604-indy-x86_64-4c-16g-372.txt] ;;;","28/Mar/18 6:57 PM;anikitinDSR;Failed tests: plenum/test/propagate:testPropagateRecvdAfterReques
log files:  [^test-result-plenum-1.ubuntu1604-indy-x86_64-4c-16g-35.txt] ;;;","05/Apr/18 7:54 PM;sergey.khoroshavin;[~dsurnin] [~Derashe]

Looked through jenkins failed builds, got following suspicious tests and builds.

EVERNYM

Failed:
testAddInactiveNodeThenActivate (843, 837, 827, 811, 786, 783) (tried to fix in build 800, seems like failed)
test_view_change_complex (836)

Freezed:
test/scripts (840, 836, 798, 789)


HYPERLEDGER

Failed:
testPropagateRecvdAfterRequest (332, 325)
test_restart_groups_wp (311)
test_6th_node_join_after_view_change_by_master_restart (310) - seen before!
test_add_node_to_pool_with_large_ppseqno_diff_views (310)
testAddInactiveNodeThenActivate (284)

Freezed:
test/scripts (339, 311)

Strange:
338: killed after 90 mins, three groups killed after 25 mins each, suspicious groups
	a: test/view_change (not finished)
	b: test/node_catchup (417 s)
	   test/primary_selection (417 s)
	c: test/node_request (1128 s)
294: hudson.plugins.git.GitException: Failed to fetch from https://github.com/hyperledger/indy-plenum
;;;","11/Apr/18 7:55 PM;Toktar;Failed tests: plenum/test/view_change:test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection
 log files: [^test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection.ubuntu-11.zip];;;","23/May/18 4:28 PM;spivachuk;Failed test: test_slow_node_reverts_unordered_state_during_catchup
Log file: https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/778/artifact/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-827.txt;;;","23/May/18 4:32 PM;spivachuk;Failed test: test_quorum_after_f_plus_2_nodes_but_not_primary_turned_off_and_later_on
Log file: https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/772/artifact/test-result-plenum-1.prd-ubuntu1604-indy-x86_64-4c-16g-743.txt;;;","26/May/18 1:06 AM;ashcherbakov;Failed test: `testInstChangeWithMoreReqLat` (see the corresponding attachment).;;;","06/Jun/18 9:30 PM;anikitinDSR;Failed test:

test_primary_recvs_3phase_message_outside_watermarks

logs:
[^test-result-plenum-1.prd-ubuntu1604-indy-x86_64-4c-16g-4309.txt];;;","18/Jun/18 4:17 PM;andkononykhin;Failed test: testPropagateRecvdAfterRequest

attachment: [^testPropagateRecvdAfterRequest.txt]

link: https://ci.evernym.com/view/ci/job/Indy-Plenum/job/indy-plenum-verify-x86_64/1574/;;;","10/Oct/18 9:13 PM;sergey.khoroshavin;*Triage*
There is now a separate epic INDY-1488 for dealing with intermittent tests. Some of failures reported here were reported in that epic as well, some are no longer reproducible, so closing this.;;;",,,,,,,,,,,,,,,
Do not rotate logs every day if they are small,INDY-1225,28545,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ashcherbakov,ashcherbakov,15/Mar/18 5:43 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,GA-0,,,"Currently we rotate logs every day regardless of the log size.

We keep only 10 day logs be default.

It means that we can easily lost old state of the node, and can not debug some issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz63b:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,mgbailey,ozheregelya,sergey.khoroshavin,,,,,,,,"16/Mar/18 1:16 AM;mgbailey;We should consider a log4j style configuration for logging, which includes settings for size of log files, number of files retained, etc.;;;","16/Mar/18 1:19 AM;ashcherbakov;We already have such settings. The problem is that day rotation logic is applied regardless of the size of the file.;;;","31/Mar/18 12:44 AM;sergey.khoroshavin;In that case do we really need day rotation logic at all when size rotation logic is applied? Or rotation should be done when both time and size conditions are satisfied?;;;","03/Apr/18 12:52 AM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/607
Implemented log file compression during rotation, changed default settings:
- turned on compression (which provides in most situation at least 8x space savings)
- increased amount of files to keep by 5x (so that more data can be kept without increasing space usage)
- increased timed rotation interval to one week
;;;","17/Apr/18 1:38 AM;ozheregelya;Environment:
indy-node 1.3.364
AWS 25-nodes pool

Steps to Validate:
1. Upgrade existing pool to the version with logging changes.
2. Run load tests to check log rotation by size.
3. Keep the pool until the next week to check weekly rotation.

Actual Results:
Logs rotation by size still works. All logs (exclude current file NodeX.log) are archived. Weekly rotation works.

Additional Information:
There is one issue with logs naming: INDY-1273.;;;",,,,,,,,,,,,,,,,,,,,
Old (Python) CLI needs to have a deprecation label,INDY-1226,28606,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,16/Mar/18 10:46 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,We need to have a deprecation message when we start old CLI. It should say that the new libindy-based CLI needs to be used.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzz61r:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,1.0,,,,,,,,,,,,ashcherbakov,Toktar,zhigunenko.dsr,,,,,,,,,,"13/Apr/18 8:20 PM;Toktar;PR: [https://github.com/hyperledger/indy-node/pull/640]

Changes:
 * added message about new getting started in readme
 * added message about new getting started in start of old cli script;;;","13/Apr/18 9:11 PM;Toktar;Problem reason:
Old python client from node has been deprecated 

Changes:
* added message about new getting started in readme
* added message about new getting started in start of old cli script

PR:
 https://github.com/hyperledger/indy-node/pull/640

Version:
indy-node 1.3.369-master

Risk factors:
Nothing is expected

Risk:
Low
;;;","24/Apr/18 7:11 PM;zhigunenko.dsr;verified on indy-node *1.3.382*;;;",,,,,,,,,,,,,,,,,,,,,,
indy-running-locally script from indy-node confuses Community,INDY-1227,28609,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,16/Mar/18 10:49 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Many people in Community tries using indy-running-locally script from indy-node to start the pool.

But this script is not intended way to start the pool.

We need to find out how to help Community to avoid confusion.

As an option, we can

1) remove all references to indy-running-locally from main documentation (README)

2) Have a bold warning in this script that it should not be used and Docker/Vagrant are appropriate options.

3) Mention Docker-based solution from libindy repo to start the pool locally in README.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzz61b:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,1.0,,,,,,,,,,,,ashcherbakov,Toktar,zhigunenko.dsr,,,,,,,,,,"13/Apr/18 8:19 PM;Toktar;PR: [https://github.com/hyperledger/indy-node/pull/645];;;","13/Apr/18 9:10 PM;Toktar;Problem reason:
indy-running-locally script is not intended way to start the pool, but Community don't know about it

Changes:
1. Remove link *Running pool locally* from https://github.com/hyperledger/indy-node/blob/master/README.md and https://github.com/hyperledger/indy-node/blob/master/getting-started.md.
2. В https://github.com/hyperledger/indy-node/blob/master/docs/indy-running-locally.md added wrning
3. В https://github.com/hyperledger/indy-node/blob/master/README.md#how-to-install-a-test-network added:
- Docker-based pool using with new libindy-based CLI: *Start Pool Locally* [https://github.com/hyperledger/indy-sdk/blob/master/README.md#how-to-start-local-nodes-pool-with-docker] and *Get Started with Libindy* [https://github.com/hyperledger/indy-sdk/blob/master/doc/getting-started/getting-started.md]

PR:
 https://github.com/hyperledger/indy-node/pull/645

Version:
indy-node 1.3.370-master

Risk factors:
Nothing is expected

Risk:
Low
;;;","24/Apr/18 7:29 PM;zhigunenko.dsr;[~Toktar] [~ashcherbakov]
environment/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md - still using old syntax in examples. Additional warning is required
docs/indy-running-locally.md - there is no links to this document from other documentation. Why don't delete it?;;;","25/Apr/18 5:58 PM;Toktar;new PR: https://github.com/hyperledger/indy-node/pull/662;;;",,,,,,,,,,,,,,,,,,,,,
Deprecation note for old GSG,INDY-1228,28610,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,16/Mar/18 10:51 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,We need to start the process of deprecation of the old GSG by providing necessary warnings in documentation in indy-node.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzz61z:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,Toktar,,,,,,,,,,,"13/Apr/18 8:17 PM;Toktar;PR: [https://github.com/hyperledger/indy-node/pull/647]

changes:
 * change link in readme
 * add warning in the old getting started;;;","13/Apr/18 9:04 PM;Toktar;Problem reason:
GSG in node has been deprecated

Changes:
* change link to a new GSG in readme in indy-node
* add warning in the old getting started

PR:
 https://github.com/hyperledger/indy-node/pull/647

Version:
indy-node 1.3.371-master

Risk factors:
Nothing is expected

Risk:
Low
;;;",,,,,,,,,,,,,,,,,,,,,,,
signus cannot be imported,INDY-1229,28649,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,MichaelWang,MichaelWang,19/Mar/18 5:57 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"When I run the test, one error: signus can ot be imported.

But as I found in indy-sdk/wrapper/python/, there is no signus module found.

I think it is a bug or something not updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz1zb:",,,,,,,,,,,,,,,,,,,,,,,,,,MichaelWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to remove TGB role from indy-node,INDY-1230,28652,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,,VladimirWork,VladimirWork,19/Mar/18 9:29 PM,16/Aug/19 11:59 PM,28/Oct/23 2:47 AM,16/Aug/19 11:59 PM,,,,,,0,,,,Indy-node still supports TGB role so we need to remove it from everywhere.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-597,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz1zz:",,,,,,,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Move Indy Anoncreds to Hyperledger Archive (possibly wait until Stable Release),INDY-1231,28663,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,SeanBohan_Sovrin,SeanBohan_Sovrin,20/Mar/18 5:36 AM,20/Mar/18 5:36 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz22f:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Investigate and evaluate the next version of Ubuntu (18) and the impact it will have,INDY-1232,28734,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,SeanBohan_Sovrin,SeanBohan_Sovrin,21/Mar/18 11:26 PM,17/May/18 11:51 PM,28/Oct/23 2:47 AM,,,,,,,0,help-wanted,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz2hb:",,,,,,,,,,,,,,,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Link 'Start Pool and Client with Docker' is to agent setup, no link to the pool setup",INDY-1233,28741,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,,ozheregelya,ozheregelya,22/Mar/18 3:16 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,Documentation,,,"From [https://github.com/hyperledger/indy-node/blob/master/getting-started.md#install-indy]:

You can install a test network in one of several ways:
 * *Automated VM Creation with Vagrant* [Create virtual machines|https://github.com/hyperledger/indy-node/blob/master/environment/vagrant/training/vb-multi-vm/TestIndyClusterSetup.md] using VirtualBox and Vagrant.

 * *Running locally* [Running pool locally|https://github.com/hyperledger/indy-node/blob/master/docs/indy-running-locally.md] or [Indy Cluster Simulation|https://github.com/hyperledger/indy-node/blob/master/docs/cluster-simulation.md]

 * *Docker:* [Start Pool and Client with Docker|https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/StartIndyAgents.md].

 * *Also coming soon:* Create virtual machines in AWS.

Docker link is to agent setup instruction ([https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/StartIndyAgents.md]). It should be to pool setup instruction ([https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/README.md]). Or instruction to agent setup should contain link to the pool setup instruction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz2in:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,Toktar,,,,,,,,,,,"10/Oct/18 10:56 PM;Toktar;Fixed in scope of the task INDY-1228;;;",,,,,,,,,,,,,,,,,,,,,,,,
Errors with configs during installation,INDY-1234,28744,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,ozheregelya,ozheregelya,22/Mar/18 4:42 AM,16/May/18 6:46 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"indy-node 1.3.412

Steps to Reproduce:
 1. Install indy-node package.

Actual Result:
 Errors appear at the end of installation:
{code:java}
/var/lib/dpkg/info/indy-node.postinst: line 129: /etc/supervisor/indy-node.conf: No such file or directory
grep: /etc/indy/node_control.conf: No such file or directory{code}
Expected Results:
Errors should not appear.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz2jb:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Simultaneous connections load testing,INDY-1235,28787,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,23/Mar/18 6:19 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,explore,,,"We need to test:
- how many simultaneous connections can the pool handle without crash?
- are we able to reach and keep ~5000 simultaneous connections for a long time?
- exploratory testing of CLIENT_CONNECTIONS_LIMIT setting.

Specific test cases will be added in this ticket after discussion.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1208,INDY-1268,,,,INDY-1259,INDY-1260,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz0x3:",,,,,,18.06,18.07 Stability & Monitoring,,,,,,,5.0,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"30/Mar/18 12:53 AM;VladimirWork;25 nodes old pool has lost consensus against 5 machines x 400 clients x 10 NYMs to add. About 15k NYMs were written of 20k expected. Pool has recovered normal state itself after several hours.;;;","03/Apr/18 1:31 AM;VladimirWork;New pool results:
Write
1 machine x 1000 clients x 10 NYMs [16 txns/s]
2 machines x 1000 clients x 10 NYMs [5-6 txns/s for each instance]
3 machines x 1000 clients x 10 NYMs [pool stops writing after 13..21k txns written of 30k expected] [archive2.zip]
Read
4 machine x 500 clients x 10 NYMs [90..95 txns/s per instance]
4 machine x 1000 clients x 10 NYMs [58..144 txns/s per instance]
4 machine x 1500 clients x 10 NYMs [134..144 txns/s per instance + one instance ""killed"" by os]

Pool lost consensus at ~178k txns written. [archive_pool_out_of_consensus_178.zip];;;","13/Apr/18 8:13 PM;VladimirWork;All additional investigations and fixes will be performed in scope of INDY-1259 and INDY-1260. All current results was posted in Performance Results spreadsheet.;;;",,,,,,,,,,,,,,,,,,,,,,
Load test fails with timeout error,INDY-1236,28802,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,23/Mar/18 11:53 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,,,,0,GA-0,,,"Size: XL (PoA + fix)

Steps to Reproduce:
Run load test: `python3 Perf_Add_nyms.py -n 1000000`

Load test output:
{code:java}
======== Created NYM request...
_indy_loop_callback: Function returned b'{""result"":{""identifier"":""Th7MpTaRZVRYnPiabds81Y"",""seqNo"":69157,""signature"":""uS7ex9tf1B5XKGgYX7on9YBqfMPdxvpKio2kiibpd3
GmJuqbKohbnhPqVoUaNvJZgsyChzdroCqm9uSSXqnEzmq"",""reqId"":1521799295295701355,""dest"":""5dTgz9qtfGkMJSziJgR6Xq"",""auditPath"":[""DNqmoGGTgpiBBAciPXesQMZrDc869zbHSp3dTU
9CCH96"",""84y14BEtKJVhvzxk9jcUkShSiLBCQwmLgBHDfSVzq6FF"",""HXuMghQ8oWzhPY9a1e6zeKDqDh47m4Y74S4XkDNfViu4"",""5zzz6XrU7YDqKfKcir5xcwFSn14Cb8BrSPDknTRBcpTe"",""FXgXuJuBy
wh96dcSzHBompV5W6mCafbhThpMSRGm19n5"",""79rKLpcLZWVvnLFPGobnVzUqWSjjNHAGViUwNgQSzdgT""],""txnTime"":1521799295,""rootHash"":""3aVbsYzWtiN84xYxiaW9kL5ZC14pYUk13Q2ekBC14
Rhn"",""signatures"":null,""type"":""1""},""op"":""REPLY""}'
 ======== Submitted NYM request
_indy_loop_callback: Function returned (b'APSapr2CQwpa3r8z6WE8CY', b'67jrf4UCHzAenHzyT596gh4C4cTZS92rZFHw5HrqNbmc')
 ======== DID69081: APSapr2CQwpa3r8z6WE8CY 
_indy_loop_callback: Function returned b'{""reqId"":1521799296030170739,""identifier"":""Th7MpTaRZVRYnPiabds81Y"",""operation"":{""dest"":""APSapr2CQwpa3r8z6WE8CY"",""type""
:""1""},""protocolVersion"":1}'
 ======== Created NYM request...
_indy_loop_callback: Function returned b'{""result"":{""signatures"":null,""signature"":""25oUYjwbpNto8bb9dU8NTphd8Y5wvhmUin6AJUWAhXm9eQUtfc9bYC2Vb8vd1q1KVJEQDtYCSqjw
eA8SyxpCAqTq"",""txnTime"":1521799296,""type"":""1"",""reqId"":1521799296030170739,""seqNo"":69158,""rootHash"":""6YSHtbLR4XPqxvdbTW1ybvn353cPKGW3KrwCW8cy98J7"",""identifier"":
""Th7MpTaRZVRYnPiabds81Y"",""auditPath"":[""F7NhGpzu3cjjpvCVdp4SodsUkKBKkp1kpDgQHkJxLR4W"",""DNqmoGGTgpiBBAciPXesQMZrDc869zbHSp3dTU9CCH96"",""84y14BEtKJVhvzxk9jcUkShSiL
BCQwmLgBHDfSVzq6FF"",""HXuMghQ8oWzhPY9a1e6zeKDqDh47m4Y74S4XkDNfViu4"",""5zzz6XrU7YDqKfKcir5xcwFSn14Cb8BrSPDknTRBcpTe"",""FXgXuJuBywh96dcSzHBompV5W6mCafbhThpMSRGm19n5
"",""79rKLpcLZWVvnLFPGobnVzUqWSjjNHAGViUwNgQSzdgT""],""dest"":""APSapr2CQwpa3r8z6WE8CY""},""op"":""REPLY""}'
 ======== Submitted NYM request
_indy_loop_callback: Function returned (b'93BvgPLfWVtWHZEDLTCN3i', b'5P68jssRddPmoHMqNjfcFi6Nr5fE6vK9ZsDKvpo6ny8w')
 ======== DID69082: 93BvgPLfWVtWHZEDLTCN3i 
_indy_loop_callback: Function returned b'{""reqId"":1521799296982578065,""identifier"":""Th7MpTaRZVRYnPiabds81Y"",""operation"":{""dest"":""93BvgPLfWVtWHZEDLTCN3i"",""type""
:""1""},""protocolVersion"":1}'
 ======== Created NYM request...
_indy_loop_callback: Function returned b'{""result"":{""signatures"":null,""dest"":""93BvgPLfWVtWHZEDLTCN3i"",""txnTime"":1521799300,""auditPath"":[""AXPu3KeHaa4cNFSL5kG7mG
Yubc6ifvRLVo16mmESKDMJ"",""DNqmoGGTgpiBBAciPXesQMZrDc869zbHSp3dTU9CCH96"",""84y14BEtKJVhvzxk9jcUkShSiLBCQwmLgBHDfSVzq6FF"",""HXuMghQ8oWzhPY9a1e6zeKDqDh47m4Y74S4XkDNf
Viu4"",""5zzz6XrU7YDqKfKcir5xcwFSn14Cb8BrSPDknTRBcpTe"",""FXgXuJuBywh96dcSzHBompV5W6mCafbhThpMSRGm19n5"",""79rKLpcLZWVvnLFPGobnVzUqWSjjNHAGViUwNgQSzdgT""],""reqId"":152
1799296982578065,""identifier"":""Th7MpTaRZVRYnPiabds81Y"",""type"":""1"",""rootHash"":""JAL5PKmUUZ3sDKCEbKrkC1pGpPZAYaQG4xYEdQ6N1mpt"",""signature"":""3FnW8mh6y1AWDTZ861p5tv
LxeapeGbYh9SSKNmpCv8jN6kDmJg2Bm6H5QS8R481nT4czwRXKcXu4U6SXqv4gyY6s"",""seqNo"":69159},""op"":""REPLY""}'
 ======== Submitted NYM request
_indy_loop_callback: Function returned (b'5Hi3oGkLzBxMTeURecMZJF', b'3LZ9DJdtzHYx1moW6Q251BcbXruCSnXLZ8UttmF3TaGK')
 ======== DID69083: 5Hi3oGkLzBxMTeURecMZJF 
_indy_loop_callback: Function returned b'{""reqId"":1521799302157613423,""identifier"":""Th7MpTaRZVRYnPiabds81Y"",""operation"":{""dest"":""5Hi3oGkLzBxMTeURecMZJF"",""type""
:""1""},""protocolVersion"":1}'
 ======== Created NYM request...
_indy_loop_callback: Function returned error 307
Exception in callback _indy_loop_callback(207253, 307, b'') at /usr/local/lib/python3.5/dist-packages/indy/libindy.py:62
handle: <Handle _indy_loop_callback(207253, 307, b'') at /usr/local/lib/python3.5/dist-packages/indy/libindy.py:62>
Traceback (most recent call last):
 File ""/usr/lib/python3.5/asyncio/events.py"", line 125, in _run
 self._callback(*self._args)
 File ""/usr/local/lib/python3.5/dist-packages/indy/libindy.py"", line 70, in _indy_loop_callback
 future.set_exception(IndyError(ErrorCode(err)))
 File ""/usr/lib/python3.5/enum.py"", line 241, in __call__
 return cls.__new__(cls, value)
 File ""/usr/lib/python3.5/enum.py"", line 476, in __new__
 raise ValueError(""%r is not a valid %s"" % (value, cls.__name__))
ValueError: 307 is not a valid ErrorCode
^CTraceback (most recent call last):
 File ""Perf_Add_nyms.py"", line 204, in <module>
 loop.run_until_complete(add_nyms(results.p, results.n, results.i, results.s))
 File ""/usr/lib/python3.5/asyncio/base_events.py"", line 375, in run_until_complete
 self.run_forever()
 File ""/usr/lib/python3.5/asyncio/base_events.py"", line 345, in run_forever
 self._run_once()
 File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1276, in _run_once
 event_list = self._selector.select(timeout)
 File ""/usr/lib/python3.5/selectors.py"", line 441, in select
 fd_event_list = self._epoll.poll(timeout, max_ev)
KeyboardInterrupt{code}
Nodes logs:
 [https://drive.google.com/file/d/1JOMG6ezqAkDEQTXRsL5eb11pHFL7KIT0/view?usp=sharing]","indy-node 1.3.350
libindy 1.3.1~416",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1288,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz9sv:",,,,,,18.06,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,ashcherbakov,ckochenower,dsurnin,ozheregelya,Toktar,,,,,,,,"28/Mar/18 10:13 PM;dsurnin;problem nym were added to ledger. it was checked later with additional get command.
be default sdk has 100 sec timeout, but adding this nym took 2.5 mins, so it was expected behavior.

there were fix in sdk that waits REQACK for command for 10 sec, if REQACK received sdk will wait longer.
https://github.com/hyperledger/indy-sdk/pull/608;;;","31/Mar/18 5:39 AM;ckochenower;[~sergey.khoroshavin] - You now have permission to create AWS spot instances appropriately sized to process logs. Please login to your home directory on ssh01.corp.evernym.com and read the awscli.howto document in your home directory.

A change had to be made to sshuttle-helper today to get all of this to work. Please pull the latest changes if you have already cloned the evernym/sshuttle-helper project.
{code}
git clone https://github.com/evernym/sshuttle-helper
cd ./sshuttle-helper
./sshuttle-helper -u sergeykhoroshavin
{code}

Evernym Technical Enablement Jira issue tracking this effort: https://evernym.atlassian.net/browse/TE-75;;;","17/Apr/18 5:12 PM;ashcherbakov;[~dsurnin]

Is it correct that sometimes pool processed requests too long (more than 100 seconds) because of a flood of MessageRequets/Responses? Can you please provide more detail?;;;","18/Apr/18 9:27 PM;dsurnin;According to logs there lots of message requests and nodes most of the time generates message responses. It could lead to performance degradation.
I cannot tell for sure if this the only reason.;;;","25/Apr/18 7:56 PM;Toktar;When the node is behind, it requests preparare and preprepare for all lost transactions. It is necessary to change this logic.;;;","27/Apr/18 10:28 PM;Toktar;In the procedure of exchanging messages requests with PREPARE and PREPREPARE, the following problems were identified:
 - When node receive pre-prepare with ppSeqNo greater than its have, it send MessageRequest with prepere and preprepare to all nodes.
 - Node send MessageRequest with prepere and preprepare to nodes in range [self.ppSeqNo, receivedPpSeqNo+1]
 - Sending message requests for every case when the order of obtaining transactions was violated with respect to their sending.

Ways for solve problems when recieved ppSeqNo greater than on the current node:
 - When node recieves *pre-prepare* message, sends message request only with pre-prepare and only for the primary node with 15 sec timeout.
 - When node recieves *prepare* message, doesn't do anything.
 - When node recieves *commit* message, doesn't do anything.
 - When node has *quorum* *of commits*, sends message request only with pre-prepare and only for the primary node.

The general idea of this logical changes in the decrease numbers of request messages, when node don't interested in response.;;;","09/May/18 12:31 AM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/664;;;","21/May/18 10:05 PM;Toktar;*Problem reason:*
 * Load test fails with timeout error because backup replicas send a lot of message requests.

*Changes:*
 * remove _setup_for_non_master from catchup
 * added condition for case when node receives prepare or pre-prepare after catchup. We need in _setup_for_non_master only in this case without catchup.
 * added logic for queues cleaning in catchup.
 * moves _setup_for_non_master() from case of receive preprepare with greater ppSeqNo to receive prepare and preprepare
 * added condition to ask pre-prepare only from primary node
 * added update watermarks in _setup_for_non_master()

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/705]
 * [https://github.com/hyperledger/indy-plenum/pull/664]

*Version:*
 * indy-node 1.3.418-master
 * indy-plenum 1.2.364-master

*Risk factors:*
 * Problems with node fall behinds
 * Probems with catchup

*Risk:*
 * Medium

*Covered with tests:*
 * [test_unit_setup_for_non_master.py|https://github.com/hyperledger/indy-plenum/pull/664/files#diff-0716bdd02759d588de5ea35b76663592]
 * [test_setup_for_non_master.py|https://github.com/hyperledger/indy-plenum/pull/664/files#diff-64cda64c306348a0d3677a3d1e09ffa9];;;","25/May/18 12:36 AM;ozheregelya;Testing of this ticket will be performed in scope of INDY-1367 because of current problems with load testing (INDY-1365).;;;",,,,,,,,,,,,,,,,
[Refactor] Remove `RegistryPoolManager` class,INDY-1237,28838,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,VladimirWork,VladimirWork,26/Mar/18 7:36 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,We need to remove `RegistryPoolManager` class according to INDY-1058 discussion.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1058,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz2yf:",,,,,,,,,,,,,,,,,,,,,,,,,,Derashe,VladimirWork,,,,,,,,,,,"24/Apr/18 10:08 PM;Derashe;Problem reason:
 - We need to remove RegistryPoolManager class from code

Changes:
 - Removed constructor parametrs like nodeRegisrty, changed some of tests and scripts which used nodeReg

PR:
- https://github.com/hyperledger/indy-plenum/pull/643

Version:
 - master, 326

Risk factors:
 - No

Risk:
 - Low

Covered with tests:
 - No

Recommendations for QA

-;;;",,,,,,,,,,,,,,,,,,,,,,,,
Node is restarted because of Out of memory error,INDY-1238,28848,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,dsurnin,dsurnin,26/Mar/18 9:57 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,GA-0,,,"The issue is reproduced on big pool (tested with 25 nodes) under the load test.
Some nodes (3 or 4) are being restarted with Out of memory message in journalctl.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1180,,,,,INDY-1250,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz30n:",,,,,,18.06,,,,,,,,,,,,,,,,,,,,dsurnin,ozheregelya,,,,,,,,,,,"26/Mar/18 11:14 PM;ozheregelya;The issue is reproducing on indy-node 1.3.350.
It reproduces on t2.medium and m4.large AWS instances.
Command for load test running (threads count is 1 by default): 
{code:java}
for s in `seq 1 2000` ; do python3 Perf_Add_nyms.py -n 500 ; done{code}
During the test load was not permanent (sometimes load test is stopped because of IS-596).;;;","28/Mar/18 11:06 PM;dsurnin;there are two main mem leaks

1 - unlimited zmq queues
it was fixed with pr https://github.com/hyperledger/indy-plenum/pull/597

2 - there is a queue of stashing requests in node. this queue is also unlimited and contains requests that cannot be ordered at the moment. if node reaches some invalid state it could stash all the requests and as a result use all the available memory. we should define the policy how to restore node from this state. for reference [INDY-1250|https://jira.hyperledger.org/browse/INDY-1250]
;;;",,,,,,,,,,,,,,,,,,,,,,,
Clarify current permissible actions with DID's roles,INDY-1239,28885,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,27/Mar/18 12:00 AM,09/Oct/19 6:12 PM,28/Oct/23 2:47 AM,09/Oct/19 6:12 PM,,,,,,0,explore,,,"Main assumptions:
1) only the person that did the operation can add them back
2) Trustee can perform any operation no matter who remove the role

Findings:
[Current state|https://docs.google.com/spreadsheets/d/1gG99pc3Zf9j15JE64GqF7o9HVbGujdqmwUuPmYnF0sQ/edit#gid=0]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwyef:",,,,,,18.07 Stability & Monitoring,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,zhigunenko.dsr,,,,,,,,,,,"09/Oct/19 6:12 PM;ashcherbakov;Currently we have configurable auth rules and a lot of tests for them. Default rules are reviewed and accepted.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Dev team needs to be able to get logs from live pool without communication with Stewards,INDY-1240,28914,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,sergey.khoroshavin,ashcherbakov,ashcherbakov,27/Mar/18 6:03 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,GA-0,,,The implementation based on the Design from INDY-1177,,,,,,,,,,INDY-1177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwyf3:",,,,,,,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"18/Apr/18 9:33 PM;ashcherbakov;Since a short-term option is chosen in INDY-1177, nothing is expected from dev team here;;;",,,,,,,,,,,,,,,,,,,,,,,,
Availability of the Public Ledger,INDY-1241,28915,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/18 6:26 PM,09/Oct/19 6:36 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-9,,Availability,To Do,,,,,,,"1|hzxwxj:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Design] Improve catch-up of huge ledgers,INDY-1242,28916,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,27/Mar/18 6:30 PM,09/Oct/19 6:35 PM,28/Oct/23 2:47 AM,,,2.0,,,,0,GA-0,,,"We need to improve catch-up of huge ledgers.

We can use Multi-signature of Pool Ledger (INDY-1068).

See [https://github.com/hyperledger/indy-plenum/blob/master/design/observers.md]

We may also considering bit torrent protocol for fast catchup.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2083,INDY-1189,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzwx4f:2rzl",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Read-ledger without storage copy in case of RocksDB (RocksDB read-only mode support),INDY-1243,28917,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,sergey-shilov,sergey-shilov,27/Mar/18 7:27 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Unlike LevelDB the RocksDB supports read-only mode. Several processes can open the same storage in read-only mode. It means that it is not necessary to create storage copy by read_ledger script to read all entities. Instead the storage can be opened in read-only mode by read_ledger script if it is a rocksdb storage, but for now read-only option is not supported by our code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1205,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz3d3:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,sergey-shilov,,,,,,,,,,,"30/May/18 10:52 PM;ashcherbakov;Already done;;;",,,,,,,,,,,,,,,,,,,,,,,,
Migration from LevelDB to RocksDB,INDY-1244,28919,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey-shilov,sergey-shilov,27/Mar/18 7:43 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,Corresponding migration scripts should be implemented since we plan to use RocksDB as a default key-value storage.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1205,,,,,INDY-1245,INDY-1330,,,,,,,,,,,,,,"08/May/18 5:06 PM;VladimirWork;journal.txt;https://jira.hyperledger.org/secure/attachment/14959/journal.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz9wn:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,2.0,,,,,,,,,,,,anikitinDSR,sergey-shilov,VladimirWork,,,,,,,,,,"04/May/18 7:36 PM;anikitinDSR;Reasons:
- need to create migration script from leveldb to rocksdb storage while indy-node upgrading

Changes:
- created migrations script

Versions:
indy-node: 1.3.399-master
indy-plenum: 1.2.347-master

Risks:
- This migrations needs to recreating rocksdb storage from leveldb. Therefore, if upgrade procedure will failed, than storages can be restored only from backup.
;;;","07/May/18 7:07 PM;VladimirWork;PoA:

Build Info:
[indy-node 1.3.395 indy-plenum 1.2.342 / indy-node=1.3.375 indy-plenum=1.2.317 python3-indy-crypto=0.4.0 python3-base58=0.2.4 / python3-base58=0.2.4 libindy-crypto=0.1.6 python3-indy-crypto=0.1.6 indy-plenum=1.2.180 indy-node=1.2.223]

Preconditions:
Pool ledger should be filled with [0 (genesis only) / 100 / 10k] txns before upgrade.

Cases:
1. Upgrade to current master all nodes with force=False.
2. Upgrade to current master all nodes with force=True simultaneously.
3. Upgrade to current master without one node with force=True.
4. Manual upgrade of single node (from Step 3).

Expected Results:
Upgrade should finish successfully (there should be no errors/stacktraces in logs/node service status).
Pool should write new txns and read txns that were added *before and after* upgrade.
Check /usr/local/lib/python3.5/dist-packages/plenum/config.py for the next entries:

{noformat}
domainStateStorage = KeyValueStorageType.Rocksdb
poolStateStorage = KeyValueStorageType.Rocksdb
configStateStorage = KeyValueStorageType.Rocksdb
reqIdToTxnStorage = KeyValueStorageType.Rocksdb
{noformat}

;;;","08/May/18 5:06 PM;VladimirWork;Build Info:
indy-node 1.3.395 indy-plenum 1.2.342

Steps to Reproduce:
1. Install pool.
2. Upgrade pool from 395 to 404 version.
3. Check journalctl.


Actual Results:
There are no migrations applied during upgrade: [^journal.txt] 

{noformat}
 | _create_backup | Creating backup for 1.3.395
 | migrate | Migrating from 1.3.395 to 1.3.404 on Ubuntu
 | migrate | Found migration scripts: ['1_0_96_to_1_0_97', '1_1_150_to_1_1_151', '1_2_188_to_1_2_189', '1_2_233_to_1_2_234', '1_2_273_to_1_2_274', 'disabled_1_0_97_to_1_0_96', 'helper_1_0_96_to_1_0_97']
 | migrate | No migrations can be applied to the current code.
 | _remove_old_backups | Removing old backups
 | _call_restart_node_script | Restarting indy
{noformat}

Expected Results:
Migration script from leveldb to rocksdb should be applied.;;;","10/May/18 4:21 PM;anikitinDSR;Versions:
  indy-node: 1.3.406;;;","10/May/18 6:33 PM;VladimirWork;Build Info:
indy-node 1.3.395 indy-plenum 1.2.342

Steps to Reproduce:
1. Install pool.
2. Upgrade pool from 395 to 406 version.
3. Check journalctl.

Actual Results:
There are errors during migration applying:

{noformat}
May 10 09:21:46 da0258d2f24a env[67]: 2018-05-10 09:21:46,820 | INFO     | migration_tool.py    (  52) | migrate | Migrating from 1.3.395 to 1.3.406 on Ubuntu
May 10 09:21:46 da0258d2f24a env[67]: 2018-05-10 09:21:46,827 | DEBUG    | migration_tool.py    (  54) | migrate | Found migration scripts: ['1_0_96_to_1_0_97', '1_1_150_to_1_1_151', '1_2_188_to_1_2_189',
May 10 09:21:46 da0258d2f24a env[67]: 2018-05-10 09:21:46,829 | INFO     | migration_tool.py    (  61) | migrate | Following migrations will be applied: ['1_3_396_to_1_3_397']
May 10 09:21:46 da0258d2f24a env[67]: 2018-05-10 09:21:46,831 | INFO     | migration_tool.py    (  63) | migrate | Applying migration 1_3_396_to_1_3_397
May 10 09:21:46 da0258d2f24a env[67]: 2018-05-10 09:21:46,831 | INFO     | migration_tool.py    (  35) | _call_migration_script | script path /usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,120 | INFO     | 1_3_396_to_1_3_397.py ( 149) | migrate_all | Starting migration of storages from LevelDB to RocksDB...
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,186 | ERROR    | 1_3_396_to_1_3_397.py (  96) | migrate_storage | None
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,186 | ERROR    | 1_3_396_to_1_3_397.py (  97) | migrate_storage | Could not put key/value to RocksDB storage 'pool_transactions'
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,187 | ERROR    | 1_3_396_to_1_3_397.py ( 112) | migrate_storages | Could not migrate pool_transactions, DB path: /var/lib/indy/sandbox/data/Node1/
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,187 | ERROR    | 1_3_396_to_1_3_397.py ( 154) | migrate_all | Storages migration from LevelDB to RocksDB failed!
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,188 | INFO     | 1_3_396_to_1_3_397.py ( 196) | <module> | Migration from LevelDB to RocksDB failed!
May 10 09:21:48 da0258d2f24a env[67]: Traceback (most recent call last):
May 10 09:21:48 da0258d2f24a env[67]:   File ""/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_396_to_1_3_397.py"", line 94, in migrate_storage
May 10 09:21:48 da0258d2f24a env[67]:     rocksdb_storage.put(key, val)
May 10 09:21:48 da0258d2f24a env[67]:   File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 52, in put
May 10 09:21:48 da0258d2f24a env[67]:     self._db.put(key, value)
May 10 09:21:48 da0258d2f24a env[67]:   File ""rocksdb/_rocksdb.pyx"", line 1467, in rocksdb._rocksdb.DB.put
May 10 09:21:48 da0258d2f24a env[67]:   File ""rocksdb/_rocksdb.pyx"", line 103, in rocksdb._rocksdb.bytes_to_slice
May 10 09:21:48 da0258d2f24a env[67]: TypeError: expected bytes, bytearray found
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,380 | ERROR    | migration_tool.py    (  44) | _call_migration_script | Migration failed: script returned 1
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,380 | DEBUG    | node_control_tool.py ( 178) | _restore_from_backup | Restoring from backup for 1.3.395
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,381 | WARNING  | node_control_tool.py ( 185) | _restore_from_backup | Copying last_version failed due to [Errno 2] No such file or directory: '/va
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,381 | WARNING  | node_control_tool.py ( 185) | _restore_from_backup | Copying next_version failed due to [Errno 2] No such file or directory: '/va
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,381 | WARNING  | node_control_tool.py ( 185) | _restore_from_backup | Copying upgrade_log failed due to [Errno 2] No such file or directory: '/var
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,381 | WARNING  | node_control_tool.py ( 185) | _restore_from_backup | Copying last_version_file failed due to [Errno 2] No such file or directory:
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,381 | WARNING  | node_control_tool.py ( 185) | _restore_from_backup | Copying restart_log failed due to [Errno 2] No such file or directory: '/var
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,521 | WARNING  | node_control_tool.py ( 194) | _restore_from_backup | Copying last_version failed due to [Errno 2] No such file or directory: '/tm
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,536 | WARNING  | node_control_tool.py ( 194) | _restore_from_backup | Copying next_version failed due to [Errno 2] No such file or directory: '/tm
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,537 | WARNING  | node_control_tool.py ( 194) | _restore_from_backup | Copying upgrade_log failed due to [Errno 2] No such file or directory: '/tmp
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,537 | WARNING  | node_control_tool.py ( 194) | _restore_from_backup | Copying last_version_file failed due to [Errno 2] No such file or directory:
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,537 | WARNING  | node_control_tool.py ( 194) | _restore_from_backup | Copying restart_log failed due to [Errno 2] No such file or directory: '/tmp
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,537 | ERROR    | node_control_tool.py ( 271) | _declare_upgrade_failed | Upgrade from 1.3.395 to 1.3.406 failed: Migration failed: script returned
May 10 09:21:48 da0258d2f24a env[67]: 2018-05-10 09:21:48,538 | ERROR    | node_control_tool.py ( 238) | _upgrade | Trying to rollback to the previous version Migration failed: script returned 1
{noformat}

Pool upgrade failed due to this errors.

Expected Results:
Upgrade and migration from leveldb to rocksdb should be performed successfully.;;;","11/May/18 12:02 AM;anikitinDSR;Version:
 1.3.410;;;","15/May/18 8:19 PM;VladimirWork;Build Info:
indy-node 1.3.410

Actual Results:
All cases from PoA passed except upgrade from indy-node=1.2.223 (upgrade and migration works but pool has lost working capacity after it, INDY-1330 was reported).;;;",,,,,,,,,,,,,,,,,,
Tune RocksDB options for the best performance,INDY-1245,28920,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey-shilov,sergey-shilov,27/Mar/18 7:49 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,explore,performance,,Now RocksDB is used with default parameters. It would be nice to tune it for the best performance according to indy-node specificity.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1205,INDY-1244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzzcov:",,,,,,EV 18.11 Stability/ViewChange,EV 18.12 Release RocksDB,,,,,,,5.0,,,,,,,,,,,,sergey-shilov,VladimirWork,,,,,,,,,,,"04/Jun/18 3:50 PM;sergey-shilov;Found the following official tuning guide:
https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide;;;","08/Jun/18 10:03 PM;sergey-shilov;*Problem state / reason:*

For now we run RocksDB instances with default options. But we can achieve much better performance/memory usage by tuning of RocksDB storages according to actual node's workloads.

But such tuning requires full understanding of each configuration parameter and all side effects of parameters changing. In fact tuning of RocksDB is very complex and understanding of each parameter takes much time. From the official [RocksDB tuning guide|https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide]:
 _""RocksDB is very flexible, which is both good and bad. You can tune it for variety of workloads and storage technologies. ... However, flexibility is not always user-friendly. We introduced a huge number of tuning options that may be confusing. ... Tuning RocksDB is often a trade off between three amplification factors: write amplification, read amplification and space amplification.""_

And the guide ends with the following:
 ""_Unfortunately, configuring RocksDB optimally is not trivial. *Even we as RocksDB developers don't fully understand the effect of each configuration change.* If you want to fully optimize RocksDB for your workload, we recommend experiments and benchmarking, while keeping an eye on the three amplification factors.""_

So that we decided to slightly reduce the scope of this ticket: final optimized RocksDB configuration is not a target. The main targets of this ticket:
 * provide possibility to get RocksDB statistics;
 * figure out the set of configurable parameters that *do not break the storage that was initially created with other parameters' values;*
 * provide mechanism to configure each RocksDB storage separately according to its' specific workload
 * set obvious parameters according to our needs and limitations

*Changes:*
 * Created RocksDB configuration structure with the set of parameters that changed values do not break existing storages
 * Implemented mechanism of applying of RocksDB configuration to RocksDB's options
 * Set *keep_log_file_num = 5* for all storages to avoid too many created log files (each time storage is open a new log file is created)
 * Set *db_log_dir = '/dev/null'* for *read_ledger* script as there is no way to disable creating of log file during opening of storage (see previous note)

Other parameters may be changed in future if we see some problems in RocksDB I/O statistics.

*NOTE: CHANGE ROCKSDB PARAMETERS VERY CAREFUL AS EVEN DEVELOPERS OF ROCKSDB DO NOT FULLY UNDERSTAND ALL SIDE EFFECTS OF CHANGED OPTIONS!*

*Committed into:*

    [https://github.com/hyperledger/indy-plenum/pull/731
]    [https://github.com/hyperledger/indy-node/pull/758]
    indy-node [1.4.458-master|https://github.com/hyperledger/indy-node/releases/tag/1.4.458-master]

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*
 * Set up and run a pool
 * Check that running _read_ledger_ does not lead to creation of new RocksDB LOG files:
 ** Check _/var/lib/indy/sandbox/data/NodeX/pool_transactions_ folder on some NodeX, it should contain one *LOG* file
 ** Run _read_ledger_ tool several times to read pool ledger: 
_# read_ledger --type pool_
 ** Then check _/var/lib/indy/sandbox/data/NodeX/pool_transactions_ folder, no any additional *LOG.old.** files like *LOG.old.1528554337890741* expected, just single *LOG* file should exist
 * Check limit of kept RocksDB LOG files:
 ** Restart indy-node service several times (> 5), after each restart check the number of *LOG** files in _/var/lib/indy/sandbox/data/NodeX/pool_transactions_ folder. Each restart should lead to new *LOG.old.XXX* file created, but overall number of *LOG** files should not be greater than *5*.;;;","18/Jun/18 9:13 PM;VladimirWork;Build Info:
indy-node 1.4.464

Steps to Validate:
1. Install \ upgrade pool of \ to 1.4.464 version.
2. Check that running read_ledger does not lead to creation of new RocksDB LOG files.
3. Check limit of kept RocksDB LOG files.
4. Check that pool writes and reads txns.

Actual Results:
Pool works and logs files normally.;;;",,,,,,,,,,,,,,,,,,,,,,
Add new node while other nodes do view change procedure,INDY-1246,28921,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,anikitinDSR,anikitinDSR,27/Mar/18 8:07 PM,09/Oct/19 6:02 PM,28/Oct/23 2:47 AM,,,,,,,0,explore,GA-0,,"We have N nodes which start view change procedure. In that time we add new node into pool.
Will new added node have finished view change procedure with other nodes?
What will happen if node count == N-f ?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwy9b:",,,,,,,,,,,,,,,,,,,,,,,,,,anikitinDSR,sergey.khoroshavin,,,,,,,,,,,"10/Oct/18 9:37 PM;sergey.khoroshavin;*Triage*
This seems more like a technical task, not actual bug;;;",,,,,,,,,,,,,,,,,,,,,,,,
Investigate DID Doc Support w/in INDY,INDY-1247,28929,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,SeanBohan_Sovrin,SeanBohan_Sovrin,27/Mar/18 10:37 PM,28/Jan/20 11:41 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"The Node needs to support the standard for DID Doc.

*Acceptance Criteria*
* Investigate the DID Doc standard.
* Create a Plan of Attack (POA) for bringing our implementation inline with the standard.
* Create the relevant issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-51,,,,,,,,,"1|hzwvif:00001yw969w49i41ismqf",,,,,,,,,,,,,,5.0,,,,,,,,,,,,SeanBohan_Sovrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Design] Fix memory leaks when primary on backup instance is disconnected,INDY-1248,28931,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,anikitinDSR,anikitinDSR,27/Mar/18 10:46 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.78,,,,0,GA-0,,,"When pool finished view change procedure and new primary for master replica is elected, each node will start selection primary on other backup replicas. Backup replica can choose disconnected node as primary, so transaction ordering will be blocked and this replica will stash all PROPAGATE request until the next view change.

 

*Acceptance criteria*
 * Possible options for fixes
 * Proposed option, plan of attack
 * Create necessary tasks",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1681,,,,,INDY-1572,INDY-1680,INDY-1682,INDY-1683,INDY-1685,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwwsn:",,,,,,EV 18.18 Service Pack 2,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,Toktar,,,,,,,,,,,"01/Sep/18 12:15 AM;Toktar;We have a problem with backup instances thouse primaries are stopped. In this case primary nodes:
 * replicas stashing messages and can't use it and dynamicly free memory
 * replicas store all new requests and other replicas can't remove already oredered messages.

Ways to solve problems:
 *1) Remove stopped replicas*
 +What's bad?+ 
 We will have less then f+1 replicas and it's going against with the RBFT.
 +Why can we do it?+ 
 In RBFT we need in f+1 instances because if f primary (backup and master) nodes will malicious, one node will correct and pool will do view changes until the correct node becomes the primary.
 If primary on backup malicious, the whole backup instance malicious and we don't need in this throughput metrics. And we can remove it.
 +Not forget.+
 If we incorrect identify backup primary as malicious more then f+1 replicas can be remove in instance and we kill not malicious instance. 
 *2) Exclude a sleeping node from consensus for removing requests.* (excluding options 1)
 +What's bad?+ 
 A sleeping replica can't free memory with data that can't use.
 +Why +can+ we do it?+ 
 It's faster to implementation then the previouse option and has a smaller risks. 
 +Not forget.+
 If we incorrect identify backup primary as malicious, replicas will lost necessary requests. More then f+1 replicas and the whole backup instance will malicious.
 *3) Choose other primary if it disconnected.*
 +What's bad?+ 
 A very big risk. For implementation this option we need to change VIEW_CHANGE_DONE format. Should be use with other option because not solve problems but do it less likely.
 +Not forget.+
 In this option we need to add list with all primary nodes in ViewChangeDone and wait consensus of a list with primaries. In theory, we may never have a quorum and never finish view change.;;;","05/Sep/18 11:49 PM;Toktar;In this situation, it was decided to remove the replicas when its primary node malicious.
 1) Firstly, we will add the ability to remove the replica without changing f with the check it is possible to clean the queue with requests (we should verify that nothing will be broken after switch off replicas, especially the monitor) INDY-1680
 2) Secondly, will add a switching off a replica that stopped because disconnected from a backup primary a long time.  INDY-1681
 3) Thirdly, will implement a new strategy which will check for performance degradation (the same way as master does) in addition to disconnections INDY-1682
 4) Finaly, will implement a new strategy for removing replica with quorum of instance changes (and not just a local check as in task 3) INDY-1683
 5) (Additional) After task will 2 add a view change (send INSTANCE CHANGE) when f / 2 (for example) replicas are switched off. INDY-1685;;;",,,,,,,,,,,,,,,,,,,,,,,
Separate NICs must be used for Client-to-Node and Node-to-Node communication,INDY-1249,28953,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,28/Mar/18 6:26 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,GA-0,,,"We do have at least one requirement related to DDoS that we really need to address, which is that we need to be able to bind listeners in indy-node to a specific NIC. (I am told that, although we can declare in config that we’re binding to a specific NIC, logs reveal that we always bind to all NICs. This may be inaccurate, but if it is, we should teach people how to do it right, because apparently nobody is.) The reason I feel like this is urgent is that a major DDoS mitigation strategy for MGL was the requirement that all validators should have 2 NICs--one dedicated to consensus with other validators, and one dedicated to clients. If we have bad clients doing DDoS, but we have two NICs, then we shouldn’t be able to defeat consensus on the other NIC.



We need to make sure that we support using two separate NICs and provide detailed instructions on how it can be achieved.

We should explore working capacity of 2 NIC node configuration before we will configure persistent pool this way because now both node and client IPs bind to 0.0.0.0.

- Will node work with 2 NIC and different IPs for node and client in pool ledger?
- Will pool work with 1 / f+1 / n-f / n nodes configured this way?",,,,,,,,,,INDY-1332,,,,,,,,,,,,,,,,,,,INDY-1282,INDY-1254,,,,,,,,,,,,,,,,,,,"16/May/18 8:26 PM;VladimirWork;docker_double_nic.PNG;https://jira.hyperledger.org/secure/attachment/14983/docker_double_nic.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzzcy7:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,ignisvulpis,VladimirWork,,,,,,,,,,"29/Mar/18 5:11 PM;ashcherbakov;From [~mgbailey]

Only 3 of the 17 validators currently do this.;;;","16/May/18 8:26 PM;VladimirWork;Steps to Reproduce in Docker:
1. Run docker pool with default network.
2. Run another docker network with another ip range (e.g. 192.0.0.0\8).
3. Add each node to second network with ip from its range.
4. Send new node txn for each node with new client/node ip from second network.
5. Check that pool works and clients can connect to it. !docker_double_nic.PNG|thumbnail! ;;;","07/Jun/18 4:45 PM;VladimirWork;Verified in scope of INDY-1332.;;;",,,,,,,,,,,,,,,,,,,,,,
Define the policy how to restore node from the state when it stasing all the reqs and there is a risk of out or memory,INDY-1250,28957,,Task,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,28/Mar/18 11:05 PM,25/Oct/19 8:59 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"there could be two main policies
1 - leave it as is  - wait for a process restart and use catchup logic to restore state. this approach does not require anything.
2 - limit the queue and start manual catchup if limit is reached. requires coding, experimenting, testing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1238,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2251,,,,,,,,,"1|i0125t:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stewards must notice within 15 minutes that there was a sudden spike in traffic,INDY-1251,28970,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,29/Mar/18 6:15 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,"Stewards must notice within 15 minutes that there was a sudden spike in traffic, and contacted Sovrin support.

 

We already have a functionality to detect Spikes (see `SpikeEventsEnabled`) and be able to send emails.

We need to
 * make sure that it works properly
 * make sure that it notifies Stewards properly
 * have detailed instructions how to configure it for Stewards
 * make sure that this is configured on all Nodes in the Live Pool.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1342,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz9vz:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,5.0,,,,,,,,,,,,ashcherbakov,esplinr,mgbailey,sergey-shilov,tharmon,zhigunenko.dsr,,,,,,,"29/Mar/18 6:17 PM;ashcherbakov;Please contact [~andrey.goncharov] for more info regarding Spikes functionality.

Please contact [~mgbailey] and [~tharmon] on applying this to the Live Pools.;;;","31/Mar/18 12:05 AM;mgbailey;We had this enabled on the STN through early January, and we were getting ~100 emails per day from the network.  This amounts to spam, and so we all just made filters for the emails and redirected them to a folder.  In other words, the tool as implemented was useless.  Has it been improved since then?;;;","20/Apr/18 9:28 PM;sergey-shilov;*Problem state / reason:*

See ticket description and Mike's comment.

*Changes:*

Changed math of node request and cluster throughtput spikes detection.
 The main changes:
 * Now smoothing constant (alpha) is really constant as declares exponential moving average (EMA), alpha = 2 / (N + 1), where N is a minimal count of passed periods needed for comparison of expected value and real value, a.g. learning period. For now N = 15, so alpha = 0.125.
 * Rised min activity from 2 to 10, a.g. we start to compare expected and real values if we handle more than 10 request per time period, that now is set to 60 secs (10 requests per minute).
 * Rised borders coefficient to extend the lower and higher borders when observed value is treated as normal relative to expected value, previous border coefficient = 3:

 - lower border:  expected_val / 3
 - higher border: expected_val * 3
 new borders coefficient = 10:
 - lower border:  expected_val / 10
 - higher border: expected_val * 10
 Also implemented weighted borders coefficient. It means adaptation of borders coefficient to larger values using formula:
      weighted_borders_coef = borders_coef / log(expected_val)
 Such approach allows to lower borders for larger values. This functionality is optional and enabled by default.

*Committed into:*

    https://github.com/hyperledger/indy-plenum/pull/640
    https://github.com/hyperledger/indy-node/pull/657
     indy-node 1.3.382-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

Check indynotifierplugin installation and configuration guide.
Check spike detection behaviour under various load, emulate spikes and check that notification emails received, check silence timeout.;;;","26/Apr/18 10:19 PM;esplinr;[~tharmon] and [~mgbailey]: Is it adequate to notify the Stewards via email? Are there security concerns with an SMTP plugin used for sending?;;;","26/Apr/18 11:15 PM;mgbailey;[~esplinr] It may be worthwhile to ask stewards what notification services they would like to use.  Most would be able to use SMTP, but probably have better tools available. For example, our node runs in AWS, so we were able to integrate with AWS SNS, which allows a range of notification paths, including SMTP and SMS. [~EvelynEvergreene], please send a group email to the steward technical contacts to survey them on notification services they use.;;;","26/Apr/18 11:24 PM;tharmon;Depending on how the SMTP is implemented there are certainly security concerns. I see that as a route of last resort.

Also, regarding the math mentioned by [~sergey-shilov], I also want thresholds added. What I mean by this is that relative changes in a small number are much more likely to happen than with the same percentage of a larger number. In other words, in order to get a spike notification, I want the traffic to have changed by not only X%, but also at least Y amount in actual transactions per second. That principle should apply to all of the alerts.;;;","27/Apr/18 12:24 AM;sergey-shilov;[~tharmon]

Please notice that we implemented weighted borders to deal with growing numbers, this logic is enable by default:
 * lower border:  expected_val / (borders_coef / log(expected_val))
 * higher border: expected_val * (borders_coef / log(expected_val))

instead of
 * lower border:  expected_val / borders_coef
 * higher border: expected_val * borders_coef

So that the higher border for expected 10000 will be 25000 instead of 100000 (if borders_coef = 10, default for now).

But it's ok to add some threshold in requests count per time period (for example, 2000), and in this case the higher border for expected 10000 will be 12000 instead of calculated 25000. Did I understand your idea properly?;;;","27/Apr/18 8:56 AM;esplinr;Tasks like this are prone to eternally evolving without meeting the actual need. We should release what we have and get some feedback after real world usage.

* We don't yet understand real world traffic patterns, so further changes to the spike detection are guesswork.
* We don't yet understand the most useful methods for node administrators to be notified of irregular traffic.
* Node administrators who are concerned about outbound SMTP can use an SMTP proxy to shuttle the notifications to other services.

If this proves inadequate, then we should create a new issue and link it to this one. In that issue we can capture clear requirements that we can field test for suitability.;;;","27/Apr/18 10:01 PM;esplinr;More context:
* This functionality has been in the product for a few releases
* It is disabled by default
* The email subsystem is pluggable
* This work is to refine the rules used to alert on a spike in requests

We will verify that the work in this ticket did not make the default configuration any worse, and then we will close it. We will raise new tickets if we decide further changes should be made to this functionality.;;;","28/Apr/18 5:56 AM;sergey-shilov;[~esplinr] Yeah, sure!

For now all our coefficients and absulte thresholds are pulled out of the air (special thanks to [~steptan] for providing this idiom in English). So I think that our (developers') main goal is to implement correct and flexible core math and provide enough configurable parameters to tune it in future according to real world traffic without upgrading of the code base. And I think that we've done it. Applyed approach of traffic forecasting based on exponential moving average seems adequate for detection of abnormal traffic.

[~tharmon] Regarding my previous comment, just for clarification (of course if I understood your idea properly): proposed static threshold (in example was 2000) is not a replacement of calculated higher border, but maximal difference between expected and actual traffic, a.g.:

_higher_border = min(expected_val * (borders_coef / log(expected_val)), expected_val + treshold)_

_if actual_val > higher_border:_
        _..........._;;;","09/May/18 12:48 AM;zhigunenko.dsr;*Reason to rejection:*
Following [manual|https://github.com/hyperledger/indy-node/blob/master/docs/node-monitoring-tools-for-stewards.md] these cases hadn't raised notifications:
- upgrade with non-existent version
- upgrade(reinstall) with the same version
- upgrade with the right version and following cancel

*Environment:*
indy-anoncreds 1.0.32
indy-cli 1.3.1~507
indy-node 1.3.404
indy-plenum 1.2.347
libindy 1.3.1~507
libindy-crypto 0.4.0
python3-indy-crypto 0.4.0
Notice that indynotifieremail artifact has version 0.0.9 and was created Jan 29, 2018

*Steps to reproduce:*
1) set up vagrant pool
2) install sendmail
3) check test email to guarantee sendmail availability
4) setup indy.env like this
{code}
NODE_NAME=Node1
NODE_PORT=9701
NODE_CLIENT_PORT=9702
CLIENT_CONNECTIONS_LIMIT=15360
INDY_NOTIFIER_EMAIL_RECIPIENTS=""login1@domain1.com NodeRequestSuspiciousSpike ClusterThroughputSuspiciousSpike ClusterLatencyTooHigh NodeUpgradeScheduled NodeUpgradeComplete NodeUpgradeFail PoolUpgradeCancel, login2@domain2.com NodeRequestSuspiciousSpike ClusterThroughputSuspiciousSpike ClusterLatencyTooHigh NodeUpgradeScheduled NodeUpgradeComplete NodeUpgradeFail PoolUpgradeCancel""
{code}
on validator01
5) restart indy-node for applying new settings
6) send these type commands:
- try _ledger pool-upgrade force=true_ with non-existent version like 3.3.0
- try _ledger pool-upgrade reinstall=true_ with the same version as used
- try _ledger pool-upgrade action=star_t and next _ledger pool-upgrade action=cancel_

*Actual results:*
- sendmail log contain record only about test mail from step 2, no errors/records after it, service is alive
- no email notification neither work email nor guerrillamail.com
- all transactions accepted by legder
- all attempts to upgrade took place on nodes

*Excpected results:*
- E-mail notifications for all scheduled/successful/failed/cancelled upgrades
- Appropriate records in sendmail log;;;","11/May/18 8:04 PM;ashcherbakov;[~zhigunenko.dsr]
Why do we care about updates here? This is not scope of the ticket. The ticket is about Spikes in the traffic according to the rules specified in the ticket.
Does it work?;;;","12/May/18 12:51 AM;zhigunenko.dsr;[~ashcherbakov]
Problem is actual both for upgrade and spikes.
Consecutive load testing 100txns * 1client then 10 then 20 to 40 clients don't provoke  any sendmail activity;;;","14/May/18 7:56 PM;sergey-shilov;[~zhigunenko.dsr]

This ticket is not about pool upgrade, it's about spikes detection.

Did you check logs? Was spike detected by spike detection module? What expected/actual load was logged?;;;","15/May/18 12:01 AM;sergey-shilov;[~zhigunenko.dsr] [~ashcherbakov]

By the way, sending of email is just a notification stage of spike detection. The core functionality is a detection math and thresholds. So the first thing that should be checked is that load thresholds are exceeded, that's why logs are important here.;;;","15/May/18 12:36 AM;sergey-shilov;[~zhigunenko.dsr]

One missed thing from me: to enable spike detection the following parameter should be specified in the indy-node config file:

{color:#333333}SpikeEventsEnabled=True{color}

{color:#333333}[~ashcherbakov] It is disabled by default for now, should we enable it by default?
{color};;;","15/May/18 12:38 AM;ashcherbakov;I think the decisions whether it should be enabled by default should come from the Customer Success Team ([~esplinr] [~mgbailey] [~tharmon])?;;;","17/May/18 9:36 PM;zhigunenko.dsr;*Actual results:*
* NodeRequestSuspiciousSpike - notification has been received
* ClusterThroughputSuspiciousSpike - notification has been received
* ClusterLatencyTooHigh - situation hadn't being reached
* NodeUpgradeScheduled - notification has been received
* NodeUpgradeComplete - notification has been received
* NodeUpgradeFail - notification has been received
* PoolUpgradeCancel - notification has been received

*Additional information:*
[Manual|https://github.com/hyperledger/indy-node/blob/master/docs/node-monitoring-tools-for-stewards.md] requires some additional updates (INDY-1342);;;",,,,,,,
Do we keep connection from clients alive if there is no requests from the client,INDY-1252,28973,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,ashcherbakov,ashcherbakov,29/Mar/18 7:27 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,GA-0,,,"We need to disable keep-alive and check whether ZMQ keeps connection to clients if clients don't send anything.

Do we have some settings in ZMQ to drop IDLE connections?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz63j:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,sergey-shilov,,,,,,,,,,,"30/Mar/18 11:58 PM;sergey-shilov;For now our both node and client stacks always enable TCP keepalive, it solves two issues:
  - unexpected IDLE connection termination by intermediate devices (as for now indy-sdk keeps connections open);
  - dead peer detection.

If we just disable TCP keepalive then connections are kept infinitely (ZMQ does not mean anything here. it's all about TCP) and the server can not detect dead peers. This solution should be applied together with some other solution like proxy/firewall that detects dead peers. Moreover, current schema of indysdk-to-node connection ACCEPT IDLE CONNECTIONS:
 * connect
 * do something
 * do something
 * <idle>
 * .......
 * disconnect

So we can not remove TCP keepalive and drop idle connections till we use current schema of indysdk-to-node sessions.

By the way, it is proven by guys from ZMQ team: ZeroMQ does not track idle connections:
 [https://github.com/zeromq/libzmq/issues/2877];;;","16/Apr/18 11:08 PM;ashcherbakov;[~danielhardman] FYI ^;;;",,,,,,,,,,,,,,,,,,,,,,,
Load testing with random delays,INDY-1253,28974,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,29/Mar/18 7:38 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,explore,GA-0,,"The QA team needs to operate a test network for 72 hours under conditions where iptables has been configured on nodes to drop packets in a way that creates periods of patchy network brownouts, and has seen no surprising loss of consensus, data corruption, crashes, stalling of catchup, or dysfunctional view change. (This experiment does NOT need to demonstrate sustained performance of 10 trans per sec; it is okay if performance is degraded during brownouts. It also might be okay to see some loss of consensus that is NOT surprising, if a brownout is very severe--as long as consensus is restored after the brownout.)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/18 6:55 AM;mgbailey;network_pings.tgz;https://jira.hyperledger.org/secure/attachment/15424/network_pings.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwxun:",,,,,,EV 18.15 Stability/Availabilit,EV 18.16 Releasing 1.6,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,mgbailey,VladimirWork,,,,,,,,,,"28/Jul/18 11:24 PM;VladimirWork;Traffic Shaper tool allows to configure 3 network parameters:
- latency
- drop percent
- bandwidth (upload\download)

So we need to decide:
- which of this parameters we will use during load testing (only one parameter enabled at the same time or several?)
- how many nodes will be affected by traffic shaping at the same time (1 bad node, f, n-f, all?)
- how long we should shape the traffic imitating brownout and how long network should be good after it
- what values of traffic shaping are acceptable (is it acceptable to set >5..10 seconds latency / >25..50% drop / <1kbps bandwidth ?)
- what load will be applied (which txn types / how many clients / how many txns by each client)
- what is the acceptance criteria (what is acceptable time for catch up / consensus recovery / % of failed requests)

FYI [~ashcherbakov] [~ozheregelya] [~zhigunenko.dsr];;;","30/Jul/18 2:00 PM;ashcherbakov;I think we should start with light random conditions, ideally with random ones on all nodes. For example, latency from 0 to 5 secs, drop from 0 to 10%.

 ;;;","30/Jul/18 11:49 PM;ashcherbakov;[~mgbailey] Do you have any thinking about how we should configure latency/drop to emulate the live network as much as possible?;;;","31/Jul/18 6:56 AM;mgbailey;[~ashcherbakov][^network_pings.tgz] I am attaching hourly pings from our nodes on the STN to the other nodes on the network. This should get you an idea of the latency.;;;","10/Aug/18 12:25 AM;VladimirWork;Things tried and found:
Continuous load against 25 nodes AWS pool with the next network latencies:

- 250..500 ms
- 500..1000 ms
- 1000..2000 ms

Also there were cases with random package drop (1..10%) but since we use tcp/ip *it causes just to additional latency* (clients and nodes resend dropped packages due to response absence) so in main cases we use just latency settings without any drop.
In all cases we sustain up to *10 writing txns/sec* (in cases with all txn types excluding revocation and payments) / up to *20 writing txns/sec* (in cases with NYMs only) together with up to *250 reading txns/sec* from up to *2800 client connections* (~800 writing and ~2000 reading) for *more than 8 hours* without any lagged nodes and consensus loss (we fall due to node no space left error because of large ledger and node metrics) so I think we can operate 72 hours against pool with increased HDD and it will be checked during the followng load test runs.

Thus, we see that increased pool network latency just slows down communication with client so we are forced to increase outgoing client load to provide target values of pool incoming load and its performance.;;;",,,,,,,,,,,,,,,,,,,,
Document use of firewall to deal with DDoS attacks,INDY-1254,28976,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,ashcherbakov,ashcherbakov,29/Mar/18 9:47 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Story*
As an administrator of an Indy Node, I want to understand policies around firewall usage to minimize network risk to DDoS attacks.

*Acceptance Criteria*
* Administrator documentation exists for configuring IPTables on Linux as a firewall protecting both node network inferfaces
* The documentation explains how to specifically protect the NIC used for communication between network nodes, restricting communication to whitelisted addresses.
* The documentation explains how to quickly blacklist an IP address on either NIC.

*Notes*
* We estimate that half of the stewards on the Sovrin network use IPTables as their firewall.
* The stewards that do not use IPTables appear to use a range of commercial firewall packages, and be educated in their use to conform with network steward guidelines. Therefore, examples with IPTables are sufficient to guide these admins.
* We evaluated other tools as firewall proxy, and decided not to include them in the recommendations at this time:
** An HTTP proxy such as Nginx could be used to meet this use case
*** [https://docs.nginx.com/nginx/admin-guide/load-balancer/tcp-udp-load-balancer/.|https://docs.nginx.com/nginx/admin-guide/load-balancer/tcp-udp-load-balancer/]
*** This introduces additional complexity, is not the core use case of Nginx, and does not appear to be needed at this time.
** A tool such as fail2ban can automatically scan a log file and update firewall rules.
*** [https://www.fail2ban.org/wiki/index.php/Main_Page]
*** Steward administrators might choose to run fail2ban, but it won't be required at this time so we do not need to document it.
* This documentation does not need to account for Observer Rings, but can be adapted for that use case when they are part of the network.
* The documentation needs to comply with the Sovrin Network guidelines for stewards:
** As the first implementation of an Indy network, the Sovrin Network Guidelines are a useful guide for how our tool should work.
** Mandates usage of two NICs, (INDY-1249, INDY-1282)
** Mandates usage of a firewall,
** Provides specific targets for how long it should take for an IP address to be blacklisted or whitelisted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1282,INDY-1249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy73:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"27/Apr/18 11:24 AM;esplinr;[~danielhardman] points out that any firewall configuration, rather manually, from a script, or with a service like Fail2Ban, does not address DDOS attacks with rotating IP addresses. Perhaps we should close this issue and design a different solution?;;;","15/May/18 7:57 PM;esplinr;Closing this issue. Our focus will instead be on making sure that a flood of messages to the external NIC does not poison the communication on the internal NIC (see INDY-500).;;;",,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.3.57,INDY-1255,29036,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,VladimirWork,VladimirWork,31/Mar/18 1:14 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,04/Apr/18 12:00 AM,0,Documentation,,,"*Version Information*
 indy-node 1.3.57
 indy-plenum 1.2.40
 indy-anoncreds 1.0.11
 sovrin 1.1.9

*Major Fixes*
 INDY-1238 - Node is restarted because of Out of memory error
 INDY-1197 - Pool does not work after not simultaneous manual pool upgrade
 INDY-1169 - Adding new schema, field 'attr_names' of schema json can be an empty list
 INDY-1111 - Need to prevent an Identity Owner from creating a schema or claimDef
 INDY-1112 - The same primary for instances 0 and 1
 INDY-1102 - Node logs are duplicated in syslog
 INDY-1148 - It's possible to create several nodes with the same alias
 INDY-1179 - Ambiguous behavior after node demotion
 INDY-1180 - One of the nodes does not respond to libindy after several running load test
 INDY-1151 - When returning N-F nodes to the pool, ""View change"" does not occur if Primary node is stopped
 INDY-1152 - Failed restart after getting unhandled exception (KeyError)
 INDY-1269 - Unable to install indy-node if sdk repo is in sources.list

*Changes and Additions*
 INDY-1186 - A developer needs to be able to distinguish logs of each replica
 INDY-1187 - As a developer, I need to be able to track the path of each request
 INDY-1205 - Use RocksDB as a key-value storage
 INDY-1124 - Refactor common Request structure
 INDY-680 - Support anoncreds revocation in Indy
 INDY-1134 - Support REVOC_REG_DEF txn
 INDY-1135 - Support GET_REVOC_REG_DEF request
 INDY-1136 - Support REVOC_REG_ENTRY txn
 INDY-1137 - Support GET_REVOC_REG request
 INDY-1138 - Support getting state root by timestamp
 INDY-1057 - Get rid of RAET code

*Known Issues*
 INDY-1250 - Define the policy how to restore node from the state when it stashing all the reqs and there is a risk of out or memory
 INDY-1199 - Re-promoted node cannot hook up to lower viewChange
 INDY-1188 - One node fails behaind others during the load_test with high load
 INDY-1256 - Pool can be broken by primary node reboot in case of network issues between nodes

*Note:* RocksDB was added as dependency (INDY-1205). It is used for revocation, but the rest part of node functionality is still using LevelDB.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwy7r:",,,,,,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,,ozheregelya,TechWritingWhiz,VladimirWork,,,,,,,,,,"03/Apr/18 6:25 AM;TechWritingWhiz;What are the indy-plenum and the indy-ancreds numbers for this release? This is missing from the list. [~VladimirWork]

Is this it?  
 indy-anoncreds 1.0.11
 indy-plenum 1.2.34

*Please verify.*;;;","04/Apr/18 6:18 AM;TechWritingWhiz;Still waiting on confirmation. Moved due date.;;;","04/Apr/18 8:45 AM;TechWritingWhiz;I received confirmation from another team member that we do not need to note those two items at this time. I will have the .md version of the release notes ready for a pull request in the morning.;;;","04/Apr/18 4:57 PM;VladimirWork;We don't know exact versions of this packages until we release the RC because this can be not just ""+1"" so if we will prepare release notes before actual RC build we can assume indy-node and sovrin versions only.;;;","05/Apr/18 1:48 AM;TechWritingWhiz;The release notes are finished. The pull request is here: https://github.com/sovrin-foundation/sovrin/pull/53;;;","18/Apr/18 10:58 PM;ozheregelya;[~TechWritingWhiz], FYI:
Packages versions were updated.
 INDY-1269 added to Fixes, INDY-1256 added to Known Issues, notice about RocksDB added.;;;","21/Apr/18 6:36 AM;TechWritingWhiz;This has been updated: The pull request is here: https://github.com/sovrin-foundation/sovrin/pull/55;;;",,,,,,,,,,,,,,,,,,
STN lost consensus,INDY-1256,29038,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ozheregelya,mgbailey,mgbailey,31/Mar/18 3:18 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"We lost the ability to post transactions to the STN ledger.  Looking in the logs, a view change, cause unknown, occurred on all the nodes that I have access to at March 30, 02:42 GMT.  The last transaction posted to the ledger is timestamped 1522356428, which is March 29, 20:47. I have access to 7 of 10 nodes, and their logs are attached. 

At 17:16 GMT I restarted the indy-node service on those nodes and saw that in the view change the original primary was restored, and I was again able to post transactions. 

Please analyze the attached logs, and determine insofar as is possible:
 * What caused the view change?
 * Why were we unable to post transactions?
 * Following the restart, were all 10 of the nodes participating in consensus?
 ** The nodes that did not restart are NewtonD, RFCU, and ibm

Secondary questions:
 * Were there attempts to post transactions to the ledger before the view change that failed?
 * When did the fault state begin, where transactions were not posting?
 * Did the view change cause the lack of consensus, and why would it?","STN, running 1.3.55",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1179,INDY-1199,INDY-1197,,,INDY-1261,INDY-1295,,,,,,,,,,,,,,"31/Mar/18 3:14 AM;mgbailey;can-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14841/can-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;can-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14885/can-stn-p001_log_20180411.tgz","31/Mar/18 3:17 AM;mgbailey;domain_ledger.tar;https://jira.hyperledger.org/secure/attachment/14834/domain_ledger.tar","11/Apr/18 9:45 PM;ozheregelya;live_system_upgrade_problem.7z;https://jira.hyperledger.org/secure/attachment/14878/live_system_upgrade_problem.7z","31/Mar/18 3:14 AM;mgbailey;lon-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14840/lon-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;lon-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14882/lon-stn-p001_log_20180411.tgz","31/Mar/18 3:14 AM;mgbailey;nva-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14839/nva-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;nva-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14884/nva-stn-p001_log_20180411.tgz","02/Apr/18 11:27 PM;mgbailey;pool_ledger.tgz;https://jira.hyperledger.org/secure/attachment/14846/pool_ledger.tgz","31/Mar/18 3:14 AM;mgbailey;sao-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14838/sao-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;sao-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14881/sao-stn-p001_log_20180411.tgz","31/Mar/18 3:14 AM;mgbailey;seo-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14837/seo-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;seo-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14883/seo-stn-p001_log_20180411.tgz","31/Mar/18 3:14 AM;mgbailey;sgp-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14836/sgp-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;sgp-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14879/sgp-stn-p001_log_20180411.tgz","31/Mar/18 3:14 AM;mgbailey;syd-stn-p001-post.tar;https://jira.hyperledger.org/secure/attachment/14835/syd-stn-p001-post.tar","12/Apr/18 1:13 AM;mgbailey;syd-stn-p001_log_20180411.tgz;https://jira.hyperledger.org/secure/attachment/14880/syd-stn-p001_log_20180411.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz6zb:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,dsurnin,krw910,mgbailey,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,"31/Mar/18 5:32 AM;krw910;[~gudkov] We need someone to take a look at this first thing on Monday morning.;;;","02/Apr/18 11:27 PM;mgbailey;Attaching the pool ledger [^pool_ledger.tgz];;;","03/Apr/18 12:19 AM;dsurnin;[~mgbailey]
According to log we have the following:

    * What caused the view change?

Primary of master protocol instance disconnected

   * Why were we unable to post transactions?

It looks like transactions failed validation with msg like ""InvalidClientRequest('LKpJgj5zdXLwge3udVJXDD can have one and only one SCHEMA with name TranscriptSchema and version 1.0',)""

    * Following the restart, were all 10 of the nodes participating in consensus? The nodes that did not restart are NewtonD, RFCU, and ibm

No, 3 nodes that did not restart are not in consensus because we cannot view change to the lower view no [INDY-1199|https://jira.hyperledger.org/browse/INDY-1199]
;;;","03/Apr/18 12:21 AM;dsurnin;[~mgbailey]
Do we need more research on this?;;;","03/Apr/18 1:29 AM;mgbailey;Why did a primary disconnect cause this problem?  Is there a ticket being filed to prevent this from happening in the future?

[~dsurnin] Ideally I want one of 2 outcomes from this ticket: either this is a known problem with a ticket already in progress to fix it, or a new issue identified and a ticket created to fix it.;;;","03/Apr/18 10:01 PM;dsurnin;Around 02:42 primary (australia) node restarted. The other nodes started view change because primary was disconnected. austarlia started and finished view changed together with 3 more nodes. After that all other nodes restarted, including primary. New view changed started because primary disconnected but was not finished.
There are two issues that happened together
1 - Nodes cannot switch to lower viewNo (similar to [INDY-1199|https://jira.hyperledger.org/browse/INDY-1199]) - not fixed
2 - Pool has issues to restore consensus in case of more than n-f nodes restarted but the other not (similar to [INDY-1197|https://jira.hyperledger.org/browse/INDY-1197]) - fixed

The reason of restarts is unclear. Probably journalctl logs for that time could help;;;","04/Apr/18 12:19 AM;mgbailey;[~dsurnin] At 02:41 Puppet, which controls our nodes, did a package upgrade on all nodes which required a node reboot.  If INDY-1199 and INDY-1197 had been fixed, would the pool have come back up cleanly? Would the external nodes, which did not reboot, have returned to consensus as well?

 ;;;","04/Apr/18 3:57 PM;dsurnin;[~mgbailey]
I believe that chances to restore cleanly with INDY-1199 and INDY-1197 being fixed are much higher, but it requires extensive testing and monitoring;;;","06/Apr/18 1:42 AM;mgbailey;[~dsurnin] We needed to reboot the nodes on the STN again today for a software upgrade. This time I did it manually instead of via puppet, and I watched closely.  Once again, we were unable to post transactions following the reboot. I was able to restore functionality by restarting the indy-node service on my 7 nodes.

As an experiment, I rebooted just the primary node.  Even though I did not see view change transactions in the logs, I was unable to post transactions after this. [~krw910] *It would appear that a reboot of the primary node kills consensus of the network.*;;;","11/Apr/18 4:43 AM;krw910;[~dsurnin] I am not seeing any comments that state we reproduced the issue with rebooting the physical machine that is the primary (only that machine) and have it cause the pool to fail. 
Was this tried with the current Stable release?
If we know the reason can someone post it as a comment?
I don't believe the cause has anything to do with INDY-1199 or INDY-1197. 

[~VladimirWork] I do not want this ticket closed until we have an answer to this issue {color:#d04437}*not a guess or a feeling*{color} that it is fixed.

[~mgbailey] FYI;;;","11/Apr/18 4:59 PM;VladimirWork;[~zhigunenko.dsr] Please add all results and findings of this case testing against our persistent pool in this ticket.;;;","11/Apr/18 7:36 PM;dsurnin;[~krw910] [~mgbailey]

INDY-1197 - is about the situation when small amount of nodes will not send current state to the newly connected nodes if total number of nodes is not enough for consensus - because of absence of insufficient current state msgs node will not be able to finish view change.
INDY-1199 - initially this issue was found with demotion/promotions tests but the issue could be reproduced with restart also. If current view change number is greater than 0 and lots of nodes restarted (enough for consensus), then restarted nodes will choose viewNo 0, but the other nodes will change view to lower number and will not be able to participate in consensus.

In our case primary restarted, view change started, several nodes changed view, other nodes restarted and restored in the pool one by one. As a result restarted nodes cannot change view to the bigger viewNo (because of INDY-1197) and nodes that were not restarted cannot change to lower viewNo (INDY-1199)

Primary restart does not lead to pool to reject txns. Just to view change. However during the view change pool stashing all the txns and start to process them only after view change is done. It might looks like pool is stopped since we cannot guaranty that view change will be finished in any constant time.

We have series of tests for pool restarting in plenum/tests/restart folder.
;;;","11/Apr/18 9:15 PM;zhigunenko.dsr;*Environment:*
AWS Persistent Pool (18 nodes)
indy-anoncreds                   1.0.11
indy-node                        1.3.55
indy-plenum                      1.2.34
libindy-crypto                   0.1.6-10
python3-indy-crypto              0.2.0

*Steps to reproduce:*
1. open CLI on non-primary node 
2. on primary node execute _sudo apt-get update && sudo apt-get upgrade_ and then _sudo reboot_
3. during updating/rebooting send NYMs from non-primary node
*Actual results:*
Ex-primary node cannot catch up new txns. Rest of nodes are consider ex-primary as unreachable
*Expected results:*
Ex-primary successfully catch up and write new txns.;;;","11/Apr/18 9:46 PM;ozheregelya;Logs from QA Persistent pool: [^live_system_upgrade_problem.7z]Unfortunately, only INFO level.;;;","11/Apr/18 10:27 PM;krw910;[~dsurnin] I agree with your assessment of the tickets you mentioned and how they would affect that scenario. See my other comments to the testers.

[~VladimirWork] [~zhigunenko.dsr] [~ozheregelya]

My concern about this ticket is [~mgbailey] can just reboot the physical machine that is the primary and have the pool stop reaching consensus. Nothing else is happening outside of just rebooting the primary. No manual upgrade and no rebooting of other nodes to cause this.

The rebooting of the other nodes in the logs is to get the pool functioning again. If Mike reboots 7 of the nodes he controls in the STN is is enough to get the  pool functioning again. He does not need to restart the entire pool. 

Due to what Mike is telling me it does not seem to match the other tickets mentioned and it does not sound like we have reproduced the issue, because in the steps mentioned in Nikita's comments it talks about updating/rebooting the primary. Mike is not updating the primary he is just rebooting the machine.;;;","11/Apr/18 11:30 PM;krw910;[~dsurnin] [~zhigunenko.dsr] [~VladimirWork]

Since the team cannot reproduce the exact issue [~mgbailey] said he can set 7 of the STN nodes logging to debug and reproduce the issue. Once he does he will get the STN logs to development to debug. Please keep this ticket open until we can resolve it with Mike.;;;","12/Apr/18 1:15 AM;mgbailey;[~krw910] [~dsurnin] I ran a new experiment on the STN. Here are the steps:
 * at 15:16, send NYM dest=Bb7stpoCv7S2GDxT1FQa1 verkey=~Snj2hv6VAAfHbBFYv4i4Tj
 ** Result: successfully *posted* to the STN
 * at 15:19, reboot 'australia', the current primary
 ** No view change seen on other nodes.  On 'australia', the same primaries are selected.
 * at 15:43, send NYM dest=Bb7stpoCv7S2GDxT1FQa2 verkey=~Snj2hv6VAAfHbBFYv4i4Tj
 ** Result: *not posted* to the STN

Attaching new logs for my 7 nodes, all with TRACE level logs.[^sgp-stn-p001_log_20180411.tgz] ;;;","12/Apr/18 1:31 AM;mgbailey;I have done some additional tests.  I am not adding additional logs, since I believe that there will be no additional data in them.
 * restart the service on the pool
 ** consensus restored
 * on the primary: sudo systemctl restart indy-node
 ** result: consensus lost
 ** view change occurred only on the primary node (this may be key)
 * restart the service on the pool
 ** consensus restored
 * on a different node: sudo systemctl restart indy-node
 ** result: consensus maintained;;;","12/Apr/18 3:07 AM;ozheregelya;*Environment:*
 indy-node 1.3.364 (master)
 AWS QALive pool (20 nodes) with 260,490 txns in ledger.

*Steps to Reproduce:*
 1. Make sure that all nodes are in consensus and that all nodes have the same primary.
 2. Reboot instance with primary node (Node10).

*Actual Results:*
 ViewChange was not happened on one node (Node15). For the rest nodes primary was changed (to Node11). Pool is still in consensus. 
After restart of problematic node (Node15) primary on this node was changed (to Node11), problematic node successfully completed catch-up and continued to work properly.

*Logs:* [https://drive.google.com/file/d/1CBZYr2pMs1dapRqVbKSvn9RgVIb5quas/view?usp=sharing]
 *Journalctl:* [https://drive.google.com/file/d/1_Nd4Exqe2Bhh1_OHh0l5c0y56obsGZMb/view?usp=sharing]

*Additional Information:*
 Similar case was tried by [~zhigunenko.dsr] and [~VladimirWork] on another pools with *stable* version. ViewChange was successfully completed on all of the nodes in case of simple reboot.;;;","12/Apr/18 9:51 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.364 (master)
AWS QALive pool (20 nodes)

*Steps to Reproduce:*
1. Make sure that all nodes are in consensus and that all nodes have the same primary.
2. execute on primary node (Node11) this ""_execute sudo apt-get update && sudo apt-get upgrade && sudo reboot_""
*Actual results:*
Ex-primary node cannot catch up new txns. Rest of nodes are consider ex-primary as unreachable
*Expected results:*
Ex-primary successfully catch up and write new txns.

[Logs|https://drive.google.com/open?id=1YxjUlWcSxpXylq1Mu09dIY9kcyqJ5WV3]
Thu Apr 12 07:31:20 2018 -  approximate reboot time
;;;","13/Apr/18 7:29 PM;dsurnin;[~mgbailey] [~krw910]
For the moment I was able to check logs from Mike.
It is propagate primary case but for primary node, i.e. primary node was disconnected, some nodes started to send instance change msgs but consensus was not reached until primary was rebooted and started initialization. It looks like node was not able to correctly restore the last ordered ppseqno and started to use 0 while the rest of pool use 55, as a result all backup replicas were able to order txn while master replica was not able to reach consensus - all the nodes dropped packets with the same viewNo and ppseqno less than last ordered ppseqno.
Recently we fixed propagate primary case for non-primary node, it was discovered during the fixing of [INDY-1018|https://jira.hyperledger.org/browse/INDY-1018] .
The case with propagate primary for primary node is difficult to reproduce with the default settings in a good and stable network environment.

Our QA even use restart primary as a simplest way to initiate view change.

I will continue to analyze the other logs and test cases attached to ticket.;;;","25/Apr/18 11:24 PM;dsurnin;The issue reproduces by [~mgbailey] is fixed in PR
https://github.com/hyperledger/indy-plenum/pull/641

Reason is the processing current state msgs issue during the propagate primary process in case of primary node - msgs were ignored.

Recommendations for QA
one can try to reproduce the issue by changing config parameter ToleratePrimaryDisconnection to some big value, i.e. 100 or more on all the nodes in the pool.;;;","25/Apr/18 11:25 PM;dsurnin;For the other mentioned issues separated ticket is created - [INDY-1295|https://jira.hyperledger.org/browse/INDY-1295];;;","03/May/18 1:08 AM;ozheregelya;*Environment:*
 indy-node=1.3.396
AWS pool of 4 nodes

*Steps to Validate:*
1. Set up the pool with ToleratePrimaryDisconnection = 100, send several transactions.
2. Reboot the primary node (ViewNo 0).
=> Pool works right after reboot, ViewNo is still 0.
3. Restart the primary.
=> View Change was happened as usual, ViewNo is changed to 1.
4. Reboot new primary.
=> Pool works right after reboot, ViewNo is still 1.

*Actual Results:*
Pool works without any issues in case of reboot/restart primary node.;;;",
Fix errors in post-install automation tests,INDY-1257,29046,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,31/Mar/18 5:28 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Build Info:
indy-node master
libindy master
python3-indy master (pip3 install python3-indy==1.3.1dev441)

Actual Results:
Acceptance cases: 5/5 failed.
Functional cases: 182/182 failed.

Expected Results:
All cases should be passed or skipped due to bugs in indy or libindy functionality.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1270,,,,,,,,,,,,,,,,,,,,"10/Apr/18 1:44 AM;VladimirWork;200_of_204_passed_4_ignored.PNG;https://jira.hyperledger.org/secure/attachment/14873/200_of_204_passed_4_ignored.PNG","13/Apr/18 8:19 PM;VladimirWork;RC_run.PNG;https://jira.hyperledger.org/secure/attachment/14890/RC_run.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz41b:",,,,,,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"10/Apr/18 2:03 AM;VladimirWork;All post-install tests are reworked (also `signus_sign_works_for_valid_data_test_for_create_key.py` is added due to libindy changes).

Now we have 200 of 204 cases passed against master environment [indy-node 1.3.363 pool / libindy 1.3.1~441 / python3-indy 1.3.1.dev442]. !200_of_204_passed_4_ignored.PNG|thumbnail! 

4 of 204 are skipped:
test_scripts/acceptance_tests/verify_messages_on_connection_test.py - *not implemented* (will be implemented in scope of new ticket about support of post-install tests)
test_scripts/functional_tests/crypto/crypto_box_open_fails_with_another_verkey_test.py - we can't specify their_vk (correct or incorrect) for message decryption in master implemetation
test_scripts/functional_tests/crypto/crypto_box_open_fails_with_incorrect_nonce_test.py - we can't specify nonce (correct or incorrect) for message decryption in master implemetation
test_scripts/functional_tests/negative_and_boundary/build_get_schema_test.py - *blocked by IS-540*

FYI [~krw910];;;","10/Apr/18 7:43 PM;VladimirWork;https://github.com/hyperledger/indy-post-install-automation/pull/9;;;","13/Apr/18 8:19 PM;VladimirWork;All tests are passed against RC 1.3.56 pool using libindy 1.3.1~441 and python3-indy 1.3.1.dev442  !RC_run.PNG|thumbnail!  but there are some errors with this tests against *current* master packages so INDY-1270 is reported to investigate and fix it.;;;",,,,,,,,,,,,,,,,,,,,,,
The node blacklisted the remaining nodes during network problems,INDY-1258,29140,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,zhigunenko.dsr,zhigunenko.dsr,04/Apr/18 7:23 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Steps to reproduce:*
 1) create pool with 15 nodes (privileged mode)
 2) apply [suppressor.sh|https://drive.google.com/open?id=1fSA-Z2HUXXe9N4vcPiHsjL1DLIzK28x2] (updated) for node1, node2, node3, node7, node13 (started with $S2=1.97..2.02 sec delay, not in the single moment)
 3) start load test 100threads * 100txns
 *Actual results:*
 Pool finished with 5072 txns, but node1, node2, node3 have only 592 txns. All nodes shows each of others as reachable.
 After connection reestablishing these nodes couldn't catch up with the pool (without or with node restarting)

[Logs|https://drive.google.com/file/d/1lGtyB4X3tub3STMhjFGwY055RwYVDotT/view?usp=sharing]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1210,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz4hj:",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,"07/Sep/18 10:12 PM;zhigunenko.dsr;*Reason to Close:*
Issue is outdated;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool stops taking txns at 3000 writing connections,INDY-1259,29177,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,sergey-shilov,VladimirWork,VladimirWork,05/Apr/18 7:51 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"T-Shirt: XL

Build Info:
indy-node 1.3.357

Steps to Reproduce:
1. Run from 3 machines {1000 clients x 10 NYMs each} load test against 25 nodes pool.
2. Check script results and validator-info at all nodes.

Actual Results:
Script doesn't finish because pool stops taking txns. Pool stops writing after 13..21k (differs on different nodes) txns written of 30k expected.
Journalctl is in attachment. Nodes' logs will be added to google drive.

Workaround:
Restart all nodes simulaneously several times to return pool in normal state.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1235,,,,,INDY-1296,INDY-1350,,,,,,,,,,,,,,"05/Apr/18 7:50 PM;VladimirWork;journalctlfromallnodes.tar.gz;https://jira.hyperledger.org/secure/attachment/14850/journalctlfromallnodes.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz9v3:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,ashcherbakov,lovesh,sergey.khoroshavin,sergey-shilov,VladimirWork,,,,,,,,"05/Apr/18 11:30 PM;VladimirWork;[~dsurnin] https://drive.google.com/drive/u/1/folders/1nCSqWRMI-TtfcGMSf6fZIRZzJmXLKXSy

Debug logs from 1, 11, 24 nodes.;;;","12/Apr/18 8:35 PM;VladimirWork;Now we have the same issue at 2000 clients (4 machines x 500 clients x 10 NYMs each) on 1.3.365 master.;;;","13/Apr/18 5:47 PM;sergey.khoroshavin;I've investigated logs from last session (2000 clients on 1.3.365 master) and found that nodes started having incorrect state trie. Also, before pool stopped working min/avg/max time to order (for now gathered only from 25th node) was ~500/600/700 seconds. Unfortunately due to rotation logs were not captured from the very start, we are going to restart load test from scratch in order to fix this.;;;","20/Apr/18 9:48 PM;sergey.khoroshavin;Incorrect state trie case was reproduced again, now with logs from the very start. Further investigation showed that all incorrect state trie events happened during view change, and after disabling view change test pool (created from scratch) was able to write (from 2000 clients) 10k+ 20k + 20k + 50k + 20k = 120k transactions without performance degradation.

Also after thoroughly looking at one of incorrect state trie events it was found that:
1) all nodes managed to get prepare quorum for (32, 14) before view change started
2) some nodes managed to get commit quorum for (32, 14) before finishing catchup, and some not
3) nodes that didn't order (32, 14) received some consistency proofs containing (32, 14) and apparently this is the reason they couldn't finish catchup
4) now nodes that didn't finish catchup started ordering new transaction (33, 1) based on (32, 13), and nodes that did finish catchup stopped ordering transactions at all

Now I'm trying to figure out what can be done to avoid this situation.;;;","26/Apr/18 2:34 AM;sergey.khoroshavin;Full logs for incorrect state trie case:
https://drive.google.com/file/d/1QW7VG2PIbArwVhcmHmc1qNCKIGa91Llp

Would be great if [~lovesh] could take a look at them.;;;","07/May/18 4:10 PM;ashcherbakov;The found cause of the issue is that view-change was started in a different time (a lag about 40 seconds).
The nodes that started view change earlier were not able to get COMMITs for last prepared certificate because of View Change timeout (60 seconds, or 5 catch-ups with 0 txns caught-up).
Some nodes (that started later) were able to get COMMITs for last prepared certificate, so ended view change with a different state.

Increasing timeout didn't help.;;;","08/May/18 4:23 PM;lovesh;[~sergey.khoroshavin] 
bq. some nodes managed to get commit quorum for (32, 14) before finishing catchup, and some not
How many?
;;;","08/May/18 11:34 PM;sergey.khoroshavin;[~lovesh]
{quote}How many?{quote}
6:

09:09:15 Node6
09:09:21 Node21
09:09:27 Node15
09:09:31 Node17
09:11:14 Node12
09:15:43 Node20
;;;","08/May/18 11:44 PM;ashcherbakov;Our current thinking about ViewChange problems:
1) The current approach has the assumption that >f nodes will eventually commit their prepared certificates. However, it's possible only if there is no big crashes. 
2) Another problem with the current protocol is that it relies on some timestamp, that is eventually the prepared certificates will be committed or caught up. However, this is not very deterministic. 

Also there are probably some issues with implementation of the current View Change approach:
1) Whether we give enough possibility for prepared certificates to be ordered (gather COMMITs) before/between rounds of catch-up that we are doing.
In particular, we start catch-up immediately, and it may revert some state (needed for committing prepared certificates). 
2) Whether we have correct timeouts
3) Whether we should separate New Primary Selection Timeout (we need it since we use round robin for primary selection, and the new Primary can be unavailable), and All-Nodes-In-Sync timeout, that is the timeout we use for making sure that all nodes eventually in sync for the new view (prepared certificates committed, catch-up finished, etc.) 
;;;","14/May/18 3:06 PM;lovesh;[~ashcherbakov] [~sergey.khoroshavin] Our current approach has been to assume that eventually every node will do catchup and get sufficient messages to get to its last prepared certificate. We should have a {{ViewChangeStart}} to complement {{ViewChangeDone}}. {{ViewChangeStart}} should indicate the state at which the node is starting its view change (got >2f InstanceChange) messages. Apart from other info {{ViewChangeStart}} should contain the last prepared batch no ({{last_prepared_certificate_in_view}});;;","14/May/18 3:25 PM;ashcherbakov;How ViewChangeStart should be propagated? As we don't have signatures over Node messages, we need to have quorums for them and can not trust just one such a message.
What is the purpose of ViewChangeStart? How other nodes should process it?
I feel like introducing this kind of messages we will end up in something very similar to PBFT.

Although I think we can make the current approach working in most of the positive cases by changing timeouts, I'm still in favour of PBFT approach as eventual option.;;;","14/May/18 3:35 PM;lovesh;{{ViewChangeStart}} is broadcasted by each node to the pool on starting view change.
{{ViewChangeStart}} makes each node aware of the exact state of other nodes before they started view change (stopped processing new requests). 
;;;","14/May/18 4:16 PM;sergey.khoroshavin;[~lovesh] [~ashcherbakov] Well, if we introduce ViewChangeStart how can we be sure that some malicious nodes are not lying, or not sending different ViewChangeStart to different nodes? Do we need ViewChangeStartAck like in PBFT?;;;","17/May/18 6:49 PM;ashcherbakov;I suggest we analyse the logs we have in a try to increase the timeouts and after this close the ticket as further work on tests and fixes will be continued in the scope of INDY-1341, INDY-1303, INDY-1304.;;;","19/May/18 1:45 AM;sergey-shilov;We've done testing with increased view change time out (30 minutes), analysed logs and got the following result:

 - the view change finished by view change time out spent;

 - after finished view change the pool continued ordering transactions.

But despite continuation of ordering transaction such a long view change time out is not a good solution, because the probability of waiting till view change time out during view changed process is very high. The reason for it is unaligned _last_prepared_certificate_ (unlike PBFT)_._ Before starting of the view change process each node calculates it's own _last_prepared_certificate_ that corresponds to the last prepare message that has a quorum, so it's highly likely that these certificates are different for different nodes. If more than _f_ but less then _N-f_ nodes have higher prepared seqno than others then they can not complete a catch up that is sub process of view change. This leads to waiting till the view change time out. For example, in our test we observed the following situation:

======================================================================

2018-04-28 14:49:45.537000 |  Node11:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:58.897000 |  Node12:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:49.527000 |  Node15:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:52.869000 |  Node16:0 setting last prepared for master to (0, 148)
2018-04-28 14:49:48.189000 |  Node18:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:57.800000 |  Node2:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:48.270000 |  Node23:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:53.251000 |  Node24:0 setting last prepared for master to (0, 148)
2018-04-28 14:49:53.727000 |  Node25:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:48.534000 |  Node3:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:51.814000 |  Node7:0 setting last prepared for master to (0, 149)
2018-04-28 14:49:47.264000 |  Node8:0 setting last prepared for master to (0, 149)

 

2018-04-28 14:49:44.947000 |  Node1:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:46.604000 |  Node10:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:50.750000 |  Node13:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:58.539000 |  Node14:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:51.600000 |  Node17:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:47.240000 |  Node19:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:50.815000 |  Node20:0 setting last prepared for master to (0, 151)
2018-04-28 14:49:55.239000 |  Node21:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:53.077000 |  Node22:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:51.266000 |  Node4:0 setting last prepared for master to (0, 150)
2018-04-28 14:49:57.592000 |  Node5:0 setting last prepared for master to (0, 151)
2018-04-28 14:49:51.674000 |  Node6:0 setting last prepared for master to (0, 150)
2018-04-28 14:50:16.465000 |  Node9:0 setting last prepared for master to (0, 150)

======================================================================

As a result the nodes from the first group were waiting for _VIEW_CHANGE_DONE_ messages from the nodes from the second group until view change time out have spent.

So the risks of long view change time out are the following:
 * high memory usage as we stash all incoming messages during the catch up process that may lead to Out Of Memory;
 * unavailable pool for a long time equal to view change time out
 * potential kind of attack to keep the pool infinitely unavailable

So one of the core problems of current view change protocol is unaligned _last_prepared_certificate_ that leads to incomplete catch up during the view change process. It's not very critical as pool nodes complete the view change process by time out and continue ordering, but it adds risks described above.

One of the possible solution is to increase view change time out up to 5 minutes (it's approximate, we will clarify it in further related tickets) as current 1 minute is too low.

There is another problem with inconsistent state trie that is not reproduced with long time out, we'll continue trying to reproduce it in scope of ticket [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350]. I think that this ticket may be closed.;;;",,,,,,,,,,
Pool stops taking txns at ~178k txns written in ledger,INDY-1260,29178,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,VladimirWork,VladimirWork,05/Apr/18 8:07 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,TShirt_M,,,"Build Info:
indy-node 1.3.357

*Steps to Reproduce:*
1. Run multiple load tests (reading and writing) against 25 nodes pool.
2. Reach ~178k txns written.
3. Try to write some additional txns (via load script or CLI).
4. Check validator-info at all nodes.

*Actual Results:*
One of nodes (24) stopped at 49k txns and doesn't catch up. One of nodes (11) stopped at 123k txns and doesn't catch up. All other nodes have 175..178k txns. All nodes show *25/25 reachable hosts* in validator-info but pool *has no consensus*. Whole pool restart doesn't help.
Journalctl is in attachment. Nodes' logs will be added to google drive.

*Acceptance Criteria:*
Diagnose the issue, decide on a Plan of Attack, and raise the appropriate epics and stories that can be scheduled.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1235,,,,,,,,,,,,,,,,,,,,"05/Apr/18 8:10 PM;VladimirWork;INDY-1260.PNG;https://jira.hyperledger.org/secure/attachment/14852/INDY-1260.PNG","05/Apr/18 7:51 PM;VladimirWork;journalctlfromallnodes.tar.gz;https://jira.hyperledger.org/secure/attachment/14851/journalctlfromallnodes.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzcyv:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,ashcherbakov,Derashe,dsurnin,VladimirWork,zhigunenko.dsr,,,,,,,,"06/Apr/18 1:14 AM;VladimirWork;[~dsurnin] https://drive.google.com/drive/u/1/folders/1zyAVAMOPI0A334mH1qfF6qKixG-5uj3b

Debug logs from 1, 11, 24 nodes.;;;","24/Apr/18 9:59 PM;ashcherbakov;[~sergey.khoroshavin] [~VladimirWork]
What load scripts were run against the pool? Is it a script simulating high load (many clients), or scripts with minimal activity?;;;","26/Apr/18 3:45 AM;VladimirWork;[~ashcherbakov] There were several high load cases with 500..2000 simultaneous writing/reading clients against this pool before it was broken.;;;","28/Apr/18 7:41 PM;zhigunenko.dsr;[~ashcherbakov]
During performance testing with _indy-node 1.3.364_ and _indy-plenum 1.2.310_  24 of 25 nodes stopped to write after 298k txns under *5 clients * 1txns / sec* load
[~dsurnin] has all available logs + journalctl from whole pool

{code}
#unsafe=set(['disable_view_change'])
MAX_CATCHUPS_DONE_DURING_VIEW_CHANGE = 5000
MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE = 1800
{code}
;;;","14/May/18 8:52 PM;ashcherbakov;We need to make sure if the issue is caused by View Change. If so, it will be fixed together with INDY-1341 and INDY-1350.;;;","05/Jun/18 1:17 AM;Derashe;Problem reason:
 * Pool, that described in description of this ticket, stopped write transactions.

Inverstigation:
 * During the inverstigation of logs 2018-04-19 - 2018-04-20, we found that pool had some troubles, but was able to write txns in ledger.
 * Founded troubles is about breaking consensus in some nodes because of incorrect state tree. This was caused by incorrect process of applying stashed batches after catchup.
 * In this particular case, error appeared from outdated code of order_3pc_key method in plenum's replica class. This method used to call apply_stashed_reqs from plenum's node class. So, after ending of catchup last request used to apply two times. Up-to-date plenum don't have this bug.
 * Another noticed bug that probably could cause incorrect state is about incorrect stashing of checkpoints. That problem was resolved in INDY-1329
 * Similar errors were found in INDY-1350 and INDY-1315
 * apply_stashed_reqs mechanism will be fixed in scope of INDY-1328

Recommendations for QA
 * Try to run updated pool after completing of INDY-1328;;;",,,,,,,,,,,,,,,,,,,
Provisional network has lost consensus,INDY-1261,29203,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,mgbailey,mgbailey,mgbailey,06/Apr/18 2:45 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"The live network is not posting transactions that are sent to it.  There are no transactions posted since late March.  

Ev1, which was probably the primary node, was rebooted at 15:27 on March 30, and a view change resulted.  I suspect that this is a manifestation of the same problem being investigated in INDY-1256. 

Attached are logs from ev1. ",Provisional network running 1.3.55,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1256,,,,,,,,,,,,,,,,,,,,"07/Apr/18 12:08 AM;mgbailey;BIGAWSUSEAST1-001.logs.tgz;https://jira.hyperledger.org/secure/attachment/14864/BIGAWSUSEAST1-001.logs.tgz","07/Apr/18 12:08 AM;mgbailey;OASFCU.logs.tgz;https://jira.hyperledger.org/secure/attachment/14863/OASFCU.logs.tgz","07/Apr/18 12:08 AM;mgbailey;ServerVS.logs.tgz;https://jira.hyperledger.org/secure/attachment/14866/ServerVS.logs.tgz","07/Apr/18 12:08 AM;mgbailey;Stuard.logs.tgz;https://jira.hyperledger.org/secure/attachment/14867/Stuard.logs.tgz","07/Apr/18 9:10 AM;mgbailey;atbsovrin.logs.tgz;https://jira.hyperledger.org/secure/attachment/14870/atbsovrin.logs.tgz","06/Apr/18 8:23 AM;mgbailey;danube.logs.tgz;https://jira.hyperledger.org/secure/attachment/14861/danube.logs.tgz","06/Apr/18 8:23 AM;mgbailey;digitalbazaar_logs_20180405.tgz;https://jira.hyperledger.org/secure/attachment/14860/digitalbazaar_logs_20180405.tgz","07/Apr/18 9:10 AM;mgbailey;ev1_log_20180406.tgz;https://jira.hyperledger.org/secure/attachment/14869/ev1_log_20180406.tgz","08/Apr/18 4:34 AM;mgbailey;ev1_log_20180407.tgz;https://jira.hyperledger.org/secure/attachment/14871/ev1_log_20180407.tgz","06/Apr/18 2:45 AM;mgbailey;ev1_logs.tgz;https://jira.hyperledger.org/secure/attachment/14859/ev1_logs.tgz","07/Apr/18 12:08 AM;mgbailey;icenode.logs.tgz;https://jira.hyperledger.org/secure/attachment/14868/icenode.logs.tgz","20/Apr/18 10:12 PM;sergey-shilov;live_pool_ips.list;https://jira.hyperledger.org/secure/attachment/14903/live_pool_ips.list","06/Apr/18 8:23 AM;mgbailey;pcValidator01.logs.tgz;https://jira.hyperledger.org/secure/attachment/14862/pcValidator01.logs.tgz","20/Apr/18 10:12 PM;sergey-shilov;ping_monitor.sh;https://jira.hyperledger.org/secure/attachment/14902/ping_monitor.sh","07/Apr/18 12:08 AM;mgbailey;zaValidator.logs.tgz;https://jira.hyperledger.org/secure/attachment/14865/zaValidator.logs.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz6zj:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,EvelynEvergreene,mgbailey,sergey-shilov,,,,,,,,"06/Apr/18 8:23 AM;mgbailey;Attaching additional logs provided by stewards. ;;;","06/Apr/18 9:54 PM;dsurnin;Before ev1 restart current viewNo was 3 and primary was royal_sovrin.
After the restart ev1 nothing had changed - viewNo 3, primary royal_sovrin.

According to logs there are lots of nodes disconnections, i.e. last log from current master digitalbazaar  for the 2018-04-05 contains 

{code:java}

2018-04-05 15:12:53,921 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from zaValidator
2018-04-05 15:13:09,936 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to zaValidator
2018-04-05 16:10:39,690 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from ev1
2018-04-05 16:11:29,565 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to ev1
2018-04-05 16:48:29,158 | INFO     | propagator.py        (179) | propagate | digitalbazaar propagating request ('J4N1K1SEB8uY2muwmecY5q', 1522946906091066) from client b'YNY!QhQzswkJE9%9uu)g(U@*AgU$Oy1{N%?27ofz'
2018-04-05 16:54:07,450 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from ev1
2018-04-05 16:54:22,446 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to ev1
2018-04-05 16:55:23,625 | INFO     | propagator.py        (179) | propagate | digitalbazaar propagating request ('J4N1K1SEB8uY2muwmecY5q', 1522947320775122) from client b'YNY!QhQzswkJE9%9uu)g(U@*AgU$Oy1{N%?27ofz'
2018-04-05 21:00:00,103 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from DustStorm
2018-04-05 21:17:05,650 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from pcValidator01
2018-04-05 21:25:39,998 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from danube
2018-04-05 21:33:52,298 | INFO     | keep_in_touch.py     (98) | _connsChanged | CONNECTION: digitalbazaar now connected to pcValidator01
2018-04-05 21:37:33,419 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from pcValidator01
2018-04-05 21:52:41,945 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from atbsovrin
2018-04-05 21:53:20,367 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from royal_sovrin
2018-04-05 22:01:16,490 | INFO     | keep_in_touch.py     (93) | _connsChanged | CONNECTION: digitalbazaar disconnected from prosovitor
digitalbazaar.log (END)

{code}

I will continue research, but for now it looks like nodes cannot reach quorum because of network issue
;;;","07/Apr/18 12:12 AM;mgbailey;[~dsurnin] can you tell what was the primary prior to the March 30th restart of ev1?  I suspect that this is when the consensus was lost.  I don't believe that there were network issues.  All validators showed as connected.  I am beginning to believe that a reboot of the primary node halts consensus.;;;","07/Apr/18 8:40 AM;mgbailey;Update: We brought down the service on all nodes.  Then I sent out a message to all stewards to start the service again, with ev1 being the first up.  Over the course of a few hours stewards brought up nodes.  I can see a view change in the logs, with ev1 selected as the primary.  However, I am still unable to post a transaction to the network.
I am uploading updated ev1 logs here.

We are also considering an upgrade with force=True to an unchanged release to force simultaneous restart of the service.;;;","08/Apr/18 4:37 AM;mgbailey;We got up to 13 / 17 nodes connected which should have been enough for consensus, with 1 to spare.  We were unable to post transaction.  Two more nodes connected up today, danube and zaValidator, so we are now up to 15 nodes.  *And we are now able to post a transaction!* 

So the new question for the ticket is _why were we unable to post a transaction with 13 nodes connected?_  One possibility is that I noticed that danube and zaValidator were among the primaries selected during the view change, even they were not up.  Could it be that these particular nodes were required for consensus?

I will post new ev1 logs for comparison, dated 20180407, now that the transaction was able to be posted: [^ev1_log_20180407.tgz]

DID of the transaction that I unsuccessfully posted last night: UCiMTm8EATcg4rEb7Pi2hX

DID of the transaction that I successfully posted today:  UCiMTm8EATcg4rEb7Pi2iX

(they vary in only 1 character);;;","09/Apr/18 9:54 PM;dsurnin;It would be good to have logs from several more nodes - at least 3 or 4 mode nodes.
According to ev1 logs 13 nodes were not enough because not all nodes responded for requests.
I cannot see actual reason since I do not have all the logs.
After 15 nodes connected consensus were reached;;;","09/Apr/18 11:17 PM;mgbailey;[~dsurnin], which nodes do we need logs from?  I will request them.;;;","10/Apr/18 12:02 AM;dsurnin;it would be good to have logs from at least zaValidator and danube. any additional nodes you can get logs from would helpful.

also could you please run get nym req with the one you marked as unsuccessful UCiMTm8EATcg4rEb7Pi2hX and provide the output of client?;;;","10/Apr/18 5:25 AM;mgbailey;zaValidator and danube were down (the service was off) until 04/07 06:42 GMT, and 04/06 22:47 GMT, respectively, so there will be no logs in that time period.  See [https://docs.google.com/spreadsheets/d/19svUEFuEPrgEtz-K-bUTfjHh7nHFqjaXDGqH6417Xd4/edit?usp=sharing.]

GET_NYM shows that the DID was posted, but further analysis of the logs shows that the posting happened 2 minutes after the client sent the message.;;;","11/Apr/18 6:41 PM;dsurnin;[~krw910] [~mgbailey]

There are only two nodes have debug logs attached, the other logs are info - a couple of strings each day.

Before ev1 restart on 2018-03-30 15:26:42,071 the viewNo is 3 and primary is royal_sovrin.
According to logs several nodes tried to make view change due to master disconnected but had no quorum (Aalto, zaValidator), it could be because of some network issues on that nodes.
After restart nothing had changed primary is royal_sovrin, viewNo is 3.
Next view change, due to master disconnected, was around 2018-04-02 05:17:39. ev1 selected digitalbazaar as primary for viewNo 4.
Several nodes, including new primary, had a view change timeout expired (no debug logs) and tried to send several INSTANCE_CHANGE msgs, but according to ev1 logs it was finished since lots of nodes send CURRENT_STATE with the same viewNo and primary name.

According to [~mgbailey] there were 6 txns posted to pool during period

{code:java}
date    time    DID verkey
2018-04-05  16:48:26,090    MbGA1UMnzv6XgRo8MFHGXp  verkey=~SbXSz233xX75CF4gQ81fXp
2018-04-05  16:55:20,774    MbGA1UMnzv6XgRo8MFHGXq  verkey=~SbXSz233xX75CF4gQ81fXp
2018-04-06  18:18:00,922    UCiMTm8EATcg4rEb7Pi2hZ  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz
2018-04-06  20:07:44,563    UCiMTm8EATcg4rEb7Pi2hY  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz
2018-04-06  23:19:43,839    UCiMTm8EATcg4rEb7Pi2hX  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz
2018-04-07  18:51:46,827    UCiMTm8EATcg4rEb7Pi2iX  verkey=~Yc5GEBBZ5sr8AqqJ7qoGZz
{code}

The first 3 of them were not actually added. I cannot detect why since the nodes with debug logs received those txns as MESSAGE_RESPONSES with discard index set, but discard reason is not specified.

The last 3 of them are added. See some ouput from new cli

{code:java}
pool(live):wallet(wallolo):did(V4S...e6f):indy> ledger get-nym did=UCiMTm8EATcg4rEb7Pi2hY
| Sequence Number | Request ID          | Transaction time    |
| 59              | 1523347539014355822 | 2018-04-06 20:07:44 |


pool(live):wallet(wallolo):did(V4S...e6f):indy> ledger get-nym did=UCiMTm8EATcg4rEb7Pi2hX
| Sequence Number | Request ID          | Transaction time    |
| 60              | 1523347543589883827 | 2018-04-06 23:21:55 |


pool(live):wallet(wallolo):did(V4S...e6f):indy> ledger get-nym did=UCiMTm8EATcg4rEb7Pi2iX
| Sequence Number | Request ID          | Transaction time    |
| 61              | 1523347563764991752 | 2018-04-07 18:51:47 |
{code}

So the pool is probably working.

From my point of view most of the problems are connected with network.
For tests [~mgbailey] uses old client. This client version has internal message queue and resends all the messages to the nodes after they becomes reachable.

Primary restart does not lead to pool to reject txns. Just to view change. However during the view change pool stashing all the txns and start to process them only after view change is done. It might looks like pool is stopped since we cannot guaranty that view change will be finished in any constant time.

We have series of tests for pool restarting in plenum/tests/restart folder.
;;;","12/Apr/18 12:13 AM;mgbailey;If there are network problems, we need to have them pin-pointed to particular nodes so that we can debug the nodes' networking configuration.  We need this in the logs, at least. Is there a ticket to provide logging for this issue?

We really can't just say that an outage might have been due to network issues on a production network, without better / more specifics. In addition, the network should be able to recover from a network outage without requiring a system-wide restart.;;;","17/Apr/18 5:16 PM;ashcherbakov;I think we need to investigate whether there are network issues on the live pool, and what is the cause.

We can create and run simple scripts (ping-pong) to test network communication. We can check firewall settings on the nodes.;;;","20/Apr/18 12:23 AM;sergey-shilov;Hi [~mgbailey],

I've implemented a simple script to measure ping time to the specified list of IPs. This is needed for ping statistics. 
 I've attached two files:
 1. *ping_monitor.sh* - script itself
 2. *live_pool_ips.list* - list of live pool nodes IPs, provided by Olga (please, re-check it).

Could I ask you to send this script and IPs list to stewards to run it from their nodes and then gather stats? Script writes result to the file named _ping_stats.out_, so that we can gather ping stats for each connection.

Usage:

_$ ping_monitor.sh ./live_pool_ips.list_;;;","20/Apr/18 12:35 AM;mgbailey;[~sergey-shilov], I will see about incorporating this into the log aggregator script that I am writing, and having the results upload to a centralized server.;;;","20/Apr/18 7:16 AM;mgbailey;[~sergey-shilov]

I ran a test with this script, and found that only about 1/2 of the stewards have ICMP turned on for their nodes (or else it is blocked at firewalls).  I don't know that it will be useful.  Maybe I should reimplement it with hping3, except that this would require stewards to install this package on their nodes.;;;","20/Apr/18 4:39 PM;ashcherbakov;I think we need to check that TCP connections are stable (hping is an option).
Also we may check firewall/iptables on the problem nodes (need to identify problem nodes from the logs).
;;;","20/Apr/18 10:19 PM;sergey-shilov;Hi [~mgbailey],

I've rewritten the _ping_monitor.sh_ tool to use *hping3* instead of *ping*. I've re-attached the _ping_monitor.sh_ tool and the list of live pool IPs _live_pool_ips.list_ (note, now this list should contain lines in form IP:PORT, it is described in script's usage message).;;;","21/Apr/18 12:35 AM;sergey-shilov;[~mgbailey]

Also, it would be nice to ask stewards provide us their firewall (iptables etc.) settings, if it is not sensitive information for them.;;;","26/Apr/18 2:06 AM;sergey-shilov;[~mgbailey] [~ashcherbakov]

Well, I've done deep investigation and matching of attached logs.
 First of all, the only full logs we have are logs from ev1 and ServerVS, so investigation of these logs are the most important because such logs show indy-node restart/crush that lead to disconnection log line on other nodes. So it is very important to distinguish indy-node restart/crush and real network issues. Unfortunately, not all of attached logs are full, thus only investigation of ev1 and ServerVS disconnections makes sense.

Regarding ev1, about 40% of disconnects of this node in other's nodes logs are caused by indy-node crush or restart (unfortunately, we can not distinguish these events without logs of journalctl where we can see back trace):

-----------------------------------------------
 2018-04-05 16:10:39,607 | DEBUG    | looper.py            (265) | handleSignal | Signal None received, stopping looper...
 2018-04-05 16:10:39,617 | INFO     | looper.py            (273) | shutdown | Looper shutting down now...
 2018-04-05 16:10:39,617 | DEBUG    | motor.py             (34) | set_status | ev1 changing status from started to stopping
 -----------------------------------------------

Regarding ServerVS, all disconnects are caused by network issues as there are no log records related to indy-node stopping.

Also I gathered disconnecting stats for each participated node in form

================================

<NodeName> which logs are investigated
 ================================

<RemoteNodeName1>: number of ""disconnected from <RemoteNodeName1>"" events
 <RemoteNodeName2>: number of ""disconnected from <RemoteNodeName2>"" events
 etc. (sorted in reverse order)
 ================================

 

So here they are:

*================================================================*

================================

BIGAWSUSEAST1-001:
 ================================
 zaValidator: 10
 ServerVS: 7
 esatus_AG: 5
 DustStorm: 5
 Aalto: 5
 ev1: 4
 Stuard: 4
 danube: 3
 pcValidator01: 2
 icenode: 2
 digitalbazaar: 2
 atbsovrin: 2
 royal_sovrin: 1
 prosovitor: 1
 ================================

OASFCU:
 ================================
 pcValidator01: 2
 ev1: 2
 zaValidator: 1
 royal_sovrin: 1
 prosovitor: 1
 icenode: 1
 esatus_AG: 1
 digitalbazaar: 1
 danube: 1
 atbsovrin: 1
 Stuard: 1
 ServerVS: 1
 DustStorm: 1
 BIGAWSUSEAST1-001: 1
 Aalto: 1
 ================================

ServerVS:
 ================================
 ev1: 11
 zaValidator: 10
 pcValidator01: 8
 icenode: 8
 esatus_AG: 8
 Stuard: 8
 royal_sovrin: 7
 prosovitor: 7
 digitalbazaar: 7
 DustStorm: 6
 iRespond: 5
 danube: 5
 atbsovrin: 5
 BIGAWSUSEAST1-001: 5
 OASFCU: 4
 Aalto: 4
 ================================

Stuard:
 ================================
 zaValidator: 9
 ServerVS: 8
 DustStorm: 6
 iRespond: 5
 ev1: 5
 esatus_AG: 5
 royal_sovrin: 4
 atbsovrin: 4
 prosovitor: 3
 icenode: 3
 digitalbazaar: 3
 danube: 3
 BIGAWSUSEAST1-001: 3
 pcValidator01: 2
 OASFCU: 2
 Aalto: 1
 ================================

atbsovrin:
 ================================
 zaValidator: 8
 Aalto: 7
 ev1: 6
 Stuard: 4
 ServerVS: 4
 DustStorm: 4
 danube: 3
 pcValidator01: 2
 esatus_AG: 2
 royal_sovrin: 1
 prosovitor: 1
 icenode: 1
 digitalbazaar: 1
 ================================

danube:
 ================================
 zaValidator: 8
 ev1: 6
 ServerVS: 4
 DustStorm: 4
 icenode: 2
 esatus_AG: 2
 royal_sovrin: 1
 pcValidator01: 1
 iRespond: 1
 Stuard: 1
 ================================

digitalbazaar:
 ================================
 zaValidator: 10
 ServerVS: 6
 royal_sovrin: 4
 ev1: 4
 esatus_AG: 4
 DustStorm: 4
 danube: 3
 Stuard: 3
 Aalto: 3
 pcValidator01: 2
 atbsovrin: 2
 prosovitor: 1
 icenode: 1
 BIGAWSUSEAST1-001: 1
 ================================

ev1
 ================================
 ServerVS: 10
 zaValidator: 9
 esatus_AG: 5
 Stuard: 5
 DustStorm: 5
 icenode: 4
 atbsovrin: 4
 Aalto: 4
 prosovitor: 3
 pcValidator01: 3
 danube: 3
 royal_sovrin: 2
 OASFCU: 2
 iRespond: 1
 digitalbazaar: 1
 BIGAWSUSEAST1-001: 1
 ================================

icenode
 ================================
 zaValidator: 9
 ServerVS: 7
 royal_sovrin: 6
 ev1: 6
 Aalto: 6
 pcValidator01: 5
 prosovitor: 4
 danube: 4
 Stuard: 4
 DustStorm: 4
 esatus_AG: 3
 atbsovrin: 3
 digitalbazaar: 2
 OASFCU: 2
 iRespond: 1
 BIGAWSUSEAST1-001: 1
 ================================

pcValidator01
 ================================
 ServerVS: 7
 DustStorm: 5
 zaValidator: 3
 icenode: 3
 ev1: 3
 esatus_AG: 2
 Stuard: 1
 Aalto: 1
 ================================

zaValidator
 ================================
 Aalto: 17
 ServerVS: 14
 ev1: 12
 danube: 12
 royal_sovrin: 11
 icenode: 10
 digitalbazaar: 10
 Stuard: 10
 DustStorm: 10
 esatus_AG: 9
 prosovitor: 7
 atbsovrin: 7
 BIGAWSUSEAST1-001: 6
 OASFCU: 5
 pcValidator01: 4
 iRespond: 4
 ================================

*================================================================*

As we can see the leaders of disconnected nodes are zaValidator and ServerVS. Logs of these nodes are one of the leaders of ""carpet disconnects"":

--------------------------------
 2018-04-05 18:56:56,520 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from icenode
 2018-04-05 18:56:56,520 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Stuard
 2018-04-05 18:56:57,151 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from pcValidator01
 2018-04-05 18:56:57,675 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from ev1
 --------------------------------
 2018-03-29 01:06:42,304 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from esatus_AG
 2018-03-29 01:06:53,769 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from ev1
 2018-03-29 01:06:55,814 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from pcValidator01
 2018-03-29 01:07:01,512 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from icenode
 2018-03-29 01:07:04,382 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Stuard
 --------------------------------
 2018-04-01 13:46:50,625 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from BIGAWSUSEAST1-001
 2018-04-01 13:46:51,008 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from esatus_AG
 2018-04-01 13:46:52,167 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from iRespond
 2018-04-01 13:46:55,240 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from danube
 2018-04-01 13:46:56,520 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Aalto
 2018-04-01 13:46:56,775 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from DustStorm
 2018-04-01 13:46:56,840 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from ev1
 2018-04-01 13:47:02,911 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from OASFCU
 2018-04-01 13:47:03,879 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from icenode
 2018-04-01 13:47:04,453 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from royal_sovrin
 2018-04-01 13:47:05,159 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from prosovitor
 2018-04-01 13:47:08,168 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from atbsovrin
 2018-04-01 13:47:08,291 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from zaValidator
 2018-04-01 13:47:08,740 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from Stuard
 2018-04-01 13:47:09,572 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from pcValidator01
 2018-04-01 13:47:09,895 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: ServerVS disconnected from digitalbazaar
 --------------------------------
 zaValidator.log.2018-03-27:2018-03-28 00:05:29,241 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from icenode
 zaValidator.log.2018-03-27:2018-03-28 00:05:29,242 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto
 zaValidator.log.2018-03-27:2018-03-28 00:05:29,257 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from danube
 zaValidator.log.2018-03-27:2018-03-28 00:05:32,485 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to royal_sovrin
 zaValidator.log.2018-03-27:2018-03-28 00:05:33,299 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to icenode
 zaValidator.log.2018-03-27:2018-03-28 00:05:33,307 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to danube
 zaValidator.log.2018-03-27:2018-03-28 00:05:35,973 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from digitalbazaar
 zaValidator.log.2018-03-27:2018-03-28 00:06:03,390 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto
 zaValidator.log.2018-03-27:2018-03-28 00:06:33,677 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to digitalbazaar
 zaValidator.log.2018-03-27:2018-03-28 00:25:39,736 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from digitalbazaar
 zaValidator.log.2018-03-27:2018-03-28 00:25:48,128 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to digitalbazaar
 zaValidator.log.2018-03-27:2018-03-28 06:26:05,857 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto
 zaValidator.log.2018-03-27:2018-03-28 06:26:22,252 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto
 zaValidator.log.2018-03-28:2018-03-28 23:06:57,322 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from esatus_AG
 zaValidator.log.2018-03-28:2018-03-28 23:07:38,923 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to esatus_AG
 zaValidator.log.2018-03-28:2018-03-28 23:21:31,086 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from icenode
 zaValidator.log.2018-03-28:2018-03-28 23:21:39,441 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to icenode
 zaValidator.log.2018-03-28:2018-03-29 06:26:51,428 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto
 zaValidator.log.2018-03-28:2018-03-29 06:26:54,773 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto
 zaValidator.log.2018-03-30:2018-03-30 06:26:41,497 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto
 zaValidator.log.2018-03-30:2018-03-30 06:26:57,918 | INFO | keep_in_touch.py (98) | _connsChanged | CONNECTION: zaValidator now connected to Aalto
 zaValidator.log.2018-03-30:2018-03-30 15:19:47,076 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from ev1
 zaValidator.log.2018-03-30:2018-03-30 15:19:47,707 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from OASFCU
 zaValidator.log.2018-03-30:2018-03-30 15:19:52,652 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from icenode
 zaValidator.log.2018-03-30:2018-03-30 15:19:55,467 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from DustStorm
 zaValidator.log.2018-03-30:2018-03-30 15:19:56,190 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from ServerVS
 zaValidator.log.2018-03-30:2018-03-30 15:19:56,335 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from danube
 zaValidator.log.2018-03-30:2018-03-30 15:19:56,359 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from BIGAWSUSEAST1-001
 zaValidator.log.2018-03-30:2018-03-30 15:19:56,429 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from prosovitor
 zaValidator.log.2018-03-30:2018-03-30 15:19:57,131 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Stuard
 zaValidator.log.2018-03-30:2018-03-30 15:19:57,579 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from atbsovrin
 zaValidator.log.2018-03-30:2018-03-30 15:20:00,734 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from digitalbazaar
 zaValidator.log.2018-03-30:2018-03-30 15:20:01,223 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from esatus_AG
 zaValidator.log.2018-03-30:2018-03-30 15:20:02,060 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from Aalto
 zaValidator.log.2018-03-30:2018-03-30 15:20:02,539 | INFO | keep_in_touch.py (93) | _connsChanged | CONNECTION: zaValidator disconnected from royal_sovrin
 zaValidator.log.2018-03-30:2018-03-30 15:20:02,541 | INFO | node.py (1106) | onConnsChanged | zaValidator lost connection to primary of master
 --------------------------------

Seems like zaValidator and ServerVS has often disconnects from the whole network segments, but it is just a suggestion. Anyway, I think we should take a look at these nodes.;;;","26/Apr/18 2:53 AM;mgbailey;[~sergey-shilov], thank you for this information.  While there are disconnect events, I think this shows that the nodes are all working much of the time.  How long did the disconnect events last? Is there something specific we can ask for these two stewards to look at or to send to us? 

One of these stewards is in South Africa, and the other is in Switzerland, and neither is in an AWS datacenter. I would expect that remote nodes like these might experience some network instability. Since a world-wide, diverse network is one of our design goals, and these nodes appear to be working most of the time, should we be looking at how we can more reliably recover when events like these occur?;;;","04/May/18 1:41 AM;ashcherbakov;[~mgbailey]
As disconnections are in general normal event in our distributed network, and, according to logs, we were able to successfully recover from disconnections, I think we can close this issue. Do you agree?;;;","04/May/18 5:06 AM;mgbailey;[~ashcherbakov], agreed.  I expect that disconnections will occur, and that we recover from them effectively.  ;;;",,,
Fix failed tests on Jenkins,INDY-1262,29279,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,anikitinDSR,anikitinDSR,anikitinDSR,09/Apr/18 9:04 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,For using last libindy and python wrappers we need to increment corresponding package version. After this we had several tests on Jenkins. Now it's a blocker for stable release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwfz:",,,,,,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,,anikitinDSR,sergey-shilov,,,,,,,,,,,"09/Apr/18 9:11 PM;sergey-shilov;Failed tests block release of indy-node so this ticket has the highest priority.

The main problem now is risen SDK timeouts. Thus our tests mainly terminated by Jenkins supervisor as its timeouts lower than SDK's. This leads to enormous time execution of tests, that's why fixing and rerun of tests takes much time. Moreover, termination of Jenkins pipeline by timeout leaves us without tests artifacts, so it forces us to look for the tests execution machine and get logs manually, that also inhibits the fixing process.;;;",,,,,,,,,,,,,,,,,,,,,,,,
[QA] Script for filling the ledger doesn't work,INDY-1263,29294,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ozheregelya,ozheregelya,10/Apr/18 2:50 AM,23/Apr/18 6:04 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Steps to Reproduce:
1. Set up the pool.
2. Make sure that it works by sending several transactions.
3. Stop the pool.
4. Set yourself as indy user.
5. Run the generate_txns.py script.
6. Run the add_json_txns_to_ledger.py.

Actual Results:
{code:java}
ubuntu@californiaQALarge1:~/txn_add$ sudo python3 add_json_txns_to_ledger.py txns100K1 
2018-04-08 11:37:57,866 | DEBUG | __init__.py ( 59) | register | Registered VCS backend: git
2018-04-08 11:37:57,920 | DEBUG | __init__.py ( 59) | register | Registered VCS backend: hg
2018-04-08 11:37:57,979 | DEBUG | __init__.py ( 59) | register | Registered VCS backend: svn
2018-04-08 11:37:57,979 | DEBUG | __init__.py ( 59) | register | Registered VCS backend: bzr
NODE_NAME=Node1
NODE_PORT=9701
NODE_CLIENT_PORT=9702
CLIENT_CONNECTIONS_LIMIT=15360
2018-04-08 11:37:58,486 | DEBUG | ledger.py ( 207) | start | Starting ledger...
2018-04-08 11:37:58,492 | DEBUG | ledger.py ( 78) | recoverTree | Recovering tree from hash store of size 18
2018-04-08 11:37:58,492 | DEBUG | ledger.py ( 88) | recoverTree | Recovered tree in 0.0003837309777736664 seconds
2018-04-08 11:37:58,512 | DEBUG | ledger.py ( 207) | start | Starting ledger...
2018-04-08 11:37:58,517 | DEBUG | ledger.py ( 78) | recoverTree | Recovering tree from hash store of size 15
2018-04-08 11:37:58,518 | DEBUG | ledger.py ( 88) | recoverTree | Recovered tree in 0.000365738058462739 seconds
2018-04-08 11:37:58,518 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node1 (Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv) order to 5
2018-04-08 11:37:58,518 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node2 (8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb) order to 5
2018-04-08 11:37:58,518 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node3 (DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya) order to 5
2018-04-08 11:37:58,518 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node4 (4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA) order to 5
2018-04-08 11:37:58,518 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node5 (4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe) order to 5
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node6 (Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8) order to 5
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node7 (BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW) order to 5
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node8 (98VysG35LxrutKTNXvhaztPFHnx5u9kHtT7PnUGqDa8x) order to 5
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node9 (6pfbFuX5tx7u3XKz8MNK4BJiHxvEcnGRBs1AQyNaiEQL) order to 5
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node10 (HaNW78ayPK4b8vTggD4smURBZw7icxJpjZvCMLdUueiN) order to 6
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node11 (2zUsJuF9suBy2iKkcgmm8uoMB6u5Dq2oHoRuchrZbj2N) order to 6
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node12 (BXV4SXKEJeYQ8XCRHgpw1Xume5ntqALsRhbUYcF85Mse) order to 6
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node13 (71WAtEevzz8aZr8baNJhQCUDLwRhM7LeaErSKNWWKxzn) order to 6
2018-04-08 11:37:58,519 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node14 (FEUGMFWCSAM725vyH8JZnsitiNUy31NPhugVKb8zDpng) order to 6
2018-04-08 11:37:58,520 | INFO | pool_manager.py ( 425) | _set_node_order | Node1 sets node Node15 (DPZ8GJ1NyNZGJMU6qQZVuBsumY1aVzvcV4FqQK9Y215x) order to 6
2018-04-08 11:37:58,540 | INFO | node.py ( 777) | _create_bls_bft | BLS: BLS Signatures will be used for Node Node1
Traceback (most recent call last):
 File ""add_json_txns_to_ledger.py"", line 91, in <module>
 config=config)
 File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 195, in __init__
 self.addGenesisNyms()
 File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2699, in addGenesisNyms
 self.addNewRole(txn)
 File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2666, in addNewRole
 v = DidVerifier(verkey, identifier=identifier)
 File ""/usr/local/lib/python3.5/dist-packages/plenum/common/verifier.py"", line 34, in __init__
 assert verkey, 'verkey must be provided'
AssertionError: verkey must be provided{code}


Expected Results:
Generated transactions should be written.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzz5ev:",,,,,,,,,,,,,,,,,,,,,,,,,,krw910,ozheregelya,,,,,,,,,,,"10/Apr/18 6:02 AM;krw910;[~ozheregelya] Don't we need this working now to get a variety of write transaction types on the ledger?;;;","10/Apr/18 5:18 PM;ozheregelya;[~krw910], these scripts are nice-to-have for some exploration tasks (like INDY-863). Last time we used them few months ago. We don't use them regularly. 
As workaround of problems with these scripts, we still can use load tests.;;;","21/Apr/18 12:36 AM;krw910;[~ozheregelya] With the other performance scripts that can generate different types of transactions do we still need  generate_txns.py or add_json_txns_to_ledger.py?;;;","23/Apr/18 6:04 PM;ozheregelya;[~krw910], yes, because these scripts allow us to fill the ledger with hundreds of thousands txns for several hours, when the rest scripts will do it during several days.;;;",,,,,,,,,,,,,,,,,,,,,
Change revocation marker into string representation,INDY-1264,29320,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,anikitinDSR,anikitinDSR,11/Apr/18 8:05 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Need to change markers for path in state:
* MARKER_REVOC_DEF
* MARKER_REVOC_REG_ENTRY
* MARKER_REVOC_REG_ENTRY_ACCUM
from byte to string represantation.
Also we need to change dynamic validation (claimDefId is a string now without binary markers), when REVOC_REG_DEF transaction was got.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz3jz:",,,,,,18.07 Stability & Monitoring,,,,,,,,,,,,,,,,,,,,anikitinDSR,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default log level should contain more data to allow analize pool state,INDY-1265,29322,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,anikitinDSR,dsurnin,dsurnin,11/Apr/18 9:20 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Need to show for INFO loglevel:
* received node message from 
* connections changed from
* stash/stashing
* discard/discarding
* ordering COMMIT
* set last ordered
* set watermarks as
* Remote ricFlair is not connected - message will not be sent immediately.If this problem does not resolve itself",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwg7:",,,,,,18.07 Stability & Monitoring,,,,,,,,3.0,,,,,,,,,,,,dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Behavior of backup instances when one of them is demoted/stopped/blacklisted,INDY-1266,29344,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,zhigunenko.dsr,zhigunenko.dsr,12/Apr/18 8:53 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Possible cases:
 # Stop backup primary (doesn't affect backup primary work - expected behavior)
 # Blacklist backup primary
 ## Possible way of blacklisting node:
 ### Fake ledger
 ### Change node keys (not sure that this will work)
 ### *!* Changing system time on the node will not result node blacklisting. PRE-PREPARE with wrong time will be rejected, but if the pool will be able to write transaction without the node with wrong time, transaction will be written. It also will be written on node with wrong time because it will found sufficient PREPAREs.
 # Demote backup primary
 ## Demote backup instance when F won't change
 *** Adding new node
 ## Demotion of backup instance lead to F change
 *** Adding new node",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz627:",,,,,,18.07 Stability & Monitoring,18.08 Stability-Monitoring,,,,,,,,,,,,,,,,,,,ozheregelya,zhigunenko.dsr,,,,,,,,,,,"12/Apr/18 10:46 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.378

*Case 1:*
Stop backup primary (indy-node service on instance1 in case)
*Actual results:*
Stop did not lead to pool disruption

*Case 2.1* Blacklist backup primary via fake ledger
1. stop _indy-node_ service
2. move subdirectory _/data_ from another pool
3. start _indy-node_ service
*Actual results:*
affected node marked as unreachable, ""view change"" hadn't happened

*Case 2.2* Blacklist backup primary via node keys changing
1. stop _indy-node_ service
2. move subdirectory _/keys_ from another pool
3. start _indy-node_ service
*Actual results:*
affected node marked as unreachable, ""view change"" hadn't happened

*Case 3.1*
Steps:
1. Demote backup instance when F won't change (from 11 nodes to 10)
2. Adding new node (from 10 nodes to 11)
*Actual results:*
""view change"" hadn't happened

*Case 3.2*
1. Demote backup instance that lead to F change (from 10 nodes to 9)
2. Adding new node (from 9 nodes to 10)
*Actual results:*
""view change"" hadn't happened

*Case 4:*
Pool of 11 nodes, node2 is primary
1) demote node3 (node 11 -> 10, F = 3 -> 3) as primary instance 1
2) demote node4 (node 10 -> 9, F = 3 -> 2) as primary instance 2

*Actual results:*
Demoting does not cause view change if demoted node is not primary
{code}
2018-04-12 13:20:14,047 | DISPLAY  | node.py              (2396) | select_primaries | PRIMARY SELECTION: Node6:0 selected primary Node2:0 for instance 0 (view 1)
2018-04-12 13:20:14,049 | DISPLAY  | node.py              (2396) | select_primaries | PRIMARY SELECTION: Node6:1 selected primary Node3:1 for instance 1 (view 1)
2018-04-12 13:20:14,050 | DISPLAY  | node.py              (2396) | select_primaries | PRIMARY SELECTION: Node6:2 selected primary Node4:2 for instance 2 (view 1)
2018-04-12 13:20:14,055 | DISPLAY  | node.py              (2396) | select_primaries | PRIMARY SELECTION: Node6:3 selected primary Node5:3 for instance 3 (view 1)
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,
Export logging strings in Indy,INDY-1267,29351,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,esplinr,esplinr,12/Apr/18 10:48 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"*Story*

As a Product Manager of Indy Node, I need a list of all the log messages so that I can review them holistically.

*Acceptance Criteria*

Produce a spreadsheet containing all the logging strings from Indy. It will have the columns:
* ""Current Message""
* ""Log Level""

We will add columns for ""Revised Message"", ""Revised Level"".

A member of the development team will review the messages and include proposed revisions.
* Reviewer can suggest a Revised Message or Revised Level, can mark level as ""None"" if the log message is unnecessary, or leave Revised Message and Revised Level blank if no changes are needed.
* If there is a ""Revised Message"" but the ""Current Message"" is empty, then the log messages is new and should be added to the product.

The spreadsheet should be delivered to Product Management and a representative of the Operations Team to review the strings.",,,,,,,,,,,,,,,,,,,,,INDY-1416,,,,,,,,INDY-484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzzck7:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,dsurnin,esplinr,gudkov,mgbailey,,,,,,,,"13/Apr/18 10:57 PM;gudkov;> As an administrator of an Indy node, I want log messages to be clear and well written.

For current moment main consumers of logs are Indy developers and troubleshooting teams. I am not sure that they can be easily adopted to Administrators auditory. 

> Log messages in Indy should be reviewed for compliance with our UX standards.

What is ""our UX standards""? Do we have any published?;;;","13/Apr/18 11:49 PM;esplinr;Thanks for the clarification about who consumes the Indy node logs. If they aren't being consumed outside of the developer teams, then this probably isn't as high of a priority. But I expect that eventually the log messages will be important for Node Stewards.

I spoke with our technical writer yesterday, and we don't have formal requirements specified yet. Once the spreadsheet is prepared, I can work with her to make sure the messages meet expectations.;;;","14/Apr/18 12:02 AM;gudkov;Can you just quickly grep indy-node repo to understand how bad logs are? It can allow you to quickly estimate how important it now to be solved without spending significant time for creation of exel sheets?;;;","03/Jul/18 5:12 PM;dsurnin;The list of message strings and possible changes could be found here
https://docs.google.com/spreadsheets/d/1vxSpN0FBf_nv1ZHsPI_Q3vAyP3yznyiIYUKnh8jQzds/edit#gid=2087561173;;;","05/Jul/18 5:10 PM;ashcherbakov;The list is prepared and sent for review to [~tharmon] [~mgbailey] [~EvelynEvergreene];;;","06/Jul/18 2:15 AM;mgbailey;The spreadsheet as it exists is of very limited utility, since each line in a multi-line message is broken into an individual entry in the spreadsheet, and message cohesion is lost.  For example, line 13 has ""nodeName"" listed as an error message.

Even if this spreadsheet were improved so that entire messages were captured, it is unclear that viewing individual messages in isolation is a useful exercise, at least at the outset of the improvement exercise. It will be much more useful to see messages in context.  What I suggest is that example log outputs be given for various scenarios, at the INFO level, which is the level that would be appropriate for admins to operate their nodes at (hopefully, loading permitting). We could then go through these log snippets and determine unnecessary messages that could be demoted, as well as needed messages that should be promoted or added. Typical scenarios should include: transaction posted to network, transaction failing to post to the network for reasons including no consensus, successful and unsuccessful pool upgrade, and so on.

One more note.  On lines 5 and 6 of the spreadsheet, a new message format is proposed.  I consider this new format to be retrograde, since useful information is obscured or lost. The date information should remain in ISO-standard human-readable format.  If a machine-readable format is required, the script that parses the logs can easily make this conversion.  In similar fashion, there is no reason to make the log level of the message difficult for humans to see. 

Finally, it is common to search on the name of the module that writes a message to the log when debugging an issue.  For example, where there is a problem in a pool upgrade, searching on ""upgrader"" will help find the pertinent messages while sorting through irrelevant log messages.  The module writing the message should continue to be included in the log messages.

[~esplinr], [~tharmon], [~ashcherbakov], [~stevetolman];;;","06/Jul/18 3:39 PM;ashcherbakov;{quote}
What I suggest is that example log outputs be given for various scenarios, at the INFO level, which is the level that would be appropriate for admins to operate their nodes at (hopefully, loading permitting). We could then go through these log snippets and determine unnecessary messages that could be demoted, as well as needed messages that should be promoted or added. Typical scenarios should include: transaction posted to network, transaction failing to post to the network for reasons including no consensus, successful and unsuccessful pool upgrade, and so on.
{quote}
The scope of this ticket was just to create a spreadsheet. Having some example of logs makes total sense, but requires more work and wasn't estimated here.
I suggest the following: do changes in log levels and messages as suggested in the spreadsheet (INDY-1416), and after this we can provide examples of the logs.

{quote}
I consider this new format to be retrograde, since useful information is obscured or lost. 
{quote}
The logs are quite huge (especially during the high load), so by doing these changes the size can be decreased by ~15%. It means that more logs can survive rotation.
I think that module and function is not very helpful in analysing the logs, the message itself is much more useful. If this is not clear from the message itself without a module, then the message should probably be reviewed. 
Also the tool to be used for analysing the logs (https://github.com/hyperledger/indy-plenum/tree/master/scripts/process_logs) is based on the messages, not module.;;;","07/Jul/18 1:55 AM;mgbailey;I apologize, but I don't think that going through many hundreds of fragmentary log messages, without context, is a useful exercise.

I will let others weigh in on the tradeoffs on log message format readability vs. log size, and on stewards learning and using the log analysis tool. [~tharmon] [~EvelynEvergreene];;;",,,,,,,,,,,,,,,,,
[QA] Simultaneous connections load testing with large ledger,INDY-1268,29374,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,13/Apr/18 6:45 PM,08/Oct/19 9:13 PM,28/Oct/23 2:47 AM,08/Oct/19 9:13 PM,,,,,,0,explore,,,"From [~krw910]'s message:
{quote}
We have some metrics for 2,000 connections to a fresh ledger to show txns per second and duration of time. Can we run those 2,000 connections against the larger ledger to see if ledger size is making a difference to performance?
{quote}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1235,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwx4f:2rzy",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,ozheregelya,,,,,,,,,,,"08/Oct/19 9:13 PM;esplinr;Our tests show a performance decrease consistent with the algorithm:  O(log(n));;;",,,,,,,,,,,,,,,,,,,,,,,,
Unable to install indy-node if sdk repo is in sources.list,INDY-1269,29375,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,VladimirWork,VladimirWork,VladimirWork,13/Apr/18 6:49 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Build Info:
indy-node 1.3.56

Steps to Reproduce:
1. Add the next keys and sources:

{noformat}
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 68DB5E88
apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BD33704C
echo ""deb https://repo.sovrin.org/deb xenial rc"" >> /etc/apt/sources.list
echo ""deb https://repo.sovrin.org/sdk/deb xenial rc"" >> /etc/apt/sources.list
{noformat}

2. Install indy-node or sovrin:

{noformat}
apt-get update -y && apt-get install -y sovrin
{noformat}

or

{noformat}
apt-get update -y && apt-get install -y indy-node
{noformat}

Actual Results:
Installation fails due to RC indy-plenum depends on indy-crypto *0.2.0* but there is indy-crypto *0.3.0* installed from sdk repo (in any branch).

Also it may affect pool upgrade in this case because we will have the same dependency tree during upgrade.

Expected Results:
*The same indy and sdk repos* (rc/master/stable) should contain *the same version of indy-crypto package* to avoid installation and upgrade issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-564,IS-565,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz62n:",,,,,,18.08 Stability-Monitoring,,,,,,,,,,,,,,,,,,,,anikitinDSR,VladimirWork,,,,,,,,,,,"17/Apr/18 10:34 PM;anikitinDSR;PRs:
indy-plenum:  https://github.com/hyperledger/indy-plenum/pull/632
indy-node:      https://github.com/hyperledger/indy-node/pull/655

indy-plenum version:  1.2.317
indy-node version:      1.3.375;;;","18/Apr/18 12:09 AM;VladimirWork;It is checked during acceptance testing (docker environment setups successfully).;;;",,,,,,,,,,,,,,,,,,,,,,,
Investigate and fix errors in post-install automation scripts,INDY-1270,29376,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,13/Apr/18 7:52 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,TShirt_L,,,"Build Info:
?

Actual Results:

anoncreds_prover_get_claims_for_proof_req_works_with_empty_req_attrs_test
1 : Create wallet :: Passed
2 : Open wallet :: Passed
3 : Create 'issuer_did' :: Passed
4 : Create 'prover_did' :: Passed
5 : Create master secret :: Passed
6 : Create and store claim definition :: Passed
7 : Create claim request :: Passed
8 : Create claim :: Passed
9 : Store claims into wallet :: Failed
Traceback: 'int' object has no attribute 'encode'

anoncreds_prover_get_claims_for_proof_req_works_with_empty_req_predicate_test
1 : Create wallet :: Passed
2 : Open wallet :: Passed
3 : Create 'issuer_did' :: Passed
4 : Create 'prover_did' :: Passed
5 : Create master secret :: Passed
6 : Create and store claim definition :: Passed
7 : Create claim request :: Passed
8 : Create claim :: Passed
9 : Store claims into wallet :: Failed
Traceback: 'int' object has no attribute 'encode'

anoncreds_prover_get_claims_for_proof_req_works_with_req_predicate_and_req_attrs_test
1 : Create wallet :: Passed
2 : Open wallet :: Passed
3 : Create 'issuer_did' :: Passed
4 : Create 'prover_did' :: Passed
5 : Create master secret :: Passed
6 : Create and store claim definition :: Passed
7 : Create claim request :: Passed
8 : Create claim :: Passed
9 : Store claims into wallet :: Failed
Traceback: 'int' object has no attribute 'encode'

anoncreds_prover_get_claims_returns_all_claims_with_empty_filter_json_test
1 : Create wallet :: Passed
2 : Open wallet :: Passed
3 : Create 'issuer_did' :: Passed
4 : Create 'prover_did' :: Passed
5 : Create master secret :: Passed
6 : Create and store claim definition :: Passed
7 : Create claim request :: Passed
8 : Create claim :: Passed
9 : Create other claim :: Passed
10 : Store claims into wallet :: Failed
Traceback: 'int' object has no attribute 'encode'

ledger_build_claim_request_test
1 : Prepare pool and wallet :: Passed
2 : Create DIDs :: Passed
3 : build claim request :: Passed
4 : Verify json claim request is correct. :: Failed
Traceback: Failed. Json response is incorrect.

ledger_build_get_attrib_request_test
1 : Prepare pool and wallet :: Passed
2 : Create DIDs :: Passed
3 : Create DIDs :: Passed
4 : send attrib request :: Passed
5 : build get attrib request :: Passed
6 : Verify json get attrib request is correct. :: Failed
Traceback: Failed. Json response is incorrect. ",,,,,,,,,,,,,,,,,INDY-1359,,,,,,,,,,,,,,,,,INDY-1257,INDY-1434,,,,,,,,,,,,,,"28/Jun/18 7:44 PM;VladimirWork;INDY-1270.PNG;https://jira.hyperledger.org/secure/attachment/15191/INDY-1270.PNG","08/May/18 10:52 PM;VladimirWork;master_pool_and_master_sdk_wrapper.PNG;https://jira.hyperledger.org/secure/attachment/14960/master_pool_and_master_sdk_wrapper.PNG","28/Jun/18 7:56 PM;VladimirWork;origin_fails_against_master_env.PNG;https://jira.hyperledger.org/secure/attachment/15192/origin_fails_against_master_env.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzclz:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,5.0,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"18/Apr/18 4:40 PM;VladimirWork;Also we should support revocation changes in post-install automation scripts in scope of this ticket.;;;","08/May/18 10:52 PM;VladimirWork;Now we have the next at indy-node 1.3.404 / libindy 1.3.1~507 / python3-indy 1.3.1~507:  !master_pool_and_master_sdk_wrapper.PNG|thumbnail! and all except 2 tests failed against indy-node 1.4.482 / libindy 1.4.0~605 / python3-indy 1.4.0~607: !origin_fails_against_master_env.PNG|thumbnail!

;;;","28/Jun/18 7:44 PM;VladimirWork;Build Info:
indy-node 1.4.482 master (1.4.64 RC analogue)
libindy 1.4.0~605 master (1.5.0 stable analogue)
python3-indy 1.4.0.dev607 master (`sudo pip3 install python3-indy==1.4.0.dev607` to install since pip3 is not affected by sources.list)

Actual Results:
172/205 tests are passed.
33/205 tests are skipped due to significant API changes. INDY-1434 is reported.
 !INDY-1270.PNG|thumbnail! 

PR: https://github.com/hyperledger/indy-post-install-automation/pull/10;;;",,,,,,,,,,,,,,,,,,,,,,
"Node shows another nodes as unreachable in validator-info output, but it is reachable for all nodes",INDY-1271,29388,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,ozheregelya,ozheregelya,14/Apr/18 2:20 AM,08/Jan/19 4:26 AM,28/Oct/23 2:47 AM,,,,validator-info,,,0,,,,"Steps to Reproduce:
1. Run load test on the pool of 19 nodes.

Actual Results:
All nodes have 300,005 transactions, but one node (Node11) have 296,512.
Validator-info outputs:
||Node11||The rest nodes||
|Validator Node11 is running
Current time: Friday, April 13, 2018 5:15:51 PM
Validator DID: 2zUsJuF9suBy2iKkcgmm8uoMB6u5Dq2oHoRuchrZbj2N
Verification Key: 3r5kLz6v7RBjEYMyEiTUiozBDskz3T2pdDCuswowhBhep9rmrwN6zTs
Node Port: 9721/tcp on 0.0.0.0/0
Client Port: 9722/tcp on 0.0.0.0/0
Metrics:
 Uptime: 1 day, 9 hours, 19 minutes, 58 seconds
 Total Config Transactions: 42
 Total Ledger Transactions: 296512
 Total Pool Transactions: 42
 Read Transactions/Seconds: 0.00
 Write Transactions/Seconds: 0.00
Reachable Hosts: 13/19
 Node11
 Node12
 Node14
 Node15
 Node17
 Node18
 Node19
 Node2
 Node20
 Node6
 Node7
 Node8
 Node9
Unreachable Hosts: 6/19
 Node4
 Node3
 Node10
 Node13
 Node24
 Node16
Software Versions:
 indy-node: 1.3.364
 sovrin: 1.1.47|Validator Node12 is running
Current time: Friday, April 13, 2018 5:18:22 PM
Validator DID: BXV4SXKEJeYQ8XCRHgpw1Xume5ntqALsRhbUYcF85Mse
Verification Key: 4wuECQnbu1sEJhGjeEaKAmhDiqqF1qaGnCYf4RmWrnwWv64DvAYpZ3f
Node Port: 9723/tcp on 0.0.0.0/0
Client Port: 9724/tcp on 0.0.0.0/0
Metrics:
 Uptime: 3 days, 21 hours, 52 minutes, 35 seconds
 Total Config Transactions: 42
 Total Ledger Transactions: 300005
 Total Pool Transactions: 42
 Read Transactions/Seconds: 0.00
 Write Transactions/Seconds: 0.00
Reachable Hosts: 19/19
 Node10
 Node11
 Node12
 Node13
 Node14
 Node15
 Node16
 Node17
 Node18
 Node19
 Node2
 Node20
 Node24
 Node3
 Node4
 Node6
 Node7
 Node8
 Node9
Unreachable Hosts: 0/19
Software Versions:
 indy-node: 1.3.364
 sovrin: 1.1.47|

Expected Results:
All nodes should be reachable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzwy9j:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Configurations should contain certain version of pip,INDY-1272,29415,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,Toktar,zhigunenko.dsr,zhigunenko.dsr,16/Apr/18 7:42 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Node service cannot start after pip 10 release

{code:java}
Apr 16 08:46:31 b9cc23063bef systemd[1]: Started Indy Node.
Apr 16 08:46:32 b9cc23063bef env[114]: Traceback (most recent call last):
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/bin/start_indy_node"", line 6, in <module>
Apr 16 08:46:32 b9cc23063bef env[114]:     from indy_node.utils.node_runner import run_node
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 10, in <module>
Apr 16 08:46:32 b9cc23063bef env[114]:     from indy_node.server.node import Node
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 15, in <module>
Apr 16 08:46:32 b9cc23063bef env[114]:     from plenum.server.node import Node as PlenumNode
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 69, in <module>
Apr 16 08:46:32 b9cc23063bef env[114]:     from plenum.server.monitor import Monitor
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/monitor.py"", line 26, in <module>
Apr 16 08:46:32 b9cc23063bef env[114]:     pluginManager = PluginManager()
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/notifier_plugin_manager.py"", line 35, in __init__
Apr 16 08:46:32 b9cc23063bef env[114]:     self.importPlugins()
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/notifier_plugin_manager.py"", line 102, in importPlugins
Apr 16 08:46:32 b9cc23063bef env[114]:     plugins = self._findPlugins()
Apr 16 08:46:32 b9cc23063bef env[114]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/notifier_plugin_manager.py"", line 131, in _findPlugins
Apr 16 08:46:32 b9cc23063bef env[114]:     for pkg in pip.utils.get_installed_distributions()
Apr 16 08:46:32 b9cc23063bef env[114]: AttributeError: module 'pip' has no attribute 'utils'
Apr 16 08:46:32 b9cc23063bef systemd[1]: indy-node.service: Main process exited, code=exited, status=1/FAILURE
Apr 16 08:46:32 b9cc23063bef systemd[1]: indy-node.service: Unit entered failed state.
Apr 16 08:46:32 b9cc23063bef systemd[1]: indy-node.service: Failed with result 'exit-code'.
Apr 16 08:46:43 b9cc23063bef systemd[1]: indy-node.service: Service hold-off time over, scheduling restart.
Apr 16 08:46:43 b9cc23063bef systemd[1]: Stopped Indy Node.
Apr 16 08:46:43 b9cc23063bef systemd[1]: indy-node.service: Start request repeated too quickly.
Apr 16 08:46:43 b9cc23063bef systemd[1]: Failed to start Indy Node.
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz62v:",,,,,,18.08 Stability-Monitoring,,,,,,,,,,,,,,,,,,,,Toktar,zhigunenko.dsr,,,,,,,,,,,"18/Apr/18 6:00 PM;Toktar;Problem reason:
pip released version 10.0 with other API

Changes:
Fixed version < 10.0.0 of pip

PR:
https://github.com/hyperledger/indy-node/pull/655
https://github.com/hyperledger/indy-plenum/pull/633

Version:
indy-plenum 1.2.317-master
indy-node 1.3.375-master

Risk factors:
Nothing is expected

Risk:
Low;;;",,,,,,,,,,,,,,,,,,,,,,,,
Date in log name doesn't accord to the day when the log was rotated,INDY-1273,29416,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ozheregelya,ozheregelya,16/Apr/18 8:03 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,help-wanted,,,"*Steps to Reproduce:*
1. Setup the pool and look at logs after several weeks:
{code:java}
ubuntu@irelandQALarge16:~$ ll /var/log/indy/sandbox/
total 979552
drwxrwxr-x 2 indy indy 4096 Apr 16 09:08 ./
drwxrwxr-x 3 indy indy 4096 Apr 16 00:00 ../
-rw-rw-r-- 1 ubuntu ubuntu 6815808 Apr 13 15:33 Node16.jctl
-rw-r--r-- 1 indy indy 36463074 Apr 16 10:33 Node16.log
-rw-r--r-- 1 indy indy 10388229 Apr 12 19:48 Node16.log.2018-04-09.139.gz
-rw-r--r-- 1 indy indy 10471811 Apr 12 19:57 Node16.log.2018-04-09.140.gz
-rw-r--r-- 1 indy indy 10697802 Apr 12 20:06 Node16.log.2018-04-09.141.gz
-rw-r--r-- 1 indy indy 10650758 Apr 12 20:15 Node16.log.2018-04-09.142.gz
-rw-r--r-- 1 indy indy 10769480 Apr 12 20:24 Node16.log.2018-04-09.143.gz
-rw-r--r-- 1 indy indy 10829762 Apr 12 20:32 Node16.log.2018-04-09.144.gz
-rw-r--r-- 1 indy indy 10408679 Apr 12 20:41 Node16.log.2018-04-09.145.gz
-rw-r--r-- 1 indy indy 11020262 Apr 12 20:50 Node16.log.2018-04-09.146.gz
-rw-r--r-- 1 indy indy 10525874 Apr 12 20:58 Node16.log.2018-04-09.147.gz
-rw-r--r-- 1 indy indy 11599637 Apr 12 21:08 Node16.log.2018-04-09.148.gz
-rw-r--r-- 1 indy indy 11236593 Apr 12 21:16 Node16.log.2018-04-09.149.gz
-rw-r--r-- 1 indy indy 10742575 Apr 12 21:25 Node16.log.2018-04-09.150.gz
-rw-r--r-- 1 indy indy 26056820 Apr 12 22:08 Node16.log.2018-04-09.151.gz
-rw-r--r-- 1 indy indy 26056722 Apr 12 23:32 Node16.log.2018-04-09.152.gz
-rw-r--r-- 1 indy indy 26056688 Apr 13 00:14 Node16.log.2018-04-09.153.gz
-rw-r--r-- 1 indy indy 26056698 Apr 13 00:56 Node16.log.2018-04-09.154.gz
-rw-r--r-- 1 indy indy 26056706 Apr 13 01:39 Node16.log.2018-04-09.155.gz
-rw-r--r-- 1 indy indy 26056444 Apr 13 02:21 Node16.log.2018-04-09.156.gz
-rw-r--r-- 1 indy indy 26056652 Apr 13 03:03 Node16.log.2018-04-09.157.gz
-rw-r--r-- 1 indy indy 26056784 Apr 13 03:45 Node16.log.2018-04-09.158.gz
-rw-r--r-- 1 indy indy 26056665 Apr 13 04:27 Node16.log.2018-04-09.159.gz
-rw-r--r-- 1 indy indy 26056440 Apr 13 05:10 Node16.log.2018-04-09.160.gz
-rw-r--r-- 1 indy indy 26056495 Apr 13 05:52 Node16.log.2018-04-09.161.gz
-rw-r--r-- 1 indy indy 26056635 Apr 13 06:34 Node16.log.2018-04-09.162.gz
-rw-r--r-- 1 indy indy 26056509 Apr 13 07:16 Node16.log.2018-04-09.163.gz
-rw-r--r-- 1 indy indy 26056722 Apr 13 07:58 Node16.log.2018-04-09.164.gz
-rw-r--r-- 1 indy indy 19567068 Apr 13 08:25 Node16.log.2018-04-09.165.gz
-rw-r--r-- 1 indy indy 10342134 Apr 13 08:33 Node16.log.2018-04-09.166.gz
-rw-r--r-- 1 indy indy 9824652 Apr 13 08:41 Node16.log.2018-04-09.167.gz
-rw-r--r-- 1 indy indy 11647462 Apr 13 08:49 Node16.log.2018-04-09.168.gz
-rw-r--r-- 1 indy indy 11298062 Apr 13 08:58 Node16.log.2018-04-09.169.gz
-rw-r--r-- 1 indy indy 12206075 Apr 13 09:08 Node16.log.2018-04-09.170.gz
-rw-r--r-- 1 indy indy 26014954 Apr 13 09:46 Node16.log.2018-04-09.171.gz
-rw-r--r-- 1 indy indy 26014942 Apr 13 10:24 Node16.log.2018-04-09.172.gz
-rw-r--r-- 1 indy indy 26014876 Apr 13 11:03 Node16.log.2018-04-09.173.gz
-rw-r--r-- 1 indy indy 26016869 Apr 13 11:41 Node16.log.2018-04-09.174.gz
-rw-r--r-- 1 indy indy 26007194 Apr 13 12:19 Node16.log.2018-04-09.175.gz
-rw-r--r-- 1 indy indy 26007242 Apr 13 12:57 Node16.log.2018-04-09.176.gz
-rw-r--r-- 1 indy indy 26007162 Apr 13 13:35 Node16.log.2018-04-09.177.gz
-rw-r--r-- 1 indy indy 26007168 Apr 13 14:14 Node16.log.2018-04-09.178.gz
-rw-r--r-- 1 indy indy 26007113 Apr 13 14:52 Node16.log.2018-04-09.179.gz
-rw-r--r-- 1 indy indy 26006671 Apr 13 15:30 Node16.log.2018-04-09.180.gz
-rw-r--r-- 1 indy indy 26004741 Apr 13 16:08 Node16.log.2018-04-09.181.gz
-rw-r--r-- 1 indy indy 26004722 Apr 13 16:46 Node16.log.2018-04-09.182.gz
-rw-r--r-- 1 indy indy 26004615 Apr 13 17:25 Node16.log.2018-04-09.183.gz
-rw-r--r-- 1 indy indy 13008079 Apr 13 18:20 Node16.log.2018-04-09.184.gz
-rw-r--r-- 1 indy indy 2877044 Apr 16 00:00 Node16.log.2018-04-09.185.gz
-rw-r--r-- 1 indy indy 26056438 Apr 12 22:50 Node16.log.2018-04-09.gz
-rw-r--r-- 1 indy indy 5136836 Apr 16 09:08 Node16.log.2018-04-16.1.gz
-rw-r--r-- 1 indy indy 5443714 Apr 16 09:03 Node16.log.2018-04-16.gz{code}

*Actual Results:*
1. The same date (Monday of this week) is specified in log name during the week.
2. It's unclear when the file NodeX.log.2018-04-09.gz (without numeric index) was created. Looks like it is created daily, but on some nodes was rewritten:
{code:java}
ubuntu@virginaQALarge24:~$ ll /var/log/indy/sandbox/*2018-04-09.gz
-rw-r--r-- 1 indy indy 26089700 Apr 13 01:57 /var/log/indy/sandbox/Node24.log.2018-04-09.gz{code}
{code:java}
ubuntu@sydneyQALarge20:~$ ll /var/log/indy/sandbox/*2018-04-09.gz
-rw-r--r-- 1 indy indy 496172 Apr 16 00:00 /var/log/indy/sandbox/Node20.log.2018-04-09.gz{code}
{code:java}
ubuntu@tokyoQALarge18:~$ ll /var/log/indy/sandbox/*2018-04-09.gz
-rw-r--r-- 1 indy indy 3440278 Apr 16 00:00 /var/log/indy/sandbox/Node18.log.2018-04-09.gz
{code}
 

*Expected Results:*
Dates in logs names should accord to the date when the log was rotated,
*or*
Dates should not be specified at all.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1225,,,,,INDY-1275,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz6z3:",,,,,,EV 18.09 Stability-RocksDB,,,,,,,,2.0,,,,,,,,,,,,esplinr,ozheregelya,sergey.khoroshavin,,,,,,,,,,"25/Apr/18 1:20 AM;esplinr;Will be fixed along with INDY-1275 while we are in that part of the code.;;;","27/Apr/18 1:50 AM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/657;;;","03/May/18 10:03 PM;sergey.khoroshavin;How to test:
- setup test pool with default configuration
- wait until rotation happens

Expected results:
- rotated log files do not contain date in their names, only sequence number
;;;","06/May/18 8:27 PM;ozheregelya;Environment:
 indy-node 1.3.403

Steps to Validate:
 1. Setup the pool.
 2. Start small load test.
 3. Wait until logs rotation.

Actual Results:
{code:java}
indy@ip-10-0-0-101:~$ ll /var/log/indy/sandbox/
total 98148
drwxr-xr-x 2 indy indy 4096 May 6 10:33 ./
drwxrwxr-x 3 indy indy 4096 May 5 17:35 ../
-rw-r--r-- 1 indy indy 84930936 May 6 11:00 Node1.log
-rw-r--r-- 1 indy indy 7645212 May 6 10:00 Node1.log.1.xz
-rw-r--r-- 1 indy indy 7908764 May 6 10:33 Node1.log.2.xz{code}
No weekly logs rotation.;;;",,,,,,,,,,,,,,,,,,,,,
"If a requested transaction is not posted due to lack of consensus, issue an error in the logs",INDY-1274,29420,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,dsurnin,dsurnin,16/Apr/18 10:04 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,,,,0,,,,"*Story*
As an administrator of a deployment of Indy Node, I want failed transactions due to lack of consensus to produce a log message alerting me to the fact the transaction failed so that I can troubleshoot the situation.

*Acceptance Criteria*
* If a node receives a request to write a transaction, and does not achieve consensus within the timeout period, then issue a log message with the level ERROR.
* The timeout period should be configurable, and default to 60 seconds.
* The log message should say:
  {noformat}
   Consensus for ReqId: [] was not achieved within [Timeout] seconds:
   Primary node is [Node ID]
   Received Pre-Prepare from [node_id]
   Received [N] Prepares from [nodes_id1, node_id2, ...]
   Received [M] Commits from [nodes_id1, node_id2, ...]
   Transaction contents: [Contents]
  {noformat}

*Notes*
* The work on [INDY-465|https://jira.hyperledger.org/browse/INDY-465] demonstrated that some transactions don't fail due to an error on the node, but due to some other problem with the network. The only symptom the node sees is that the transaction is not recorded to the ledger.
* We could send a keepalive to sender, but there are concerns that would be exploitable for denial of service.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-465,INDY-1300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzz9un:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,Derashe,dsurnin,esplinr,ozheregelya,SeanBohan_Sovrin,,,,,,,"17/Apr/18 5:55 AM;SeanBohan_Sovrin;[~ashcherbakov] [~nage] - can you both take a look at this and give this some consideration?;;;","17/Apr/18 5:56 AM;esplinr;[~dsurnin] raises a useful usability / administration concern. I think this is more of a feature request than a task.

We should consider the right way to give feedback on slow responses, whether that's to the requestor or to the node admin in the logs. The right approach will not create a vulnerability to denial of service attacks.;;;","26/Apr/18 7:26 AM;esplinr;[~mgbailey] considers this critical for taking the network into a production state in order to diagnose problems with the network.;;;","28/Apr/18 6:20 AM;esplinr;Trev suggests two improvements. As much as possible, log output should be contextualized to assist administrators in understanding what is going on, and how they should respond.

1. It is unlikely that an admin will know that a ""Primary node"" should match the ""Pre-Prepare node"". So it is only useful to show the Pre-Prepare node when they do not match.
{noformat}
 Display Primary node is [Node ID].
 [Pre-Prepare received from [node_id] does not match.]
{noformat}

2. Trevor and I are both unclear as to the usefulness of knowing ""Received [N] Prepares from [...]"". Should we just drop this line?

Because the work has already been taken into a sprint, I did not edit the description of the issue. If the architect and developer agree with the suggestions, then it would be good to incorporate the improvements in this comment. But if you do not agree or if they significantly change the size of the story, then we can proceed with the current specification and decide later if more changes are necessary.;;;","28/Apr/18 4:43 PM;ashcherbakov;[~esplinr] [~tharmon] 
As for 1:
We may not receive PrePrepare at all, so it's always useful to display information whether we receive PrePrepare.

As for 2:
This is the most essential part of this ticket. Without these lines I do not see much value in the ticket at all (we can just display that we do not reach consensus).
This information is the only one allowing to provide more info about why we didn't have consensus.;;;","28/Apr/18 4:48 PM;ashcherbakov;I would rather suggest more extensions to the message as follows:
- mention whether the PrePrepare was suspicious
- mention whether Prepare/Commit was invalid

{code}
   Consensus for ReqId: [] was not achieved within [Timeout] seconds:
   Primary node is [Node ID]
   Received valid Pre-Prepare from [node_id] | Didn't receive a Pre-Prepare
   Received invalid Pre-Prepare from [node_id1]: <short desc of validation error>, Received invalid Pre-Prepare from [node_id2]: <short desc of validation error>, .....
   Received [N] valid Prepares from [nodes_id1, node_id2, ...]
   Received invalid Prepare from [node_id1]: <short desc of validation error>, Received invalid Prepare from [node_id2]: <short desc of validation error>, ... 
   Received [M] valid Commits from [nodes_id1, node_id2, ...]
   Received invalid Commit from [node_id1]: <short desc of validation error>, Received invalid Commit from [node_id2]: <short desc of validation error>, ... 
   Transaction contents: [Contents]
{code};;;","04/May/18 1:33 AM;Derashe;Problem reason:
 - We need to add log information when request have not been ordered for a while.

Changes:
 - Logging functionality added to the monitor class in form of a handler. 
 - Timeout is configurable with UnorderedCheckFreq in plenum/config.py.

PR:
- https://github.com/hyperledger/indy-plenum/pull/663

Version:
 - master, 346

Risk factors:
 - No

Risk:
 - Low

Covered with tests:
 - There are tests that cause this message appear in [plenum/test/monitoring/test_warn_unordered_log_msg.py|https://github.com/hyperledger/indy-plenum/pull/663/files#diff-008e1a17f6d7a88b51b579007b117893]

Recommendations for QA

Reproduce situation when txn have not been ordered and check that log message present. You can try to start 4-nodes pool, dissconnect two of them and send request.;;;","04/May/18 2:30 AM;esplinr;Sorry for the delay in responding to Alex's feedback. I spoke with [~mgbailey]. We want to provide admins with the information they need without drowning them in verbosity. Once Mike understood that messages about the status of the Pre-Prepare and Prepare will only be given as a summary of the node state when an error is received, and are not emitted each time a Pre-Prepare or Prepare message is received, he felt like we are hitting the right balance.

1. As you recommend, we will keep the Pre-Prepare message.

2. As you recommend, we will keep the message about Received N Prepares from . . .

3. We would like to defer your suggestion to report on whether the PrePrepare was suspicious.

4. We would like to defer your suggestion to report on whether the Prepare/Commit was invalid.

5. The message counts [N] and [M] in the description should only refer to valid messages (it might help to add the word ""valid"" to those lines of the log output): ""Received [N] valid Prepares from . . . "".

It seems to us that invalid messages should be extremely rare, and would be the results of bugs in Indy Node rather than something that an administrator can address. Validation failures might be reported as separate messages, but Mike doesn't think they improve an admins understanding of how their node fits into the rest of the network which is the goal of this story.;;;","04/May/18 6:00 PM;ashcherbakov;{quote}
3. We would like to defer your suggestion to report on whether the PrePrepare was suspicious.
4. We would like to defer your suggestion to report on whether the Prepare/Commit was invalid.
{quote}
I agree that we can defer it for now, but I would create a separate ticket.

{quote}
5. The message counts [N] and [M] in the description should only refer to valid messages (it might help to add the word ""valid"" to those lines of the log output): ""Received [N] valid Prepares from . . . "".
{quote}
[~Derashe] I think we already show only valid messages, but can we please double-check this and add 'valid' to the message?

{quote}
It seems to us that invalid messages should be extremely rare, and would be the results of bugs in Indy Node rather than something that an administrator can address. Validation failures might be reported as separate messages, but Mike doesn't think they improve an admins understanding of how their node fits into the rest of the network which is the goal of this story.
{quote}
No, this is not necessary bugs. For example, a Primary may have wrong clocks, and Pre-Prepares from this Primary will be discarded.
Also Pre-Prepare can be discarded if the Node's ledger doesn't match the rest of the Network (we can consider this as a bug in the code, but I think that this is possible in theory; for example, someone may edit the ledger manually).
Another example of invalid Messages (Pre-Prepare and Commit): wrong BLS signature. So, we can identify that either our node has wrong BLS keys, or another node has.

I think we already log such validation errors, so this is most probably not a high priority to show this info in the scope of the ticket, but we may consider create a separate ticket for this.;;;","04/May/18 8:03 PM;Derashe;Ok, then for now log message should looks like that:

_Consensus for ReqId: [] was not achieved within [Timeout] seconds:_ 

 _Primary node is [Node ID]_ 

 _Received Pre-Prepare from [node_id]_ 

 _Received [N] valid Prepares from [nodes_id1, node_id2, ...]_

_Received [M] valid Commits from [nodes_id1, node_id2, ...]_ 

 _Transaction contents: [Contents]_

[~esplinr] is this right?;;;","09/May/18 2:00 AM;ozheregelya;Following warnings appear for not ordered transactions:
{code:java}
2018-05-08 15:55:30,529 | WARNING | replicas.py ( 193) | unordered_request_handler_logging | Consensus for ReqId: 1525794469250069 was not achieved within 461.2516935040003 seconds. Primary node is Node1. Received Pre-Prepare from Node1:0. Received 3 Prepares from Node2:0, Node3:0, Node4:0. Received 0 Commits from noone. Transaction contents: {'protocolVersion': 1, 'reqId': 1525794469250069, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '3nCSDWLkUsvbgphuesim1nRqWeB5N1E9hHgk34tfe3PLdYmSFF99EKeA9BCcaN56Yud8RxNfURkXKarf3wFfzPnp', 'operation': {'dest': 'V4SGRU86Z58d6TV7PBUe64', 'type': '1'}}. {code}
 

This massage appears every minute.  [~esplinr], is it ok? After implementation of INDY-465 it was shown only once for each transaction. As an option, we can show this message when transaction was not ordered during [timeout] (60 sec by default) and another message when transaction will be ordered (if it will).;;;","15/May/18 1:44 AM;esplinr;Thank you [~ozheregelya] and [~Derashe] for the suggested improvement.

I spoke with [~mgbailey]. The message looks great. We would prefer it to only be sent once per transaction, as suggested, with a follow up message if a transaction that had exceeded the timeout does eventually become ordered.

Sorry about the delay in responding.;;;","18/May/18 12:23 AM;Derashe;Problem reason:
 - Log message of unordered txn appears regulary

Changes:
 * Log message about unordered txn appears once. Another message will appear when txn will be ordered

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/675]

Version:
 - master, 1.3.416

Risk factors:
 - No

Risk:
 - Low

Covered with tests:
 - There are test that cause this message appear in [plenum/test/monitoring/test_warn_unordered_log_msg.py|https://github.com/hyperledger/indy-plenum/pull/663/files#diff-008e1a17f6d7a88b51b579007b117893]

Recommendations for QA:

Reproduce situation when txn have not been ordered and check that both log message present.;;;","22/May/18 6:56 AM;ozheregelya;*Environment:*
 indy-node 1.3.420
 docker pool

*Steps to Validate:*
 1. Setup the pool.
 2. Send txns to make sure that pool works.
 3. Stop several nodes to lose consensus.
 4. Send txn.
 => ERROR message appears after timeout.
 5. Start stopped nodes.
 6. Send one more txn.
 => txn was written, INFO message appear.

*Actual Results:*
{code:java}
root@aa5cc4fc5831:/home/indy# tail -f /var/log/indy/sandbox/Node3.log | grep ""Consensus for ReqId""
2018-05-21 21:51:35,894 | ERROR | replicas.py ( 194) | unordered_request_handler_logging | Consensus for ReqId: 1526939404507905 was not achieved within 91.35529555899984 seconds. Primary node is Node1. Received Pre-Prepare from Node1:0. Received 3 valid Prepares from Node3:0, Node2:0, Node4:0. Received 0 valid Commits from noone. Transaction contents: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'reqId': 1526939404507905, 'signature': '2gG482E4dDk2XL4uzXpFibLwAX25JcrVSwb3KSTPwLcrCYjcbmWERmziqVked4erd5djF3FK6vpwARsV6tjcx7bu', 'operation': {'type': '1', 'dest': 'V4SGRU86Z58d6TV7PBUe63'}}. 
2018-05-21 21:52:30,662 | INFO | monitor.py ( 318) | requestOrdered | Consensus for ReqId: 1526939404507905 was achieved by Node3:0 in 146.12384285400003 seconds.{code};;;",,,,,,,,,,,
Move log compression into separate process,INDY-1275,29423,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,16/Apr/18 10:27 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,Currently log compression during rotation is performed in same process as request processing which can lead to 2-second pauses in processing during rotation (for 100 Mb log size threshold). Also if we switch to xz compression (which provides another 5-10 fold compression over currently used gzip) these pauses will be increased to 15-20 seconds. Moving compression into other process will eliminate these problems.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1273,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz6yv:",,,,,,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,,1.0,,,,,,,,,,,,ozheregelya,sergey.khoroshavin,,,,,,,,,,,"27/Apr/18 1:51 AM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/657;;;","03/May/18 10:02 PM;sergey.khoroshavin;How to test:
- setup test pool with default configuration
- run load test so that logs get lots of messages with close timestamps
- wait until rotation happens

Expected results:
- rotated log files are compressed with xz
- there's no time gap between last message in previous log and first message in next log
- there are no warnings in logs stating ""Needed to join log compression process"";;;","06/May/18 8:41 PM;ozheregelya;Environment:
 indy-node 1.3.403

Steps to Validate:
1. Setup test pool with default configuration.
2. Run load test so that logs get lots of messages with close timestamps.
3. Wait until rotation happens

Actual Results:
1. Rotated log files are compressed with xz. (/)
2. There's no time gap between last message in previous log and first message in next log. (/)
3. There are no warnings in logs stating ""Needed to join log compression process"". (/);;;",,,,,,,,,,,,,,,,,,,,,,
Explore 3pc batch settings,INDY-1276,29441,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ozheregelya,ozheregelya,17/Apr/18 8:46 PM,09/Oct/19 6:10 PM,28/Oct/23 2:47 AM,09/Oct/19 6:10 PM,,,,,,0,explore,help-wanted,,Need to perform load testing with different 3pc batch settings to find out the best setting for pool performance.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwyb3:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"09/Oct/19 6:10 PM;ashcherbakov;We've already done this;;;",,,,,,,,,,,,,,,,,,,,,,,,
[QA] Dependency of the pool performance on pool size.,INDY-1277,29442,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Won't Do,,ozheregelya,ozheregelya,17/Apr/18 8:49 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,explore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwyan:",,,,,,,,,,,,,,,,,,,,,,,,,,ozheregelya,zhigunenko.dsr,,,,,,,,,,,"14/May/18 7:43 PM;ozheregelya;This task was added to _Things to be tried_ list of [Performance spreadsheet|https://docs.google.com/spreadsheets/d/1DTjDsLSysFBiKU-9z4-IzunJk4wEy44hE_PGZYxnN_8/edit#gid=39274920]. This ticket duplicates it. So, it can be closed.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As a Steward, I need to receive notifications about important events on the Node",INDY-1278,29474,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,18/Apr/18 4:42 PM,11/Oct/19 6:33 PM,28/Oct/23 2:47 AM,11/Oct/19 6:33 PM,,,,,,0,,,,"We already have some ways and plugins for notification (email notifications, SQS, etc.)

Currently we have a feature to notify Stewards about Suspicious Spikes (by email): INDY-1251.

We need to send more notifications to Stewards:
 * View Change start
 * View change end
 * Catchup start
 * Catchup end
 * Blacklisting a Node
 * Blacklisting a Client
 * Suspicious Primary
 * Performance degradation (InstanceChange)
 * Upgrade start/end
 * other severe errors and warnings",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1300,,,,,INDY-1526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy7b:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,ckochenower,dsurnin,esplinr,mgbailey,,,,,,,,"26/Apr/18 11:26 PM;mgbailey;I love this ticket, but here is a concern.  If notifications that require no action from a steward become too plentiful, like more than one a day, then stewards are likely to start filtering and ignoring notifications. We need to make sure that stewards are able to enable / disable each type of notification at their discretion.  I believe that this is already the case, and it should continue to be so.;;;","26/Apr/18 11:34 PM;ashcherbakov;Agree. The main thing to decide in practise is what notification type(s) should we use. I believe we've just addressed this issue in INDY-1251.;;;","26/Apr/18 11:49 PM;mgbailey;Thinking on this more, a steward should be able to configure not only whether to receive a notification, but also what notification method to use for the notification. For example, urgent messages may go to SMS, and pedestrian ones to email.  This will probably be accomplished by whatever notification service they use.  We should provide the hooks / tags / whatever is necessary for this.;;;","26/Apr/18 11:57 PM;ashcherbakov;Yes. I agree that eventually we should have it.
But I think we should start with something simple before TDE.;;;","27/Apr/18 9:19 AM;esplinr;This appears to be another risky issue due to it containing vague requirements based on our guesses at what people need rather than user research.

I recognize the importance for admins to have proactive information about the state of their nodes, but Mike's feedback causes me to be concerned that this implementation won't meet the need in practice.

In addition, responding to log events is a solved problem and we shouldn't reimplement it. We should just ensure that our notifications can be fed into systems built for this purpose.

I propose that we proceed with the implementation, but keep it as minimal as possible. As long as the administrators can feed the notifications through the system of their choice, they can send it through a system that allows them to configure filters for the events that they consider most useful. We can adapt our implementation over time based on that feedback.
;;;","27/Apr/18 10:18 PM;esplinr;These system events result in log messages. I'm going to remove this from the team's backlog until we better understand whether additional notifications are necessary.;;;","26/Jul/18 7:24 AM;ckochenower;[~esplinr] or [~ashcherbakov] - Do we have all system events defined anywhere? Like a glossary or dictionary of events.

I agree that tools for interpreting their meaning based on frequency, severity, etc. are needed.

I think the automotive industry has done a great job evolving the [OBD|https://en.wikipedia.org/wiki/On-board_diagnostics] standards (currently OBD2). We could learn a lot from the OBD standards that directly apply to Indy Node (like a car), Stewards (driver/owner), and Engineers (technician, mechanic, hardware engineer, protocol engineer). I believe we can model our diagnostic/health-check status reporting in the same/similar fashion and expose the right data to the right people in a safe and useful manner.

Just like in a vehicle, there may be conditions external to indy-node inter-node communication that causes false positives. Some conditions may be symptomatic of a more severe problem, but turn out to be out of control of Indy Node (i.e. failing hardware).

In a modern car some conditions immediately illuminate the check engine light. Other conditions must be experienced a certain number of times and possibly within a certain time frame to cause the check engine light to illuminate. Temporary codes/events/conditions are stored when non-critical symptoms/conditions are experienced (i.e. low or high voltage readings on oxygen sensors). When frequency and timeframe meet certain criteria, a code is ""stored"" permanently, requiring a tool/technician to clear the code. Some permanent codes illuminate the check engine light and others do not. Some temporary codes illuminate the check engine light, but may get cleared after a restart or further diagnostics don't see the problem anymore; causing the check engine light to turn off without intervention.

When the check engine light is illuminated, subsystems (i.e. power train control module) enter fail-safe mode to minimize the chance for catastrophic failure without stranding the driver on the side of the road. The car doesn't run efficiently, but it will run.

I believe we have some conditions we don't want Stewards to effectively ignore. A big red check engine light needs to be in their face and possibly blink or make audible sounds when critical sub-systems/components experience fault/failure.

On one of my long road trips, I blew one of 6 spark plugs out of the engine head. My car immediately illuminated the check engine light and entered ""limp mode"". The oxygen fuel mixture was set to the safest (not most efficient) levels and the engine would cutout over 3000 RPMs to prevent further heat-related damage.

Root Cause Analysis (RCA): The spark plug had backed out of the threads slowly over time due to a ""crush washer"" that had not been replaced after the last spark plug inspection. The old plug was put back in and torqued to spec, but apparently wasn't sufficient to maintain torque. As the plug began backing out of the threads (likely due to vibration), eventually sensors began registering misfires. Air gap caused arching and arching caused the spark plug to melt. It melted to the point that the pressure from the pressure stroke produced enough pressure to blow the spark plug out of the head, taking the threads with it.

It was winter, my entire family was in the car, and we were high in a mountain pass about 15 miles from help. I was able to ""limp"" back to safety. Pretty impressive engineering in my opinion.;;;","11/Oct/19 6:33 PM;esplinr;Validator-info now reports all system events that we believe need to be monitored. Logging those events, and notifying network administrators, should be set up for each Indy network. If additional changes are required to assist with monitoring, we will create new tasks to track that work.;;;",,,,,,,,,,,,,,,,,
Modify existing load scripts for a better load testing,INDY-1279,29475,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ashcherbakov,ashcherbakov,18/Apr/18 6:15 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"The [current scripts|https://github.com/hyperledger/indy-node/tree/master/scripts/performance] that we got look over-complicated and don't provide all necessary features that we need.
 # We have multiple scripts that all do almost the same. Why can't we have just one script with parameters.
 # We need to support at least the following parameters:

 - type of requests (including random)
 - delay between requests
 - flag whether to wait for reply before sending the next request
 - number of messages per thread
 - number of threads",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-775,,,,,,,,,"1|hzz9tr:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,dsurnin,krw910,ozheregelya,zhigunenko.dsr,,,,,,,,"20/Apr/18 12:10 AM;zhigunenko.dsr;1) unstable error during statistics collecting
{code}
Traceback (most recent call last):
  File ""perf_runner.py"", line 374, in <module>
    PerformanceTestRunner().run()
  File ""perf_runner.py"", line 188, in run
    self.collect_result()
  File ""perf_runner.py"", line 207, in collect_result
    self.find_start_and_finish_time()
  File ""perf_runner.py"", line 279, in find_start_and_finish_time
    if self.finish_time < tester.finish_time:
TypeError: '<' not supported between instances of 'NoneType' and 'NoneType'
Error in sys.excepthook:
Traceback (most recent call last):
  File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
    from apport.fileutils import likely_packaged, get_recent_crashes
  File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>
    from apport.report import Report
  File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in <module>
    import apport.fileutils
  File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in <module>
    from apport.packaging_impl import impl as packaging
  File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 23, in <module>
    import apt
  File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 23, in <module>
    import apt_pkg
ModuleNotFoundError: No module named 'apt_pkg'

Original exception was:
Traceback (most recent call last):
  File ""perf_runner.py"", line 374, in <module>
    PerformanceTestRunner().run()
  File ""perf_runner.py"", line 188, in run
    self.collect_result()
  File ""perf_runner.py"", line 207, in collect_result
    self.find_start_and_finish_time()
  File ""perf_runner.py"", line 279, in find_start_and_finish_time
    if self.finish_time < tester.finish_time:
TypeError: '<' not supported between instances of 'NoneType' and 'NoneType'
{code}

2) perf_traffic.py line 99
{code}
    __kinds_of_request = [""nym"", ""attribute"", ""schema"", ""claim"",
                          ""get_nym"", ""get_attribute"", ""get_schema"",
                          ""get_claim""]
{code}
can raise this exception
{code:java}
Cannot submit claim request:
{""reqId"":1524148729640475841,""identifier"":""5jzdEyYxTj37RGbr168Xko"",""operation"":{""ref"":367210,""data"":{""primary"":{""n"":""55312796410659626845"",""s"":""43238942971209000644"",""rms"":""57777090240537072580"",""r"":{""name"":""75798985861406769916""},""rctxt"":""52360851049817003064"",""z"":""63056656501435427588""},""revocation"":{}},""type"":""102"",""signature_type"":""CL""}}
ErrorCode.LedgerInvalidTransaction
{code};;;","20/Apr/18 5:38 PM;ozheregelya;3) Error message permanently appears right after running traffic mode:
{code:java}
object NoneType can't be used in 'await' expression{code};;;","20/Apr/18 6:52 PM;ozheregelya;4) BTW, why the scripts should be run from the root user? It should be possible to run them from usual users.;;;","08/May/18 10:09 PM;zhigunenko.dsr;5) don't forget about measuring_transactions.py - it raises many errors if applied to big ledger;;;","17/May/18 2:43 AM;krw910;One of the things I would like to see is a script that generates traffic with different transaction types not just NYMs;;;","18/May/18 7:36 PM;dsurnin;new load script is provided
 [https://github.com/hyperledger/indy-node/pull/699]

it uses processes instead of threads to maximize number of concurrent requests to pool and stores all the statistics to csv file
 for the moment only non-blocking send is supported. blocking send implementation in progress.

[~ozheregelya] [~VladimirWork] [~zhigunenko.dsr]
 to avoid some useless indy python wrapper error output it is possible to run the script with stderr redirected to some file in a following way
{code:java}
python3 perf_processes.py -c 2 -n 100 2>error_file
{code};;;","23/May/18 7:30 PM;ozheregelya;New load script was implemented. Improvements for this script moved to separated tickets and will be implemented later: INDY-1355, INDY-1356, INDY-1357, INDY-1358.;;;",,,,,,,,,,,,,,,,,,
Provide ability to configure node network stack to emulate packets' delays,INDY-1280,29476,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,dsurnin,dsurnin,18/Apr/18 6:29 PM,08/Oct/19 9:18 PM,28/Oct/23 2:47 AM,08/Oct/19 9:18 PM,,,,,,0,quality,Stability,,"It would be helpful for test to configure network delays for different packets

Some thoughts
Add simple delay before calling send func
Delays should configurable via some config (dedicated or general)
Ability to use random delays from some predefined interval
Probably we can implement delays mapping - node delays message only for some nodes from the pool while communicating with the others normally or with different delays
Delay ping-pong",,,,,,,,,,,,,,,,,INDY-1115,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwy93:",,,,,,,,,,,,,,,,,,,,,,,,,,dsurnin,esplinr,,,,,,,,,,,"25/Apr/18 12:50 AM;esplinr;How would this differ from TrafficShaper.py that Kelly shared with the testers?;;;","25/Apr/18 6:53 PM;dsurnin;[~esplinr]
Thx for mention this! I wasn't aware of it. It is a good starting point. We definitely should try it first.;;;","08/Oct/19 9:18 PM;esplinr;TrafficShaper.py is meeting our needs.;;;",,,,,,,,,,,,,,,,,,,,,,
"As a Network Maintainer, I need to be able to check whether write works for every txn type without writing garbage to the ledger",INDY-1281,29478,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,dsurnin,ashcherbakov,dsurnin,18/Apr/18 9:25 PM,13/Jul/19 6:56 AM,28/Oct/23 2:47 AM,13/Jul/19 6:56 AM,,,,,,0,,,,"*Story*
 As an engineer responsible for the stability of an Indy network, I need to understand if the pool is in consensus even when there are no active transactions being written so that I can diagnose problems before they network is under and active transactions are disrupted.

*Acceptance Criteria*
 * Add optional parameter for every txn: `test`
 * Only Maintainer role can send txns with `test=True`
 * The rules for validation and ordering of txns are the same as for non-test
 * Execution of test txns doesn't write anything to the ledger (that is it's validated but not applied to uncommitted state)

*DESIGN TODO*
 As an addition response for this txn can include
 - send time and start processing time - to detect if pool under high load;
 - how long it took to process - to be able to notice pool degradation;
 - what nodes took part in consensus - determine node's availability, performance;  <---- an Item for SDK
 etc

Combine with INDY-933?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-1124,INDY-1916,,,,INDY-933,INDY-1928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwx4f:2s",,,,,,,,,,,,,,8.0,,,,,,,,,,,,dsurnin,esplinr,,,,,,,,,,,"18/Apr/18 9:57 PM;dsurnin;After the implementation we could think of some way to automate analysis of such responses.
It could be a standalone app or maybe some node plugin.;;;","13/Jul/19 6:56 AM;esplinr;We addressed this issue in a different way:
* We added a way to check the freshness of query responses
* That freshness check also ensures that the network is in consensus
* We report that consensus status as part of validator info

This provides significant confidence that the network is properly functioning, but falls short of validating that writes can occur to the ledger. However, as network traffic grows, the need for test writes decreases.

I am closing this issue as ""deferred"". If it becomes clear that we still need test writes, we can reopen or recreate the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,
Explore 2 NIC node configuration,INDY-1282,29480,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,,VladimirWork,VladimirWork,18/Apr/18 9:42 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,explore,,,"We should explore working capacity of 2 NIC node configuration before we will configure persistent pool this way because now both node and client IPs bind to 0.0.0.0.

- Will node work with 2 NIC and different IPs for node and client in pool ledger?
- Will pool work with 1 / f+1 / n-f / n nodes configured this way?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1254,,,,,INDY-1249,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzz6cf:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"18/Apr/18 10:00 PM;VladimirWork;FYI [~ozheregelya] [~zhigunenko.dsr];;;","04/May/18 10:22 PM;ashcherbakov;Duplicates INDY-1249;;;",,,,,,,,,,,,,,,,,,,,,,,
Tools to debug issues with Indy ledger,INDY-1283,29493,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,devin-fisher,esplinr,esplinr,19/Apr/18 8:15 AM,20/Apr/18 9:54 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*Story*

As a developer contributing to Indy Node, I need to be able to run experiments on an Indy Network that will cause known or unknown failures and reproduce those network failures in a development environment so that I can diagnose the problem. .

*Acceptance Criteria*
 * Be able to capture (record) all interactions with an individual node.
 * Be able to replay these interactions on a single node and cause the same fault from the experiment.
 * The code is contributed to Indy through a pull request.
 * Design documents are contributed to Indy as a pull request in Markdown format.

*Notes*

Key terminology:
 * interaction: all inputs and outputs seen by the node (network, disk, etc)
 * failure: the network is behaving poorly
 * fault: a node is behaving poorly
 * error: a bad event on a node (cpu spike, network outage, etc) ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwy8v:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Blocking Issue: Unable to use read_ledger tool with the parameter ""to""",INDY-1284,29526,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,krw910,krw910,20/Apr/18 2:51 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Mike used the following command line for the read_ledger tool with the latest stable release Indy Node 1.3.57


 The issue is the default for read_ledger only shows the first 100 transactions. Now we are unable to read the ledger past the first 100 transactions which is a problem.

*{color:#d04437}Upgrading the live pool will not occur until this is fixed{color}*

*sudo read_ledger --type domain --frm 400 --to 500*
 *{color:#d04437}Traceback (most recent call last):{color}*
 File ""/usr/local/bin/read_ledger"", line 163, in <module>
 print_txns(ledger, args)
 File ""/usr/local/bin/read_ledger"", line 120, in print_txns
 print_all(ledger, serializer)
 File ""/usr/local/bin/read_ledger"", line 141, in print_all
 for txn in ledger.getAllTxn(frm=frm, to=to):
 File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 233, in getAllTxn
 if to is None or int(seq_no) <= to:
 TypeError: unorderable types: int() <= str()",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzz707:",,,,,,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,,krw910,sergey-shilov,zhigunenko.dsr,,,,,,,,,,"20/Apr/18 8:40 PM;sergey-shilov;*Problem state / reason:*

See ticket description.

*Changes:*

Added conversion of read_ledger's 'from' and 'to' parameters to int.

*Committed into:*

    https://github.com/hyperledger/indy-node/pull/659
    indy-node 1.3.382-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

Check read_ledger with different (and without) from and to parameters.;;;","23/Apr/18 10:33 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.382

These calls have successfully finished (for a ledger of 10015 records):
{code:java}
read_ledger --type domain --frm 10000 --to 100
read_ledger --type domain --frm 10000 --to 1000000
read_ledger --type domain --frm 10000 --node node3
read_ledger --type domain --frm 400 --to 500
read_ledger --type domain --frm 400 --to 400
read_ledger --type domain --seq_no 10000
read_ledger --type domain --seq_no 100000
read_ledger --type domain --client_name default
read_ledger --type pool --frm 100.5
read_ledger --type pool --frm -10
read_ledger --type pool --serializer text
read_ledger --type pool --serializer json
{code}
;;;","24/Apr/18 1:17 AM;krw910;[~zhigunenko.dsr] I just tested this against the master build and if you use any option besides both --frm xx --to xx it will crash. I put this back into testing so you can validate what [~mgbailey] and myself are seeing. Gather the information and send this back to  [~sergey-shilov]. Please test all parameters before closing this ticket.;;;","24/Apr/18 1:25 AM;sergey-shilov;Sorry guys, that was my mistake, I've just reproduced and fixed it.;;;","24/Apr/18 2:34 AM;krw910;[~sergey-shilov] Thanks;;;","24/Apr/18 10:25 PM;sergey-shilov;*Committed into:*

    [https://github.com/hyperledger/indy-node/pull/660]
    indy-node 1.3.384-master;;;","26/Apr/18 7:55 PM;zhigunenko.dsr;*Environment:*
 indy-node 1.3.388, ledger with 10k+ txns

*Cases used to prove*
||Command||Result||
|read_ledger --type domain --frm 10000 --to 100|expected empty output|
|read_ledger --type domain --frm 10000 --to 1000000|correct output|
|read_ledger --frm 10000 --type domain --to 1000000|correct output|
|{color:#FF0000}read_ledger --type domain --frm 10000{color}|{color:#FF0000}empty output{color}|
|{color:#FF0000}read_ledger --type domain --frm 100{color}|{color:#FF0000}only one txn (like --seq_no 100){color}|
|read_ledger --type domain --to 10000|output with 10k|
|read_ledger --type domain --count --node Node2|correct output|
|read_ledger --type domain --to 10000 --node Node2|output with 10k|
|read_ledger --type domain --to 10000 --node Node2 --frm 9999|output with 2 txns|
|read_ledger --type domain --frm 400 --to 500|output with 101 txns|
|read_ledger --type domain --frm 400 --to 400|output with 1 txns|
|read_ledger --type domain --seq_no 10000|output with 1 txns|
|read_ledger --type domain --seq_no 100000|expected ""not found""|
|read_ledger --type domain --client_name default|error ""folder not found""|
|read_ledger --type pool --frm 100.5|expected type error|
|read_ledger --type pool --frm -10|output with 4 txns|
|read_ledger --type pool --serializer text|expected error|
|read_ledger --type pool --serializer json|output with 4 txns|

[~sergey-shilov]

*Required to fix:* script call with only --frm, without --to;;;","26/Apr/18 11:22 PM;sergey-shilov;[~zhigunenko.dsr]

I've re-checked and can not confirm the issue described above, please re-test.;;;","28/Apr/18 7:04 PM;zhigunenko.dsr;[~krw910] [~mgbailey]

regarding the use of _read_ledger --type domain --frm N_
Please, notice that ""_to_"" default value equals 100 (if not given). So if you work with ledger more than 100txns, you would get empty or incomplete list if only _--frm_ parameter is specified
You need to update requirements if it is unacceptable;;;",,,,,,,,,,,,,,,,
Update environment/docker/pool/README typo,INDY-1285,29539,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,Steve-Boyd,Steve-Boyd,Steve-Boyd,20/Apr/18 11:27 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"In ""Prerequisites"" section second bullet should state ""user"" instead of ""used""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz70v:",,,,,,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,,,esplinr,Steve-Boyd,,,,,,,,,,,"23/Apr/18 9:56 PM;esplinr;Pull request is here:

https://github.com/hyperledger/indy-node/pull/658;;;",,,,,,,,,,,,,,,,,,,,,,,,
Schema should have a limited number of attributes,INDY-1286,29588,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,andrey.goncharov,andrey.goncharov,23/Apr/18 4:40 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Currently we have no constraint on how many attributes a schema can have. There should be one. I suggest something reasonable, like 256.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwh3:",,,,,,,,,,,,,,,,,,,,,,,,,,andrey.goncharov,esplinr,slafranca,,,,,,,,,,"25/Apr/18 9:54 PM;esplinr;Thanks for the feature request.

Instead of adding a limit by number of attributes, we have a maximum size to the transaction (something like 128 KB).

Specifically adding a limit to the number of attributes would allow us to give more helpful errors. But there are many use cases that require large numbers of attributes (healthcare, regulated procurement). Given the wide number of use cases we want to support, we don't think the ledger should be enforcing an arbitrary limit.

We will continue to pay attention to requests like this to see if we should change our position.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Configurations should contain certain version of base58,INDY-1287,29592,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,Toktar,Toktar,23/Apr/18 8:23 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,A new version of the bass58 1.0.0 was released. The format of the response of the b58encode() was changed from string to bytes. Need to fix the version and update the code.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1291,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz70n:",,,,,,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,,Toktar,zhigunenko.dsr,,,,,,,,,,,"25/Apr/18 5:59 PM;Toktar;PR: 
 * [https://github.com/hyperledger/indy-plenum/pull/645]
 * [https://github.com/hyperledger/indy-node/pull/661]

 ;;;","27/Apr/18 11:12 PM;Toktar;Problem reason:
 base58 released version 1.0.0 with other API

Changes:
 Fixed version 1.0.0 of base58

PR:
 * [https://github.com/hyperledger/indy-node/pull/661]
 * [https://github.com/hyperledger/indy-plenum/pull/645]

Version:
 * indy-plenum 1.2.340-master
 * indy-node 1.3.393-master

Risk factors:
 Nothing is expected

Risk:
 Low;;;","28/Apr/18 7:10 PM;zhigunenko.dsr;Verified on:
* indy-node 1.3.395
* indy-plenum 1.2.342;;;",,,,,,,,,,,,,,,,,,,,,,
Nodes are lagging under permanent low load with disabled View Change,INDY-1288,29593,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,23/Apr/18 8:29 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,Stability,,,"Steps to Reproduce:
 1. Run load test with writing nym transactions with 1 sec delay (modified traffic mode).

Actual Results:
 Some nodes started lagging after ~240,000 written transactions.

Expected Results:
 Pool should work without any problems.

Logs: [https://drive.google.com/file/d/1VF2jqWNmrmG40vC2UnDHwuzt3CwWu6Bj/view?usp=sharing]
Journalctl: [https://drive.google.com/file/d/1_z9x73fJoLVFeB2ZZ2NWc862CQR01l6D/view?usp=sharing]
ps output: [https://drive.google.com/file/d/1aOuipT2Wa_eOFKKWYBFefv-IZxiRLqjf/view?usp=sharing]","indy-node 1.3.364
AWS 25-nodes pool",,,,,,,,,,,,,,,,,,,,,,,INDY-1188,,,,,INDY-1236,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz6xz:",,,,,,18.08 Stability-Monitoring,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,,dsurnin,ozheregelya,,,,,,,,,,,"26/Apr/18 6:15 PM;dsurnin;Lagged nodes failed several time due to out of mem and were not able to restore their states.
So it looks like we have at least two issue
- node was out of mem
- node was not able to restore;;;","27/Apr/18 6:26 PM;dsurnin;it looks like provided logs are not enough - the most interested parts are absent.
we need to reproduce it again with INFO logging.;;;","04/May/18 8:19 PM;dsurnin;Provided logs do not contain the moment of out of memory crash and node's start after this crash.
Logs starts after 1.5 hours after start and node already lagging a bit and generates huge amount of message requests.
This issue will be fixed in [INDY-1236|https://jira.hyperledger.org/browse/INDY-1236]

According to our tests with info level logging this issue is not reproduced no matter view change disabled or enabled;;;","06/May/18 2:52 AM;ozheregelya;With INFO log level and enabled view change the issue is the same as INDY-1315. Logs will be shared later.;;;","07/May/18 6:50 PM;ozheregelya;The issue was additionally discussed and it was decided to reproduce it one more time with new logs archiving, logLevel=INFO and disabled view change. Waiting for end of load and performance tests on QA pools.;;;","11/May/18 12:04 AM;ozheregelya;Now retesting of this issue don't make sense due to problem from INDY-1315. It will be retested in scope of INDY-1315 confirmation testing.;;;",,,,,,,,,,,,,,,,,,,
Support read-only state in read_ledger script with RocksDB,INDY-1289,29598,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,23/Apr/18 10:31 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1318,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz6xj:",,,,,,EV 18.09 Stability-RocksDB,,,,,,,,2.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,ozheregelya,,,,,,,,,,"04/May/18 11:12 PM;anikitinDSR;Reasons:
- need to add read_only option for read_ledger

Changes:
- added read_only option while openning storages

Versions:
indy-node: 1.3.402-master
indy-plenum: 1.2.347-master

;;;","06/May/18 7:32 PM;ozheregelya;Environment:
indy-node 1.3.403
AWS pool of 25 nodes (QA Live)

Reason for Rejection:
Read_ledger doesn't work if the indy-node is running.
{code:java}
ubuntu@sydneyQALarge7:~$ sudo su - indy -c ""read_ledger --type domain --count""
Traceback (most recent call last):
 File ""/usr/local/bin/read_ledger"", line 177, in <module>
 ledger = get_ledger(args.type, ledger_data_dir)
 File ""/usr/local/bin/read_ledger"", line 99, in get_ledger
 return Ledger(CompactMerkleTree(hashStore=hash_store), dataDir=ledger_data_dir, fileName=ledger_name)
 File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger.py"", line 13, in __init__
 super().__init__(*args, **kwargs)
 File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 60, in __init__
 self.start()
 File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 214, in start
 config=self.config)
 File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 26, in _defaultStore
 dataDir, logName, open)
 File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 36, in initKeyValueStorageIntKeys
 return KeyValueStorageRocksdbIntKeys(dataLocation, keyValueStorageName, open, read_only)
 File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb_int_keys.py"", line 21, in __init__
 super().__init__(db_dir, db_name, open, read_only)
 File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 23, in __init__
 self.open()
 File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb_int_keys.py"", line 27, in open
 self._db = rocksdb.DB(self._db_path, opts)
 File ""rocksdb/_rocksdb.pyx"", line 1437, in rocksdb._rocksdb.DB.__cinit__
 File ""rocksdb/_rocksdb.pyx"", line 84, in rocksdb._rocksdb.check_status
rocksdb.errors.RocksIOError: b'IO error: While lock file: /var/lib/indy/sandbox/data/Node7/domain_transactions/LOCK: Resource temporarily unavailable'{code};;;","10/May/18 1:56 AM;ozheregelya;Environment:
indy-node 1.3.405

Steps to Reproduce:
1. Setup the 'sandbox' pool.
2. Check all read_ledger parameters.
3. Setup the 'live' pool.
4. Check that read_ledger works.

Actual Results:
Read_ledger is basically works. Small issue noticed during testing was moved to INDY-1318.

 ;;;","11/May/18 8:04 PM;ozheregelya;Environment:
indy-node 1.3.410
AWS pool of 25 nodes (QA Large)

Reason for Reopen:
Sometimes read_ledger doesn't work when the pool is under writing load.

Steps to Reproduce:
1. Setup the pool and run load test.
{code:java}
for s in `seq 1 1000000` ; do echo ""===$s==="" ; sudo python3.6 perf_runner.py -t -c 20 -k ""nym"" &>>oz_load_additional.log ; done{code}
2. Run read_ledger during writing.

Actual Results:
{code:java}
ubuntu@ohioQALarge21:~$ sudo su - indy -c ""read_ledger --type domain --count""
Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 82, in recoverTree
self.recoverTreeFromHashStore()
File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 112, in recoverTreeFromHashStore
self.tree.verify_consistency(self._transactionLog.size)
File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 287, in verify_consistency
raise ConsistencyVerificationFailed()
ledger.util.ConsistencyVerificationFailed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
File ""/usr/local/bin/read_ledger"", line 177, in <module>
ledger = get_ledger(args.type, ledger_data_dir)
File ""/usr/local/bin/read_ledger"", line 99, in get_ledger
return Ledger(CompactMerkleTree(hashStore=hash_store), dataDir=ledger_data_dir, fileName=ledger_name, read_only=True)
File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger.py"", line 13, in __init__
super().__init__(*args, **kwargs)
File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 64, in __init__
self.recoverTree()
File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 87, in recoverTree
self.recoverTreeFromTxnLog()
File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 104, in recoverTreeFromTxnLog
self._addToTreeSerialized(entry)
File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 144, in _addToTreeSerialized
audit_path = self.tree.append(serializedLeafData)
File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 160, in append
self._push_subtree([new_leaf])
File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 130, in _push_subtree
self.hashStore.writeLeaf(h)
File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/db_hash_store.py"", line 33, in writeLeaf
self.leavesDb.put(str(self.leafCount + 1), leafHash)
File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 52, in put
self._db.put(key, value)
File ""rocksdb/_rocksdb.pyx"", line 1472, in rocksdb._rocksdb.DB.put
File ""rocksdb/_rocksdb.pyx"", line 78, in rocksdb._rocksdb.check_status
rocksdb.errors.NotSupported: b'Not implemented: Not supported operation in read only mode.'{code}
Expected Results:
Read_ledger should work.;;;","11/May/18 11:04 PM;anikitinDSR;Version:
indy-node: 1.3.411;;;","14/May/18 8:26 PM;ozheregelya;Environment:
indy-node 1.3.411
AWS pool of 25 nodes

Steps to Validate:
1. Setup the pool.
2. Run the load test.
3. Run the read_ledger during the load on ~1,000 txns.
=> read_ledger works.
4. Run the read_ledger on ~300,000 txns.

Actual Results:
read_ledger works well.;;;",,,,,,,,,,,,,,,,,,,
[Design] ViewChange protocol must be as defined in PBFT,INDY-1290,29599,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,23/Apr/18 10:45 PM,15/Aug/19 5:47 PM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"We found some issues with existing View Change.
We need to implement it in a proper way as defined in PBFT: https://www.microsoft.com/en-us/research/wp-content/uploads/2017/01/thesis-mcastro.pdf.

See details in https://jira.hyperledger.org/browse/INDY-1080

Please take the following requirements into consideration:
* We need to take into account RBFT and 3PC batching specific
* Have a look at the Fabric's PBFT experience and Sawtooth's RBFT one: INDY-1290
* The new View Change must be a pluggable strategy 
* State machine needs to be used
* PBFT viewchange is more conservative. So this could be more succeptible to follow-the-leader DDOS attacks. We need to take this into account
* We need to have a suite of aggressive real-world simulation tests where we hammer the protocol such that we can quickly be confident we've improved without deploying.
* We need to prove that the new implementation is better by executing the tests written in the scope of INDY-1296",,,,,,,,,,,,,,,,,,,,,,,,INDY-970,,,,,,,,,,INDY-1338,INDY-1339,INDY-1344,INDY-1345,INDY-1346,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|hzwqov:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"04/May/18 6:17 PM;sergey.khoroshavin;Considering experience of other projects:

1. Fabric doesn't have PBFT implementation now, although it does state that they have plans for implementing it: https://github.com/hyperledger/fabric/tree/master/orderer
Also Fabric had PBFT implementation two years ago (https://github.com/hyperledger/fabric/tree/0addcfb1e574e6a18f1e87b4a0282f9a57a17843/consensus/pbft), but it was dumped later. What's interesting is that while it did have view change implementation in separate module apparently it wasn't covered by tests.

2. Sawtooth doesn't have RBFT implementation now, although they plan to implement it, furthermore they plan to use Indy project as a reference: https://jira.hyperledger.org/browse/STL-1131 Also I found interesting PR with high-level overview of different consensus algorithms and proposal of generic consensus API to make them pluggable: https://github.com/hyperledger/sawtooth-rfcs/pull/4/files;;;","04/May/18 7:09 PM;sergey.khoroshavin;Considering follow-the-leader DDOS attack - one way to mitigate it could be making algorithm of choosing next primary unpredictable. It could be as follows:
1) nodes use multi-peer DH protocol to obtain common secret _key_
2) for some _viewNo_ we can obtain primary node _id = hash(concat(key, viewNo)) mod N_ where _hash_ is some cryptographic hash function, and _N_ is total number of nodes
This will prevent attacker from knowing which node will be next primary unless some malicious node cooperates with him and shares secret key.;;;","17/May/18 12:49 AM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/673;;;","17/May/18 10:40 PM;ashcherbakov;Design is created.
Technical tasks are created (INDY-1335, INDY-1336, INDY-1337, INDY-1338, INDY-1339, INDY-1340). Implementation will be done in the scope of these tasks.;;;",,,,,,,,,,,,,,,,,,,,,
Make sure that we have predictable dependencies version in plenum/node,INDY-1291,29635,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,24/Apr/18 9:39 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"We need to make sure that setup.py explicitly sets version for each dependency following the semver.
We've recently faced the issue with base58 which is fixed in the scope of INDY-1287.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1287,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz6zr:",,,,,,EV 18.09 Stability-RocksDB,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,,,,,,,,,,,"26/Apr/18 4:22 PM;anikitinDSR;PRs:
indy-node: https://github.com/hyperledger/indy-node/pull/670
indy-plenum: https://github.com/hyperledger/indy-plenum/pull/650;;;","27/Apr/18 8:28 PM;anikitinDSR;Because of some packages are built from pypi, some package's dependencies are lost. Therefore need to investigate some kind of approaches for solve ""Dependency hell"" problems. Approaches in pypi and apt repos are difficult. In that case if we use both of this repos, need to create ""merged"" method for deps resolving.
;;;","03/May/18 11:03 PM;ashcherbakov;The build is working now. Dependencies were fixed where possible
;;;",,,,,,,,,,,,,,,,,,,,,,
Blocking Issue: Unable to install Sovrin due to unmet dependencies,INDY-1292,29654,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ashcherbakov,krw910,krw910,25/Apr/18 4:04 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Trying to install the latest master version today by just running apt install sovrin the following error message is thrown.


 {code}
The following packages have unmet dependencies:
indy-plenum : Depends: python3-base58 (= 0.2.5) but 0.2.4 is to be installed
{code}

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz60v:",,,,,,18.08 Stability-Monitoring,,,,,,,,,,,,,,,,,,,,ashcherbakov,EvelynEvergreene,krw910,,,,,,,,,,"26/Apr/18 1:23 AM;ashcherbakov;This is because we have 0.2.5 dependency in setup.py and 0.2.4 dependency in build scripts (to build deb packages).
Switching to 0.2.4:

PR: https://github.com/hyperledger/indy-plenum/pull/649;;;","26/Apr/18 2:39 AM;ashcherbakov;Fixed in 1.3.387;;;",,,,,,,,,,,,,,,,,,,,,,,
Failed upgrade on node results in hundreds of 'in-progress' transactions,INDY-1293,29658,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,mgbailey,mgbailey,25/Apr/18 5:56 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"On the STN, an upgrade transaction was posted with force=True:
{code:java}
[29,{""action"":""start"",""force"":true,""identifier"":""6feBTywcmJUriqqnGc1zSJ"",""justification"":null,""name"":""STNUpgrade20180419"",""reinstall"":false,""reqId"":1524151227480884,""schedule"":{""2B8bkZX3SvcBq3amP7aeATsSPz82RyyCJQbEjZpLgZLh"":""2018-04-19T09:45:05.555000-06:00"",""2MHGDD2XpRJohQzsXu4FAANcmdypfNdpcqRbqnhkQsCq"":""2018-04-19T09:45:05.555000-06:00"",""8NZ6tbcPN2NVvf2fVhZWqU11XModNudhbe15JSctCXab"":""2018-04-19T09:45:05.555000-06:00"",""DNuLANU7f1QvW1esN3Sv9Eap9j14QuLiPeYzf28Nub4W"":""2018-04-19T09:45:05.555000-06:00"",""Dh99uW8jSNRBiRQ4JEMpGmJYvzmF35E6ibnmAAf7tbk8"":""2018-04-19T09:45:05.555000-06:00"",""EoGRm7eRADtHJRThMCrBXMUM2FpPRML19tNxDAG8YTP8"":""2018-04-19T09:45:05.555000-06:00"",""Eq7m7GMFKPeq5Ek3HH1PkHxzZ46R9VL1Eube3U9wfjp5"":""2018-04-19T09:45:05.555000-06:00"",""HCNuqUoXuK9GXGd2EULPaiMso2pJnxR6fCZpmRYbc7vM"":""2018-04-19T09:45:05.555000-06:00"",""UZH61eLH3JokEwjMWQoCMwB3PMD6zRBvG6NCv5yVwXz"":""2018-04-19T09:45:05.555000-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""signature"":""61LPVMEt4dKSHCac2d7hxyRKczoDGYgfyTjzVBJcyy6xMiJSSY1EGmfUvUvb5K95hK2QpbnzLaTsoBTBbjdLrMZg"",""signatures"":null,""timeout"":15,""txnTime"":1524151227,""type"":""109"",""version"":""1.3.57""}]{code}
One node, ""ibm"", failed to upgrade for reasons that are not germane to this ticket. The issue that needs to be investigated and resolved is that the issue of the un-upgraded node was not resolved for several days, and until it was resolved, hundreds of transactions like this were written to the config ledger, one every 15 minutes:
{code:java}
[500,{""data"":{""action"":""in_progress"",""version"":""1.3.57""},""identifier"":""Eq7m7GMFKPeq5Ek3HH1PkHxzZ46R9VL1Eube3U9wfjp5"",""reqId"":1524558624156459,""signature"":""8XSFy673fX84Wt7zHac4XQ2bctJCQtDSaxv3kJ1fc8ForEd7FA7Cpx8smTpKWjbmn4imFjRxAxu5EQqyaty3Vmx"",""signatures"":null,""txnTime"":1524558624,""type"":""110""}]{code}
I would expect that the behavior would be that after the upgrade timeout (set to 15 minutes in this case), that the upgrade transaction for the node is nullified, a failed-to-upgrade transaction is written to the ledger, and no further transactions would be posted after this.

I don't have logs for ""ibm"", but I am attaching logs for another node in the pool, as well as the config ledger contents.","STN, upgrading from 1.3.55 to 1.3.57",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/18 5:57 AM;mgbailey;config_ledger.txt;https://jira.hyperledger.org/secure/attachment/14914/config_ledger.txt","25/Apr/18 5:57 AM;mgbailey;england_logs.tgz;https://jira.hyperledger.org/secure/attachment/14913/england_logs.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwyen:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,mgbailey,,,,,,,,,,,"06/Sep/18 5:09 AM;esplinr;We believe this is addressed by the Indy Node 1.6 release.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Publish to ReadTheDocs.io,INDY-1294,29676,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,25/Apr/18 11:03 PM,11/Oct/19 10:26 PM,28/Oct/23 2:47 AM,11/Oct/19 10:26 PM,,,,,,0,,,,"*Story*
 As a consumer of Indy, I want to the documentation to be easy to find and easy to use so that I can quickly get my questions answered and deploy a node.

*Acceptance Criteria*
 * Documentation exists within the /docs/ folder of the Indy source code repository.
 * Documentation should be divided based on the target audience: contributors to Indy vs consumers of Indy.
 * Documentation meant for consumers of Indy should be published to ReadTheDocs.io.

*Notes*
 * Advantages to using ReadTheDocs.io:
 ** It is consistent with other Hyperledger projects.
 ** It is easier to find via Google than documentation only contained in GitHub.
 * Documentation in ReadTheDocs must be in RST format. Converters exist from MarkDown to RST.
 * See the Docs guidance provided by the Fabric project:
 [https://github.com/hyperledger/fabric/tree/release-1.1/docs]",,,,,,,,,,,,,,,,,,,,,,,IS-664,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-320,,,,,,,,,"1|hzwqr3:",,,,,,CommunityContribution,,,,,,,,,,,,,,,,,,,,esplinr,,,,,,,,,,,,"11/Oct/19 10:26 PM;esplinr;Documentation is now being published to ReadTheDocs.IO;;;",,,,,,,,,,,,,,,,,,,,,,,,
Reboot machine running Node leads to issues with consensus ,INDY-1295,29677,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,dsurnin,dsurnin,25/Apr/18 11:14 PM,23/Oct/19 11:41 PM,28/Oct/23 2:47 AM,23/Oct/19 11:41 PM,,,,,,0,,,,"This issue is create to split different problems from [INDY-1256|https://jira.hyperledger.org/browse/INDY-1256]

[~ozheregelya]
Environment:
indy-node 1.3.364 (master)
AWS QALive pool (20 nodes) with 260,490 txns in ledger.

Steps to Reproduce:
1. Make sure that all nodes are in consensus and that all nodes have the same primary.
2. Reboot instance with primary node (Node10).

Actual Results:
ViewChange was not happened on one node (Node15). For the rest nodes primary was changed (to Node11). Pool is still in consensus. 
After restart of problematic node (Node15) primary on this node was changed (to Node11), problematic node successfully completed catch-up and continued to work properly.

Logs: https://drive.google.com/file/d/1CBZYr2pMs1dapRqVbKSvn9RgVIb5quas/view?usp=sharing
Journalctl: https://drive.google.com/file/d/1_Nd4Exqe2Bhh1_OHh0l5c0y56obsGZMb/view?usp=sharing

Additional Information:
Similar case was tried by NIkita Zhigunenko and Vladimir Shishkin on another pools with stable version. ViewChange was successfully completed on all of the nodes in case of simple reboot.

[~zhigunenko.dsr]
Environment:
indy-node 1.3.364 (master)
AWS QALive pool (20 nodes)

Steps to Reproduce:
1. Make sure that all nodes are in consensus and that all nodes have the same primary.
2. execute on primary node (Node11) this ""execute sudo apt-get update && sudo apt-get upgrade && sudo reboot""
Actual results:
Ex-primary node cannot catch up new txns. Rest of nodes are consider ex-primary as unreachable
Expected results:
Ex-primary successfully catch up and write new txns.

Logs: https://drive.google.com/open?id=1YxjUlWcSxpXylq1Mu09dIY9kcyqJ5WV3
Thu Apr 12 07:31:20 2018 - approximate reboot time

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1256,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwyaf:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,,,,,,,,,,,"23/Oct/19 11:41 PM;ashcherbakov;Outdated issue;;;",,,,,,,,,,,,,,,,,,,,,,,,
Analyse the current View Change protocol issues,INDY-1296,29682,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,26/Apr/18 12:44 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,"We are facing some issues with the current View Change protocol (see INDY-1259 for example).
One of the options is to implement PBFT View Change protocol. 

But in order to fully understand the issues with the existing protocol and have more confidence in the new implementation, we need to perform deep analysis of the existing protocol.

*The output for this task*
    Result of analysis and a list of issues.

For future tasks:
* test plan with test cases we need to cover
* implementation of these tests (a separate ticket)
* fixes

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1259,INDY-1346,,,,,,,,,,,,,,,,,,,"25/May/18 12:02 AM;sergey.khoroshavin;vc_problems.pdf;https://jira.hyperledger.org/secure/attachment/15016/vc_problems.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzz9vb:",,,,,,EV 18.10 Stability and VC,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,sergey-shilov,,,,,,,,,,"14/May/18 8:21 PM;ashcherbakov;This needs to be fixed together with INDY-1259;;;","14/May/18 8:52 PM;ashcherbakov;Our current thinking about ViewChange problems:
1) The current approach has the assumption that >f nodes will eventually commit their prepared certificates. However, it's possible only if there is no big crashes.
2) Another problem with the current protocol is that it relies on some timestamp, that is eventually the prepared certificates will be committed or caught up. However, this is not very deterministic.

Also there are probably some issues with implementation of the current View Change approach:
1) Whether we give enough possibility for prepared certificates to be ordered (gather COMMITs) before/between rounds of catch-up that we are doing.
In particular, we start catch-up immediately, and it may revert some state (needed for committing prepared certificates).
2) Whether we have correct timeouts
3) Whether we should separate New Primary Selection Timeout (we need it since we use round robin for primary selection, and the new Primary can be unavailable), and All-Nodes-In-Sync timeout, that is the timeout we use for making sure that all nodes eventually in sync for the new view (prepared certificates committed, catch-up finished, etc.)
;;;","17/May/18 6:57 PM;ashcherbakov;Summary of the problems in existing protocol:

# The existing protocol doesn't guarantee in a deterministic and provable way that all requests ordered at at least f+1 nodes will be ordered in the new view.
# The existing protocol doesn't propagate the state of the maximum ordered request in the pool (prepared certificate) but relies on its own calculation. So, the only way to make sure that the node ordered everything that needed is to have some ""positive"" and ""optimistic"" timeouts, that is wait for some (maybe significant) time until the node ordered what it thinks needs to be ordered.
Timeouts are very undetermenistic, and it can be an issue in a distributed and asynced networks.

Further work will be continued in the scope of INDY-1341, INDY-1303, INDY-1304.;;;","18/May/18 1:35 AM;sergey.khoroshavin;I'd like to summarize things found during analysis of our view change protocol and PBFT and VR (precursor of PBFT) articles.

1) View Change protocol must guarantee that new view will contain all requests that were ordered in previous view by at least one correct and f potentially faulty nodes (in other words - if there is chance that for given request client received reply confirming that transaction was written to ledger it should stay in ledger after view change no matter what).

2) In a distributed system we cannot determine which requests were confirmed to user without extra phase after commit, but we can determine upper bound on requests that could have been potentially confirmed. This upper bound is largest prepare certificate that exists on n-f nodes - only for these requests some nodes could get commit quorum.

3) In order to determine this upper bound nodes need to know list of prepared certificates on other nodes. PBFT solves this deterministically by explicitly propagating this information during view change, and if it fails to do so by any reason (including timeout) it starts another view change immediately. This means that in PBFT when we finally enter some new view all non-faulty nodes are guaranteed to know all requests that should be carried from previous view (and this list is same on all nodes).

4) On the other hand current view change protocol makes an assumption that we can determine upper bound by waiting enough time for requests to commit. Given that all messages will be deivered this is valid assumption but problem is the time point when this happens is unbounded. If at some point some node doesn't receive enough commits it has no way of knowing whether messages it didn't receive are just late or won't arrive at all because of lack of prepare quorum.

5) So all in all current problems with view change can be solved in one of two ways:
- just increase timeouts reducing probability of failure (but it will never reach zero) and fix any remaining issues in code
- implement explicit prepare certificate propagation, which, if done properly, will lead to PBFT view change protocol
;;;","21/May/18 5:23 PM;sergey-shilov;We've done testing with increased view change time out (30 minutes), analysed logs and got the following result:

 - the view change finished by view change time out spent;

 - after finished view change the pool continued ordering transactions.

But despite continuation of ordering transaction such a long view change time out is not a good solution, because the probability of waiting till view change time out during view changed process is very high. The reason for it is unaligned _last_prepared_certificate_ (unlike PBFT)_._ Before starting of the view change process each node calculates it's own _last_prepared_certificate_ that corresponds to the last prepare message that has a quorum, so it's highly likely that these certificates are different for different nodes. If more than _f_ but less then _N-f_ nodes have higher prepared seqno than others then they can not complete a catch up that is sub process of view change. This leads to waiting till the view change time out. For example, in our test we observed the following situation:

======================================================================

2018-04-28 14:49:45.537000 |  Node11:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:58.897000 |  Node12:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:49.527000 |  Node15:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:52.869000 |  Node16:0 setting last prepared for master to (0, 148)
 2018-04-28 14:49:48.189000 |  Node18:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:57.800000 |  Node2:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:48.270000 |  Node23:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:53.251000 |  Node24:0 setting last prepared for master to (0, 148)
 2018-04-28 14:49:53.727000 |  Node25:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:48.534000 |  Node3:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:51.814000 |  Node7:0 setting last prepared for master to (0, 149)
 2018-04-28 14:49:47.264000 |  Node8:0 setting last prepared for master to (0, 149)

 

2018-04-28 14:49:44.947000 |  Node1:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:46.604000 |  Node10:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:50.750000 |  Node13:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:58.539000 |  Node14:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:51.600000 |  Node17:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:47.240000 |  Node19:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:50.815000 |  Node20:0 setting last prepared for master to (0, 151)
 2018-04-28 14:49:55.239000 |  Node21:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:53.077000 |  Node22:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:51.266000 |  Node4:0 setting last prepared for master to (0, 150)
 2018-04-28 14:49:57.592000 |  Node5:0 setting last prepared for master to (0, 151)
 2018-04-28 14:49:51.674000 |  Node6:0 setting last prepared for master to (0, 150)
 2018-04-28 14:50:16.465000 |  Node9:0 setting last prepared for master to (0, 150)

======================================================================

As a result the nodes from the first group were waiting for _VIEW_CHANGE_DONE_ messages from the nodes from the second group until view change time out have spent.

So the risks of long view change time out are the following:
 * high memory usage as we stash all incoming messages during the catch up process that may lead to Out Of Memory;
 * unavailable pool for a long time equal to view change time out
 * potential kind of attack to keep the pool infinitely unavailable

So one of the core problems of current view change protocol is unaligned _last_prepared_certificate_ that leads to incomplete catch up during the view change process. It's not very critical as pool nodes complete the view change process by time out and continue ordering, but it adds risks described above.

One of the possible solution is to increase view change time out up to 5 minutes (it's approximate, we will clarify it in further related tickets) as current 1 minute is too low.

There is another problem with inconsistent state trie that is not reproduced with long time out, we'll continue trying to reproduce it in scope of ticket INDY-1350. I think that this ticket may be closed.;;;","24/May/18 8:28 PM;sergey.khoroshavin;There were proposals for adding _VIEW_CHANGE_START_ messages to current view change protocol to mitigate found problems without increasing time needed for view change. Basic idea is that if we propagate last prepared certificate this way all nodes can select same last prepared certificate that was reached on _N-f_ nodes and be sure that they will get commits eventually, making possible to increase timeout for receiving COMMITs indefinitely.

Unfortunately in case of malicious nodes it won't work.

Suppose we have 4 nodes: _Alpha_, _Beta_, _Gamma_, _Delta_, with _Delta_ being malicious.

*Attack #1*
1) _Alpha_ reach prepared certificate 1 (and send corresponding commits), _Beta_ and _Gamma_ reach prepared certificate 2, _Delta_ state is irrelevant
2) View change starts, _Alpha_, _Beta_ and _Gamma_ report their prepared certificates normally, _Delta_ reports to _Beta_ that it reached prepared certificate 2, but at the same time it reports to _Alpha_ and _Gamma_ that it reached prepared certificate 1
3) Now _Alpha_ and _Gamma_ think that last prepared certificate on _N-f_ is 1, but _Beta_ thinks that its 2
4) _Alpha_ and _Gamma_ order requests till 1 and finish view change with ledger state 1
5) _Beta_ waits for commits quorum till 2, and given that it has it's own commit, it can receive commits from _Gamma_ and malicious _Delta_ and finish view change with ledger state 2
6) _Delta_ finish view change with ledger state 1
7) View change finishes with _Beta_ thrown out of consensus

This attack can be mitigated by adding _VIEW_CHANGE_START_ACK_ messages to make sure that malicious node cannot trick others into ordering different requests. However this still leaves pool open for denial of service attack.

*Attack #2*
1) _Alpha_ reach prepared certificate 1 (and send corresponding commits), _Beta_ and _Gamma_ reach prepared certificate 2, _Delta_ state is irrelevant
2) View change starts, _Alpha_, _Beta_ and _Gamma_ report their prepared certificates normally, _Delta_ reports that it reached prepared certificate 2
3) All nodes wait for commits quorum till 2, but _Delta_ doesn't send them, and 2 commits from _Beta_ and _Gamma_ are insufficient
4) As a result pool is halted for maximum wait time for commits which theoretically should be infinite

Further investigation is needed to determine if allowing processing prepares till last prepared certificate can mitigate this attack, but there is still high probability that it will still leave pool open for more complicated attacks.

On the other hand PBFT view change protocol is not susceptible to these attacks and has a formal proof of correctness.
;;;",,,,,,,,,,,,,,,,,,,
Remove ledger status based catch-up trigger together with wrong catch-up workflow,INDY-1297,29685,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,spivachuk,spivachuk,26/Apr/18 12:57 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,"The workflow of catch-up being initiated by this trigger is wrong: a catch-up initiated in such the way is started from the ledger which the received newer ledger status belongs to - so not all ledgers may eventually catch up.

Also it seems that this catch-up trigger is redundant because we have another catch-up trigger - the checkpoint-based one - for the case if a ledger lags behind from the ledgers on other nodes.

In scope of this task we should remove ledger status based catch-up trigger and eliminate the related wrong catch-up workflow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1298,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzz9t3:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,5.0,,,,,,,,,,,,ozheregelya,spivachuk,zhigunenko.dsr,,,,,,,,,,"18/May/18 8:27 AM;spivachuk;*Problem reason:*
- Catch-up initiated by {{LedgerStatus}}-based trigger had a wrong workflow. Moreover, {{LedgerStatus}}-based trigger of catch-up was redundant because catch-up has also {{Checkpoint}}-based trigger.
- Commits were not requested for missed ranges of 3PC-batches.
- A number of issues were found in the catch-up logic (see the list of them in INDY-1298).
- {{Replica.processStashedMsgsForNewWaterMarks}} had a wrong logic which might lead to {{IndexError}} in case of a recursive call of this method.

*Changes:*
- Removed the faulty logic of a catch-up start from an arbitrary ledger on reception of a newer {{LedgerStatus}}.
- Added requesting of missed {{Commits}} on reception of an out-of-order {{PrePrepare}} to make it possible for the node's replicas to fill the gap in the sequence of 3PC-messages and order them after the node disconnection and further re-connection (because catch-up is not done anymore on reception of a newer {{LedgerStatus}}).
- Updated the tests that verify the message requesting mechanism and requesting and processing of missed 3PC-messages.
- Removed handling of {{PrePrepares}} being received during catch-up.
- Added clearing of the queues related to 3PC-process to {{Node.no_more_catchups_needed}} method in order to eliminate attempts to execute 3PC-batches reverted before catch-up and not re-applied later.
- Removed postponing of processing {{LedgerStatuses}} in case a catch-up is in progress. Now older {{LedgerStatuses}} are responded with {{ConsistencyProofs}} immediately in any case.
- Fixed issues with use of a wrong quorum for none-proofs.
- Fixed a bug with a lack of setting discovering node mode.
- Fixed a bug with a possible {{IndexError}} in {{Replica.processStashedMsgsForNewWaterMarks}} method.
- Reworked {{test_non_primary_recvs_3phase_message_outside_watermarks}}.
- Corrected tests according to the changes made in the catch-up logic.
- Disabled the tests which became irrelevant after {{LedgerStatus}}-based catch-up trigger had been removed.
- Fixed a bug in test fixtures of indy-node with modifying the genesis domain transactions after the pool nodes have already been initiated using the initial genesis domain transactions.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/638
- https://github.com/hyperledger/indy-node/pull/694

*Version:*
- indy-node 1.3.413-master
- indy-plenum 1.2.358-master

*Risk factors:*
- {{Checkpoint}}-based catch-up trigger. (Previously we did not deal with it in system tests and real cases because when a node was lagging, {{LedgerStatus}}-based catch-up trigger was raised first.)
- Interaction between catch-up and processing of 3PC-messages.

*Risk:*
- Medium

*Covered with tests:*
- The tests in {{plenum.test.node_catchup}} package.
- The tests in {{plenum.test.node_request.message_request}} package.
- {{test_non_primary_recvs_3phase_message_outside_watermarks}}

*Recommendations for QA:*
- Catch-up is not performed anymore on network reconnection. This also means that catch-up is not performed on node promotion. However, in such situations either the node replicas will request missed 3PC-messages on reception of new PrePrepares and eventually order the missed 3PC-batches (in case a lag in scope of one checkpoint of the master protocol instance) or the node will start catch-up on reception of 2 generation of future {{Checkpoints}} from other nodes (in case of a significant lag). Please note that catch-up is still performed on node start / restart.
- Please test that a node requests 3PC-messages and eventually orders the 3PC-batches missed while it was being disconnected (but turned on) in case the lag is in scope of one checkpoint of the master protocol instance.
- Please test catch-up under load to check operability of the interaction between catch-up and processing of 3PC-messages.;;;","23/May/18 11:17 PM;zhigunenko.dsr;*Test scenario:*
1) create pool with 20 nodes
2) demote 2 of nodes
3) make load test session
4) check logs for catch-up by checkpoint
5) promote one node and observe its behavior (without load)
6) promote another one during next load test session, observe its behavior
7) during load test session down and up network interface
8) during load test session stop and start indy-node service
9) add new node to pool (when pool is idle)
10) add new node to pool (when pool is under load);;;","25/May/18 12:37 AM;ozheregelya;Testing of this ticket will be performed in scope of INDY-1367 because of current problems with load testing (INDY-1365).
[|https://jira.hyperledger.org/secure/AddComment!default.jspa?id=28802];;;",,,,,,,,,,,,,,,,,,,,,,
Fix the issues found in the current logic of catch-up,INDY-1298,29686,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,spivachuk,spivachuk,26/Apr/18 1:06 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"Size: S (PoA)

In scope of this ticket fix the following issues in the current logic of catch-up:
* Execution of 3PC-batch after catch-up in case it was applied before catch-up and then reverted at catch-up start.
* Absence of removal of {{PrePrepares}} from {{Replica.stashingWhileCatchingUp}}.
* If catch-up is in progress and some ledger catch-up has not been started yet then the node does not respond older {{LedgerStatuses}} for this ledger with {{ConsistencyProofs}} until it starts catch-up of this ledger.
* Different quorums for not newer {{LedgerStatuses}} and none-proofs which actually mean the same.
* Mode.syncing is used instead of Mode.discovering.
* Wrong log message on reaching the quorum of none-proofs in {{LedgerManager.canProcessConsistencyProof}}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1297,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzz9wf:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,spivachuk,VladimirWork,,,,,,,,,,,"18/May/18 8:28 AM;spivachuk;Fixed in scope of INDY-1297.;;;","18/May/18 9:08 PM;VladimirWork;It will be tested in scope of INDY-1297.;;;",,,,,,,,,,,,,,,,,,,,,,,
[Design] Design interaction between catch-up actors and 3PC actor,INDY-1299,29687,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,spivachuk,spivachuk,26/Apr/18 1:32 AM,09/Oct/19 5:10 PM,28/Oct/23 2:47 AM,09/Oct/19 5:10 PM,,,,,,0,,,,In scope of this task design interaction between catch-up actors (see INDY-971) and 3PC actor (see INDY-979).,,,,,,,,,,INDY-971,INDY-979,,,,,,INDY-1301,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzwybb:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,spivachuk,,,,,,,,,,,"09/Oct/19 5:10 PM;esplinr;Done as part of other work in this epic.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Nodes should log who the primary is on a daily basis,INDY-1300,29704,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,26/Apr/18 6:39 AM,11/Oct/19 6:24 PM,28/Oct/23 2:47 AM,11/Oct/19 6:24 PM,,,,,,0,,,,"*Story*
As an administrator of an Indy node, I want to understand who my node thinks is the primary node on a daily basis so that I can identify network problems.

*Acceptance Criteria*
* A log message of level INFO should be printed each day.
* The message should read: This node thinks [node identifier] is the primary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-465,INDY-1274,INDY-1278,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy7j:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,mgbailey,,,,,,,,,,"26/Apr/18 7:26 AM;esplinr;[~mgbailey] considers this important to have before taking the network into a production state in order to diagnose problems.;;;","26/Apr/18 10:18 PM;esplinr;[~mgbailey] Does INDY-1278 meet this need in a better way?;;;","26/Apr/18 11:22 PM;mgbailey;I like INDY-1278.  I do have concern that the notifications may become too common, though.  If a steward is receiving several notifications a day that require no action on his part, he will probably start filtering and ignoring notifications. Where I am going with this, is that I think that a log message like this continues to be useful, as well as notifications.;;;","27/Apr/18 9:10 AM;esplinr;[~mgbailey] Thank you for sharing your perspective.;;;","11/Oct/19 6:24 PM;ashcherbakov;We have this in Validator Info;;;",,,,,,,,,,,,,,,,,,,,
[Design] Design catch-up procedure divided into separate sub-procedures for each replica,INDY-1301,29712,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,spivachuk,spivachuk,spivachuk,26/Apr/18 6:06 PM,26/Apr/18 10:12 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Design a catch-up procedure divided into separate sub-procedures for each of the replicas. Please see the comments to INDY-1153 for details.,,,,,,,,,,INDY-971,INDY-1299,,,,,,INDY-1153,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwye7:",,,,,,,,,,,,,,,,,,,,,,,,,,spivachuk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PBFT View Change,INDY-1302,29719,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,26/Apr/18 11:09 PM,21/Nov/19 7:03 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Implement ViewChange protocol as in PBFT for better stability and predictability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-4,,PBFT View Change,To Do,,,,,,,"1|hzyvfj:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test Plan for View Change protocol,INDY-1303,29722,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,26/Apr/18 11:42 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"After analysis done in the scope of INDY-1261,
we need to come with a test plan. The plan will help us to test both existing view change as well as the new PBFT approach.

The tests can be used to
* prove that the old protocol doesn't work
* prove that fixes of the old protocol work
* prove that the new implementation (PBFT) works 

*Output*
Test plan

",,,,,,,,,,,,,,,,,INDY-1304,,,,,,,,,,,,,,,,,INDY-1347,INDY-1349,,,,,,,,,,,,,,"25/May/18 12:02 AM;sergey.khoroshavin;vc_problems.pdf;https://jira.hyperledger.org/secure/attachment/15015/vc_problems.pdf",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzz9u7:",,,,,,EV 18.10 Stability and VC,,,,,,,,5.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,sergey.khoroshavin,spivachuk,Toktar,,,,,,,,"14/May/18 8:19 PM;anikitinDSR;Test cases for proving, that old protocol doesn't work:
* Delaying commit messages for all nodes. Send some requests, undelaying commit messages for non primary node and force view change process. When view change complete, we can get wrong state root for non primary node.
* If we have n-f+1 nodes and the next step is disconnect primary. In that case, we have n-f nodes, which will start to initiate view_change process by sending instance change messages. After this if one of other nodes will be restarted, then remaining nodes will not elect new primary, besause restarted node will lost current viewNo and will try to send IC message with viewNo ""0"". If instance change will be initiated by ""primary degraded"", then restarted node must order some resuests, before understanding, that ""primary degraded"";;;","17/May/18 4:55 PM;ashcherbakov;Scenario from INDY-1259:
- Have the same last_prepared (uncommiited) certificate on all nodes, so that all nodes, for example, have 1 uncommitted request for which they have a prepared certificate but haven't ordered it yet.
- Delay COMMITs on all nodes except NodeX
- Start view change
=>View change ends and the prepared certificate will be reached on NodeX but not on other nodes.
=> we have 1 node with 1 more txn than others which can not be recovered since the ledger is append only.;;;","22/May/18 12:57 AM;sergey.khoroshavin;*#1*

Test case:
- given 4 nodes
- increase view change timeout to exceed allowed test running time
- disable normal view change to make tests deterministic
- indefinitely delay receiving commit messages on all nodes
- send some requests
- wait until all nodes have same last prepare certificate
- indefinitely delay receiving prepare messages on two nodes
- send one more request
- wait until other two nodes increase last prepare certificate
- trigger view change on all nodes (using view_changer.on_master_degradation)
- stop delaying commits
- wait for view change done on all nodes

Expected result with correct view change: view change will be successfully finished
Expected result with current view change: view change will take maximum time allowed for view change, triggering test timeout;;;","23/May/18 6:58 PM;sergey.khoroshavin;*#2*

Subcase:
- given node A
- indefinitely delay receiving commit messages on all nodes
- send some requests
- wait until all nodes have same last prepared certificate
- trigger view change on all nodes
- stop delaying commits on A
- wait until view change is complete
- stop delaying commits on other nodes

Test case:
given nodes N1, N2, N3, N4
disable normal view change to make tests deterministic
run test subcase for N4
run test subcase for N1
try ordering transactions

Expected result with correct view change: transactions should be ordered normally
Expected result with current view change: second view change won't finish because pool is in inconsistent state;;;","24/May/18 6:29 PM;Toktar;#3

Test case:
 - disable normal view change to make tests deterministic
 - delay commits for all nodes except node X
 - send request
 - check ordered transaction in node X
 - start view_change
 - check end of view change for all nodes
 - switch off commits' delay
 - get reply (means that request was ordered in all nodes)
 - repeat

Expected result with correct view change: transactions should be ordered normally
 Expected result with current view change: node X can't finish second transaction;;;","24/May/18 10:35 PM;spivachuk;*#4*

Test subcase:
- given node A
- indefinitely delay receiving InstanceChange and ViewChangeDone messages on node A
- indefinitely delay receiving Commit messages on all nodes
- send some requests
- wait until all nodes have same last prepare certificate
- trigger view change on all nodes (using view_changer.on_master_degradation)
- stop delaying InstanceChange messages on all nodes except A
- wait for view change done on all nodes except A
- stop delaying Commit messages
- stop delaying InstanceChange messages on node A
- give time for node A to proceed with view change up to sending ViewChangeDone
- stop delaying ViewChangeDone messages on node A
- wait for view change done on A

Test case:
- given nodes N1, N2, N3, N4
- disable normal view change to make tests deterministic
- run test subcase for N4
- run test subcase for N1
- try ordering transactions

Expected result with correct view change: transactions should be ordered normally
Expected result with current view change: second view change won't finish because pool is in inconsistent state;;;","28/May/18 8:36 PM;spivachuk;*#5*

Test subcase:
- given node A
- indefinitely delay receiving InstanceChange and ViewChangeDone messages on node A
- indefinitely delay receiving Commit messages on all nodes
- send some requests
- wait until all nodes have same last prepare certificate
- trigger view change on all nodes (using view_changer.on_master_degradation)
- stop delaying InstanceChange messages on all nodes except A
- wait for view change done on all nodes except A
- stop delaying Commit messages
- stop delaying ViewChangeDone messages on node A
- wait for propagate primary done on node A
- stop delaying InstanceChange messages on node A

Test case:
- given nodes N1, N2, N3, N4
- disable normal view change to make tests deterministic
- run test subcase for N4
- run test subcase for N1
- try ordering transactions

Expected result with correct view change: transactions should be ordered normally
Expected result with current view change: second view change won't finish because pool is in inconsistent state;;;",,,,,,,,,,,,,,,,,,
Implement more tests for view change,INDY-1304,29755,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,27/Apr/18 4:09 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"Based on the test plan from INDY-1303, we need to write more tests for view change.
The tests can be used to
* prove that the old protocol doesn't work
* prove that fixes of the old protocol work
* prove that the new implementation (PBFT) works 

*Output*
New tests (integration, functional, load).
",,,,,,,,,,INDY-1303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzd0v:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,spivachuk,Toktar,,,,,,,,,"23/May/18 6:59 PM;sergey.khoroshavin;Failing integration test cases can be found in this [PR|https://github.com/hyperledger/indy-plenum/pull/693];;;","24/May/18 7:26 PM;Toktar;Another one test showing problem with view change: 
https://github.com/hyperledger/indy-plenum/pull/701;;;","24/May/18 11:23 PM;spivachuk;Added tests for Case 4 from INDY-1303:
https://github.com/hyperledger/indy-plenum/pull/703;;;","29/May/18 5:55 PM;ashcherbakov;Tests for Case 5: https://github.com/hyperledger/indy-plenum/pull/708;;;","29/May/18 6:01 PM;sergey.khoroshavin;Tests intermediate refactoring: https://github.com/hyperledger/indy-plenum/pull/705;;;",,,,,,,,,,,,,,,,,,,,
Blocking Issue: Dependency missing during installation python3-pygments ,INDY-1305,29776,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,krw910,krw910,28/Apr/18 1:44 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"There is a dependency in the master branch on a package version that does not appear in the repo:
{code}
indy-plenum : Depends: python3-pygments (= 2.2) but 2.1+dfsg-1 is to be installed
{code}

This issue is blocking testing for a new Steward.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz6zz:",,,,,,EV 18.09 Stability-RocksDB,,,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,,,,,,,,,,,"28/Apr/18 9:41 PM;ashcherbakov;Fixed in the scope of INDY-1291;;;",,,,,,,,,,,,,,,,,,,,,,,,
Need to fix periodically failed test test_primary_selection_after_demoted_node_promotion,INDY-1306,29793,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,anikitinDSR,anikitinDSR,28/Apr/18 8:05 PM,12/Oct/18 1:50 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Test location:
plenum/test/primary_selection/test_primary_selection_after_demoted_node_promotion.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwya7:",,,,,,,,,,,,,,,,,,,,,,,,,,anikitinDSR,spivachuk,,,,,,,,,,,"12/Oct/18 1:50 AM;spivachuk;The bug is actual. This test fails on my local machine on the recent master.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Improve ansible playbooks for testing pools,INDY-1307,29799,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,29/Apr/18 6:32 PM,04/Aug/18 12:50 AM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"I've created simple ansible playbooks for test pool setup, configuration, log retrieval and cleanup: https://github.com/hyperledger/indy-node/tree/master/scripts/ansible
They are primitive, but do their job of simple tasks automation (at least for me). Now in order to become more useful these playbooks should be improved in several ways (from most to least important in my opinion):
- pool configuration options should be in separate file
- playbooks should be refactored into a number of reusable roles and tasks
- playbooks should be documented (especially after refactoring)
- playbooks should be capable of installing indy node on clean system (now they assume that instances have multiple things configured)
- test harness should be configured so further development of playbooks and roles can be done through TDD",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwxrj:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ozheregelya,sergey.khoroshavin,,,,,,,,,,,"04/Aug/18 12:31 AM;sergey.khoroshavin;I'd like to elaborate on how and what should be done here.

First of all, as stated in issue, we already have simple scripts for some bulk management of nodes. I wrote them as fast as possible as part of load testing that I had to perform myself, and used them many times afterwards. I chose Ansible because:
- I had positive prior experience with it
- it's learning curve is very low (and I wanted these scripts to be usable by anyone in our team)
- it doesn't require central server (so no additional infrastructure is needed)
- it doesn't require agents on target hosts
- it has large community

Main problem with current playbooks is that they are monolithic, variables are embedded in playbooks and there is no code reuse. Proper way of handling this is with _roles_, which can be parametrized and used in other roles and end-user playbooks. I think there should be separate reusable roles at least for:
- node installation
- config updates
- restarts
- log retrieval
- cleanups

Another problem is that there are no tests now. Recently I've evaluated some testing options in my pet project.
- Test Kitchen + Kitchen-Ansible/Ansiblepush + Inspec - seemed very nice, very modular, Inspec has tons for ready to use modules, there's also DevSec community providing some security profiles written in Inspec which can be run both in tests and against staging/production environment. There are downsides however: it's not meant to test multi-machine environments (but there are some workarounds, at least for Vagrant), and it cannot handle cases when Ansible reboots target nodes (and it seems like there's no workaround for that case).
- Molecule + Inspec - also seemed very nice at first, Molecule has native support for Ansible, handles multi-machine environments well, doesn't have issues when playbook needs to reboot target machine, and we can still get all benefits of writing tests in Inspec. However, integration between Inspec and Molecule is poor. First, it doesn't run Inspec against target nodes, it actually installs it and runs it ON target nodes. It's slow, it changes target state (and there were reports that for some people it really caused problems) and it's output is harder to read. And on top of that when Inspec tests are parametrized by attributes these attributes should be provided in separate file instead of central test case config.
- Molecule + TestInfra - all benefits of Molecule, TestInfra is nicely integrated, and tests are written in Python with PyTest. Downside is that TestInfra doesn't have as much features as Inspec and there are no 3rd-party tests for security hardening.

Given that Indy Node is written in Python and chances are high that we'll need  multinode setups (check that we can really create working pool) and reboots during tests (check that node was correctly installed and successfully starts after reboot) I'd go with last option, but there might be other opinions.
 
Finally, after refactoring of current code and covering it with tests is done additional roles can be added:
- uploading logs to S3 bucket, instead of downloading them directly to workstation
- downloading logs from S3 bucket to workstation
- running log processing directly in testing pool (INDY-1373)
- performance scripts installation
- provisioning node pools and load agents

Also it might be beneficial to publish node installation role to Ansible Galaxy, so it can be easily used by anyone who wish to install their own node.
;;;",,,,,,,,,,,,,,,,,,,,,,,,
Indy requires particular version of Python,INDY-1308,29868,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,RyanWest,devin-fisher,devin-fisher,02/May/18 4:25 AM,09/Aug/19 3:40 PM,28/Oct/23 2:47 AM,09/Aug/19 3:40 PM,,,,,,0,help-wanted,,,"On May 1, 2018:

python 3.5.1 did not work!

python 3.6.3 did not work!

 

Only python 3.5.2 seem to work for me.

(Nathan ask me to log this bug. I believe he wants to discuss this issue so please do not dismiss this ticket without that discussion.)

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwqr3:c",,,,,,CommunityContribution,,,,,,,,,,,,,,,,,,,,ashcherbakov,devin-fisher,RyanWest,stevetolman,swcurran,,,,,,,,"15/May/18 9:24 AM;RyanWest;Running Python 3.6.1, getting the error: ""AttributeError: module '_sha3' has no attribute 'sha3'"" over and over again. This may be an issue related to 3.6 upgrade, will investigate further.;;;","18/May/18 8:35 AM;RyanWest;Confirmed that this is the issue. Python interpreter code comparisons (sha3 pypi package version 0.2.1 for both). See code below for difference between 3.5 and 3.6.

So the upgrade from Python3.5 to 3.6 must have changed something with regards to either how Cython compiles the C code to be Python-callable, or something else. Still figuring out how to get the sha3 dependency to work with Python 3.6+.

However, Python3.6 introduced support for SHA3-256 in its own hashlib. We could instead use that and get rid of the other sha3 dependency entirely, and avoid whatever problem this is. However, that would mean plenum would then have to run on Python 3.6 or later. 
{code:java}
Python 3.5.2 (v3.5.2:4def2a2901a5, Jun 26 2016, 10:47:25)

[GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import _sha3

>>> dir(_sha3)

['__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__version__', 'sha3']

>>> # contains 'sha3' above, which is what is required

>>> callable = _sha3.sha3()

>>> dir(callable)

['__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', 'copy', 'digest', 'init', 'squeeze', 'update']{code}
{code:java}
Python 3.6.5 (default, Apr 25 2018, 14:23:58)

[GCC 4.2.1 Compatible Apple LLVM 9.1.0 (clang-902.0.39.1)] on darwin

Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import _sha3

>>> dir(_sha3)

['__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'implementation', 'keccakopt', 'sha3_224', 'sha3_256', 'sha3_384', 'sha3_512', 'shake_128', 'shake_256']

>>> # directory no longer contains 'sha3'

>>> callable = _sha3.sha3()

Traceback (most recent call last):

  File ""<stdin>"", line 1, in <module>

AttributeError: module '_sha3' has no attribute 'sha3'{code};;;","20/Jun/18 12:07 AM;RyanWest;This problem has been solved but has not yet been accepted as a PR.;;;","21/Jun/18 5:29 AM;stevetolman;[~RyanWest] What's the PR# for this issue?;;;","21/Jun/18 5:32 AM;RyanWest;h1. #714 https://github.com/hyperledger/indy-plenum/pull/714;;;","28/Jun/18 10:55 PM;stevetolman;[~RyanWest] it looks like the PR is awaiting your input.;;;","18/Oct/18 9:00 AM;swcurran;[~resplin] - it would be nice to see the Python 3.6 solution pushed out. ;;;","09/Aug/19 3:39 PM;ashcherbakov;Python 3.6 fixes have been implemented in the scope of [https://github.com/hyperledger/indy-plenum/pull/957.]
CI/CD support for Python 3.6 will come with Ubuntu 18.04 support (INDY-2187);;;",,,,,,,,,,,,,,,,,
"Following catchup, indy on new validator crashes during primary selection",INDY-1309,29907,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,mgbailey,mgbailey,03/May/18 6:43 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"The  indy process on the a new steward's validator node, which is in preparation for the live Sovrin network, is crashing after the catchup process is complete.  This is very easy to see in the attached files. The tar contains 2 files.  error.log contains the stack trace from journalctl.  VeridiumID.log is the indy log.  From my observation of the logs, it appears the error may be during primary selection, following the catchup of the ledgers.  Please compare the 16:16:15 time hacks in each file.

The attempt shown is to bring the node onto the STN.","The STN, running 1.3.57 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/18 11:21 PM;mgbailey;STN_pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/14949/STN_pool_ledger.txt","03/May/18 6:43 AM;mgbailey;logs.tgz;https://jira.hyperledger.org/secure/attachment/14937/logs.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz9x3:",,,,,,EV 18.09 Stability-RocksDB,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,Derashe,mgbailey,,,,,,,,,,,"05/May/18 2:24 AM;mgbailey;We have seen this in another node now. We also do not see it in others. Where we have seen it occur, the node did not come up cleanly on the first try, and it was necessary to remove the data directory to retry bringing up the node.;;;","07/May/18 10:35 PM;Derashe;[~mgbailey] Hello, can i ask you a few things:
 * Was there any promote/demote operations with any nodes? Particullary, how was Veridium node added?
 * Can i get full journalctl from Veridium node?
 * What verison of node do you use?;;;","07/May/18 11:21 PM;mgbailey;[~Derashe], There are many promotions and demotions on this network (the STN).  This node was added by a trustee putting steward credentials onto the STN, and then the steward putting his node attributes onto the pool ledger. [^STN_pool_ledger.txt]

I could request full journalctl, however I don't believe that this would help.  I was on with the steward in a screen share, and there was no other information in the journalclt of use.

This is indy-node 1.3.57;;;","08/May/18 11:15 PM;mgbailey;[~Derashe], the contents of indy.env are:
{code:java}
NODE_NAME=VeridiumID
NODE_PORT=9701
NODE_CLIENT_PORT=9702
CLIENT_CONNECTIONS_LIMIT=15360
{code}
 ;;;","09/May/18 1:08 AM;mgbailey;[~Derashe], as you pointed out, the alias in indy.env does not match the alias posted to the pool ledger. Could this explain the error we are seeing?;;;","09/May/18 2:55 AM;Derashe;[~mgbailey] That error may happen, when txn of addition of new node was written to ledger and this node trying to connect with pool, and it's local node's name differs from one was written to ledger. During the process of primary selection, node looks for primary rank for it's instances, but because of names mismatch, get_rank_by_name function may return None value, as mentioned in get_rank_of function. So this cause TypeError.

P.S: Mentioned functions contains in TxnPoolManager class ;;;","11/May/18 11:08 PM;mgbailey;[~Derashe], good catch.  The problem is resolved when the alias is changed to match what is in the ledger. This ticket is resolved.;;;",,,,,,,,,,,,,,,,,,
The /var/log/indy/validator-info.log is inappropriately owned by root,INDY-1310,29908,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,mgbailey,mgbailey,03/May/18 7:19 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,validator-info,,,0,help-wanted,TShirt_S,,"This file should be owned by indy, like everything else in this directory.  Failure to do this needlessly complicates admin tasks, and especially scripts that use this tool.",1.3.57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzczz:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,anikitinDSR,mgbailey,VladimirWork,,,,,,,,,,"29/May/18 5:57 PM;anikitinDSR;Reasons:
- need to fix owner for validator-info log's file

Changes:
- log file creattion with indy:indy as owner
- if log file already exist, then try to chown to indy:indy

Versions:
indy-node 1.3.430

PRs:
https://github.com/hyperledger/indy-node/pull/728

Steps to validate:
- install indy-node
1.1 run validator-info under `root` and check, that file /var/log/indy/validator-info.log is owned by indy:indy
2.1 chown of log file to root:root
2.2 run validator-info under indy user and check, that validator-info print to output string like:
`Cannot set owner of /var/log/indy/validator-info.log file to indy
The owner of /var/log/indy/validator-info.log must be indy:indy`
;;;","31/May/18 11:25 PM;VladimirWork;Build Info:
indy-node 1.3.435

Steps to Validate:
1. Install pool.
2. Run validator-info under `root` and check, that file /var/log/indy/validator-info.log is owned by indy:indy
3. `chown` log file to root:root.
4. Run validator-info under indy user and check, that validator-info print to output string like:
`Cannot set owner of /var/log/indy/validator-info.log file to indy
The owner of /var/log/indy/validator-info.log must be indy:indy`

Actual Results:
Log file has a correct owner. Tool throws correct error if log file has an incorrect owner.;;;",,,,,,,,,,,,,,,,,,,,,,,
Decrease amount of logging with INFO level,INDY-1311,29918,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ashcherbakov,dsurnin,dsurnin,03/May/18 9:39 PM,09/Oct/19 7:02 PM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"During the tests each node from the pool of 25 nodes generates about 70 Gb logs per 3 days.
 100 Mb log files are rotated each 8 minutes.

Acceptance criteria:
 * Changes of messages and log levels will be done in the scope of INDY-1416
 * Reduce message sizes by other means:
 ** get rid of class and method name
 ** use one letter for message level
 ** consider removing of prefixes for Replicas",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1416,,,,,,,INDY-724,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzzhpr:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,dsurnin,krw910,mgbailey,spivachuk,,,,,,,,"04/May/18 1:04 AM;ashcherbakov;Some ideas how we can reduce INFO log sizes:
* Do not log positive cases (if txn was ordered successfully, then most probably there is no sense to log all details for 3PC)
* Extend logging in negative cases: if txn was failed to be ordered (see INDY-1274), then we need to extend logging to provide more info. 
We can just dump the current state, or use a circular buffer with more detailed state for this purpose
* Use separate log files for detailed logs and for warning/error logs, so that warning/error logs are rotated less often.
* Reduce logging on backup instances
* Do not log in files, but rather in some indexed DBs ;;;","04/May/18 1:42 AM;spivachuk;It makes sense to write logs to 2 different destinations in parallel with different log levels (for example, WARNING and INFO) and correspondingly with different rotation frequencies. With this approach we would have, for example, a week-long history in the WARNING log that would be sufficient for stewards not to lose sight of errors and warnings and an hour-long history in the INFO log that would give stewards a chance to get also a detailed prehistory of detected warnings and errors in case they occurred not longer than an hour ago.;;;","12/May/18 12:33 AM;krw910;[~ashcherbakov] [~spivachuk] You need to check in with [~tharmon] and [~mgbailey] when considering changes to the logging. They rely heavily on the information provided by the logs. Do we understand why we use so much memory? It seems that there is more to the issue than just the amount being logged at INFO level.
;;;","12/May/18 12:53 AM;mgbailey;I agree with [~krw910], it would appear that something beyond the amount written to the log is influencing performance when logging is on at the info level.

I do not believe that the ticket description is accurate for logs at the INFO level, which is much more parsimonious than TRACE.  Nonetheless, some of the suggestions here are valid. For successful posts, the INFO logging can be confined to two events: the transaction received from the client for processing, and the transaction being successfully added to the ledger.

Please do not dump the state in INFO in a form that is not easily consumable by an admin. Unless you convince me otherwise, I still want data in a form like what is described in INDY-1274.

I don't understand the two log suggestion at all. This will not reduce the burden of logging on the system, which I think is the primary goal of this ticket.;;;","12/May/18 1:39 AM;ashcherbakov;[~mgbailey] [~krw910]
The ticket description is accurate. We really have Gigabytes of logs even on INFO level under the high load.
And the cause of this is that we log on INFO level every request that comes (there can be even multiple messages for one request).
So, even on INFO level we are logging quite a lot, and this is I/O operations, so this affects both Performance and Stability (introducing more random delays in the pool).

One of the ideas is to not log too much in case of positive events (for example, if we successfully ordered 100 txns, then it doesn't make sense to have a line in the log for each request, maybe this is enough to log just once that we ordered 100 txns successfully). It doesn't mean that we should not output data in case on errors or when something goes wrong. So, I believe INDY-1274 is still valid and doesn't contradict with this issue. Moreover, I think we can extend some troubleshooting information even more in some of the cases.
;;;","02/Jul/18 9:52 PM;ashcherbakov;Duplicates INDY-1416;;;","05/Jul/18 12:38 AM;dsurnin;* time format changed to yymmddHHMMSSFFF
* log level is now represented with 1 letter
* REPLICA: prefix was removed
* file name and function name were removed

in test environment these changes reduced log file size from almost 40M to almost 33M

https://github.com/hyperledger/indy-plenum/pull/794;;;","11/Jul/18 5:40 PM;ashcherbakov;It was decided that we don't change time format, log level, and keep file name.
The only changes that were done is getting rid of REPLICA prefix and function name.;;;",,,,,,,,,,,,,,,,,
"Backend: ""Internal server error"" is displayed for more than 5 concurrent user",INDY-1312,29922,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,,viswa0269,viswa0269,03/May/18 10:54 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Unable to handle 5 concurrent user for connect enterprise call
Logs for 6 users:
sucessfull post:200 Code response
sucessfull post:200 Code response
sucessfull post:200 Code response
sucessfull post:200 Code response
Error when posting There was an internal server error.
Error when posting There was an internal server error.

Env:[https://enym-eagency.pdev.evernym.com/]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz8nb:",,,,,,,,,,,,,,,,,,,,,,,,,,viswa0269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support PBFT Strategy for View Change,INDY-1313,29949,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,sergey.khoroshavin,ashcherbakov,ashcherbakov,04/May/18 10:21 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Support PBFT Strategy for View Change. We need to be able to turn it on or off.

PBFT View Change implementation will be done in another task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1314,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwhz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"17/May/18 10:10 PM;ashcherbakov;Replaced by INDY-1335 - INDY-1340;;;",,,,,,,,,,,,,,,,,,,,,,,,
Implement PBFT View Change,INDY-1314,29950,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,04/May/18 10:25 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Implement PBFT View Change in the scope of the strategy defined in INDY-1313.
Take into account Design from INDY-1290",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwi7:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"17/May/18 10:17 PM;ashcherbakov;Will be replaced by INDY-1335 - INDY-1340.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Pool stopped writing after ~300,000 txns from 5 clients.",INDY-1315,29968,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,dsurnin,ozheregelya,ozheregelya,05/May/18 4:16 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,TShirt_L,,,"Environment:
 indy-node 1.3.364
 view change - on 
 logLevel - info

Steps to Reproduce:
 1. Setup the pool of 25 nodes.
 2. Run the load test from 5 clients.

Actual Result:
 One of nodes was lagged on 291,395, the rest ones have wrote 307,554 and have stopped writing.

Logs were shared with [~dsurnin], archives names: view_change_info_log_pool_crashed_5_clients_p1.7z, view_change_info_log_pool_crashed_5_clients_p2.7z view_change_info_log_pool_crashed_5_clients_p3.7z

 

UPD: The issue was reproduced with load test from 1 client. Logs shared with [~dsurnin], archives names:
 low_load_1client_stopped_writting_p1.7z
 low_load_1client_stopped_writting_1315_p2.7z
 low_load_1client_stopped_writting_1315_p3.7z

 

Acceptance Criteria:

* Find the cause, determine a Plan of Attack, and log the new task / epic.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1403,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz6bz:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,ozheregelya,,,,,,,,,,"14/May/18 8:52 PM;ashcherbakov;We need to make sure if the issue is caused by View Change. If so, it will be fixed together with INDY-1341 and INDY-1350.;;;","08/Jun/18 10:08 PM;dsurnin;One node starts catchup and sending catcup reqs to other nodes. Receives some replies from fast nodes, writes some txns and waits for the rest replies. As long as timeout expires node regenerates catchup reqs but with updated staring seqNo. After that node receives replies from fast nodes and responses from slow nodes for the previous catchup request.
As a result node has several catchup responses with overlapping sqeNo intervals. See [INDY-1403|https://jira.hyperledger.org/browse/INDY-1403];;;",,,,,,,,,,,,,,,,,,,,,,,
Unhandled exception during node working,INDY-1316,29996,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,zhigunenko.dsr,zhigunenko.dsr,07/May/18 5:43 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Size: S (PoA + fix)

During load test 16 from 25 nodes had caught these exception at the same time. Pool stopped to writing.

{code}
Traceback (most recent call last):
File ""/usr/local/bin/start_indy_node"", line 17, in <module>
  run_node(config, self_name, int(sys.argv[2]), int(sys.argv[3]))
File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 57, in run_node
  looper.run()
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 261, in run
  return self.loop.run_until_complete(what)
File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
  return future.result()
File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
  raise self._exception
File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
  result = coro.send(None)
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 224, in runForever
  await self.runOnceNicely()
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 207, in runOnceNicely
  msgsProcessed = await self.prodAllOnce()
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
  s += await n.prod(limit)
File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 293, in prod
  c = await super().prod(limit)
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 989, in prod
  c += await self.serviceNodeMsgs(limit)
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1023, in serviceNodeMsgs
  await self.processNodeInBox()
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1599, in processNodeInBox
  await self.nodeMsgRouter.handle(m)
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
  res = self.handleSync(msg)
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
  return self.getFunc(msg[0])(*msg)
File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 471, in processCatchupReq
  cons_proof = self._make_consistency_proof(ledger, end, req.catchupTill)
File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 488, in _make_consistency_proof
  proof = ledger.tree.consistency_proof(end, catchup_till)
File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 216, in consistency_proof
  self._subproof(first, 0, second, True)]
File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 215, in <listcomp>
  return [self.merkle_tree_hash(a, b) for a, b in
File ""/usr/local/lib/python3.5/dist-packages/ledger/compact_merkle_tree.py"", line 201, in merkle_tree_hash
  raise ValueError(""end must be greater than start"")
{code}

[Logs|https://drive.google.com/open?id=1PQUXYCzqrFVl7eVKYwlly9dMQZA2Og7i]","indy-anoncreds 1.0.32
indy-node 1.3.396
indy-plenum 1.2.342
libindy-crypto 0.4.0
python3-indy-crypto 0.4.0",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/18 6:54 PM;zhigunenko.dsr;journalctl.log;https://jira.hyperledger.org/secure/attachment/14940/journalctl.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzz6c7:",,,,,,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,,ozheregelya,Toktar,zhigunenko.dsr,,,,,,,,,,"17/May/18 5:19 PM;Toktar;PR with bugfix: https://github.com/hyperledger/indy-plenum/pull/680;;;","21/May/18 10:14 PM;Toktar;*Problem reason:*
 * During load test a lot of nodes throw compact_merkle_tree exceptions

*Changes:*
 * Added check to ""end"" greater then ""catchupTill"" in processCatchupReq

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/680]

*Version:*
 * indy-node 1.3.418-master
 * indy-plenum 1.2.363-master

*Risk factors:*
 * problem with discard CatchupReq messages

*Risk:*
 * Low

*Covered with tests:*
 * [test_incorrect_catchup_request.py|https://github.com/hyperledger/indy-plenum/pull/680/files#diff-a38ef1338fb7c374c13371b093506c58];;;","25/May/18 7:57 AM;ozheregelya;Environment:
indy-node=1.3.425

Steps to Validate:
1. Setup the pool.
2. Run load tests.

Actual Results:
Pool works without exceptions in logs and journalctl.;;;",,,,,,,,,,,,,,,,,,,,,,
read_ledger failure,INDY-1317,30012,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,danielhardman,smithbk,smithbk,08/May/18 2:22 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,libsovrin,,,0,,,,"When trying to read the ledger, I am getting the following error.

root@validator-5f89c9d84-qjpwr:/var/log/indy/sandbox# read_ledger --type domain
Traceback (most recent call last):
  File ""/usr/local/bin/read_ledger"", line 177, in <module>
    ledger = get_ledger(args.type, ledger_data_dir)
  File ""/usr/local/bin/read_ledger"", line 98, in get_ledger
    hash_store = initHashStore(ledger_data_dir, type_, config, read_only=True)
  File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 54, in initHashStore
    read_only=read_only)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/db_hash_store.py"", line 22, in __init__
    self.open()
  File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/db_hash_store.py"", line 90, in open
    self.db_type, self.dataDir, self.nodes_db_name, read_only=self._read_only)
  File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 24, in initKeyValueStorage
    return KeyValueStorageRocksdb(dataLocation, keyValueStorageName, open, read_only)
  File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 23, in __init__
    self.open()
  File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 28, in open
    self._db = rocksdb.DB(self._db_path, opts, read_only=self._read_only)
  File ""rocksdb/_rocksdb.pyx"", line 1437, in rocksdb._rocksdb.DB.__cinit__
  File ""rocksdb/_rocksdb.pyx"", line 84, in rocksdb._rocksdb.check_status
rocksdb.errors.RocksIOError: b'IO error: While opening a file for sequentially reading: /var/lib/indy/sandbox/data/ibmTest/domain_merkleNodes/CURRENT: No such file or directory'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz9wv:",,,,,,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,esplinr,mgbailey,nage,ozheregelya,smithbk,,,,,,"08/May/18 2:40 AM;mgbailey;This is running indy code from the current master branch of indy-node. The /var/lib/indy directory is an nfs mount.;;;","08/May/18 3:14 AM;nage;It looks like perhaps RocksDB has an issue running over an NFS mount?;;;","08/May/18 4:16 AM;smithbk;I googled and didn't find any issues with rocksdb on NFS unless multiple processes try to read at the same time, but we have only a single process running.

I tried the following sample on the same NFS file system and didn't see an error.  If you want me to try something else, let me know.

import rocksdb
db = rocksdb.DB(""test.db"", rocksdb.Options(create_if_missing=True))
db.put(b'a', b'data')
val = db.get(b'a')
print(val)

;;;","08/May/18 5:28 AM;esplinr;[~ashcherbakov] Could this be because the migration from LevelDB to RocksDB () hasn't yet moved the tools like read_ledger and validator-info?;;;","08/May/18 2:22 PM;ashcherbakov;This is a known issues we've been working on (INDY-1289). Sorry for the inconvenience.;;;","08/May/18 11:47 PM;smithbk;Thanks, but are you sure they are the same?  The exceptions look different to me.;;;","08/May/18 11:50 PM;ashcherbakov;Most probably yes. We can test it after INDY-1289 is fixed.;;;","11/May/18 11:08 PM;anikitinDSR;According to [INDY-1289|https://jira.hyperledger.org/browse/INDY-1289]  this issue can be tested too.

Version:
indy-node: 1.3.411;;;","14/May/18 8:27 PM;ozheregelya;Verified in scope of INDY-1289. Read_ledger works.;;;",,,,,,,,,,,,,,,,
--network parameter of read_ledger doesn't work,INDY-1318,30069,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ozheregelya,ozheregelya,10/May/18 1:47 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,,,,0,help-wanted,TShirt_S,,"Steps to Reproduce:
1. Setup the pool with 'sandbox' network in config and write some txns.
2. Run `sudo su - indy -c ""read_ledger --type domain --network sandbox""`.
=> Read_ledger works fine, sandbox domain ledger is shown.
3. Run `sudo su - indy -c ""read_ledger --type domain --count --network live""`.
=> Sandbox domain ledger is shown.
Expected: error should be shown.

4. Stop indy-node, rename `/var/lib/indy/sandbox` to `/var/lib/indy/live`.
5. Run `sudo su - indy -c ""read_ledger --type domain --count --network live""`.
=> 
{code:java}
root@fef6dd9bf54e:/var/lib/indy# sudo su - indy -c ""read_ledger --type domain --network live""
Traceback (most recent call last):
File ""/usr/local/bin/read_ledger"", line 161, in <module>
ledger_data_dir = get_ledger_dir(args.node_name, args.client_name, args.network)
File ""/usr/local/bin/read_ledger"", line 75, in get_ledger_dir
dirs = os.listdir(config_helper.ledger_data_dir)
FileNotFoundError: [Errno 2] No such file or directory: '/var/lib/indy/sandbox/data'
 
{code}
Actual Results:
Read_ledger works only with network specified in indy_config.py.

Expected Results:
It should work with network specified in --network parameter (ideally, value from indy_config.py should be used as default value of --network parameter if it is not specified), or --network parameter should be removed from script parameters.",indy-node 1.3.405,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1289,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzzczr:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,anikitinDSR,ozheregelya,zhigunenko.dsr,,,,,,,,,,"30/May/18 3:51 PM;anikitinDSR;Reasons:
- need to fix --network option

Changes:

- dir_to_data path building was changed

Version:
indy-node 1.3.432

PRs:

[https://github.com/hyperledger/indy-node/pull/729]

 

Steps to validate:
 * setup pool (network sandbox by default)
 * run read_ledger with `--network live. Error message must be shown
 * rename /var/lib/indy/sandbox into /var/lib/indy/live
 * run read_ledger with option `–network live`. standart out must be shown;;;","01/Jun/18 10:49 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.437

*Checked cases:*
_read_ledger --type=domain --count_ - valid
_read_ledger --type=domain --count --network=sandbox_ - valid
_read_ledger --type=domain --count --network=live_ - expected ""no such file""
_read_ledger --type=domain --count --network=/var/lib/indy/sandbox/_ - valid
_read_ledger --type domain --count --network live (for non-existed folder)_ - expected ""no such file""
_read_ledger --type domain --count --network live (for empty folder)_ - expected ""Node's 'data' folder not found"";;;",,,,,,,,,,,,,,,,,,,,,,,
Support new libindy with refactored txn format,INDY-1319,30090,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,10/May/18 5:49 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.4,,,,0,,,,"The txn format was re-factored in the scope of INDY-1123 (`txn-refactoring` branches in plenum and node).
We need to make sure that all tests pass with a new libindy supporting the new format:

https://github.com/hyperledger/indy-plenum/tree/txn-refactoring
https://github.com/hyperledger/indy-node/tree/txn-refactroing
",,,,,,,,,,IS-674,,,,,,,INDY-1348,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1374,,,,,,,,,"1|hzzd13:",,,,,,EV 18.10 Stability and VC,EV 18.11 Stability/ViewChange,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"29/May/18 6:44 PM;ashcherbakov;Changes:
 - re-factored txns to the new format separating `data` and `metadata` (request format and read reply format are not changed; write reply format is changed):
 ** [https://github.com/hyperledger/indy-node/blob/master/docs/transactions.md]
 ** [https://github.com/hyperledger/indy-node/blob/master/docs/requests.md]
 - use utility methods to access txn fields
 - use utility methods to create txns
 - migration script for the new format

PR:
 - [https://github.com/hyperledger/indy-node/pull/727]
 - [https://github.com/hyperledger/indy-plenum/pull/710]

Version:
 - 1.3.433

Risk factors:
 - ledger migration
 - base functionality

Risk:
 - Med

Covered with tests:
 - new tests in plenum/transactions, plenum/requests, plenum/ledger
 - fixes in existing tests (all tests pass)

Recommendations for QA
 - test migration
 - test base functionality;;;","29/May/18 8:07 PM;ashcherbakov;The validation will be performed in the scope of INDY-1348;;;",,,,,,,,,,,,,,,,,,,,,,,
Create basic framework for indy-agent in python,INDY-1322,30126,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dbluhm,dbluhm,dbluhm,11/May/18 8:06 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,Set up basic framework of indy-agent in python.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IA-2,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwsfj:",,,,,,,,,,,,,,3.0,,,,,,,,,,,,dbluhm,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add indy-crypto package to hold list,INDY-1323,30142,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,12/May/18 1:31 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,help-wanted,,,We should add indy-crypto package to hold list in our code the same way as the other indy packages to avoid its upgrade.,,,,,,,,,,,,,,,,,,,,,,,,INDY-1710,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzd07:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,2.0,,,,,,,,,,,,anikitinDSR,esplinr,krw910,VladimirWork,,,,,,,,,"12/May/18 1:38 AM;krw910;[~resplin] We need to push this ticket as soon as possible to get it int the next upgrade. The issue here is if a Steward just does an upgrade on their system it will pull down the latest indy-crypto without upgrading everything else and could break the system. We added a hold to all indy packages so they would not automatically upgrade which keeps everyone at the same version.
;;;","12/May/18 1:39 AM;VladimirWork;We should add indy-crypto to this list in the next stable because upgrade of this package can break working capacity of indy-node.;;;","18/May/18 9:50 PM;esplinr;We evaluated this for the 1.3.a hotfix (INDY-1331), but decided it didn't need to be included in the hot-fix because we don't have a plan to release indy-crypto before the release of Indy Node 1.4, and even if we do release before a steward is upgraded, it is likely that the release will be backwards compatible and not cause a problem.

We agree that this needs to be fixed in the 1.4.0 release, but we are minimizing our effort on the hotfix.;;;","29/May/18 3:55 PM;anikitinDSR;Reasons:
- need to hold libindy-crypto package while making upgrade

Changes:
- added libindy-crypto package into list HOLD_PACKAGES for node_control_tool

Versions:
indy-node: 1.3.429

PRs:
https://github.com/hyperledger/indy-node/pull/726

Steps to validate:
- install indy-node 1.3.429 and stop node-control-tool
- manually install libindy-crypto with lower version (0.3.0 for example)
- start indy-node with node-control-tool
- schedule upgrade of indy-node
- check, that version of libindy-crypto was not changed;;;","31/May/18 9:51 PM;VladimirWork;Build Info:
indy-node 1.3.429

Steps to Validate:
1. Install indy-node 1.3.429 and stop node-control-tool.
2. Manually install libindy-crypto 0.2.0.
3. Start indy-node with node-control-tool.
4. Schedule upgrade of indy-node to 1.3.430.
5. Check that version of libindy-crypto was not changed after the upgrade.
6. Run `apt update` and `apt upgrade` commands.
7. Check that libindy-crypto is in list of packages that have been kept back together with indy-node and indy-plenum.

Actual Results:
Version of libindy-crypto was not changed during the upgrade. Libindy-crypto package is in list of packages that have been kept back during manual `apt upgrade`.;;;",,,,,,,,,,,,,,,,,,,,
Fix typo error (Continues) to (Continuous) in indy documentation,INDY-1324,30147,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,rameshthoomu,rameshthoomu,rameshthoomu,12/May/18 3:56 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.4,,,,0,doc,,,"fix typo error (Continues) to (Continuous) in indy documentation

https://github.com/hyperledger/indy-node/pull/692 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzd1r:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,rameshthoomu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ledger.build_schema_request requires extra params,INDY-1325,30159,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,trthhrtz,trthhrtz,14/May/18 2:21 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,," 
{code:java}
# 1.
print_log('\n1. Creates a new local pool ledger configuration that is used '
          'later when connecting to ledger.\n')
pool_config = json.dumps({'genesis_txn': genesis_file_path})
try:
    await pool.create_pool_ledger_config(config_name=pool_name, config=pool_config)
except Exception:
    await pool.delete_pool_ledger_config(config_name=pool_name)
    await pool.create_pool_ledger_config(config_name=pool_name, config=pool_config)
# 2.
print_log('\n2. Open pool ledger and get handle from libindy\n')
pool_handle = await pool.open_pool_ledger(config_name=pool_name, config=None)

# 3.
print_log('\n3. Creating new secure wallet\n')
try:
    await wallet.create_wallet(pool_name, wallet_name, None, None, None)
except:
    await wallet.delete_wallet(wallet_name, None)
    await wallet.create_wallet(pool_name, wallet_name, None, None, None)

# 4.
print_log('\n4. Open wallet and get handle from libindy\n')
wallet_handle = await wallet.open_wallet(wallet_name, None, None)

# 5.
print_log('\n5. Generating and storing steward DID and verkey\n')
steward_seed = '000000000000000000000000Steward1'
did_json = json.dumps({'seed': steward_seed})
steward_did, steward_verkey = await signus.create_and_store_my_did(wallet_handle, did_json)
print_log('Steward DID: ', steward_did)
print_log('Steward Verkey: ', steward_verkey)

# 6.
print_log('\n6. Generating and storing trust anchor DID and verkey\n')
trust_anchor_did, trust_anchor_verkey = await signus.create_and_store_my_did(wallet_handle, ""{}"")
print_log('Trust anchor DID: ', trust_anchor_did)
print_log('Trust anchor Verkey: ', trust_anchor_verkey)

# 7.
print_log('\n7. Building NYM request to add Trust Anchor to the ledger\n')
nym_transaction_request = await ledger.build_nym_request(submitter_did=steward_did,
                                                         target_did=trust_anchor_did,
                                                         ver_key=trust_anchor_verkey,
                                                         alias=None,
                                                         role='TRUST_ANCHOR')
print_log('NYM transaction request: ')
pprint.pprint(json.loads(nym_transaction_request))

# 8.
print_log('\n8. Sending NYM request to the ledger\n')
nym_transaction_response = await ledger.sign_and_submit_request(pool_handle=pool_handle,
                                                                wallet_handle=wallet_handle,
                                                                submitter_did=steward_did,
                                                                request_json=nym_transaction_request)
print_log('NYM transaction response: ')
pprint.pprint(json.loads(nym_transaction_response))

# 9.
print_log('\n9. Build the SCHEMA request to add new schema to the ledger as a Steward\n')
seq_no = 1
schema = {
    'seqNo': seq_no,
    'dest': steward_did,
    'data': {
        'name': 'gvt',
        'version': '1.0',
        'attr_names': ['age', 'sex', 'height', 'name']
    }
}
schema_data = schema['data']
print_log('Schema data: ')
pprint.pprint(schema_data)
print_log('Schema: ')
pprint.pprint(schema)
schema_request = await ledger.build_schema_request(steward_did, json.dumps(schema_data))
{code}
The last line will raise
{code:java}
_indy_loop_callback: Function returned error 113
Error occurred: ErrorCode.CommonInvalidStructure
{code}
However, if you substitute the schema to: 
{code:java}
schema = {
    'seqNo': seq_no,
    'dest': steward_did,
    'data': {
        ""id"": ""1"",
        ""name"": ""gvt"",
        ""version"": ""1.0"",
        ""attrNames"": [""age"", ""sex"", ""height"", ""name""],
        ""ver"": ""1.0""
    }
}
{code}
It works.

According to the docs (https://github.com/hyperledger/indy-node/blob/master/docs/transactions.md#schema[)|https://github.com/hyperledger/indy-node/blob/master/docs/transactions.md)] it should be the other way around.

 

Full code is available here [https://github.com/hyperledger/indy-sdk/blob/master/doc/how-tos/save-schema-and-cred-def/python/write_schema_and_cred_def.py]

 

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwyav:",,,,,,,,,,,,,,,,,,,,,,,,,,trthhrtz,,,,,,,,,,,,"15/May/18 2:50 AM;trthhrtz;Fixed with python3-indy-1.4.0 release;;;",,,,,,,,,,,,,,,,,,,,,,,,
Remove Checkpoints,INDY-1326,30161,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,ashcherbakov,lovesh,lovesh,14/May/18 9:00 AM,08/Oct/19 9:09 PM,28/Oct/23 2:47 AM,08/Oct/19 9:09 PM,,,,,,0,,,,"PBFT requires checkpoints as a mechanism that is triggered periodically to make sure every node has processed (and persisted) the same requests (across batches) in the same order.
We do not require checkpoints since Pre-Prepare contains expected merkle roots (txn and state merkle tr(i/e)e roots after applying the requests of the PrePrepare) and a Pre-Prepare can be only processed if the previous Pre-Prepare was received and processed successfully (not committed necessarily).
Thus it cannot happen that a node skips processing any PrePrepare.
To handle the case where a node gets a Pre-Prepare(s) and maybe Prepares too but is not getting sufficient Commits, a node can use `MESSAGE_REQUEST` to request missing 3 phase messages or start catchup once it has seen `n` Pre-Prepares (and maybe some Prepares too) but not been able to order them or it has >2f COMMIT(s) for a `pp_seq_no` that is greater than its last ordered sequence number by `n` 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1118,,,,,,,,,"1|hzwy9z:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,lovesh,,,,,,,,,,"29/May/18 5:34 PM;ashcherbakov;I believe we need Checkpoints because of the following:
 * View Change (PBFT View Change which we will implement eventually) is based on checkpoints
 * garbage collection is based on checkpoints
 * Catch-up start is based on checkpoints;;;","08/Oct/19 9:09 PM;esplinr;The checkpoints are currently required for the PBFT protocol. With enough work, we suspect that the checkpoints can be removed, but this isn't a priority for us.;;;",,,,,,,,,,,,,,,,,,,,,,,
`ReqIdrToTxn` does not store information about the ledger,INDY-1327,30162,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,lovesh,lovesh,14/May/18 2:06 PM,13/Jun/19 6:10 PM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,"Currently {{ReqIdrToTxn}} stores a map of {{sha256(identifier||req_id) -> txn seq_no}}. It should store which ledger the seq no belongs to, this can be done by changing the map to {{sha256(identifier||req_id) -> ledger_id<delimiter>txn seq_no}}

It also makes sense to take into account requests' digest, not only reqId.

We also need a migration script for seqNoDb.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcxj:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,5.0,,,,,,,,,,,,lovesh,Toktar,zhigunenko.dsr,,,,,,,,,,"25/May/18 12:48 AM;Toktar;We suggest a new ReqIdrToTxn format digest -> ledger_id<delimiter>txn seq_no. Adding digest will be more useful for a new View change because we will not need to find seq_no in every ledger. And we don't want to remove ledger_id because this will create a need to find a digest for each View Change. In this case, we lose a bit of disk space, but we get better performance.

Migration way. We have sha256(identifier||req_id) -> txn seq_no and it's not enough for value migration from txn seq_no to ledger_id<delimiter>txn seq_no because we haven't any storage with req_id. If we will store (digest -> ledger_id<delimiter>txn seq_no) then we can remove current ReqIdrToTxn and create a new version based on ledger transaction log.;;;","01/Jun/18 6:25 PM;Toktar;PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/712]
 * [https://github.com/hyperledger/indy-node/pull/743];;;","07/Jun/18 1:16 AM;Toktar;*Problem reason:*
 * If one client send two requests with same reqId and DID, pool answer for second request as for first because uses cash from  ReqIdrToTxn by key (reqId, DID)

*Changes:*
 * Changed ReqIdrToTxn key to digest = hash(reqId, DID, operation), that excludes erroneous search by key
 * Added migration script for SeqNoDb
 * Added digest to transaction metadata (change converter of request to transaction ) 

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/712]
 * [https://github.com/hyperledger/indy-node/pull/743]

*Version:*
 * indy-node 1.3.445-master
 * indy-plenum 1.2.377-master

*Risk factors:*
 * The transaction sent twice, with the same reqId will be written twice in the ledger.

*Risk:*
 * Low

*Covered with tests:*
 * test_get_protocol_version - [test_txn_general_access_utils.py|https://github.com/hyperledger/indy-plenum/pull/727/files#diff-2356c08903aa87cd57b7387f30e84c22]
 * [test_req_idr_to_txn.py|https://github.com/hyperledger/indy-plenum/pull/712/files#diff-f58a73496209fe3d4c4894782a950118]
 * test_get_digest - [test_txn_general_access_utils.py|https://github.com/hyperledger/indy-plenum/pull/712/files#diff-2356c08903aa87cd57b7387f30e84c22];;;","07/Jun/18 7:26 PM;zhigunenko.dsr;*Steps to check:*
1) create pool with indy-node=1.3.428
2) write 10k nyms
3) upgrade pool to 1.3.446
4) write transactions
{code}
ledger custom {""reqId"":1,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""1"",""dest"":""V4SGRU86Z58d6TV7PBU111""},""protocolVersion"":1} sign=true
ledger custom {""reqId"":1,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""105"",""dest"":""V4SGRU86Z58d6TV7PBU111""},""protocolVersion"":1} sign=true
{code}

*Actual results:*
Outputs are different

*Environment:*
indy-cli 1.4.0~554
indy-node 1.3.446
indy-plenum 1.2.389
libindy 1.4.0~554;;;",,,,,,,,,,,,,,,,,,,,,
Resolve the issue with ordering till the prepared certificate during view change,INDY-1328,30165,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,spivachuk,spivachuk,14/May/18 4:36 PM,21/May/19 10:58 PM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.9.0,,,,0,TShirt_L,,,"Discarding PrePrepares during catch-up may result in a situation where a replica will not be able to order till the prepared certificate during view change if some PrePrepares have not been received (async messaging out of order, or lost).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwy5b:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,spivachuk,,,,,,,,,,,"13/Feb/19 9:55 PM;ashcherbakov;Fixed in the scope of INDY-1876;;;",,,,,,,,,,,,,,,,,,,,,,,,
Add short checkpoints stabilization without matching digests,INDY-1329,30169,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,spivachuk,spivachuk,15/May/18 12:53 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"Now a short checkpoint which is created after catch-up cannot be stabilized because it has a not even lower bound and does not have an aggregated digest and so cannot be matched with checkpoint messages from other replicas in the protocol instance.

The next checkpoint can be stabilized. However, the threshold for stashed generations of checkpoint messages from other replicas is 2. So, if a replica lags even for one 3PC-batch at the end of the next checkpoint after the short one, there is a risk that it will gather the quorum of checkpoint messages from other replicas before it completes its own checkpoint and thus an undesired catch-up will be triggered.

To avoid such undesired triggering of catch-up, we would stabilize short checkpoints using only their upper bound for matching with checkpoint messages from other replicas and without matching digests. Such the logic of short checkpoints stabilization must be added in scope of this ticket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzzczb:",,,,,,EV 18.10 Stability and VC,EV 18.11 Stability/ViewChange,,,,,,,3.0,,,,,,,,,,,,spivachuk,VladimirWork,,,,,,,,,,,"24/May/18 3:26 AM;spivachuk;After a discussion with [~ashcherbakov], we decided not to stabilize short own checkpoints because it is not completely correct to name a checkpoint stable when we don't check its digest. Instead we will not consider a quorumed stashed checkpoint corresponding by the upper bound to a completed short own checkpoint when evaluating the size of the replica lag for catch-up.;;;","28/May/18 10:31 PM;spivachuk;*Problem reason:*
- Short own checkpoints were never stabilized, so received checkpoints with the same upper bound were kept in the stash. This might lead to a redundant catch-up because the threshold of a lag in stashed checkpoints which causes catch-up was reached earlier than it should in case the short own checkpoint had already been completed and the next checkpoint had not been completed yet.
- Stashed received checkpoints were not removed after catch-up. So they continue to be considered in the lag for catch-up and thus a redundant catch-ups may be triggered further while the node is being in the same view.

*Changes:*
- Added logic of excluding a complete short checkpoint from the lag for catch-up. Added integration tests for this logic.
- Added removal of own and stashed received checkpoints on non-primary replicas after catch-up. Added unit tests for this logic.
- Added removal of previous stashed received checkpoints on own checkpoint stabilization. Enhanced {{test_second_checkpoint_after_catchup_can_be_stabilized}} to verify this logic.
- Simplified code for removal of stashed checkpoints.
- Rolled back the recent changes in garbage collection since they had turned out to be not completely correct.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/690
- https://github.com/hyperledger/indy-plenum/pull/704
- https://github.com/hyperledger/indy-node/pull/714
- https://github.com/hyperledger/indy-node/pull/724

*Version:*
- indy-node 1.3.428-master
- indy-plenum 1.2.375-master

*Risk factors:*
- The moment of triggering a catch-up on a lag of the master replica occurred right after another catch-up in the same view.
- The moment of triggering a catch-up on a lag of the master replica in case another catch-up was performed some time ago in the same view.

*Risk:*
- Low

*Covered with tests:*
- {{test_node_catchup_after_checkpoints}}
- {{test_lag_size_for_catchup}}
- {{test_complete_short_checkpoint_not_included_in_lag_for_catchup}}
- {{test_incomplete_short_checkpoint_included_in_lag_for_catchup}}
- {{test_second_checkpoint_after_catchup_can_be_stabilized}}
- {{test_checkpoints_removed_on_master_non_primary_replica_after_catchup}}
- {{test_checkpoints_removed_on_backup_non_primary_replica_after_catchup}}
- {{test_checkpoints_not_removed_on_backup_primary_replica_after_catchup}};;;","01/Jun/18 10:34 PM;VladimirWork;Cannot run load test to check large catchups because of IS-740.;;;","05/Jun/18 1:12 AM;VladimirWork;Build Info:
indy-node 1.3.439

Steps to Validate:
1. Install pool.
2. Check catchup for 1..f nodes disconnected with
2.1. 20..25.000 txns in ledger.
2.2. More than 1 catchup in one view.
2.3. Catchup under load.

Actual Results:
All nodes have the same amount of txns in all ledgers after all cases.;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade from 1.2.223 (1.3.55 stable analogue) to 1.3.410 (rocksdb) doesn't work,INDY-1330,30170,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,15/May/18 12:55 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"Size: M (PoA + fix)

Build Info:
python3-base58=0.2.4
libindy-crypto=0.1.6
python3-indy-crypto=0.1.6
indy-plenum=1.2.180
indy-node=1.2.223

Steps to Reproduce:
1. Install 1.2.223 pool (1.3.55 stable analogue).
2. Add ~10 NYMs to ledger.
3. Upgrade pool to 1.3.410.
4. Try to add NYM to ledger.
5. Check indy-node version \ status and journalctl at all nodes.

Actual Results:
Pool is upgraded successfully but doesn't write NYMs and all nodes restart spontaneously each ~5 seconds. There is an error in journalctl:

{noformat}
May 14 14:09:04 9b1cb6eeb215 env[9129]: 2018-05-14 14:09:04,956 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: git
May 14 14:09:05 9b1cb6eeb215 env[9129]: 2018-05-14 14:09:05,754 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: hg
May 14 14:09:06 9b1cb6eeb215 env[9129]: 2018-05-14 14:09:06,711 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: svn
May 14 14:09:06 9b1cb6eeb215 env[9129]: 2018-05-14 14:09:06,712 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: bzr
May 14 14:09:07 9b1cb6eeb215 env[9130]: 2018-05-14 14:09:07,283 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: git
May 14 14:09:08 9b1cb6eeb215 env[9130]: 2018-05-14 14:09:08,104 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: hg
May 14 14:09:09 9b1cb6eeb215 env[9130]: 2018-05-14 14:09:09,021 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: svn
May 14 14:09:09 9b1cb6eeb215 env[9130]: 2018-05-14 14:09:09,022 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: bzr
May 14 14:09:09 9b1cb6eeb215 env[9129]: 2018-05-14 14:09:09,479 | INFO     | node_control_tool.py (  78) | __init__ | Node control tool is starting up on localhost port 30003
May 14 14:09:21 9b1cb6eeb215 env[9129]: E: Unable to locate package
May 14 14:09:21 9b1cb6eeb215 env[9129]: Traceback (most recent call last):
May 14 14:09:21 9b1cb6eeb215 env[9129]:   File ""/usr/local/bin/start_node_control_tool"", line 22, in <module>
May 14 14:09:21 9b1cb6eeb215 env[9129]:     nodeControlTool.start()
May 14 14:09:21 9b1cb6eeb215 env[9129]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 289, in start
May 14 14:09:21 9b1cb6eeb215 env[9129]:     self._hold_packages()
May 14 14:09:21 9b1cb6eeb215 env[9129]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 107, in _hold_packages
May 14 14:09:21 9b1cb6eeb215 env[9129]:     ret = self._run_shell_command(cmd)
May 14 14:09:21 9b1cb6eeb215 env[9129]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 280, in _run_shell_command
May 14 14:09:21 9b1cb6eeb215 env[9129]:     timeout=TIMEOUT)
May 14 14:09:21 9b1cb6eeb215 env[9129]:   File ""/usr/lib/python3.5/subprocess.py"", line 708, in run
May 14 14:09:21 9b1cb6eeb215 env[9129]:     output=stdout, stderr=stderr)
May 14 14:09:21 9b1cb6eeb215 env[9129]: subprocess.CalledProcessError: Command 'apt-mark hold indy-anoncreds indy-plenum indy-node python3-indy-crypto """"' returned non-zero exit status 100
May 14 14:09:22 9b1cb6eeb215 systemd[1]: indy-node-control.service: Main process exited, code=exited, status=1/FAILURE
May 14 14:09:22 9b1cb6eeb215 systemd[1]: indy-node-control.service: Unit entered failed state.
May 14 14:09:22 9b1cb6eeb215 systemd[1]: indy-node-control.service: Failed with result 'exit-code'.
root@9b1cb6eeb215:/home/indy# vim /usr/local/lib/python3.5/dist-packages/plenum/config.py
root@9b1cb6eeb215:/home/indy# journalctl -ex
May 14 14:34:01 9b1cb6eeb215 env[13849]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 107, in _hold_packages
May 14 14:34:01 9b1cb6eeb215 env[13849]:     ret = self._run_shell_command(cmd)
May 14 14:34:01 9b1cb6eeb215 env[13849]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 280, in _run_shell_command
May 14 14:34:01 9b1cb6eeb215 env[13849]:     timeout=TIMEOUT)
May 14 14:34:01 9b1cb6eeb215 env[13849]:   File ""/usr/lib/python3.5/subprocess.py"", line 708, in run
May 14 14:34:01 9b1cb6eeb215 env[13849]:     output=stdout, stderr=stderr)
May 14 14:34:01 9b1cb6eeb215 env[13849]: subprocess.CalledProcessError: Command 'apt-mark hold indy-anoncreds indy-plenum indy-node python3-indy-crypto """"' returned non-zero exit status 100
May 14 14:34:02 9b1cb6eeb215 systemd[1]: indy-node-control.service: Main process exited, code=exited, status=1/FAILURE
May 14 14:34:02 9b1cb6eeb215 systemd[1]: indy-node-control.service: Unit entered failed state.
May 14 14:34:02 9b1cb6eeb215 systemd[1]: indy-node-control.service: Failed with result 'exit-code'.
May 14 14:34:12 9b1cb6eeb215 systemd[1]: indy-node-control.service: Service hold-off time over, scheduling restart.
May 14 14:34:12 9b1cb6eeb215 systemd[1]: Stopping Indy Node...
{noformat}

Expected Results:
Pool should work normally after the upgrade.

Additional Info:
Upgrades [1.3.375 (1.3.57 stable analogue) -> 1.3.410] and [1.3.395 -> 1.3.410] perform successfully.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz9uf:",,,,,,EV 18.10 Stability and VC,,,,,,,,,,,,,,,,,,,,anikitinDSR,VladimirWork,,,,,,,,,,,"18/May/18 7:25 PM;VladimirWork;[~anikitinDSR] [~ashcherbakov] We have the similar error during upgrade from 1.3.57 (last stable) to 1.3.61 (current rc):

{noformat}
-- The start-up result is done.
May 18 10:02:30 6ed02ff77784 env[1880]: 2018-05-18 10:02:30,265 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: git
May 18 10:02:30 6ed02ff77784 env[1880]: 2018-05-18 10:02:30,316 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: hg
May 18 10:02:30 6ed02ff77784 env[1880]: 2018-05-18 10:02:30,365 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: svn
May 18 10:02:30 6ed02ff77784 env[1880]: 2018-05-18 10:02:30,366 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: bzr
May 18 10:02:30 6ed02ff77784 env[1880]: 2018-05-18 10:02:30,518 | INFO     | node_control_tool.py (  75) | __init__ | Node control tool is starting up on localhost port 30003
May 18 10:02:31 6ed02ff77784 env[1880]: E: Unable to locate package sovrin
May 18 10:02:31 6ed02ff77784 env[1880]: Traceback (most recent call last):
May 18 10:02:31 6ed02ff77784 env[1880]:   File ""/usr/local/bin/start_node_control_tool"", line 22, in <module>
May 18 10:02:31 6ed02ff77784 env[1880]:     nodeControlTool.start()
May 18 10:02:31 6ed02ff77784 env[1880]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 277, in start
May 18 10:02:31 6ed02ff77784 env[1880]:     self._hold_packages()
May 18 10:02:31 6ed02ff77784 env[1880]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 104, in _hold_packages
May 18 10:02:31 6ed02ff77784 env[1880]:     ret = self._run_shell_command(cmd)
May 18 10:02:31 6ed02ff77784 env[1880]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_control_tool.py"", line 268, in _run_shell_command
May 18 10:02:31 6ed02ff77784 env[1880]:     timeout=TIMEOUT)
May 18 10:02:31 6ed02ff77784 env[1880]:   File ""/usr/lib/python3.5/subprocess.py"", line 708, in run
May 18 10:02:31 6ed02ff77784 env[1880]:     output=stdout, stderr=stderr)
May 18 10:02:31 6ed02ff77784 env[1880]: subprocess.CalledProcessError: Command 'apt-mark hold indy-anoncreds indy-plenum indy-node python3-indy-crypto ""sovrin ""' returned non-zero exit status 100
May 18 10:02:31 6ed02ff77784 systemd[1]: indy-node-control.service: Main process exited, code=exited, status=1/FAILURE
May 18 10:02:31 6ed02ff77784 systemd[1]: indy-node-control.service: Unit entered failed state.
May 18 10:02:31 6ed02ff77784 systemd[1]: indy-node-control.service: Failed with result 'exit-code'.
May 18 10:02:31 6ed02ff77784 env[1881]: 2018-05-18 10:02:31,598 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: git
May 18 10:02:31 6ed02ff77784 env[1881]: 2018-05-18 10:02:31,653 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: hg
May 18 10:02:31 6ed02ff77784 env[1881]: 2018-05-18 10:02:31,697 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: svn
May 18 10:02:31 6ed02ff77784 env[1881]: 2018-05-18 10:02:31,700 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: bzr
May 18 10:02:41 6ed02ff77784 systemd[1]: indy-node-control.service: Service hold-off time over, scheduling restart.
May 18 10:02:41 6ed02ff77784 systemd[1]: Stopped Service for upgrade of existing Indy Node and other operations.
{noformat}
;;;","21/May/18 11:54 PM;anikitinDSR;Reasons: 
- need to fix parameters in node_control.conf, which passed to start_node_control script, by systemd or supervisord

Version:
indy-node 1.3.419-master

PR:
https://github.com/hyperledger/indy-node/pull/704;;;","22/May/18 1:29 AM;VladimirWork;Second case is checked during 1.3.62 RC acceptance testing.;;;","22/May/18 7:53 PM;VladimirWork;Build Info:
python3-base58=0.2.4
libindy-crypto=0.1.6
python3-indy-crypto=0.1.6
indy-plenum=1.2.180
indy-node=1.2.223

Steps to Validate:
1. Install 1.2.223 pool (1.3.55 stable analogue).
2. Add ~10 NYMs to ledger.
3. Upgrade pool to 1.3.410.
4. Add and get NYM to ledger.
5. Check indy-node version \ status and journalctl at all nodes.

Actual Results:
Pool is upgraded successfully without any errors in journalctl. Pool works after the upgrade.;;;",,,,,,,,,,,,,,,,,,,,,
Hot-fix of indy-node,INDY-1331,30211,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,15/May/18 11:37 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.3.62,,,,0,,,,"We need to create and test a hot-fix release of indy-node with the following fixes:
* INDY-1256 (Issues with Primary restart)
* INDY-1284 (Issues with read_ledger)
* Changes for supervisord support: https://github.com/hyperledger/indy-node/pull/588

Acceptance criteria:
* Cherry-pick the fixes to stable and create a new RC
* Acceptance testing for the RC

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1333,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzz9tz:",,,,,,EV 18.10 Stability and VC,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,dsurnin,esplinr,VladimirWork,,,,,,,,,"16/May/18 11:51 PM;dsurnin;Hotfixes were merged to stable branches

https://github.com/hyperledger/indy-plenum/pull/679
https://github.com/hyperledger/indy-node/pull/696
;;;","18/May/18 10:08 PM;esplinr;The team found a problem with upgrades caused by PR 588. We are evaluating whether it is better to ship the hotfix without the release, or delay in order to fix the issue.;;;","22/May/18 7:17 PM;VladimirWork;Checked in scope of acceptance testing. RC 1.3.62 is approved to stable.;;;",,,,,,,,,,,,,,,,,,,,,,
Support binding on separate NICs for Client-to-Node and Node-to-Node communication,INDY-1332,30214,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,sergey-shilov,sergey-shilov,16/May/18 12:17 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.4,,,,0,,,,"Now indy-plenum's and indy-node's listener sockets are always bound on 0.0.0.0, it is hard-coded, so that we listen all interfaces. We need to bind listener sockets on addresses specified for node stack and client stack for complete support of separate NICs.
Also we need to be able to set different max queue sizes for node-to-node and client-to-node communications.",,,,,,,,,,,,,,,,,INDY-1249,,,,,,,,,,,,INDY-1379,,,,,,,,,,,,,,,,,,,,"08/Jun/18 12:11 AM;VladimirWork;2NIC_NYMs_adding.PNG;https://jira.hyperledger.org/secure/attachment/15075/2NIC_NYMs_adding.PNG","08/Jun/18 12:11 AM;VladimirWork;2NIC_env_config.PNG;https://jira.hyperledger.org/secure/attachment/15073/2NIC_env_config.PNG","08/Jun/18 12:11 AM;VladimirWork;2NIC_network_config.PNG;https://jira.hyperledger.org/secure/attachment/15074/2NIC_network_config.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzzcxr:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,3.0,,,,,,,,,,,,ignisvulpis,sergey-shilov,VladimirWork,,,,,,,,,,"01/Jun/18 10:17 PM;sergey-shilov;*Problem state / reason:*

Indy node binds both node and client listeners to 0.0.0.0.

*Changes:*
 * Made IP to bind to optional in code instead of hard coded ""0.0.0.0"" or ""*"".
 * Now node/client IPs are got from /etc/indy/indy.env.
 * Changed installation and indy node initialisation scripts.
 * Implement migration script that adds node/client IPs to /etc/indy/indy.env if they are not present there. Node IP is got from pool ledger, client IP is set as 0.0.0.0.
 * Fix docs and test network setup scripts.

*Committed into:*

https://github.com/hyperledger/indy-plenum/pull/707
https://github.com/hyperledger/indy-plenum/pull/715
https://github.com/hyperledger/indy-plenum/pull/716   

https://github.com/hyperledger/indy-node/pull/732
https://github.com/hyperledger/indy-node/pull/734
https://github.com/hyperledger/indy-node/pull/736
https://github.com/hyperledger/indy-node/pull/738
[https://github.com/hyperledger/indy-node/pull/740]

indy-node 1.3.438-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

Set up the pool and check that all nodes connected and can do network exchange with each other.

Check /etc/indy/indy.env after installation, it should contain the following two variables: NODE_IP and NODE_CLIENT_IP. Their values should be set according to initialisation tool (init_indy_node, generate_indy_pool_transactions etc.).

Also check that nodes really listen IPs listed in their /etc/indy/indy.env files using _netstat_ utility:
 # apt install net-tools
 # netstat -ln

Example:
 root@0ffd7dc802b9:/home/indy# netstat -ltn
 Active Internet connections (only servers)
 Proto Recv-Q Send-Q Local Address           Foreign Address         State      
 tcp        0      0 10.0.0.3:9703           0.0.0.0:*               LISTEN     
 tcp        0      0 0.0.0.0:9704            0.0.0.0:*               LISTEN     
 tcp        0      0 127.0.0.11:43056        0.0.0.0:*               LISTEN     
 tcp        0      0 127.0.0.1:30003         0.0.0.0:*               LISTEN     ;;;","02/Jun/18 1:19 AM;VladimirWork;There is an issue with AWS setup (node NICs don't match IPs in .env file), it is reproduced on QA Live Pool now.;;;","04/Jun/18 7:54 PM;VladimirWork;We should keep NODE_IP and NODE_CLIENT_IP as *0.0.0.0* during upgrade/migration/installation to let Stewards configure it manually since we can fall into issue with NICs that are set in /etc/indy/indy.env but aren't listed in node machine NIC list (`ip a`).;;;","04/Jun/18 8:18 PM;sergey-shilov;Agreed, seems like it's the right solution.;;;","08/Jun/18 12:11 AM;VladimirWork;Build Info:
indy-node 1.3.447

Actual Results:
We keep NODE_IP and NODE_CLIENT_IP as 0.0.0.0 during upgrade(migration)/installation to let Stewards configure it manually.
Docker pool works normally with any amount of nodes with fair separate and custom configured NICs for node/client connections. !2NIC_env_config.PNG|thumbnail!  !2NIC_network_config.PNG|thumbnail!  !2NIC_NYMs_adding.PNG|thumbnail! 
AWS pool works normally with custom configured NICs only (but not separate).;;;",,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.3.62,INDY-1333,30250,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,VladimirWork,VladimirWork,16/May/18 6:35 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,18/May/18 12:00 AM,0,Documentation,,,"*Version Information*
indy-node 1.3.62
indy-plenum 1.2.42
indy-anoncreds 1.0.11
sovrin 1.1.10

*Major Fixes*
INDY-1256 - STN lost consensus
INDY-1284 - Unable to use read_ledger tool with the parameter ""to""
INDY-1330 - Upgrade from 1.2.223 (1.3.55 stable analogue) to 1.3.410 (rocksdb) doesn't work

*Changes and Additions*
Support for supervisord is added (https://github.com/hyperledger/indy-node/pull/588)
Indy-node dependencies are fixed
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1331,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwjr:",,,,,,,,,,,,,,1.0,,,,,,,,,,,,TechWritingWhiz,VladimirWork,,,,,,,,,,,"16/May/18 6:37 PM;VladimirWork;FYI [~krw910];;;","17/May/18 4:54 AM;TechWritingWhiz;The pull request for this item is here: https://github.com/sovrin-foundation/sovrin/pull/58;;;","22/May/18 6:30 PM;VladimirWork;[~TechWritingWhiz] Could you please update your PR according to this ticket changes?;;;","30/May/18 3:45 AM;TechWritingWhiz;Here is the pull request for this: https://github.com/sovrin-foundation/sovrin/pull/61;;;","31/May/18 3:22 AM;TechWritingWhiz;Here is another pull request for it: forgot to update the numbers...here. I updated them elsewhere, but not here. Now that is fixed. https://github.com/sovrin-foundation/sovrin/pull/62;;;",,,,,,,,,,,,,,,,,,,,
Explore config parameters to find the best performance/stability settings,INDY-1334,30257,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,16/May/18 9:15 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"Parameters to explore and its default values:

*3PC batching:*
 Max3PCBatchSize = 100 : we send a batch if we reach this amount of 3PC messages (PROPAGATE/PRE-PREPARE/PREPARE/COMMIT) in it
 Max3PCBatchWait = .001 : we send a batch if we reach this timeout from previous batch sent *and* we have at least one 3PC message in it

*Performance statistics:*
 DELTA = 0.4 : sensitivity of node to master performance degradation, *increasing/reducing of it leads to increasing/reducing number of view changes respectively*
 LAMBDA = 60 : master latency marker (seconds), *increase it to reduce number of view changes*
 OMEGA = 5 : master/backup latency difference (seconds), *increase it to reduce number of view changes*

*ZMQ message quotes:*
ZMQ_INTERNAL_QUEUE_SIZE = 10000 : number of messages (any) that we keep in ZMQ queue, all above this number will be discarded

 

*List of config parameters:*
https://docs.google.com/document/d/1tqpHNdAhgLY0hftIY-tIxp2TkmXNWrMyHv_LzT8czkE/edit#heading=h.4pqcps2qiqrj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1429,INDY-1435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzd1j:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,3.0,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"16/May/18 9:24 PM;VladimirWork;*PoA:*
3PC batching parameters - increase and reduce during load testing against 25 nodes pool to check performance changes

Performance statistics parameters - reduce thresholds and frequences to reduce performance calculations and amount of view changes done

ZMQ message quotes - increase and reduce during load testing against 25 nodes pool to check performance changes;;;","25/May/18 7:46 PM;VladimirWork;We have better throughput (*~10%* for first 25k written txn) and lesser amount of failed txns (up to *1k* max failed from first 50k written vs up to *100* max failed from first 50k written) at this parameters:

{noformat}
DELTA = 0.1
LAMBDA = 240
OMEGA = 20

Max3PCBatchSize = 10000 (100000)
Max3PCBatchWait = 1
{noformat}

Also there are inconsistent results for ZMQ queue size exploration so additional investigation against AWS pool is needed.;;;",,,,,,,,,,,,,,,,,,,,,,,
"Enable full ordering of batches from last view that have been already ordered, make execution on replicas that executed them no-op",INDY-1335,30288,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,sergey.khoroshavin,sergey.khoroshavin,17/May/18 12:54 AM,01/Oct/19 7:56 PM,28/Oct/23 2:47 AM,17/Aug/19 1:02 AM,,1.10.0,,,,0,,,,"*Acceptance criteria*
 # feature implemented
 # all tests pass
 # new tests to prove that this works
 # make sure that we correctly process requests with the same did+reqId from clients (return already ordered txn)",,,,,,,,,,INDY-2136,,,,,,,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|hzwvif:00001yw969vc",,,,,,Ev-Node 19.16,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"31/May/19 6:47 PM;ashcherbakov;*PoA*
 * `prepared` and `preprepared` should have a list of BatchIDs instead of a list of PrePrepares
 * Correctly update shared state by dedicated services only
 ** Ordering: last_ordered_3pc, prepared, prepared
 ** Checkpointer: checkpoints, stableCheckpoints, lowWatermark, highWatermark
 ** ViewChanger: primaries. primaryName, viewNo, waitingForNewView
 * Propagate View change stages to other services via Internal Messages
 ** NeedViewChange:
 *** by ViewChangeService:
 **** starts View Change
 ** ViewChangeStarted (sent by ViewChangeService)
 *** by OrderingService:
 **** Clear 3PC log (prepares/preapares/commits)
 **** Save all PrePrepares to a list old_view_pre_prepares
 **** Reset uncommitted state
 ** ViewChangeFinished (sent by ViewChangeService)
 *** by CheckpointerService
 **** updates stable checkpoint
 ** ApplyNewView (sent by CheckpointerService)
 *** by OrderingService:
 * Create a new `*3PCValidator*` class that mostly copies the existing one and performs validation as follows:
 ** for 3PC messages (PrePrepare, Prepare, Commit) + ApplyNewView msg:
 *** Stash if
 **** Catchup in progress
 **** Future view
 **** above upper watermark
 **** waiting for new view (except NewView)
 *** Discard if
 **** below watermark
 **** old view
 **** already ordered (for PrePrepares only)
 *** Process otherwise
 **** it includes processing of already ordered prepares and commits
 * Make changes in *Orderer* service:
 ** Make sure that PREPAREs and COMMITs are sent for every 3PC Batch regardless if it's ordered or not
 ** Do not re-apply PRE_PREPARE if it's already ordered (however send Prepare)
 ** Do not Order already ordered requests once a quorum of COMMITs is received
 ** Clear old_view_pre_prepares on GC

 ;;;","16/Aug/19 10:25 PM;ashcherbakov;Done according to PoA:
 * Implemented message-based communication between services during view change
 * Update shared data by dedicated services (Ordering: last_ordered_3pc, prepared, prepared, Checkpointer: checkpoints, stableCheckpoints, lowWatermark, highWatermark, ViewChanger: primaries. primaryName, viewNo, waitingForNewView)
 * More unit tests for ViewChangeService
 * Implemented new message validation for Ordering Service
 * Implemented a first version of applying New View by Ordering service
 * Improved message validation
 * get rid of some old code

PR: [https://github.com/hyperledger/indy-plenum/pull/1297]

 

Tests:
 * [plenum/test/view_change/test_re_order_pre_prepares.py|https://github.com/hyperledger/indy-plenum/pull/1297/files#diff-586bf767b906c4838a760dff1e65fa7e]
 * plenum/test/consensus/view_change/test_view_change_service.py
 * plenum/test/consensus/order_service/test_ordering_service_on_view_change.py
 * plenum/test/consensus/order_service/test_ordering_process_commit.py
 * plenum/test/consensus/order_service/test_ordering_msg_validator.py;;;",,,,,,,,,,,,,,,,,,,,,,,
Stop resetting ppSeqNo (and relying on this) in new view,INDY-1336,30290,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,sergey.khoroshavin,sergey.khoroshavin,17/May/18 12:55 AM,01/Oct/19 7:56 PM,28/Oct/23 2:47 AM,14/Sep/19 1:25 AM,,1.10.0,,,,0,,,,"PBFT View Change assumes that ppSeqNo is not reset in new views.
 Plenum starts ppSeqNo from 1 on each new view.

We need to have Plenum not resetting ppSeqNo to have correct checkpoints after view change

Acceptance criteria:
 * Keep (viewno, ppseqno) as 3PC key, but do not reset ppseqno on new views.
 * Compare based on ppSeqNo only
 ** check all Validators
 * do not clear checkpoints after view change
 * do not reset stable checkpoint (in `reset_checkpoints`)",,,,,,,,,,,,,,,,,INDY-1340,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/19 8:03 PM;VladimirWork;performance_results_11_09_2019.tar.gz;https://jira.hyperledger.org/secure/attachment/17808/performance_results_11_09_2019.tar.gz","12/Sep/19 8:03 PM;VladimirWork;performance_results_12_09_2019.tar.gz;https://jira.hyperledger.org/secure/attachment/17807/performance_results_12_09_2019.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|hzwvif:00001yw969v98632r",,,,,,Ev-Node 19.17,Ev-Node 19.18,,,,,,,5.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,"16/Aug/19 8:41 PM;anikitinDSR;h2. PoA

Quick solution is:
 * remove dropping last_ordered_3pc to after view_change complete;
 * remove view_no comparing for compare_3PC_keys function;
 * run integration tests and analize results;;;","06/Sep/19 10:12 PM;anikitinDSR;Reason:
 * need to stop dropping pp_seq_no after view_change

Changes:
 * pp_seq_no as a part of last_ordered_3pc is increasing sequence now, lastPrePrepareSeqNo too

Versions:
 * indy-plenum: 1.10.0.dev888
 * indy-node: 1.10.0.dev1079
 * token-plugins: 1.0.3~dev89

Recommendation for QA:
 * need to run load test and analize result;;;","12/Sep/19 8:03 PM;VladimirWork;Build Info:
indy-node 1.10.0.dev1079
plugins 1.0.3~dev89

Steps to Reproduce:
1. Run production load test against 25 nodes pool.
2. Run production load test with forced VC (every 30 minutes) against 25 nodes pool.

Actual Results:
Pool sustains production load for more than 24 hours but it looks like that backup replicas don't order txns after some VC in 2nd test run.

Expected Results:
Backup replicas must order txns after VCs.

Summary Test Results:
 [^performance_results_11_09_2019.tar.gz] 
 [^performance_results_12_09_2019.tar.gz] 

Full Logs:
ev@evernymr33:logs/1336_1st_run_without_VC.tar.gz
ev@evernymr33:logs/1336_2nd_run_with_VC.tar.gz;;;","14/Sep/19 1:25 AM;ashcherbakov;It was discovered in the last load, not dropping ppSeqNo after view change on backup instances may stop instances from ordering if they have been removed by some nodes. 
This will be fixed in INDY-2226;;;",,,,,,,,,,,,,,,,,,,,,
Modify WriteReqManager to meet Executor interface needs,INDY-1337,30292,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,ashcherbakov,sergey.khoroshavin,sergey.khoroshavin,17/May/18 12:56 AM,01/Oct/19 7:56 PM,28/Oct/23 2:47 AM,11/Jul/19 10:08 PM,,1.10.0,,,,0,,,,,,,,,,,,,,INDY-1338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|i00x3f:",,,,,,Ev-Node 19.14,,,,,,,,1.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"11/Jul/19 9:21 PM;ashcherbakov;*PoA*
 * Once message is ordered, OrdererService needs to do the following:

 ** Call `WriteRequestManager#commit_batch` (for master replica only)
 ** Send `Ordered` message to the Internal Bus
 *** The `Ordered` message will be processed by MonitorService, Node (Propagate) Service, and ObservablerService
 *** Monitor service will update statistics
 *** Node (Propagate) Service will send Reply to the client and mark requests as executed
 *** Observable Service will update Observers if needed\
 *** See [https://raw.githubusercontent.com/hyperledger/indy-plenum/master/design/plenum_2_0_architecture_object.png]
 * What is stated above effectively replaces `processOrdered` message in Node
 * So, the following needs to be done right now:
 ** `WriteRequestManager#commit_batch` will be called by the OrdererService, not Node;;;","11/Jul/19 10:08 PM;ashcherbakov;This will be done in the scope of INDY-2136;;;",,,,,,,,,,,,,,,,,,,,,,,
 Define Interfaces needed for View Change Service ,INDY-1338,30294,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,17/May/18 1:02 AM,01/Oct/19 7:56 PM,28/Oct/23 2:47 AM,21/Jun/19 12:41 AM,,1.10.0,,,,0,,,,"Start PBFT View Change Service Implementation. Define Interfaces needed for View Change Service, such as ConsensusDataProvider and NetworkService.

*Acceptance Criteria:*
 * Interfaces for services needed to implement View Change

 

 ",,,,,,,,,,,,,,,,,INDY-1340,INDY-1337,INDY-2139,INDY-2135,,,,,,,,,INDY-1290,,,,,INDY-1344,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|hzwqp3:",,,,,,Ev-Node 19.12,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"20/Jun/19 8:37 PM;ashcherbakov;* Updated PBFT View Change design with recent thinking and implementation plans: `design/view-change.md`
 * Added Plenum 2.0 Architecture Descriptions and diagrams (it's required to properly implement PBFT View Change protocol): 
 ** design/plenum_2_0_architecture.md
 ** design/plenum_2_0_architecture_class.puml
 ** design/plenum_2_0_architecture_object.puml
 ** design/plenum_2_0_communication.puml
 * Draft implementation of interfaces needed for View Change is done:
 ** External Event Bus
 ** Internal Event Bus
 ** View Changer Service
 ** Consensus Data Provider

PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/1244]
 * https://github.com/hyperledger/indy-plenum/pull/1242;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Implement network, executor, orderer and checkpointer as adaptors for existing codebase",INDY-1339,30295,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,sergey.khoroshavin,sergey.khoroshavin,17/May/18 1:03 AM,01/Oct/19 7:57 PM,28/Oct/23 2:47 AM,13/Jun/19 5:44 PM,,1.10.0,,,,0,,,,,,,,,,,,,,,,,,,,,INDY-1345,,,,,,,,,,,,INDY-1290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|hzwvif:00001yw969wi",,,,,,,,,,,,,,5.0,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,"13/Jun/19 5:44 PM;sergey.khoroshavin;As per new plan Orderer and Checkpointer will be extracted from replicas instead of becoming adaptors (INDY-2136 and INDY-2137 created for this task) and Executor is basically a WriteRequestManager from new pluggable request handlers.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Implementation: Make PBFT view change working,INDY-1340,30296,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,sergey.khoroshavin,sergey.khoroshavin,17/May/18 1:05 AM,01/Oct/19 7:57 PM,28/Oct/23 2:47 AM,14/Sep/19 1:12 AM,,1.10.0,,,,0,,,,"This includes making sure that current integration tests pass.

*Acceptance criteria:*
 * Everything to make View change working should be done
 * Basic integration and simulation tests (INDY-2149) with a new View Change protocol need to pass
 * Enabling and replacing of the old view change protocol by the new one needs to be done in INDY-2223
",,,,,,,,,,INDY-1335,INDY-1338,INDY-2137,INDY-2136,INDY-2139,INDY-1336,INDY-2177,INDY-2140,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1302,,,,,,,,,"1|hzwvif:00001yw969v98632u",,,,,,Ev-Node 19.17,Ev-Node 19.18,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"31/Aug/19 1:27 AM;sergey.khoroshavin;Some work that was expected to be done in scope of INDY-2177 turned out to be blocked by INDY-1336, so it was decided to move it into scope of this issue. PR is already raised (https://github.com/hyperledger/indy-plenum/pull/1317), but more work is required in order to make all tests pass:
* finish INDY-1336
* stop resetting checkpoint after view change
* remove as much as possible view_no references from CheckpointService and related tests (I left lots of TODO comments in relevant parts of code, they can be easily found by looking through PRs raised in scope of INDY-2177);;;","02/Sep/19 10:51 PM;ashcherbakov;*PoA:*
# [DONE] Add `pp_view_no` field to `BatchID` - https://github.com/hyperledger/indy-plenum/pull/1326
 ** This is the viewNo where the PrePrepare associated with the BatchID has been applied first time (so persisted in the audit ledger)
# [DONE] Support `old_view_preprepares` in Ordering Service - https://github.com/hyperledger/indy-plenum/pull/1326 -
 ** It needs to take into account (pp_view_no, pp_seq_no, pp_digest) when searching for a PrePrepare in `process_new_view_checkpoints_applied`
 ** `old_view_preprepares` needs to be correctly GCed-
 # [DONE] Support requesting old view PrePrepares https://github.com/hyperledger/indy-plenum/pull/1329
 ** call it from `process_new_view_checkpoints_applied`
 ** create a new MessageReq type for this, so that it's sent to all nodes, and reply from any node can be processed (not from a primary only as in a case of common PrePrepare requests)
 # [DONE] Add more unit tests for `process_new_view_checkpoints_applied` https://github.com/hyperledger/indy-plenum/pull/1329
 ** check that `process_new_view_checkpoints_applied` sends a Prepare and adds a PrePrepare if BatchID is already ordered
 ** check that `process_new_view_checkpoints_applied` applies a PrePrepare (and sends a Prepare if it's not Primary) if BatchID is not already ordered
 ** check that `process_new_view_checkpoints_applied` requests missing old view PrePrepares from other nodes
 ** check that `old_view_preprepares` GCed
# [DONE] Merge [https://github.com/hyperledger/indy-plenum/pull/1317]
# [DONE] Revert unordered batches and set lastPPSeqNo on `ViewChangeStarted` in Ordering Service
# [DONE] Changes in View Change Service:
 ** Move to a new view if there is no NEW VIEW message from a primary for more than X second. Send INSTANCE_CHANGE in this case
 ** Find out the default value for X

;;;","14/Sep/19 1:11 AM;ashcherbakov;*Changes*
- all tasks mentioned in the PoA are done

*PRs*:
- https://github.com/hyperledger/indy-plenum/pull/1326
- https://github.com/hyperledger/indy-plenum/pull/1329
- https://github.com/hyperledger/indy-plenum/pull/1330

Implementation and testing will be continued in INDY-2223 and INDY-2146;;;",,,,,,,,,,,,,,,,,,,,,,
Simple Timeout fixes of the current View Change protocol,INDY-1341,30337,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,17/May/18 6:43 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"There are problems in existing View Change protocol (see INDY-1296 and INDY-1303).
Although the correct way to deal with it is to implement PBFT View Change, we can try to have some fixes which can improve stability of the View Change.

Possible fix (with options):
   # do not process any PrePrepares and Prepares above prepared_certificate.
   # [optional]: send COMMITs to all nodes ot make sure they order till their prepared certificates.
   # get rid of the logic on exiting on multiple rounds of catch-up without new txns caught-up.
   # [optional]: we may have some positive timeout for ordering till  last_prepared
   # we may continue doing catch-ups all the time, or just do one catch-up at the end, or don't do them at all for simplicity.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzcz3:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"04/Jun/18 5:52 PM;ashcherbakov;Let's do a simple timeout fix there and continue other improvements in the scope of INDY-1383;;;","06/Jun/18 6:43 PM;sergey.khoroshavin;PR with simple fix: https://github.com/hyperledger/indy-plenum/pull/717
Validation was done in scope of [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350];;;",,,,,,,,,,,,,,,,,,,,,,,
"""Monitoring Tools for Stewards"" requires additions",INDY-1342,30344,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,sergey-shilov,zhigunenko.dsr,zhigunenko.dsr,17/May/18 9:35 PM,11/Oct/19 6:30 PM,28/Oct/23 2:47 AM,11/Oct/19 6:30 PM,,,,,,0,,,,"1) indy_config.py requires to have SendMonitorStats and SpikeEventsEnabled switchers (at least in docs)
2) these triggers must configured: nodeRequestSpike, clusterThroughputSpike
3) related changes in [docs|https://github.com/hyperledger/indy-node/blob/master/docs/node-monitoring-tools-for-stewards.md]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1251,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy47:",,,,,,,,,,,,,,,,,,,,,,,,,,esplinr,zhigunenko.dsr,,,,,,,,,,,"11/Oct/19 6:30 PM;esplinr;The information currently returned by validator-info appears to be sufficient for administration, and the additional debugging information available to developers appears to meet our needs at this point in time.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Prove production stability of an Indy network,INDY-1343,30346,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,esplinr,esplinr,17/May/18 10:28 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"Before encouraging people to use the Sovrin network for live loads, we need to prove that it will be stable under conditions similar to production use.

*Acceptance Criteria*
 Perform a test of an Indy network that has the following attributes:
 * The ledger is pre-loaded with 1 million transactions
 * Pool size at least matches the number of network nodes initially expected in the Sovrin network (currently 25 nodes).
 * 1K concurrent clients
 * Over a 3 hour period induce a sustained throughput of 10 write transactions per second and 100 read transactions per second on average.
 * Write load is a mixture of:
 ** writing credentials schema (5%),
 ** writing credential definition (5%)
 ** revoke registry definition (5%)
 ** revoke registry update (5%)
 ** write DID to ledger (20%)
 ** write payment to ledger (45%)
 ** write attrib to ledger (15%)
 * Read load is a mixture of:
 ** read DID from ledger (45%)
 ** read credential schema (10%)
 ** read credential definition (10%)
 ** read revoke registry definition (10%)
 ** read revoke registry delta (10%)
 ** read attrib from ledger (10%)
 ** read payment balance from ledger (5%)
 * Write response time should be less that 5 seconds (would also like a report of the average).
 * Read response time should be less than 1 second (would also like a report of the average).

Any problems found will be logged in JIRA as separate issues for independent prioritization.

As part of this issue, it is recognized that it will be necessary to create a load testing tool sufficient to perform the test.",,,,,,,,,,INDY-1355,INDY-1356,INDY-1357,INDY-1358,,,,,,,,INDY-1388,INDY-2214,,,,,,,INDY-1448,,,,,INDY-1448,INDY-1460,INDY-1477,INDY-1478,INDY-1483,INDY-1607,INDY-1717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzwwjb:",,,,,,EV 18.14 Monitoring/Stability,EV 18.15 Stability/Availabilit,EV 18.16 Releasing 1.6,,,,,,8.0,,,,,,,,,,,,esplinr,krw910,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,,"18/May/18 12:29 AM;esplinr;Our test pool can have more variety if we use images from multiple organizations:
* Evernym
* BC.gov's VON Docker image (Alpine Linux), published to Docker Hub
** Source here: https://github.com/PSPC-SPAC-buyandsell/von-image
** Variations published to Docker Hub here: https://hub.docker.com/r/bcgovimages/von-image/
* IBM's Docker Image

It would also benefit from having a variety of agents:
* IBM
* BC.gov
* BYU
* Evernym;;;","18/May/18 1:15 AM;krw910;[~esplinr] You left off the required pool size. We are testing with and planning on a 25 node pool. Do you want to add that to the requirements and let us know if it needs to be higher than 25? We cannot go lower since I believe we are already on track to have that many Stewards.
;;;","18/May/18 2:30 AM;esplinr;Thanks Kelly. I added that.;;;","24/May/18 2:07 AM;esplinr;I updated the description with feedback from James, Daniel, Steve, etc.

Failures will not necessarily block production deployment of the system, but they should be logged in order for them to be discussed.;;;","30/May/18 4:36 AM;krw910;[~esplinr] I have a question about your comment that failures not necessarily blocking production deployment. I now see the requirement as a goal, but do we have a bare minimum requirement. In other words do we shoot for the goal and just take any area we fall short in on a case by case basis or is there a ""we cannot fall below 'this' line""? I am not asking for a lower bar, but it might help to distinguish a high priority issue from a medium or low priority issue.;;;","05/Jun/18 6:43 AM;esplinr;[~krw910] Instead of negotiating a minimum requirement in advance of the test, I would prefer to see if we are happy with the test results. If we are not happy, we can evaluate the impact on delivery any additional work would have.;;;","04/Jul/18 1:15 AM;zhigunenko.dsr;*Test run:* 1
*Environment:*
indy-node 1.4.483 (25 AWS nodes)
Ledger size: 975k -> 1035k
*Goal:* proof that big ledger can successfully write
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -t 10 -c 10 -n 11 -k ""{\""nym\"": {\""count\"": 4}}
{code}
*Results:*
Duration - {color:green}3 hours{color}
Transactions written: 54479
Throughput: write - {color:orange}5 txns per second{color}
*Findings:*
* Big ledger can successfully write
* Simultaneous reading and writing haven't been checked
* Possibly load script generates less transactions than expected from parameters calculation

----

*Test run:* 2
*Environment:*
indy-node 1.4.483 (25 AWS nodes)
Ledger size: 1035k -> 1069k
*Goal:* pool works with ~250 concurrent clients that read 100 txns/sec and write 10 txns/sec
*Load:*
Four instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 38 -t 15 -n 1 -k nym
{code}
152 clients with total throughput 10 txns / sec

Two instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 50 -t 1 -n 1 -k ""{\""get_nym\"": {\""file_name\"": \""./load_test_1/successful\""}}""
{code}
100 clients with total throughput 100 txns / sec

*Results:*
Duration - {color:green}3 hours{color}
Transactions written: 34249 (also another 1605 failed by pool timeout)
Transactions read: 472059 (also another 928 failed by pool timeout)
Throughput: write - {color:orange}3.17 txns/sec{color}, read - {color:orange}43.7 txns/sec{color}
*Findings and further actions:*
* If pool is not bottleneck, parameterized script throughput must be increased twice to achieve KPI
* Schemas, attribs and cred-defs must be added to load (INDY-1378 required for revocation)
* ""Thread mode"" allows to generate more connections on the same hardware;;;","04/Jul/18 9:16 PM;zhigunenko.dsr;*Test run:* 3
*Environment:*
indy-node 1.4.483 (25 AWS nodes)
Ledger size: 1069k -> 1100k
logLevel = INFO
*Goal:* pool works with ~378 concurrent clients that read 100 txns/sec and write 10 txns/sec in total
*Load:*
Four instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 57 -t 20 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
228 clients with total throughput 11 txns / sec

Two instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 10 -m t -c 75 -t 2 -n 4 -k ""{\""get_nym\"": {\""file_name\"": \""./load_test_1/successful\""}}""
{code}
150 clients with total throughput 100 txns / sec

*Results:*
Last ViewNo - 82
Duration - {color:green}12 hours{color}
Pool {color:red}stopped writing on 1100k transactions{color} - INDY-1448;;;","09/Jul/18 7:25 PM;zhigunenko.dsr;*Test run:* 4
*Environment:*
indy-node 1.4.494 (25 AWS nodes)
Ledger size: 975k -> 1065k
logLevel = INFO
*Goal:* pool works with ~1000 concurrent clients that read 100 txns/sec and write 10 txns/sec in total
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 286 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
572 clients with total throughput 10 txns / sec

{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_schema\"": {\""file_name\"": \""./load_schema/successful\""}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_attrib\"": {\""file_name\"": \""./load_attrib/successful\""}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_nym\"": {\""file_name\"": \""./load_nym/successful\""}}""
{code}
429 clients with total throughput 107 txns / sec

*Results:*
Last ViewNo - 1
Duration - {color:green}3 hours{color}
Pool {color:green} doesn't stopped writing{color}
Transactions written: 89911 (no fails)
Transactions read: 869357 (no fails)
Throughput: write - {color:orange}8.32 txns/sec{color}, read - {color:orange}80.5 txns/sec{color};;;","10/Jul/18 5:22 AM;zhigunenko.dsr;*Test run:* 5
*Environment:*
indy-node 1.4.494 (25 AWS nodes)
Ledger size: 975k -> 1065k
logLevel = INFO
*Goal:* pool works with ~1000 concurrent clients that read 100 txns/sec and write 10 txns/sec in total
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 286 -t 19 -n 4 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 19 -n 4 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 19 -n 4 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
572 clients with total throughput 120 txns / sec

{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 7 -n 2 -k ""{\""get_schema\"": {\""file_name\"": \""./load_schema/successful\""}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 7 -n 2 -k ""{\""get_attrib\"": {\""file_name\"": \""./load_attrib/successful\""}}""
{code}
-python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 7 -n 2 -k ""{\""get_nym\"": {\""file_name\"": \""./load_nym/successful\""}}""-

{color:orange}286 clients{color} with total throughput 81 txns / sec

*Results:*
Last ViewNo - no view changes
Duration - {color:orange}2 hours 20 minutes{color}
Pool {color:orange} stopped writing because of ""no space left""{color}
Transactions written: 91k+
Throughput: write - {color:green}12 txns/sec{color};;;","16/Jul/18 11:19 PM;zhigunenko.dsr;*Test run:* 6
*Environment:*
indy-node 1.4.500 (25 AWS nodes)
Ledger size: 230k -> 926k
logLevel = INFO
STACK_COMPANION=1
*Goal:* pool works with ~1000 concurrent clients that write 15 txns/sec in total
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 22 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k schema
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k attrib
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
{code}

*Results:*
Last ViewNo - {color:orange}11 view changes{color}
Duration - {color:green}12 hours +{color}
Pool {color:orange} stopped writing{color}
Transactions written: {color:green}700k+{color}
Throughput: write - {color:green}15 txns/sec{color}
INDY-1477 has been created

*Additional info:*
Lack of RAM or disk space could have arisen
Free RAM stably decreasing in work time, look like a memory leaks;;;","16/Jul/18 11:35 PM;zhigunenko.dsr;*Test run:* 7
*Environment:*
indy-node 1.4.500 (25 AWS nodes)
Ledger size: 27 -> 242k
logLevel = INFO
STACK_COMPANION=1
*Goal:* pool works with ~1000 concurrent clients that write 20 txns/sec in total
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 33 -n 2 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 50 -n 1 -k schema
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 50 -n 1 -k attrib
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 100 -n 1 -k cred_def
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 100 -n 1 -k cred_def
{code}

*Results:*
Last ViewNo - {color:red}24 view changes{color}
Duration - {color:red}6 hours{color}
Pool {color:orange} stopped writing{color}
Transactions written: {color:green}700k+{color}
Throughput: write - {color:green}15 txns/sec{color} in peak

After first 2 hours and 105k txns performance starts decreased
INDY-1478 has been created;;;","17/Jul/18 11:13 PM;zhigunenko.dsr;*Test run:* 8
*Environment:*
indy-node 1.4.500 (25 AWS nodes)
Ledger size: 247k -> 354k
logLevel = INFO
STACK_COMPANION=1
*Goal:* pool works with ~1000 concurrent clients that write 10 txns/sec and read 100 txns/sec in total 
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 33 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 33 -n 1 -k ""{\""schema\"": 1, \""attrib\"": 3}""
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 132 -n 1 -k cred_def
---
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 2.7 -n 1 -k get_nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 6 -n 1 -k ""{\""get_schema\"": 1, \""get_attrib\"": 1}""
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 12 -n 1 -k get_cred_def
{code}

*Results:*
Last ViewNo - {color:green}no view change{color}
Duration - {color:green}3 hours{color}
Pool {color:green} in action {color}
Transactions written: {color:green}108k+{color}
Throughput: write - {color:green}10 txns/sec{color} , read - {color:orange}56 txns/sec{color};;;","18/Jul/18 3:00 AM;zhigunenko.dsr;*Test run:* 9
*Environment:*
indy-node 1.4.500 (25 AWS nodes)
Ledger size: 354k -> 461k
logLevel = INFO
STACK_COMPANION=1
*Goal:* pool works with ~1000 concurrent clients that write 10 txns/sec and read 100 txns/sec in total 
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 33 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 33 -n 1 -k ""{\""schema\"": 1, \""attrib\"": 3}""
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 132 -n 1 -k cred_def
---
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 1.35 -n 1 -k get_nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 3 -n 1 -k ""{\""get_schema\"": 1, \""get_attrib\"": 1}""		
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 6 -n 1 -k get_cred_def
{code}
*Results:*
Last ViewNo - {color:#008000}no view change{color}
Duration - {color:#008000}3 hours{color}
Pool {color:#008000}in action {color}
Transactions written: {color:#008000}106k+{color}
Transactions read: {color:#FFA500}798k+{color}
Throughput: write - {color:#008000}10 txns/sec{color} , read - {color:#FFA500}73.9 txns/sec{color};;;","19/Jul/18 5:03 PM;zhigunenko.dsr;*Test run:* 10
*Environment:*
indy-node 1.4.504 (25 AWS nodes)
Ledger size: 0k -> 106k
logLevel = INFO
STACK_COMPANION=0
*Goal:* pool works with ~1000 concurrent clients that write 10 txns/sec and read 100 txns/sec in total 
*Load:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 33 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 33 -n 1 -k ""{\""schema\"": 1, \""attrib\"": 3}""
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 132 -n 1 -k cred_def
---
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 1.35 -n 1 -k get_nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 3 -n 1 -k ""{\""get_schema\"": 1, \""get_attrib\"": 1}""		
python3 perf_processes.py -g pool_transactions_genesis -m t -c 167 -t 6 -n 1 -k get_cred_def
{code}
*Results:*
Last ViewNo - {color:#008000}no view change{color}
Duration - {color:#008000}3 hours{color}
Pool {color:#008000}in action {color}
Transactions written: {color:#008000}106k+{color}
Transactions read: {color:#FF0000}510k+{color}
Throughput: write - {color:#008000}10 txns/sec{color} , read - {color:#FF0000}51 txns/sec{color}

Reading throughput *degradation*: 30%;;;","24/Jul/18 7:37 PM;ozheregelya;Note that with such values of -k parameter (get_nym, get_schema, get_attrib, get_cred_def, without specifying of the source file) the load script reads random data, not existing in the ledger.;;;","17/Aug/18 9:25 PM;zhigunenko.dsr;*Test run:* 11
*Environment:*
indy-node 1.4.500 (25 AWS nodes)
Ledger size: 0k -> 156k + 9k
logLevel = INFO
*Goal:* pool works with ~1000 concurrent clients that write 10 txns/sec and read 100 txns/sec in total 
*Load:*
5 instances, each of them creates 200 clients generates required distribution of requests/transactions

*Results:*
{color:#FFA500}Regular view changes{color}
Duration - {color:#008000}10 hours{color}
Pool {color:#008000}in action {color}
Transactions written: {color:#FFA500}165k+{color}
Transactions read: {color:#FFA500}2.5kk+{color}
Throughput: write - {color:#FF0000}10 txns/sec{color} , read - {color:#FFA500}67+ txns/sec{color};;;","17/Aug/18 9:26 PM;zhigunenko.dsr;*Reason to close:*
Future exploration will be continued in scope of INDY-1607;;;",,,,,,,
Implement legacy viewchanger using design for PBFT view change,INDY-1344,30382,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,sergey.khoroshavin,sergey.khoroshavin,18/May/18 9:47 PM,11/Jun/19 10:16 PM,28/Oct/23 2:47 AM,11/Jun/19 10:16 PM,,,,,,0,,,,"Current implementation of view change is quite tangled with other codebase. This leads to following problems:
- it cannot be made pluggable as is
- it cannot be unit tested
- it's hard to modify

Reimplementing legacy view change using TDD practices and adhering to new design will solve all above mentioned problems. 

This ticket is just for implementation of core logic (implementing mocks for Network, Orderer and Checkpointer as needed). Work done on mocks can be shared with [INDY-1338|https://jira.hyperledger.org/browse/INDY-1338].",,,,,,,,,,,,,,,,,INDY-1345,,,,,,,,,,,,INDY-1290,INDY-1338,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzwy4f:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integrate refactored legacy viewchanger into current codebase,INDY-1345,30383,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,sergey.khoroshavin,sergey.khoroshavin,18/May/18 9:55 PM,13/Jun/19 6:11 PM,28/Oct/23 2:47 AM,11/Jun/19 10:17 PM,,,,,,0,,,,"This task is for integration of work done in [INDY-1344|https://jira.hyperledger.org/browse/INDY-1344] into current codebase.",,,,,,,,,,INDY-1339,INDY-1344,,,,,,INDY-1346,,,,,,,,,,,,INDY-1290,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzwy4n:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement hybrid legacy/PBFT viewchanger,INDY-1346,30384,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,sergey.khoroshavin,sergey.khoroshavin,18/May/18 10:00 PM,13/Jun/19 6:11 PM,28/Oct/23 2:47 AM,11/Jun/19 10:17 PM,,,,,,0,,,,"Add minimal version of PBFT view change protocol to propagate upper bound on messages that should be ordered before entering new view - this should fix main theoretical issue with current view change approach as identified in [INDY-1296|https://jira.hyperledger.org/browse/INDY-1296]. 

While proper fix looks like implementing [INDY-1340|https://jira.hyperledger.org/browse/INDY-1340] it could turn out quite time consuming given current state of codebase.",,,,,,,,,,INDY-1345,,,,,,,,,,,,,,,,,,,INDY-1290,,,,,INDY-1296,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzwy4v:",,,,,,,,,,,,,,8.0,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a developer I need a method to make nodes issue instance change message in order to create tests for view change edge cases,INDY-1347,30387,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,sergey.khoroshavin,sergey.khoroshavin,18/May/18 11:02 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzb5r:",,,,,,,,,,,,,,,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,"22/May/18 8:12 PM;sergey.khoroshavin;It turned out that {code}node.viewchanger.on_master_degradation(){code} can be used for this purpose;;;",,,,,,,,,,,,,,,,,,,,,,,,
Validate new txn format,INDY-1348,30390,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,18/May/18 11:15 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"Perform validation of a new txn format done in the scope of INDY-1123 and INDY-1319. 
Make sure that there is no regression.
",,,,,,,,,,INDY-1319,IS-740,INDY-1379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1374,,,,,,,,,"1|hzzcxz:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"29/May/18 8:07 PM;ashcherbakov;Changes:
 - re-factored txns to the new format separating `data` and `metadata` (request format and read reply format are not changed; write reply format is changed):
 ** [https://github.com/hyperledger/indy-node/blob/master/docs/transactions.md]
 ** [https://github.com/hyperledger/indy-node/blob/master/docs/requests.md]
 - use utility methods to access txn fields
 - use utility methods to create txns
 - migration script for the new format

PR:
 - [https://github.com/hyperledger/indy-node/pull/727]
 - [https://github.com/hyperledger/indy-plenum/pull/710]

Version:
 - 1.3.433

Risk factors:
 - ledger migration
 - base functionality

Risk:
 - Med

Covered with tests:
 - new tests in plenum/transactions, plenum/requests, plenum/ledger
 - fixes in existing tests (all tests pass)

Recommendations for QA
 - test migration
 - test base functionality;;;","01/Jun/18 5:51 AM;ozheregelya;Test results are stored here: [https://docs.google.com/spreadsheets/d/1OVjua8JMwW7RhBWsdd9vSfGvJkgxWxKhEjB3T0yil1U/edit#gid=0] 
 (see verdicts for versions 1.3.433-1.3.446)

*Found issues:*
 --IS-740--: Indy-cli can't connect to the pool after changes in transactions format
 INDY-1379: Migration fails in case of upgrade to version with new transactions format
 IS-746: Support new format of replies on write transactions

*Summary:*
 General cases for indy-node were verified and work correctly. The only thing which was not tested yet is compatibility with current version of libindy. It will be checked during confirmation testing of IS-745 and IS-746.;;;",,,,,,,,,,,,,,,,,,,,,,,
Add configurable option to force periodic view change in pool,INDY-1349,30392,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,19/May/18 1:17 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,This is needed to increase probability of view change during load test in order to increase probability of appearance of related bugs,,,,,,,,,,,,,,,,,INDY-1350,,,,,,,,,,,,INDY-1303,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzz9uv:",,,,,,EV 18.10 Stability and VC,,,,,,,,1.0,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,"22/May/18 12:26 AM;sergey.khoroshavin;*PR*: https://github.com/hyperledger/indy-plenum/pull/686

*Changes*:
Added option _ForceViewChangeFreq_ to _config.py_ which can be:
- 0 (default): usual node behaviour
- non-zero: force view change with given period in seconds

Testing of this task can be performed in scope of [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350];;;",,,,,,,,,,,,,,,,,,,,,,,,
Perform load testing of 25-nodes pool with increased timeouts for catchups and viewchange with enabled periodic view change,INDY-1350,30393,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,19/May/18 1:20 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"During analysis of logs retrieved from reproduction of failing case from [INDY-1259|https://jira.hyperledger.org/browse/INDY-1259] with increased timeouts for catchup and viewchange it was found that pool went through view change successfully (but view change took maximum timeout set at that time - 30 minutes). Now we need:
1) determine minimal timeout for which probability of ending up in incorrect state is low enough
2) try to catch any other possible bugs in view change that have low probability of appearance",,,,,,,,,,INDY-1349,,,,,,,,,,,,,,,,,,,INDY-1259,,,,,INDY-1400,INDY-1404,INDY-1405,,,,,,,,,,,,,"01/Jun/18 6:13 AM;sergey.khoroshavin;process.yml;https://jira.hyperledger.org/secure/attachment/15034/process.yml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzcyf:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,3.0,,,,,,,,,,,,sergey.khoroshavin,spivachuk,VladimirWork,,,,,,,,,,"22/May/18 11:41 PM;sergey.khoroshavin;This should be done in two steps.

First one is to find load test parameters so that it can corrupt nodes state trie. In order to do this following things could be helpful:
1) enable periodic view change by adding _ForceViewChangeFreq=300_ into _/etc/indy/indy_config.py_
2) start load scripts with enough sustained load so that some nodes get corrupted state
3) corrupted state can be diagnosed by:
  - if some nodes stop at some txn and don't advance for more than 10 minutes they are under suspicion
  - if on some node _find /var/log/indy/sandbox/*.xz -exec xzcat {} \; | grep ""incorrect state trie""_ gives nonzero output it is certainly corrupted

Second step is to try to determine view change timeout parameters to minimize chance of ending up in corrupted state. In order to do this:
1) increase view change period to something like 600 seconds (there is very high probability that timeout will be around 300 seconds, and we don't want viewchanges to overlap)
2) ensure that state still gets corrupted with high probability
3) start increasing some (or all) of the following parameters:
- _MAX_CATCHUPS_DONE_DURING_VIEW_CHANGE = 5_
- _MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE = 15_
- _VIEW_CHANGE_TIMEOUT = 60_

Hopefully there will be some combination for which pool will be stable enough.;;;","24/May/18 9:27 PM;sergey.khoroshavin;After writing tests in scope of [INDY-1304|https://jira.hyperledger.org/browse/INDY-1350] it turned out that:
- increasing just _VIEW_CHANGE_TIMEOUT_ won't help
- increasing _MAX_CATCHUPS_DONE_DURING_VIEW_CHANGE_ most probably won't help
- increasing _MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE_ will help, but it should always be less than _VIEW_CHANGE_TIMEOUT_

So the following experiments are proposed (_MIN_TIMEOUT_CATCHUPS_DONE_DURING_VIEW_CHANGE_/_VIEW_CHANGE_TIMEOUT_):
- 30/60
- 60/120
- 120/240
- 300/600;;;","30/May/18 7:18 PM;sergey.khoroshavin;*Case 120/240*

Filtered logs from Node15 (which ended up with corrupted ledger):
{code}
2018-05-29 13:37:24.248000 | VIEW CHANGE: Node15 found master degraded after receiving instance change message from Node13
2018-05-29 13:37:24.249000 | VIEW CHANGE: Node15 initiating a view change to 7 from 6
2018-05-29 13:37:34.871000 |  Node15:0 ordered batch request, view no 6, ppSeqNo 8, ledger 1, state root Fzw6og2SMYNzVXxqXQbbHkDJzNkxLiuyvQqZLFCPzKYf, txn root 2UUjpU3Z1mvQcueSY34AZPtSWTQ1qpwVrbGn8XFBNdwT, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598980168603577), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980251888485), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974793269159), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980289180890), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980170907791), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980275875142), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974811390274), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980213381772), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974828292487), ('V4SGRU86Z58d6TV7PBUe6f', 1527598967875067662), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980307293384), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980219317282), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200523028), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980173395848), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980145479905), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980141102604), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980246254848), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980134782656), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974856945369), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980232964496), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974811724615), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980141027779), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974836951351), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226805241), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980191857831), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974814681692), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980139523688), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980190357107), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980211081979), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974835752695), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980246696533), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980293130952), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980228829098), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980247290459), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980265963867), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980263562611), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974819681939), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974852644095), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980264187869), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161168066), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980134151865), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980315921899), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980245404177), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980257095727), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980150969019), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974800579631), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226528726), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980154144477), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980241088671), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230417180), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980265708645), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974855886334), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974832789206), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974854932993), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974845791596), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975099630130), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980149878811), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974821779250), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974844775121), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980197526610), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980236163043), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980199680060), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980225143580), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980150498324), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980281785902), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980222591218), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975076798273), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980180772136), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980267752774), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980243356398), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980237994733), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980189599301), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161025250), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974853184294), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980254449807), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980194726164), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980311696136), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980137422851), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980184285098), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980183723239), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980213304723), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980279029171), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975135591754), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201198674), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980220067983), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974820136224), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980260326228), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974808225718), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980133217914)], discarded []
2018-05-29 13:38:00.966000 |  Node15:0 ordered batch request, view no 6, ppSeqNo 9, ledger 1, state root H4DmaS4maPD7NrdpEGVAPPKmDyNSDvXqXZ9uwx6QeiTN, txn root 7J4h83892wyKL4M4YFV2SjURew9JnnrBYo79gTo9UFD3, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598974828343406), ('V4SGRU86Z58d6TV7PBUe6f', 1527598967918719057), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226741359), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155144618), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155050844), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980141161016), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980287751388), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974817860020), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980140473612), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980144049583), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980261007849), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980151023707), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980154955911), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974813282371), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980130861557), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980199220857), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980231617477), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974856425533), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975008846716), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980169307422), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980160804603), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980238067759), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974792881780), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980274350180), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980165497032), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980288366003), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980204714824), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980187556367), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248175072), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295550109), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248007815), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980241391731), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980291346799), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980136922747), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192473528), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980191422002), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248901252), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980136464309), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980255912701), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980261474867), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974802079878), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980149661559), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980247110294), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200753955), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980220304016), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980225197029), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155097445), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980241896339), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980258537448), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980274768784), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248834112), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980165284883), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980288987564), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980263411085), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974823237156), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980277258994), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980154575921), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974844450581), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980134427394), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974771923260), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980227254932), ('V4SGRU86Z58d6TV7PBUe6f', 1527598967957263401), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974845859006), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201265190), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980169726802), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980292127802), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980195121060), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980205117233), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980294228594), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980231550838), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974817953078), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980284962747), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980279482594), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980277856236), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980175509629)], discarded []
2018-05-29 13:38:00.979000 |  Node15:0 ordered batch request, view no 6, ppSeqNo 10, ledger 1, state root EGoayNxqwWYeMR7J4ujqbRecCB6fBwrsPjrr24Lo68FN, txn root FEonAxTKaryDrBRPkGqpHvsj3tNb1NqzaFSNAUUdrkFr, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598980152664104), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980143435004), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980195491224), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200928646), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980137680107), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980309518083), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980145540135), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980261107198), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980314797801), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980168902509), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980225059539), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980139593545), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980249099359), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155190464), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230018856), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980270158710), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980244879617), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980313450125), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980277609202), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192998273), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980285421434), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980270727580), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980273890350), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980206955488), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980227858545), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980313898991), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226123843), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980211658918), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980139989547), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980202889784), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980220579550), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980182305537), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980311022292), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980263854204), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980140057325), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980207175777), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192812276), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161217007), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980237433725), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201319473), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980205186325), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161073735), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980131435755), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980153270534), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980259111703), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192755235), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980285636585), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980310504529), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295054370), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980252618894), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295373734), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980160924373), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980249413721), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980254529205), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980314201923), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980210040890), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980186955961), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201439376), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980292208857), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980254671376), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295446337), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980275044435), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980147104164), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980188519877), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230489029), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980315277956), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980164823008), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980231473735), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980281930383), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980314554224), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980316540584), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980194647229), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980313524282), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980259351436), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980253580458), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980219392263), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980244601476), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980278288691), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980173729358), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980240415583), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980289245933), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980285341992), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980259648565), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980229920098), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980209396674), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230557733), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980153649347), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980271511655), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980250204323), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980239174052), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980193622990), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980270871378), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980193949724), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155004230), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980284317324), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980237338748), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980238331692), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980274064744), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200206313), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980256092259)], discarded []
2018-05-29 13:38:01.671000 |  Node15:0 ordered batch request, view no 6, ppSeqNo 11, ledger 1, state root EBsbGMPoPdcLNRYDGNuL5pgyQWDZ5n5jkYU6d3LSjtrd, txn root 9CibPFLHwi9LXzH8dGrMFEvm2Y9MAye4VhNYeqFfaFnG, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598980183561579)], discarded []
2018-05-29 13:40:42.670000 | VIEW CHANGE: Node15 initiating a view change to 8 from 7
2018-05-29 13:40:43.893000 | CATCH-UP: Node15 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:41:24.263000 | VIEW CHANGE: Node15 sending an instance change with view_no 9 since View change could not complete in time
2018-05-29 13:44:42.671000 | VIEW CHANGE: Node15 sending an instance change with view_no 9 since View change could not complete in time
2018-05-29 13:44:42.705000 | VIEW CHANGE: Node15 initiating a view change to 9 from 8
2018-05-29 13:44:43.387000 | CATCH-UP: Node15 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:44:43.505000 | VIEW CHANGE: Node15:0 declares view change 9 as completed for instance 0, new primary is Node10:0, ledger info is [(0, 22, '5fh5nz5XPDJSzNW6hDfJwqp3NXnSjpeRt4AzAEGsSXKb'), (1, 42113, '4Zw2wLCLkYKokY3YFWnMhofRbkTrEgBbkVBHSPRCFnKG'), (2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn')]
{code}

Filtered logs from Node1 (with correct ledger):
{code}
2018-05-29 13:37:12.421000 | VIEW CHANGE: Node1 found master degraded after receiving instance change message from Node14
2018-05-29 13:37:12.600000 | VIEW CHANGE: Node1 initiating a view change to 7 from 6
2018-05-29 13:37:13.209000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:13.798000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:14.407000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:14.989000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:15.529000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:16.085000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:16.967000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:17.548000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:18.143000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:19.434000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:19.845000 |  Node1:0 ordered batch request, view no 6, ppSeqNo 8, ledger 1, state root Fzw6og2SMYNzVXxqXQbbHkDJzNkxLiuyvQqZLFCPzKYf, txn root 2UUjpU3Z1mvQcueSY34AZPtSWTQ1qpwVrbGn8XFBNdwT, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598980168603577), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980251888485), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974793269159), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980289180890), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980170907791), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980275875142), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974811390274), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980213381772), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974828292487), ('V4SGRU86Z58d6TV7PBUe6f', 1527598967875067662), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980307293384), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980219317282), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200523028), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980173395848), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980145479905), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980141102604), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980246254848), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980134782656), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974856945369), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980232964496), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974811724615), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980141027779), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974836951351), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226805241), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980191857831), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974814681692), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980139523688), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980190357107), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980211081979), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974835752695), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980246696533), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980293130952), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980228829098), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980247290459), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980265963867), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980263562611), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974819681939), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974852644095), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980264187869), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161168066), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980134151865), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980315921899), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980245404177), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980257095727), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980150969019), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974800579631), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226528726), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980154144477), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980241088671), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230417180), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980265708645), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974855886334), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974832789206), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974854932993), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974845791596), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975099630130), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980149878811), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974821779250), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974844775121), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980197526610), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980236163043), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980199680060), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980225143580), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980150498324), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980281785902), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980222591218), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975076798273), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980180772136), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980267752774), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980243356398), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980237994733), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980189599301), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161025250), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974853184294), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980254449807), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980194726164), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980311696136), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980137422851), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980184285098), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980183723239), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980213304723), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980279029171), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975135591754), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201198674), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980220067983), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974820136224), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980260326228), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974808225718), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980133217914)], discarded []
2018-05-29 13:37:20.265000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:20.854000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:21.422000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:21.978000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:22.571000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:24.867000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:25.466000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:26.023000 |  Node1:0 ordered batch request, view no 6, ppSeqNo 9, ledger 1, state root H4DmaS4maPD7NrdpEGVAPPKmDyNSDvXqXZ9uwx6QeiTN, txn root 7J4h83892wyKL4M4YFV2SjURew9JnnrBYo79gTo9UFD3, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598974828343406), ('V4SGRU86Z58d6TV7PBUe6f', 1527598967918719057), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226741359), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155144618), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155050844), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980141161016), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980287751388), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974817860020), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980140473612), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980144049583), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980261007849), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980151023707), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980154955911), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974813282371), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980130861557), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980199220857), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980231617477), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974856425533), ('V4SGRU86Z58d6TV7PBUe6f', 1527598975008846716), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980169307422), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980160804603), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980238067759), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974792881780), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980274350180), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980165497032), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980288366003), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980204714824), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980187556367), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248175072), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295550109), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248007815), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980241391731), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980291346799), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980136922747), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192473528), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980191422002), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248901252), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980136464309), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980255912701), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980261474867), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974802079878), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980149661559), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980247110294), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200753955), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980220304016), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980225197029), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155097445), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980241896339), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980258537448), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980274768784), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980248834112), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980165284883), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980288987564), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980263411085), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974823237156), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980277258994), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980154575921), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974844450581), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980134427394), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974771923260), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980227254932), ('V4SGRU86Z58d6TV7PBUe6f', 1527598967957263401), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974845859006), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201265190), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980169726802), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980292127802), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980195121060), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980205117233), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980294228594), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980231550838), ('V4SGRU86Z58d6TV7PBUe6f', 1527598974817953078), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980284962747), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980279482594), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980277856236), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980175509629)], discarded []
2018-05-29 13:37:26.263000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:26.874000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:27.483000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:27.580000 |  Node1:0 ordered batch request, view no 6, ppSeqNo 10, ledger 1, state root EGoayNxqwWYeMR7J4ujqbRecCB6fBwrsPjrr24Lo68FN, txn root FEonAxTKaryDrBRPkGqpHvsj3tNb1NqzaFSNAUUdrkFr, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598980152664104), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980143435004), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980195491224), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200928646), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980137680107), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980309518083), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980145540135), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980261107198), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980314797801), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980168902509), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980225059539), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980139593545), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980249099359), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155190464), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230018856), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980270158710), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980244879617), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980313450125), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980277609202), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192998273), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980285421434), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980270727580), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980273890350), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980206955488), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980227858545), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980313898991), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980226123843), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980211658918), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980139989547), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980202889784), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980220579550), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980182305537), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980311022292), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980263854204), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980140057325), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980207175777), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192812276), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161217007), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980237433725), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201319473), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980205186325), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980161073735), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980131435755), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980153270534), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980259111703), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980192755235), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980285636585), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980310504529), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295054370), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980252618894), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295373734), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980160924373), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980249413721), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980254529205), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980314201923), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980210040890), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980186955961), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980201439376), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980292208857), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980254671376), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980295446337), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980275044435), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980147104164), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980188519877), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230489029), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980315277956), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980164823008), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980231473735), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980281930383), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980314554224), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980316540584), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980194647229), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980313524282), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980259351436), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980253580458), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980219392263), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980244601476), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980278288691), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980173729358), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980240415583), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980289245933), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980285341992), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980259648565), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980229920098), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980209396674), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980230557733), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980153649347), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980271511655), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980250204323), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980239174052), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980193622990), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980270871378), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980193949724), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980155004230), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980284317324), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980237338748), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980238331692), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980274064744), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980200206313), ('V4SGRU86Z58d6TV7PBUe6f', 1527598980256092259)], discarded []
2018-05-29 13:37:27.590000 |  Node1:0 ordered batch request, view no 6, ppSeqNo 11, ledger 1, state root EBsbGMPoPdcLNRYDGNuL5pgyQWDZ5n5jkYU6d3LSjtrd, txn root 9CibPFLHwi9LXzH8dGrMFEvm2Y9MAye4VhNYeqFfaFnG, requests ordered [('V4SGRU86Z58d6TV7PBUe6f', 1527598980183561579)], discarded []
2018-05-29 13:37:28.867000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:37:47.565000 | VIEW CHANGE: Node1:0 declares view change 7 as completed for instance 0, new primary is Node8:0, ledger info is [(0, 22, '5fh5nz5XPDJSzNW6hDfJwqp3NXnSjpeRt4AzAEGsSXKb'), (1, 42074, '9CibPFLHwi9LXzH8dGrMFEvm2Y9MAye4VhNYeqFfaFnG'), (2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn')]
2018-05-29 13:40:42.639000 | VIEW CHANGE: Node1 initiating a view change to 8 from 7
2018-05-29 13:40:43.147000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:44:42.645000 | VIEW CHANGE: Node1 sending an instance change with view_no 9 since View change could not complete in time
2018-05-29 13:44:42.723000 | VIEW CHANGE: Node1 initiating a view change to 9 from 8
2018-05-29 13:44:43.219000 | CATCH-UP: Node1 completed catching up ledger 1, caught up 0 in total
2018-05-29 13:44:43.481000 | VIEW CHANGE: Node1:0 declares view change 9 as completed for instance 0, new primary is Node10:0, ledger info is [(0, 22, '5fh5nz5XPDJSzNW6hDfJwqp3NXnSjpeRt4AzAEGsSXKb'), (1, 42074, '9CibPFLHwi9LXzH8dGrMFEvm2Y9MAye4VhNYeqFfaFnG'), (2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn')]
{code};;;","30/May/18 7:33 PM;sergey.khoroshavin;It can be seen that:
- view change started rougly at same time on both correct and faulty node
- both nodes were ordering requests during view change _6->7_
- _Node15_ was ordering requests about 30 seconds late compared to _Node1_
- both nodes ordered same number of requests during view change
- _Node15_ started another view change without finishing view change _6->7_
- _Node1_ successfully finished view change _6->7_ with 42074 transactions in ledger
- both nodes failed to finish view change _7->8_ in time, triggering another view change _8->9_
- no requests were ordered during this period on both nodes, and apparently no requests were caught up
- both nodes finished view change _8->9_, but with different ledgers (_Node1_ still got 42074 transactions, but _Node15_ got 42113)

Further investigation will be done with relaxed filtering rules.;;;","01/Jun/18 6:22 AM;sergey.khoroshavin;[Filtered logs|https://drive.google.com/open?id=1ND-iQ9SYlKYNY0zE01wpNAvmXM7RbJ76] from 50 minutes before incorrect state trie to 10 minutes after + filter rules used. This is from timeout case 300/600 with DEBUG log level. 

Interesting findings up to date:
- lots of MESSAGE_REQUESTS and MESSAGE_RESPONSES for PREPARES (177k in 2 minutes on just one node)
- events when looper took more than 20 second to run nicely
- there were problems with state before incorrect state trie event, resulting in blacklisting nodes

Will continue investigation, hopefully now it will be much faster.;;;","02/Jun/18 2:42 AM;spivachuk;The issue with the incorrect state trie on Node16 is most likely caused by a broken order of applying / executing of 3PC-batches. There is the following message in the log of Node16 that indicates this:

{code}
2018-05-31 09:33:49.858000 | WARNING | 3PC: Node16: The first created batch has not been committed or reverted and yet another batch is trying to be committed, b'\xf1\x9e""rJ\x86\x82\xe6\xaa&\n\xf5\x80\xea\n/\x90\xfa\xf2\xf5=\x917#\xe1\xbamT\xf0\xbe\xd1\xbe' b'\xc1\xcfi\xab\xc8z \xd0\xb2I_~|\xa7\x04(\xeciR\x91\xcfQ8\xe0z2\x80\xdbT\x17\xe7%'
{code};;;","07/Jun/18 6:18 PM;VladimirWork;All cases from initial scope of this ticket are performed and logs are gathered. All issues found will be investiated and fixed in scope of INDY-1400.;;;",,,,,,,,,,,,,,,,,,
STN not accepting transactions with only one node down,INDY-1351,30408,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,19/May/18 7:16 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,TShirt_M,,,"The STN currently has 11 nodes, 7 of which are owned by Sovrin. When one node of our seven is brought down, the network fails to post transactions. We _should_ be well above consensus. An additional fact that confuses matters is that when we attempt to connect to the pool using the legacy CLI, it shows that it is connecting to nodes that are not currently part of the pool, but are now part of the live pool.  These nodes have all been demoted on this ledger.

Validator-info shows the correct pool nodes:
{code:java}
Validator england is running
Current time: Friday, May 18, 2018 9:57:57 PM
Validator DID: DNuLANU7f1QvW1esN3Sv9Eap9j14QuLiPeYzf28Nub4W
Verification Key: 5PFZeZLWxaH8LxumLkLKq9LbfDNiCNb2xXR2TrGxSbrHeyu6Pfd8Kan
Node Port: 9701/tcp on 0.0.0.0/0
Client Port: 9702/tcp on 0.0.0.0/0
Metrics:
Uptime: 1 minute, 0 seconds
Total Config Transactions: 501
Total Ledger Transactions: 593
Total Pool Transactions: 35
Read Transactions/Seconds: 0.00
Write Transactions/Seconds: 0.00
Reachable Hosts: 11/11
RFCU
VeridiumIDC
australia
brazil
canada
england
findentity
ibm
korea
singapore
virginia
Unreachable Hosts: 0/11
Software Versions:
indy-node: 1.3.57
sovrin: 1.1.9
{code}
If you look in the attached cli log file, you will see erroneous connections to nodes such as TNO. The strange behavior of the CLI is not the thrust of this ticket, it is only a strange symptom. The emphasis of the investigation should be why one node being up or down can prevent consensus.

This problem is repeatable on the STN. If you bring down any node, the pool does not achieve consensus. Korea was down at the time that these logs were obtained. When all seven of the sovrin-owned nodes are up, the pool is in consensus, and the CLI connects and acts normally.

Logs for the sovrin-owned validators are also included. Logs will be requested from our external stewards and will be attached as they are received.

*Acceptance Criteria*
* Diagnose the issue and create a Plan of Attack, including associated stories and epics that can be scheduled.
* If the problem proves to be a configuration issue, we can solve it immediately.",STN running 1.3.57,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/18 7:15 AM;mgbailey;australia.tgz;https://jira.hyperledger.org/secure/attachment/14998/australia.tgz","19/May/18 7:15 AM;mgbailey;brazil.tgz;https://jira.hyperledger.org/secure/attachment/14997/brazil.tgz","19/May/18 7:15 AM;mgbailey;canada.tgz;https://jira.hyperledger.org/secure/attachment/14996/canada.tgz","19/May/18 7:15 AM;mgbailey;cli.tgz;https://jira.hyperledger.org/secure/attachment/14994/cli.tgz","19/May/18 7:15 AM;mgbailey;england.tgz;https://jira.hyperledger.org/secure/attachment/14993/england.tgz","19/May/18 7:15 AM;mgbailey;korea.tgz;https://jira.hyperledger.org/secure/attachment/14992/korea.tgz","19/May/18 7:15 AM;mgbailey;singapore.tgz;https://jira.hyperledger.org/secure/attachment/14991/singapore.tgz","19/May/18 7:15 AM;mgbailey;virginia.tgz;https://jira.hyperledger.org/secure/attachment/14990/virginia.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcyn:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,mgbailey,sergey.khoroshavin,,,,,,,,,"21/May/18 11:12 PM;esplinr;[~mgbailey]: We plan to take this into the next sprint. Can you provide some additional information for all nodes?
* results from validator_info
* log files
* journal_ctl;;;","25/May/18 5:57 PM;ashcherbakov;[~mgbailey]
can you please also give us `pool_ledger` results;;;","29/May/18 7:12 PM;sergey.khoroshavin;Results of analyzing all _send NYM_ found in _cli.log_ that were run against STN:

*#1*
{code:java}
2018-05-14 20:26:04,329 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=QDpRcG3NzCdebPYMfGHLCu verkey=~64fSD6sumsMw7cbXEvjx6e
{code}
According to client logs it had reqId _1526329564330712_, was received by pool but was _not ordered_, although pool was sending REQACK for more than a minute. Unfortunatelly it's impossible to find out what happened in pool because logs from pool start from 2018-05-17.

*#2*
{code:java}
2018-05-14 21:01:40,952 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=QDpRcG3NzCdebPYMfGHLCv verkey=~64fSD6sumsMw7cbXEvjx6e
{code}
According to client logs it had reqId _1526331700953882_, was received by pool but was _not ordered_ before entering _exit_ command, which happened just 15 seconds later. As with #1 it's impossible to find out what happened in pool because logs from pool start from 2018-05-17.

*#3*
{code:java}
2018-05-14 21:44:31,307 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=QDpRcG3NzCdebPYMfGHLCw verkey=~64fSD6sumsMw7cbXEvjx6e
{code}
According to client logs it had reqId _1526334271308655_ and was _ordered_. Although pool logs start much later there was one trace of this transaction in them, saying that this request was freed from previous checkpoints, which means that request was ordered in past and after some time volatile data was garbage collected (but request stayed in ledger).

*#4*
{code:java}
2018-05-16 19:05:56,937 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=QZ6oA5DRYeXcTjZ9wcz6W6 verkey=~3fSg4QSYqib6m5k1uYTM6J
{code}
According to client logs it had reqId _1526497556938749_ and was _ordered_. Pool logs are as with #3.

*#5*
{code:java}
2018-05-17 21:10:08,361 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=K7nyddFfeig8TZyywTGdBP verkey=~8zxeohE2qitTcN116TrS8e
{code}
According to client logs it had reqId _1526591408362386_ and was _ordered_. Pool logs confirm that:
{code:java}
2018-05-17 21:10:08,908 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(australia:0) australia:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
2018-05-17 21:10:08,929 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(brazil:0) brazil:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
2018-05-17 21:10:08,883 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(canada:0) canada:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
2018-05-17 21:10:08,905 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(england:0) england:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
2018-05-17 21:10:08,902 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(korea:0) korea:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
2018-05-17 21:10:08,887 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(singapore:0) singapore:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
2018-05-17 21:10:08,875 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(virginia:0) virginia:0 ordered batch request, view no 2, ppSeqNo 1, ledger 1, state root GBKDV9XmGVgqEBkwkKYD9gUwULJXMZA5dxyYf6iWwiPo, txn root HHgwcMagafGrVur8DBNLgawXisiCq2adnv6gWvedgmgt, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526591408362386)], discarded []
{code}
*#6*
{code:java}
2018-05-18 15:21:19,101 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=QCBRocePuf5Uh25t2gvxVF verkey=~5XLxgQMj3TGF4NDinf5zN
{code}
According to client logs it had reqId _1526656879102446_ and was _ordered_. Pool logs confirm that:
{code:java}
2018-05-18 15:37:34,910 | DEBUG    | node.py              (1999) | processRequest | australia received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'verkey': '~5XLxgQMj3TGF4NDinf5zN7', 'type': '1', 'dest': 'QCBRocePuf5Uh25t2gvxVF'}, 'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n', 'reqId': 1526656879102446, 'identifier': '3U8HUen8WcgpbnEz1etnai'} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'
2018-05-18 15:37:34,919 | DEBUG    | node.py              (1999) | processRequest | brazil received client request: SafeRequest: {'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n', 'operation': {'dest': 'QCBRocePuf5Uh25t2gvxVF', 'verkey': '~5XLxgQMj3TGF4NDinf5zN7', 'type': '1'}, 'protocolVersion': 1, 'identifier': '3U8HUen8WcgpbnEz1etnai', 'reqId': 1526656879102446} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'
2018-05-18 15:37:34,860 | DEBUG    | node.py              (1999) | processRequest | canada received client request: SafeRequest: {'operation': {'type': '1', 'verkey': '~5XLxgQMj3TGF4NDinf5zN7', 'dest': 'QCBRocePuf5Uh25t2gvxVF'}, 'protocolVersion': 1, 'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n', 'reqId': 1526656879102446, 'identifier': '3U8HUen8WcgpbnEz1etnai'} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'
2018-05-18 15:37:34,900 | DEBUG    | node.py              (1999) | processRequest | england received client request: SafeRequest: {'identifier': '3U8HUen8WcgpbnEz1etnai', 'reqId': 1526656879102446, 'operation': {'dest': 'QCBRocePuf5Uh25t2gvxVF', 'type': '1', 'verkey': '~5XLxgQMj3TGF4NDinf5zN7'}, 'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n', 'protocolVersion': 1} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'
2018-05-18 15:37:34,895 | DEBUG    | node.py              (1999) | processRequest | korea received client request: SafeRequest: {'reqId': 1526656879102446, 'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n', 'protocolVersion': 1, 'operation': {'verkey': '~5XLxgQMj3TGF4NDinf5zN7', 'dest': 'QCBRocePuf5Uh25t2gvxVF', 'type': '1'}, 'identifier': '3U8HUen8WcgpbnEz1etnai'} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'
2018-05-18 15:37:34,913 | DEBUG    | node.py              (1999) | processRequest | singapore received client request: SafeRequest: {'identifier': '3U8HUen8WcgpbnEz1etnai', 'protocolVersion': 1, 'operation': {'verkey': '~5XLxgQMj3TGF4NDinf5zN7', 'dest': 'QCBRocePuf5Uh25t2gvxVF', 'type': '1'}, 'reqId': 1526656879102446, 'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n'} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'
2018-05-18 15:37:34,856 | DEBUG    | node.py              (1999) | processRequest | virginia received client request: SafeRequest: {'operation': {'verkey': '~5XLxgQMj3TGF4NDinf5zN7', 'type': '1', 'dest': 'QCBRocePuf5Uh25t2gvxVF'}, 'identifier': '3U8HUen8WcgpbnEz1etnai', 'signature': 'EcH7bEDjMieexXpyUSvbnmpUuXapR9QgFgF75d5c5W6eP1H8dkCSbk6gamwkc3WaWnF6aARxszufGL86g47vP1n', 'reqId': 1526656879102446, 'protocolVersion': 1} from b'B?9LSJnT#gd1K/*0npJmEE1aoKPj6Qtcr8!IVg$n'

2018-05-18 15:37:35,451 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(australia:0) australia:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
2018-05-18 15:37:35,512 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(brazil:0) brazil:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
2018-05-18 15:37:35,460 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(canada:0) canada:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
2018-05-18 15:37:35,485 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(england:0) england:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
2018-05-18 15:37:35,419 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(korea:0) korea:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
2018-05-18 15:37:35,446 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(singapore:0) singapore:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
2018-05-18 15:37:35,456 | INFO     | replica.py           (1644) | order_3pc_key | REPLICA:(virginia:0) virginia:0 ordered batch request, view no 0, ppSeqNo 1, ledger 1, state root tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm, txn root 33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9, requests ordered [('3U8HUen8WcgpbnEz1etnai', 1526656879102446)], discarded []
{code}
Also it seems that time on client and in pool was off by more than 15 minutes.

*#7*
{code:java}
2018-05-18 19:31:39,672 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=S1JyATGKWBg8WvoVydsbmW verkey=~9oe3weBSuwTwnTELUVi7Ai
{code}
According to client logs there were no replies at all from pool. Also there are no traces of this request in pool:
{code:java}
$ ls
australia.log  brazil.log  canada.log  cli.log  england.log  korea.log  singapore.log  virginia.log

$ find *.log -exec cat {} \; | grep ""S1JyATGKWBg8WvoVydsbmW""
2018-05-18 19:31:39,672 | INFO     | cli.py               (1965) | parse | CLI command entered: send NYM dest=S1JyATGKWBg8WvoVydsbmW verkey=~9oe3weBSuwTwnTELUVi7Ai
{code}
although for most nodes logs were available for much later time:
{code:java}
$ find *.log -exec echo {} \; -exec tail -n 1 {} \;
australia.log
2018-05-18 21:27:39,236 | DEBUG    | message_processor.py (  29) | discard | australia discarding message deque([b'po']) because CONNECTION: rid b'mCCh!JxOp$OwLp#m{bh)FPMMB:Q*m*DLSZ&*e1>4' no longer available
brazil.log
2018-05-18 21:27:39,923 | DEBUG    | message_processor.py (  29) | discard | brazil discarding message deque([b'po']) because CONNECTION: rid b'mCCh!JxOp$OwLp#m{bh)FPMMB:Q*m*DLSZ&*e1>4' no longer available
canada.log
2018-05-18 21:27:39,404 | DEBUG    | message_processor.py (  29) | discard | canada discarding message deque([b'po']) because CONNECTION: rid b'mCCh!JxOp$OwLp#m{bh)FPMMB:Q*m*DLSZ&*e1>4' no longer available
cli.log
2018-05-18 20:13:14,065 | INFO     | looper.py            ( 282) | shutdown | Looper shut down in 0.016 seconds.
england.log
2018-05-18 21:27:39,141 | DEBUG    | message_processor.py (  29) | discard | england discarding message deque([b'po']) because CONNECTION: rid b'mCCh!JxOp$OwLp#m{bh)FPMMB:Q*m*DLSZ&*e1>4' no longer available
korea.log
2018-05-18 16:07:08,119 | DEBUG    | looper.py            ( 285) | shutdown | Unsetting handler for SIGTERM
singapore.log
2018-05-18 21:27:40,406 | DEBUG    | message_processor.py (  29) | discard | singapore discarding message deque([b'po']) because CONNECTION: rid b'mCCh!JxOp$OwLp#m{bh)FPMMB:Q*m*DLSZ&*e1>4' no longer available
virginia.log
2018-05-18 21:27:39,109 | DEBUG    | message_processor.py (  29) | discard | virginia discarding message deque([b'po']) because CONNECTION: rid b'mCCh!JxOp$OwLp#m{bh)FPMMB:Q*m*DLSZ&*e1>4' no longer available
{code}
This means that unless client had very large local time offset (which is highly unlikely I hope) the only reason for this is that pool did not receive request at all, not even single node. I'm going to investigate logs from pool during this time period.

 

As a side note - it would be very helpful if further bug reports like these (STN not accepting transactions) contained not only logs but also some info about time when problem occured so we can better focus on problem.;;;","29/May/18 8:14 PM;sergey.khoroshavin;Further log analysis around _2018-05-18 19:31:39_ showed that there was request posted to pool (although it had destination different from ones found in _cli.log_). I'm posting here filtered logs from _australia_ (they are the most interesting since _australia_ was primary on master instance), but logs from other nodes contain very similar information (they received same request and same number of _PREPARE_):
{code}
2018-05-18 19:36:44.475000 | australia | australia received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'verkey': '~Uo9bXMbYHPTzZtpnwTdapL', 'type': '1', 'dest': 'KypizRsg73qrX75NPFY6H2'}, 'signature': 'AkmVpx5tVMrSgaoVaQ6Wb8A74foMKaAJMTdVmdwWw5zLfBWq44Z5FRk5CWSCYXqxFNVbxjPijPak3mFCou453LU', 'reqId': 1526671235863619430, 'identifier': 'RtMtkBrkHCEDtGXXqRfGoV'} from b'[Sa3Q.==%.kjGRpmeuAE3U(@x^}XcXq]Y{fK%Fr+'
2018-05-18 19:36:44.518000 | australia | australia sending message PREPREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'blsMultiSig': ['RLjZ1GGzWfQSRSc45sxtgcGcxKrAsKn8KBeFxdgscCAqhyVT7vfRLr6bdLSpLvNFtxi69vzMVTfmeQ5DjwticaWDbqT3W7nj83SDmmxwWsvFKNH6MRPhtTKHbaY62k1XLfMzYoGwtfv9XHQWLBAVtSaVXumyWPW6QwRE9iKf4uBskB', ['ibm', 'canada', 'virginia', 'singapore', 'england', 'korea', 'australia', 'brazil'], [1, 'tY6WXMe5e87ZmTt1u8CWC3VkkDEN4FCfBA9ZC95UyMm', 'GzjYafMVAJBDujzjXMmKtwxzxHQpmz1U9xt1eaB8p5ge', '33Bpy7rZZDa3DnSF6kvr5om3La19JougCiu1mBBHmvy9', 1526657854]], 'discarded': 1, 'reqIdr': [('RtMtkBrkHCEDtGXXqRfGoV', 1526671235863619430)], 'ledgerId': 1} to all recipients: ['canada', 'ibm', 'korea', 'VeridiumIDC', 'virginia', 'findentity', 'RFCU', 'england', 'singapore', 'brazil']
2018-05-18 19:36:44.756000 | australia | australia received node message from ibm: PREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4'}
2018-05-18 19:36:44.763000 | australia | australia received node message from singapore: PREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4'}
2018-05-18 19:36:44.779000 | australia | australia received node message from virginia: PREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4'}
2018-05-18 19:36:44.797000 | australia | australia received node message from canada: PREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4'}
2018-05-18 19:36:44.862000 | australia | australia received node message from england: PREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4'}
2018-05-18 19:36:44.903000 | australia | australia received node message from brazil: PREPARE{'txnRootHash': 'F7wzzYTPRZ3dyWMmrpgJnMkfXc7GEng1eGvjD9RBwg8W', 'viewNo': 0, 'instId': 0, 'ppSeqNo': 2, 'digest': 'c95900a51c10cbcfa487c19d99b90bd5af5588402b8978d907da3f0fcb080afb', 'ppTime': 1526672204, 'stateRootHash': '9fNC4BboZcD7in2Cjj3ZAMSbHreM9XAwUQZ1E6a24uJ4'}
{code}

What can be seen here is that _PREPREPARE_ for received request was generated and sent to all other nodes. Also 6 _PREPAREs_ were were received, which is insufficient for achieving prepare certificate and issuing _COMMIT_ (in fact 7 were required). Nodes that did not send _PREPAREs_ were _korea_ (which was down), _VeridiumIDC_, _findentity_ and _RFCU_. 

All in all this means that when _korea_ was online pool had just minimum number of nodes to process requests and turning _korea_ down reduced number participating of nodes below this minimum, so it's expected that STN stopped accepting transactions.;;;","30/May/18 2:01 AM;sergey.khoroshavin;Further investigation of CLI logs showed more things.

*#1* Regarding connection to TNO. This is a _bug_ in CLI. During initial pool ledger catchup there was _CATCHUP_REP_ from _england_ containing transaction where _TNO_ is indeed validator node:
{code}
'23': {'identifier': 'FzUUYiVKCDnSWd77NHfhpZ', 'dest': 'TZxmZoXwNk1X5o48pXqbDFz6mTJT5QkiRme9z5p86KQ', 'txnTime': 1521195244, 'type': '0', 'signatures': None, 'reqId': 1521195244613677, 'signature': '3Q3ijuEPihRyGsZFvmb414AWWQ7iMskCHoP7bC14FgPesNdPNY3pdfddEfPf5FrCg4wzdvdAFjKaBunHHwXCG4V3', 'data': {'node_port': 9701, 'node_ip': '134.221.127.143', 'client_ip': '134.221.127.143', 'services': ['VALIDATOR'], 'alias': 'TNO', 'blskey': '37d7DmcwGWM7yfnpwLGzwVy6zZwoc6cAgeeSJFBWbVh6jq5tP8dPf7s2XDxxtWafmr1JdyzycBcNztEsE8Uf9qX2jRoXzhCnjEEYJCAByEn5hWC2VQ9EqkuKzq28Vob7Piof7rEJeUPxuBZtrXL1khyTN2waQtix6CYtv9QejNPZVJ2', 'client_port': 9702}}
{code}
but there was another _CATCHUP_REP_ from _singapore_ containing later transaction in which _TNO_ was demoted:
{code}
'29': {'identifier': '6feBTywcmJUriqqnGc1zSJ', 'dest': 'TZxmZoXwNk1X5o48pXqbDFz6mTJT5QkiRme9z5p86KQ', 'txnTime': 1522073964, 'type': '0', 'signatures': None, 'reqId': 1522073964061797, 'signature': 'xzjjfKfJJJeQxZb5WhMYyHDcDG5dZCAvsYmBh6FtP9J1ckQUazaJ7AC2ksHzjARW9kkcFBS4B1M4R7y5Bc3BN5S', 'data': {'services': [], 'alias': 'TNO'}}
{code}
I checked two different sessions (#6 and #7 from previous comment) and in both of them there was a connection attempt, although only in #7 it was successful.

*#2* One more thing regarding losing consensus. According to _CONSISTENCY_PROOFs_ received by client:
{code}
2018-05-18 19:26:25,442 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node virginiaC: {'ppSeqNo': 0, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 7, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'ledgerId': 0, 'op': 'CONSISTENCY_PROOF'}

2018-05-18 19:26:25,457 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node canadaC: {'ppSeqNo': 0, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 7, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'ledgerId': 0, 'op': 'CONSISTENCY_PROOF'}

2018-05-18 19:26:25,952 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node englandC: {'ppSeqNo': 0, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 7, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'ledgerId': 0, 'op': 'CONSISTENCY_PROOF'}

2018-05-18 19:26:26,039 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node singaporeC: {'ppSeqNo': 0, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 7, 'op': 'CONSISTENCY_PROOF', 'ledgerId': 0, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87']}

2018-05-18 19:26:26,039 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node australiaC: {'ppSeqNo': 0, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 7, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'ledgerId': 0, 'op': 'CONSISTENCY_PROOF'}

2018-05-18 19:26:26,175 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node brazilC: {'ppSeqNo': 0, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 7, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'ledgerId': 0, 'op': 'CONSISTENCY_PROOF'}

2018-05-18 19:26:32,584 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node RFCUC: {'ppSeqNo': 6, 'oldMerkleRoot': 'HrTzhpj2xhPZsCoHx5FvMpa16DxvY947QAHQTaz6eXmi', 'viewNo': 1, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 23, 'hashes': ['3GpAThdsKFtjHBqZEdJKGEYt8KdMrbqZyZkEPTBqMsoF', 'DEMjYAfHYJcn7QSyYV9cUPJruQ71CRZZFHpcKZAuvXBH', '5WaTGgtS1coqKcbJeR7EkWAyKbzD4coCBHAQqHDTLnFd', '362orio8KomXSEm9MRJXVRB74VADABfxrEU5xQ5gvpQK', 'FHQoUKKNVh1Cyi6gLAUvZtEKcnMQBgmprq1RuKUZvfcN', 'HTAkRXkbYyiLKE1FJoi7TLefLxN8WnEkSb2Hb3YLTUrB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'ledgerId': 0, 'op': 'CONSISTENCY_PROOF'}

2018-05-18 19:26:32,827 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client CqviWMMFNdi5nSn1UKz6LHVmV3LSw1H8WgHMaTLdZpDg got msg from node ibmC: {'ppSeqNo': 0, 'oldMerkleRoot': 'HrTzhpj2xhPZsCoHx5FvMpa16DxvY947QAHQTaz6eXmi', 'viewNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'seqNoEnd': 35, 'seqNoStart': 23, 'op': 'CONSISTENCY_PROOF', 'ledgerId': 0, 'hashes': ['3GpAThdsKFtjHBqZEdJKGEYt8KdMrbqZyZkEPTBqMsoF', 'DEMjYAfHYJcn7QSyYV9cUPJruQ71CRZZFHpcKZAuvXBH', '5WaTGgtS1coqKcbJeR7EkWAyKbzD4coCBHAQqHDTLnFd', '362orio8KomXSEm9MRJXVRB74VADABfxrEU5xQ5gvpQK', 'FHQoUKKNVh1Cyi6gLAUvZtEKcnMQBgmprq1RuKUZvfcN', 'HTAkRXkbYyiLKE1FJoi7TLefLxN8WnEkSb2Hb3YLTUrB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87']}
{code}
_RFCU_ had later viewNo compared to majority of nodes, and there were no responses (and no connection attempts to) _VeridiumIDC_, _findentity_. I suspect that majority of nodes controlled by Sovrin were restarted, leading to problem described in [INDY-1199|https://jira.hyperledger.org/browse/INDY-1199].;;;","30/May/18 2:06 AM;sergey.khoroshavin;*Summary*
1. STN lost consensus after shutting down just one node because it had minimum required amount of participating nodes.
2. Some nodes were not participating because Sovrin nodes were restarted, and others not, leading to already known problem [INDY-1199|https://jira.hyperledger.org/browse/INDY-1199]
3. Attempt to connect to TNO and other demoted nodes is bug in CLI and will be addressed in separate issue.
4. It is still unknown why transaction logged in CLI didn't get to pool at all, and what was origin of another transaction that did get to pool roughly at same time;;;","30/May/18 2:33 AM;mgbailey;Thanks for the analysis, [~sergey.khoroshavin]. The key info here is that we now know that 3 of the external steward nodes were not functioning properly, so we can look at those in more depth to see why. I am looking forward to upcoming logging enhancements where information like this will be easy to find.;;;","30/May/18 6:14 PM;sergey.khoroshavin;[~mgbailey] I looked a bit more at CLI logs, and I can see that in previous session (#6) more nodes reported their status:
{code}
2018-05-18 15:16:11,459 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node canadaC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'op': 'CONSISTENCY_PROOF', 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'ppSeqNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0}
2018-05-18 15:16:11,486 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node virginiaC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'op': 'CONSISTENCY_PROOF', 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0, 'ppSeqNo': 0}
2018-05-18 15:16:11,824 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node koreaC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'ppSeqNo': 0, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0, 'op': 'CONSISTENCY_PROOF'}
2018-05-18 15:16:11,982 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node englandC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'op': 'CONSISTENCY_PROOF', 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'ppSeqNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0}
2018-05-18 15:16:12,489 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node brazilC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'op': 'CONSISTENCY_PROOF', 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0, 'ppSeqNo': 0}
2018-05-18 15:16:12,490 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node singaporeC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'ppSeqNo': 0, 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0, 'op': 'CONSISTENCY_PROOF'}
2018-05-18 15:16:12,490 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node australiaC: {'ledgerId': 0, 'seqNoStart': 7, 'oldMerkleRoot': 'AWJrEjCRsAvXEJryWWGcpDuKYWop5yfgAsD6PhVFNU2a', 'op': 'CONSISTENCY_PROOF', 'hashes': ['6dTvY3hdgwyf9QgrwwHda82w4io7L1XSHiS4z4aUuRvi', '6PiBHFqi1Ggzu1vqg5wFuXRnRDknqKsR85ipQvS1TTii', '9Gxsi1LUfPoS5QS5CVPqhF4qb7B2ETFDqYPwCM1Bo24p', 'Hx7wWN6N7RHFPSGcaRc2mszECpV4W9ZEUoeqhBoZxqjC', '2BU2FYa5cjSvrZZ6TzjHPLhJgXjMug7mJUwgBWwKaPoW', 'BzdDypK9W4vhhPLQCaujx2CteCjwp1dNujSeuuPGHJiB', 'GQg9SVd4oRariiESNo2jiG4NVeS9Y2TEDquByrmiKm87'], 'seqNoEnd': 35, 'ppSeqNo': 0, 'newMerkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 0}
2018-05-18 15:16:18,567 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node ibmC: {'ledgerId': 0, 'ppSeqNo': None, 'op': 'LEDGER_STATUS', 'txnSeqNo': 35, 'viewNo': None, 'merkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk'}
2018-05-18 15:16:18,778 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node RFCUC: {'ledgerId': 0, 'ppSeqNo': 6, 'op': 'LEDGER_STATUS', 'txnSeqNo': 35, 'merkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 1}
2018-05-18 15:16:19,509 | INFO     | client.py            ( 346) | handleOneNodeMsg | Client GEDD6TWU5n5BVgGt6R2vqZvjMhBs3uXwmWvjyVWGuBou got msg from node VeridiumIDCC: {'ledgerId': 0, 'ppSeqNo': 6, 'op': 'LEDGER_STATUS', 'txnSeqNo': 35, 'merkleRoot': 'EqNHLGkrjeiD9hFU7xrX1VRELW3MGYEsKYVzVvkVuuAk', 'viewNo': 1}
{code}
As you can see the majority of pool is at viewNo 0, but _RFCU_ and _VeridiumIDC_ are at viewNo 1. Most likely this happened due to restart of Sovrin nodes, and I believe these two nodes can be brought back to consensus by restarting them as well.

All in all, I think this is a known problem [INDY-1199|https://jira.hyperledger.org/browse/INDY-1199], and until it is solved general recommendation is that _nodes should be restarted either one by one or all at once, otherwise some nodes can stop participating because of inability to downgrade their viewNo, and if this still happened it should be solved by restarting nodes with higher viewNo_.

Also, to diagnose this kind of problem you can look at _CONSISTENCY_PROOF_ and _LEDGER_STATUS_ messages received by client during connection as they contain info about current viewNo on nodes. It should be same everywhere, but if logs show that on some minority of nodes it's higher than on others then these nodes should be restarted. To make this easier following command can be used:
{code}
egrep ""CONSISTENCY_PROOF|LEDGER_STATUS"" cli.log | grep viewNo
{code};;;","06/Jun/18 11:55 PM;esplinr;[~sergey.khoroshavin] Please also create an issue in the Indy SDK project for:

{quote}
3. Attempt to connect to TNO and other demoted nodes is bug in CLI and will be addressed in separate issue.
{quote}

And link it to this issue. Thank you.;;;",,,,,,,,,,,,,,,,
Performance,INDY-1352,30439,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,gudkov,gudkov,21/May/18 10:32 PM,11/Oct/19 10:08 PM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-4,,Performance,Done,,,,,,,"1|hzyvfr:",,,,,,,,,,,,,,,,,,,,,,,,,,gudkov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a Steward, I need to be able to configure iptables (or other firewall) so that client connections can be closed on timeouts",INDY-1353,30471,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,sergey-shilov,ashcherbakov,ashcherbakov,22/May/18 8:02 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,"ZMQ (and hence Indy-Node) can not drop client connections from the server side if the client doesn’t close it on her side.
# https://jira.hyperledger.org/browse/INDY-1087
# https://jira.hyperledger.org/browse/INDY-1252
# https://github.com/zeromq/libzmq/issues/2877
So, If there are a lot of open connections from clients to pool, then the pool will not be able to accept any new clients/connections.

We need to define firewall rules (use iptables as a reference) to somehow deal with open connections problem.
A possible problem is that Stewards can use any firewalls. But we should at least give some recommendations and get them aware of the issue.

*Acceptance criteria*
# Instructions for Stewards
# Rules for iptables (or other firewall).
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzwxxr:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"27/Jun/18 10:04 PM;ashcherbakov;According to https://jira.hyperledger.org/browse/INDY-1417, it's not possible to do so...
We need to implement a solution defined in the scope of INDY-1417;;;",,,,,,,,,,,,,,,,,,,,,,,,
use of exec() function and executable metadata file allow for possible remote code execution,INDY-1354,30494,,Bug,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,brentzundel,brentzundel,23/May/18 12:49 AM,12/Oct/18 9:49 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"A security audit of the indy-plenum code revealed a possible malicious code insertion point via the __metadata__.py file and its execution through the exec() command in  setup.py.

It is recommended that the __metadata__.py file be replaced by a metadata.json file, and the use of a json parser to bring the metadata values into a dictionary that can be consumed by setup.py.

 

The same vulnerability exists in indy-plenum and indy-node.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzbnr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,brentzundel,,,,,,,,,,,"12/Oct/18 6:32 PM;ashcherbakov;This is already fixed.;;;","12/Oct/18 9:17 PM;brentzundel;I am not sure I understand. I still see an exec() function in setup.py;;;","12/Oct/18 9:49 PM;ashcherbakov;Yes, sorry, it's not fixed yet, thanks for the notice.;;;",,,,,,,,,,,,,,,,,,,,,,
Support all missing writing Indy txns in load script,INDY-1355,30524,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,23/May/18 6:04 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"1. Credential definition writing.
2. Registry definition revocation.
3. Registry entry revocation. (!) - not ""update""
",,,,,,,,,,IS-780,,,,,,,INDY-1343,INDY-2214,,,,,,,,,,,INDY-1378,,,,,,,,,,,,,,,,,,,,"21/Jun/18 1:02 AM;VladimirWork;INDY-1355.PNG;https://jira.hyperledger.org/secure/attachment/15129/INDY-1355.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1368,,,,,,,,,"1|hzzcof:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"31/May/18 12:59 AM;VladimirWork;1. Credential definition writing - implemented as ""cred_def"" and waits for tag uniqueness fix.
2. Registry definition revocation - implemented as ""revoc_reg_def"".
3. Registry entry revocation - implemented as ""revoc_reg_entry"" and waits for INDY-1378 fix since now this txn works correct until the first failed writing.;;;","21/Jun/18 1:02 AM;VladimirWork;Build Info:
indy-node 1.4.469

Actual Results:
All writing txn are implemented and checked against latest master pool / libindy / python wrapper (ErrorCode.AnoncredsRevocationRegistryFullError will be fixed in scope of INDY-1378).
 !INDY-1355.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,,
Support all reading Indy txns in load script,INDY-1356,30525,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,VladimirWork,VladimirWork,23/May/18 6:08 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"1. DID reading.
2. Credential schema reading.
3. Credential definition reading.
4. Registry definition revocation reading.
5. Registry revocation reading. (!) - not listed in initial ticket
6. Registry revocation delta reading.
7. Attrib reading.",,,,,,,,,,,,,,,,,INDY-1343,INDY-1378,INDY-2214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1368,,,,,,,,,"1|hzzcmv:",,,,,,EV 18.12 Release RocksDB,,,,,,,,5.0,,,,,,,,,,,,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,,,,"22/Jun/18 7:47 PM;ozheregelya;*Environment:*
indy-node 1.4.472
libindy 1.4.0~591

**Following requests were implemented and tested:
|Txn kind|Result|
|get_nym|pass|
|get_attrib|pass|
|get_schema|pass|
|get_cred_def|pass|
|get_revoc_reg_def|pass|
|get_revoc_reg_entry|pass|
|get_revoc_reg_delta|pass|
|Read mix|pass|

Documentation: https://github.com/hyperledger/indy-node/blob/master/docs/process-based-load-script.md;;;",,,,,,,,,,,,,,,,,,,,,,,,
"Implement ""mixture"" mode in load script or by cron",INDY-1357,30526,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,23/May/18 6:08 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"We need to implement ""mixture"" mode in load script or by cron to run mix of txns with predefined % of each txn type easily.",,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1368,,,,,,,,,"1|hzzcjb:",,,,,,EV 18.12 Release RocksDB,,,,,,,,2.0,,,,,,,,,,,,dsurnin,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,,,"29/May/18 10:53 PM;zhigunenko.dsr;Current algorithm does not take into account the probability of a certain type of transaction.

*Option A:*
 After creating all transaction types
{code:java}
    async def gen_random(self):
        reqs = [self.gen_attrib, self.gen_nym, self.gen_schema]
        idx = random.randint(0, 1000) % len(reqs)
        return await reqs[idx]()
{code}
this function should be replaced by code like this
{code:java}
import random

probabilities_w = {'writing credentials schema': 5,
                   'writing credential definition': 5,
                   'revoke registry definition': 5,
                   'revoke registry update': 5,
                   'write DID to ledger': 20,
                   'write payment to ledger': 45,
                   'write attrib to ledger': 15}

probabilities_r = {'read DID from ledger': 45,
                   'read credential schema': 10,
                   'read credential definition': 10,
                   'read revoke registry definition': 10,
                   'read revoke registry delta': 10,
                   'read attrib from ledger': 10,
                   'read payment balance from ledger': 5}


def get_action(action_probabilities):
    total = 0
    for key, value in action_probabilities.items():
        total += value
    choice = random.randint(1, total)

    for key, value in action_probabilities.items():
        if value >= choice:
            return key
        else:
            choice -= value
{code}

For example, reqs array has filling once during script initialization relying on the dict content (each transaction type repeated in array appropriate number of times)

*Option B:* 
modify -k parameter parser to set certain quantity of each transactions type
(args file would save all parameters of corresponding run)

*Remark from Dmitry S:*
it's better to move request generator into separate class;;;","22/Jun/18 6:16 PM;dsurnin;mixed mode implemented with a help of -k parameter - now it can accept complex test description in form of JSON.

For detailed description please check the https://github.com/hyperledger/indy-node/blob/master/docs/process-based-load-script.md;;;","22/Jun/18 7:51 PM;ozheregelya;Command for writing:
{code:java}
python3.5 perf_processes.py -n 1 -t 1 -c 1 -r 1 -g ./pool_transactions_genesis -k ""[{\""schema\"":{\""count\"": 5}}, {\""cred_def\"":{\""count\"": 5}}, {\""revoc_reg_def\"":{\""count\"": 5}}, {\""revoc_reg_entry\"":{\""count\"": 5}}, {\""nym\"":{\""count\"": 20}}, {\""attrib\"":{\""count\"": 15}}]""{code}
Command for reading:
{code:java}
python3.5 perf_processes.py -n 1 -t 1 -c 1 -r 1 -g ./pool_transactions_genesis -k ""[{\""get_nym\"": {\""count\"": 45, \""file_name\"": \""get_nym.source\""}}, {\""get_schema\"": {\""count\"": 10, \""file_name\"": \""get_schema.source\""}}, {\""get_cred_def\"": {\""count\"": 10, \""file_name\"": \""get_cred_def.source\""}}, {\""get_revoc_reg_delta\"": {\""count\"": 10, \""file_name\"": \""get_revoc_reg.source\""}}, {\""get_attrib\"": {\""count\"": 10, \""file_name\"": \""get_attrib.source\""}}]""{code};;;",,,,,,,,,,,,,,,,,,,,,,
Support Payment API txns in load script,INDY-1358,30527,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,VladimirWork,VladimirWork,23/May/18 6:13 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.73,,,,0,,,,We need to support Payment API txns (provided by libindy and its wrappers) in load script.,,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1368,,,,,,,,,"1|hzwxu7:",,,,,,EV 18.15 Stability/Availabilit,EV 18.16 Releasing 1.6,,,,,,,5.0,,,,,,,,,,,,spivachuk,VladimirWork,zhigunenko.dsr,,,,,,,,,,"08/Aug/18 5:40 PM;spivachuk;Added support for {{payment}}, {{verify_payment}} and {{get_payment_sources}} transactions to the performance test script. Updated the user manual on the performance test script.

PRs:
- https://github.com/hyperledger/indy-node/pull/867
- https://github.com/hyperledger/indy-node/pull/877

Version:
- indy-node 1.5.548-master;;;","08/Aug/18 10:02 PM;VladimirWork;Payment \ verify_payment \ get_payment_sources txns are sent without any errors against docker pool with expected throughput. Need to be checked against 25 nodes AWS pool with heavy load.;;;","14/Aug/18 11:07 PM;zhigunenko.dsr;Will be checked in scope of INDY-1343 or it's inheritors;;;",,,,,,,,,,,,,,,,,,,,,,
As a QA I want post-install automation tests to be run automatically in CI/CD pipeline,INDY-1359,30528,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,andkononykhin,VladimirWork,VladimirWork,23/May/18 7:06 PM,11/Oct/19 9:11 PM,28/Oct/23 2:47 AM,11/Oct/19 9:11 PM,,,,,,0,devops,,,"Things to do:
1. Install pool of 25 nodes *for each master/stable build of indy-node package* in CI/CD infrastructure.
2. Run post-install tests (https://github.com/hyperledger/indy-post-install-automation) against this pool in scope of CI/CD pipeline.
3. Send pytest results produced by this tests to QA team via mail.

[~ashcherbakov] [~andkononykhin] Can we estimate effort for this task and plan it to some sprint?",,,,,,,,,,INDY-1270,,,,,,,,,,,,,,,,,,,,,,,,INDY-505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwy5j:",,,,,,,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"02/Oct/18 10:02 PM;VladimirWork;*Setup steps:*
1. Set package versions in `https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/core.ubuntu.dockerfile` and add necessary packages (e.g. pytest).
2. Run `https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/pool_start.sh 25` to create pool of 25 nodes.
3. Run `https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/client_for_pool_start.sh` to create client (this scripts also runs old CLI but it is not necessary.

*Test run steps:*
1. Get `https://github.com/VladimirWork/tests` repository.
2. `docker_genesis` file should contain actual pool_transactions_genesis data from test pool installed above.
3. Run test_ledger.py using pytest to perform test steps and get default pytest pass/fail report.

FYI [~andkononykhin] [~Sergey.Kupryushin]
;;;","11/Oct/19 9:11 PM;ashcherbakov;We have system tests integrated into our CI/CD instead;;;",,,,,,,,,,,,,,,,,,,,,,,
Out of memory during non-completed viewChange process (under load),INDY-1360,30529,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,zhigunenko.dsr,zhigunenko.dsr,23/May/18 7:15 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,TShirt_L,,,"*Prehistory:*
1) use AWS Perfomance pool 
2) create new pool with 20 nodes
3) write about 1k txns
4) demote node2 and node3
5) write about 97k txns 

After all this start  the load test
*Used load settings:* sudo python3.5 perf_processes.py -g pool_transactions_genesis_performance -c 100 -n 5000 (from single agent)

*Actual results:*
* Pool stops to write after ~139k ledger size
* Many nodes have been blacklisted (the first time it happened an hour after load start)
* Current viewNo: 411 (in progress), but we have ""INSTANCE_CHANGE{'viewNo': 959, 'reason': 28}"" in validator-info output
* From some point ""cannot allocate memory"" began to appear every ~10 minutes

_Logs will be attached later_","indy-anoncreds 1.0.32
indy-cli 1.4.0~523
indy-node 1.3.422
indy-plenum 1.2.367
libindy 1.4.0~523 
libindy-crypto 0.4.0
python3-indy-crypto 0.4.1 
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzcj3:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,anikitinDSR,sergey.khoroshavin,zhigunenko.dsr,,,,,,,,,,"26/Jun/18 3:37 PM;anikitinDSR;Please, recheck this issue on the last version of indy-node with logLevel=DEBUG;;;","28/Jun/18 11:04 PM;zhigunenko.dsr;*Environment:*
indy-node 1.4.478
libindy 1.4.0~596

*Steps to Validate:*
1. use AWS Live pool 
2. create new pool with 22 nodes
3. write about 1k txns
4. demote node2 and node3
5. Repeatedly run 
{code}
for (( i=1; i <= 70; i++ ))
do
        python3.5 perf_processes.py -g pool_transactions_genesis -c 1 -n 350 -k nym &
done
{code}

*Actual Results:*
Pool ceases to form preperpare after 295k txns

*Additional Information:*
Please, avoid standart numeration. live_node1, live_node11, live_node23 were unreachable, so numeration had been shifted
live_node2 == Node1
...
live_node10 == Node9
live_node12 == Node10
...
live_node22 == Node20
live_node24 == Node21
live_node25 == Node22;;;","05/Jul/18 11:56 PM;sergey.khoroshavin;Analysis of logs from recent load test showed evidence of same problem as first issue in [INDY-1429|https://jira.hyperledger.org/browse/INDY-1429] - first PREPREPAREs in view having size larger than MSG_LEN_LIMIT. Unfortunately relevant part of logs from Node5 (which was primary that generated these messages) were lost, but looking at all other nodes it can be seen that only PREPREPARES starting from ppSeqNo 3 managed to get to non-primaries. We already have fix for this (plenum starting from 1.4.429, [PR|https://github.com/hyperledger/indy-plenum/pull/785]), validation will be done in scope of [INDY-1343|https://jira.hyperledger.org/browse/INDY-1343] and [INDY-1425|https://jira.hyperledger.org/browse/INDY-1425];;;",,,,,,,,,,,,,,,,,,,,,,
journalctl is unavailable for indy user,INDY-1361,30535,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,zhigunenko.dsr,zhigunenko.dsr,23/May/18 10:30 PM,11/Oct/19 6:25 PM,28/Oct/23 2:47 AM,11/Oct/19 6:25 PM,,,,,,0,,,,"In _validator-info -v_ some data are unavailable because of _indy_ user permissions

{code}
""stops_stat"":   n/a                   
""journalctl_exceptions"": n/a
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy3r:",,,,,,,,,,,,,,3.0,,,,,,,,,,,,esplinr,zhigunenko.dsr,,,,,,,,,,,"11/Oct/19 6:25 PM;esplinr;After considering this suggestion, we decided that the current information exposed by validator-info is sufficient. Admins can send over their journalctl when they need help with additional debugging.;;;",,,,,,,,,,,,,,,,,,,,,,,,
--log option for validator-info should be delete,INDY-1362,30539,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Won't Do,,zhigunenko.dsr,zhigunenko.dsr,23/May/18 11:36 PM,11/Oct/19 1:01 AM,28/Oct/23 2:47 AM,11/Oct/19 1:01 AM,,,validator-info,,,0,,,,"Option is meaningless at the moment because validator-info is only parse data from different certain files, without any logging",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1175,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwx4f:2rzv",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GetValidatorInfo should have correct validation for signature and permissions,INDY-1363,30542,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,sergey.minaev,sergey.minaev,24/May/18 12:55 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,validator-info,,,0,,,,"Right now Node require signature, but doesn't check role/permission for GET_VALIDATOR_INFO
I see 2 possible options here
1) Node doesn't require any signature
2) Node requires signature and check role/permission of the sender",,,,,,,,,,,,,,,,,IS-588,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzd1b:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,esplinr,sergey.minaev,spivachuk,Toktar,zhigunenko.dsr,,,,,,,"24/May/18 12:59 AM;sergey.minaev;[~ashcherbakov] [~spivachuk] [~Toktar] we should choose 1st or 2nd option;;;","24/May/18 1:06 AM;Toktar;PR-s for case we do not check signature for validator-info command request:
[https://github.com/hyperledger/indy-node/pull/715]
[https://github.com/hyperledger/indy-plenum/pull/698]

 ;;;","24/May/18 6:08 PM;Toktar;PR for case we change permission to validator-info command and add it only for Steward and Trustee:
https://github.com/hyperledger/indy-node/pull/717;;;","24/May/18 7:36 PM;ashcherbakov;I'm voting for Option 2.;;;","24/May/18 8:26 PM;spivachuk;Voting for Option 2.;;;","25/May/18 11:54 PM;esplinr;The concern is that the results from validator-info provide details on local hardware and node characteristics that could be used by an attacker:

{code}
{""HDD_all"":""51280 Mbs"",""RAM_used_by_node"":""500 Mbs"",""HDD_used_by_node"":""51280 MBs"",""RAM_all_free"":""751 Mbs""} 
{code}

The right approach depends on the use case:
* When validator-info.py is called from a local machine, it should provide the requested information for the local machine without needing Steward Credentials (we don't want the Steward keys to be on the local machine). Local users already have the information contained in the result. This is also necessary for automated processes.
* When this API is called remotely (see IS-588) then it should required Steward or Trustee credentials.;;;","26/May/18 12:31 AM;Toktar;Problem reason:
 - Command GET_VALIDATOR_INFO can be sent by everyone. Results of this command contains details of hardware node characteristics that could be used by an attacker.

Changes:
 - Add permission to validator_info command only for Trustee and Steward

PR:
 - [https://github.com/hyperledger/indy-node/pull/717]

Version:
 - indy-plenum 1.2.427-master

Risk factors:
 - Problem with permission to validator_info command

Risk:
 - Low

Covered with tests:
 - [test_validator_info_command.py|https://github.com/hyperledger/indy-node/pull/717/files#diff-d749e19cba0699ad708964683965609b]
 - [test_auth_validator_info.py|https://github.com/hyperledger/indy-node/pull/717/files#diff-611c7d5026b482a379449c7f1c3ce3a7];;;","28/May/18 10:30 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.427
indy-plenum 1.2.369
libindy 1.4.0~533
indy-cli 1.4.0~533

*Checked cases:*
* run _ledger get-validator_ info without connected pool - ""no active did"" error
* run _ledger get-validator_ info without opened wallet - ""no active did"" error
* run _ledger get-validator_ info without active did - ""no active did"" error
* run _ledger get-validator_ info with TRUSTEE did - returns valid json with info from each node
* run _ledger get-validator_ info with STEWARD did - returns valid json with info from each node
* run _ledger get-validator_ info with TRUS_ANCHOR did - Transaction has been rejected: TRUST_ANCHOR cannot do action with type = 119
* run _ledger get-validator_ info with did (no role) - Transaction has been rejected: None role cannot do action with type = 119
* run _ledger get-validator_ info when one node disconnected - there is no message that info is not fetched from node
* run _ledger get-validator_ info when one node stopped - there is no message that info is not fetched from node
* run _ledger get-validator_ info when one node demoted - info from demoted node is still available
* run _ledger get-validator_ info when pool is broken - correct output
* run _ledger get-validator_ info when corresponding json files are deleted - correct output
* run _ledger get-validator help_ - correct output;;;",,,,,,,,,,,,,,,,,
Can't post to the STN client request invalid: CouldNotAuthenticate,INDY-1364,30567,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,EvelynEvergreene,EvelynEvergreene,24/May/18 7:39 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"I have confirmed that multiple users are unable to post new TRUST_ANCHORS to the STN. 

Keys fail under the error:

FROM INDY CLI
Error: client request invalid: CouldNotAuthenticate('Can not find verkey for WgqHmfdDVkfTzTNF8c3iAY',)

FROM VALIDATOR LOGS
2018-05-23 19:58:17,853 | TRACE    | zstack.py            ( 748) | transmitThroughListener | brazilC transmitting b'\{""op"":""REQNACK"",""reason"":""client request invalid: CouldNotAuthenticate(\'Can not find verkey for W5o26v356i6TBpkrhGiLJD\',)"",""identifier"":""W5o26v356i6TBpkrhGiLJD"",""reqId"":1527105497724599707}' to b'qY-dJUY8^zy:FB-Ca>xiZv!r!7B0=-=k7>b^3xIS' through listener socket",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzgi7:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,Derashe,EvelynEvergreene,,,,,,,,,,"24/May/18 7:52 PM;ashcherbakov;According to the error message, there is no verkey for DID `WgqHmfdDVkfTzTNF8c3iAY` (that is sender's DID). Are we sure that DID=`WgqHmfdDVkfTzTNF8c3iAY` was actually posted to the ledger with an appropriate role?;;;","10/Oct/18 6:57 PM;Derashe;As Alex said, problem is about that one of nym that you've include in multisignature (as I understand _multiple users_), not written in ledger.

And as I've just checked, we can multisig nym txns and write it to ledger. The requirements here is that identifier's (sender) nym must present in multisignature and that every multisig participant's nyms were written to ledger

[~EvelynEvergreene] please open a new ticket if you encounter this problem again with upper satisfied requirements;;;",,,,,,,,,,,,,,,,,,,,,,,
"Pool stopped accepting transactions on 5731 txns (1 sec delays, no logging)",INDY-1365,30577,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,sergey.khoroshavin,ozheregelya,ozheregelya,24/May/18 8:12 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,Stability,,,"*Environment:*
 indy-node 1.3.425
 AWS 22-nodes pool

*Steps to Reproduce:*
 1. Setup the pool of 25 nodes without logging.
 2. Run several load new test with following parameters:
{code:java}
python3.5 perf_processes.py -n 1 -t 1 -c 10 -r 100 -g /home/ubuntu/live_transactions_genesis
...
Server Time: 754 Sent: 4783 Succ: 4760 Failed: 23 Nacked: 0 Rejected: 0 Clients Alive 0{code}
 
{code:java}
python3.5 perf_processes.py -n 1 -t 1 -c 10 -r 100 -g /home/ubuntu/live_transactions_genesis 2>err
ors.txt
...
Server Time: 18 Sent: 104 Succ: 104 Failed: 0 Nacked: 0 Rejected: 0 Clients Alive 0{code}
{code:java}
python3.5 perf_processes.py -n 1 -t 1 -c 10 -r 100 -b 10 -g /home/ubuntu/live_transactions_genesis
...
Server Time: 133 Sent: 6193 Succ: 822 Failed: 5371 Nacked: 0 Rejected: 0 Clients Alive 0 
{code}
*Actual Results:*
 Pool stopped taking transactions after writing 5731 txns. 
 9 view changes were happened:
{code:java}
 ""View_change_status"":
 ""View_No"": 9
 ""Last_complete_view_no"": 9
 ""Last_view_change_started_at"": 2018-05-23 18:37:38
 ""VC_in_progress"": n/a{code}
*Expected Results:*
 Pool should work.

*Additional Information:*
 Logging was turned off, so the only information that we have is validator-info: [^indy1365.7z]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1370,,,,,,,,,,,,,,,"24/May/18 9:39 PM;ozheregelya;indy1365.7z;https://jira.hyperledger.org/secure/attachment/15014/indy1365.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzd0n:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,sergey.khoroshavin,,,,,,,,,,"25/May/18 5:52 PM;ashcherbakov;We need to make sure that this is because of already identified View Change problems, and, if so, continue fixing in the scope of INDY-1341, INDY-1350.;;;","28/May/18 6:19 PM;sergey.khoroshavin;Subsequent run with _INFO_ logs showed that problem is with pre-prepares having different digests on different nodes. There is high probabilty that this is related to (and can be solved by) [INDY-1370|https://jira.hyperledger.org/browse/INDY-1370], but in order to confirm this one more run is required with _logLevel=DEBUG_;;;","28/May/18 11:44 PM;sergey.khoroshavin;Related logs (after filtering):
{code}
2018-05-28 09:19:33.158000 | Node2 received client request: SafeRequest: {'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.159000 | Node2 received client request: SafeRequest: {'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.159000 | Node21 received client request: SafeRequest: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'protocolVersion': 1, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.160000 | Node21 received client request: SafeRequest: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'protocolVersion': 1, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.172000 | Node1 received client request: SafeRequest: {'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'reqId': 1527499173126845211, 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.173000 | Node1 received client request: SafeRequest: {'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'reqId': 1527499173126845211, 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.183000 | Node14 received client request: SafeRequest: {'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'reqId': 1527499173126845211, 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.184000 | Node14 received client request: SafeRequest: {'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'reqId': 1527499173126845211, 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.187000 | Node22 received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.188000 | Node22 received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.195000 | Node3 received client request: SafeRequest: {'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.196000 | Node3 received client request: SafeRequest: {'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.205000 | Node13 received client request: SafeRequest: {'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.205000 | Node13 received client request: SafeRequest: {'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.206000 | Node4 received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.207000 | Node4 received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.215000 | Node9 received client request: SafeRequest: {'protocolVersion': 1, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.216000 | Node9 received client request: SafeRequest: {'protocolVersion': 1, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.217000 | Node8 received client request: SafeRequest: {'reqId': 1527499173126845211, 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.218000 | Node8 received client request: SafeRequest: {'reqId': 1527499173126845211, 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.230000 | Node20 received client request: SafeRequest: {'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.231000 | Node20 received client request: SafeRequest: {'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.232000 | Node10 received client request: SafeRequest: {'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'reqId': 1527499173126845211, 'protocolVersion': 1} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.233000 | Node10 received client request: SafeRequest: {'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'reqId': 1527499173126845211, 'protocolVersion': 1} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.244000 | Node7 received client request: SafeRequest: {'protocolVersion': 1, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.245000 | Node7 received client request: SafeRequest: {'protocolVersion': 1, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.261000 | Node6 received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'reqId': 1527499173126845211, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.262000 | Node6 received client request: SafeRequest: {'protocolVersion': 1, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'reqId': 1527499173126845211, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.309000 | Node11 received client request: SafeRequest: {'protocolVersion': 1, 'reqId': 1527499173126845211, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.311000 | Node11 received client request: SafeRequest: {'protocolVersion': 1, 'reqId': 1527499173126845211, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.313000 | Node17 received client request: SafeRequest: {'reqId': 1527499173126845211, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'protocolVersion': 1} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.313000 | Node17 received client request: SafeRequest: {'reqId': 1527499173126845211, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'protocolVersion': 1} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.321000 | Node16 received client request: SafeRequest: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'reqId': 1527499173126845211} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.322000 | Node16 received client request: SafeRequest: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'reqId': 1527499173126845211} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.368000 | Node1 sending message PREPREPARE{'ppTime': 1527499173, 'stateRootHash': 'BvsTnR4Mbu7opeYD47uMbiK1SEvcgTR7aTWM7H3QuR21', 'blsMultiSig': ['RMHTtfZLJWFr7krUf45A6QQFTibBuPBorP54cBk9zKn2X2VgCeMM8iZTjJrFEj2Mx62bazxXDDmKLtdTLXzUp8VpE2zqAZZm6gaHyCTcx9DbJj3TBKyHksRx7bfkNJ4zhZ9uvXDzkPbLcDdhBAkVrMuMbHGpPyPdzCEEQLcpJBXVDh', ['Node14', 'Node13', 'Node18', 'Node3', 'Node22', 'Node7', 'Node21', 'Node4', 'Node8', 'Node19', 'Node2', 'Node15', 'Node1', 'Node17', 'Node20'], [1, 'BVHVT2JjvLW7btXeTNpC8zrB3pxKw2kyaFLtrKC36UcE', '5sg32CVuA3ov789Ww2ok3tvqtouimRtN9AmcmUtWgjFa', 'FuUEbiGU12DX2FCVtmj8v8zzMvmyFadG3D48wWfgzHDW', 1527499170]], 'discarded': 1, 'reqIdr': [('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)], 'instId': 0, 'viewNo': 0, 'ppSeqNo': 695, 'ledgerId': 1, 'digest': '87bfdc3edb66f8570e379fb8f8dca987c0645e610daa24a9179597028136e3d5', 'txnRootHash': 'HVTfU1xrn8TWGWdD3GdjhiNbSJz9QrQtzkhvLbxKwHhd'} to all recipients: ['Node14', 'Node11', 'Node6', 'Node9', 'Node18', 'Node12', 'Node20', 'Node13', 'Node22', 'Node7', 'Node21', 'Node3', 'Node4', 'Node10', 'Node5', 'Node8', 'Node19', 'Node2', 'Node15', 'Node17', 'Node16']
2018-05-28 09:19:33.474000 | Node18 received client request: SafeRequest: {'protocolVersion': 1, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'reqId': 1527499173126845211, 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.475000 | Node18 received client request: SafeRequest: {'protocolVersion': 1, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'reqId': 1527499173126845211, 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.629000 | Node19 received client request: SafeRequest: {'reqId': 1527499173126845211, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'operation': {'type': '1', 'dest': 'KVSAWWcGAYv3FwWrpymKTD'}, 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.629000 | Node19 received client request: SafeRequest: {'reqId': 1527499173126845211, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'operation': {'type': '1', 'dest': 'CpB1Eqxke6WAJX4Vz5AvZv'}, 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.689000 | Node15 received client request: SafeRequest: {'reqId': 1527499173126845211, 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.690000 | Node15 received client request: SafeRequest: {'reqId': 1527499173126845211, 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.765000 | Node5 received client request: SafeRequest: {'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR'} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.765000 | Node5 received client request: SafeRequest: {'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}, 'reqId': 1527499173126845211, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'protocolVersion': 1, 'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv'} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.775000 | Node12 received client request: SafeRequest: {'signature': '5QQUMaJMwkLubR4Vgqn1f45Z7Jq8pcBUcgfAu3dYrjaLXtKSx1G61xti1cMhQzPpkij8z33b637B8D9p648HYhkv', 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211, 'operation': {'dest': 'KVSAWWcGAYv3FwWrpymKTD', 'type': '1'}} from b'V%{M&RZdTC7-.+qDfSp$WL[6=X)I=ig.l5V8:f@G'
2018-05-28 09:19:33.776000 | Node12 received client request: SafeRequest: {'signature': '5iJXYpDYPBoXKmcssWaFZEjDQfV8NPmyaWotzMCtnWWnTFN7Um9SZSYUeS8VG96GS5XrSg8YkKEh7heC8a1pEZyR', 'protocolVersion': 1, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'reqId': 1527499173126845211, 'operation': {'dest': 'CpB1Eqxke6WAJX4Vz5AvZv', 'type': '1'}} from b'y-S7>lQ{#btfCTFR2$rhB&Q5)yi]x>dlg378p1TD'
2018-05-28 09:19:33.895000 | Node19 discarding message PREPREPARE{'instId': 0, 'ppTime': 1527499173, 'txnRootHash': 'HVTfU1xrn8TWGWdD3GdjhiNbSJz9QrQtzkhvLbxKwHhd', 'blsMultiSig': ['RMHTtfZLJWFr7krUf45A6QQFTibBuPBorP54cBk9zKn2X2VgCeMM8iZTjJrFEj2Mx62bazxXDDmKLtdTLXzUp8VpE2zqAZZm6gaHyCTcx9DbJj3TBKyHksRx7bfkNJ4zhZ9uvXDzkPbLcDdhBAkVrMuMbHGpPyPdzCEEQLcpJBXVDh', ['Node14', 'Node13', 'Node18', 'Node3', 'Node22', 'Node7', 'Node21', 'Node4', 'Node8', 'Node19', 'Node2', 'Node15', 'Node1', 'Node17', 'Node20'], [1, 'BVHVT2JjvLW7btXeTNpC8zrB3pxKw2kyaFLtrKC36UcE', '5sg32CVuA3ov789Ww2ok3tvqtouimRtN9AmcmUtWgjFa', 'FuUEbiGU12DX2FCVtmj8v8zzMvmyFadG3D48wWfgzHDW', 1527499170]], 'discarded': 1, 'reqIdr': [('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)], 'ppSeqNo': 695, 'viewNo': 0, 'ledgerId': 1, 'digest': '87bfdc3edb66f8570e379fb8f8dca987c0645e610daa24a9179597028136e3d5', 'stateRootHash': 'BvsTnR4Mbu7opeYD47uMbiK1SEvcgTR7aTWM7H3QuR21'} because Pre-Prepare message has incorrect digest
2018-05-28 09:19:33.973000 | Node15 discarding message PREPREPARE{'ppTime': 1527499173, 'txnRootHash': 'HVTfU1xrn8TWGWdD3GdjhiNbSJz9QrQtzkhvLbxKwHhd', 'instId': 0, 'viewNo': 0, 'digest': '87bfdc3edb66f8570e379fb8f8dca987c0645e610daa24a9179597028136e3d5', 'ledgerId': 1, 'reqIdr': [('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)], 'blsMultiSig': ['RMHTtfZLJWFr7krUf45A6QQFTibBuPBorP54cBk9zKn2X2VgCeMM8iZTjJrFEj2Mx62bazxXDDmKLtdTLXzUp8VpE2zqAZZm6gaHyCTcx9DbJj3TBKyHksRx7bfkNJ4zhZ9uvXDzkPbLcDdhBAkVrMuMbHGpPyPdzCEEQLcpJBXVDh', ['Node14', 'Node13', 'Node18', 'Node3', 'Node22', 'Node7', 'Node21', 'Node4', 'Node8', 'Node19', 'Node2', 'Node15', 'Node1', 'Node17', 'Node20'], [1, 'BVHVT2JjvLW7btXeTNpC8zrB3pxKw2kyaFLtrKC36UcE', '5sg32CVuA3ov789Ww2ok3tvqtouimRtN9AmcmUtWgjFa', 'FuUEbiGU12DX2FCVtmj8v8zzMvmyFadG3D48wWfgzHDW', 1527499170]], 'ppSeqNo': 695, 'discarded': 1, 'stateRootHash': 'BvsTnR4Mbu7opeYD47uMbiK1SEvcgTR7aTWM7H3QuR21'} because Pre-Prepare message has incorrect digest
2018-05-28 09:19:34.170000 | Node12 already requested PROPAGATE recently for ('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)
2018-05-28 09:19:34.351000 | Node11 already requested PROPAGATE recently for ('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)
2018-05-28 09:19:35.421000 | Node19 discarding message PREPREPARE{'instId': 0, 'ppTime': 1527499173, 'txnRootHash': 'HVTfU1xrn8TWGWdD3GdjhiNbSJz9QrQtzkhvLbxKwHhd', 'blsMultiSig': ['RMHTtfZLJWFr7krUf45A6QQFTibBuPBorP54cBk9zKn2X2VgCeMM8iZTjJrFEj2Mx62bazxXDDmKLtdTLXzUp8VpE2zqAZZm6gaHyCTcx9DbJj3TBKyHksRx7bfkNJ4zhZ9uvXDzkPbLcDdhBAkVrMuMbHGpPyPdzCEEQLcpJBXVDh', ['Node14', 'Node13', 'Node18', 'Node3', 'Node22', 'Node7', 'Node21', 'Node4', 'Node8', 'Node19', 'Node2', 'Node15', 'Node1', 'Node17', 'Node20'], [1, 'BVHVT2JjvLW7btXeTNpC8zrB3pxKw2kyaFLtrKC36UcE', '5sg32CVuA3ov789Ww2ok3tvqtouimRtN9AmcmUtWgjFa', 'FuUEbiGU12DX2FCVtmj8v8zzMvmyFadG3D48wWfgzHDW', 1527499170]], 'discarded': 1, 'reqIdr': [('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)], 'ppSeqNo': 695, 'viewNo': 0, 'ledgerId': 1, 'digest': '87bfdc3edb66f8570e379fb8f8dca987c0645e610daa24a9179597028136e3d5', 'stateRootHash': 'BvsTnR4Mbu7opeYD47uMbiK1SEvcgTR7aTWM7H3QuR21'} because Pre-Prepare message has incorrect digest
2018-05-28 09:19:35.643000 | Node15 discarding message PREPREPARE{'ppTime': 1527499173, 'txnRootHash': 'HVTfU1xrn8TWGWdD3GdjhiNbSJz9QrQtzkhvLbxKwHhd', 'instId': 0, 'viewNo': 0, 'digest': '87bfdc3edb66f8570e379fb8f8dca987c0645e610daa24a9179597028136e3d5', 'ledgerId': 1, 'reqIdr': [('V4SGRU86Z58d6TV7PBUe6f', 1527499173126845211)], 'blsMultiSig': ['RMHTtfZLJWFr7krUf45A6QQFTibBuPBorP54cBk9zKn2X2VgCeMM8iZTjJrFEj2Mx62bazxXDDmKLtdTLXzUp8VpE2zqAZZm6gaHyCTcx9DbJj3TBKyHksRx7bfkNJ4zhZ9uvXDzkPbLcDdhBAkVrMuMbHGpPyPdzCEEQLcpJBXVDh', ['Node14', 'Node13', 'Node18', 'Node3', 'Node22', 'Node7', 'Node21', 'Node4', 'Node8', 'Node19', 'Node2', 'Node15', 'Node1', 'Node17', 'Node20'], [1, 'BVHVT2JjvLW7btXeTNpC8zrB3pxKw2kyaFLtrKC36UcE', '5sg32CVuA3ov789Ww2ok3tvqtouimRtN9AmcmUtWgjFa', 'FuUEbiGU12DX2FCVtmj8v8zzMvmyFadG3D48wWfgzHDW', 1527499170]], 'ppSeqNo': 695, 'discarded': 1, 'stateRootHash': 'BvsTnR4Mbu7opeYD47uMbiK1SEvcgTR7aTWM7H3QuR21'} because Pre-Prepare message has incorrect digest
{code};;;","29/May/18 12:01 AM;sergey.khoroshavin;Now it's clear that the problems is that different clients with same DID sent different requests with same reqId, resulting in collisions. This can be solved be [INDY-1370|https://jira.hyperledger.org/browse/INDY-1370];;;","13/Jun/18 7:43 PM;ozheregelya;This case was double checked after fix of INDY-1370. Initial problem was not appear.;;;",,,,,,,,,,,,,,,,,,,,
Cover read_ledger functionality by unit/integration tests,INDY-1366,30587,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,24/May/18 10:08 PM,02/Aug/18 2:23 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,We should cover read_ledger functionality by unit/integration tests the same way as validator-info.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzwy6v:",,,,,,,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[QA] Perform load testing after catch-up changes and stability fixes,INDY-1367,30596,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,25/May/18 12:25 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,After implementation of INDY-1297 and INDY-1236 need to perform load testing and compare results with  the previous ones: https://docs.google.com/spreadsheets/d/1DTjDsLSysFBiKU-9z4-IzunJk4wEy44hE_PGZYxnN_8/edit#gid=1813415708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzd0f:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,5.0,,,,,,,,,,,,ozheregelya,,,,,,,,,,,,"31/May/18 2:23 AM;ozheregelya;Catch-up under load was tested.
Nothing special was noticed during load. Results are more or less the same as before (351891 txns on indy-node 1.3.425 and 339250 txns on indy-node 1.3.396).;;;",,,,,,,,,,,,,,,,,,,,,,,,
Load Script Support Epic,INDY-1368,30632,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,VladimirWork,VladimirWork,25/May/18 9:23 PM,09/Oct/19 6:25 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-5,,Load Script,Done,,,,,,,"1|hzyvdj:",,,,,,,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create 1.4 Release,INDY-1369,30643,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,26/May/18 12:39 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"*Acceptance criteria*
* Create a new RC 1.4 containing all the tasks from Release 1.4
* Make acceptance testing of the new RC, in particular RocksDB, new txn format, migrations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzclb:",,,,,,EV 18.12 Release RocksDB,EV 18.13 Benchmark hardening,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"02/Jul/18 3:09 PM;ashcherbakov;A new stable release of Indy Node 1.4.66 was issued, verified and approved by QA.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Сhange key in requests map and field reqIdr in Pre Prepare and Ordered,INDY-1370,30681,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,Toktar,Toktar,28/May/18 5:20 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,"Change key in requests map from (request id, DID) to digest owing to which will change field reqIdr in Pre Prepare and Ordered. Because if 2 clients with same DID send request with same request_id. Request map uses this value as a key. In this case we will have collision of requests and different request data on different nodes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1365,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcpj:",,,,,,EV 18.11 Stability/ViewChange,EV 18.12 Release RocksDB,,,,,,,5.0,,,,,,,,,,,,ozheregelya,Toktar,zhigunenko.dsr,,,,,,,,,,"01/Jun/18 6:27 PM;Toktar;PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/713]
 * [https://github.com/hyperledger/indy-node/pull/748]

 

[Presentation|https://docs.google.com/presentation/d/18mGt0tCd2YJhms2ZBGam8q5yqumbITmAILHd57WwUso/edit#slide=id.g3bbe9cf0dc_0_313] with description of the problems found and information about fixes.;;;","09/Jun/18 4:49 PM;Toktar;*Problem reason:*
 * The client sends requests with different operation data, but the same identifier and reqID. As a result, there are different requests under the same key on different nodes. Which leads to a collision. No one of the requests will be recorded and the consensus will be lost.

*Changes:*
 * We fixed this by replacing the identifier and recID with a digest as a key in the following structures:
 ** Ordered messages
 ** PrePrepare messages
 ** Request
 ** Message Requests with Propagate
 ** Monitor.requestTracker
 ** Monitor.requests
 ** Node.requests
 ** Node.requestSender
 ** Transactions

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/713]
 * [https://github.com/hyperledger/indy-node/pull/748]
 * [https://github.com/hyperledger/indy-plenum/pull/735]

*Version:*
 * indy-node 1.3.449-master
 * indy-plenum 1.2.393-master

*Risk factors:*
 * The transactions can not be ordered in specific case such as after catch up.
 * Two requests with same identifier and requestId but different operation will not be recorded in the ledger. 

*Risk:*
 * Normal

*Recommendations for QA:*

Changes made within this task affect processing of all kinds of requests. Integration tests cover standard cases, it may be worth checking the behavior under load.
 However, firstly need to check the case when two requests are sent in one time (preferably to different nodes) with the same identifier and requestID, but with a different field of the ""operation"". Both requests must correctly work out and record both transactions in the ledger.;;;","09/Jun/18 10:18 PM;zhigunenko.dsr;*Environment:*
indy-cli 1.4.0~562
indy-node 1.3.450

*Steps to Validate:*
1) setup pool
2) open two different CLI
3) run
{code:java}
ledger custom {""reqId"":1,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""1"",""dest"":""V4SGRU86Z58d6TV7PBU111""},""protocolVersion"":1} sign=true
{code}
and
{code:java}
ledger custom {""reqId"":1,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""105"",""dest"":""V4SGRU86Z58d6TV7PBU111""},""protocolVersion"":1} sign=true
{code}
from both CLI simultaneously
4) run 
{code:java}
perf_processes.py -n 1 -t 1 -c 10 -r 100 -g pool_transactions_genesis 
{code}
from 2 CLI

*Actual results:*
All transactions have been written;;;",,,,,,,,,,,,,,,,,,,,,,
"As an Issuer, I need to be able to create multiple ClaimDefs for the same Schema, DID and Signature Type",INDY-1372,30733,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,30/May/18 5:24 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"As of now only one CLAIM_DEF can be created for the same Schema, DID (creator) and Signature Type.

We need to introduce a `tag` field which can be used to distinguish CLAIM_DEFs for the same Schema/DID/Type.

See the proposed format here: [https://github.com/hyperledger/indy-node/blob/master/docs/requests-new.md#claim_def]

See the design for anoncreds txns (including CLAIM_DEF) here: https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1424,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1374,,,,,,,,,"1|hzzcn3:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,VladimirWork,zhigunenko.dsr,,,,,,,,,"13/Jun/18 9:52 PM;ashcherbakov;Problem reason:
 * we need to be able to created multiple CRED_DEF for the same Schema by the same Issuer. In order to support this, a new attribute needs to be added to the CRED_DEF key in state trie. Since this is a breaking change, we did it in the scope of 1.4

Changes:
 * added optional {{tag}} field to CLAIM_DEF
 * use utility methods for SCHEMA and CLAIM_DEF
 * added more tests
 * doc is updated (requests.md and transactions.md)

PR:
  [https://github.com/hyperledger/indy-node/pull/755
 [https://github.com/hyperledger/indy-node/pull/760]
|https://github.com/hyperledger/indy-node/pull/755]|

Version:
 - master 1.4.453

Risk factors:
 - CRED_DEF txn and request (write and get)
 - SCHEMA txn and request (write and get)

Risk:
 - Low

Covered with tests:
 - test_req_utils_schema
 - test_req_utils_claim_def
 - test_state_path
 - fixes and improvements in existing tests

Recommendations for QA
 - make sure that SCHEMA and CRED_DEF write and read work
 - there is no way to test that a new `tag` parameter works now from CLI. A default tag is set is it's not set explicitly in a request.;;;","14/Jun/18 9:38 PM;zhigunenko.dsr;*Environment:*
indy-cli 1.4.0~566
indy-node 1.4.457

*Steps to Validate:*
1. Create new schema
2. Read created schema
3 .Modify schema
4. Read schema again
5. Create cred-def
6. Read cred-def
7. Modify cred-def JSON
8. Read cred-def again

*Actual Results:*
All changes have been written successfully;;;","18/Jun/18 9:45 PM;anikitinDSR;Need to test migration for indy-node with following steps:
 * install indy-node ( < 1.3.428)
 * write some REVOC_REG_DEF transactions
 * do upgrade to version 1.4.464
 * check, that migration was successfully done and previous written REVOC_REG_DEF transacions can be read from pool.;;;","22/Jun/18 12:34 AM;VladimirWork;Build Info:
indy-node 1.3.375 -> 1.4.470

Steps to Reproduce:
1. Install pool 1.3.375 master.
2. Send REVOC_REG_DEF and REVOC_REG_ENTRY txns to ledger.
3. Upgrade pool to 1.4.470 master.
4. Check `journalctl -ex`.

Actual Results:
{noformat}
Jun 21 13:43:46 7605f1e9ecb8 env[3658]: Traceback (most recent call last):
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     ha=node_ha, cliha=client_ha)
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 94, in __init__
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     config=config)
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 205, in __init__
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     self.initDomainState()
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2804, in initDomainState
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     self.domainLedger, self.get_req_handler(DOMAIN_LEDGER_ID))
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2799, in initStateFromLedger
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     reqHandler.updateState([txn, ], isCommitted=True)
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/domain_req_handler.py"", line 69, in updateState
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     self._updateStateWithSingleTxn(txn, isCommitted=isCommitted)
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/domain_req_handler.py"", line 72, in _updateStateWithSingleTxn
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     self.state_update_handlers[txn_type](txn, isCommitted)
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/domain_req_handler.py"", line 778, in _addRevocRegEntry
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     req_id=get_req_id(txn)
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/domain_req_handler.py"", line 334, in _get_current_revoc_entry_and_revoc_def
Jun 21 13:43:46 7605f1e9ecb8 env[3658]:     ""There is no any REVOC_REG_DEF by path: {}"".format(revoc_reg_def_id))
Jun 21 13:43:46 7605f1e9ecb8 env[3658]: plenum.common.exceptions.InvalidClientRequest: InvalidClientRequest('There is no any REVOC_REG_DEF by path: 5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:
Jun 21 13:43:46 7605f1e9ecb8 systemd[1]: indy-node.service: Main process exited, code=exited, status=1/FAILURE
{noformat}

REVOC_REG_DEF and REVOC_REG_ENTRY before migration:
{noformat}
[50,{""credDefId"":""5YKabdJyAEDpAUwKGhwhhr:3:CL:48"",""id"":""5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:CL_ACCUM:tag"",""identifier"":""5YKabdJyAEDpAUwKGhwhhr"",""reqId"":1529587664836135531,""revocDefType"":""CL_ACCUM"",""signature"":""5PZXXvUXERLn87732Rd4YvCNen56VnVdRCFcZyN6iM1UuXk19ZfFGNwzzVxtadWCWT8fSBxYr6vVp8GZJsy19D1L"",""signatures"":null,""tag"":""tag"",""txnTime"":1529587664,""type"":""113"",""value"":{""issuanceType"":""ISSUANCE_ON_DEMAND"",""maxCredNum"":10,""publicKeys"":{""accumKey"":{""z"":""1279C06C12B5B4 EDAC0C50787B80 71E0FB8D721708 1DF3ACC7F662B6 6A17E6A 6B0C5FDB018AB8 1B3DA9606B28F C64F437386E28F FF1339D9A8DB91 2373714 94660DE158F294 846C6EEDE688A8 86D79CE45B4B3A DC9BA0C3207FAF 29E633C BD6017C76F7EC5 BE2B47069A49CD B351D27F4DCDB0 D5E80795C3AE02 1F9BA234 8F7A711E9CC9F4 998E066B1390F1 E0E9DBD991DAC0 92AA1AB5686CCB 5EA5CD 532C2B60C22875 430E80EE85042F B9FC830EBDBC96 BAC68257A701E8 22B0217B ED6BC79DAB6534 DF72EECA97FF2A 91515778F6735C 3793D923624175 123EA4 DC3F4FFCE79E0D AD35E14FA56BC4 B941CBC949E756 AFF57CA00BDD7E 12AD584B 287AFE59932A91 FD3EACEE786F99 F4BD2A1AE0FC29 5EA078EE2ABB36 58483CD FE84CC49FE12EF 8669DFA153B409 F99F67DB34AA42 7A0A39AD21715C 98E50A8 349B90CA4FBE42 A29D3631C6780 FDD0B11EA84C11 F2080418316618 24468423 3E12E9C33763D1 336A01AC988D0F 1FFD575D7F9593 138407DB562A9B 17C045EC""}},""tailsHash"":""7hcdv6iZvq4mfKbSQq8jrN5qjTvxHvmB5H4dm3mgjTj9"",""tailsLocation"":""tails/7hcdv6iZvq4mfKbSQq8jrN5qjTvxHvmB5H4dm3mgjTj9""}}]
[51,{""identifier"":""5YKabdJyAEDpAUwKGhwhhr"",""reqId"":1529587665060067475,""revocDefType"":""CL_ACCUM"",""revocRegDefId"":""5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:CL_ACCUM:tag"",""signature"":""62hB8nE2CymQZcE29kZeDR1gk73M8yEiKA2dS7wSQgAm6rP1js4uA45uo9gvRnRxt3WbD2s5eNMM3MmBDt5sEnwL"",""signatures"":null,""txnTime"":1529587665,""type"":""114"",""value"":{""accum"":""true 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0""}}]
{noformat}


REVOC_REG_DEF and REVOC_REG_ENTRY after migration:
{noformat}
50 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""5YKabdJyAEDpAUwKGhwhhr"",""value"":""5PZXXvUXERLn87732Rd4YvCNen56VnVdRCFcZyN6iM1UuXk19ZfFGNwzzVxtadWCWT8fSBxYr6vVp8GZJsy19D1L""}]},""txn"":{""data"":{""credDefId"":""5YKabdJyAEDpAUwKGhwhhr:3:CL:48:tag"",""id"":""5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:CL_ACCUM:tag"",""revocDefType"":""CL_ACCUM"",""tag"":""tag"",""value"":{""issuanceType"":""ISSUANCE_ON_DEMAND"",""maxCredNum"":10,""publicKeys"":{""accumKey"":{""z"":""1279C06C12B5B4 EDAC0C50787B80 71E0FB8D721708 1DF3ACC7F662B6 6A17E6A 6B0C5FDB018AB8 1B3DA9606B28F C64F437386E28F FF1339D9A8DB91 2373714 94660DE158F294 846C6EEDE688A8 86D79CE45B4B3A DC9BA0C3207FAF 29E633C BD6017C76F7EC5 BE2B47069A49CD B351D27F4DCDB0 D5E80795C3AE02 1F9BA234 8F7A711E9CC9F4 998E066B1390F1 E0E9DBD991DAC0 92AA1AB5686CCB 5EA5CD 532C2B60C22875 430E80EE85042F B9FC830EBDBC96 BAC68257A701E8 22B0217B ED6BC79DAB6534 DF72EECA97FF2A 91515778F6735C 3793D923624175 123EA4 DC3F4FFCE79E0D AD35E14FA56BC4 B941CBC949E756 AFF57CA00BDD7E 12AD584B 287AFE59932A91 FD3EACEE786F99 F4BD2A1AE0FC29 5EA078EE2ABB36 58483CD FE84CC49FE12EF 8669DFA153B409 F99F67DB34AA42 7A0A39AD21715C 98E50A8 349B90CA4FBE42 A29D3631C6780 FDD0B11EA84C11 F2080418316618 24468423 3E12E9C33763D1 336A01AC988D0F 1FFD575D7F9593 138407DB562A9B 17C045EC""}},""tailsHash"":""7hcdv6iZvq4mfKbSQq8jrN5qjTvxHvmB5H4dm3mgjTj9"",""tailsLocation"":""tails/7hcdv6iZvq4mfKbSQq8jrN5qjTvxHvmB5H4dm3mgjTj9""}},""metadata"":{""digest"":""e86c3b0350483bd91a55de59b37155f418516e4748bc83ae5070d81ffbd78e1b"",""from"":""5YKabdJyAEDpAUwKGhwhhr"",""reqId"":1529587664836135531},""type"":""113""},""txnMetadata"":{""seqNo"":50,""txnId"":""5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:tag:CL_ACCUM:tag"",""txnTime"":1529587664},""ver"":""1""}
51 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""5YKabdJyAEDpAUwKGhwhhr"",""value"":""62hB8nE2CymQZcE29kZeDR1gk73M8yEiKA2dS7wSQgAm6rP1js4uA45uo9gvRnRxt3WbD2s5eNMM3MmBDt5sEnwL""}]},""txn"":{""data"":{""revocDefType"":""CL_ACCUM"",""revocRegDefId"":""5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:CL_ACCUM:tag"",""value"":{""accum"":""true 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0""}},""metadata"":{""digest"":""8c5829c92e45ad0a12e686208e6aad59d12078d4713825161d90868c752064a0"",""from"":""5YKabdJyAEDpAUwKGhwhhr"",""reqId"":1529587665060067475},""type"":""114""},""txnMetadata"":{""seqNo"":51,""txnId"":""5:5YKabdJyAEDpAUwKGhwhhr:4:5YKabdJyAEDpAUwKGhwhhr:3:CL:48:CL_ACCUM:tag"",""txnTime"":1529587665},""ver"":""1""}
{noformat}


Expected Results:
There should be no errors after migration.;;;","22/Jun/18 4:18 PM;ashcherbakov;Fixed in 1.4.472;;;","22/Jun/18 6:35 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.375 -> 1.4.472

*Steps to Validate:*
1. Install pool 1.3.375 master.
2. Send all types of txns to ledger.
3. Upgrade pool to 1.4.472 master.
4. Check `journalctl -ex`.

*Actual Results:*
read_ledger haven't show unexpectable diffs

*Additional Information:*
INDY-1424 revealed;;;",,,,,,,,,,,,,,,,,,,
As a developer I need to run log processor in parallel on all test nodes in order to get processing results faster,INDY-1373,30735,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,30/May/18 6:50 PM,11/Oct/19 8:57 PM,28/Oct/23 2:47 AM,11/Oct/19 8:57 PM,,,,,,0,,,,"Main problem with processing logs is that it's very long CPU-bound (and sometimes memory-bound) task. Also, downloading logs from pool to process them locally takes a lot of time.

If we could run log processor on large 25-nodes test pool directly we:
- effectively get 50 cores and 200 Gb of RAM to process logs, which is much more than any desktop have
- can get results before downloading all logs, which takes time

In order to do this we need:
- remove dependency on matplotlib from log processor
- create ansible playbook to upload log processor and its config to pool, run it in pool and then download results",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwx4f:2rzx",,,,,,,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"11/Oct/19 8:57 PM;ashcherbakov;We don't use log processor a lot now.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support evolving of transactions to guarantee backward-compatibility,INDY-1374,30736,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,30/May/18 7:21 PM,28/Sep/18 7:21 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,Support evolving of transactions to guarantee backward-compatibility,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-7,,Evolving of transactions,Done,,,,,,,"1|hzxx1z:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support evolving of requests to guarantee backward-compatibility,INDY-1375,30737,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,30/May/18 7:27 PM,05/Jun/18 9:50 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"* Support evolving of requests to guarantee backward-compatibility
 * Support deterministic signatures over requests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-9,,Evolving of requests,To Do,,,,,,,"1|hzxx1r:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
View Change Hardening,INDY-1376,30739,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,30/May/18 7:53 PM,09/Oct/19 6:03 PM,28/Oct/23 2:47 AM,09/Oct/19 6:03 PM,,,,,,0,,,,View Change Hardening and fixes in the current protocol,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-2,,View Change Hardening,Done,,,,,,,"1|hzyw5r:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Catch-up hardening,INDY-1377,30740,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,30/May/18 7:58 PM,09/Oct/19 5:14 PM,28/Oct/23 2:47 AM,09/Oct/19 5:14 PM,,,,,,0,,,,Applying state machine to catch-up and bug fixing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-4,,Catch-up hardening,Done,,,,,,,"1|hzxwy7:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
We should update revocation registry delta value during REG_ENTRY_REVOC writing,INDY-1378,30751,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,VladimirWork,VladimirWork,31/May/18 12:53 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,We should update revocation registry delta during REG_ENTRY_REVOC writing by sending registry revocation reading txn (that will be implemented in scope of INDY-1356) because if writing txn is failed the next txns to write are rejected due to incorrect revocation registry delta.,,,,,,,,,,INDY-1356,,,,,,,,,,,,,,,,,,,,,,,,INDY-1355,INDY-1718,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1368,,,,,,,,,"1|hzwxwv:",,,,,,EV 18.15 Stability/Availabilit,,,,,,,,3.0,,,,,,,,,,,,dsurnin,VladimirWork,zhigunenko.dsr,,,,,,,,,,"24/Jul/18 6:30 PM;dsurnin;Changes implemented in
https://github.com/hyperledger/indy-node/pull/806

Script doc updated.
Please note REG_ENTRY_REVOC must be used with batch size equal to 1 only.;;;","01/Aug/18 7:10 PM;zhigunenko.dsr;*Actual results:*
Revocation transactions are workable;;;",,,,,,,,,,,,,,,,,,,,,,,
Migration fails in case of upgrade to version with new transactions format,INDY-1379,30754,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,zhigunenko.dsr,ozheregelya,ozheregelya,31/May/18 2:04 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Environment:*
indy-node 1.3.428 -> 1.3.434
(docker pool)

*Steps to Reproduce:*
1. Setup the pool with indy-node=1.3.428
2. Upgrade the pool using POOL_UPGRADE or upgrade it using apt and apply migrations manually.

*Actual Results:*
POOL_UPGRADE was failed, migrations were not applied manually with similar error:
{code:java}
>>> migrate(""1.3.428"",""1.3.434"",10)
2018-05-30 16:53:37,046 | INFO | migration_tool.py ( 52) | migrate | Migrating from 1.3.428 to 1.3.434 on Ubuntu
2018-05-30 16:53:37,051 | DEBUG | migration_tool.py ( 54) | migrate | Found migration scripts: ['1_0_96_to_1_0_97', '1_1_150_to_1_1_151', '1_2_188_to_1_2_189', '1_2_233_to_1_2_234', '1_2_273_to_1_2_274', '1_3_396_to_1_3_397', '1_3_428_to_1_3_429', '1_3_433_to_1_3_434', 'disabled_1_0_97_to_1_0_96', 'helper_1_0_96_to_1_0_97']
2018-05-30 16:53:37,066 | INFO | migration_tool.py ( 61) | migrate | Following migrations will be applied: ['1_3_428_to_1_3_429', '1_3_433_to_1_3_434']
2018-05-30 16:53:37,068 | INFO | migration_tool.py ( 63) | migrate | Applying migration 1_3_428_to_1_3_429
2018-05-30 16:53:37,070 | INFO | migration_tool.py ( 35) | _call_migration_script | script path /usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_428_to_1_3_429.py
2018-05-30 16:53:39,408 | INFO | 1_3_428_to_1_3_429.py (172) | archive_old_ledger | Archive of old transaction format ledger created: /tmp/Node1_old_txn_ledger.tar.gz
2018-05-30 16:53:40,578 | INFO | 1_3_428_to_1_3_429.py (194) | migrate_all | All txn logs migrated successfully from old to new transaction format
2018-05-30 16:53:40,579 | INFO | 1_3_428_to_1_3_429.py (201) | migrate_all | All hash stores migrated successfully from old to new transaction format
2018-05-30 16:53:40,586 | INFO | 1_3_428_to_1_3_429.py (208) | migrate_all | All states migrated successfully from old to new transaction format
2018-05-30 16:53:40,587 | INFO | 1_3_428_to_1_3_429.py (215) | migrate_all | Timestamp store migrated successfully from old to new transaction format
2018-05-30 16:53:40,587 | INFO | 1_3_428_to_1_3_429.py (222) | migrate_all | BLS signature store migrated successfully from old to new transaction format
2018-05-30 16:53:40,587 | INFO | 1_3_428_to_1_3_429.py (231) | <module> | Migration of txns format
2018-05-30 16:53:40,710 | INFO | migration_tool.py ( 67) | migrate | Migration 1_3_428_to_1_3_429 applied in 3.640324115753174 seconds
2018-05-30 16:53:40,711 | INFO | migration_tool.py ( 63) | migrate | Applying migration 1_3_433_to_1_3_434
2018-05-30 16:53:40,711 | INFO | migration_tool.py ( 35) | _call_migration_script | script path /usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_433_to_1_3_434.py
Traceback (most recent call last):
File ""/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_433_to_1_3_434.py"", line 84, in <module>
if migrate_all():
File ""/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_433_to_1_3_434.py"", line 54, in migrate_all
hash_store = initHashStore(config_helper.ledger_dir, ""pool"", config, read_only=True)
File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 54, in initHashStore
read_only=read_only)
File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/db_hash_store.py"", line 22, in __init__
self.open()
File ""/usr/local/lib/python3.5/dist-packages/plenum/persistence/db_hash_store.py"", line 90, in open
self.db_type, self.dataDir, self.nodes_db_name, read_only=self._read_only)
File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 24, in initKeyValueStorage
return KeyValueStorageRocksdb(dataLocation, keyValueStorageName, open, read_only)
File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 23, in __init__
self.open()
File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_rocksdb.py"", line 28, in open
self._db = rocksdb.DB(self._db_path, opts, read_only=self._read_only)
File ""rocksdb/_rocksdb.pyx"", line 1437, in rocksdb._rocksdb.DB.__cinit__
File ""rocksdb/_rocksdb.pyx"", line 84, in rocksdb._rocksdb.check_status
rocksdb.errors.RocksIOError: b'IO error: While opening a file for sequentially reading: /var/lib/indy/sandbox/data/Node1/pool_merkleNodes/CURRENT: No such file or directory'
2018-05-30 16:53:42,798 | ERROR | migration_tool.py ( 44) | _call_migration_script | Migration failed: script returned 1
Traceback (most recent call last):
File ""<stdin>"", line 1, in <module>
File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/migration_tool.py"", line 65, in migrate
_call_migration_script(migration, current_platform, timeout)
File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/migration_tool.py"", line 45, in _call_migration_script
raise Exception(msg)
Exception: Migration failed: script returned 1
{code}
 

*Expected Results:*
Pool upgrade should work.",,,,,,,,,,,,,,,,,INDY-1348,,,,,,,,,,,,,,,,,INDY-1332,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzczj:",,,,,,EV 18.11 Stability/ViewChange,,,,,,,,,,,,,,,,,,,,ozheregelya,sergey-shilov,zhigunenko.dsr,,,,,,,,,,"04/Jun/18 3:39 PM;sergey-shilov;Fixed in scope of [INDY-1332|https://jira.hyperledger.org/browse/INDY-1332].;;;","04/Jun/18 11:48 PM;zhigunenko.dsr;Pool uprade successfully finished (via pool-upgrade transaction)
indy-node 1.3.428 -> 1.3.440;;;",,,,,,,,,,,,,,,,,,,,,,,
Looper enhancements,INDY-1380,30771,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,31/May/18 11:13 PM,05/Jun/18 9:50 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"As stated in RBFT, the work must be equally and efficiently distributed between actors and IO operations must be done efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-6,,Looper enhancements,To Do,,,,,,,"1|hzyvdz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Client-to-node communication,INDY-1381,30772,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,31/May/18 11:15 PM,18/Jan/20 12:31 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,Use Agent-to-Agent pattern as Client-to-Node communication,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-8,,Client-to-node communication,To Do,,,,,,,"1|hzxwzr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"18/Jan/20 12:30 AM;esplinr;This is the number one issue that Evernym sees with new installations.

Private Evernym upstream issue is here:
https://evernym.atlassian.net/browse/CM-2383;;;",,,,,,,,,,,,,,,,,,,,,,,,
Independent Replicas,INDY-1382,30773,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,31/May/18 11:16 PM,25/Oct/19 9:12 PM,28/Oct/23 2:47 AM,11/Oct/19 12:01 AM,,,,,,0,,,,Replicas in RBFT must be independent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ghx-label-1,,Independent Replicas,Done,,,,,,,"1|hzyg73:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"11/Oct/19 12:01 AM;ashcherbakov;Doesn't make sense since we are moving to Aardvark;;;",,,,,,,,,,,,,,,,,,,,,,,,
Simplify current view change process,INDY-1383,30782,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,01/Jun/18 2:28 AM,01/Oct/19 7:57 PM,28/Oct/23 2:47 AM,14/Jun/19 9:30 PM,,,,,,0,,,,"Different investigations showed that while increasing view change timeouts did help, it seems that next problem is with catchup that goes on during view change. 

View change should be simplifed:
- catchup should not start at the view change start
- no catchup should happen during view change for any reason
- at the end of view change uncommited state should be cleared
- catchup should start after the end of view change (it's still to be determined whether explicit start is needed, or nodes will be smart enough to start it themselves when needed)
",,,,,,,,,,,,,,,,,INDY-1384,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzwvif:00001ywhii",,,,,,,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"14/Jun/19 9:30 PM;ashcherbakov;PBFT View Change will be implemented instead;;;",,,,,,,,,,,,,,,,,,,,,,,,
Simplify current catchup process,INDY-1384,30783,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey.khoroshavin,sergey.khoroshavin,01/Jun/18 2:31 AM,09/Oct/19 5:11 PM,28/Oct/23 2:47 AM,09/Oct/19 5:11 PM,,2.0,,,,0,,,,"Current catchup process is very complex, mainly due to tight entanglement with view change. When INDY-1383 is done it should be possible to get rid of most functionality associated with view change in the catchup, which should greatly simplify code and make further bug fixing faster.",,,,,,,,,,INDY-1383,,,,,,,,,,,,,,INDY-478,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzwx4f:2rzl2",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"09/Oct/19 5:11 PM;ashcherbakov;We simplified it in the scope of other tasks;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix calculation of prepared certificates during View Change,INDY-1385,30828,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,01/Jun/18 5:36 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"As of now, prepared certificate is calculated based on received COMMITs which means that 3PC batch may be not actually prepared (have quorum of PREPARE) on the Replica.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzcjj:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,2.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"04/Jul/18 5:30 PM;anikitinDSR;Problem reason;
 * prepared sertificate was calculated by using commits and did not take into account prepare messages and quorum of it.

Changes:
 * For now, last prepared sertificate is calculated by using quorum of prepares

Versions:
 * indy-node: 1.4.490-master
 * indy-plenum: 1.4.431-master

Recommendations for QA:
 * Affect of this change can be checked by load_test running;;;","06/Jul/18 5:47 PM;VladimirWork;Build Info:
indy-node 1.4.492

Steps to Validate:
1. Run load tests against QA Live Pool with forced view change / default pool parameters.
2. Check all nodes' logs for ""incorrect state trie"" messages.
3. Check all nodes' ledger count and consensus.

Actual Results:
There is no ""incorrect state trie"" entries during both load tests. The only issue found was reported as INDY-1454.;;;",,,,,,,,,,,,,,,,,,,,,,,
Limit the number of requested PROPAGATES in MessageRequests,INDY-1386,30829,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,01/Jun/18 6:21 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Jul/18 9:19 PM;VladimirWork;Node17.log.350.xz;https://jira.hyperledger.org/secure/attachment/15272/Node17.log.350.xz","06/Jul/18 9:19 PM;VladimirWork;Node17.log.360.xz;https://jira.hyperledger.org/secure/attachment/15271/Node17.log.360.xz","02/Jun/18 7:22 AM;sergey.khoroshavin;Screenshot_2018-06-02_00-23-28.png;https://jira.hyperledger.org/secure/attachment/15040/Screenshot_2018-06-02_00-23-28.png","02/Jun/18 7:21 AM;sergey.khoroshavin;Screenshot_2018-06-02_00-59-13.png;https://jira.hyperledger.org/secure/attachment/15039/Screenshot_2018-06-02_00-59-13.png","02/Jun/18 7:21 AM;sergey.khoroshavin;logs.tar.xz;https://jira.hyperledger.org/secure/attachment/15038/logs.tar.xz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhqf:",,,,,,EV 18.13 Benchmark hardening,EV 18.14 Monitoring/Stability,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,Toktar,VladimirWork,,,,,,,,,"02/Jun/18 7:27 AM;sergey.khoroshavin;Attached graphs with different timescale to show size of problem (Y is MESSAGE_REQUEST(PROPAGATE) sent per second, scale is about 5600). Also attached detailed graph in time area where problem was first found and filtered logs from Node14.

At the first glance it can be seen that at first node receives and discards (because already ordered) lots of propagated requests, then does some view change and catchup, and then throws a burst of MESSAGE_REQUESTSs many of which contain same reqId that was discarded previously.;;;","29/Jun/18 6:02 PM;Toktar;*Problem reason:*
 * If view change happen than master replicas already ordered transaction and not all backup replicas do it, then slow replicas continue ordering after view change but can't to do it because most replicas ordereded this transaction and clean data from request list.

*Changes:*
 * add tests in test_different_last_ordered_before_view_change.py,
 * clear requests after view change for ordered txns,
 * change last ordered after view change to maximum of ordered on master and current value.

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/781]
 * [https://github.com/hyperledger/indy-node/pull/808]

*Version:*
 * indy-node 1.3.493 -master
 * indy-plenum 1.4.435 -master

*Risk factors:*
 * Problem with backup replicas, сonsequently often view changes and large logs and bad performance

*Risk:*
 * Low

*Covered with tests:*
 * *[test_different_last_ordered_before_view_change.py|https://github.com/hyperledger/indy-plenum/pull/781/files#diff-7666afce4380260ea92fe195bcd62c83]*
 ** test_different_last_ordered_on_backup_before_view_change
 ** test_different_prepares_on_backup_before_view_change
 ** test_different_last_ordered_on_master_before_view_change
 * [test_api.py|https://github.com/hyperledger/indy-plenum/pull/781/files#diff-936570e7f1de2f0f56e3ea3e4c63b421]
 ** test_create_3pc_batch_with_empty_requests

*Recommendations for QA:*

Run load test and check load, count of view changes and log size. Check decrease of count of message requests with propagate if it possible. ;;;","06/Jul/18 9:12 PM;VladimirWork;Build Info:
indy-node 1.4.493

Steps to Reproduce:
1. Run load test against 22 nodes pool of version 492 master.
2. Check time between log archives and count number of lines with PROPAGATE word.
3. Upgrade this pool to 493 master and run load test with the same parameters.
4. Check time between log archives and count number of lines with PROPAGATE word.

Actual Results:
The number of view changes before and after upgrade is about the same (0) except the period right after the upgrade (it is expected because of nodes restarting).
Time between log archives making is about 3-4 minutes (493) against 7-10 (492) so it looks like we have more logs after the upgrade.
The number of lines with PROPAGATE word (`find /var/log/indy/sandbox/NodeXX.log.XXX.xz -exec xzcat {} \; | grep ""PROPAGATE"" | wc -l`) is about 60k-80k (493) against 33k-34k (492) in the same size log archive so it looks like we have more message requests with propagate after the upgrade.

Expected Results:
There should be less amount of logs and propagate messages after the upgrade against the same load.;;;","07/Jul/18 12:06 AM;VladimirWork;Retest with forced view changes (492 -> 494).;;;","10/Jul/18 7:35 PM;VladimirWork;Build Info:
indy-node 1.4.495

Steps to Validate:
0. Set forced view change frequency every 1800 seconds in indy_config.py.
1. Run load test against 22 nodes pool of version 492 master.
2. Check time between log archives and count number of lines with ""MESSAGE_REQUEST.*PROPAGATE"".
3. Upgrade this pool to 495 master and run load test with the same parameters.
4. Check time between log archives and count number of lines with ""MESSAGE_REQUEST.*PROPAGATE"".

Actual Results:

Before the upgrade:
{noformat}
ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.45*.xz -exec xzcat {} \; | grep ""MESSAGE_REQUEST.*PROPAGATE"" | wc -l
1539564
ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.46*.xz -exec xzcat {} \; | grep ""MESSAGE_REQUEST.*PROPAGATE"" | wc -l
959577
ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.47*.xz -exec xzcat {} \; | grep ""MESSAGE_REQUEST.*PROPAGATE"" | wc -l
795270
{noformat}

After the upgrade:
{noformat}
ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.49*.xz -exec xzcat {} \; | grep ""MESSAGE_REQUEST.*PROPAGATE"" | wc -l
17190
ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.50*.xz -exec xzcat {} \; | grep ""MESSAGE_REQUEST.*PROPAGATE"" | wc -l
6675
ubuntu@oregonQALarge22:/var/log/indy/sandbox$ find /var/log/indy/sandbox/Node22.log.51*.xz -exec xzcat {} \; | grep ""MESSAGE_REQUEST.*PROPAGATE"" | wc -l
35523
{noformat};;;",,,,,,,,,,,,,,,,,,,,
Change default config settings for a better stability,INDY-1387,30852,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,04/Jun/18 6:17 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,"There are recommendations for a better stability found in the scope of INDY-1334.


*Acceptance criteria:*
 * Change default config according to this settings
 * Make sure that all the tests pass",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzcnz:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,ozheregelya,,,,,,,,,,"21/Jun/18 4:14 PM;anikitinDSR;Reason:
 * using recommendations for a better stability from [INDY-1334|https://jira.hyperledger.org/browse/INDY-1334], need to change parameters into main config file.

Changes:
 * change parameters into configs according to recommendation

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/737]

Version:
 * indy-node: 1.4.470

Recommendation for QA;
 * test again with load_scripts as in [INDY-1334|https://jira.hyperledger.org/browse/INDY-1334]

 

 ;;;","21/Jun/18 7:32 PM;ozheregelya;Load test with specified parameters was performed. The pool was able to write ~970,000 txns. View change was happened on ~950,000 txns. Some nodes were broken (INDY-1422), but pool didn't lose consensus. 

Additional testing of these parameters will be performed in scope of INDY-1419.;;;",,,,,,,,,,,,,,,,,,,,,,,
Prove stability under a DOS of an Indy network,INDY-1388,30876,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,esplinr,esplinr,05/Jun/18 6:21 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,blocked,,,"Before encouraging people to use the Sovrin network for live loads, we need to prove that network stability will be preserved even if availability to the network is denied due to very high loads.

*Acceptance Criteria*
 Perform a test of an Indy network that has the following attributes:
 * The ledger is pre-loaded with at least 1 million transactions
 * Pool size at least matches the number of network nodes initially expected in the Sovrin network (currently 25 nodes).
* While maintaining a base line of legitimate read and write transactions, flood the pool with sufficient new read and write requests so that the external connection buffer is full and requests are denied for 10 minutes.
** These requests should be in a similar mix to INDY-1343 (90% reads / 10% writes)
** The requests should come from at least 10K concurrent connections
** Throughput sufficient to cause the pool to delay in responding
* Stop flooding the pool and allow the network to catch up.
* Report on:
** the level of traffic necessary to cause the network to deny new connections
** any inconsistencies in the state of the pool after the network has caught up
** the time necessary for the network to catch up and begin processing new transactions

Any problems found will be logged in JIRA as separate issues for independent prioritization.

As part of this issue, it is recognized that it will be necessary to create a load testing tool sufficient to perform the test.",,,,,,,,,,,,,,,,,,,,,,,INDY-1343,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,"1|hzwwjj:",,,,,,EV 18.14 Monitoring/Stability,EV 18.15 Stability/Availabilit,EV 18.16 Releasing 1.6,,,,,,8.0,,,,,,,,,,,,esplinr,krw910,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,,"31/Jul/18 12:05 AM;ozheregelya;*Test run: 1*
*Environment:*
indy-node 1.5.527 (25 AWS nodes)
libindy 1.6.1~655
*Ledger size:* 325K -> 450K
*Goal:* Pool should catch up after stop the flooding. New clients should be able to connect and write.
*Load:*
{code:java}
python3 perf_processes.py -g ~/stab_transactions_genesis -m t -c 450 -t 1 -n 1 -k nym{code}
from 10 client instances

*Results:*
Duration of load - 16 min
Duration of processing all txns: 2.5 hours
Transactions written: ~140K of 160K
Load script throughput: 160txns/sec
Pool throughput: 15.8 txns/sec

*Current behavior:*
160 writing txns/sec were send to the pool during 16 min. After that load test was stopped and there were no load on the pool. Pool was working on processing of txns during 3 hours. ~20K txns were missed. New clients were not able to connect to the pool because of timeout error.

[~esplinr], [~krw910], FYI.;;;","08/Aug/18 12:30 AM;ozheregelya;*Test run: 2*
 *Environment:*
 indy-node 1.5.543 (25 AWS nodes, standard pool)
 libindy 1.6.1~655
 *Ledger size:* 30 -> 87K
 *Goal:* Pool should catch up after stop the flooding. New clients should be able to connect and write.
 *Load:*

Writing (from 2 client instances):
{code:java}
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 563 -g ~/stab_transactions_genesis_new -k ""[{\""nym\"": {\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}]""{code}
Reading (from 8 client instances):
{code:java}
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 1125 -g ~/stab_transactions_genesis_new -k ""[{\""get_nym\"": {\""count\"": 9}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}]""{code}
*Results:*
 Duration of read load - 2 h
Duration of write load - 30 min
 Duration of processing all txns: 2 h 15 min
 Transactions written: 85836 
Transactions read: 5555965 (~20K requests got timeout error after start of writing load).
 Load script throughput: 47 txns/sec - write, 770 txns/sec - read
 Pool throughput for writing: 10.6 txns/sec

*Current behavior:*
 There is no big differences with the previous results. New clients were not able to connect to the pool because of timeout error during all 2 hours of txns processing.;;;","17/Aug/18 5:57 PM;ozheregelya;All test results are placed here (all cases with Clients=""2x500 (w) 8x1125 (r)"", Delay=""0.0001""):
[https://docs.google.com/spreadsheets/d/1DTjDsLSysFBiKU-9z4-IzunJk4wEy44hE_PGZYxnN_8/edit#gid=1813415708]

Current behavior is that the pool tries to process all incoming requests (even if it was send several hours ago). But it can't write more than ~10.5 txns/sec (min throughput 9 txns/sec, max throughput 12 txns/sec). If clients write more than 10-11 txns/sec, pool will not get on time with processing of requests. So, pool still will be busy even after stopping the test. For load of 50 writing and 500 reading requests pool process all requests during ~2.5 hours. Note that during this time new clients will get timeout errors on attempts to connect.

This data is actual for all txns exclude payments and revocations because now we have some problems with them in load script. Test will be run with all types of txns after these problems will be fixed.

This test will run on weekly base. Results will be added to the 'Load and Performance Info' spreadsheet.;;;",,,,,,,,,,,,,,,,,,,,,,
Support Proof of Possession for BLS keys,INDY-1389,30895,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,esplinr,esplinr,06/Jun/18 1:44 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.73,,,,0,,,,"Details are contained here:

[https://docs.google.com/document/d/14wcYSn9XXX6-0M5le1icT6N-funiB7K9cActzYfILto/edit]

*Acceptance Criteria:*
- Use the improved indy-crypto library from IS-750
- Make sure that we can extend NODE txn in a non-breaking way
- Extend NODE txn with a (optional) field to specify Proof of possession for BLS key
- Validate the Proof of possession for BLS once a NODE txn is received
- Blacklist the Node if verification failed
- Make sure that we verify the proof after the node is started


For future: Make the proof required, not optional, so every node should have the proof if it has a BLS key.

 ",,,,,,,,,,IS-750,,,,,,,IS-848,,,,,,,,,,,,INDY-1796,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwmv:",,,,,,EV 18.17 Service Pack,,,,,,,,5.0,,,,,,,,,,,,esplinr,lovesh,ozheregelya,Toktar,,,,,,,,,"15/Jun/18 12:04 AM;lovesh;Please keep me in loop on this;;;","26/Jun/18 12:47 AM;lovesh;Please see the update in the doc;;;","29/Jun/18 6:42 AM;lovesh;Please see the short term and long term solution in the doc;;;","30/Jun/18 12:53 AM;lovesh;The PoA has been approved. Check the doc.;;;","15/Aug/18 6:10 PM;Toktar;PR:
 *  [https://github.com/hyperledger/indy-plenum/pull/867]
 * [https://github.com/hyperledger/indy-node/pull/899];;;","20/Aug/18 4:58 PM;Toktar;Problem reason:
 - The previous BLS scheme has a bug called the rogue public key attack where an attacker can forge aggregate signatures by non-participating members.In view change a lot of client requests lead to out_of_memory problem and clients receive responces timeout error.

Changes:
 - Added bls public key proof of possession in transactions with adding and changing bls key.
 - Added validation of bls key via proof of possession
 - Increment indy crypto to the stable version 0.4.3

PR:
 * [https://github.com/hyperledger/indy-node/pull/902]
 * [https://github.com/hyperledger/indy-plenum/pull/880]
 * [https://github.com/hyperledger/indy-node/pull/899]
 * [https://github.com/hyperledger/indy-plenum/pull/867]

Version:
 * indy-node 1.6.570 -master
 * indy-plenum 1.6.513 -master

Risk factors:
 - Problem with adding new nodes and changing a node data.

Risk:
 - Low

Covered with tests:
 - [test_pool_req_handler_static_validation.py|https://github.com/hyperledger/indy-plenum/pull/867/files#diff-f4b9684b90a1ca7bfbcc06535abee8ee]
 - [test_nodes_data_changed.py|https://github.com/hyperledger/indy-plenum/pull/867/files#diff-2847b93c9ba9f724b810a04903d6a491]

Recommendations for QA:
 * Try to add new nodes, change bls key, change other data.
 * May be update all nodes besides one and change bls keys on it. Pull shouldn't process it transaction and should send NACK.
 * Send a Node transaction without proof or only proof without bls key.
 * Try to reproduce the rogue public key attack.;;;","25/Aug/18 1:49 AM;ozheregelya;*Environment:*
 indy-node 1.6.575
 indy-cli 1.6.2~714

*Steps to Validate:*
~~~Valid cases~~~
 1. Add node with valid pair blsley+blskey_pop. (/)
 2. Update node data (IPs, ports, services). (/)
 3. Demote and promote node back. (/)
 4. Rotate node blskey. (/)
 5. Check old CLI. (/)
~~~Invalid cases~~~
 1. Try to add node with valid blskey and invalid blskey_pop.
 => REQNACK from node received, error message in indy-cli: Transaction has been rejected: Proof of possession <blskey_pop> is incorrec for BLS key <blskey>
 2. Try to add node with valid blskey and without blskey_pop.
 => REQNACK from node received, error message in indy-cli: Transaction has been rejected: A Proof of possession must be provided with BLS key
 3. Try to add node with valid blskey_pop and without blskey.
 => REQNACK from node received, error message in indy-cli: Transaction has been rejected: A Proof of possession is not needed without BLS key
 4. Try to send blskey_pop in invalid format. 
 => Format validation works.

*Actual Results:*
 Proof of Possession was added for BLS keys and works as expected. Existing functionality works as well, no regression was found. ;;;",,,,,,,,,,,,,,,,,,
Processing of missed batches was not happened on added node after sending several 3PC batches,INDY-1390,30924,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,07/Jun/18 3:17 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Steps to Reproduce:*
 1. Setup the pool of 4 nodes.
 2. Add one more node:
 2.1. Copy genesis files from active nodes to additional one.
 2.2. Run `sudo su - indy -c ""init_indy_node Node5 0.0.0.0 9701 0.0.0.0 9702 000000000000000000000000000node5""`.
 2.3. Start and enable indy-node.
 2.4. Send node txn using indy-cli: `ledger node target=4Tn3wZMNCvhSTXPcLinQDnHyj56DTLQtL61ki4jo2Loc client_port=9702 client_ip=10.0.0.6 alias=Node5 node_ip=10.0.0.6 node_port=9701 services=VALIDATOR blskey=2RdajPq6rCidK5gQbMzSJo1NfBMYiS3e44GxjTqZUk3RhBdtF28qEABHRo4MgHS2hwekoLWRTza9XiGEMRCompeujWpX85MPt87WdbTMysXZfb7J1ZXUEMrtE5aZahfx6p2YdhZdrArFvTmFWdojaD2V5SuvuaQL4G92anZ1yteay3R`
 => Processing of missed batches was not happened on added node.
 3. Send new txn.

*Actual Results:*
 Domain ledger size on added node is still 9 (genesis txns only).

*Expected Results:*
 Added node should complete processing of missed batches and should work.

*Additional Information:*
 1. Genesis files are the same as on all nodes (md5sum of all genesis files are the same). The only difference is that 10.0.0.6 was used as node and client IP in init_indy_node instead of usual 0.0.0.0, but this configuration works (this issue was not reproduced second time with the same configuration).
 2. After described steps node was demoted in order to have only working nodes in the pool. After that one node was stopped to check that catch up basically works, after that pool stopped working by some reason (as far as I understand, we should be still in consensus).","indy-node 1.3.440
docker pool 4 + 1 nodes",,,,,,,,,,,,,,,,,,,,,,,INDY-1401,,,,,INDY-1391,,,,,,,,,,,,,,,,,,,,"07/Jun/18 3:20 AM;ozheregelya;Node1.log;https://jira.hyperledger.org/secure/attachment/15060/Node1.log","07/Jun/18 3:20 AM;ozheregelya;Node2.log;https://jira.hyperledger.org/secure/attachment/15061/Node2.log","07/Jun/18 3:20 AM;ozheregelya;Node3.log;https://jira.hyperledger.org/secure/attachment/15062/Node3.log","07/Jun/18 3:20 AM;ozheregelya;Node4.log;https://jira.hyperledger.org/secure/attachment/15063/Node4.log","07/Jun/18 3:20 AM;ozheregelya;Node5.log;https://jira.hyperledger.org/secure/attachment/15064/Node5.log","18/Jun/18 8:26 PM;ozheregelya;indy1390_MAX_RECONNECT_RETRY_ON_SAME_SOCKET_1.7z;https://jira.hyperledger.org/secure/attachment/15119/indy1390_MAX_RECONNECT_RETRY_ON_SAME_SOCKET_1.7z","18/Jun/18 8:12 PM;ozheregelya;indy1390_MAX_RECONNECT_RETRY_ON_SAME_SOCKET_1000.7z;https://jira.hyperledger.org/secure/attachment/15118/indy1390_MAX_RECONNECT_RETRY_ON_SAME_SOCKET_1000.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwwin:",,,,,,EV 18.12 Release RocksDB,,,,,,,,,,,,,,,,,,,,ozheregelya,sergey-shilov,,,,,,,,,,,"18/Jun/18 8:26 PM;ozheregelya;The issue was reproduced on the latest master version (indy-node 1.4.464) and node behavior after adding differs from initial one. All missed txns are processed right after node adding, before first batch sending. 
 Logs:
 MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 1000 :
 -> [^indy1390_MAX_RECONNECT_RETRY_ON_SAME_SOCKET_1000.7z]
 MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 1 : 
 -> [^indy1390_MAX_RECONNECT_RETRY_ON_SAME_SOCKET_1.7z]

Note that initial steps (up the node -> add the node -> send txn *before* processing of missed txn by added node) are impossible with small ledger because node processes missed txns right after adding.;;;","19/Jun/18 9:45 PM;sergey-shilov;See the [comment|https://jira.hyperledger.org/browse/INDY-1401?focusedCommentId=46192&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-46192] in ticket [INDY-1401|https://jira.hyperledger.org/browse/INDY-1401].;;;","21/Jun/18 8:48 PM;sergey-shilov;*Problem state / reason:*

Sometimes we face troubles with node-to-node connecting process. Each node-to-node logical connection consists of two TCP connections. A node connects to other node and sends packets using only this connection. A node receives packets using incoming connection. If one of these connections is not established then nodes can not communicate.
In this ticket nodes had been failed to connect until they recreated their sockets to connect. Seems like it is a bug of ZeroMQ.

*Changes:*

As soon as recreating of ZeroMQ sockets solves connection issue we've made a fix to reduce connections retrying using the same socket:

_MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 1_

*Committed into:*

[https://github.com/hyperledger/indy-plenum/pull/749]

[https://github.com/hyperledger/indy-node/pull/767]

indy-node 1.4.470-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

Repeate tests with adding of nodes and check that all nodes are connected.;;;","22/Jun/18 4:18 AM;ozheregelya;*Environment:*
indy-node 1.4.470
libindy 1.4.0~596

*Steps to Validate:*
1. Setup the pool of 4 nodes.
2. Init 2 more nodes (init_indy_node).
3. Send Node txn for one of these nodes.
4. Start both of additional nodes.
5. Check txns count on all nodes.
=> Added node processed all missed txns. Not added node has only genesis txns.
6. Send txn.
=> Txn successfully written on added node.
7. Add the second node, check txns count.
=> Added node processed all missed txns. All nodes have the same count of txns.
8. Send txn.

*Actual Results:*
Nodes were successfully added, all nodes have the same amount of txns.;;;",,,,,,,,,,,,,,,,,,,,,
3PC batch which initiated processing of missed batches was not written on promoted node,INDY-1391,30925,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ozheregelya,ozheregelya,07/Jun/18 3:17 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,TShirt_M,,,"*Steps to Reproduce:*
 1. Setup the pool of 4 nodes.
 2. Add one more node:
 2.1. Copy genesis files from active nodes to additional one.
 2.2. Run `sudo su - indy -c ""init_indy_node Node5 10.0.0.6 9701 10.0.0.6 9702 000000000000000000000000000node5""`.
 2.3. Start and enable indy-node.
 2.4. Send node txn using indy-cli: `ledger node target=4Tn3wZMNCvhSTXPcLinQDnHyj56DTLQtL61ki4jo2Loc client_port=9702 client_ip=10.0.0.6 alias=Node5 node_ip=10.0.0.6 node_port=9701 services=VALIDATOR blskey=2RdajPq6rCidK5gQbMzSJo1NfBMYiS3e44GxjTqZUk3RhBdtF28qEABHRo4MgHS2hwekoLWRTza9XiGEMRCompeujWpX85MPt87WdbTMysXZfb7J1ZXUEMrtE5aZahfx6p2YdhZdrArFvTmFWdojaD2V5SuvuaQL4G92anZ1yteay3R`
 => Node was successfully added and it processed all missed batches. Domain ledger size on all nodes is 11 txns (9 genesis + 2 nyms).
 3. Make sure that node works by adding new txns.
 => Domain ledger size on all nodes is 13 txns.
 4. Demote added node: `ledger node target=4Tn3wZMNCvhSTXPcLinQDnHyj56DTLQtL61ki4jo2Loc client_port=9702 client_ip=10.0.0.6 alias=Node5 node_ip=10.0.0.6 node_port=9701 services=`
 5. Add several txns.
 => Domain ledger size on demoted node is 13 txns, for the rest ones domain ledger size is 16.
 6. Promote demoted node: `ledger node target=4Tn3wZMNCvhSTXPcLinQDnHyj56DTLQtL61ki4jo2Loc client_port=9702 client_ip=10.0.0.6 alias=Node5 node_ip=10.0.0.6 node_port=9701 services=VALIDATOR`
 => Domain ledger sizes are not changed.
 7. Send one txn.

*Actual Results:*
 Domain ledger size on node which was demoted is 16 txns, for the rest ones domain ledger size is 17. Domain ledger size on node which was demoted is not changing after writting more txns.

*Expected Results:*
 17th txn should be also written after processing of missed batches.","indy-node 1.3.443
docker pool 4 + 1 nodes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1401,INDY-1390,,,,,,,,,,,,,,"26/Jun/18 7:25 PM;VladimirWork;INDY-1391.PNG;https://jira.hyperledger.org/secure/attachment/15176/INDY-1391.PNG","07/Jun/18 3:18 AM;ozheregelya;Node1.log;https://jira.hyperledger.org/secure/attachment/15055/Node1.log","07/Jun/18 3:18 AM;ozheregelya;Node2.log;https://jira.hyperledger.org/secure/attachment/15056/Node2.log","07/Jun/18 3:18 AM;ozheregelya;Node3.log;https://jira.hyperledger.org/secure/attachment/15057/Node3.log","07/Jun/18 3:18 AM;ozheregelya;Node4.log;https://jira.hyperledger.org/secure/attachment/15058/Node4.log","07/Jun/18 3:18 AM;ozheregelya;Node5.log;https://jira.hyperledger.org/secure/attachment/15059/Node5.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzcmf:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,ozheregelya,VladimirWork,,,,,,,,,,,"26/Jun/18 7:26 PM;VladimirWork;The issue is reproducing on 1.4.63 with exact the same steps.

Workaround: Restart the 5th node to catch it up and work normally. !INDY-1391.PNG|thumbnail! ;;;","26/Jun/18 9:51 PM;VladimirWork;It is expected behaviour according to latest checkpoint/catch up changes (we should restart indy-node service after node adding/promoting).;;;",,,,,,,,,,,,,,,,,,,,,,,
"As a developer, I need to have migration guide from Indy-node 1.3 to 1.4 ",INDY-1392,30932,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,07/Jun/18 4:15 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,1.4,1.4,,,,0,,,,"As upcoming Indy-node 1.4 release contains breaking changes, we need to write a migration with all the changes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzzcpb:",,,,,,EV 18.12 Release RocksDB,,,,,,,,1.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"13/Jun/18 10:47 PM;anikitinDSR;Reason:
 * need to create migration doc with breacking change description for release 1.4

Changes:
 * migration guide

PR:

[https://github.com/hyperledger/indy-node/pull/754]

 ;;;","14/Jun/18 8:31 PM;VladimirWork;Migration guide is created and reviewed.;;;",,,,,,,,,,,,,,,,,,,,,,,
Indy-node 1.4 should reject requests from old unsupported clients gracefully,INDY-1393,30933,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,07/Jun/18 4:21 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,1.4,1.4,,,,0,,,,"As Indy Node 1.4 release contains breaking changes, it can work with the new clients supporting this changes only.

*Acceptance criteria:*
 * Add PROTOCOL_VERSION=2
 * Add PROTOCOL_VERSION to `LedgerStatus` to handle catch ups from old clients
 * If there is LEDGER_STATUS or Request with a protocol version !=2, then discard with the following message: ""make sure that the latest LibIndy is used and `indy_set_protocol_version(2)` is called"".
 * Wait until LibIndy supporting `indy_set_protocol_version(2)` is issued and use this LibIndy in integration tests
 * Call `indy_set_protocol_version(2)` before opening a pool in integration tests
 * If there is a Ledger Status with a protocol version < 2 (that is the one came from old SDK), send back LedgerStatus to force it finish catch-up, so that next reqeusts to the ledger will fail gracefully with an error message to update SDK.",,,,,,,,,,IS-748,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1374,,,,,,,,,"1|hzzcnj:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,Derashe,VladimirWork,,,,,,,,,,"21/Jun/18 12:48 AM;Derashe;Problem reason:
 - We have problems with compatibility between different sdk's and node's protocoloVersion

Changes:
 - protocolVersion added in node's LEDGER_STATUS message
 - New protocol version value added ( TXN_FORMAT_1_0_SUPPORT == 2), which now is actual
 - Add logic that actual node handles incorrect protocol verisons of LEDGER_STATUS message by sending client's LEDGER_STATUS back and RequestNotAcknowledge with message 'make sure that the latest LibIndy is used and `indy_set_protocol_version(2) is called'.
 - Fixed tests which were using hard-coded protocol version

Committed into:
 - https://github.com/hyperledger/indy-node/pull/772
 - https://github.com/hyperledger/indy-plenum/pull/733

Risk:
 - Low

Covered with tests:
 - test_protocol_version

Recommendations for QA: 

      check these cases:
 * 1.3 node (last stable) works with 1.5 sdk
 * 1.4 node do not work with 1.4 sdk, but gives an error: 'make sure that the latest LibIndy is used and `indy_set_protocol_version(2) is called
 * 1.4 node do not work with 1.5 sdk, if sdk did not set protocol_version
 * 1.4 node works with 1.5 sdk, if sdk set protocol verison;;;","21/Jun/18 1:27 AM;VladimirWork;It will be tested in scope of IS-748.;;;",,,,,,,,,,,,,,,,,,,,,,,
"As I Steward or Indy-Node runner, I need to have a doc with best practises on how to configure firewall rules so that the pool can survive during DDoS attacks",INDY-1394,30935,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,07/Jun/18 5:07 PM,13/Feb/19 10:03 PM,28/Oct/23 2:47 AM,,1.5,,,,,0,,,,"*Acceptance criteria*
 * The doc needs to be in indy-node/docs folder
 * The doc needs to contain at least the following guidelines for working with IPTables on AWS to:
 ** Limit open file descriptors
 *** [https://github.com/hyperledger/indy-node/blob/master/docs/setup-iptables.md]
 *** [https://github.com/hyperledger/indy-node/blob/master/scripts/setup_iptables]
 ** Use separate NICs
 *** with a link of how-to docs for AWS, Azure, DigitalOcean
 *** These external documents do not need to be tested
 ** Node's interface must listen on whitelisted IPs only (other Nodes' IPs)
 *** with a link to how-to configure such firewall rules
* The rules in the document should be tested to ensure that they work
 ** Testing only needs to be done in an AWS environment
 * Target audience is a Linux system administrator with experience on Ubuntu Linux.
 * This story is for combining INDY-1396 and INDY-1397",,,,,,,,,,INDY-1397,INDY-1396,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwx4f:2rzo",,,,,,,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a dev or Steward reading log files, I need to know what nodes didn't participate in consensus even for ordered requests so that nodes with problems can be detected",INDY-1395,30936,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,07/Jun/18 5:14 PM,11/Oct/19 1:02 AM,28/Oct/23 2:47 AM,11/Oct/19 1:02 AM,1.9.0,,,,,0,,,,"We already log if a request wasn't ordered in 1 minute specifying the nodes that didn't participate in consensus.

We need to log what nodes didn't participate even for ordered requests so that we can detect what nodes may have problems

 

*Acceptance criteria*
 * If a node didn't participate in ordering of N sequential requests (that is no PREPAREs nor COMMITs received from the node), we need to log a warning: ""Node X didn't participate in ordering of N requests""",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy2f:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"11/Oct/19 1:02 AM;ashcherbakov;We have other means to detect it from logs, validator info and metrics;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As I Steward or Node runner, I need to have instructions on how to configure separate NICs",INDY-1396,30937,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,07/Jun/18 5:18 PM,13/Feb/19 10:03 PM,28/Oct/23 2:47 AM,,1.5,,,,,0,,,,"*Acceptance criteria*
 * The doc need to be in indy-node/doc folder
 * It should link to instructions for configuring a second NIC for AWS, Azure, and DigitalOcean
 * It should contain instructions for configuring Ubuntu 16.04 on AWS
 ** Linking to a well maintained external official document for this topic is acceptable
 * The Indy Node server should be configured to bind to two NICs: one for client communications and one for consensus

See INDY-1394 for additional details.",,,,,,,,,,,,,,,,,INDY-1394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwx4f:2rzr",,,,,,,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As I Steward or Node runner, I need to have instructions on how to whitelist IPs on node-to-node NIC",INDY-1397,30938,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,07/Jun/18 5:20 PM,08/Oct/19 8:54 PM,28/Oct/23 2:47 AM,,1.5,,,,,0,,,,"*Acceptance criteria*
 * The doc needs to be in indy-node/docs folder
 * It should contains the rules for iptables in Ubuntu 16.04 on AWS as a reference
 * 1 NIC is public and used for Client communications, and 1 NIC is whitelisted to only talk to other consensus nodes
 * System administrators are expected to use the list provided by the Sovrin Foundation (it is assumed that they already have the list).
 * Documentation should clarify that adding a new node to the pool require these manual steps to keep the list updated
 * Target audience consists of system administrators with experience on Linux systems

Excluded:
* We will not be automatically manipulating the firewall rules

See INDY-1394 for additional details.",,,,,,,,,,,,,,,,,INDY-1394,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-792,,,,,,,,,"1|hzwx4f:2rzp",,,,,,,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a Steward or node runner, I need to be able to be notified when Node's IP is changed or a new Node is added so that whitelist rules can be modified accordingly",INDY-1398,30939,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,07/Jun/18 5:25 PM,11/Oct/19 7:00 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*Acceptance criteria*
 * Every time when a Node's IP is changed or a new Node is added, a Steward needs to be notified
 * We can react when receiving NODE txns with such changes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2249,,,,,,,,,"1|hzwy3j:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a non-Steward, I need to be able to get the status of Validator Nodes",INDY-1399,30940,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,07/Jun/18 5:29 PM,11/Oct/19 6:24 PM,28/Oct/23 2:47 AM,11/Oct/19 6:24 PM,,,,,,0,,,,"*Acceptance criteria*
 * Create a new command GET_VALIDATOR_STATUS
 * Create a new role `observer` to be able to send this command (so that not everyone can do it, but it's not necessary to be a Steward) 
 * a NYM with a `observer` role can be created by Trustees and Stewards
 * The reply should be quite similar to GET_VALIDATOR_INFO one, but should not contain information about machines running the code.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzwy3b:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"28/Feb/19 6:52 AM;esplinr;I think this issue was completed as part of INDY-1916. [~ashcherbakov], do you think something additional should be done?;;;","11/Oct/19 6:24 PM;esplinr;Validator-Info provides lots of information to people with the role of Steward and Network Monitor, but there could be security concerns with providing it to people with no role on the network. Therefore we've decided not to expose additional information at this time.;;;",,,,,,,,,,,,,,,,,,,,,,,
Investigate issues found during load testing of 25-nodes pool with increased timeouts for catchups and viewchange,INDY-1400,30941,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,VladimirWork,VladimirWork,07/Jun/18 6:17 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,We should investigate and fix issues found (incorrect state trie) in scope of INDY-1350 testing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1350,,,,,INDY-1404,INDY-1405,,,,,,,,,,,,,,"21/Jun/18 6:16 PM;ashcherbakov;bug.png;https://jira.hyperledger.org/secure/attachment/15148/bug.png","21/Jun/18 6:16 PM;ashcherbakov;bug_fixed.png;https://jira.hyperledger.org/secure/attachment/15149/bug_fixed.png","08/Jun/18 8:30 AM;sergey.khoroshavin;filtered_logs.tar.xz;https://jira.hyperledger.org/secure/attachment/15083/filtered_logs.tar.xz","21/Jun/18 6:16 PM;ashcherbakov;node_init.png;https://jira.hyperledger.org/secure/attachment/15150/node_init.png","21/Jun/18 6:16 PM;ashcherbakov;node_init_fixed.png;https://jira.hyperledger.org/secure/attachment/15151/node_init_fixed.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcpr:",,,,,,EV 18.12 Release RocksDB,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,spivachuk,VladimirWork,,,,,,,,,"08/Jun/18 8:38 AM;sergey.khoroshavin;Attached filtered logs around problematic case (along with rules to filter). It can be seen that all nodes can be split into 4 groups:
1) nodes that were able to receive (155,11) through COMMITs during view change, and they were not catching up anything
2) nodes that received (155,11) through catch up, they didn't receive COMMITs because they were pended due to last prepared being (155, 10)
3) nodes that were late and received (155,11) through catch up (along with many other things)
4) bad node (16) which started catchup, received some transactions, then received enough COMMITs to order (155,11), then received more transactions through catchup, then view change timeout fired and during next view change ordered batch was commited on top of some transactions that were added due to catchup ;;;","09/Jun/18 5:09 AM;spivachuk;h2. The issue with an incorrect state trie root on Node16

A catch-up on Node16 being performed in scope of view change to the view 156 was interrupted by starting the propagate primary view change to the view 157. In result, {{LedgerManager.last_caught_up_3PC}} had not been updated and this made it possible for Node16 to order 3PC-batch (155, 11) including already caught up transactions during the new view change. Thus duplicates were added to the ledger and so it was corrupted. Under such conditions Node16 fell out from consensus and the next {{PrePrepare}} was discarded as having an incorrect state trie root.

Node4 that remained in consensus executed the batch (155, 10) with the {{txn_seq_no}} range (60330, 60429):
{code:java}
2018-05-31 09:30:19.729 | DEBUG | node.py              (2593) | executeBatch | Node4 storing 3PC key (155, 10) for ledger 1 range (60330, 60429)
{code}
Then it started view change to the view 156 on a quorum of {{InstanceChanges}}:
{code:java}
2018-05-31 09:30:23.064000 | Node4:- | INFO | view_changer.py      ( 468) | do_view_change_if_possible | VIEW CHANGE: Node4 initiating a view change to 156 from 155
{code}
During the view change it executed the batch (155, 11) with the {{txn_seq_no}} range (60430, 60529):
{code:java}
2018-05-31 09:30:41.946 | DEBUG | node.py              (2593) | executeBatch | Node4 storing 3PC key (155, 11) for ledger 1 range (60430, 60529)
{code}
Node16 executed the batch (155, 10) with the same {{txn_seq_no}} range as Node4:
{code:java}
2018-05-31 09:30:14.875 | DEBUG | node.py              (2593) | executeBatch | Node16 storing 3PC key (155, 10) for ledger 1 range (60330, 60429)
{code}
Then it started a view change to the view 156 on a quorum of {{InstanceChanges}}:
{code:java}
2018-05-31 09:30:20.189 | INFO | view_changer.py      ( 468) | do_view_change_if_possible | VIEW CHANGE: Node16 initiating a view change to 156 from 155
{code}
During the view change Node16 started to perform the domain ledger synchronization in scope of some catch-up round:
{code:java}
2018-05-31 09:30:43.117 | DEBUG | ledger_manager.py    ( 802) | startCatchUpProcess | Node16 started catching up with consistency proof CONSISTENCY_PROOF{'oldMerkleRoot': ..., 'ppSeqNo': 11, 'seqNoStart': 60429, 'seqNoEnd': 60529, 'hashes': (...), 'viewNo': 155, 'newMerkleRoot': ..., 'ledgerId': 1}
{code}
It actually caught up only 7 of 100 transactions:
{code:java}
2018-05-31 09:30:43.333 | DEBUG | ledger_manager.py    ( 499) | processCatchupRep | Node16 processed 5 catchup replies with sequence numbers [60430, 60431, 60432, 60433, 60434]
2018-05-31 09:32:43.248 | DEBUG | ledger_manager.py    ( 499) | processCatchupRep | Node16 processed 2 catchup replies with sequence numbers [60435, 60436]
{code}
Then it started a propagate primary view change to the view 157 on a quorum of {{ViewChangeDones}} to a future view from other nodes, so that the catch-up was interrupted:
{code:java}
2018-05-31 09:33:49.359 | INFO | view_changer.py      ( 481) | _start_view_change_if_possible | VIEW CHANGE: Node16 starting view change for 157 after 8 view change indications from other nodes
{code}
During this new view change it executed the batch (155, 11) with the {{txn_seq_no}} range {color:#d04437}(60437, 60536){color} shifted 7 txns upper because some transactions from this batch had already been added to the ledger in scope of the uncompleted catch-up:
{code:java}
2018-05-31 09:33:49.872 | DEBUG | node.py              (2593) | executeBatch | Node16 storing 3PC key (155, 11) for ledger 1 range (60437, 60536)
{code}
This was revealed as a mismatch between the resulting state and the state from {{Ordered}} message (where it was taken from {{PrePrepare}} message initially applied and later rolled back at catch-up start):
{code:java}
2018-05-31 09:33:49.858 | WARNING | idr_cache.py         ( 106) | onBatchCommitted | 3PC: Node16: The first created batch has not been committed or reverted and yet another batch is trying to be committed, b'\xf1\x9e""rJ\x86\x82\xe6\xaa&\n\xf5\x80\xea\n/\x90\xfa\xf2\xf5=\x917#\xe1\xbamT\xf0\xbe\xd1\xbe' b'\xc1\xcfi\xab\xc8z \xd0\xb2I_~|\xa7\x04(\xeciR\x91\xcfQ8\xe0z2\x80\xdbT\x17\xe7%'
{code}
Eventually this resulted in that Node16 fell out from consensus when trying to apply the next batch (157, 1) after the view change completion:
{code:java}
2018-05-31 09:37:06.734 | WARNING | node.py              (2821) | Node16 raised suspicion on node Node4 for Pre-Prepare message has incorrect state trie root; suspicion code is 21
{code};;;","21/Jun/18 6:16 PM;ashcherbakov;`bug.png` - workflow of initial problem found in INDY-1400

`bug_fixed.png` - workflow of the fix done in the scope of INDY-1404 and INDY-1405

`node_init.png` - workflow of a similar problem with node initiation

`node_init_fixed.png` - workflow of the fix done in the scope of INDY-1404 and INDY-1405;;;",,,,,,,,,,,,,,,,,,,,,,
Added node can't order new batches after catch up,INDY-1401,30942,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,07/Jun/18 11:04 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,TShirt_L,,,"*Steps to Reproduce:*
 1. Setup the pool of 4 node. (11 txns in domain ledger: 9 genesis + 2 nyms)
 2. Add the 5th node:
 2.1. Initialize node.
 2.2. Copy genesis files to new node.
 2.3. Start node.
 2.4. Send Node txn with new node.
 3. Send several txns to make sure that added node works. (13 txns in domain ledger)
 => Node successfully added. (Note that steps are exactly the same as in INDY-1390 and INDY-1390 may reproduce.)
 4. Add the 6th node:
 4.1. Initialize node.
 4.2. Copy genesis files to new node.
 4.3. Send Node txn with new node.
 4.4. Start node.
 => Domain and pool ledgers are the same on all nodes.
 5. Send several txns.

*Actual Results:*
 New txns were not written on newly added (6th) node. Domain ledger size on the 6th node is 13, for the rest nodes it's 15 txns.

*Expected Results:*
 Node should work correctly after adding.

 

 

*Acceptance criteria:*
- find the cause, PoA and fix",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1390,,INDY-1391,,,,,,,,,,,,,,,,,,,,"07/Jun/18 11:32 PM;ozheregelya;Node1.log;https://jira.hyperledger.org/secure/attachment/15067/Node1.log","07/Jun/18 11:32 PM;ozheregelya;Node2.log;https://jira.hyperledger.org/secure/attachment/15068/Node2.log","07/Jun/18 11:32 PM;ozheregelya;Node3.log;https://jira.hyperledger.org/secure/attachment/15069/Node3.log","07/Jun/18 11:32 PM;ozheregelya;Node4.log;https://jira.hyperledger.org/secure/attachment/15070/Node4.log","07/Jun/18 11:32 PM;ozheregelya;Node5.log;https://jira.hyperledger.org/secure/attachment/15071/Node5.log","07/Jun/18 11:32 PM;ozheregelya;Node6.log;https://jira.hyperledger.org/secure/attachment/15072/Node6.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzzciv:",,,,,,EV 18.12 Release RocksDB,,,,,,,,,,,,,,,,,,,,ozheregelya,sergey-shilov,spivachuk,,,,,,,,,,"10/Jun/18 2:10 AM;spivachuk;There were troubles with node connections: {{Node4}} and {{Node5}} succeeded to connect to {{Node6}} only after 1 minute 42 seconds since the latter had started. Due to this {{Node4:0}} and {{Node5:0}} failed to send PREPARE(0, 7) and COMMIT(0, 7) to {{Node6:0}} and so {{Node6:0}} did not gather the quorum of COMMITs to order the batch (0, 7). Later when {{Node4}} and {{Node5}} had already been connected to {{Node6}}, {{Node6:0}} was not able to order the batch (0, 8) on the quorum of COMMITs because the previous batch (0, 7) had not been ordered yet. The missed messages were not requested because requesting of missed messages is performed only when PREPREPAREs are missed.

Note: {{Node6}}, however, successfully connected to {{Node4}} and {{Node5}} right after had started.;;;","19/Jun/18 9:41 PM;sergey-shilov;That issue is similar to [INDY-1390|https://jira.hyperledger.org/browse/INDY-1390].

There were troubles with connections:
 * Node5 to Node3
 * Node4 to Node6
 * Node5 to Node6

They had been failed to connect until they recreated their sockets to connect. Moreover, they recreated sockets twice to be able to connect, Seems like it is a bug of ZeroMQ.

As soon as recreating of ZeroMQ sockets solves connection issue we've made a fix to reduce connections retrying using the same socket:

_MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 1_;;;","21/Jun/18 8:46 PM;sergey-shilov;*Problem state / reason:*

Sometimes we face troubles with node-to-node connecting process. Each node-to-node logical connection consists of two TCP connections. A node connects to other node and sends packets using only this connection. A node receives packets using incoming connection. If one of these connections is not established then nodes can not communicate.
In this ticket nodes had been failed to connect until they recreated their sockets to connect. Seems like it is a bug of ZeroMQ.

*Changes:*

As soon as recreating of ZeroMQ sockets solves connection issue we've made a fix to reduce connections retrying using the same socket:

_MAX_RECONNECT_RETRY_ON_SAME_SOCKET = 1_

*Committed into:*

[https://github.com/hyperledger/indy-plenum/pull/749]

[https://github.com/hyperledger/indy-node/pull/767]

indy-node 1.4.470-master

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

Repeate tests with adding of nodes and check that all nodes are connected.

 ;;;","22/Jun/18 4:18 AM;ozheregelya;*Environment:*
indy-node 1.4.470
libindy 1.4.0~596

*Steps to Validate:*
1. Setup the pool of 4 nodes.
2. Init 2 more nodes (init_indy_node).
3. Send Node txn for one of these nodes.
4. Start both of additional nodes.
5. Check txns count on all nodes.
=> Added node processed all missed txns. Not added node has only genesis txns.
6. Send txn.
=> Txn successfully written on added node.
7. Add the second node, check txns count.
=> Added node processed all missed txns. All nodes have the same count of txns.
8. Send txn.

*Actual Results:*
Nodes were successfully added, all nodes have the same amount of txns.;;;",,,,,,,,,,,,,,,,,,,,,
Applying txns from catchup reply in case several nodes sent replies with overlapping seqNo intervals ,INDY-1403,30963,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,dsurnin,dsurnin,08/Jun/18 9:54 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"If node receives catchup replies from different nodes with overlapping seqNo intervals there is a possibility that one of replies will not be validated properly and node will be blacklisted.
It could happen because node applying txns from sorted list containing merged txns from all CRs based on first CR that includes starting seqNo. If starting seqNo is not the first seqNo of the CR the node will apply more txns than CR contains and proofs will not match",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1315,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzzcnr:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,dsurnin,,,,,,,,,,,"21/Jun/18 9:16 PM;anikitinDSR;After code analizing and writing successfully passed test, ""overlapping seqNo"" assumtion was not confirmed. Another investigation will be continued in the scope of ""stabilty issue"" tickets.
PR with passed test:
https://github.com/hyperledger/indy-plenum/pull/763;;;",,,,,,,,,,,,,,,,,,,,,,,,
Catchup should not be interrupted by external events,INDY-1404,30973,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,09/Jun/18 8:21 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"In [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350] and [INDY-1400|https://jira.hyperledger.org/browse/INDY-1400] it was found that next big problem with view change is that it can interrupt catchup process, leaving partially catched up state which leads to problems with applying batches that contain some transactions that were received through catch up.

Simplest possible fix looks like disabling ability to interrupt catch up by external events.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1400,INDY-1350,,,,INDY-1450,,,,,,,,,,,,,,,"03/Jul/18 9:19 PM;VladimirWork;INDY-1404.PNG;https://jira.hyperledger.org/secure/attachment/15225/INDY-1404.PNG","03/Jul/18 11:43 PM;VladimirWork;Node1.7z;https://jira.hyperledger.org/secure/attachment/15226/Node1.7z","03/Jul/18 11:43 PM;VladimirWork;Node1.log.1.xz;https://jira.hyperledger.org/secure/attachment/15227/Node1.log.1.xz","03/Jul/18 11:43 PM;VladimirWork;Node1.log.2.xz;https://jira.hyperledger.org/secure/attachment/15228/Node1.log.2.xz","03/Jul/18 11:43 PM;VladimirWork;Node1.log.3.xz;https://jira.hyperledger.org/secure/attachment/15229/Node1.log.3.xz","03/Jul/18 11:43 PM;VladimirWork;Node1.log.4.xz;https://jira.hyperledger.org/secure/attachment/15230/Node1.log.4.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.7z;https://jira.hyperledger.org/secure/attachment/15231/Node2.7z","03/Jul/18 11:44 PM;VladimirWork;Node2.log.1.xz;https://jira.hyperledger.org/secure/attachment/15232/Node2.log.1.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.log.2.xz;https://jira.hyperledger.org/secure/attachment/15233/Node2.log.2.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.log.3.xz;https://jira.hyperledger.org/secure/attachment/15234/Node2.log.3.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.log.4.xz;https://jira.hyperledger.org/secure/attachment/15235/Node2.log.4.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.log.5.xz;https://jira.hyperledger.org/secure/attachment/15236/Node2.log.5.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.log.6.xz;https://jira.hyperledger.org/secure/attachment/15237/Node2.log.6.xz","03/Jul/18 11:44 PM;VladimirWork;Node2.log.7.xz;https://jira.hyperledger.org/secure/attachment/15238/Node2.log.7.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.7z;https://jira.hyperledger.org/secure/attachment/15251/Node3.7z","04/Jul/18 4:40 PM;VladimirWork;Node3.log.1.xz;https://jira.hyperledger.org/secure/attachment/15252/Node3.log.1.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.log.2.xz;https://jira.hyperledger.org/secure/attachment/15253/Node3.log.2.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.log.3.xz;https://jira.hyperledger.org/secure/attachment/15254/Node3.log.3.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.log.4.xz;https://jira.hyperledger.org/secure/attachment/15255/Node3.log.4.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.log.5.xz;https://jira.hyperledger.org/secure/attachment/15256/Node3.log.5.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.log.6.xz;https://jira.hyperledger.org/secure/attachment/15257/Node3.log.6.xz","04/Jul/18 4:40 PM;VladimirWork;Node3.log.7.xz;https://jira.hyperledger.org/secure/attachment/15258/Node3.log.7.xz","21/Jun/18 6:13 PM;ashcherbakov;bug.png;https://jira.hyperledger.org/secure/attachment/15144/bug.png","21/Jun/18 6:13 PM;ashcherbakov;bug_fixed.png;https://jira.hyperledger.org/secure/attachment/15145/bug_fixed.png","21/Jun/18 6:13 PM;ashcherbakov;node_init.png;https://jira.hyperledger.org/secure/attachment/15146/node_init.png","21/Jun/18 6:13 PM;ashcherbakov;node_init_fixed.png;https://jira.hyperledger.org/secure/attachment/15147/node_init_fixed.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzzcjz:",,,,,,EV 18.12 Release RocksDB,EV 18.13 Benchmark hardening,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,,"21/Jun/18 6:15 PM;ashcherbakov;`bug.png` - workflow of initial problem found in INDY-1400

`bug_fixed.png` - workflow of the fix done in the scope of INDY-1404 and INDY-1405

`node_init.png` - workflow of a similar problem with node initiation

`node_init_fixed.png` - workflow of the fix done in the scope of INDY-1404 and INDY-1405;;;","28/Jun/18 10:13 PM;ashcherbakov;Problem reason: 
- Catchup can be interrupted in the middle because of a new view change. It means that the ledger will be in incorrect state (part of a 3PC batch will be applied)
- see attached diagrams 

Changes: 
- do not process any View Change related messages (including ViewChangeDone from Current State for primary propagation) until the current catchup is finished;
- this fixes initialization of a node where we mixed together initial catchup and primary propagation (initial view change);
- also fixed recovering when primary is disconnected and (n-f)th node joins the pool


PR:
- https://github.com/hyperledger/indy-plenum/pull/757

Version:
- master Indy Node 1.4.487

Risk factors:
- Catchup
- View Change
- Node initialization
- Node restart
- New node adding
- Recovering from f+1 nodes

Risk:
- Med

Covered with tests:
- test_no_view_change_while_catchup.py
- test_no_future_view_change_while_catchup.py

Recommendations for QA
- Run acceptance for adding of a new node
- Run acceptance against cases with restart of nodes 
- Run acceptance against cases with recovering from f+1 nodes
- Run load test with view changes;;;","03/Jul/18 9:22 PM;VladimirWork;Build Info:
indy-node 1.4.487

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Run load test for ~5k NYMs.
3. Add 5th node.
4. Run load test for ~5k NYMs.
5. Restart 1st node to change the primary (since there were no view changes at this moment).
6. Stop 1st node.
7. Run load test for ~25k NYMs.
*8. Add 6th node and start 1st node simultaneously to check their catchup under load.*

Actual Results:
1st node doesn't catch up (11.1k txns in ledger) even after an hour from starting (but 6th node does). Validator Info shows that all nodes are reachable. !INDY-1404.PNG|thumbnail!  Logs from 1st (bad) node and 2nd (good) are in attachment.

Expected Results:
1st node should catch up normally.;;;","04/Jul/18 6:03 PM;ashcherbakov;We can see the following in the logs:

2018-07-03 11:13:56,265 | INFO     | node.py              (3014) | send | Node3 sending message MESSAGE_RESPONSE\{'msg': LEDGER_STATUS\{'viewNo': None, 'txnSeqNo': 0, 'protocolVersion': 2, 'ppSeqNo': None, 'merkleRoot': 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn', 'ledgerId': 2}, 'msg_type': 'LEDGER_STATUS', 'params': \{'ledgerId': 2}} to 1 recipients: ['Node1']

2018-07-03 11:13:55,800 | INFO     | node.py              (3014) | send | Node2 sending message MESSAGE_RESPONSE\{'msg_type': 'LEDGER_STATUS', 'params': \{'ledgerId': 2}, 'msg': LEDGER_STATUS\{'ppSeqNo': None, 'txnSeqNo': 0, 'viewNo': None, 'ledgerId': 2, 'protocolVersion': 2, 'merkleRoot': 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn'}} to 1 recipients: ['Node1']


Node1 received ledger status: LEDGER_STATUS\{'viewNo': None, 'ledgerId': 2, 'ppSeqNo': None, 'txnSeqNo': 0, 'protocolVersion': 2, 'merkleRoot': 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn'} from Node2

 

So, Node 1 is doing catchup of Config ledger (ledgerId=2), but it received requested LEDGER_STATUS from Node2 only, although Node3 also sent it.

 


It looks like this is a problem we always had: if a lot of catchup-related messages (LEDGER_STATUS, CONSISTENCY_PROOFS) are lost, the node doesn't try to start catchup again.
The messages can be lost due to high load (as in the test), and limited size of node-to-node's message queue.

The recent fixes regarding catchup and fixes done in the scope of INDY-1404 made the problem more visible, since we require that the previous round of catchup is finished before starting the next round.

 

 ;;;","04/Jul/18 6:06 PM;ashcherbakov;Created INDY-1450 for this issue;;;","04/Jul/18 11:57 PM;VladimirWork;Build Info:
indy-node 1.4.487

Steps to Validate:
1. Check adding of new nodes (and their catchup).
2. Check consensus count and f+1 recovery.
3. Check view change forced by primary shutdown and primary degradation (under load).

Actial Results:
New nodes are added and caught up correctly. Write/read consensus and f+1 recovery are correct. View change caused by primary shutdown and primary degradation (up to ~570 view) also works. The issue found will be investigated and fixed in scope of INDY-1450.;;;",,,,,,,,,,,,,,,,,,,
Batch containing some already executed requests should be applied correctly,INDY-1405,30974,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,09/Jun/18 8:26 PM,13/Jun/19 6:10 PM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"In [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350] and [INDY-1400|https://jira.hyperledger.org/browse/INDY-1400] it was found that next big problem with view change is that it can interrupt catchup process, leaving partially catched up state which leads to problems with applying batches that contain some transactions that were received through catch up.

Proper fix looks like enabling correct handling of this case. Also, this is a prerequisite for implementing PBFT view change: [INDY-1335|https://jira.hyperledger.org/browse/INDY-1335]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1400,INDY-1350,,,,,,,,,,,,,,,,,,,"21/Jun/18 6:16 PM;ashcherbakov;bug.png;https://jira.hyperledger.org/secure/attachment/15152/bug.png","21/Jun/18 6:16 PM;ashcherbakov;bug_fixed.png;https://jira.hyperledger.org/secure/attachment/15153/bug_fixed.png","21/Jun/18 6:16 PM;ashcherbakov;node_init.png;https://jira.hyperledger.org/secure/attachment/15154/node_init.png","21/Jun/18 6:16 PM;ashcherbakov;node_init_fixed.png;https://jira.hyperledger.org/secure/attachment/15155/node_init_fixed.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzco7:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"19/Jun/18 6:34 PM;sergey.khoroshavin;PR with simple fix: https://github.com/hyperledger/indy-plenum/pull/747;;;","21/Jun/18 2:02 AM;sergey.khoroshavin;Now stashed ordered batches are applied correctly even if some of their requests were written during catchup process.
It is recommended to rerun load tests from [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350] to see if pool is still unstable.;;;","21/Jun/18 6:16 PM;ashcherbakov;`bug.png` - workflow of initial problem found in INDY-1400

`bug_fixed.png` - workflow of the fix done in the scope of INDY-1404 and INDY-1405

`node_init.png` - workflow of a similar problem with node initiation

`node_init_fixed.png` - workflow of the fix done in the scope of INDY-1404 and INDY-1405;;;","21/Jun/18 9:13 PM;sergey.khoroshavin;*Problem reason:*
Catchup interruption caused by view change can leave transactions in ledger that are only part of some batch. Current application of stashed ordered messages was not checking whether some transactions are already written, which leaded to corrupted ledger state.

*Changes:*
Now transactions in stashed ordered messages are applied only if they were not already written to ledger.

*PR:*
https://github.com/hyperledger/indy-plenum/pull/747

*Version:*
indy-plenum: 1.4.413-master
indy-node: 1.4.470-master

*Risk:*
Low

*Covered with tests:*
https://github.com/hyperledger/indy-plenum/pull/747/files#diff-6a289ded022270a06e44a66b22b5f012

*Recommendations for QA:*
It is recommended to rerun load tests from [INDY-1350|https://jira.hyperledger.org/browse/INDY-1350] to see if pool is still unstable;;;",,,,,,,,,,,,,,,,,,,,,
validator-info reading empty file,INDY-1406,31000,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ignisvulpis,ignisvulpis,12/Jun/18 7:46 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,validator-info,,,0,patch-attached,TShirt_S,,"validator-info is reading files which might be updated exactly when validator-info access them.

This is rare when validator-info is called manually but when validator-info is called by a script e.g. for monitoring purposes this happens quite often.

I was able to force this by just repeatedly starting validator-info on the cli.

Ways to solve this:

1) don't read frequently changing files but query the indy-node service and the OS

2) keep a number of input files around and if reading and parsing the latest fails go for the second oldest

3) if reading and parsing files wait a second and try again

 

(Intermediate solution: don't fail to provide any validator info but only fail on the info read from file but continue to provide info gathered from OS and services.)

Traceback of a modified version of validator-info so the line numbers might not match your's:
Traceback (most recent call last):
  File ""/usr/local/bin/validator-info2"", line 830, in <module>
    sys.exit(main())
  File ""/usr/local/bin/validator-info2"", line 818, in main
    print(get_stats_from_file(file_path, args.verbose, args.json, args.nagios))
  File ""/usr/local/bin/validator-info2"", line 590, in get_stats_from_file
    stats = json.loads(f.read())
  File ""/usr/lib/python3.5/json/__init__.py"", line 319, in loads
    return _default_decoder.decode(s)
  File ""/usr/lib/python3.5/json/decoder.py"", line 339, in decode
    obj, end = self.raw_decode(s, idx=_w(s, 0).end())
  File ""/usr/lib/python3.5/json/decoder.py"", line 357, in raw_decode
    raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
 
Here the file is empty and therefor not valid json.
 ","This is on the live sovrin network, running 1.3.62",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzhq7:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,anikitinDSR,ignisvulpis,VladimirWork,,,,,,,,,,"14/Jun/18 2:21 AM;ignisvulpis;I changed validator-info adding a try-catch and one retry after a one second sleep.

Currently testing whether the error occurs again.

Trying to provoke it be calling validator-info 600 times with 1 second sleep did not work so the patch seems to fix the issue.

Axel
{quote}    with open(fpath) as f:
        try:
            stats = json.loads(f.read())
        except json.decoder.JSONDecodeError as e:
            # if the first read fails try again after a short while
            logger.warning(""reading \{} failed \{}. retrying"".format(f,e))
            sleep(1);
            stats = json.loads(f.read())

 
{quote}
 ;;;","09/Jul/18 11:55 PM;anikitinDSR;Reason:
 * Need to handle empty node info files properly

Changes:
 * for now, while validator-info try to read empty json file, ""invalid json format"" will be return as result

Version:
 * indy-node 1.4.495

Recommendation for QA:
 * clean all info files for indy-node (for example, Node1_info.json and Node1_additional_info.json) and run validator-info script.
 * Expected result - any Traceback or other errors must be absent;;;","10/Jul/18 10:04 PM;VladimirWork;Build Info:
indy-node 1.4.495

Steps to Validate:
1. Clean all info files for indy-node.
2. Run validator-info script.

Actual Results:
{noformat}
root@d2a7213239e9:/var/lib/indy/sandbox# validator-info 
There are no info files in /var/lib/indy/sandbox
root@d2a7213239e9:/var/lib/indy/sandbox# validator-info -v
There are no info files in /var/lib/indy/sandbox
root@d2a7213239e9:/var/lib/indy/sandbox# validator-info --json
There are no info files in /var/lib/indy/sandbox
{noformat};;;",,,,,,,,,,,,,,,,,,,,,,
Investigate potential for dynamic analysis in CI,INDY-1407,31013,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,SteveGoob,SteveGoob,SteveGoob,13/Jun/18 1:32 AM,09/Oct/19 5:28 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,CII standards suggest implementing dynamic analysis on our projects. Look into this to see if this an option and would help us.,,,,,,,,,,,,,,,,,,,,,,,IS-757,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-766,,,,,,,,,"1|hzzem7:",,,,,,,,,,,,,,,,,,,,,,,,,,SteveGoob,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add Code coverage tools to our CI,INDY-1408,31014,,Task,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,SteveGoob,SteveGoob,SteveGoob,13/Jun/18 1:33 AM,11/Oct/19 10:27 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"For the CII badge, we need to maintain some code coverage metrics.

[kcov|https://github.com/SimonKagstrom/kcov] might be a good tool for this because it is language agnostic, (will work with both rust and python).

For rust specifically, [this|https://stackoverflow.com/questions/32521800/why-does-kcov-calculate-incorrect-code-coverage-statistics-for-rust-programs#38371687] might be something to keep in mind when implementing it",,,,,,,,,,,,,,,,,,,,,,,IS-754,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1758,,,,,,,,,"1|hzwqr3:o",,,,,,CommunityContribution,,,,,,,,,,,,,,,,,,,,esplinr,SteveGoob,,,,,,,,,,,"09/Oct/19 5:25 PM;esplinr;We are now running Flake8 on  Jenkins and LGTM on GitHub PRs.;;;","11/Oct/19 8:44 AM;SteveGoob;This ticket is concerned with *Code Coverage,* not static analysis, and Flake8 is not a code coverage tool. Moving back to open;;;",,,,,,,,,,,,,,,,,,,,,,,
Check for evidence of adherance to our testing policy,INDY-1409,31015,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,SteveGoob,SteveGoob,SteveGoob,13/Jun/18 1:44 AM,24/Jul/19 3:28 AM,28/Oct/23 2:47 AM,11/Apr/19 6:51 AM,,,,,,0,,,,The CII requires us to have evidence that our testing policy has been followed.,,,,,,,,,,,,,,,,,,,,,,,IS-755,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-48,,,,,,,,,"1|hzwqr4:",,,,,,CommunityContribution,,,,,,,,,,,,,,,,,,,,SteveGoob,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One of the nodes stopped writing after 44287 txns with errors in status,INDY-1410,31027,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,13/Jun/18 7:05 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,Stability,,,"Steps to Reproduce:
1. Setup the pool of 25 nodes and 1 client for load test.
2. Run the load test from client machine:
python3.5 perf_processes.py -n 1 -t 1 -c 10 -r 100 -g perf25_transactions_genesis 2>error.txt

Actual Result:
Load test stopped working on 52215 txns. One of the nodes (Node19) stopped writing on 44287 txns. After that pool was without the load during the night and lagged node was not processed missed txns. After restart of load test all nodes exclude lagged one (Node19) continued writing.
Following error appear in `systemctl status indy-node` of lagged node:
{code:java}
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: self.doOrder(commit)
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1640, in doOrder
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: return self.order_3pc_key(key)
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1671, in order_3pc_key
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: self.addToCheckpoint(pp.ppSeqNo, pp.digest)
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1791, in addToCheckpoint
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: self.processStashedCheckpoints((s, e))
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1879, in processStashedCheckpoints
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: del self.stashedRecvdCheckpoints[self.viewNo][key]
Jun 11 15:14:24 canadaQALive.qatest.evernym.com env[24509]: KeyError: 0
{code}
Expected Results:
All nodes should write txns all the time.

Additional Information:
Logs shared with [~sergey.khoroshavin].","indy-node 1.3.450
libindy 1.4.0~565
AWS pool of 25 nodes (QA Performance)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcon:",,,,,,EV 18.12 Release RocksDB,,,,,,,,,,,,,,,,,,,,ozheregelya,spivachuk,,,,,,,,,,,"15/Jun/18 6:28 PM;spivachuk;*Problem reason:*
- There was a bug in {{Replica.processStashedCheckpoints}} method with removal of entries from {{Replica.stashedRecvdCheckpoints}} map. Actually consumed messages were removed from the map only after processing of all the stashed messages for the given key. But at that moment these messages might have already been removed from the map due to checkpoint stabilization and so the attempt to remove them in {{Replica.processStashedCheckpoints}} method resulted in {{KeyError}}. Also during iteration over checkpoint messages for the given key from different senders we might process messages that had already been removed from {{Replica.processStashedCheckpoints}} map due to checkpoint stabilization.

*Changes:*
- Fixed the bug in {{Replica.processStashedCheckpoints}} method with repeated removal of entries from {{Replica.stashedRecvdCheckpoints}} map. Now when iterating over a snapshot of checkpoint messages for the given key {{Replica.processStashedCheckpoints}} method checks if the message is still in the map before processing it and if the message is still in the map then removes it from there and then processes it.
- Added a test that verifies processing of stashed checkpoints from all other nodes and own checkpoint stabilization on own checkpoint completion.
- Added a test that verifies processing of stashed checkpoints from other nodes on own checkpoint completion and own checkpoint stabilization on reception of more checkpoints from other nodes.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/743
- https://github.com/hyperledger/indy-node/pull/766

*Version:*
- indy-node 1.4.462-master
- indy-plenum 1.4.403-master

*Risk factors:*
- Nothing is expected

*Risk:*
- Low

*Covered with tests:*
- {{test_lagged_checkpoint_completion}}
- {{test_stashed_checkpoint_processing}};;;","19/Jun/18 2:27 AM;ozheregelya;Environment:
indy-node 1.4.463
libindy 1.4.0~565

Steps to Validate:
1. Setup the pool.
2. Run the load test from 10 clients with 1 txn/sec.

Actual Results:
Pool successfully reached 800K txns without errors in status.;;;",,,,,,,,,,,,,,,,,,,,,,,
getting_started_turnkey run-demo fails,INDY-1411,31074,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,herveblanc,herveblanc,15/Jun/18 1:37 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,help-wanted,,,"I'm trying to run this on WSL (Windows 10 1803, running Ubuntu 16.04.4 with Windows Subsystem for Linux) and get the following error when running ""make run-demo"":

...

docker run --rm --name Indy -it indy-base /bin/bash -c ""
create_dirs.sh; generate_indy_pool_transactions --nodes 4 --clients 5 --ips 192.168.0.6,192.168.0.6,192.168.0.6,192.168.0.6; 
./indy-cli 
'connect sandbox' 
'new key with seed 000000000000000000000000Steward1' 
'send NYM dest=ULtgFQJe6bjiFbs7ke3NJD role=TRUST_ANCHOR verkey=~5kh3FB4H3NKq7tUDqeqHc1' 
'send NYM dest=CzkavE58zgX7rUMrzSinLr role=TRUST_ANCHOR verkey=~WjXEvZ9xj4Tz9sLtzf7HVP' 
'send NYM dest=H2aKRiDeq8aLZSydQMDbtf role=TRUST_ANCHOR verkey=~3sphzTb2itL2mwSeJ1Ji28' 
'new key with seed Faber000000000000000000000000000' 
'send ATTRIB dest=ULtgFQJe6bjiFbs7ke3NJD raw=\{""endpoint"": \{""ha"": ""192.168.0.6:5555"", ""pubkey"": ""5hmMA64DDQz5NzGJNVtRzNwpkZxktNQds21q3Wxxa62z""}}' 
'new key with seed Acme0000000000000000000000000000' 
'send ATTRIB dest=CzkavE58zgX7rUMrzSinLr raw=\{""endpoint"": \{""ha"": ""192.168.0.6:6666"", ""pubkey"": ""C5eqjU7NMVMGGfGfx2ubvX5H9X346bQt5qeziVAo3naQ""}}' 
'new key with seed Thrift00000000000000000000000000' 
'send ATTRIB dest=H2aKRiDeq8aLZSydQMDbtf raw=\{""endpoint"": \{""ha"": ""192.168.0.6:7777"", ""pubkey"": ""AGBjYvyM3SFnoiDGAEzkSLHvqyzVkXeMZfKDvdpEsC2x""}}' 
'save wallet'; 
/bin/bash 
""
BLS Public key is 4N8aUNHSgjQVgkpm8nhNEfDf6txHznoYREg9kirmJrkivgL4oSEimFF6nsQ6M41QvhM2Z33nves5vfSn9n1UwNFJBYtWVnHYMATn76vLuL3zU88KyeAYcHfsih3He6UHcXDxcaecHVz6jhCYz1P2UZn2bDVruL5wXpehgBfBaLKm3Ba
BLS Public key is 37rAPpXVoxzKhz7d9gkUe52XuXryuLXoM6P6LbWDB7LSbG62Lsb33sfG7zqS8TK1MXwuCHj1FKNzVpsnafmqLG1vXN88rt38mNFs9TENzm4QHdBzsvCuoBnPH7rpYYDo9DZNJePaDvRvqJKByCabubJz3XXKbEeshzpz4Ma5QYpJqjk
BLS Public key is 3WFpdbg7C5cnLYZwFZevJqhubkFALBfCBBok15GdrKMUhUjGsk3jV6QKj6MZgEubF7oqCafxNdkm7eswgA4sdKTRc82tLGzZBd6vNqU8dupzup6uYUf32KTHTPQbuUM8Yk4QFXjEf2Usu2TJcNkdgpyeUSX42u5LqdDDpNSWUK5deC5
BLS Public key is 2zN3bHM1m4rLz54MJHYSwvqzPchYp8jkHswveCLAEJVcX6Mm1wHQD1SkPYMzUDTZvWvhuE6VNAkK3KxVeEmsanSmvjVkReDeBEMxeDaayjcZjFGPydyey1qxBHmTvAnBKoPydvuTAqx5f7YNNRAdeLmUi99gERUU7TD8KfAa6MpQ9bw
/usr/bin/env: 'python3\r': No such file or directory","Windows 10 1803, running Ubuntu 16.04.4 with Windows Subsystem for Linux",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzevz:",,,,,,,,,,,,,,,,,,,,,,,,,,herveblanc,,,,,,,,,,,,"15/Jun/18 1:55 AM;herveblanc;managed to solve this by:
$ git config --global core.autocrlf input
and clone again indy-node

more info at https://github.com/Microsoft/WSL/issues/2318#issuecomment-314631096 for those interested in running indy on windows wsl environment.;;;",,,,,,,,,,,,,,,,,,,,,,,,
reduce nsdiff complexity,INDY-1413,31049,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,devin-fisher,devin-fisher,14/Jun/18 5:32 AM,12/Oct/18 6:27 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"nsdiff was written during the transition from levelDB to rockDB and was written fairly quickly. 

 

Because of this, there are a few areas that overly complex and easy to maintain. This tech debt was incurred to keep the project moving forward. 

 

This ticket is to revisit nsdiff and remove complexity when more time and resource are available. 

 

The main area that should be refactored is the detection, reading and diffing of DB files or directories. 

the ""transform_db"" function is a prime example and should be reviewed.

 

nsdiff was written by [~ckochenower]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzerb:",,,,,,,,,,,,,,,,,,,,,,,,,,devin-fisher,gudkov,,,,,,,,,,,"15/Jun/18 8:35 PM;gudkov;[~devin-fisher] Are you sure it is relevant for Indy SDK? Indy Node is managed iunder INDY jira project, not IS;;;","16/Jun/18 5:24 AM;devin-fisher;Sorry, [~gudkov] , moved to INDY where it belongs. Thanks for heads up. ;;;",,,,,,,,,,,,,,,,,,,,,,,
Review and replace 'assert' with exceptions in indy-node where needed,INDY-1414,31132,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ashcherbakov,andkononykhin,andkononykhin,18/Jun/18 4:05 PM,13/Feb/19 9:56 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Currently production builds use optimization for bytecode (INDY-211) when running [node|https://github.com/andkononykhin/indy-node/blob/master/build-scripts/ubuntu-1604/postinst_node#L54] and [node_control_tool|https://github.com/andkononykhin/indy-node/blob/master/build-scripts/ubuntu-1604/postinst_node#L78].

This silently [removes|https://docs.python.org/3.5/tutorial/modules.html#compiled-python-files] all assert statements from the code. I think all asserts (in both indy-plenum and indy-node) should be reviewed and fixed (e.g. replaced with custom/built-in exeptions) where needed.",,,,,,,,,,,,,,,,,,,,,,,INDY-810,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwy3z:",,,,,,,,,,,,,,3.0,,,,,,,,,,,,andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect read_ledger info with seq_no parameter,INDY-1415,31136,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,18/Jun/18 7:49 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,TShirt_S,,,"Build Info:
indy-node 1.4.464

Steps to Reproduce:
1. Install or upgrade pool to 1.4.464 version.
2. Run `read_ledger --type=domain --seq_no 1` or with any existing seq_no.
3. Check output.

Actual Results:
""hKxyZXFTaWduYXR1cmWAo3R4boOkZGF0YYOkZGVzdLZWNFNHUlU4Nlo1OGQ2VFY3UEJVZTZmpHJvbGWhMKZ2ZXJrZXm3fkNvUkVSNjNEVlluV1p0Szh1QXpOYniobWV0YWRhdGGApHR5cGWhMat0eG5NZXRhZGF0YYGlc2VxTm8Bo3ZlcqEx""

Expected Results:
There should be the same output as with any other key:
1 {""reqSignature"":{},""txn"":{""data"":{""dest"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""0"",""verkey"":""~CoRER63DVYnWZtK8uAzNbx""},""metadata"":{},""type"":""1""},""txnMetadata"":{""seqNo"":1},""ver"":""1""}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/18 7:51 PM;VladimirWork;INDY-1415.PNG;https://jira.hyperledger.org/secure/attachment/15117/INDY-1415.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-984,,,,,,,,,"1|hzwvif:00001ywby",,,,,,,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include reviewed logging strings in Indy,INDY-1416,31139,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,esplinr,esplinr,18/Jun/18 11:09 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"*Story*

As an administrator of an Indy node, I want log messages to be clear and well written.

*Acceptance Criteria*

Log messages that have been reviewed by Product Management and the Operations Team should be incorporated into the product.

Log messages and levels in the product should match the Revised Message and Revised Level in the spreadsheet. If the revised level is ""None"" then the log message is unnecessary. Blank revisions means that no changes are needed.

If there is a ""Revised Message"" but the ""Current Message"" is empty, then the log messages is new and should be added to the product.",,,,,,,,,,,,,,,,,,,,,,,INDY-1267,INDY-1311,,,,,INDY-484,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-56,,,,,,,,,"1|hzzhpj:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,3.0,,,,,,,,,,,,dsurnin,esplinr,gudkov,,,,,,,,,,"03/Jul/18 5:21 PM;dsurnin;Discussion about messages and log levels could be found here
https://docs.google.com/spreadsheets/d/1vxSpN0FBf_nv1ZHsPI_Q3vAyP3yznyiIYUKnh8jQzds/edit#gid=2087561173;;;","12/Jul/18 5:41 PM;dsurnin;logs were modified according to spreadsheet above
PRs
https://github.com/hyperledger/indy-plenum/pull/794
https://github.com/hyperledger/indy-plenum/pull/805
https://github.com/hyperledger/indy-node/pull/812
;;;",,,,,,,,,,,,,,,,,,,,,,,
POA: Ensure new client connections can be accepted on a periodic basis,INDY-1417,31142,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,esplinr,esplinr,18/Jun/18 11:42 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"*Story*
As an administrator of a node on an Indy network, I want an automatic process for new client connections to be accepted by the network.

*Acceptance Criteria*
Design a process by which new client connections can be accepted even when there are a lot of connections on the existing external NIC. The process must not require manual intervention.

The process may happen on a periodic basis.

*Notes*
Potential approaches:
* Force ZeroMQ to close connections on a timeout
* An external process such as a firewall monitors the length of open connections and kills them on a timeout
* * When the connection buffer becomes full, clear the connection buffer and force legitimate clients to reconnect
",,,,,,,,,,,,,,,,,,,,,INDY-1418,,,,,,,,,,,,,INDY-1431,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzzckf:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,5.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,esplinr,sergey-shilov,,,,,,,,,"26/Jun/18 5:37 PM;anikitinDSR;In the scope of this issue some experiment was spent. The steps were:
 * create pool of 4 nodes in docker;
 * patch 'scheduled' method `checkPerformance` which run every 15 seconds for node2. The main goal of this changes is to stop node's clientstack, wait some second and start it again.
 * restart indy-node service for node2
 * connect to pool using old cli, because new cli does have any monitoring features and there is no any method for connection watching.
 * using netstat tool check, that previous connection was close immediately, and new connection is realy new connection with different socket

Periodically clientstack closing is not break the pool functionality and realy new socket/connection is created. As a small fix, we can add scheduled job for periodically stoping/starting clientstack with configurable timeout plus some random delay. ;;;","27/Jun/18 8:39 PM;sergey-shilov;Since we have a connections limit (no matter whether it is controlled by external firewall for which we have a recommended setup or just by a kernel fds limit per process) we can face the situation when permanent established connections don't allow new clients to connect infinitely. To solve this problem we should implement some mechanism that disconnects connected clients so that a new clients have a chance to connect.

The main problem here is that we use ZeroMQ that fully encapsulates all work with clients connection sockets and there is no way to force ZeroMQ to close particular client connection.

During our research we figured out three possible solutions:
 * External firewall: configure iptables so that it drops established connection after certain time.
 * Close client sockets from node's code.
 * Periodic restart of client stack.

Let's look at each solution in details.

*1. External firewall (iptables).*

The iptables is the most standart firewall that available on the most linux distribution, that's why saying ""firewall"" we assume ""iptables"". The main advantage of using firewall is that modification of node's code is not needed. The main risk here is communication with stewards, checking configuration correctness and so on.

But reading docs showed that there is no way to achieve needed functionality using iptables, links:

[https://linux.die.net/man/8/iptables]
 [https://serverfault.com/questions/694024/can-iptables-automatically-drop-established-session-after-certain-seconds]

*2. Close client sockets from node's code.*

Despite ZeroMQ encapsulates all work with clients connection sockets we can retrieve a new connection socket file descriptor from *CONNECTED* event reported by ZeroMQ monitor socket. So that we can close this file descriptor using an ordinary _close()_ system call after certain time from *CONNECTED* event.

I've done a lot of experiments with various options of closing connections and faced with some issues that seems to be very hard to avoid. The main of them is that _ZeroMQ does not expect that it's controlled sockets may be closed externally._ Internally ZeroMQ is the epoll-based network library and seems like even closing with _linger_ option does not trigger any epoll events. So this invalid descriptor continues to be kept in ZeroMQ's epoll and *DISCONNECTED* event is not triggered. This leads to ZeroMQ crash with _""Bad file descriptor""_ error during ZeroMQ finalizing. Moreover, seems like newly open socket with the same file descriptor number leads to mess in inner ZeroMQ structures associated with externally closed file descriptor as they were not finalized.

There is another method of initiating of closing connection called _""half closing""._ This is done by calling _shutdown_ system call with various options. In my experiments graceful half closing (I mean client side does it right, sends _FIN_ segment etc.) there is triggered epoll event and then triggered *DISCONNECTED* ZeroMQ event. But this approach is dangerous if client side becomes silent and does not send _FIN_ segment. In this case client connection socket fall into _FIN_WAIT1_ long state and ZeroMQ keeps associated structures that leads to issues described above.

One more concern regarding this solution: node's code and ZeroMQ work with the same client socket in different threads unsynchronized, this may lead to crashes and unpredictable behavior. So this solution can not be considered as acceptable.

There is one thing that I don't understand for now. For each client connection socket we enable TCP keepalive for dead peer detection. After several probes TCP keepalive mechanism drop client connection socket (_RST_ segment is sent to client that means lingering close) and we see *DISCONNECTED* ZeroMQ event triggered. Seems like the kernel uses some other mechanisms unreachable from user space to close sockets  and triggers some epoll event (may be EPOLLHUP). But seems like the answer on the question how the kernel does it does not solve our issues as we are working in the user space.

*3. Periodic restart of client stack.*

Periodic restart of client stack does not consider how long clients are connected, it just drops all connections, and of course this is the main disadvantage. But the main advantage of such approach is safety. In this case we do not do any things that are unexpected by ZeroMQ. There is some sockets overhead during client stack stop/start as all clients sockets for which was data transmission fall into _TIME_WAIT_ state for about one minute before final dropping, but this is not critical, we just should consider this.

There are two options of scheduling of stack restart event: by timeout and by number of connections. I prefer a combination of them. Say if we restart stack every 10 minutes unconditionally we will drop all 10 connected clients while it is not necessary. From the other hand if we restart just on connection limit (we can track it using *CONNECTED/DISCONNECTED* events) then we may fall into infinite stack restart because all ZeroMQ-based clients try to re-establish connection immediately, So if we have a limit 1000 then we can observe something like this:
 * have 1001 client connected
 * stop client stack and drop 1001 connected clients
 * start stack and immediately have 1001 re-established clients connection
 * stop client stack
 * etc.

So we should restart client stack after some timeout even if limit is reached. Of course we assume that our recommended firewall setting for connections limit are applied.

*CONCLUSION*

Since we still use ZeroMQ we can consider client stack restart as the most appropriate solution. It is not hard to implement and meets our requirements. One thing here is that stack restart should be tested under load as connections closing may occur in various data transmission and client/server exchange phases.;;;","27/Jun/18 8:57 PM;sergey-shilov;Created [INDY-1431|https://jira.hyperledger.org/browse/INDY-1431] for implementation.;;;","05/Jul/18 5:15 PM;sergey-shilov;Some info about service downtime during client stack restart.

All TCP stack clean up is done on the operating system level asynchronously relating to our application level. All open client connections sockets are fallen into TIME_WAIT state, we should consider that in our file descriptors limits calculation logic, but it does not affect service downtime. As ZeroMQ uses SO_REUSEADDR for listener socket we can re-bind client listener immediately.  Also we close listener socket with linger option, so listener socket kernel structure should be freed immediately. So ZeroMQ client stack restart itself takes at most milliseconds regardless load.
 
 There are two other things that really affects service downtime:
 1) May be it would be nice to add some sleep between client stack stop and start, in our tests we added sleep 0.2 seconds, but seems like it is not necessary as I checked that stop/start works well even without that sleep.
 2) The more important option is to add some sleep AFTER client stack restart. Rationale for this is the fact that we disconnect all connected clients while we may have prepared replies for some clients. Each client is identified by connection identity and we store connection identities to be able to match replies and clients. After client stack restart we still try to send replies to clients by their identities. All IndySDK-based (and thus ZeroMQ-based) clients try to re-establish connection immediately after disconnect from the server side with the same identities. So the client will receive reply from the server if this client re-establish TCP connection before sending reply by the server even in case of client stack restart. To increase the probability of re-established TCP connection we can add some sleep (about 3-5 seconds, may be configurable) after client stack restart. But anyway not received reply by client is not a critical situation. In case of read or write requests it case re-send request and get desired result. But it would be much better to decrease probability of requests re-sending by clients.
  
 So according to 1) and 2) overall service downtime is about 3-5 seconds (may be decreased to 1-2 seconds by configuration for real network) and most of this downtime is a sleep after client stack restart to allow disconnected clients to reconnect and receive their replies.;;;","05/Jul/18 8:16 PM;ashcherbakov;Implementation of the proposed option will be done in the scope of NDY-1431.

 ;;;","06/Jul/18 1:07 AM;sergey-shilov;I've got to fully understanding why we can not close ZeroMQ sockets from node's code (option 2 in my research report, see https://jira.hyperledger.org/browse/INDY-1417?focusedCommentId=46595&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-46595).

In my research I was wondered why TCP keep-alive does not lead to ZeroMQ crash with ""Bad file descriptor"" error in epoll during finalization.

The answer was found during testing of sample ZeroMQ-based python server, sample epoll-based server in C, sniffing traffic, reading of ZeroMQ sources and (heh-heh) Linux kernel sources related to TCP and keep-alive. By the way, ZeroMQ is epoll-based library.

First of all, closing of socket leads to silent removing it from epoll (if there are no any socket duplicates, it is not our case, ZeroMQ does not creates any duplicates). This happens when close is called for socket from node's code. Such closing does not lead to ZeroMQ immediate crash, but ZeroMQ still thinks that socket is alive and keeps associated application-level handle structure. When it is time to finalize ZeroMQ library finalizes each active handle and removes associated file descriptor from epoll. If a socket was closed from node's code then attempt to remove corresponding file descriptor from epoll fails with ""Bad file descriptor"" error and ZeroMQ crashes.

So that, the problems of closing of sockets from node's code are:
 1) memory leak as ZeroMQ keeps handles of closed sockets;
 2) ZeroMQ can not be normally finalized due to ""Bad file descriptor"" error;
 3) ZeroMQ works in separate thread, manipulation with sockets in other threads is not a good idea.

But how TCP keep-alive works? In fact, TCP keep-alive does not close socket. As I supposed in research report, the kernel uses its own mechanisms to generate EPOLLHUP, EPOLLERR and EPOLLIN events for socket. ZeroMQ wakes up from epoll_wait, finds out that events on socket, closes it and finalizes corresponding handle. Thats why nothing is broken when TCP keep-alive is enabled.

But we are working in user space and we can not generate such socket events even having its file descriptor. That's why client stack restart is still the most appropriate and safe solution since we use ZeroMQ. But of course in should be tested very aggressively.;;;",,,,,,,,,,,,,,,,,,,
Ensure new client connections can be accepted on a periodic basis,INDY-1418,31143,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,sergey-shilov,esplinr,esplinr,18/Jun/18 11:44 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Story*
As an administrator of a node on an Indy network, I want an automatic process for new client connections to be accepted by the network.

*Acceptance Criteria*
Implement the solution designed in INDY-1418.",,,,,,,,,,,,,,,,,,,,,,,INDY-1417,,,,INDY-1431,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzwwiv:",,,,,,,,,,,,,,8.0,,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"02/Jul/18 9:51 PM;ashcherbakov;Duplicates INDY-1431;;;",,,,,,,,,,,,,,,,,,,,,,,,
Need to explore view change parameters in bad network conditions,INDY-1419,31147,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ozheregelya,ozheregelya,19/Jun/18 2:37 AM,11/Oct/19 8:54 PM,28/Oct/23 2:47 AM,11/Oct/19 8:54 PM,,,,,,0,,,,"Need to explore view change parameters from INDY-1334 in bad network conditions.
 Parameters:
{noformat}
DELTA = 0.1
LAMBDA = 240
OMEGA = 20

Max3PCBatchSize = 10000
Max3PCBatchWait = 1
{noformat}
Script for bad network conditions simulation: http://www.uponmyshoulder.com/blog/2013/simulating-bad-network-conditions-on-linux/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwx4f:2rzt",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"11/Oct/19 8:54 PM;ashcherbakov;We implemented PBFT View Change so not relevant anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,
txnId should be part of transaction,INDY-1420,31164,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,19/Jun/18 5:08 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"We need to have `txnId` as part of `txnMetadata` for every txn (as described inhttps://github.com/hyperledger/indy-node/blob/master/docs/requests.md).

Migration script should also be updated.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1374,,,,,,,,,"1|hzzcnb:",,,,,,EV 18.12 Release RocksDB,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,dsurnin,ozheregelya,VladimirWork,,,,,,,,,"21/Jun/18 12:28 AM;dsurnin;txnId added to transaction
migration fixed
node version 470

last version of load script should be able now to read txnId from the JSON response.
;;;","21/Jun/18 3:57 AM;ozheregelya;Environment:
indy-node 1.4.470
libindy 1.4.0~591

Steps to Validate:
1. Run load test with following write requests:
--> nym, schema, attrib, cred_def, revoc_reg_def
2. Check txnId field in output files.

Actual Results:
'txnId' field was added.

Additional Information:
txnId field was not checked for revoc_reg_entry because of problems with load test.;;;","21/Jun/18 6:49 PM;ozheregelya;Migration will be tested by [~VladimirWork].;;;","22/Jun/18 12:40 AM;VladimirWork;Build Info:
indy-node 1.3.375 -> 1.4.470

Steps to Validate:
1. Upgrade pool from 1.3.375 to 1.4.470 with all ledgers filled with all txns types.
2. Check all ledgers before and after migration.

Actual Results:
Pool ledger has `txnId` before and after migration.
All domain ledger txns get `txnId` after migration (excluding genesis ones).
Config ledger has no `txnId` before and after migration.;;;",,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.4.66,INDY-1421,31166,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,VladimirWork,VladimirWork,19/Jun/18 6:33 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,22/Jun/18 12:00 AM,0,,,,"*Version Information*
 indy-node 1.4.66
 indy-plenum 1.4.45
 indy-anoncreds 1.0.11
 sovrin 1.1.11

*Major Fixes*
 INDY-1410 - One of the nodes stopped writing after 44287 txns with errors in status
 INDY-1365 - Pool stopped accepting transactions on 5731 txns (1 sec delays, no logging)
 INDY-1315 - Pool stopped writing after ~300,000 txns from 5 clients
 INDY-1351 - STN not accepting transactions with only one node down
 INDY-1260 - Pool stops taking txns at ~178k txns written in ledger
 INDY-1327 - `ReqIdrToTxn` does not store information about the ledger
 INDY-1341 - Simple Timeout fixes of the current View Change protocol
 INDY-1379 - Migration fails in case of upgrade to version with new transactions format
 INDY-1318 - --network parameter of read_ledger doesn't work
 INDY-1310 - The /var/log/indy/validator-info.log is inappropriately owned by root
 INDY-1298 - Fix the issues found in the current logic of catch-up
 INDY-1363 - GetValidatorInfo should have correct validation for signature and permissions
 INDY-1316 - Unhandled exception during node working
 INDY-1219 - validator-info and read_ledger give inconsistent responses in node on provisional
 INDY-1259 - Pool stops taking txns at 3000 writing connections

*Changes and Additions*
 INDY-810 - Review and replace 'assert' with exceptions in indy-plenum where needed
 INDY-1245 - Tune RocksDB options for the best performance
 INDY-1392 - As a developer, I need to have migration guide from Indy-node 1.3 to 1.4
 INDY-1370 - Сhange key in requests map and field reqIdr in Pre Prepare and Ordered
 INDY-1400 - Investigate issues found during load testing of 25-nodes pool with increased timeouts for catchups and viewchange
 INDY-1332 - Support binding on separate NICs for Client-to-Node and Node-to-Node communication
 INDY-1329 - Add short checkpoints stabilization without matching digests
 INDY-1323 - Add indy-crypto package to hold list
 INDY-1297 - Remove ledger status based catch-up trigger together with wrong catch-up workflow
 INDY-1243 - Read-ledger without storage copy in case of RocksDB (RocksDB read-only mode support)
 INDY-971 - Apply state machine to Catchup code
 INDY-1124 - Refactor common Request structure
 INDY-1123 - Refactor common transactions structure
 INDY-1319 - Support new libindy with changed txn format
 INDY-1334 - Explore config parameters to find the best performance/stability settings
 INDY-1175 - Extend of Validator Info tool to provide more information about the current state of the pool
 INDY-1184 - A Steward needs to be able to get validator-info from all nodes
 INDY-1279 - Modify existing load scripts for a better load testing
 INDY-1244 - Migration from LevelDB to RocksDB
 INDY-1173 - A Trustee needs to be able to restart the pool in critical situations
 INDY-1275 - Move log compression into separate process

*Known Issues*
 INDY-1415 - Incorrect read_ledger info with seq_no parameter

(!) *Pool upgrade should be performed simultaneously for all nodes due to txn format changes.*
(!) *All indy-cli pools should be recreated with actual genesis files.*
(!) *Added and promoted nodes should be restarted (`systemctl restart indy-node`) after adding/promoting.*

*List of breaking changes for migration from indy-node 1.3 to 1.4:*
 [https://github.com/hyperledger/indy-node/blob/master/docs/1.3_to_1.4_migration_guide.md]

*IndyNode 1.4 and LibIndy 1.5 compatibility:*

_General_

By default LibIndy 1.5 will be compatible with IndyNode 1.3 (current stable), and not 1.4 (the new one).
 LibIndy 1.5 can become compatible with IndyNode 1.4 if `indy_set_protocol_version(2)` is called during app initialization.

_Guideline for teams and apps_

Applications can freely update to LibIndy 1.5 and still use stable Node 1.3
 If an app wants to work with the latest master or Stable Node 1.4, then they need to
 support breaking changes (there are not so many, mostly a new reply for write txns as txn format is changed, see 1.3_to_1.4_migration_guide.md)
 call `indy_set_protocol_version(2)` during app initialization

*CLI Upgrading:*

Old CLI (`indy`):
- upgrade from 1.3 to 1.4 version
- delete `~.ind-cli/networks/<network_name>/data` folder
- replace both old genesis files by new ones (from 1.4 node or from sovrin repo)

New CLI (`indy-cli`):
- upgrade from 1.4 to 1.5 version
- recreate indy-cli pool using 1.4 pool genesis file (from 1.4 node or from sovrin repo)

Use https://github.com/hyperledger/indy-sdk/blob/b4a2bb82087e2eafe5e55bddb20a3069e5fb7d0b/cli/README.md#old-python-based-cli-migration to export dids from your old CLI wallet to the new one (new indy-cli).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcl3:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,2.0,,,,,,,,,,,,TechWritingWhiz,VladimirWork,,,,,,,,,,,"20/Jun/18 12:48 AM;VladimirWork;FYI [~krw910];;;","22/Jun/18 11:21 PM;TechWritingWhiz;This is complete. The pull request is here: [https://github.com/sovrin-foundation/sovrin/pull/68]

 ;;;","26/Jun/18 1:39 AM;TechWritingWhiz;Here is another pull request that adds the information that was added to this ticket after the last PR was submitted and merged. 
[https://github.com/sovrin-foundation/sovrin/pull/69]

 ;;;","30/Jun/18 1:41 AM;VladimirWork;[~TechWritingWhiz] Could you please update your PR according to this ticket changes?;;;","03/Jul/18 3:38 AM;TechWritingWhiz;Here is another pull request that adds the information that was added to this ticket after the last PR was submitted and merged.

https://github.com/sovrin-foundation/sovrin/pull/71;;;",,,,,,,,,,,,,,,,,,,,
Part of nodes continued ordering txns after `incorrect state trie` under load,INDY-1422,31171,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ozheregelya,ozheregelya,19/Jun/18 11:01 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,TShirt_L,,,"*Environment:*
indy-node 1.4.463
libindy 1.4.0~565

*Steps to Reproduce:*
1. Setup the pool of 25 nodes with following parameters in config:

 
{code:java}
DELTA = 0.1
LAMBDA = 240
OMEGA = 20
Max3PCBatchSize = 10000
Max3PCBatchWait = 1 
{code}
2. Run the load test:
{code:java}
python3.5 perf_processes.py -n 1 -t 1 -c 10 -r 100 -g perf25_transactions_genesis{code}
*Actual Results:*
Pool have wrote ~975K txs during several days. First View Change was happened on ~950K txs due to primary node failure because of out of memory. After that one more view change was happened and `incorrect state trie` appeared on part of nodes. But nodes with `incorrect state trie` were continued txns ordering.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1199,,,,,,,,,,,,,,,,,,,,"04/Jul/18 8:33 PM;sergey.khoroshavin;1422-dashboard.ods;https://jira.hyperledger.org/secure/attachment/15259/1422-dashboard.ods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzckn:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,ozheregelya,sergey.khoroshavin,,,,,,,,,,,"05/Jul/18 2:57 AM;sergey.khoroshavin;Logs investigation showed that:
1) there were multiple node restarts due to unhandled exceptions (which are now fixed in plenum starting from *1.4.429*, [PR|https://github.com/hyperledger/indy-plenum/pull/785])
2) these restarts triggered multiple edge cases (will be described as a separate comment)
3) some nodes stopped ordering simply because of insufficient disk space
4) no ledger was corrupted, which means if all nodes were restarted and given some time to catch up they would end up in same state finally. Also if another view change happened lagging nodes would be able to catch up given that no more restarts happened
;;;","05/Jul/18 7:50 PM;sergey.khoroshavin;What really happened during load test:

19:21 
Node1 ran out of disk space and restarted, triggering view change to view 1. All other nodes were able to continue ordering

20:24 
Majority of nodes (including master primary Node2) restarted due to unhandled exception. By that time they were able to order 2916 batches in view 1. Nodes 6, 21, 24 and 25 did not restart. From now pool was split into two groups:
- restarted majority - entered view 0, quickly found Node1 non responsive, changed view to 1 and started ordering requests from ppSeqNo 1
- intact minority - remained in view 1, found master primary disconnected, tried to trigger view change but failed (they are minority), then found lots of 3pc messages from view 1 that seemed like already ordered but had incorrect digests - which is no surprise since these are new messages from restarted majority

21:23
Restarted majority reached ppSeqNo 2917, which triggered incorrect state trie reports from intact minority, which is also expected. No ledger was corrupted at this point.

22:03
Node2 (as well as Node4) ran out of space and restarted, triggering view change to view 2. Intact minority also entered view 2 and started catchup, which required quite a lot of time since they missed more than 4000 batches. Node6 was able to finish this catchup and continued ordering requests.

22:39
Another restart due to unhandled exception happened on all ordering nodes. It did not happen on Nodes 21, 24 and 25, which were still trying to catchup. After that:
- restarted majority (now including Node6) entered view 0, found Node1 dead, changed view to 1, found Node2 dead as well, changed view to 2 and continued ordering new requests
- intact minority remained in view 2, tried to change view to 3 due to primary restart but failed, then continued catchup, but since incoming checkpoints shifted low watermarks back to 0 catchup and ordering logic was broken again. These nodes could be restored by either restarting them, restarting whole pool or entering view 3.



There were also a couple of suspicious symptoms:
- intact minority were stashing lots of messages (>10000) when they were in view 1, and it seemed like all of them were discarded
- when first catchup after view change finished it correctly set low watermark to ~1000 (since it did catchup not only for messages from previous view, but for current view as well), and then this watermark was reset back to 0 as well. This triggered discarding of lots of PREPREPAREs followed by requesting them again through MESSAGE_REQUESTS, and then another catchup  (when CHECKPOINT with much higher watermark was received) which discarded these messages again;;;","05/Jul/18 8:26 PM;sergey.khoroshavin;Validation will be done in scope of [INDY-1343|https://jira.hyperledger.org/browse/INDY-1343] and [INDY-1425|https://jira.hyperledger.org/browse/INDY-1425];;;",,,,,,,,,,,,,,,,,,,,,,
Cannot import name 'Replica',INDY-1423,31177,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,RyanWest,RyanWest,RyanWest,20/Jun/18 12:11 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"When running the unit tests of indy-plenum on MacOS, Ubuntu 16.04, and Fedora 28, the tests start failing with the error 'ImportError: cannot import name 'Replica' several times, even though the modules in question are definitely there. I plan to debug this and provide documenation / PR if necessary on how to fix it.
{code:java}
Traceback (most recent call last):
File ""/usr/lib64/python3.5/unittest/case.py"", line 60, in testPartExecutor
yield
File ""/usr/lib64/python3.5/unittest/case.py"", line 622, in run
testMethod()
File ""/usr/lib64/python3.5/unittest/loader.py"", line 34, in testFailure
raise self._exception
ImportError: Failed to import test module: plenum.test.batching_3pc.catch-up.test_3pc_paused_during_catch_up
Traceback (most recent call last):
File ""/usr/lib64/python3.5/unittest/loader.py"", line 428, in _find_test_path
module = self._get_module_from_name(name)
File ""/usr/lib64/python3.5/unittest/loader.py"", line 369, in _get_module_from_name
_import_(name)
File ""/home/ry/github/indy-plenum/plenum/test/batching_3pc/catch-up/test_3pc_paused_during_catch_up.py"", line 3, in <module>
from plenum.test.test_node import getNonPrimaryReplicas
File ""/home/ry/github/indy-plenum/plenum/test/test_node.py"", line 34, in <module>
from plenum.server import replica
File ""/home/ry/github/indy-plenum/plenum/server/replica.py"", line 38, in <module>
import plenum.server.node
File ""/home/ry/github/indy-plenum/plenum/server/node.py"", line 99, in <module>
from plenum.server.primary_selector import PrimarySelector
File ""/home/ry/github/indy-plenum/plenum/server/primary_selector.py"", line 7, in <module>
from plenum.server.replica import Replica
ImportError: cannot import name 'Replica'{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzfbr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,RyanWest,,,,,,,,,,,"20/Jun/18 12:28 AM;RyanWest;It appears that (part) of the problem is a circular dependency issue. When opening up Python and typing in 'from plenum.server.replica import Replica', plenum/server/replica.py imports another file, which imports another, which then tries to import replica.py again, and it fails.;;;","25/Jun/18 9:16 PM;ashcherbakov;We haven't seen such issues. And all the tests pass on Jenkins as well.
How do you configure your dev environment?
Do you follow the guide from https://github.com/hyperledger/indy-node/blob/master/docs/setup-dev.md?
BTW I can see that you use Unittest, while all the tests in Plenum/Node are based on pytest. The link above contains info on how to configure PyCharm to use pytest for test execution.;;;",,,,,,,,,,,,,,,,,,,,,,,
Migration from 1.3.x to 1.4.x failed,INDY-1424,31289,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,22/Jun/18 6:20 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,TShirt_S,,,"*Steps to Validate:*
1. Install pool 1.3.375 master.
2. Send all type of txns to ledger.
3. Upgrade pool to 1.4.472 master.
4. Check `journalctl -ex`.

*Actual Results:*
{code:java}
Jun 22 09:01:11 c7981384e90a env[3658]: 2018-06-22 09:01:11,529 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: git
Jun 22 09:01:11 c7981384e90a env[3658]: 2018-06-22 09:01:11,569 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: hg
Jun 22 09:01:11 c7981384e90a env[3658]: 2018-06-22 09:01:11,595 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: svn
Jun 22 09:01:11 c7981384e90a env[3658]: 2018-06-22 09:01:11,595 | DEBUG    | __init__.py          (  60) | register | Registered VCS backend: bzr
Jun 22 09:01:12 c7981384e90a env[3658]: Traceback (most recent call last):
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/bin/start_indy_node"", line 17, in <module>
Jun 22 09:01:12 c7981384e90a env[3658]:     run_node(config, self_name, int(sys.argv[2]), int(sys.argv[3]))
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 57, in run_node
Jun 22 09:01:12 c7981384e90a env[3658]:     looper.run()
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 261, in run
Jun 22 09:01:12 c7981384e90a env[3658]:     return self.loop.run_until_complete(what)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Jun 22 09:01:12 c7981384e90a env[3658]:     return future.result()
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Jun 22 09:01:12 c7981384e90a env[3658]:     raise self._exception
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Jun 22 09:01:12 c7981384e90a env[3658]:     result = coro.send(None)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 224, in runForever
Jun 22 09:01:12 c7981384e90a env[3658]:     await self.runOnceNicely()
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 207, in runOnceNicely
Jun 22 09:01:12 c7981384e90a env[3658]:     msgsProcessed = await self.prodAllOnce()
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Jun 22 09:01:12 c7981384e90a env[3658]:     s += await n.prod(limit)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 290, in prod
Jun 22 09:01:12 c7981384e90a env[3658]:     c = await super().prod(limit)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 980, in prod
Jun 22 09:01:12 c7981384e90a env[3658]:     self.nodestack.serviceLifecycle()
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/network/keep_in_touch.py"", line 44, in serviceLifecycle
Jun 22 09:01:12 c7981384e90a env[3658]:     self.checkConns()
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/network/keep_in_touch.py"", line 75, in checkConns
Jun 22 09:01:12 c7981384e90a env[3658]:     self.conns = self.connecteds
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/network/keep_in_touch.py"", line 69, in conns
Jun 22 09:01:12 c7981384e90a env[3658]:     self._connsChanged(ins, outs)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/network/keep_in_touch.py"", line 108, in _connsChanged
Jun 22 09:01:12 c7981384e90a env[3658]:     self.onConnsChanged(ins, outs)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1117, in onConnsChanged
Jun 22 09:01:12 c7981384e90a env[3658]:     self.send_ledger_status_to_newly_connected_node(node)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1143, in send_ledger_status_to_newly_connected_node
Jun 22 09:01:12 c7981384e90a env[3658]:     self.ledgerManager.ledger_sync_order[0])
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1918, in sendLedgerStatus
Jun 22 09:01:12 c7981384e90a env[3658]:     ledgerStatus = self.getLedgerStatus(ledgerId)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1915, in getLedgerStatus
Jun 22 09:01:12 c7981384e90a env[3658]:     return self.build_ledger_status(ledgerId)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 626, in build_ledger_status
Jun 22 09:01:12 c7981384e90a env[3658]:     return LedgerStatus(ledger_id, ledger.size, v, p, ledger.root_hash)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 91, in __init__
Jun 22 09:01:12 c7981384e90a env[3658]:     self.validate(input_as_dict)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 21, in validate
Jun 22 09:01:12 c7981384e90a env[3658]:     self._validate_fields_with_schema(dct, self.schema)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 40, in _validate_fields_with_schema
Jun 22 09:01:12 c7981384e90a env[3658]:     self._raise_invalid_fields(k, v, validation_error)
Jun 22 09:01:12 c7981384e90a env[3658]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 62, in _raise_invalid_fields
Jun 22 09:01:12 c7981384e90a env[3658]:     field, value))
Jun 22 09:01:12 c7981384e90a env[3658]: TypeError: validation error [LedgerStatus]: expected types 'str', got 'bytes' (merkleRoot=b'EXpY2StrjRki5N46aMDdjbwBPfZUz1AnLQupuVa1Tnwk')
Jun 22 09:01:12 c7981384e90a systemd[1]: indy-node.service: Main process exited, code=exited, status=1/FAILURE
Jun 22 09:01:12 c7981384e90a systemd[1]: indy-node.service: Unit entered failed state.
Jun 22 09:01:12 c7981384e90a systemd[1]: indy-node.service: Failed with result 'exit-code'.
Jun 22 09:01:23 c7981384e90a systemd[1]: indy-node.service: Service hold-off time over, scheduling restart.
Jun 22 09:01:23 c7981384e90a systemd[1]: Stopped Indy Node.
-- Subject: Unit indy-node.service has finished shutting down
{code}
and pool is down without uprgraing
{code:java}
Jun 22 08:58:44 3e500d879fe8 env[92]: 2018-06-22 08:58:44,684 | DEBUG    | node_control_tool.py ( 170) | _create_backup | Creating backup for 1.3.375
Jun 22 08:58:44 3e500d879fe8 env[92]: 2018-06-22 08:58:44,722 | INFO     | migration_tool.py    (  52) | migrate | Migrating from 1.3.375 to 1.4.472 on Ubuntu
Jun 22 08:58:44 3e500d879fe8 env[92]: 2018-06-22 08:58:44,724 | DEBUG    | migration_tool.py    (  54) | migrate | Found migration scripts: ['1_0_96_to_1_0_97', '1_1_150_to_1_1_151', '1_2_188_to_1_2_189', '1_2_233_to_1_2_234', '1_2_273_to_1_2_274', '1_3_396_to_1_3_397', 
Jun 22 08:58:44 3e500d879fe8 env[92]: 2018-06-22 08:58:44,724 | INFO     | migration_tool.py    (  61) | migrate | Following migrations will be applied: ['1_3_396_to_1_3_397', '1_3_428_to_1_3_429', '1_3_433_to_1_3_434']
Jun 22 08:58:44 3e500d879fe8 env[92]: 2018-06-22 08:58:44,724 | INFO     | migration_tool.py    (  63) | migrate | Applying migration 1_3_396_to_1_3_397
Jun 22 08:58:44 3e500d879fe8 env[92]: 2018-06-22 08:58:44,724 | INFO     | migration_tool.py    (  35) | _call_migration_script | script path /usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_396_to_1_3_397.py
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,251 | INFO     | 1_3_396_to_1_3_397.py ( 149) | migrate_all | Starting migration of storages from LevelDB to RocksDB...
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,386 | INFO     | 1_3_396_to_1_3_397.py ( 152) | migrate_all | All storages migrated successfully from LevelDB to RocksDB
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,432 | INFO     | 1_3_396_to_1_3_397.py (  67) | archive_leveldb_ledger | Archive of LevelDB-based ledger created: /tmp/Node1_ledger_leveldb.tar.gz
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,441 | INFO     | 1_3_396_to_1_3_397.py ( 194) | <module> | Migration from LevelDB to RocksDB complete
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,514 | INFO     | migration_tool.py    (  67) | migrate | Migration 1_3_396_to_1_3_397 applied in 0.7893426418304443 seconds
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,514 | INFO     | migration_tool.py    (  63) | migrate | Applying migration 1_3_428_to_1_3_429
Jun 22 08:58:45 3e500d879fe8 env[92]: 2018-06-22 08:58:45,514 | INFO     | migration_tool.py    (  35) | _call_migration_script | script path /usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_428_to_1_3_429.py
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,077 | DEBUG    | __init__.py          (60) | register | Registered VCS backend: git
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,118 | DEBUG    | __init__.py          (60) | register | Registered VCS backend: hg
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,161 | DEBUG    | __init__.py          (60) | register | Registered VCS backend: svn
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,162 | DEBUG    | __init__.py          (60) | register | Registered VCS backend: bzr
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,322 | INFO     | 1_3_428_to_1_3_429.py (346) | archive_old_ledger | Archive of old transaction format ledger created: /tmp/Node1_old_txn_ledger.tar.gz
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,405 | ERROR    | 1_3_428_to_1_3_429.py (241) | migrate_txn_log | None
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,405 | ERROR    | 1_3_428_to_1_3_429.py (242) | migrate_txn_log | Could not put key/value to the new ledger 'domain_transactions'
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,406 | ERROR    | 1_3_428_to_1_3_429.py (295) | migrate_txn_logs | Could not migrate domain_transactions, DB path: /var/lib/indy/sandbox/data/Node1/domain_transactions
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,406 | ERROR    | 1_3_428_to_1_3_429.py (369) | migrate_all | Txn log migration from old to new format failed!
Jun 22 08:58:46 3e500d879fe8 env[92]: 2018-06-22 08:58:46,406 | INFO     | 1_3_428_to_1_3_429.py (409) | <module> | Migration of txns format failed!
Jun 22 08:58:46 3e500d879fe8 env[92]: Traceback (most recent call last):
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_428_to_1_3_429.py"", line 229, in migrate_txn_log
Jun 22 08:58:46 3e500d879fe8 env[92]:     digest = put_into_seq_no_db(new_val)
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_428_to_1_3_429.py"", line 180, in put_into_seq_no_db
Jun 22 08:58:46 3e500d879fe8 env[92]:     digest = sha256(serialize_msg_for_signing(dct)).hexdigest()
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/common/serializers/serialization.py"", line 31, in serialize_msg_for_signing
Jun 22 08:58:46 3e500d879fe8 env[92]:     return signing_serializer.serialize(msg, topLevelKeysToIgnore=topLevelKeysToIgnore)
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/common/serializers/signing_serializer.py"", line 74, in serialize
Jun 22 08:58:46 3e500d879fe8 env[92]:     str(k) + "":"" + self.serialize(obj[k], level + 1, onm, toBytes=False))
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/common/serializers/signing_serializer.py"", line 74, in serialize
Jun 22 08:58:46 3e500d879fe8 env[92]:     str(k) + "":"" + self.serialize(obj[k], level + 1, onm, toBytes=False))
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/common/serializers/signing_serializer.py"", line 60, in serialize
Jun 22 08:58:46 3e500d879fe8 env[92]:     error(""invalid type found {}: {}"".format(objname, obj))
Jun 22 08:58:46 3e500d879fe8 env[92]:   File ""/usr/local/lib/python3.5/dist-packages/common/error.py"", line 9, in error
Jun 22 08:58:46 3e500d879fe8 env[92]:     raise exc_type(msg)
Jun 22 08:58:46 3e500d879fe8 env[92]: Exception: invalid type found operation.id: b'V4SGRU86Z58d6TV7PBUe6f:4:V4SGRU86Z58d6TV7PBUe6f:3:CL:16:tag:CL_ACCUM:0f03cf3ee82e69f36a489100715b48c9'
{code}

*Expected Results:*
Pool upgraded succesfully, no errors in journalctl",indy-node 1.3.375 -> 1.4.472,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1372,,,,,INDY-1428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcm7:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,"27/Jun/18 11:35 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.375 -> 1.4.482
*Actual Results:*
Migration has been successfully finished without seq_no_db data corruption;;;",,,,,,,,,,,,,,,,,,,,,,,,
Load testing of Indy Node 1.4,INDY-1425,31300,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,esplinr,esplinr,22/Jun/18 11:40 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"*Acceptance Criteria*
* Run the load tests scripts against Indy Node 1.4
* Log bugs that are found",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzckv:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,5.0,,,,,,,,,,,,esplinr,zhigunenko.dsr,,,,,,,,,,,"05/Jul/18 8:25 PM;zhigunenko.dsr;This ticket will be resolved in scope of INDY-1343. INDY-1448 and INDY-1455 have been revealed;;;",,,,,,,,,,,,,,,,,,,,,,,,
Node crashes on _remove_stashed_checkpoints,INDY-1427,31338,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,25/Jun/18 6:54 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,TShirt_M,,,"During load tests there were several cases of node crash:
{code}
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1880, in allLedgersCaughtUp
    replica.caught_up_till_3pc(last_caught_up_3PC)
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2497, in caught_up_till_3pc
    self._remove_stashed_checkpoints(till_3pc_key=last_caught_up_3PC)
File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2586, in _remove_stashed_checkpoints
    del self.stashedRecvdCheckpoints[self.viewNo][(s, e)]
KeyError: 28
{code}

This bug was quick [patched|https://github.com/hyperledger/indy-plenum/pull/768/commits/56e0995fdd431673c258656680b28086034d0601], but it seems that original code had typos, so further analysis is recommended, probably [~spivachuk] can help here.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcjr:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,sergey.khoroshavin,spivachuk,VladimirWork,,,,,,,,,,"04/Jul/18 10:40 PM;spivachuk;*Problem reason:*
- Mistake in source code.

*Changes:*
- Fixed the bug in the logic of stashed checkpoints removal which appeared after a catch-up performed during view change.
- Added tests which verify stashed checkpoints removal on master and backup replicas after a catch-up performed during view change.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/791
- https://github.com/hyperledger/indy-node/pull/805

*Version:*
- indy-node 1.4.491-master
- indy-plenum 1.4.432-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_checkpoints_removed_on_master_replica_after_catchup_during_view_change}}
- {{test_checkpoints_removed_on_backup_replica_after_catchup_during_view_change}};;;","06/Jul/18 5:47 PM;VladimirWork;Build Info:
indy-node 1.4.492

Steps to Validate:
1. Run load tests against QA Live Pool with forced view change / default pool parameters.
2. Check all nodes' logs for ""KeyError"" messages.
3. Check all nodes' ledger count and consensus.

Actual Results:
There is no ""KeyError"" entries during both load tests. The only issue found was reported as INDY-1454.;;;",,,,,,,,,,,,,,,,,,,,,,,
Ledger cannot reply on get transaction before write transaction,INDY-1428,31374,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,26/Jun/18 11:02 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.4,,,,0,,,,"*Steps to Reproduce:*
1) setup pool
2) write some txns
3) migrate to another version
4) connect via CLI and check pool workability via reading default Trustee NYM

*Actual Results:*
Existed NYM not found until another NYM will be written

*Expected Results:*
First action is successfull - no matter _get-X_ or _X_

*Workaround:*
Send any writing transactions before further work

*Additional Information:*
This is long-lived bug that was unrevealed because of typical workability check via writing new NYM
Fix can lead to a large regression","indy-node 1.3.375 -> 1.4.478
or
indy-node 1.3.62 -> 1.4.63",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1424,,,,,,,,,,,,,,,,,,,,"26/Jun/18 11:22 PM;anikitinDSR;test_get_genesis_txn.py;https://jira.hyperledger.org/secure/attachment/15180/test_get_genesis_txn.py",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzclr:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,zhigunenko.dsr,,,,,,,,,,"26/Jun/18 11:24 PM;anikitinDSR;Added test([^test_get_genesis_txn.py] which fails if 'data' section in reply is None.;;;","28/Jun/18 10:26 PM;ashcherbakov;Fixed in the scope of INDY-1436;;;","29/Jun/18 7:01 PM;zhigunenko.dsr;*Environment:*
indy-node 1.3.62 -> 1.4.66

*Steps to Validate:*
1) setup pool
2) write some txns
3) migrate to another version
4) connect via CLI and check pool workability via reading default Trustee NYM

*Actual Results:*
Read as first action is successfull;;;",,,,,,,,,,,,,,,,,,,,,,
Perf pool stopped ordering txns during load test,INDY-1429,31379,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,26/Jun/18 11:53 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,TShirt_M,,,"*Versions:*
indy-node: 1.4.478
indy-plenum: 1.4.418

*Node config*
logLevel=DEBUG
forceViewChangeFreq=600

*What happened*
When running load test on with two agents running
{code}
python3.5 perf_processes.py -c 20 -n 1000 -k nym -g pool_transactions_genesis
{code}
in a loop after some view change pool stopped ordering transactions. Ledgers were NOT corrupted, all nodes eventually reached same state, but refused to order any new txn. Restarting all nodes in a pool fixed problem temporarily.

After investigation of log files it turned out that in the beginning of some view primary generated first PREPREPARE with more than 9000 txns, with size about 640k, while default MSG_LEN_LIMIT is just 128k. So, this PREPREPARE was not delivered to other nodes and no new transactions were ordered.

It's recommended to either increase MSG_LEN_LIMIT to 768k or decrease Max3PCBatchSize accordingly.

*Acceptance criteria*
Find the root cause and create a ticket for fixing",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1334,,,,,INDY-1435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzcmn:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,sergey.khoroshavin,,,,,,,,,,,,"27/Jun/18 2:13 AM;sergey.khoroshavin;Tried two more cases:
1) MSG_LEN_LIMIT=768k, Max3PCBatchSize=10000 (current default)
2) MSG_LEN_LIMIT=128k (current default), Max3PCBatchSize=1000
In both cases pool stopped writing with same symptoms (equal ledgers, but new transactions are not accepted until restart)

Logs retrieved, going to investigate further.;;;","28/Jun/18 7:31 PM;sergey.khoroshavin;During investigation of two more failure cases it was found that:
1) Large PREPREPARE after view change were delivered to non-primary nodes, which means that increasing MSG_LEN_LIMIT did help
2) First PREPREPARE after view change was large enough to make dynamic validation take significant amount of time (>5 seconds) on master replica, so it was sent later compared to PREPREPAREs from backups, which were sent almost immediately after view change
3) Backup replices ordered first batch in just about 5-10 seconds, while just delivery of first PREPREPARE from master replica took >10 seconds. Also, dynamic validation of PREPREPARE on non-primary replicas in master instance took another 5 seconds.
4) Given above timings first batch in master instance should have been ordered in >25 seconds, but by that time backup instances ordered enough batches so that monitor found master thoughput severely degraded, which triggered another view change.
5) When new view change ended there were even more transactions in first PREPREPARE, so everything repeated.

Conclusions:
1) When increasing max batch size MSG_LEN_LIMIT should be increased as well (probably it should be some function of batch size)
2) Thoughput measurements by monitor should be windowed, with window length based on maximum allowed latency, this should be addressed in [INDY-1435|https://jira.hyperledger.org/browse/INDY-1435]
3) Dynamic validation time on master instance should be taken into account by monitor somehow, one possible approach is [INDY-34|https://jira.hyperledger.org/browse/INDY-34]
;;;",,,,,,,,,,,,,,,,,,,,,,,
Research the opportunity of automated AWS pool deployment,INDY-1430,31384,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,zhigunenko.dsr,zhigunenko.dsr,27/Jun/18 12:50 AM,21/Sep/18 4:49 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,via AWS CLI,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzg9j:",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement periodic restart of client stack to allow new clients to connect,INDY-1431,31407,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,sergey-shilov,sergey-shilov,27/Jun/18 8:55 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"All research was done in scope of [INDY-1417|https://jira.hyperledger.org/browse/INDY-1417], please check [research result and PoA|https://jira.hyperledger.org/browse/INDY-1417?focusedCommentId=46595&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-46595].

As a client stack restart option I prefer a combination of timeout and connections limit to avoid unnecessary restart and infinite restarts.

Note that as a result of stack restart we may have 2X sockets during about one minute, it should be considered in our file descriptors limits calculations.",,,,,,,,,,,,,,,,,,,,,,,,INDY-1418,,,,,INDY-1417,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzwwj3:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,5.0,,,,,,,,,,,,sergey-shilov,VladimirWork,,,,,,,,,,,"12/Jul/18 2:01 AM;sergey-shilov;*Problem state / reason:*

Need to implement client stack restart if connections limit reached to allow new clients to connect.

*Changes:*

Client stack is restarted if connections threshold is reached and specified minimal time (configurable, should be greater than TIME_WAIT) is spent since previous client stack restart to avoid sockets overhead in TIME_WAIT or FIN_WAIT state.
 Connections tracking mechanism is based on monitor socket that reports events occurred on listener socket.
 Connections tracking and stack restart are implemented as separate features that may be enabled or disabled. NOTE: connections tracking must be enabled if stack restart is enabled as stack restart uses connections tracking mechanism for triggering of restart.

Both connections tracking and stack restart features are disabled by default.

*Committed into:*

    [https://github.com/hyperledger/indy-plenum/pull/799]
     [https://github.com/hyperledger/indy-node/pull/814]
     indy-node 1.4.498-master

*Risk factors:*

    Clients may not receive their replies.

*Risk:*

    Medium

*Recommendations for QA:*

Set the following parameters in indy config file:
 _MAX_CONNECTED_CLIENTS_NUM = 1_  # connections limit
 _MIN_STACK_RESTART_TIMEOUT = 15  # seconds_
 _MAX_STACK_RESTART_TIME_DEVIATION = 5_  _# seconds_

NOTE: client stack is restarted in time period {color:#333333}_[MIN_STACK_RESTART_TIMEOUT, MIN_STACK_RESTART_TIMEOUT + MAX_STACK_RESTART_TIME_DEVIATION]_{color} since connections limit is reached.

Test several scenarios:

1) Enable only connections tracking: _TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True._ Connect one client and check logs, ""connections limit reached"" should be reported. Then disconnect client, ""connections number fell below the limit"" should be reported.

2) Enable both connections tracking and stack restart: _TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True, CLIENT_STACK_RESTART_ENABLED = True._ Connect one client and check logs, ""connections limit reached"" and ""connections limit reached but too few time spent since client stack start"" should be reported. Wait a bit more than sum of _MIN_STACK_RESTART_TIMEOUT and MAX_STACK_RESTART_TIME_DEVIATION,_ than check logs, ""Going to restart client stack"" and stack restart messages should be reported.

3) Enable both connections tracking and stack restart. Connect one client and check logs, ""connections limit reached"" and ""connections limit reached but too few time spent since client stack start"" should be reported. Disconnect client immediately and check logs, ""connections number fell below the limit"" and ""client stack restart is not needed anymore"" should be reported.

4) Enable both connections tracking and stack restart, set _MAX_CONNECTED_CLIENTS_NUM_ not so high (100, 300...), other parameters leave as default. Run pool under heavy load periodically reaching and exceeding connections limit, find out if there is some strange behaviour of nodes or clients.

5) Please check upgrade procedure. The _/etc/indy/indy.env_ file should be changed by migration script:
 * old value: CLIENT_CONNECTIONS_LIMIT=15360
 * new value: CLIENT_CONNECTIONS_LIMIT=10000

 ;;;","19/Jul/18 3:56 PM;VladimirWork;All cases are checked against 4 and 25 nodes pools. Need to be retested with new libindy with increased connection timeout against 25 nodes pool.;;;","20/Jul/18 1:08 AM;VladimirWork;Build Info:
indy-node 1.5.514

Steps to Validate:
1) Enable only connections tracking: TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True. Connect one client and check logs, ""connections limit reached"" should be reported. Then disconnect client, ""connections number fell below the limit"" should be reported.

2) Enable both connections tracking and stack restart: TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True, CLIENT_STACK_RESTART_ENABLED = True. Connect one client and check logs, ""connections limit reached"" and ""connections limit reached but too few time spent since client stack start"" should be reported. Wait a bit more than sum of MIN_STACK_RESTART_TIMEOUT and MAX_STACK_RESTART_TIME_DEVIATION, than check logs, ""Going to restart client stack"" and stack restart messages should be reported.

3) Enable both connections tracking and stack restart. Connect one client and check logs, ""connections limit reached"" and ""connections limit reached but too few time spent since client stack start"" should be reported. Disconnect client immediately and check logs, ""connections number fell below the limit"" and ""client stack restart is not needed anymore"" should be reported.

4) Enable both connections tracking and stack restart, set MAX_CONNECTED_CLIENTS_NUM not so high (100, 300...), other parameters leave as default. Run pool under heavy load periodically reaching and exceeding connections limit, find out if there is some strange behaviour of nodes or clients.

5) Check upgrade procedure. The /etc/indy/indy.env file should be changed by migration script:

old value: CLIENT_CONNECTIONS_LIMIT=15360
new value: CLIENT_CONNECTIONS_LIMIT=10000

Actual Results:
Client stack restart works as expected in all cases. Also additional testing will be performed in scope of INDY-1496.;;;",,,,,,,,,,,,,,,,,,,,,,
Load script should modify pool and config ledgers,INDY-1432,31411,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,zhigunenko.dsr,zhigunenko.dsr,27/Jun/18 9:45 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Implement txns to ledgers of different types:
- pool
- config
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1368,,,,,,,,,"1|hzzgdj:",,,,,,,,,,,,,,,,,,,,,,,,,,dsurnin,NataliaDracheva,VladimirWork,zhigunenko.dsr,,,,,,,,,"01/Nov/18 12:05 AM;NataliaDracheva;*Changes reason:*
Load script should be able to send requests to pool and config ledgers in order to test such transactions along with domain requests.

*Changes:*
- Added pool ledger transaction type
- Added config ledger transaction type

*PR:*
- https://github.com/hyperledger/indy-node/pull/1006

*Version:*
- master

*Risk factors:*
- Didn't test both requests type together or along with other transactions.

*Risk:*
- Medium

*Recommendations for QA:*
Run load script with:
1. -k config_writes 
2. -k pool_writes
parameters;
3. check with both kinds simultaneously;
4. check both types with nym or any other domain transactions.;;;","01/Nov/18 8:00 PM;dsurnin;fixed review notes
https://github.com/hyperledger/indy-node/pull/1008

-k ""cfg_writes""
-k ""demoted_node"";;;","07/Nov/18 11:15 PM;VladimirWork;Load script writes in both ledgers successfully but some unexpected behaviour was found during both domain and pool ledgers loading (INDY-1823).;;;",,,,,,,,,,,,,,,,,,,,,,
Evaluate network performance with more nodes,INDY-1433,31414,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,esplinr,esplinr,27/Jun/18 10:39 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"*Background*

We currently test with a pool of 25 nodes, but we are on track to recruit more than that number of stewards. Additional nodes provides more confidence that the network cannot be undermined. We need to discover how the pool will behave with a larger number of stewards so that we can appropriately schedule the on-boarding of additional stewards.

 We recognize that there are problems with view change that will not be resolved until late 2018 (See INDY-1376), so we need to know how the current implementation behaves.

*Acceptance Criteria*

Test consensus with the specified numbers of nodes. There may be three tests: 50 nodes, 40 nodes, and 30 nodes. If the test fails, test again with fewer nodes. If network performance is acceptable, then end the test and report back that we are ready for further testing at that level.
 * Set up the pool and load it with traffic for 30 minutes
 * Evaluate the quality of the network
 ** Is there a consistent view of the master?
 ** Is performance reasonable?
 ** Are there lots of failed transactions?
 * Trigger a view change, continue load for 10 minutes, and re-evaluate the quality of the network.

Report on the performance of the network at each tested level.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1004,INDY-1003,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwvif:00001yw969w4c98",,,,,,,,,,,,,,3.0,,,,,,,,,,,,esplinr,,,,,,,,,,,,"17/Jul/18 12:28 AM;esplinr;[~ashcherbakov] pointed out that the number of messages increases exponentially with the number of nodes:
Increasing the number of validator nodes will increase the average number of node-to-node messages to process 1 3PC batch (of 100 requests) for each node from 3000 (25 nodes) to 6000 (31 nodes).

I spoke with [~nage], [~danielhardman], and [~tharmon], who each said it is a common expectation that the validator node pool of the Sovrin network will be capped at 25 nodes for the time being.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Rewrite some skipped post-installation tests and add new instead of skipped,INDY-1434,31439,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,28/Jun/18 5:50 PM,01/Nov/19 10:04 PM,28/Oct/23 2:47 AM,01/Nov/19 10:04 PM,,,,,,0,,,,We have 33 skipped post-installation tests due to Indy-SDK API changes. At least 10 of them (parameterized schema_invalid_test.py with 2 test cases against 5 parameters each) can be rewritten from scratch based on schema_json with various parameters returned from anoncreds module. Also new tests can be added instead of skipped ones due to removed / completely reworked SDK methods.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1270,,,,,INDY-1590,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzghz:",,,,,,,,,,,,,,,,,,,,,,,,,,VladimirWork,,,,,,,,,,,,"01/Nov/19 10:04 PM;VladimirWork;This repo is archived now. All necessary tests were implemented in indy-test-automation repo.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Throughput measurements in monitor should be windowed,INDY-1435,31442,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,28/Jun/18 7:44 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,"Current thoughput measurement in monitor is done as soon as there are a few batches ordered. It turned out that with large enough batches (and max batch size was significantly increased recently due to [INDY-1334|https://jira.hyperledger.org/browse/INDY-1334]) this behavior makes it's possible to enter perptual view change (details are in [INDY-1429|https://jira.hyperledger.org/browse/INDY-1429]).

Things to do:
1) Extract measurement code in monitor into unit testable class and cover it with enough tests, including ones showing problems with current throughput measurement
2) Implement windowed throughput measurements
3) Re-run load tests from [INDY-1429|https://jira.hyperledger.org/browse/INDY-1429] to see if problem is really gone",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1429,INDY-1334,,,,INDY-1468,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzhpz:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,5.0,,,,,,,,,,,,anikitinDSR,sergey.khoroshavin,VladimirWork,,,,,,,,,,"10/Jul/18 10:20 PM;anikitinDSR;Reasons:
 * need to change throughput measurements for excluding any false-positive ""master degraded"" events.

Changes:
 * Added threshold window = ThroughputInnerWindowSize * ThroughputThresholdWindowCount seconds. For now, throughput will return not None value only after this threshold.
By default, ThroughputInnerWindowSize = 15 seconds, ThroughputThresholdWindowCount = 16. Therefore, resulted threshold by default is 240 seconds.
 * Throughput value is calculating now by using exponential moving average with alpha = 2 / (n+1), where 'n' is ThroughputThresholdWindowCount.

Version:
 * indy-node 1.4.496
 * indy-plenum 1.4.441

Recomendation for QA:

- Corresponding to description, retest INDY-1429 and frequency of view_change should be decrease.;;;","12/Jul/18 12:20 AM;VladimirWork;Build Info:
indy-node 1.4.496

Steps to Validate:
1. Run `while true; do python3 perf.py -c 20 -n 1000 -k nym -g pool_transactions_genesis; done` against 22 nodes pool with forced view changes.
2. Check logs for view change intervals.

Actual Result:
There are no unexpected view changes caused by master *throughput* degradation but there are several view changes caused by master *latency* degradation (due to regular forced view changes that increase master latency according to current latency measurements) so INDY-1468 is reported.;;;",,,,,,,,,,,,,,,,,,,,,,,
Unable to read from ledger after the upgrade,INDY-1436,31443,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,28/Jun/18 8:08 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.4,,,,0,,,,"Build Info:
indy-node 1.3.62 rc -> 1.4.64 rc

Steps to Reproduce:
1. Upgrade the whole pool with force=True from 1.3.62 to 1.4.64 simultaneously.
2. Send NYM txn via old or new CLI.
3. Send GET_NYM txns to read NYMs that were added before and after the upgrade.
4. Send SCHEMA txn to the ledger.
5. Send GET_SCHEMA txn to read SCHEMA from Step 4.
6. Send GET_NYM txns to read NYMs that were added before and after the upgrade.

Actual Results:
GET_NYM txns from Step 3 returns `NYM not found` for existing NYMs but GET_NYM txns from Step 6 returns correct data for the same NYMs.

Expected Results:
GET_NYM txns should work normally in both cases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzclj:",,,,,,EV 18.13 Benchmark hardening,,,,,,,,,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"28/Jun/18 10:25 PM;ashcherbakov;Problem reason: 
- State trie is re-created after the Upgrade and BLS signature store is deleted. 
- There were some nodes in the pool which didn't have BLS keys correctly initialized
- Since we don't requite that all (n-f) Commits need to have BLS for ordering, it's possible that for a couple of first writes BLS signature was still not created for the latest state.
- When we send read request, and there is no BLS signature for the currently committed state (` get_value_from_state`), we call `self.state.get_for_root_hash`, and there was a bug in this branch of code, *that parameters were switched*.
- There were no unit tests for this branches

Changes: 
- Added unit tests for all branches
- Fixed order of arguments for `get_for_root_hash` in `get_value_from_state`.

PR:
- https://github.com/hyperledger/indy-plenum/pull/778

Version:
- RC 1.4.66

Risk factors:
- Read requests

Risk:
- Low

Covered with tests:
- test_get_value_default_head_hash
- test_get_value_old_head_hash

Recommendations for QA
- create a new pool and try to read genesis txn data without any writes (like in INDY-1428)
- have a pool with some data (after some write txns), remove `signature_store` on all nodes, and try to read the data
- read data after migration (test on RC?);;;","30/Jun/18 12:36 AM;VladimirWork;Build Info:
indy-node 1.4.66 rc

Steps to Validate:
1. Create a new 1.4.66 pool / upgrade it from 1.3.62 to 1.4.66 and try to read genesis txn data without any writes.
2. Upgrade a pool from 1.3.62 to 1.4.66 -> remove /state_signature/ folder at all nodes and restart them -> try to read data from ledger without any writes.

Actual Results:
Get txns works correct in both cases. Persistent pool (where an issue was found) upgraded from 1.4.64 to 1.4.66 also works normally.;;;",,,,,,,,,,,,,,,,,,,,,,,
Check in CI that python package in installable ,INDY-1438,31534,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Jul/18 11:30 PM,21/Sep/18 4:49 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,Some PRs might change structure and content of python package that is built and published in CD pipeline. It's out of the unit/integration tests scope. It would be useful to make CI test if the python package could be built and installed.,,,,,,,,,,,,,,,,,,,,,INDY-1439,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzgvj:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Check in CI that debian package in installable ,INDY-1439,31536,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Jul/18 11:31 PM,21/Sep/18 4:49 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,Some PRs might change structure and content of debian package that is built and published in CD pipeline. It's out of the unit/integration tests scope. It would be useful to make CI test if the package could be built and installed.,,,,,,,,,,,,,,,,,,,,,,,INDY-1438,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzgvz:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use pipenv to make python builds more deterministic,INDY-1440,31539,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Jul/18 11:45 PM,21/Sep/18 4:48 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"[pipenv|https://docs.pipenv.org/] seems as very promising and handy tool to make separated environment for python projects. It provides clean API and introduces lock file for python projects dependencies that should make python env more deterministic.

In particular it could act as a replacement for virtualenv (virtualenvwrapper):
 * for all indy projects to generate and manage lock files
 * in [one of base dockerfiles|https://github.com/hyperledger/indy-node/blob/master/environment/docker/baseimage/scripts/user.sh] for CI (where usage if virtualenv is implemented with some rough workarounds)
 * in [dev scripts|https://github.com/hyperledger/indy-node/tree/master/dev-setup/ubuntu] which setup dev environment",,,,,,,,,,INDY-1706,,,,,,,,,,,,,,,,,,,INDY-1701,INDY-1702,INDY-1703,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzgwv:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Require PR's head to be up-to-date with master branch,INDY-1441,31541,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,02/Jul/18 11:50 PM,31/Jul/18 7:28 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,As long as currently PRs' CI pipelines are not triggered when master is updated it is possible that one of two concurrent PRs could be merged without testing against current master. We can fix that turning on *Require branches to be up to date before merging* protection for master branch for all indy core repos.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzgw7:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Consider to use MagicStack/uvloop as a looper for indy-plenum asyncio,INDY-1442,31544,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,03/Jul/18 12:10 AM,03/Jul/18 12:12 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"{quote}uvloop is a fast, drop-in replacement of the built-in asyncio event loop. uvloop is implemented in Cython and uses libuv under the hood.
{quote}
([https://github.com/MagicStack/uvloop)]

We may consider to use it instead of default asyncio looper and indy-plenum's custom looper wrapper.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-414,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzgxr:",,,,,,,,,,,,,,,,,,,,,,,,,,andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
validator-info -v --json does not produce valid JSON,INDY-1443,31563,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ckochenower,ckochenower,03/Jul/18 4:03 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,validator-info,,,0,,,,"'validator\-info \-\-json' produces terse JSON output. The '\-v' option produces verbose output, but when combined with '\-\-json', the output is not valid JSON.

Steps to reproduce:

1. Run 'validator-info \-v \-\-json'

Sample output:

1. 'validator\-info \-\-json'
{noformat}
sudo validator-info --json
{
  ""response-version"": ""0.0.1"",
  ""timestamp"": 1530557924,
  ""Node_info"": {
    ""Name"": ""Node8"",
    ""did"": ""98VysG35LxrutKTNXvhaztPFHnx5u9kHtT7PnUGqDa8x"",
    ""verkey"": ""<redacted>"",
    ""Node_port"": [
      {
        ""ip"": ""0.0.0.0/0"",
        ""port"": 9715,
        ""protocol"": ""tcp""
      }
    ],
    ""Client_port"": [
      {
        ""ip"": ""0.0.0.0/0"",
        ""port"": 9716,
        ""protocol"": ""tcp""
      }
    ],
    ""Metrics"": {
      ""uptime"": 4505,
      ""transaction-count"": {
        ""config"": 0,
        ""ledger"": 412,
        ""pool"": 10
      },
      ""average-per-second"": {
        ""read-transactions"": 0.0,
        ""write-transactions"": 0.0
      }
    }
  },
  ""state"": ""running"",
  ""enabled"": true,
  ""Pool_info"": {
    ""Total_nodes_count"": 10,
    ""Reachable_nodes"": [
      ""Node1"",
      ""Node10"",
      ""Node2"",
      ""Node3"",
      ""Node4"",
      ""Node5"",
      ""Node6"",
      ""Node7"",
      ""Node8"",
      ""Node9""
    ],
    ""Reachable_nodes_count"": 10,
    ""Unreachable_nodes"": [],
    ""Unreachable_nodes_count"": 0
  },
  ""software"": {
    ""indy-node"": ""1.4.480"",
    ""sovrin"": null
  }
}
{noformat}

2. 'validator\-info \-v \-\-json` \- Only the first 61 lines included for brevity. Just enough to prove the output is not valid JSON.
{noformat}
  1 ""Pool_info"":
  2      ""Read_only"":    n/a
  3      ""Reachable_nodes_count"":  10
  4      ""f_value"":      3
  5      ""Blacklisted_nodes"":
  6      ""Total_nodes_count"":  10
  7      ""Suspicious_nodes"":  n/a
  8      ""Unreachable_nodes_count"":  n/a
  9      ""Unreachable_nodes"":
 10      ""Reachable_nodes"":
 11            Node1
 12            Node10
 13            Node2
 14            Node3
 15            Node4
 16            Node5
 17            Node6
 18            Node7
 19            Node8
 20            Node9
 21      ""Quorums"":      {'observer_data': Quorum(4), 'timestamp': Quorum(4), 'ledger_status': Quorum(6), 'bl    s_signatures': Quorum(7), 'election': Quorum(7), 'consistency_proof': Quorum(4), 'propagate': Quorum(4),     'checkpoint': Quorum(6), 'commit': Quorum(7), 'ledger_status_last_3PC': Quorum(4), 'propagate_primary': Q    uorum(4), 'same_consistency_proof': Quorum(4), 'view_change_done': Quorum(7), 'f': 3, 'prepare': Quorum(6    ), 'reply': Quorum(4), 'view_change': Quorum(7)}
 22 ""Hardware"":
 23      ""HDD_used_by_node"":  2650 MBs
 24      ""HDD_all"":      2650 Mbs
 25      ""RAM_all_free"":  7512 Mbs
 26      ""RAM_used_by_node"":  478 Mbs
 27 ""response-version"":  0.0.1
 28 ""Software"":
 29      ""Indy_packages"":
 30            hi  indy-anoncreds                   1.0.32                                     amd64        A    nonymous credentials
 31            hi  indy-node                        1.4.480                                    amd64        I    ndy node
 32            hi  indy-plenum                      1.4.419                                    amd64        P    lenum Byzantine Fault Tolerant Protocol
 33            hi  libindy-crypto                   0.4.0                                      amd64        T    his is the shared crypto libirary for Hyperledger Indy components.
 34            hi  python3-indy-crypto              0.4.1                                      amd64        T    his is the official wrapper for Hyperledger Indy Crypto library (https://www.hyperledger.org/projects).
 35 
 36      ""OS_version"":   Linux-4.4.0-1061-aws-x86_64-with-Ubuntu-16.04-xenial
 37      ""sovrin"":       n/a
 38      ""indy-node"":    1.4.480
 39      ""Installed_packages"":
 40            pytest 3.5.1
 41            semver 2.7.9
 42            indy-node 1.4.480
 43            rlp 0.5.1
 44            indy-crypto 0.4.1
 45            Pygments 2.2.0
 46            base58 1.0.0
 47            ioflo 1.5.4
 48            orderedset 2.0
 49            sha3 0.2.1
 50            setuptools 38.5.2
 51            sortedcontainers 1.5.7
 52            timeout-decorator 0.4.0
 53            indy-plenum 1.4.419
 54            python-dateutil 2.6.1
 55            python-rocksdb 0.6.9
 56            Charm-Crypto 0.0.0
 57            pluggy 0.6.0
 58            pyzmq 17.0.0
 59            indy-plenum-dev 1.4
 60            more-itertools 4.1.0
 61            six 1.11.0
{noformat}","{panel:title=My Indy Node environment}
~$ sudo apt list --installed | grep indy

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

indy-anoncreds/xenial,now 1.0.32 amd64 [installed]
indy-node/xenial,now 1.4.480 amd64 [installed,upgradable to: 1.4.483]
indy-plenum/xenial,now 1.4.419 amd64 [installed,upgradable to: 1.4.423]
libindy-crypto/xenial,now 0.4.0 amd64 [installed]
python3-indy-crypto/xenial,now 0.4.1 amd64 [installed]

~$ pip3 freeze | grep indy
You are using pip version 8.1.1, however version 10.0.1 is available.
You should consider upgrading via the 'pip install --upgrade pip' command.
indy-anoncreds==1.0.32
indy-crypto==0.4.1
indy-node==1.4.480
indy-plenum==1.4.419
indy-plenum-dev==1.4
{panel}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jul/18 3:55 PM;VladimirWork;INDY-1443.PNG;https://jira.hyperledger.org/secure/attachment/15288/INDY-1443.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzhqn:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,anikitinDSR,ckochenower,mgbailey,VladimirWork,,,,,,,,,"03/Jul/18 4:20 AM;ckochenower;1) \-\-json should produce completely parsable JSON
2) \-\-json should be the highest verbosity. No need for \-v to get more data.;;;","03/Jul/18 4:27 AM;mgbailey;I'll add support to what [~ckochenower] is saying. --json should always imply the highest verbosity, since it is by nature machine consumable, and thus is intended to be filtered and used by an automated system.

Changes made for the local validator-info script should also be included in the client get validator info transaction.;;;","09/Jul/18 11:46 PM;anikitinDSR;Reasons:
 * --json option should be the highest verbosity

Changes:
 * For now, --json option return valid JSON output with the highest verbosity

Versions:
 * indy-node: 1.4.495

Recommendation for QA:

- run validator-info with --json option and check, that all possible information is displayed as a valid JSON;;;","10/Jul/18 10:10 PM;VladimirWork;Build Info:
indy-node 1.4.495

Steps to Validate:
1. Run validator-info with --json / -v / --json -v / -v --json.

Actual Results:
Validator-info -v returns human-readable output. All other combinations returns valid JSON with full node information.;;;","11/Jul/18 3:11 AM;ckochenower;[~VladimirWork] - The output of \-\-json \-v appears to be valid JSON, but it is not. It is, however, valid python.

result.stdout in the following Pdb session contains the output from `validator-info \-v \-\-json`.
{code}
(Pdb) validatorInfoDict = json.loads(result.stdout)
*** json.decoder.JSONDecodeError: Expecting property name enclosed in double quotes: line 1 column 2 (char 1)
(Pdb) validatorInfoDict = eval(result.stdout)
(Pdb) validatorInfoJSON = json.dumps(validatorInfoDict)
(Pdb) validatorInfoDict = json.loads(validatorInfoJSON)
(Pdb)
{code}

Notice that json.loads fails with a json.decoder.JSONDecodeError because property names are not enclosed in double quotes? Drop the output in any JSON linter (i.e. jsonlint.org) and you will see what I mean.

Notice that eval parses the results into a python dict w/o error?

I can provide you concrete examples via slack.

The workaround for now is to use eval(...) instead of json.loads(...);;;","11/Jul/18 3:55 PM;VladimirWork;It looks like we should change single quotes to double. !INDY-1443.PNG|thumbnail! ;;;","11/Jul/18 8:25 PM;anikitinDSR;was released new indy-node 1.4.497 with fix json output. ;;;","11/Jul/18 11:27 PM;ckochenower;[~VladimirWork] and [~anikitinDSR] - I have upgraded to indy-node 1.4.497 and tested the change. I can confirm that we now have valid JSON. Thank you!;;;","11/Jul/18 11:28 PM;ckochenower;Would be nice to have a regression test for this.;;;",,,,,,,,,,,,,,,,
Catchup requires indy-node service restart,INDY-1444,31564,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ckochenower,ckochenower,03/Jul/18 4:16 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Simulating packet/message loss using iptables/nftables produced unexpected behavior in catchup process. The indy-node service must be restarted to trigger catchup when packages/messages are lost due to dropped packets/messages at the firewall.

[~sergey.khoroshavin] asked that the following properties be added to /etc/indy/indy_config.py on each of the validator nodes to test if this issue is related to INDY-1429. The change in configuration did not change the outcome.

Steps to reproduce:

Setup:
# Open 4 ssh sessions (terminal 1 - 4)
# (terminal 1) - Generate some nym transactions. ssh to a client node. This is where you will run the indy-node perf_processes load script to write 400 nyms to the domain ledger.
## ssh to a client node and be prepared to run the following load script:
{code} time python3 /home/ubuntu/indy-node/scripts/performance/perf_processes.py -c 20 -n 20 -k nym -g /home/ubuntu/pool_transactions_genesis {code}
# (terminal 2) - Monitor ""Number_txns_in_catchup"" for the domain ledger (""0"") using validator-info output.
## ssh to one of the validator nodes and run the following command. You may need to adjust -C 30 to something smaller or larger to capture all of the Catchup_status output. Unfortunately, 'validator-info -v --json` does not currently produce valid JSON output ( see INDY-1443 )
{code}watch -n3 'sudo validator-info -v 2>/dev/null | grep -C 30 Catchup_status'{code}
# (terminal 3) - Tail the log for 'catchup' log messages
{code}
tail -f /var/log/indy/sandbox/Node8.log | grep -i catch
{code}
# (terminal 4) - Block and unblock the node port during the experiment.

Experiment:
(terminal 4) - Add a firewall rule to block the node port (9715 in this case. Get the node port from the genesis transaction file)
{code}
sudo iptables -A INPUT -p tcp --destination-port 9715 -j DROP
{code}
(terminal 1) - Generate 400 nym transactions
{code}
time python3 /home/ubuntu/indy-node/scripts/performance/perf_processes.py -c 20 -n 20 -k nym -g /home/ubuntu/pool_transactions_genesis
{code}
(terminal 4) - Once all 400 nym transactions are written, unblock the node port
{code}
sudo iptables -D INPUT -p tcp --destination-port 9715 -j DROP
{code}
(terminal 2) Observe that the domain ledger (Catchup_status > Number_txns_in_catchup > 1) count has not changed. It doesn't matter how long you wait, it will never change.
(terminal 3) Observe that no catchup log messages are being written to the log. Be aware that log rotation may require you to re-tail the log if the file handle changes.
(terminal 4) After you have waited long enough to be convinced that unblocking the port isn't enough to trigger catchup... Restart the indy-node service
{code}
sudo systemctl restart indy-node
{code}
(terminals 2 and 3) Both 'validator-info -v' and the logs should show evidence that catchup is being performed. After about a minute or two, 'validator-info -v' should report something similar to the following. Notice the 400 next to ""1"" under ""Number_txns_in_catchup""?

{code}
...
     ""Catchup_status"":
          ""Last_txn_3PC_keys"":
               ""1"":            
               ""0"":            
               ""2"":            
          ""Ledger_statuses"":
               ""1"":            synced
               ""0"":            synced
               ""2"":            synced
          ""Waiting_consistency_proof_msgs"":
               ""1"":            n/a
               ""0"":            n/a
               ""2"":            n/a
          ""Number_txns_in_catchup"":
               ""1"":            400
               ""0"":            n/a
               ""2"":            n/a
          ""Received_LedgerStatus"":  n/a
...
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jul/18 5:45 AM;ckochenower;dump-of-journalctl.txt;https://jira.hyperledger.org/secure/attachment/15215/dump-of-journalctl.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzh47:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,ckochenower,,,,,,,,,,,"03/Jul/18 5:35 AM;ckochenower;[~ashcherbakov] says: ""...restart is just one of the ways to start catchup. Another strategy to start Cathup is when node understands that it is behind by receiving a number of stable checkpoints. So, a load is need (about 200 requests) before a node starts catchup.""

I attempted to generate several thousand transactions (4000+) and wait a sufficient amount of time (10+ minutes) after unblocking the node port and was unable to observe the node ""catch up"" in the logs or in validator-info output. As expected, restarting indy-node brought the node back in sync.

{noformat}
Node8.log:2018-07-02 20:18:30,383 | INFO     | ledger_manager.py    ( 860) | mark_ledger_synced | CATCH-UP: Node8 completed catching up ledger 1, caught up 4600 in total
{noformat}

 I still think we have a bug.

[~sergey.khoroshavin] asked that I attach journalctl and node logs. See attached:

dump-of-journalctl.txt

jira.hyperledger.org has a 10MB file size limit for uploads. You may retrieve the indy-node logs at:

https://drive.google.com/file/d/1QkxxTHGOSND35ms7arVitsXjXA6BjaLx/view?usp=sharing;;;","03/Jul/18 9:12 PM;ashcherbakov;Catchup is started if a node realizes that it's behind by *200 3PC batches,* not 200 requests. Each 3PC bath is created each second, and may contain up to 10000 requests.

So, you need to make sure that 200 batches are created, that is that your load script send at least 200 txns with a delay of >= 1 sec.

The load script that you run doens't specify delay between requests, so they all may be sent at once, and be placed into just a couple of 3PC batches.

Please use `-t` option to specify a delay between requests. See, [https://github.com/hyperledger/indy-node/blob/master/docs/process-based-load-script.md#examples] and, for example, `{{python3 perf_processes.py -c 100 -n 10 -t 1 -k TXN_TYPE}}`;;;","03/Jul/18 11:53 PM;ckochenower;[~ashcherbakov] - Thank you for the detailed explanation. I see now that ""So, a load is need (about 200 requests) before a node starts catchup."" is more accurately written ""So, a load is need *(about 200 3PC batches)* before a node starts catchup.""

I will add the '-t 1' when generating load and try again. I will also attempt to discover how many 3PC batches are generated using validator info to ensure the load script generates at least 200 3PC batches.

I will close this ticket if catchup is triggered. Otherwise, I will modify the description to account for changes in my experiment.;;;","04/Jul/18 1:41 AM;ckochenower;[~VladimirWork] successfully demonstrated that catchup is triggered after unblocking the node port AND generating a sufficient number of transactions / 3PC batches.

He followed the same steps I outlined in the description. However, his performance script invocation differed from mine. The difference being I used '-c 20 -n 20' and he used '-c 1 -n 400'.

I retried my experiment using '-c 1 -n 400' while the node port was blocked by the firewall followed by two invocations of '-c 1 -n 400' (800 txns in total - not sure how many 3PC batches this equated to) after unblocking the node port and observed the node ""catch up"".

I won't attempt to understand or explain how '-c 20 -n 20' and '-c 1 -n 400' differs at the transaction and 3PC batch level. However, I will create a chaosindy experiment that ensures catchup is triggered.

Closing this issue as 'Won't Fix'. Please change the status if 'Won't Fix' is not accurate.;;;",,,,,,,,,,,,,,,,,,,,,
UJSON should be upgraded to 1.35,INDY-1445,31584,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,03/Jul/18 6:30 PM,03/Jul/18 6:30 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Version 1.33 which is currently used doesn't handle big 64-bit unsigned integers which are used for reqId, in some cases this leads to node not accepting client requests:
{code}
2018-07-02 12:16:35,397 | ERROR    | zstack.py            ( 551) | processReceived | Error Value is too big while converting message {""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""address"":""DB3eBYTCr9NvNVZNp1GwV12iDfqftoGrDKqBedRZV4SgdeTbi"",""type"":""10002""},""protocolVersion"":2,""reqId"":14396229700369999030,""signature"":""3BTLyQ1jLk9dBM55KLTjyhLHzpDbHTDZgQdqi8xmrWKneArpCFT5n35quaQQvB5aCurRkFmDcjRLqEsUezve2E4g""} to JSON from b'Le!dm0<>264oWdqNZR+PYnlZx)nUsQs#D*O}2oT='
{code}

This issue is fixed in ujson 1.35, so we should upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzh87:",,,,,,,,,,,,,,,,,,,,,,,,,,KitHat,sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
indy-node service stop/start can freeze node work,INDY-1446,31622,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,zhigunenko.dsr,zhigunenko.dsr,04/Jul/18 12:38 AM,09/Oct/19 6:12 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*Environment:*
indy-node 1.4.488
docker pool with 4 nodes

*Possible steps to validate:*
Stop and start service on certain node few times with medium intervals (about few minutes)
Additional load isn't expected

*Possible results:*
Service cannot stop via systemctl (command is not respond)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzhfr:",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrade failed on pool from 1.3.62 to 1.4.66,INDY-1447,31628,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,VladimirWork,krw910,krw910,04/Jul/18 3:10 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,"I have 7 nodes spread globally.
I had upgraded them from 1.3.57 to 1.3.62 successfully.
Some nodes have an upgrade.log file showing the successful upgrade, but most do not.

Issue
I upgraded yesterday from 1.3.62 to 1.4.66
I sent the upgrade txn at noon and scheduled the upgrade for 1:30pm

Schedule
{code}
ledger pool-upgrade name=upgrade1366 version=1.4.66 action=start sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 schedule={""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00""} timeout=15 force=true
{code}

I checked on the upgrade the next day and found 3 of the 4 nodes successfully upgraded. The other 4 nodes did not and do not have anything in the journalctl file showing they even tried to perform an upgrade.
I could not see any errors in any logs and most don't have an upgrade.log file even from the last upgrade.

The node log files show a view change was being requested before and after the upgrade.

I am attaching the node logs.
The machines are located in AWS and are listed in the [AWS QA Group - Regional Usage|https://docs.google.com/spreadsheets/d/1DrmN1Gv47P8UwTAFu6KjIyR2agIC2USJG5S9rCQVCwM/edit#gid=716840249]spreadsheet with evernym. Go to the tab ""Pool-QA RC""

The machines have the indy-node service shut off on all of them to be checked before attempting a manual upgrade.
To gain access talk with [~VladimirWork] or [~krw910] on how to login",,,,,,,,,,,,,,,,,,,,,,,,INDY-1498,,,,,,,,,,INDY-1508,,,,,,,,,,,,,,,"19/Jul/18 9:06 PM;VladimirWork;1447.tar.gz;https://jira.hyperledger.org/secure/attachment/15335/1447.tar.gz","20/Jul/18 9:31 PM;VladimirWork;1447_without_drop.tar.gz;https://jira.hyperledger.org/secure/attachment/15346/1447_without_drop.tar.gz","04/Jul/18 3:10 AM;krw910;Node1.log;https://jira.hyperledger.org/secure/attachment/15249/Node1.log","13/Jul/18 7:08 PM;spivachuk;Node1_control.log;https://jira.hyperledger.org/secure/attachment/15310/Node1_control.log","13/Jul/18 7:08 PM;spivachuk;Node1_upgrade_log;https://jira.hyperledger.org/secure/attachment/15303/Node1_upgrade_log","04/Jul/18 3:10 AM;krw910;Node2.log;https://jira.hyperledger.org/secure/attachment/15248/Node2.log","13/Jul/18 7:09 PM;spivachuk;Node2_control.log;https://jira.hyperledger.org/secure/attachment/15309/Node2_control.log","13/Jul/18 7:08 PM;spivachuk;Node2_upgrade_log;https://jira.hyperledger.org/secure/attachment/15302/Node2_upgrade_log","04/Jul/18 3:10 AM;krw910;Node3.log;https://jira.hyperledger.org/secure/attachment/15247/Node3.log","13/Jul/18 7:09 PM;spivachuk;Node3_control.log;https://jira.hyperledger.org/secure/attachment/15308/Node3_control.log","13/Jul/18 7:08 PM;spivachuk;Node3_upgrade_log;https://jira.hyperledger.org/secure/attachment/15301/Node3_upgrade_log","04/Jul/18 3:10 AM;krw910;Node4.log;https://jira.hyperledger.org/secure/attachment/15246/Node4.log","13/Jul/18 7:08 PM;spivachuk;Node4_control.log;https://jira.hyperledger.org/secure/attachment/15307/Node4_control.log","04/Jul/18 3:10 AM;krw910;Node5.log;https://jira.hyperledger.org/secure/attachment/15245/Node5.log","13/Jul/18 7:08 PM;spivachuk;Node5_control.log;https://jira.hyperledger.org/secure/attachment/15306/Node5_control.log","13/Jul/18 7:08 PM;spivachuk;Node5_upgrade_log;https://jira.hyperledger.org/secure/attachment/15300/Node5_upgrade_log","04/Jul/18 3:10 AM;krw910;Node6.log;https://jira.hyperledger.org/secure/attachment/15244/Node6.log","13/Jul/18 7:09 PM;spivachuk;Node6_control.log;https://jira.hyperledger.org/secure/attachment/15305/Node6_control.log","04/Jul/18 3:10 AM;krw910;Node7.log;https://jira.hyperledger.org/secure/attachment/15243/Node7.log","13/Jul/18 7:09 PM;spivachuk;Node7_control.log;https://jira.hyperledger.org/secure/attachment/15304/Node7_control.log","13/Jul/18 7:07 PM;spivachuk;config_ledger.txt;https://jira.hyperledger.org/secure/attachment/15299/config_ledger.txt","13/Jul/18 7:07 PM;spivachuk;pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/15298/pool_ledger.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzhon:",,,,,,EV 18.13 Benchmark hardening,EV 18.14 Monitoring/Stability,EV 18.15 Stability/Availabilit,,,,,,,,,,,,,,,,,,ashcherbakov,krw910,spivachuk,VladimirWork,,,,,,,,,"10/Jul/18 1:17 AM;VladimirWork;Config ledgers on all nodes match, there are the last entries about 1.4.66 upgrade:
{noformat}
nva-stn-q001.qa.evernym.com
27 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""V4SGRU86Z58d6TV7PBUe6f"",""value"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB""}]},""txn"":{""data"":{""action"":""start"",""force"":true,""name"":""upgrade1366"",""reinstall"":false,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""timeout"":15,""version"":""1.4.66""},""metadata"":{""digest"":""f5c626cd1e6ee57986c17345a06430e8b4939ccdf1a60c5f92d525dc03209cc5"",""from"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1530556300625867000},""type"":""109""},""txnMetadata"":{""seqNo"":27,""txnTime"":1530556303},""ver"":""1""}
28 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""value"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""9e99995de7c6180a0fec7ebc3e283927571171cc1dd1bf5a1955f22a4a2928b0"",""from"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709},""type"":""110""},""txnMetadata"":{""seqNo"":28,""txnTime"":1530559800},""ver"":""1""}
29 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""value"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""63b2a83f5b2a94151aa7f7bb60b808f63b0f60ec61409017a5d03f51d1f329fb"",""from"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900},""type"":""110""},""txnMetadata"":{""seqNo"":29,""txnTime"":1530559800},""ver"":""1""}
30 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""value"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""b715ce1d67552dce1a1c8999fc1530a73108933b205e529bcfd3620625c6bf7a"",""from"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887},""type"":""110""},""txnMetadata"":{""seqNo"":30,""txnTime"":1530559800},""ver"":""1""}

sgp-stn-q001.qa.evernym.com
[27,{""action"":""start"",""force"":true,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""name"":""upgrade1366"",""reinstall"":false,""reqId"":1530556300625867000,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""signature"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB"",""signatures"":null,""timeout"":15,""txnTime"":1530556303,""type"":""109"",""version"":""1.4.66""}]
[28,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709,""signature"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[29,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900,""signature"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[30,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887,""signature"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]

syd-stn-q001.qa.evernym.com
[27,{""action"":""start"",""force"":true,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""name"":""upgrade1366"",""reinstall"":false,""reqId"":1530556300625867000,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""signature"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB"",""signatures"":null,""timeout"":15,""txnTime"":1530556303,""type"":""109"",""version"":""1.4.66""}]
[28,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709,""signature"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[29,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900,""signature"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[30,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887,""signature"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]

tky-stn-q001.qa.evernym.com
27 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""V4SGRU86Z58d6TV7PBUe6f"",""value"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB""}]},""txn"":{""data"":{""action"":""start"",""force"":true,""name"":""upgrade1366"",""reinstall"":false,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""timeout"":15,""version"":""1.4.66""},""metadata"":{""digest"":""f5c626cd1e6ee57986c17345a06430e8b4939ccdf1a60c5f92d525dc03209cc5"",""from"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1530556300625867000},""type"":""109""},""txnMetadata"":{""seqNo"":27,""txnTime"":1530556303},""ver"":""1""}
28 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""value"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""9e99995de7c6180a0fec7ebc3e283927571171cc1dd1bf5a1955f22a4a2928b0"",""from"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709},""type"":""110""},""txnMetadata"":{""seqNo"":28,""txnTime"":1530559800},""ver"":""1""}
29 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""value"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""63b2a83f5b2a94151aa7f7bb60b808f63b0f60ec61409017a5d03f51d1f329fb"",""from"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900},""type"":""110""},""txnMetadata"":{""seqNo"":29,""txnTime"":1530559800},""ver"":""1""}
30 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""value"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""b715ce1d67552dce1a1c8999fc1530a73108933b205e529bcfd3620625c6bf7a"",""from"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887},""type"":""110""},""txnMetadata"":{""seqNo"":30,""txnTime"":1530559800},""ver"":""1""}

can-stn-q001.qa.evernym.com
27 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""V4SGRU86Z58d6TV7PBUe6f"",""value"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB""}]},""txn"":{""data"":{""action"":""start"",""force"":true,""name"":""upgrade1366"",""reinstall"":false,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""timeout"":15,""version"":""1.4.66""},""metadata"":{""digest"":""f5c626cd1e6ee57986c17345a06430e8b4939ccdf1a60c5f92d525dc03209cc5"",""from"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":1530556300625867000},""type"":""109""},""txnMetadata"":{""seqNo"":27,""txnTime"":1530556303},""ver"":""1""}
28 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""value"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""9e99995de7c6180a0fec7ebc3e283927571171cc1dd1bf5a1955f22a4a2928b0"",""from"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709},""type"":""110""},""txnMetadata"":{""seqNo"":28,""txnTime"":1530559800},""ver"":""1""}
29 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""value"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""63b2a83f5b2a94151aa7f7bb60b808f63b0f60ec61409017a5d03f51d1f329fb"",""from"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900},""type"":""110""},""txnMetadata"":{""seqNo"":29,""txnTime"":1530559800},""ver"":""1""}
30 {""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""value"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb""}]},""txn"":{""data"":{""data"":{""action"":""in_progress"",""version"":""1.4.66""}},""metadata"":{""digest"":""b715ce1d67552dce1a1c8999fc1530a73108933b205e529bcfd3620625c6bf7a"",""from"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887},""type"":""110""},""txnMetadata"":{""seqNo"":30,""txnTime"":1530559800},""ver"":""1""}

irl-stn-q001.qa.evernym.com
[27,{""action"":""start"",""force"":true,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""name"":""upgrade1366"",""reinstall"":false,""reqId"":1530556300625867000,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""signature"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB"",""signatures"":null,""timeout"":15,""txnTime"":1530556303,""type"":""109"",""version"":""1.4.66""}]
[28,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709,""signature"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[29,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900,""signature"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[30,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887,""signature"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]

sao-stn-q001.qa.evernym.com
[27,{""action"":""start"",""force"":true,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""name"":""upgrade1366"",""reinstall"":false,""reqId"":1530556300625867000,""schedule"":{""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-07-02T13:30:00.258870-06:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-07-02T13:30:00.258870-06:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-07-02T13:30:00.258870-06:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-07-02T13:30:00.258870-06:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-07-02T13:30:00.258870-06:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-07-02T13:30:00.258870-06:00"",""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-07-02T13:30:00.258870-06:00""},""sha256"":""e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"",""signature"":""5UAut2SQaZUBGw3frT3vx92LmX9tx1uF2JkDSknEhuK1xqEyYwntXvU6o4xKcdqPHwXHxGjnRQABQQbCBRYhK3AB"",""signatures"":null,""timeout"":15,""txnTime"":1530556303,""type"":""109"",""version"":""1.4.66""}]
[28,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"",""reqId"":1530559800266709,""signature"":""2ZAFnAK3sqTizTqowrqVanggk2976KeopzCmAEPCZS4TF3tCx59tCwVx5eRFYLUHBpgEJ4KQg3V8adj2HZFNDoCi"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[29,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"",""reqId"":1530559800261900,""signature"":""2gMqtyeK41DiGfL8YTFxBG33yu3h3pJsEXmkahyQknEXY5D9idW3tHHAgJzyWrPpQYhP5qR2k3ArfomvQng7vWUG"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
[30,{""data"":{""action"":""in_progress"",""version"":""1.4.66""},""identifier"":""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"",""reqId"":1530559799531887,""signature"":""3M6p1uWu3tTY2svfpLbGH42RmqwTGywXohy3uz1iTU4GK5nndEMALvi5hRqfavgBtx9HdTq8vMzUp6w5sdNLdekb"",""signatures"":null,""txnTime"":1530559800,""type"":""110""}]
{noformat}
;;;","12/Jul/18 7:02 PM;spivachuk;*Node1*, *Node2*, *Node3* and *Node5* are currently on the version *1.3.62*. So they definitely succeeded with the upgrade to *1.3.62*. Also these 4 nodes are the only ones which have upgrade logs now. The upgrade logs on all of them also indicate the successful upgrade to *1.3.62*. For all the nodes in the pool there are {{NODE_UPGRADE action=in_progress}} transactions for this version in the config ledger. However, only for *Node1*, *Node2*, *Node3* {color:#d04437}but not for{color} *Node5* and for no one else there are {{NODE_UPGRADE action=complete}} transactions for this version.

The rest of the nodes - *Node4*, *Node6* and *Node7* are currently on the version *1.4.66*. So they succeeded with the upgrade to *1.4.66*. For all of them there are {{NODE_UPGRADE action=in_progress}} transactions for this version in the config ledger. But there are no {{NODE_UPGRADE action=complete}} transactions for this version for any of these nodes. Also none of these nodes have upgrade logs now.;;;","13/Jul/18 7:11 PM;spivachuk;Attached the pool ledger, the config ledger, the upgrade logs for those nodes which have them now and {{indy-node-control}} logs for all the nodes.;;;","13/Jul/18 7:32 PM;spivachuk;Most likely *Node5* was upgraded to *1.3.62* *manually* because its {{indy-node-control}} log reported by {{journalctl}} is for the period from 2018-03-31 to 2018-07-12 and they do not contain messages about any performed upgrades (the automated upgrade to *1.3.62* was scheduled for 2018-05-30).

On all the other nodes in the pool {{indy-node-control}} logs reported by {{journalctl}} are for the period from 2018-06-12 to 2018-07-12. On *Node4*, *Node6* and *Node7* these logs indicate the successful upgrade to *1.4.66* (it was scheduled for 2018-07-02).;;;","14/Jul/18 2:30 AM;spivachuk;According to {{indy-node}} logs *Node1*, *Node3* and *Node5* received the request {{POOL_UPGRADE version=1.4.66 action=start force=true}} _directly_ from the client _after_ they had already added the corresponding transaction to the ledger (the latter was possible due to they however had received these requests via {{PROPAGATEs}}). Since nodes handle forced requests _only when_ receive them _directly_ from clients, so *Node1*, *Node3* and *Node5* attempted to handle this forced {{POOL_UPGRADE}} request _after_ they had already added the corresponding transaction to the ledger. Due to this, validation of this request on these nodes prevented its further handling because the validation treated this upgrade as already scheduled.

As to *Node2*, according to its {{indy-node}} log, it {color:#d04437}did not receive{color} the {{POOL_UPGRADE}} request _directly_ from the client. So it did not even attempt to handle this forced {{POOL_UPGRADE}} request.

As to *Node4*, *Node6* and *Node7*, after the completed upgrade to *1.4.66* and restart they were unable to complete propagate primary view change because they were unable to complete catch-up since they did not gather the quorum of {{LedgerStatuses}}. They did not gather the quorum because they discarded {{LedgerStatuses}} from the 4 not upgraded nodes due to a changed message format (missed {{protocolVersion}} field). Due to *Node4*, *Node6* and *Node7* did not completed catch-up, they did not send {{NODE_UPGRADE action=complete}} requests for the completed upgrade.;;;","14/Jul/18 2:46 AM;spivachuk;*PoA:*

Handle a forced request not when just receiving it directly from a client but when receiving it for the first time either directly from a client or in {{PROPAGATE}} from another node.;;;","18/Jul/18 8:02 PM;spivachuk;*Problem reason:*
 - The node handled forced requests only when receiving them directly from clients but not when receiving them in {{PROPAGATEs}} from other nodes.

*Changes:*
 - Added tests verifying different cases of handling of {{POOL_UPGRADE force=true}} request.
 - Fixed a bug with a wrong workflow of handling forced requests.
 - Combined {{processRequest}} methods of {{Node}} classes in plenum and indy-node into one method in plenum.
 - Divided {{Node.getReplyFromLedger}} method into two ones for usage simplification.
 - Corrected the type of values being retrieved from {{ReqIdrToTxn}} storage.
 - Renamed {{handleActionTxn}} methods in {{Upgrader}} and {{Restarter}} classes using different names since they do not use common interface.

*PRs:*
 - [https://github.com/hyperledger/indy-plenum/pull/819]
 - [https://github.com/hyperledger/indy-plenum/pull/822]
 - [https://github.com/hyperledger/indy-node/pull/822]
 - [https://github.com/hyperledger/indy-node/pull/825]

*Version:*
 - indy-node 1.4.510-master
 - indy-plenum 1.4.462-master

*Risk factors:*
 - Nothing is expected.

*Risk:*
 - Low

*Covered with tests:*
 - {{test_node_handles_forced_upgrade_on_client_request}}
 - {{test_node_handles_forced_upgrade_on_propagate}}
 - {{test_forced_upgrade_no_consensus_on_single_node}}
 - {{test_pool_upgrade_force_scheduled_only_once}}
 - {{test_forced_upgrade_handled_once_if_request_received_after_propagate}}
 - {{test_forced_upgrade_handled_once_if_ordered_and_then_request_received}};;;","19/Jul/18 9:06 PM;VladimirWork;Build Info:
indy-node 1.4.510 -> 1.5.511

Steps to Reproduce:
1. Install pool of 4 nodes.
2. Slow down 1st node by traffic_shaping script.
3. Send upgrade command with force=True, same time for all nodes in future.
4. Check nodes' versions and logs.

Actual Results:
All nodes except 1st upgraded successfully. 1st node didn't upgrade but there are entries in upgrade log about upgrade scheduling and starting (but there are no entries about upgrade in journalctl on this node). [^1447.tar.gz] 

Additional Info:
Node1:
{noformat}
root@edf7725b7bc4:/var/lib/indy/sandbox/data/Node1# cat upgrade_log 
2018-07-19 11:24:31.572324	scheduled	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
2018-07-19 11:25:00.282208	started	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
2018-07-19 11:35:00.351796	scheduled	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
2018-07-19 11:35:00.352301	started	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
{noformat}

Node2, 3, 4:
{noformat}
root@be8e95711cef:/var/lib/indy/sandbox/data/Node2# cat upgrade_log 
2018-07-19 11:24:31.591067	scheduled	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
2018-07-19 11:25:00.260194	started	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
2018-07-19 11:25:48.330038	succeeded	2018-07-19 11:25:00.258870+00:00	1.5.511	1531999471535575
{noformat}
;;;","20/Jul/18 1:03 AM;spivachuk;Due to the logs from the previous message, on *Node1* {{indy-node}} service sent an upgrade message to {{indy-node-control}} service but, however, after this the upgrade was not performed. After 10 minutes timeout {{indy-node}} service reported a message about an exceeded timeout for the upgrade to its log and sent an upgrade message to {{indy-node-control}} service once again. In 1 minute after this {{indy-node}} service was stopped by an external signal.

As I can suggest, {{indy-node-control}} did not receive the first upgrade message due to {{traffic_shaping}} influence, then, however, received the second upgrade message, but after that failed to download the new version packages due to {{traffic_shaping}} influence too and so failed to complete the upgrade.

I recommend to re-test the scenario with disabled dropping of packets in {{traffic_shaping}}.;;;","20/Jul/18 9:30 PM;VladimirWork;All logs produced by the same steps but without package dropping. [^1447_without_drop.tar.gz] ;;;","23/Jul/18 7:56 PM;spivachuk;The upgrade was scheduled for 07/20 11:20. *Node1* started the upgrade but failed to complete it at 11:30:33 because 300 seconds timeout for {{apt update}} command was exceeded. Before the moment of {{apt update}} timeout was exceeded, {{indy-node}} service had re-sent an upgrade message to {{indy-node-control}} service at 11:30:00 because {{Upgrader}}'s 10 minute timeout for upgrade was exceeded.

*Node2*, *Node3* and *Node4* succeeded with the upgrade.

However, in the upgrade logs of all the nodes in the pool there are multiple pairs of {{scheduled}} and {{started}} messages. This was so because the nodes were made multiple catch-ups at that period and so re-handled the {{POOL_UPGRADE}} transaction multiple times in {{Node.postConfigLedgerCaughtUp}} calls since the version from the transaction was being detected as not yet installed. The nodes were made repeated catch-ups in scope of view changes due to 2 reasons: 1) no quorum of {{ViewChangeDone}} messages and 2) having a quorum of {{ViewChangeDone}} messages but being behind the accepted state.;;;",,,,,,,,,,,,,,
Reveal the cause of write stopping on 1100k ledger,INDY-1448,31644,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,anikitinDSR,zhigunenko.dsr,zhigunenko.dsr,04/Jul/18 4:09 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Pool stops writing during 12-hour load session
*Load config:*
{noformat}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 57 -t 20 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{noformat}
 x 4 instances
{noformat}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 10 -m t -c 75 -t 2 -n 4 -k ""{\""get_nym\"": {\""file_name\"": \""./load_test_1/successful\""}}""
{noformat}
 x 2 instances

Ledger size are the same on all the nodes. There is no view_change in progress

Logs are available on LogProcessor:
journalctl.nodeX
last_10k_txns.nodeX
validator-info.nodeX
and basic Node logs","indy-node 1.4.483
indy-plenum 1.4.423",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,INDY-1343,INDY-2214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzhjz:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,zhigunenko.dsr,,,,,,,,,,,"04/Jul/18 11:17 PM;ashcherbakov;There are no space left on device messages on all nodes (RocksDB no space error is only on 8 (f) nodes).;;;",,,,,,,,,,,,,,,,,,,,,,,,
Nsreplay tool throws error on run,INDY-1449,31647,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,04/Jul/18 5:28 PM,09/Oct/18 7:00 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Build Info:
indy-node 1.4.487

Steps to Reproduce:
1. Install the pool.
2. Set `STACK_COMPANION = 1` in `/etc/indy/plenum_config.py` on all nodes.
3. Restart indy-node service on all nodes.
4. Run some cases/experiments against this pool.
5. Stop indy-node service on any node.
6. Run `nscapture` or `sudo nscapture` to get archive.
7. Run `nsreplay` or `sudo nsreplay` in the same folder to replay archive.

Actual Results:
{noformat}
Traceback (most recent call last):
  File ""/usr/local/bin/nsreplay"", line 36, in <module>
    from indy_node.server.config_helper import create_config_dirs
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/config_helper.py"", line 3, in <module>
    from indy_node.general_config import general_config
ImportError: cannot import name 'general_config'
{noformat}

Expected Results:
Captured node state should be replayed without any errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jul/18 10:34 PM;VladimirWork;Node4.sandbox.20180703153342.tar.gz;https://jira.hyperledger.org/secure/attachment/15267/Node4.sandbox.20180703153342.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzhkn:",,,,,,,,,,,,,,,,,,,,,,,,,,anikitinDSR,devin-fisher,VladimirWork,,,,,,,,,,"04/Jul/18 5:28 PM;VladimirWork;FYI [~ckochenower] [~devin-fisher];;;","12/Jul/18 1:52 AM;devin-fisher;@vladimir.shishkin Looking at the ticket, it looks like you are running nsreplay in an environment where indy-node and indy-plenum is not available. nsreplay must be run in an environment with indy-node and indy-plenum. Typically, nsreplay would be run an development environment for indy-node.;;;","18/Jul/18 11:44 PM;anikitinDSR;[~devin-fisher] what do you mean in concept of ""nsreplay would be run an development environment""? What the ""development environment"" do you mean?
Is it just like `pip3 install -e .` inside virtualenv into indy-node's source code? ;;;","20/Jul/18 1:02 AM;devin-fisher;I understand the problem better. I'll be fixing this now.;;;",,,,,,,,,,,,,,,,,,,,,
Catchup needs to be finished during high load,INDY-1450,31649,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,04/Jul/18 6:06 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"As was found in INDY-1404, if a lot of catchup-related messages (LEDGER_STATUS, CONSISTENCY_PROOFS) are lost, the node doesn't try to start catchup again.
 The messages can be lost due to high load (as in the test), and limited size of node-to-node's message queue.

The recent fixes regarding catchup and fixes done in the scope of INDY-1404 made the problem more visible, since we require that the previous round of catchup is finished before starting the next round.

 

*Acceptance criteria:*
 * The following tests need to be written:
 ** Skip sending of Message Responses to LEDGER STATUS request once for domain ledger to a node doing catchup; make sure that the node eventually finishes catchup
 ** Skip sending of CONSISTENCY_PROOFs for domain ledger once to a node doing catchup; make sure that the node eventually finishes catchup
 * A Node must finish catchup regardless of the load in the pool
 * Catchup can be finished with some delay (in a timeout),",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1404,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzzhpb:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,5.0,,,,,,,,,,,,ashcherbakov,Toktar,VladimirWork,,,,,,,,,,"09/Jul/18 10:43 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/800;;;","12/Jul/18 3:19 PM;Toktar;*Problem reason:*
 * If the node does not have the quorum of ledger statuses or quorum of f + 1 of the first consistentcy proofs, it stops in the catchup and never completes.

*Changes:*
 * Send message request with ledger status if node did not recieve ledger status in some time.
 * Send ledger status for request consistency proof of last ordering if node have not f+1 consistensy proofs

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/800]

*Version:*
 * indy-node 1.3.499 -master
 * indy-plenum 1.4.446 -master

*Risk factors:*
 * Problems with catchup

*Risk:*
 * Low

*Covered with tests:*
 * [test_node_catchup_with_connection_problem.py|https://github.com/hyperledger/indy-plenum/pull/800/files#diff-8bc091580aee91c53dee265fc1fa708d]
 ** test_catchup_with_lost_ledger_status
 ** test_catchup_with_lost_first_consistency_proofs
 ** test_cancel_request_cp_and_ls_after_catchup

*Recommendations for QA:*

Test catchup under load as for task https://jira.hyperledger.org/browse/INDY-1404;;;","13/Jul/18 12:17 AM;VladimirWork;Build Info:
indy-node 1.4.500

Steps to Validate:
1. Install pool of 4 nodes.
2. Run load test for ~5k NYMs.
3. Add 5th node.
4. Run load test for ~5k NYMs.
5. Restart 1st node to change the primary (since there were no view changes at this moment).
6. Stop 1st node.
7. Run load test for ~25k NYMs.
8. Add 6th node and start 1st node simultaneously to check their catchup under load.

Actual Results:
Both nodes finished catch up successfully (checked against docker and AWS environment).;;;",,,,,,,,,,,,,,,,,,,,,,
Deprecate Vagrant Test Network Method on Indy-node,INDY-1451,31659,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,RyanWest,RyanWest,RyanWest,05/Jul/18 4:31 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"To install and run a test network of nodes on one's computer for development purposes, there are several methods available to use. However, lately only Docker has been supported, and Vagrant has stopped working. Several new, interested developers have been caught trying to use Vagrant and failing because they should be following the more-updated Docker instructions instead.

To fix this problem, Vagrant needs to be marked as deprecated in various places in indy-node's Github repository.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzwwav:",,,,,,,,,,,,,,,,,,,,,,,,,,RyanWest,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
A node must send LEDGER_STATUS with correct last ordered 3PC after catch-up,INDY-1452,31664,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,05/Jul/18 7:23 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"There is a structure (IntervalTree) in node that can map seqNo in ledger to 3PC key.

This is in-memory structure, so it's empty after node restart, and updated only during common ordering and execution of 3PC batches.

We need to have an entry for the current ledger size in this map after catchup too, as this is used in LEDGER_STATUSes sent to others, and LEDGER_STATUSes are used for initial calcualtion of `last_ordered-3PC`.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1377,,,,,,,,,"1|hzzhtb:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"06/Jul/18 10:20 PM;ashcherbakov;Changes: 
- added an entry to interval tree for each ledger after the ledger is caught up. The key is ledger size.

PR:
- https://github.com/hyperledger/indy-plenum/pull/795

Version:
- master Indy Node 1.4.494

Risk factors:
- Catchup
- View Change
- Node initialization
- Node restart
- New node adding
- Recovering from f+1 nodes

Risk:
- Low

Covered with tests:
- test_build_ledger_status.py
- test_update_txn_seq_range_after_catchup.py

Recommendations for QA
- Run acceptance for adding of a new node
- Run acceptance against cases with restart of nodes 
- Run acceptance against cases with recovering from f+1 nodes
- Test catch-up
- Run load test with view changes
;;;","10/Jul/18 9:50 PM;ozheregelya;*Environment:*
 indy-node 1.4.494
 libindy/indy-cli 1.5.0
 AWS acceptance pool with enabled recording.

*Steps to Validate:*
 1. Perform regression of following cases:
 - Run acceptance for adding of a new node (/)
 - Run acceptance against cases with restart of nodes (/)
 - Run acceptance against cases with recovering from f+1 nodes (/)
 - Test catch-up (/)
 - Run load test with view changes (!)

*Actual Results:*
Load test with view changes was verified on small pool (6-nodes). It will be verified on standard (25-nodes) pools in scope of confirmation testing of INDY-1453.
The rest cases work normally.;;;",,,,,,,,,,,,,,,,,,,,,,,
Do not process any client requests during view change,INDY-1453,31665,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,05/Jul/18 7:31 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"Do not process any client requests during catch-up, just keep them in zstack internal queue.

If they reach the limit (5000), they will be silently dropped (we may consider sending some graceful Reject in future).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhtj:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"06/Jul/18 10:22 PM;ashcherbakov;Changes: 
- so not process client queue during view change

PR:
- https://github.com/hyperledger/indy-plenum/pull/796

Version:
- master Indy Node 1.4.494

Risk factors:
- View Change

Risk:
- Low

Covered with tests:
- test_client_req_during_view_change.py

Recommendations for QA
- Run load test with view changes
;;;","11/Jul/18 6:12 PM;VladimirWork;Build Info:
indy-node 1.4.495

Steps to Validate:
1. Install pool of 22 nodes.
2. Set forced view change frequency to 1800 seconds (every 30 minutes).
3. Run load test writing 10 NYMs/second for ~24 hours.

Actual Results:
Pool writes ~500k txns without any view change issues (all nodes are in consensus and have the same amount of txn written after load stopping).;;;",,,,,,,,,,,,,,,,,,,,,,,
Pool has stopped working due to several incomplete view changes,INDY-1454,31667,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,sergey.khoroshavin,VladimirWork,VladimirWork,05/Jul/18 7:38 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"Build Info:
indy-node 1.4.492

Steps to Reproduce:
1. Install pool of 22 nodes.
2. Run load test `python3 perf_processes.py -n 1 -t 1 -c 10 -g pool_transactions_genesis`.

Actual Results:
Load script has stopped writing at 71760 txns in ledger because of `_indy_loop_callback: Function returned error 307` (PoolLedgerTimeout error).

There are the last entries about incomplete view changes in logs:
{noformat}
2018-07-05 08:29:09,498 | INFO     | view_changer.py      ( 448) | sendInstanceChange | VIEW CHANGE: Node22 sending an instance change with view_no 203 since View change could not complete in time
2018-07-05 08:29:10,041 | INFO     | view_changer.py      ( 485) | do_view_change_if_possible | VIEW CHANGE: Node22 initiating a view change to 203 from 202
2018-07-05 08:29:10,044 | INFO     | node.py              ( 570) | on_view_change_start | VIEW CHANGE: Node22 changed to view 203, will start catchup now
--------------------
2018-07-05 08:34:09,939 | INFO     | view_changer.py      ( 448) | sendInstanceChange | VIEW CHANGE: Node22 sending an instance change with view_no 204 since View change could not complete in time
2018-07-05 08:34:10,616 | INFO     | view_changer.py      ( 485) | do_view_change_if_possible | VIEW CHANGE: Node22 initiating a view change to 204 from 203
2018-07-05 08:34:10,637 | INFO     | node.py              ( 570) | on_view_change_start | VIEW CHANGE: Node22 changed to view 204, will start catchup now
{noformat}

Additional Information:
Pool works after all nodes restarting.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhtr:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,sergey.khoroshavin,VladimirWork,,,,,,,,,,,"07/Jul/18 3:21 AM;sergey.khoroshavin;Part of problem may be related to interference between normal and forced view change. Log investigation will be continued, but at the same time it is recommended to rerun load test with
{code}
ForceViewChangeFreq=1800
{code}
and see if this problem persists.;;;","10/Jul/18 7:04 PM;VladimirWork;The issue doesn't reproduce against the same version pool against the same load with `ForceViewChangeFreq=1800`.;;;","11/Jul/18 7:23 PM;sergey.khoroshavin;There were two issues found:
1) when starting next view change view change timeout was not reset, which led to situation when two consecutive view changes (each taking 5 minutes, so 10 minutes total) triggered another view change, which took another 5 minutes and since already 15 minutes passed another forced view change was triggered. Quick fix was to increase forced view change period, proper fix is to reset view change timeout each time new view change starts. PR with fix: https://github.com/hyperledger/indy-plenum/pull/802
2) there were situations where most nodes finished view change very fast and started ordering transactions, and some nodes were slow enough to interleave their catchup during view change with ordering of new transactions by other nodes. This led to never ending catchup during view change (as well as never ending view change) because on every catchup round they were receiving new transactions, and catchup logic during view change was to wait until there are no new transactions caught up. PR with fix: https://github.com/hyperledger/indy-plenum/pull/804

Validation will done in scope of [INDY-1343|https://jira.hyperledger.org/browse/INDY-1343] and [INDY-1464|https://jira.hyperledger.org/browse/INDY-1464];;;",,,,,,,,,,,,,,,,,,,,,,
Pool cannot order transactions because Node set incorrect watermarks after it's restart,INDY-1455,31668,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,spivachuk,zhigunenko.dsr,zhigunenko.dsr,05/Jul/18 8:08 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"*Step to reproduce:*
 1) setup pool with 25 nodes on AWS
 2) set 1,06kk txns to ledger
 3) run concurrent load test:
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 286 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_schema\"": {\""file_name\"": \""./load_schema/successful\""}}""
{code}
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_attrib\"": {\""file_name\"": \""./load_attrib/successful\""}}""
{code}
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_nym\"": {\""file_name\"": \""./load_nym/successful\""}}""
{code}
*Actual results:*
{code:java}
2018-07-05 07:16:58,838 | INFO     | replica.py           ( 884) | dispatchThreePhaseMsg | REPLICA:(Node18:5) Node18:5 stashing 3 phase message PREPREPARE{'discarded': 1, 'digest': '28eb27c43db6b7472b72e9fa547f7fc7e7f1eb8e6d4fc521c225a98c5fc3ca91', 'reqIdr': ['2f051e9054ea521e0acce5949839e3a3f174ba6bc063974b6fb796f04564dc34'], 'ppSeqNo': 21116, 'instId': 5, 'ppTime': 1530775018, 'ledgerId': 1, 'txnRootHash': None, 'stateRootHash': None, 'viewNo': 3} since ppSeqNo 21116 is not between 0 and 300)
{code}
*Workaround:*
 Restart whole pool via CLI

Logs are available on LogProcessor",indy-node 1.4.490,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzhtz:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,spivachuk,zhigunenko.dsr,,,,,,,,,,,"11/Jul/18 8:16 PM;spivachuk;Found a problem with narrowing watermarks on backup replicas from {{(0, sys.maxsize)}} to {{(0, LOG_SIZE)}} on primary propagation completion. This breaks ordering on a backup replica if {{pp_seq_no}} of the next 3PC-batch being processing is greater than {{LOG_SIZE}} (by default is {{300}}). INDY-1462 has been logged about this problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Remove restrictions on DID endpoint format,INDY-1456,31686,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,swcurran,swcurran,06/Jul/18 5:37 AM,06/Jul/18 5:49 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"When submitting an endpoint using the ""build_attrib_request"" with a URL as the endpoint (e.g. ""https://www.google.com""), the following is returned:

{quote}client request invalid: InvalidClientRequest('validation error [ClientAttribOperation]: invalid endpoint address (ha=https:\\/\\/google.com)
{quote}

It appears that there is a format check in Indy-Node to require the endpoint be in the form <ip>:<port>

As agents are being built, this is becoming a higher priority issue.  DID endpoints for Agents cannot be limited to <ip>:<port> - they can be any sort of URI format, possibly even including a DID.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzhwn:",,,,,,,,,,,,,,,,,,,,,,,,,,swcurran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Propagate Primary mode should not be set for already started view change,INDY-1458,31697,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,06/Jul/18 5:48 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"If there is a view change in progress from viewNo=X to viewNo > X+1, that is consisting of multiple rounds, then no nodes should start it in propagate primary mode (on processing future view change done messages). Otherwise these nodes will not send ViewChangeDone, and will have incorrect quorum of VCD (f+1 instead of n-f).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhyn:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,2.0,,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"06/Jul/18 10:25 PM;ashcherbakov;Changes: 
- added an entry to interval tree for each ledger after the ledger is caught up. The key is ledger size.

PR:
- https://github.com/hyperledger/indy-plenum/pull/798

Version:
- master Indy Node 1.4.494

Risk factors:
- View Change
- Node initialization
- Node restart
- New node adding
- Recovering from f+1 nodes

Risk:
- Med

Covered with tests:
- test_no_future_view_change_while_view_change.py

Recommendations for QA:
- Run acceptance for adding of a new node
- Run acceptance against cases with restart of nodes 
- Run acceptance against cases with recovering from f+1 nodes
- Run load test with view changes
;;;","10/Jul/18 9:50 PM;ozheregelya;*Environment:*
indy-node 1.4.494
libindy/indy-cli 1.5.0
AWS acceptance pool with enabled recording.

*Steps to Validate:*
1. Perform regression of following cases:
 - Run acceptance for adding of a new node (/)
 - Run acceptance against cases with restart of nodes (/)
 - Run acceptance against cases with recovering from f+1 nodes (/)
 - Test catch-up (/)
 - Run load test with view changes (!)

*Actual Results:*
Load test with view changes was verified on small pool (6-nodes). It will be verified on standard (25-nodes) pools in scope of confirmation testing of INDY-1453.
The rest cases work normally.;;;",,,,,,,,,,,,,,,,,,,,,,,
First Pre-Prepare message has incorrect state trie root right after view_change (on master replica),INDY-1459,31699,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,zhigunenko.dsr,zhigunenko.dsr,06/Jul/18 7:36 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"*Steps to reproduce:*
 1) Restart pool via CLI
 2) send transactions via perf_processes.py
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 2 -n 1 -k nym
{code}
*Actulal results:*
 6 of 25 nodes stopped writing

 Logs will be available on LogProcessor",indy-node 1.4.490,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhz3:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,anikitinDSR,zhigunenko.dsr,,,,,,,,,,,"10/Jul/18 9:19 PM;anikitinDSR;Log analyzing showed that:
 * All of 6 stopped writing node failed by ""Out of memory error"". Kernel will kill the python application unexpectedly and already ordered request can be written in ledger's storage but not in state storage. Therefore, node hasn't consistency storages after restart. Possible way for solving this is to remove state's storages on all nodes. In this case, node will restore state from transaction_log.
 * ""Out of memory"" error should be fixed in the scope of INDY-1455 and [INDY-1462|https://jira.hyperledger.org/browse/INDY-1462];;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool stopped writing after 1114k txns (different view_no),INDY-1460,31700,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,dsurnin,zhigunenko.dsr,zhigunenko.dsr,06/Jul/18 9:47 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"*Steps to reproduce:*
Run concurrent load test:
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 286 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 57 -n 1 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_schema\"": {\""file_name\"": \""./load_schema/successful\""}}""
{code}{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_attrib\"": {\""file_name\"": \""./load_attrib/successful\""}}""
{code}{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 143 -t 4 -n 1 -k ""{\""get_nym\"": {\""file_name\"": \""./load_nym/successful\""}}""
{code}

*Actual results:*
After 2 hours pool stopped to write new transactions

*Additional info:*
Pool start to write after pool-restart via CLI
Logs are available here:
Logprcessor: ~/logs/view_change_problem_05jun2018/",indy-node 1.4.490,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,,,,,,,,,,,,,,,,"13/Jul/18 10:12 PM;dsurnin;Node10_status_proof_filter;https://jira.hyperledger.org/secure/attachment/15314/Node10_status_proof_filter",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhl3:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,ashcherbakov,dsurnin,zhigunenko.dsr,,,,,,,,,,"07/Jul/18 12:33 AM;ashcherbakov;Some nodes fail with nos space left on device again:
jctl.node10:Jul 05 16:15:10 seoulQALive.qatest.evernym.com env[19635]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node10/domain_state/000782.sst: No space left on device'
jctl.node11:Jul 05 16:15:08 seoulQALive.qatest.evernym.com env[13817]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node11/domain_state/000789.sst: No space left on device'
jctl.node12:Jul 05 16:15:09 singaporeQALive.qatest.evernym.com env[13123]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node12/domain_state/000779.sst: No space left on device'
jctl.node15:Jul 05 16:15:09 sydneyQALive.qatest.evernym.com env[17995]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node15/domain_state/000791.sst: No space left on device'
jctl.node2:Jul 05 16:04:33 ohioQALive.qatest.evernym.com env[18072]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node2/domain_state/000791.sst: No space left on device'
jctl.node20:Jul 05 16:15:09 frankfurtQALive.qatest.evernym.com env[17054]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node20/domain_state/000788.sst: No space left on device'
jctl.node5:Jul 05 16:15:09 californiaQALive.qatest.evernym.com env[26763]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node5/domain_state/000788.sst: No space left on device'
jctl.node7:Jul 05 16:04:48 oregonQALive.qatest.evernym.com env[21183]: rocksdb.errors.RocksIOError: b'IO error: No space left on deviceWhile appending to file: /var/lib/indy/sandbox/data/Node7/domain_state/000783.sst: No space left on device'
;;;","13/Jul/18 10:12 PM;dsurnin;it looks like failed Nodes were not able to start catchup since not all ledger statuses were received
For example Node10 after last restart sent 139 LEDGER_STATUS messages to different nodes, but only 46 messages were received by other nodes.
Meanwhile other nodes sent 139 msgs to Node10 but only 128 msgs were received by Node10.

attached file contains filtered messages ledger status and consistency proof to and from Node10  [^Node10_status_proof_filter] ;;;","17/Jul/18 4:15 PM;dsurnin;As a short term solution we increased the node network stack
https://github.com/hyperledger/indy-plenum/pull/813

The better solution would be to split all node messages into two types: service messages (ping, catchup, statuses) and 3pc messages, and create a dedicated network stack for each type
https://jira.hyperledger.org/browse/INDY-1472
;;;",,,,,,,,,,,,,,,,,,,,,,
Numerous blacklists under high load,INDY-1461,31754,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,zhigunenko.dsr,zhigunenko.dsr,10/Jul/18 7:25 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"*Steps to Reproduce:*
1. Prepare new 
2. Run load from 6 instances:
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 286 -t 19 -n 4 -k ""{\""nym\"": {\""count\"": 4},\""nym\"": {\""attrib\"": 1},\""nym\"": {\""schema\"": 1},\""nym\"": {\""cred_def\"": 1}}""
{code}
Total incoming load: 361 writes/sec

*Actual results:*
Pool made more than 50 view changes and lost consensus after 300ktxns
Logs and replays are available on LogProcessor:/media/data/logs/mass_blacklist_10jul2018","indy-node 1.4.494
25 AWS nodes",,,,,,,,,,,,,,,,,,,,,,,INDY-1463,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzhlb:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,Toktar,zhigunenko.dsr,,,,,,,,,,,"11/Jul/18 7:12 PM;Toktar;Nodes receive CATCHUP_REP with inconsistency consProof and blacklisting each other. Example:

{code}Node1 could not verify catchup reply CATCHUP_REP{'ledgerId': 1, 'consProof': ['ap2gZnEvUGQ8XJ3S6jTGAyV3Dkwj7bw4xqbtYKCgKPu', '2qPZqpEuEgXoGm3ibFdh16Xem9PnfS8mn3rLPfJvZ1Dt', '7rdpBLumEwGRhzULJigLrtPA3fpKPHAZTnLfWwTf5Udk', '5XH22xQLuaQF7cMmCJEYQh6KKVGmsGiBQkNTTJJoaTZr', 'FXEahvwDUYNvb2ruTy2UYEp3L3bqDXDNw7xv5F5EUsMd', '2bbkhQs3VHemaehtLrfXCzSWiLCeLzCQbyzrTQEXaQtv', 'APhxKsrqHgv1CLqx3BF5H5pRpqkVbt3yoKjCEuDuoY5w', '9FVGELMH2wg47LnVqvbD5as7BMck4w4uRp2Fag9EjW2t', '7LDys6Y1Ly4GQZ3kXPHoPvJVz4icwrvaAGG72TRHn7Kk', '64nq4vPf1GV3Hdq2xgXn1GjJZktUa5ivTSktTK38VTpW', 'FiDV9ngABcJuP84jcWZUFqnDZmGdEQXPRT4CnxpX3Qxm', 'CYn6G6aDiz5bvinS8kbcsFGWBD27AzDhhE4i5BcP79im', '9isp7TsUEsLeBUG98ZjU3Q7cSjo1h9RDehsz9kVP5Hz6', 'AyoxEVY8ZQRyTBPzrpAjabz9Q722JbbQwxeEeyup62LV', 'AM3HC6SKRPdTUJ7mp6uoFwnbh6mHAyTpVtsKHqaEnpEs', '5onzA2nrxBNGJFCW1sVAFETmh1x9mo6w6b3fs788Eiwf'], 'txns': {'25751': {'ver': '1', 'reqSignature': {'type': 'ED25519', 'values': [{'from': 'V4SGRU86Z58d6TV7PBUe6f', 'value': '5zgcSjZWVhHJkmFXnGgbtCbJs4d4Vg9ee9Hj6UMHx1bSEk7LHCgKXg9CdPXfYKFf2tQtt6RjgLZXFxHK1UpVdSY5'}]}, 'txn': {'data': {'dest': 'Tfo7nYvPzFcVLKztDwP3Pz'}, 'protocolVersion': 2, 'metadata': {'reqId': 1531146067433417757, 'digest': '11d98dad0f85022ad60c1a9b7c0787541676878949730604f98156a3f636edab', 'from': 'V4SGRU86Z58d6TV7PBUe6f'}, 'type': '1'}, 'txnMetadata': {'txnId': 'c3c4ce2961681e4f68e77ae295fa4cff9347b894680ee886a8a76b3273c8fe58', 'txnTime': 1531147363, 'seqNo': 25751}}}} since Inconsistency: first root hash does not match. Expected hash: b'2656cb42f43444407ce661db1935a0f76bbda1ac195cbef992f3e5d7f403d391', computed hash: b'198b64ed10faa123b89212a8877435d698eaacf59ff824ae0496ceb7af1bb624'
{code};;;","12/Jul/18 11:03 PM;Toktar;During the analysis, was revealed:

7 nodes blacklisting other 18 because _first root hash does not match_ in catchup in 2nd view change. They start 3d  view change but couldn't finish it.;;;","16/Jul/18 7:01 PM;Toktar;The problem can be solved using the following methods:
1. To correct the way of obtaining the last prepare certificate. At the moment we take the last value for which there is a quorum of the prepares. This is incorrect, since we do not check for the availability of the pre-prepares in the current node.
2. Make a view change according to PBFT, as in the current protocol this problem does not have a solution.
 The first option was chosen. Despite the fact that this does not solve the problem in 100% of cases, this will solve part of the problem. Other bags lead to the problem should be solved in the latest bugfixes, which already in master. Thus, the task will require re-testing after the implementation of the fix on the prepare certificate.;;;","17/Jul/18 5:24 PM;Toktar;Problem with incorrect prepare certificate was not found by TDD tests. Added test proves correct work.

The task will be re-tested in [INDY-1343|https://jira.hyperledger.org/browse/INDY-1343]

 ;;;",,,,,,,,,,,,,,,,,,,,,
High Watermark on backup may be reset to 300,INDY-1462,31755,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,10/Jul/18 8:33 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"High watermark (H) can be reset to 300 after primary propagation for a joined node,
It leads to unavailability to restore last ordered on backup if ppSeqNo of other nodes in the pool is more than 300.
Also backup starts stashing messages which may lead to out of memory.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhlj:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,3.0,,,,,,,,,,,,anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"17/Jul/18 1:34 AM;anikitinDSR;Reasons:
 * High watermark on backup replicas after on propagate primary must be set as (0, infinity) for availability to restore last ordered ppSeqNo.

Changes:
 * Set watermark on propagate primary to (0, infinity)

Version:
 * indy-node 1.4.504

PRs:

- [https://github.com/hyperledger/indy-node/pull/820]

- [https://github.com/hyperledger/indy-plenum/pull/809]

Steps to validate:
 * create pool of 4 nodes;
 * set Max3PCBatchSize to 1 and write more than LOG_SIZE count transactions (300 by default);
 * add new node (also set  Max3PCBatchSize to 1 and logLevel to debug)
 * write another several transactions.
 * check that there is no any lines in logs of new added node like :
""stashing 3 phase message <aaa> since ppSeqNo <aaaa> is not between 0 and 300""

 

 ;;;","18/Jul/18 11:17 PM;VladimirWork;Build Info:
indy-node 1.4.504

Steps to Validate:
1. Create pool of 4 nodes.
2. Set Max3PCBatchSize to 1 and write 301 txns.
3. Add new node (also set  Max3PCBatchSize to 1 and logLevel to debug).
4. Write another 300 transactions.

Actual Results:
There are no lines in logs of new added node like ""stashing 3 phase message <aaa> since ppSeqNo <aaaa> is not between 0 and 300"".;;;",,,,,,,,,,,,,,,,,,,,,,,
Catchup during view change may last forever under the load,INDY-1463,31760,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,11/Jul/18 1:16 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"If one of the node is still doing catchup during view change while other nodes in the pool finished view change and already order transactions, the catchup on the slow node may last forever, so that the node will not finish the view change.",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1461,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzi8v:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,3.0,,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,zhigunenko.dsr,,,,,,,,,,"11/Jul/18 3:38 AM;zhigunenko.dsr;*Environment:*
indy-node 1.4.494
25 AWS nodes

*Steps to Reproduce:*
1) setup new pool
2) run load test from 5 instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 200 -t 100 -n 3 -k ""{\""nym\"": 4, \""attrib\"": 1, \""schema\"": 1, \""cred_def\"": 1}""
{code}

*Actual results:*
Session duration: 3h
Ledger size :27 -> 106955 (25view changes, no restart)
After next hours catchup hadn't been finished
All nodes service restart didn't restore pool

*Logs:* /logs/endless_catchup_syncing_10jul2018;;;","11/Jul/18 11:46 PM;sergey.khoroshavin;Log analysis showed the problem is same as in [INDY-1461|https://jira.hyperledger.org/browse/INDY-1461];;;",,,,,,,,,,,,,,,,,,,,,,,
AttributeError: 'NoneType' object has no attribute 'request' during load,INDY-1464,31774,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,11/Jul/18 5:02 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,"*Steps to Reproduce:*
1. Prepare new pool with 25 nodes
2. Run load test from 5 instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 100 -t 50 -n 3 -k ""{\""nym\"": 4, \""attrib\"": 1, \""schema\"": 1, \""cred_def\"": 1}""
{code}
500 clients, 30 writes / sec

*Actual results:*
Pool stopped writing after only 11k txns (and less than 90 minutes) and cannot finish catchup

*Additional info:*
Logs will be available on LogProcessor:~/logs/attribute_error_11jul2018",indy-node 1.4.496,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1465,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzzhlr:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,,,,,,,,,,,,,anikitinDSR,ashcherbakov,zhigunenko.dsr,,,,,,,,,,"11/Jul/18 5:27 PM;anikitinDSR;Some code investigation showed, that problem was in processing MessageResponse for PROPAGATE. MessageResponse has field 'params' and 'msg', which contain digest in 'params' section and request in msg. Therefore, the main assumption is that there is bug in MessageResponse building.

Traceback for this AttributeError is:
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: Traceback (most recent call last):
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: looper.run()
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: return self.loop.run_until_complete(what)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: return future.result()
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: raise self._exception
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: result = coro.send(None)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: await self.runOnceNicely()
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: msgsProcessed = await self.prodAllOnce()
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: s += await n.prod(limit)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 297, in prod
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: c = await super().prod(limit)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1046, in prod
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: c += await self.serviceNodeMsgs(limit)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1080, in serviceNodeMsgs
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: await self.processNodeInBox()
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1642, in processNodeInBox
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: await self.nodeMsgRouter.handle(m)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: res = self.handleSync(msg)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: return self.getFunc(msg[0])(*msg)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_req_processor.py"", line 51, in process_message_re
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: return handler.process(msg, frm)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 63, in process
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: return self.processor(valid_msg, params, frm)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 238, in processor
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: self.node.processPropagate(validated_msg, frm)
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2215, in processPropagate
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: reqDict = msg.request
Jul 11 06:14:25 virginaQALive.qatest.evernym.com env[3208]: AttributeError: 'NoneType' object has no attribute 'request';;;","12/Jul/18 5:41 PM;ashcherbakov;Problem reason:
- Client Request class is overridden in IndyNode. IndyNode's Request overrides calculation of digests for ATTRIB txns
- Propagate requestor used Request class of IndyPlenum which calculated digest for ATTRIB differently. 
- Propagate requestor compared digests, they were different, and returned None. The code didn't expect None there.
- Additionally found: incorrect Request class (from Plenum) is used in `reqToTxn` method.
- Additionally found: a bug in SigningSerializer (INDY-1469)

Changes: 
- use correct Request class when processing requested PROPAGATE (since request class is overridden in indy-node)
- use correct Request class in `reqToTxn`
- fixed a small issue in SigningSerializer
- added tests for SigningSerializer (there is an issue there: INDY-1469)
- added tests for PROPGATE requests of ATTRIB, NYM, SCHEMA and CLAIM_DEF
- handle None values as result of request

PR:
- https://github.com/hyperledger/indy-plenum/pull/807
- https://github.com/hyperledger/indy-node/pull/816

Version:
- master Indy Node 1.4.500

Risk factors:
- ATTRIB txns

Risk:
- Low

Covered with tests:
- test_attrib_txn_digest.py
- test_request_propagates.py

Recommendations for QA:
- Run load tests with ATTRIB txns
;;;","16/Jul/18 10:05 PM;zhigunenko.dsr;*Environment:*
indy-node 1.4.500
libindy 1.5.610

*Steps to Validate:*
Run load tests with ATTRIB txns

*Actual results:*
ATTRIB txns have been read and written successfully;;;",,,,,,,,,,,,,,,,,,,,,,
Concurrent view change ,INDY-1465,31776,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,zhigunenko.dsr,zhigunenko.dsr,11/Jul/18 5:53 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Steps to Reproduce:*
1. Prepare new pool with 25 nodes
2. Run load test from 5 instances
{code}
python3 perf_processes.py -g pool_transactions_genesis -b 10 -r 2 -m t -c 100 -t 50 -n 3 -k ""{\""nym\"": 4, \""attrib\"": 1, \""schema\"": 1, \""cred_def\"": 1}""
{code}
500 clients, 30 writes / sec

*Actual results:*
14/25 nodes
{code}
[[0, 25, 'D8fs16NGgAQwyC4zcS6BG7VEfLTDE8PqGjpPP9N2mhJp'], [1, 11641, 'FYWkswwb8CVRvo7Wx2DwPiDGChjudSnFMSQHJQaWr3m'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]
{code}
6/25 nodes
{code}
[[0, 25, 'D8fs16NGgAQwyC4zcS6BG7VEfLTDE8PqGjpPP9N2mhJp'], [1, 327, '2hsWKtdn7rE3WEAs5s59bGcipipdcpXGEx67CVjGtBXV'], [2, 0, 'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn']]
{code}
Cannot reach consensus due to nodes mass restart in a short period

*Additional info:*
CURRENT_STATE response may contain some mistakes when formed

Logs will be available on LogProcessor:~/logs/attribute_error_11jul2018",indy-node 1.4.496,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1199,INDY-1464,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzib3:",,,,,,,,,,,,,,,,,,,,,,,,,,zhigunenko.dsr,,,,,,,,,,,,"24/Aug/18 12:31 AM;zhigunenko.dsr;*Reason to Close:*
Version is outdated;;;",,,,,,,,,,,,,,,,,,,,,,,,
Review monitor socket usage in Remote class,INDY-1466,31779,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,sergey-shilov,sergey-shilov,sergey-shilov,11/Jul/18 10:13 PM,11/Jul/18 10:13 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"{color:#75715e}It looks strange to call get_monitor_socket() each time we {color}{color:#75715e}want to get it instead of get it once and save reference. {color}{color:#75715e}May be side effects here, so the ticket is created to check and clean {color}{color:#75715e}up the implementation.{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzibj:",,,,,,,,,,,,,,,,,,,,,,,,,,sergey-shilov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Get rid of peersWithoutRemotes,INDY-1467,31780,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,sergey-shilov,sergey-shilov,11/Jul/18 10:38 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6,,,,0,,,,"The peersWithoutRemotes ZStack class member is a set of idents got from received messages (used in DEALER-ROUTER ZMQ pattern to distinguish clients during sending replies).

There are two purposes of peersWithoutRemotes usage:

1) it is an old mechanism to tell connected clients about changed pool membership;
2) to interrupt sending of answer to client in case of propagated request (in this case ZMQ does not have corresponding ident in inner routing table).

Regarding the first option: new SDK client does not use such mechanism. 
Regarding the second option: it is absolutely safe to send message to unknown ident using ZMQ. If ident is not known then such message is silently dropped by ZMQ, reference:

[http://zguide.zeromq.org/page%3aall#ROUTER-Error-Handling]

So there is no ZMQ crashes or memory leaks here, it is up to ZMQ to drop such messages, we don't need to control this on our application level.

Usage of peersWithoutRemotes is not just useless, but rather dangerous especially taking into account new SDK client behaviour of keeping connections. Now each new ident is added to peersWithoutRemotes but never removed. Moreover, we can not detect disconnection of particular ident even having disconnected event as we can not match fds and idents, ZMQ does not provide such mechanism. So it is always memory leak. Since now new SDK client implements pretty fast rotation of idents that makes described memory leak even faster.

That's why we should get reed of peersWithoutRemotes usage.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1241,,,,,,,,,"1|hzzhlz:",,,,,,EV 18.14 Monitoring/Stability,,,,,,,,2.0,,,,,,,,,,,,sergey-shilov,zhigunenko.dsr,,,,,,,,,,,"16/Jul/18 7:05 PM;sergey-shilov;*Problem state / reason:*

See ticket description.

*Changes:*

Removed peersWithoutRemotes class member, changed several class methods that used peersWithoutRemotes, fixed tests that relied on push notification about changed pool membership.

*Committed into:*

    https://github.com/hyperledger/indy-plenum/pull/808
    https://github.com/hyperledger/indy-node/pull/817
     indy-node 1.4.503-master

*Risk factors:*

    Deprecated client will not receive push notification about changed pool membership from nodes.
    May be some side effects related to propagated request, but probability is very low.

*Risk:*

    Medium

*Recommendations for QA:*

Do load testing and check whether something strange happens.;;;","19/Jul/18 4:49 PM;zhigunenko.dsr;*Environment:*
indy-node 1.4.504
libindy 1.5.610

*Steps to Validate:*
1) setup new pool
2) set load test: 1000 concurrent users, 10 txns/sec write, 45 txns/sec write, duration - 3h

*Actual results:*
Poos is working, nothing strange;;;",,,,,,,,,,,,,,,,,,,,,,,
Latency measurements in monitor should be windowed,INDY-1468,31782,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,12/Jul/18 12:19 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,Regular view changes increase master latency (but not backup latencies?) according to current latency measurements and it causes to additional view changes due to master latency degradation so latency measurements also should be windowed the same way as throughput measurements in INDY-1435.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1435,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1376,,,,,,,,,"1|hzzhnb:",,,,,,EV 18.15 Stability/Availabilit,,,,,,,,3.0,,,,,,,,,,,,alphax0111,anikitinDSR,VladimirWork,,,,,,,,,,"20/Jul/18 11:23 PM;anikitinDSR;Reasons:
 * need to improve average latency calculating

Changes:
 * for now, average latency is calculated by EMA algorithm, as for throughput

Version:
 * indy-node 1.5.515

Steps to validate:
 * steps are the as for INDY-1435;;;","27/Jul/18 12:50 AM;VladimirWork;There is an incorrect value in metrics dump in logs:
{noformat}
            client avg request latencies: [<plenum.server.monitor.LatencyMeasurement object at 0x7f82edc2d940>, <plenum.server.monitor.LatencyMeasurement object at 0x7f82e9709c50>, <plenum.server.monitor.LatencyMeasurement object a
t 0x7f82eb8682b0>, <plenum.server.monitor.LatencyMeasurement object at 0x7f82eb8a0d30>, <plenum.server.monitor.LatencyMeasurement object at 0x7f82eac8eb70>, <plenum.server.monitor.LatencyMeasurement object at 0x7f82eb18de80>, <plen
um.server.monitor.LatencyMeasurement object at 0x7f82eb868e10>, <plenum.server.monitor.LatencyMeasurement object at 0x7f82eb18e748>, <plenum.server.monitor.LatencyMeasurement object at 0x7f82ea8fbeb8>]
{noformat}
;;;","27/Jul/18 12:54 AM;VladimirWork;Recheck *without* forced view changes after fix.;;;","27/Jul/18 5:41 PM;anikitinDSR;Fixed in version 1.5.526;;;","28/Jul/18 4:51 PM;VladimirWork;Build Info:
indy-node 1.5.526

Steps to Validate:
1. Run writing load test (medium load) with forced view changes.
2. Run the same load test without forced view changes.

Actual Results:
There was only 1 unexpected view change during Step 1 (forced view change failed due to timeout and master prorocol didn't order any requests and its latency dropped too low during ""window"" so it's ok).
There were no view changes at all during Step 2.;;;",,,,,,,,,,,,,,,,,,,,
Signing Serializer doesn't work properly with complex dicts,INDY-1469,31786,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,12/Jul/18 12:45 AM,12/Jul/18 12:47 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"See Plenum, `test_signing_serializer.test_serialize_complex_dict` and `test_serialize_dicts_with_different_keys`.
Expected: b'1:a|2:3:b|2:4:5:6:c'
Result:  b'1:a|2:3:b|4:5:6:c'

So, the following two messages will be serialized to the same value:
{code}
    v1 = {
            1: 'a',
            2: {
                3: 'b',
                4: {
                    5: {
                        6: 'c'
                    }
                }
            }
        })
    v2 = {
            1: 'a',
            2: {
                3: 'b',
            },
            4: {
                5: {
                    6: 'c'
                }
            }
        })
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"1|hzzibr:",,,,,,,,,,,,,,,,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
One of the nodes laged behind others after forced view changes,INDY-1470,31795,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ozheregelya,ozheregelya,12/Jul/18 1:51 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"*Steps to Reproduce:*
 1. Setup AWS acceptance pool of 6 nodes (4+2).
 2. Set in indy_config.py.
 3. Periodically run the load test with not very high load (~5 nyms/sec) during several days.
 => Domain ledger size is about 67K txns, viewNo is 164.
 4. Run several force-view-change experiments.
 5. Restart the pool.
 6. Run several force-view-change experiments.

*Actual Results:*
 After one of force-view-change experiments one of the nodes (Node2) is lagged behind the others.

*Expected Results:*
 Nodes should be synchronized.

Logs and nscaptures: [https://s3.console.aws.amazon.com/s3/buckets/qanodelogs/indy-1470/]

*Additional Information:*
Lagged node was not able to write missed txns while it was without load and after writing of several txns. It wrote missed txns after restart the pool.","indy-node 1.4.494
libindy 1.5.0~613",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jul/18 5:44 AM;ckochenower;Screen Shot 2018-07-20 at 2.17.30 PM.png;https://jira.hyperledger.org/secure/attachment/15355/Screen+Shot+2018-07-20+at+2.17.30+PM.png","21/Jul/18 5:44 AM;ckochenower;Screen Shot 2018-07-20 at 2.19.13 PM.png;https://jira.hyperledger.org/secure/attachment/15354/Screen+Shot+2018-07-20+at+2.19.13+PM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1032,,,,,,,,,"1|hzwxxb:",,,,,,EV 18.15 Stability/Availabilit,,,,,,,,,,,,,,,,,,,,anikitinDSR,ckochenower,ozheregelya,,,,,,,,,,"18/Jul/18 1:32 AM;anikitinDSR;During log investigation was found that:
 # Node3 was stopped
 # Node2 was stopped (view change forcing by primary restart) Current viewNo = 2
 # Other nodes try to send start view_change procedure by sending INSTANCE_CHANGE messages.
 # Needed quorum is n -f . Therefore need at least 5 instance change messages (1 from self and 4 from other) but reachable node count is 4.
 # Node2 was started and receive 2 INSTANCE_CHANGE message from b'public key' nodes (not connected yet)
 # After this, Node2 complete propagate primary by current state and Node2's current viewNo is 2 now.
 # Node2 received 3 INSTANCE_CHANGE messages after connecting to other nodes. Then, this node has 3 INSTANCE_CHANGE messages from identificated nodes (Node6, Node4, Node1) and 2 message from unknown (b'BEhc!q7/\{^9)vgy&p$f!kx.sTfa9gxeT9(BqKVil'  and  b'-2aEQy3=<9:V.^B.sB#>dXd^DGfVU69[2yMSLsc9')
 # Node2 initiated view change procedure from 2 to 3, completed catchup and try to initiate new view change, because proposed Node3 is not connected (there is no received VIEW_CHANGE_DONE message from proposed primary node)
 # In other words, nodes: Node1, Node4, Node5 and Node6 never finish view_change to 3 number, Node2 never finish view change to number 4 and will not order transaction.

Fixes:
 # Don't send INSTANCE_CHANGE to disconnected nodes
 # Check, that 'from' section in INSTANCE_CHANGE message is identified node (connected node) for incoming INSTANCE_CHANGE messages;;;","19/Jul/18 8:16 PM;anikitinDSR;Reasons:
 * we can receive instance change messages from anyone (also not identified node)

Changes:
 * added check, that received instance change message was got from known/identified node

Versions:
 * indy-node: 1.5.514
 * indy-plenum: 1.5.465

Steps to validate:
 * setup pool from 6 nodes
 * restart primary (node 1)
 * ensure, that now primary node is number 2
 * stop node3
 * ensure view change by stopping node2
 * wait about 1 minute and check, that new primary is not elected yet
 * start node2 and check that node2 propagate primary and current viewno is 1 (as for other)
 * check, that node2 will not start view_change to view 2;;;","21/Jul/18 5:46 AM;ckochenower;[~anikitinDSR] - In response to the stacktrace you posted in the Berdyaev 6 slack channel:
{code}
My steps is:
1. git checkout <corresponding version of indy-node and indy-plenum>
2. in my develop environment i run `nsreplay Node1.sandbox.20180711163531.tar.gz` and got the next traceback:
`Aprox run time: 2 days, 0:41:05
Replaying 530682 messages in total
node msg count: 47 of 530682
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
Traceback (most recent call last):
File ""./nsreplay"", line 447, in <module>
  sys.exit(main(arguments))
File ""./nsreplay"", line 440, in main
  replayer.replay_node(args.recording)
File ""./nsreplay"", line 365, in replay_node
  start_times)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/recorder/replayer.py"", line 110, in prepare_node_for_replay_and_replay
  return replay_patched_node(looper, replaying_node, node_recorder, cr)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/recorder/replayer.py"", line 186, in replay_patched_node
  looper.run(replaying_node.prod())
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/stp_core/loop/looper.py"", line 264, in run
  return self.loop.run_until_complete(what)
File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
  return future.result()
File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
  raise self._exception
File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
  result = coro.send(None)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/stp_core/loop/looper.py"", line 255, in wrapper
  raise ex
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/stp_core/loop/looper.py"", line 242, in wrapper
  results.append(await coro)
File ""/home/anikitin/projects/evernym/sample_project/indy-node/indy_node/server/node.py"", line 297, in prod
  c = await super().prod(limit)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/node.py"", line 1047, in prod
  c += await self.serviceReplicas(limit)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/node.py"", line 1070, in serviceReplicas
  inbox_processed = self.replicas.service_inboxes(limit)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/replicas.py"", line 74, in service_inboxes
  sum(replica.serviceQueues(limit) for replica in self._replicas)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/replicas.py"", line 74, in <genexpr>
  sum(replica.serviceQueues(limit) for replica in self._replicas)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/replica.py"", line 853, in serviceQueues
  self.node.isParticipating) else 0
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/replica.py"", line 715, in send3PCBatch
  ppReq = self.create3PCBatch(lid)
File ""/home/anikitin/projects/evernym/sample_project/indy-plenum/plenum/server/replica.py"", line 766, in create3PCBatch
  pp_seq_no)
TypeError: 'NoneType' object is not iterable
{code}

It appears to be bug in the replayer:
!Screen Shot 2018-07-20 at 2.19.13 PM.png|thumbnail!
!Screen Shot 2018-07-20 at 2.17.30 PM.png|thumbnail! 

I changed...

{code}
...
            for key in req_ids:
                if key not in self.requestQueues[ledger_id]:
                    # Request not available yet
                    return
...
{code}

...to...

{code}
...
            for key in req_ids:
                if key not in self.requestQueues[ledger_id]:
                    # Request not available yet
                    return None, None, None, tm
...
{code}

...and the replayer is no longer stacktracing. 

{code}
(indy-1470) vagrant@ubuntu-xenial:~$ ./indy-node/tools/diagnostics/nsreplay -c n ./Node1.sandbox.20180711163531.tar.gz 
2018-07-20 20:31:16,451 | INFO     | notifier_plugin_manager.py ( 121) | importPlugins | Found notifier plugins: []
2018-07-20 20:31:16,654 | INFO     | notifier_plugin_manager.py ( 121) | importPlugins | Found notifier plugins: []
Aprox run time: 2 days, 0:41:05
Replaying 530682 messages in total
node msg count: 47 of 530682
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node msg count: 69 of 530682
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
node is idle
{code};;;","21/Jul/18 6:23 AM;ckochenower;[~lovesh] - says that the code in replica.py calling consume_req_queue_for_pre_prepare has been changed. The change is causing the stacktrace.

In simplest terms, when binding the left side (l-value) of an expression in python that expects a tuple, the right side of the expression (r-value) must be a tuple with the same arity. When this is not the case, python produces the following stacktrace:

An example of incorrect followed by correct arity.
{code}
>>> foo = None
>>> bar, baz = foo
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'NoneType' object is not iterable
>>> foo = None, None
>>> bar, baz = foo
>>> 
{code}

I will try to find the commit that overwrote [~lovesh]'s original commit. If it can be reverted, I will do so. Otherwise, I will reimplement his original changes to replica.py and submit a PR.;;;","21/Jul/18 6:32 AM;ckochenower;[~lovesh]'s PR is [here|https://github.com/hyperledger/indy-plenum/pull/692] (a month ago)

This is the commit that broke the replay: https://github.com/hyperledger/indy-plenum/commit/0ee85972fce5eece7cac95fa943bf8e66f44a878 (18 days ago)

The commit that broke the replay added a test. Therefore, I will submit a PR sufficient to revert the changes in the commit made to replay.py.;;;","31/Jul/18 4:50 AM;ozheregelya;Environment:
 indy-node 1.5.529
 libindy 1.6.1~659

Steps to Validate:
 1. Setup the pool of 6 nodes
 2. Initiate View Change by primary (Node 1) restart.
 => Primary is Node 2, ViewNo 1.
 3. Stop the Node3.
 4. Stop the primary (Node 2).
 => View Change was not happened.
 5. Start the Node2.

Actual Results:
 After starting of Node2 View Change was not happened. View No is still 1, Primary is Node 2. Pool still works, new txns were written on all nodes.

Additional Information:
 Node 2 have not started View Change, as described in [~anikitinDSR]'s comment, but the following messages were noticed in logs of Node2:
{code:java}
2018-07-30 19:29:47,605|DEBUG|node.py|Node2 received node message from Node5: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,606|TRACE|node.py|Node2 msg validated ({'reason': 26, 'viewNo': 2, 'op': 'INSTANCE_CHANGE'}, 'Node5') 2018-07-30 19:29:47,606|TRACE|node.py|Node2 appending to nodeInbox INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,606|DEBUG|node.py|Node2 sending message to view changer: (INSTANCE_CHANGE{'reason': 26, 'viewNo': 2}, 'Node5') 2018-07-30 19:29:47,606|INFO|view_changer.py|Node2 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} from Node5 2018-07-30 19:29:47,607|DEBUG|monitor.py|Node2 master throughput is not measurable. 2018-07-30 19:29:47,607|TRACE|monitor.py|Node2 found difference between master and backups avg latencies to be acceptable 2018-07-30 19:29:47,607|INFO|view_changer.py|Node2 received instance change message INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} but did not find the master to be slow 2018-07-30 19:29:47,652|TRACE|zstack.py|Node2 got 1 messages through listener 2018-07-30 19:29:47,652|DEBUG|node.py|Node2 received node message from Node1: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,652|TRACE|node.py|Node2 msg validated ({'reason': 26, 'viewNo': 2, 'op': 'INSTANCE_CHANGE'}, 'Node1') 2018-07-30 19:29:47,652|TRACE|node.py|Node2 appending to nodeInbox INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,652|DEBUG|node.py|Node2 sending message to view changer: (INSTANCE_CHANGE{'reason': 26, 'viewNo': 2}, 'Node1') 2018-07-30 19:29:47,653|INFO|view_changer.py|Node2 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} from Node1 2018-07-30 19:29:47,653|DEBUG|monitor.py|Node2 master throughput is not measurable. 2018-07-30 19:29:47,653|TRACE|monitor.py|Node2 found difference between master and backups avg latencies to be acceptable 2018-07-30 19:29:47,653|INFO|view_changer.py|Node2 received instance change message INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} but did not find the master to be slow 2018-07-30 19:29:47,667|TRACE|zstack.py|Node2 got 1 messages through listener 2018-07-30 19:29:47,667|DEBUG|node.py|Node2 received node message from Node4: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,667|TRACE|node.py|Node2 msg validated ({'op': 'INSTANCE_CHANGE', 'reason': 26, 'viewNo': 2}, 'Node4') 2018-07-30 19:29:47,667|TRACE|node.py|Node2 appending to nodeInbox INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,667|DEBUG|node.py|Node2 sending message to view changer: (INSTANCE_CHANGE{'reason': 26, 'viewNo': 2}, 'Node4') 2018-07-30 19:29:47,668|INFO|view_changer.py|Node2 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} from Node4 2018-07-30 19:29:47,668|DEBUG|monitor.py|Node2 master throughput is not measurable. 2018-07-30 19:29:47,668|TRACE|monitor.py|Node2 found difference between master and backups avg latencies to be acceptable 2018-07-30 19:29:47,668|INFO|view_changer.py|Node2 received instance change message INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} but did not find the master to be slow 2018-07-30 19:29:47,745|TRACE|zstack.py|Node2 got 1 messages through listener 2018-07-30 19:29:47,746|DEBUG|node.py|Node2 received node message from Node6: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,746|TRACE|node.py|Node2 msg validated ({'reason': 26, 'op': 'INSTANCE_CHANGE', 'viewNo': 2}, 'Node6') 2018-07-30 19:29:47,746|TRACE|node.py|Node2 appending to nodeInbox INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} 2018-07-30 19:29:47,746|DEBUG|node.py|Node2 sending message to view changer: (INSTANCE_CHANGE{'reason': 26, 'viewNo': 2}, 'Node6') 2018-07-30 19:29:47,747|INFO|view_changer.py|Node2 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} from Node6 2018-07-30 19:29:47,747|DEBUG|monitor.py|Node2 master throughput is not measurable. 2018-07-30 19:29:47,747|TRACE|monitor.py|Node2 found difference between master and backups avg latencies to be acceptable 2018-07-30 19:29:47,747|INFO|view_changer.py|Node2 received instance change message INSTANCE_CHANGE{'reason': 26, 'viewNo': 2} but did not find the master to be slow
{code}
[~anikitinDSR], Is it expected behavior that the rest nodes sent 'INSTANCE_CHANGE' to Node2?

 ;;;","31/Jul/18 6:27 AM;ozheregelya;One more thing.
{quote}Fixes:
 # Don't send INSTANCE_CHANGE to disconnected nodes{quote}
Following messages were noticed in logs of active node while Node3 and Node 2 were stopped (after step 4 from previous comment):
{code:java}
2018-07-30 21:17:49,655|DEBUG|view_changer.py|Node6's view_changer sending INSTANCE_CHANGE{'viewNo': 2, 'reason': 26}
2018-07-30 21:17:49,655|TRACE|has_action_queue.py|Node6 scheduling action partial(send_instance_change_if_needed) with id 3 to run in 60 seconds
2018-07-30 21:17:49,655|INFO|view_changer.py|Count of rounds without quorum of instance change messages: 0
2018-07-30 21:17:49,656|DEBUG|node.py|Node6 sending message INSTANCE_CHANGE{'viewNo': 2, 'reason': 26} to all recipients: ['Node2', 'Node5', 'Node4', 'Node1', 'Node3']
2018-07-30 21:17:49,656|TRACE|batched.py|Node6 sending msg b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node2
2018-07-30 21:17:49,656|TRACE|zstack.py|Node6 transmitting message b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node2
2018-07-30 21:17:49,656|INFO|zstack.py|Remote Node2 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings
2018-07-30 21:17:49,656|TRACE|batched.py|Node6 sending msg b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node1
2018-07-30 21:17:49,656|TRACE|zstack.py|Node6 transmitting message b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node1
2018-07-30 21:17:49,661|TRACE|batched.py|Node6 sending msg b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node5
2018-07-30 21:17:49,661|TRACE|zstack.py|Node6 transmitting message b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node5
2018-07-30 21:17:49,661|TRACE|batched.py|Node6 sending msg b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node4
2018-07-30 21:17:49,661|TRACE|zstack.py|Node6 transmitting message b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node4
2018-07-30 21:17:49,661|TRACE|batched.py|Node6 sending msg b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node3
2018-07-30 21:17:49,661|TRACE|zstack.py|Node6 transmitting message b'{""op"":""INSTANCE_CHANGE"",""viewNo"":2,""reason"":26}' to Node3
2018-07-30 21:17:49,661|INFO|zstack.py|Remote Node3 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings
{code}
Is it ok?;;;","31/Jul/18 6:43 PM;anikitinDSR;Yes, INSTANCE_CHANGE messages to disconnected node would be sent. ""Don't send INSTANCE_CHANGE to disconnected nodes"" it's just a suggestion and fix with ignoring INSTANCE_CHANGE message from unknown will enough;;;",,,,,,,,,,,,,,,,,
Logs appears in old CLI,INDY-1471,31814,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,12/Jul/18 8:34 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"*Steps to Reproduce:*

1. Run old cli.
2. Set yourself as default Trustee.
3. Try to send NYM.

*Actual Results:*
{code:java}
indy@sandbox> new key with seed 000000000000000000000000Trustee1
INFO:cli:CLI command entered: new key with seed [redacted]
Key created in wallet Default
DID for key is V4SGRU86Z58d6TV7PBUe6f
Verification key is ~CoRER63DVYnWZtK8uAzNbx
Current DID set to V4SGRU86Z58d6TV7PBUe6f
indy@sandbox> send NYM dest=V4SGRU86Z58d6TV7PBUe61
INFO:cli:CLI command entered: send NYM dest=V4SGRU86Z58d6TV7PBUe61
DEBUG:cli:wallet context check: Default
DEBUG:cli: wallet.getEnvName: None
DEBUG:cli: active env: sandbox
Adding nym V4SGRU86Z58d6TV7PBUe61
indy@sandbox> INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node1C: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQACK', 'reqId': 1531394637578734}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node3C: {'reqId': 1531394637578734, 'op': 'REQACK', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node4C: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQACK', 'reqId': 1531394637578734}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node6C: {'reqId': 1531394637578734, 'op': 'REQACK', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node5C: {'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'op': 'REQACK', 'reqId': 1531394637578734}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node2C: {'reqId': 1531394637578734, 'op': 'REQACK', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node1C: {'op': 'REPLY', 'result': {'reqSignature': {'type': 'ED25519', 'values': [{'value': '4vW51dwzyRkD9ZzRvWvAyaz}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node4C: {'op': 'REPLY', 'result': {'reqSignature': {'type': 'ED25519', 'values': [{'value': '4vW51dwzyRkD9ZzRvWvAyaz}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node6C: {'op': 'REPLY', 'result': {'reqSignature': {'values': [{'value': '4vW51dwzyRkD9ZzRvWvAyazX8tDThmEK2Wm9DQgU63}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node5C: {'op': 'REPLY', 'result': {'reqSignature': {'type': 'ED25519', 'values': [{'value': '4vW51dwzyRkD9ZzRvWvAyaz}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node3C: {'op': 'REPLY', 'result': {'reqSignature': {'type': 'ED25519', 'values': [{'value': '4vW51dwzyRkD9ZzRvWvAyaz}
INFO:stp_core.common.log:Client 9sAoeNvHpHnEnXtkaZ3JThPyr1ner8sc8defumwqyvbe got msg from node Node2C: {'op': 'REPLY', 'result': {'reqSignature': {'values': [{'value': '4vW51dwzyRkD9ZzRvWvAyazX8tDThmEK2Wm9DQgU63}
Nym V4SGRU86Z58d6TV7PBUe61 added{code}

*Expected Results:*
Logs should not appear in CLI.",indy-node 1.4.500,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,,,,,,No,,,No,,,"1|hzzivz:",,,,Unset,,EV 18.14 Monitoring/Stability,,,,,,,,,Unset,,,,,,,,,,,ashcherbakov,ozheregelya,,,,,,,,,,,"18/Jul/18 11:45 PM;ashcherbakov;Fixed in IndyNode 1.4.509
PR: https://github.com/hyperledger/indy-plenum/pull/821;;;","19/Jul/18 2:37 AM;ozheregelya;*Environment:*
indy-node 1.5.511

*Steps to Validate:*

1. Run old cli.
2. Set yourself as default Trustee.
3. Try to send NYM.

*Actual Results:*
No logs appeared in the old CLI. ;;;",,,,,,,,,,,,,,,,,,,,,,,
Implement one more zmq node stack,INDY-1472,31824,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,13/Jul/18 1:28 AM,11/Jun/19 5:59 PM,28/Oct/23 2:47 AM,,,2.0,,,,0,,,,"Under high load we can lost some msg due to queue overflow or detect node connection with  a time lag.
Possible solution is to send service messages such as ping-pong, ledger status, current state, view change via special dedicated socket different from 3pc msgs socket.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2083,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,,,INDY-1241,,,No,,,No,,,"1|hzwx4f:2rzk",,,,Unset,,,,,,,,,,,Unset,,,,,,,,,,,dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Several nodes (less than f) get ahead the rest ones under load,INDY-1473,31827,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,13/Jul/18 2:51 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,TShirt_L,,,"*Steps to Reproduce:*
1. Install pool of 1.4.494 version with forced view changes each 1800 seconds.
2. Run low load test (10 txn/s) to fill ledger with 500k txns.
3. Upgrade pool to 1.4.496 version (forced view change interval is not changed).
4. Run high load test `while true; do python3 perf.py -c 20 -n 1000 -k nym -g pool_transactions_genesis; done`.

*Actual Results:*
 View Change was not completed on all of nodes.
 Domain ledger sizes:
  - Node1: 655059, 
  - Node2 and Node3: *700497*, 
  - Node7 *700427*, 
  - The rest 18: *700227*.
 Pool stopped working.

*Expected Results:*
 Pool should work.

Logs: https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1473","AWS pool of 22 nodes (1 - 22 nodes of QA Live pool)
indy-node 1.4.496
libindy 1.5.0~613 (client)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,,,INDY-1032,,,No,,,No,,,"1|hzwy33:",,,,Unset,,EV 18.16 Releasing 1.6,,,,,,,,,Unset,,,,,,,,,,,Derashe,ozheregelya,,,,,,,,,,,"09/Aug/18 12:01 AM;Derashe;Problem reason:
 * Pool stopped working

Inverstigation:
 * Logs of this pool's run shows that pool doesn't write new txns because it cannot choose primary.
 ** It cannot choose primary because every node stuck in endless viewchange.
 ** Viewchange cannot successfully end because corresponding catchups endlessly repeating.
 ** Catchups cannot successfully end because node did not collect quorum of VIEW_CHANGE_DONE messages for current view_no.
 ** VIEW_CHANGE_DONE messages can't collect quorum in the right time, because endless catchup takes almost all the looper's time.
 ** That situation happens because of forcing view_change when another view_change is in progress. Actual Node correctly handle this behaviour. That was fixed in scope of https://jira.hyperledger.org/browse/INDY-1454
 * The reason why 2, 3 and 7 Nodes got ahead of other pool is uneven receiving of commit certificates (quorum of nodes got prepare certificate, but only 3 of them got commit certificates to order new txns).

Recommendations for QA: 
 * Try case of this ticket with actual node (1.5.551);;;",,,,,,,,,,,,,,,,,,,,,,,,
Create 1.5 Release,INDY-1474,31855,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,13/Jul/18 11:04 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.5,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,,,,,,No,,,No,,,"1|hzzhnj:",,No,,Unset,,EV 18.14 Monitoring/Stability,EV 18.15 Stability/Availabilit,,,,,,,2.0,Unset,,,,,,,,,,,ashcherbakov,VladimirWork,,,,,,,,,,,"19/Jul/18 9:06 PM;ashcherbakov;Indy Node RC 1.5.67 is issued;;;","28/Jul/18 4:52 PM;VladimirWork;RC 1.5.68 has been approved to stable.;;;",,,,,,,,,,,,,,,,,,,,,,,
Explore timing and execution time,INDY-1475,31860,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,14/Jul/18 12:41 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"In order to find out proper values for ZMQ queue sizes and a number of messages we process by each run of the looper (DEFAULT_LISTENER_QUOTA, DEFAULT_SENDER_QUOTA, ZMQ_CLIENT_QUEUE_SIZE, ZMQ_NODE_QUEUE_SIZE), we need to measure the following:
 * How many messages we process in one run in node-to-node and client-to-node stacks
 ** under the load and not
 ** average, minimum and maximum values, distribution graph)
 * How much time each run of the looper takes (from one call to network stack to another)
 * Size of 3PC batches
 * Size of messages
 * How many node-to-node messages we have for 1 3PC Batch and 1 request
 * How many transport Batches we create and their size (on both KB and number of messages)

For each value we need to measure:
* System not under load
* System with a steady-state of significant load (see INDY-1343)
We need to know the average, minimum and maximum values, distribution graph.
This baseline testing should only follow our recommended production configuration.

This baseline testing does not include:
* Edge cases
* Forced view changes
* Combinations of the various configuration values

*Acceptance criteria*
 * Hooks in the code to perform the measurement
 * A spreadsheet with info above
 * A summary document (about 1 page) of findings and recommendations for network node resourcing that can be provided to the Sovrin Foundation
 * Identified improvements should be raised as separate issues",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1549,,,,,,,,,,,,,,,"03/Aug/18 7:53 PM;sergey.khoroshavin;Screenshot from 2018-08-03 12-44-31.png;https://jira.hyperledger.org/secure/attachment/15448/Screenshot+from+2018-08-03+12-44-31.png","03/Aug/18 8:02 PM;sergey.khoroshavin;metrics.log;https://jira.hyperledger.org/secure/attachment/15449/metrics.log","03/Aug/18 7:53 PM;sergey.khoroshavin;perf5.csv;https://jira.hyperledger.org/secure/attachment/15447/perf5.csv",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,,,INDY-1118,,,No,,,No,,,"1|hzwxtz:",,No,,Unset,,EV 18.15 Stability/Availabilit,,,,,,,,5.0,Unset,,,,,,,,,,,ashcherbakov,esplinr,sergey.khoroshavin,,,,,,,,,,"16/Jul/18 11:14 PM;esplinr;This information will help us refine the minimum hardware requirements for validator nodes run by network stewards.;;;","03/Aug/18 9:32 PM;sergey.khoroshavin;I've attached some data gathered from 25-node pool under load of ~10 writes (nym, attrib, schema, claim_def) and 100 reads per second from 125 clients in total. Timestamps are in UTC. Revocations were not tested because they were causing out of memory errors very quickly, this is already being investigated in INDY-1546. Most interesting moments were (time is approximate):
- 16:10 - required load parameters achieved
- 16:30 - something happened in pool leading to 2-fold increase in amount of incoming traffic, although there was almost no impact on ordering performance
- 16:40 - view change as a result of manual restart of master primary node
- 16:50 - pool fully recovered from view change and continued performing with required rate
- 20:10 - another increase of incoming traffic, but then pool recovered
- 21:10 - another view change happened, and at the same time writing load script crashed, aftewards only reads were executed (100 per second)

I believe some number require explanation to avoid confusion:
1) request_count_per_second are counted for all instances, so given that there are 9(f+1) total instances in this pool real request number should be divided by 9
2) despite large number of read per second only 4 incoming client requests are seen - this is expected, because reads are spread across all (25) nodes in pool, 100/25 = 4;;;","03/Aug/18 10:05 PM;sergey.khoroshavin;Now to the topic of  recommendations for network node resourcing.
1) looking at average message sizes during steady states it can be seen that node-to-node messages average to a little more than 3kB, and client-to-node message a little less than 2 kB. Given that currently we have node queue of 20000 messages, client queue of 3000 and it seems like ZMQ is keeping separate queue for each connection it can be seen that node queues can require up to 3.5 * 20000 * 24 * 2(in/out) ~ 3.2 Gb of RAM, client queues (if we allow 1000 connected clients) 2.0 * 3000 * 1000 * 2 = 11 Gb of RAM. Besides this there are also different internal queues in node which can also take significant amount of RAM
2) however sometimes node-to-node messages max out to about 700 Kb (and there is already PR to reduce this limit to 128 Kb), and during testing with heavier load I've seen average 500 Kb per message sustained for tens of minutes, which raises potential RAM requirements for node queues to 110 Gb (with PR merged). Also some client messages were as large as 10 Kb, which means that potential RAM requirements for client queues can be up to 55 Gb
3) on the other hand looking at real node-to-node traffic (~1 Mb per second) during moderate load it's unlikely that queues can grow that large in short amount of time (it would take them more than 30 hours to max out at 110 Gb). Also we have quota of 100 messages processed per one looper run, and it can be seen (from metrics.log) that most of time queues were under 100 messages

Given all that it can be seen that node shouldn't take huge amount of RAM during normal operation, but in some cases can potentially require more than *165 Gb*. I would recommend to *increase minimum RAM requirement to 32 Gb*, create 25-node test pool with that amount of RAM on each node and run DDoS scenario to see if can handle such load. Also further research should be done to see if we can reduce queues size without major impact on performance. ;;;",,,,,,,,,,,,,,,,,,,,,,
ev1 not posting transactions that are posting to others,INDY-1476,31884,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,14/Jul/18 3:51 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"After updating firewall rules on ev1, a test NYM transaction (DID: 9qFVrUVE1UjgQcvLfdHTLm) was written to the ledger.  The transaction successfully posted to the ledger, but was not found in the ev1 database using read_ledger.  We verified that there was not stale read_ledger data interfering with that tool.

We rebooted the node and observed that the node was able to catch up and get that transaction.  We then posted a second test NYM transaction (DID: 9qFVrUVE1UjgQcvLfdHTLx), and observed that it once again successfully posted to the ledger, but not to the local copy on ev1.  There are numerous WARNING messages in the ev1 logs related to this failure.

Please diagnose this error.  It is preventing proper operation of the ev1 node.  We believe that the firewall changes are not related to the failure since we can see connections from the other validators on the incoming node port.","live network, running 1.3.62",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jul/18 1:37 AM;mgbailey;BIGAWSUSEAST1-001.log.2018-07-09.gz;https://jira.hyperledger.org/secure/attachment/15336/BIGAWSUSEAST1-001.log.2018-07-09.gz","21/Jul/18 4:51 AM;mgbailey;Read_Ledger_BIG.zip;https://jira.hyperledger.org/secure/attachment/15353/Read_Ledger_BIG.zip","14/Jul/18 3:51 AM;mgbailey;ev1.log.tgz;https://jira.hyperledger.org/secure/attachment/15316/ev1.log.tgz","14/Jul/18 3:51 AM;mgbailey;ev1_domain_ledger.tgz;https://jira.hyperledger.org/secure/attachment/15315/ev1_domain_ledger.tgz","21/Jul/18 2:35 AM;mgbailey;ev1_pool_ledger.tgz;https://jira.hyperledger.org/secure/attachment/15348/ev1_pool_ledger.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,INDY-1032,,,No,,,No,,,"1|hzwxvz:",,No,,Unset,Not Required,EV 18.15 Stability/Availabilit,,,,,,,,,Unset,,,,,,,,,,,ashcherbakov,mgbailey,Toktar,,,,,,,,,,"16/Jul/18 10:17 PM;ashcherbakov;[~mgbailey] How did you run `read_ledger`? It outputs just first 100 txns by default.;;;","17/Jul/18 12:48 AM;mgbailey;[~ashcherbakov], read_ledger is working fine.  The issue is that the node that Evernym is the steward of, ev1, is not writing transactions to its local database that are successfully being written to the rest of the ledger.  This is true even after rebooting the node.;;;","19/Jul/18 6:53 PM;Toktar;After analyzing logs follow problems with nodes were found:
 prosovitor, NewtonD, OASFCU (doesn't take part in transaction ordering)
 iRespond, ibm (some time disconnected, doesn't send any messages)
 atbsovrin, ServerVS (has problem with BLS signature)
[~mgbailey], please, attach to ticket logs from one of other correct nodes (Stuard, BIGAWSUSEAST1-001, icenode, Aalto, DustStorm, zaValidator, digitalbazaar, esatus_AG, royal_sovrin, TNO, danube, pcValidator01) if it possible. One node will be enough.
 At this moment on the ev1 I see more than f (=6) faulty nodes and it's a reason to not ordered transactions.
 My recommendation is all pool restarting if this indy-node version contains command to restart pool for trustee. If pool restart is a difficult or wouldn't help, check problem with BLS signature on atbsovrin, ServerVS and ping other faulty nodes. Maybe they have problems with connection.;;;","20/Jul/18 1:37 AM;mgbailey;[~Toktar], I am attaching logs from BIGAWSUSEAST1-001 which should include the same time slice as the ev1 logs.[^BIGAWSUSEAST1-001.log.2018-07-09.gz];;;","20/Jul/18 9:04 PM;Toktar;[~mgbailey], thank you very much. We can recommend to check configuration of keys on nodes atbsovrin, ServerVS. 
For understanding this problem reason we need in domain and pool ledgers database for the node BIGAWSUSEAST1-001 and ev1. I apologize for the disturbance.;;;","21/Jul/18 2:34 AM;mgbailey;I have contacted the steward of the other node requesting ledgers, and am attaching those from ev1 now.  I am also reaching out to the stewards of atbsovrin and ServerVS regarding their node keys.

 ;;;","21/Jul/18 4:52 AM;mgbailey;The ledgers' contents from BIG are attached.;;;","24/Jul/18 7:55 AM;mgbailey;[~Toktar] I successfully tcp-pinged all nodes on their node port.  IBM does have a communications issue that I am aware of, but connectivity with all other nodes appears normal.  Have you learned anything additional with the ledger information from BIG?;;;","24/Jul/18 8:32 PM;Toktar;[~mgbailey], thank you! With the new ledger information we see ledgers were same in both nodes. It means nodes work correct but have problems with connection and BLS keys configuration.
 We recommend to check that the BLS key in the pool ledger database equal the key in /var/lib/indy/bls_keys/bls_pk on atbsovrin and ServerVS
 Besides, we think the pool restart command will fix bad connections between nodes. It is good that nodes tcp-pinged but it means nodes are available and busy with something. View Change for example, we fixed a lot of bugs in it in last time. We can found ""sleeping"" nodes problems by analyzing their logs if you could get it. If you need reestablish pool now, pool_restart should help.;;;","31/Jul/18 8:19 PM;ashcherbakov;[~mgbailey] Any info from other nodes? Are issues with BLS keys checked and fixed?;;;","01/Aug/18 8:09 AM;mgbailey;Status update:

prosovitor - emailed, but no response from steward yet

NewtonD - His ledger is up-to-date, but it is running 1.3.55 instead of 1.3.62.  I have sent him instructions for a manual update.

OASFCU - His domain ledger was behind.  He restarted the service on his node, and it caught up. No other obvious problems.

iRespond - His ledger is up-to-date. There appear to be no problems with this node.

ibm - This node is functional, but is not communicating with ev1 through our firewall since we set up an IP address whitelist for the node port. The ibm node is doing NAT on outgoing communications. A fix is expected the end of August. For now we have modified our firewall whitelist to account for the NAT.

atbsovrin - The BLS key on the node does not match the ledger entry. I'm working with the steward to correct this.

ServerVS - The BLS key on the node does not match the ledger entry. I'm working with the steward to correct this.

Unfortunately those last two may not be corrected immediately, since the ledger is not achieving consensus presently.

 ;;;","01/Aug/18 4:32 PM;ashcherbakov;If we need to recover consensus, we should probably update the Pool to the latest version (force Upgrade is required).;;;","02/Aug/18 5:52 AM;mgbailey;All nodes except for atbsovrin and ServerVS appear to be working, including 2 new validators that were added this week.  Consensus has been re-established.  I am continuing to reach out to those last 2 to get them to fix their BLS keys.;;;",,,,,,,,,,,,
Pool starts cyclic view change after 12 hours load,INDY-1477,31892,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,zhigunenko.dsr,zhigunenko.dsr,14/Jul/18 8:30 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"*Steps to Reproduce:*
1) prepare pool with 25 nodes and 100l txns
2) setup load - 2000 clients, 15 txns/sec write-only
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 22 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k schema
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k attrib
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
{code}

*Actual results:*
After ~12 hours pool performance started to degrade and 11 view_changes were happened one by one (no veiw_change happened before)

*Additional info:*
Logs are available on LogProcessor:logs/cyclic_view_change_14jul2018
Historical states of nodes are available in status_nodeX.log files",indy-node 1.4.500,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,INDY-1547,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,INDY-1032,,,No,,,No,,,"1|hzwxvr:",,,,Unset,Not Required,EV 18.15 Stability/Availabilit,,,,,,,,,Unset,,,,,,,,,,,ashcherbakov,sergey-shilov,zhigunenko.dsr,,,,,,,,,,"01/Aug/18 7:58 PM;sergey-shilov;*Current results of ticket research:*

View Changes reasons:
 ====================================================================================
 0 -> 1: PRIMARY_DISCONNECTED (reason 26)
 1 -> 2: PRIMARY_DEGRADED (reason 25)
 2 -> 3: PRIMARY_DEGRADED (reason 25)
 3 -> 4: PRIMARY_DEGRADED (reason 25)
 4 -> 5: INSTANCE_CHANGE_TIMEOUT (reason 28)
 5 -> 6: PRIMARY_DEGRADED (reason 25)
 6 -> 7: INSTANCE_CHANGE_TIMEOUT (reason 28)
 7 -> 8: PRIMARY_DISCONNECTED (reason 26)
 8 -> 9: INSTANCE_CHANGE_TIMEOUT (reason 28)
 9 -> 10: PRIMARY_DEGRADED (reason 25)
 10 -> 11: 16 - PRIMARY_DEGRADED (reason 25), 14 - INSTANCE_CHANGE_TIMEOUT (reason 28)
 ==================================================================================== 

After 11 view change several nodes (Node10, Node13, Node18, Node24, Node3, Node4) failed with ""No space left"" error approximately at the same time:
 ====================================================================================
 Node10 restart: Jul 14 10:12:27
 Node13 restart: Jul 14 10:12:39
 Node18 restart: Jul 14 10:12:44
 Node24 restart: Jul 14 10:12:32
 Node3 restart: Jul 14 10:12:28
 Node4 restart: Jul 14 10:12:26
 ====================================================================================

Considering several fallen behind nodes (Node15, Node17, Node21) that blacklisted other nodes (Sent transactions that could not be verified) the pool was in lost consensus state. Seems like it cause inconsistent uncommitted domain ledger root that occurred after 11 view change (committed roots are the same):
 ====================================================================================
 vldtr.node12: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node14: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node16: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node2: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node20: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node22: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node23: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node6: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'
 vldtr.node7: ""1"": b'zMfuFA7YzHSi1gYqRNrKUyUs97n56DDwxL71tHZMtqE'

vldtr.node10: ""1"": b'CnWAL84bgEeztexgCo2Fj79xc6kNZKyKTx9ki5jnrumD'
 vldtr.node13: ""1"": b'CnWAL84bgEeztexgCo2Fj79xc6kNZKyKTx9ki5jnrumD'
 vldtr.node18: ""1"": b'CnWAL84bgEeztexgCo2Fj79xc6kNZKyKTx9ki5jnrumD'
 vldtr.node24: ""1"": b'CnWAL84bgEeztexgCo2Fj79xc6kNZKyKTx9ki5jnrumD'
 vldtr.node3: ""1"": b'CnWAL84bgEeztexgCo2Fj79xc6kNZKyKTx9ki5jnrumD'
 vldtr.node4: ""1"": b'CnWAL84bgEeztexgCo2Fj79xc6kNZKyKTx9ki5jnrumD'
 ====================================================================================

All nodes from the second group are nodes that failed with ""No space left"" error (described above). The main difference between two groups is the fact that nodes from the second group flushed their last prepared certificate and uncommitted roots during restart:
 ====================================================================================
 2018-07-14 10:14:21.440000 | Node10:0 setting last prepared for master to None
 ====================================================================================

while nodes from the first group has it:
 ====================================================================================
 2018-07-14 10:04:31.480000 | Node12:0 setting last prepared for master to (7, 14)
 ====================================================================================

There were no any discards of Pre-Prepares after 11th view change and ""incorrect state trie"" log messages. Log level was INFO.

Last ordered transaction was in 5 view.

Unfortunately validator info and logs does not provide state roots.;;;","02/Aug/18 9:29 PM;ashcherbakov;The nodes crashed at 10:12 have Uncommitted State persisted, and it looks like they were not able to recover it.

It will be investigated and fix in the scope of INDY-1547;;;",,,,,,,,,,,,,,,,,,,,,,,
Pool stopped writing under 20txns/sec load,INDY-1478,31906,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,16/Jul/18 5:17 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"*Steps to Reproduce:*
1) Prepare new pool 
2) Run load test with 20 txns/sec writing in total, 1000 concurrent clients
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 22 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k schema
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k attrib
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
{code}

*Actual results:*
Pool stopped writing after 247k txns
Pool restores after service restart

*Additional info:*
Logs are available in _logs/20txns_15jul2018_
Logs after restart and historical node statuses are also available",indy-node 1.4.500,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,INDY-1032,,,No,,,No,,,"1|hzwxtj:",,,,Unset,Not Required,EV 18.15 Stability/Availabilit,,,,,,,,,Unset,,,,,,,,,,,anikitinDSR,Toktar,zhigunenko.dsr,,,,,,,,,,"18/Jul/18 7:28 PM;Toktar;On different nodes view changes switch about 25 viewNo to 0. It happened on most of nodes from 19:29 14 July to 00:25 15 July because more then f nodes restarted in this time.
 In the moment situation in logs looks like a problem with a connection then node tries to send message to another.
{code:java}
Remote Node16 is not connected - message will not be sent immediately.If this problem does not resolve itself - check your firewall settings{code};;;","20/Jul/18 11:47 PM;anikitinDSR;After log analizing was noted that (for example for Node15):
 # Node15 start up after ""out of memory"" crash
 # After getting LEDGER_STATUS from other nodes and check, that all ledgers are the same, compliting first catchup (getting LEDGER_STATUS for all of ledger), allLedgersCaughtUp was called. Then, _start_selection in on_cacthup_complete return error, because there is no any ViewChangeDone msgs. But after this, select_primaries() was called (view_change_in_progress is False now).
 # Therefore, because primaryName is None for all replicas, select_primaries method will elect new primary by round robin (Node1 will be primary). Primary was selected without view_change procedure. 
 # Then, propagate_primary logic will used for view_change and will need only f+1 view_change_done messages with propagated view_no and restarted node can complete view change before others;;;","25/Jul/18 8:10 PM;anikitinDSR;Also, after code analizing was found, that CurrentState message is building with viewNo (usually it's a proposed viewNo. For example when view change is in progress, then viewNo would be proposed/future viewNo instead of last completed). In that case, if node send  a CurrentState message, then it will include not last completed viewNo and in ViewChangeDone message too. The fix is using view_changer.last_completed_view_no instead of view_changer.view_no (or node.viewNo which is linked into view_changer);;;","27/Jul/18 5:56 PM;anikitinDSR;Reasons:
 * needs to fix CurrentState building and initial propagate primary logic

Changes:
 * For now, CurrentState messages are building using last_completed_view_no, instead of just viewNo (which is proposed)
 * Also, FutureViewChangeDone messages were divided to initial_propagate_primary and ViewChangeDone message for future view

Versions:
 * indy-node: 1.5.527

How to validate:
 * setup pool of 6 nodes
 * ensure, that current view is 0
 * stop 6'th node
 * force view_change
 * ensure that current view is 1
 * stop 5'th node
 * force view_change and ensure, that is not complete
 * start 6'th node and ensure, that last_completed_view_no would be set to 1;;;","03/Aug/18 4:37 PM;zhigunenko.dsr;This case cannot be validated manually by proposed steps.
There is the same autotest in source code, so it is reasonable to repeat load test and check the results;;;",,,,,,,,,,,,,,,,,,,,
dev env ledger (from master version) is crashing with add nym and add schema,INDY-1479,31907,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,rajeshkalaria,rajeshkalaria,rajeshkalaria,16/Jul/18 5:35 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"We are seeing some weird issue on our dev env ledger. So we are just trying to do add nym operation, and what it ends up is crashing 3 nodes with given exception. We are pointing to below mentioned master versions of node components:

indy-anoncreds                     1.0.32
 indy-node                              1.4.463
 indy-plenum                          1.4.406
 libindy-crypto                        0.4.0
 python3-indy-crypto            0.4.1

FYI: *This was working fine since last few days (at least might be a week as well), somehow today we are seeing this issue.*

{color:#ff0000}When we saw below error, we tried to confirm if all node processes are running or not and we found that only node1 was running, rest 3 nodes were already crashed.
{color}

------------------------------------------------------------

Traceback (most recent call last):
  File ""/usr/local/bin/start_indy_node"", line 19, in <module>
    client_ip=sys.argv[4], client_port=int(sys.argv[5]))
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 57, in run_node
    looper.run()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 264, in run
    return self.loop.run_until_complete(what)
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
    return future.result()
  File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
    raise self._exception
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
    result = coro.send(None)
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
    await self.runOnceNicely()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
    msgsProcessed = await self.prodAllOnce()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
    s += await n.prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 298, in prod
    c = await super().prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1033, in prod
    c += await self.serviceNodeMsgs(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1067, in serviceNodeMsgs
    await self.processNodeInBox()
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1637, in processNodeInBox
    await self.nodeMsgRouter.handle(m)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
    res = self.handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_req_processor.py"", line 51, in process_message_rep
    return handler.process(msg, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 63, in process
Traceback (most recent call last):
  File ""/usr/local/bin/start_indy_node"", line 19, in <module>
    return self.processor(valid_msg, params, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 238, in processor
    self.node.processPropagate(validated_msg, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2156, in processPropagate
    reqDict = msg.request
AttributeError: 'NoneType' object has no attribute 'request'
    client_ip=sys.argv[4], client_port=int(sys.argv[5]))
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 57, in run_node
    looper.run()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 264, in run
    return self.loop.run_until_complete(what)
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
    return future.result()
  File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
    raise self._exception
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
    result = coro.send(None)
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
    await self.runOnceNicely()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
    msgsProcessed = await self.prodAllOnce()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
    s += await n.prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 298, in prod
    c = await super().prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1033, in prod
    c += await self.serviceNodeMsgs(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1067, in serviceNodeMsgs
    await self.processNodeInBox()
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1637, in processNodeInBox
    await self.nodeMsgRouter.handle(m)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
    res = self.handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_req_processor.py"", line 51, in process_message_rep
    return handler.process(msg, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 63, in process
    return self.processor(valid_msg, params, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 238, in processor
    self.node.processPropagate(validated_msg, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2156, in processPropagate
    reqDict = msg.request
AttributeError: 'NoneType' object has no attribute 'request'
Traceback (most recent call last):
  File ""/usr/local/bin/start_indy_node"", line 19, in <module>
    client_ip=sys.argv[4], client_port=int(sys.argv[5]))
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 57, in run_node
    looper.run()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 264, in run
    return self.loop.run_until_complete(what)
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
    return future.result()
  File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
    raise self._exception
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
    result = coro.send(None)
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
    await self.runOnceNicely()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
    msgsProcessed = await self.prodAllOnce()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
    s += await n.prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 298, in prod
    c = await super().prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1033, in prod
    c += await self.serviceNodeMsgs(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1067, in serviceNodeMsgs
    await self.processNodeInBox()
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1637, in processNodeInBox
    await self.nodeMsgRouter.handle(m)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
    res = self.handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_req_processor.py"", line 51, in process_message_rep
    return handler.process(msg, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 63, in process
    return self.processor(valid_msg, params, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 238, in processor
    self.node.processPropagate(validated_msg, frm)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2156, in processPropagate
    reqDict = msg.request
AttributeError: 'NoneType' object has no attribute 'request'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,,,,No,,,No,,,"1|hzzipb:",,,,Unset,Not Required,,,,,,,,,,Unset,,,,,,,,,,,ashcherbakov,rajeshkalaria,,,,,,,,,,,"16/Jul/18 5:56 PM;ashcherbakov;It turned out that nodes were started not in a recommended way (as services), but manually. 
The exception means that multiple processes tries to access the same data.
We need to check that there are no processes left during re-starts.;;;","16/Jul/18 6:21 PM;ashcherbakov;So, after the update of the stacktrace, it looks like this is the same issue as https://jira.hyperledger.org/browse/INDY-1464.
This is fixed in IndyNode 1.4.500;;;","31/Jul/18 5:28 PM;ashcherbakov;[~rajeshkalaria]
Is the issue reproduced? can we close the ticket?;;;",,,,,,,,,,,,,,,,,,,,,,
Support latest SDK in Indy Plenum and Node,INDY-1480,31913,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,16/Jul/18 9:14 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"There are some breaking changes in the latest SDK related to Wallet API.

We need to support the changes in plenum and node tests to use the latest Master version of SDK.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,,,,No,,,No,,,"1|hzwxx3:",,,,Unset,Not Required,EV 18.15 Stability/Availabilit,,,,,,,,1.0,Unset,,,,,,,,,,,ashcherbakov,dsurnin,,,,,,,,,,,"31/Jul/18 8:31 PM;ashcherbakov;PRs:
https://github.com/hyperledger/indy-plenum/pull/814
https://github.com/hyperledger/indy-node/pull/850;;;",,,,,,,,,,,,,,,,,,,,,,,,
Do not calculate CRED_DEF's Keys in load script on every request,INDY-1481,31914,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,16/Jul/18 9:16 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"Creation of new CRED_DEF's keys may take a couple of seconds, which breaks the statistics for the load script.

We can create CRED_DEF's keys just once, and use the same keys when sending CRED_DEF txns.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,INDY-1368,,,No,,,No,,,"1|hzzhnz:",,,,Unset,Not Required,EV 18.15 Stability/Availabilit,,,,,,,,2.0,Unset,,,,,,,,,,,ashcherbakov,dsurnin,zhigunenko.dsr,,,,,,,,,,"18/Jul/18 12:52 AM;dsurnin;CRED_DEF is not recreating any more on each req generation
https://github.com/hyperledger/indy-node/pull/806;;;","25/Jul/18 11:37 PM;zhigunenko.dsr;*Environment:*
indy-node 1.5.521
libindy 1.5.0~648

*Actual results:*
_python3.5 perf_processes.py -g pool_transactions_genesis -c 1 -n 1 -t 0.5 -k cred_def_
Server Time: 592 Sent: 585 Succ: 585 Failed: 0 Nacked: 0 Rejected: 0 Clients Alive 0

_python3.5 perf_processes.py -g pool_transactions_genesis -c 1 -n 1 -t 1 -k cred_def_
Server Time: 1779 Sent: 1754 Succ: 1754 Failed: 0 Nacked: 0 Rejected: 0 Clients Alive 0

_python3.5 perf_processes.py -g pool_transactions_genesis -c 1 -n 1 -t 3 -k cred_def_
Server Time: 604 Sent: 202 Succ: 202 Failed: 0 Nacked: 0 Rejected: 0 Clients Alive 0

_python3.5 perf_processes.py -g pool_transactions_genesis -c 50 -n 1 -t 1 -m t -k cred_def_
Server Time: 705 Sent: 1225 Succ: 1213 Failed: 12 Nacked: 0 Rejected: 0 Clients Alive 0

Total throughput threshold is about 1 txns/sec for this transaction type, but it's enough for our current load testing;;;",,,,,,,,,,,,,,,,,,,,,,,
Provide feedback on recorder / replayer,INDY-1482,31917,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,16/Jul/18 10:03 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,test-automation,,,0,,,,"2 developers should complete the following tasks:
* Setup the recorder
* Do a recording
* Replay the recording
* Provide feedback
* Replay a recording provided by someone else

This effort will be limited to 2 days per developer, after which we will provide any feedback we have if we didn't get the recorder working.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,INDY-781,,,No,,,No,,,"1|hzzho7:",,,,Unset,Not Required,EV 18.15 Stability/Availabilit,,,,,,,,3.0,Unset,,,,,,,,,,,anikitinDSR,ashcherbakov,esplinr,spivachuk,,,,,,,,,"24/Jul/18 7:49 PM;spivachuk;_Below please find our discussion of the message recorder / replayer tool with_ [~ckochenower] _and_ [~devin-fisher] _(copied from berdyev6 channel in Slack):_

[~spivachuk]: @corin.kochenower @dfisher @lovesh I've sent you our questions on the message recorder / replayer tool by email. I'm also duplicating these questions here to share them with the team. Could you please clarify them?
 - Can nscapture be used for getting a record in case indy-node services was terminated abnormally, for example, in case of abrupt power-off or in case of out-of-memory?
 - Can nscapture be used for getting a record of multiple node sessions (start-stop / start-terminate cycles)? If yes, how does nsreplay replay them: as one session or as multiple sessions?
 - How to reproduce an issue occurred on a pool with a big count of nodes when it is unknown which node caused the issue or the issue was not caused by a particular node? In other words, how to reproduce the issue on a whole pool?
 - How to reproduce an issue on a node from a pool working under load which revealed after a significant period after start (for example, a week), taking into account that we do not know when the issue reason originated? Do we need to replay the captured record during the same period?

In general, most of issues that we face occur on a large pool (25 nodes) during load testing running for hours or days and writing multiple thousands (and sometimes millions) of transaction to the ledger. When an issue occurs, it is unknown / unclear at what time and on what node(s) it occurred. Replaying all the records of all 25 nodes running for days can take months. How can the message recorder / replayer tool be efficiently used in such cases?
 Or is it assumed that this tool is used for dealing with bugs occurred only on not large pools and after not long period after start?
 Can you please advise what is the expected usage of the recorder / replayer tool?

[~ckochenower]: Very good questions @nikita.spivachuk.

Q: - Can nscapture be used for getting a record in case indy-node services was terminated abnormally, for example, in case of abrupt power-off or in case of out-of-memory?

A: It sounds like you would like to debug (root cause analysis) ""abend"" and ""abrupt power-off"" scenarios with nsreplay. Debugging the ""abend"" scenario will be possible, if we can figure out how to get the operating system to call nscapture when indy-node is terminated abnormally. Perhaps the systemd script can be made smart enough to know to call nscapture before subsequent startup.

I'm not sure the OS can help us call nscapture in an ""abrupt"" power-off scenario.

Q: Can nscapture be used for getting a record of multiple node sessions (start-stop / start-terminate cycles)? If yes, how does nsreplay replay them: as one session or as multiple sessions?

A: Yes. Currently, nsreplay requires the indy-node service to be stopped before calling it. The recording persists across restarts. Every subsequent nscapture archive will be a superset of the previous ones. In other words, the last nscapture archive created will contain all messages from the time STACK_COMPANION = 1 was added to /etc/indy/indy_config.py

Q: How to reproduce an issue occurred on a pool with a big count of nodes when it is unknown which node caused the issue or the issue was not caused by a particular node? In other words, how to reproduce the issue on a whole pool?

A: Each node in the cluster is capable of recording it's own messages. Adding features like ""skip idle time"", ""replay multiplier (sped up replay)"", ""replay from a snapshot"", ""replay the last N transactions/messages"" will hopefully reduce months to days, hours, or minutes.

Q: How to reproduce an issue on a node from a pool working under load which revealed after a significant period after start (for example, a week), taking into account that we do not know when the issue reason originated? Do we need to replay the captured record during the same period?

A: The answer to the previous question also applies to this question.

The ""capture state, then replay to find root cause of a problem"" approach, seems to be analogous to explaining symptoms to a doctor and asking the doctor to diagnose the problem.

Hopefully, we are building in self-diagnosing and self-healing features (I think we are) for simple failures/faults.

The capture and replay tools seem more appropriate for the very difficult to diagnose problems.

If a patient has heart arrhythmia, a doctor will perform either a chemical stress test or a physical stress test to hopefully cause a ""cardiac event"" in a controlled environment where the patient can be monitored and quickly treated (QA stress and system tests). However, conditions like heart arrhythmia aren't easily triggered by these conventional ""stress tests"" and the doctor has the patient wear a ""recorder"" that monitors his/her vitals constantly day and night. Once the patient experiences a ""cardiac event"", the recording is then shared with the doctor and the patient helps the doctor pinpoint when the event happened.

I hope our patient, indy-node, will tell us when these sometimes hard to diagnose problems happen. Then we can replay that part of the recording and ignore the rest.

In other words, indy-node needs to experience an anomaly, we make indy-node as smart as we can to recognize symptoms associated the anomaly (abnormal heart beat, blood pressure, temperature, blood oxygen level), enable the recorder, and then have indy-node tell us when it experiences the symptoms again.

Perhaps we need to record indy-node ""vitals"" to help us pinpoint anomalies?

@dfisher - Do my answers *^* align with your thoughts?

[~devin-fisher]: Corin thoughts are good. But I have a few of my own.

Frist, nscapture is a simple script that captures into an archive file the state of the node. Should be run when the node is stoped. The function of the script is mostly to discover what files need to be captured and to run the required tar function to archive them. nscapture captures only one node's data. Additionally, it can only capture data that is on the disk. So, if the recorder was not activated with the config setting, a replay will not be possible. Although, nscaputure can grab the logs, configuration and data files.

1) The recorder will messages and signals for a long as the node is running. The signals have to be trapable. I'm not sure (Lovesh would know more), but I'm guessing that only SIGTERM signal is traped. On the replay, the SIGTERM will be simulated (see the below info for more detail). But It is likely not to be a very interesting replay if the messages don't lead to the SIGTERM.

If the signal is not trapable, like SIGKIL from the OM killer, the recorder will record messages up to the point of being killed by the OS. The replay will likely not be of much use unless the sequence of messages lead to the conditions that caused the OS to kill the process. Similarly, in the abrupt power-off, the recorder will record all messages up to the power loss (assuming that the data had been flushed to disk in the recorder).

2) The recorder does record the node sessions. There is a file called 'start_times' that records when these events happen. The start and stops are simulated by removing the node from the lopper. And a new node object is created and added back the looper and execution is resumed. This process is time to happen in a similar timing as was seen by the recorder.

3) Each node is recorded individually and so the number of nodes does not change much. The larger the cluster will cause more messages to be recorded since the size of the pool affect the number of redundant messages sent and also the number of replicas that are run. Running a recording on a large pool should be totally possible.

If all nodes are recording, the choice of which node to replay can be decided after the fault is detected (or pool failure).

4) Currently, the recorder and replayer are limited in their ability to replay long recordings. We had hoped to find a way to snapshot the recording but that proved too difficult for our small team.

Generally, if you think that the only issue we will be diagnosting will take days to weeks to trigger, then the recorder and replayer in their current form will not be that useful. The recorder and replayer should work fine for pools with a large number of nodes and for heavy load scenarios.

[~ashcherbakov]: Thanks for the answers.
 Actually for me it's still not clear how to use the tools for large number of nodes and heavy load scenarios.
{quote}If all nodes are recording, the choice of which node to replay can be decided after the fault is detected (or pool failure).
{quote}
This is usually the main unknown we have to detect. And it looks like it's hard to do it with Recorder/Replayer in the current state...

[~spivachuk]: @corin.kochenower @dfisher
 Thank you for the answers.

Let me share our thoughts about the features intended for reducing replay time. The checkpoints feature would be useful in cases when we know the time frame when the issue reason originated.

But the replay speed multiplier feature does not seem to be useful because a significant part of incoming messages are sent during a dialog of the node with other nodes. So the speed of the message workflow depends on the node performance, among other things. If the replay is sped up, the order of events may be broken.;;;","24/Jul/18 9:26 PM;spivachuk;Completed the steps listed in the ticket description. Found several issues with the message recorder / replayer tool and discussed them with Berdyaev6 team in Slack.

Provided our feedback on the message recorder / replayer tool with a number of questions and discussed them with Berdyaev6 team (this discussion has been copied from Slack to the previous comment to this ticket).;;;","24/Jul/18 9:42 PM;anikitinDSR;As for me, i ran nsreplay for reproducing problems in INDY-1470. Some problems was described in comments for INDY-1470:
https://jira.hyperledger.org/browse/INDY-1470?focusedCommentId=47616&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-47616
and
https://jira.hyperledger.org/browse/INDY-1470?focusedCommentId=47619&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-47619

Also,  i were chatting with team in the berdyaev6 channel in the slack.;;;","25/Jul/18 4:17 PM;ashcherbakov;My feedback:
1) I don't see how Replayer (even with snapshot feature) can be the only tool to find the issue. We simply don't know when and on what nodes the issues happened until run the Replayer.
However, Replayer (with snapshot feature) can be an additional tool used *after* initial analysis of logs and validator-info input.
2) I don't understand how can I easily analyse a problem (on my dev machine) if it occurred on all 25 nodes. I need to run Replayer at least 25 time (for each node), and it can be that I will need to run it multiple times for each node. 
3) The requirement for Steward machine will be increased, and can be about 32 GB of RAM.
In order to run Replayer on my dev machine and not fail with OutOfMemory, I will have to have even more than 32 GB RAM.
4) I'm not sure that snapshot feature can be easily implemented.

So, I think that Recorder/Replayer can be useful tools in some cases (especially in combination with chaos testing).
But I'm not sure that it can be the primary tool for analysis of issues on large pools and heavy scenarios. However, it can be used in addition (after initial analysis of logs and validator info output);;;",,,,,,,,,,,,,,,,,,,,,
Benchmark performance impact of recorder tool,INDY-1483,31918,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,esplinr,esplinr,16/Jul/18 10:08 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,test-automation,,,0,,,,"*Story*
As a developer fixing issues with Indy Node, I want to understand the performance impact of running the recording tool so that I can deploy the recorder in situations where it will be helpful and not prevent my addressing issues.

*Acceptance Criteria*
* Reproduce the test in INDY-1343 with the recorder running, and compare the results with what we saw without the recorder running.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1343,INDY-2214,,,,,,,,,,,,,,,,,,,"18/Jul/18 4:19 PM;zhigunenko.dsr;chart.png;https://jira.hyperledger.org/secure/attachment/15329/chart.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Not Required,Unset,Not Required,,INDY-781,,,No,,,No,,,"1|hzwxuf:",,,,Unset,Not Required,EV 18.14 Monitoring/Stability,,,,,,,,3.0,Unset,,,,,,,,,,,esplinr,zhigunenko.dsr,,,,,,,,,,,"16/Jul/18 11:41 PM;zhigunenko.dsr;*Steps to Validate:*
1. Repeat test from [this case|https://jira.hyperledger.org/browse/INDY-1343?focusedCommentId=47359&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-47359] with STACK_COMPANION=0
2. Compare stats with cyclic_view_change_14jul2018/status_nodeX.log files

Monitoring tool source (run it every 10 minutes)
{code}
DATE=$(date '+%Y-%m-%d %H:%M:%S')
SIZE=$(sudo read_ledger --type=domain --count)
LAST=$(sudo validator-info -v | grep Last_complete_view_no | cut -c 11-)
PROGRESS=$(sudo validator-info -v | grep VC_in_progress | cut -c 11-)
START=$(sudo validator-info -v | grep Last_view_change_started_at | cut -c 11-)
VNO=$(sudo validator-info -v | grep View_No | cut -c 11-)
UPTIME=$(sudo systemctl status indy-node | grep running | cut -c 19-)
RAM=$(vmstat -s | grep ""free memory"" | cut -c 6-)
DATA=$(du -sh /var/lib/indy/sandbox/data/)
SPACE=$(df -h | grep xvda1)
echo -e ""$DATE\t$SIZE\t$LAST\t$PROGRESS\t$START\t$VNO\t$UPTIME\t$RAM\t$DATA\t $SPACE""
{code};;;","18/Jul/18 4:19 PM;zhigunenko.dsr;[~esplinr] [~ckochenower]
There is no measurable performance impact when recording 15 transactions per second between _STACK_COMPANION=1_ and _#STACK_COMPANION=1_
 !chart.png|thumbnail!
*Load setup:*
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 22 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k schema
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k attrib
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
{code}
*Environment:*
indy-node 1.4.500
libindy 1.5.610;;;","21/Jul/18 1:17 AM;esplinr;Thank you for doing this analysis Nikita.;;;",,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_multiple_view_change_retries_by_timeouts ,INDY-1484,31949,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,17/Jul/18 6:04 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,,0,,,,https://ci.evernym.com/job/Indy-Plenum/job/indy-plenum-verify-x86_64/1881/artifact/test-result-plenum-1.ubuntu-03.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/18 6:05 PM;ashcherbakov;test-result-plenum-1.ubuntu-03.txt;https://jira.hyperledger.org/secure/attachment/15322/test-result-plenum-1.ubuntu-03.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1488,,,No,,,No,,,"1|hzzivj:",,,,Unset,Unset,EV 18.14 Monitoring/Stability,,,,,,,,1.0,Unset,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"17/Jul/18 8:24 PM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/817;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_recover_stop_primaries_no_view_change,INDY-1485,31950,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,17/Jul/18 6:07 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,https://ci.evernym.com/job/Indy-Plenum/job/indy-plenum-verify-x86_64/1873/artifact/test-result-plenum-3.ubuntu-25.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/18 10:09 PM;sergey.khoroshavin;test-result-plenum-3.ubuntu-13.txt.xz;https://jira.hyperledger.org/secure/attachment/15360/test-result-plenum-3.ubuntu-13.txt.xz","17/Jul/18 6:07 PM;ashcherbakov;test-result-plenum-3.ubuntu-25.txt.zip;https://jira.hyperledger.org/secure/attachment/15323/test-result-plenum-3.ubuntu-25.txt.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1488,,,No,,,No,,,"1|hzzipj:",,,,Unset,Unset,,,,,,,,,,Unset,,,,,,,,,,,ashcherbakov,esplinr,sergey.khoroshavin,,,,,,,,,,"23/Jul/18 10:07 PM;sergey.khoroshavin;https://ci.evernym.com/job/Indy-Plenum/job/indy-plenum-verify-x86_64/1926/artifact/test-result-plenum-3.ubuntu-13.txt;;;","11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_new_node_catchup_plugin_ledger,INDY-1486,31951,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,17/Jul/18 6:10 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1320/artifact/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-1572.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/18 6:10 PM;ashcherbakov;test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-1572.txt;https://jira.hyperledger.org/secure/attachment/15324/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-1572.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1488,,,No,,,No,,,"1|hzzhov:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,,1.0,Unset,,,,,,,,,,,ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"20/Jul/18 12:16 AM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/828;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_restart_groups_6_of_7_wp_tm,INDY-1487,31952,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,17/Jul/18 6:11 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1325/artifact/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-1627.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/18 6:11 PM;ashcherbakov;test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-1627.txt.zip;https://jira.hyperledger.org/secure/attachment/15325/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-1627.txt.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1488,,,No,,,No,,,"1|hzzivr:",,,,Unset,Unset,EV 18.14 Monitoring/Stability,,,,,,,,1.0,Unset,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermmitent test failures on Jenkins,INDY-1488,31953,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,17/Jul/18 6:12 PM,11/Oct/19 7:23 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,ghx-label-8,,Intermmitent test failures,Done,No,,,No,,,"1|hzziv3:",,,,Unset,Unset,,,,,,,,,,Unset,,,,,,,,,,,ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool stopped working under load without view changes,INDY-1489,31956,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ozheregelya,ozheregelya,17/Jul/18 8:08 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"*Steps to Reproduce:*
 1. Restore the pool from backup [https://s3-us-west-2.amazonaws.com/qanodelogs/backups/qa_large/node1.7z
 3. Start the load test with all types of writing txns.

*Actual Results:*
 Pool was writing during ~5 hours with throughput ~13txns/sec. 9 of 25 nodes were lagged. No View Changes were happened. Domain ledger sizes:
 Node4,9,10,17,21,24: 752253
 Node5: 732253
 Node7: 792253
 Node12,23: 782253
 Node25: 675282

*Expected Results:*
 Pool should work.

Logs: [https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1489]

*Case 2:*
 Looks like the same issue: [https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1489-2]
 The only difference is that there are more than f+1 node is out of sync (Only 13 of 25 nodes have the same amount of domain txns).
UPD: case 2 was reproduced not with default config:
{code:java}
MAX_CONNECTED_CLIENTS_NUM = 250 # connections limit
MIN_STACK_RESTART_TIMEOUT = 30 # seconds
MAX_STACK_RESTART_TIME_DEVIATION = 10 # seconds
TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True
CLIENT_STACK_RESTART_ENABLED = True{code}","AWS pool of 25 nodes (QA Live), default config
 indy-node 1.4.504
libindy 1.5.0~613",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1032,,,No,,,No,,,"1|hzzhn3:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,,,Unset,,,,,,,,,,,ozheregelya,Toktar,,,,,,,,,,,"25/Jul/18 6:38 PM;Toktar;*Case 1:*

+Reason of the break:+

From 23:31 to 00:00 all nodes restarted because can't allocate memory.
 The gap between nodes restarting was insufficient to ending catchup.

Explored in INDY-1493

+Reason for non-recovery:+

Logs are not enough. We can see messages with processing view_change_done but can not see with sending because have not logs between 22:25 and 22:44.
 On ViewChange 12 -> 14 most of the nodes lost logs. In this time they sent view_change_done, but after 22:44 they are not in a view change and have not a primary. 

*Case 2:*

+Reason of the break:+

18 Jul from 16:34 to 17:25 all nodes restarted with ZMQ error ""Address already in use"".
 The gap between nodes restarting was insufficient to ending catchup.

Fixed in INDY-1431

+Reason for non-recovery:+

On 17:33 load test was switch off and pool had no an opportunity to recover.;;;",,,,,,,,,,,,,,,,,,,,,,,,
wallet already exists error observed on calling importWallet api,INDY-1490,31957,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,viswa0269,viswa0269,17/Jul/18 8:30 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"getting the error on calling importWallet api


07-17 16:08:42.632 4336-4777/com.connectme D/RNIndy::: importWallet() called with: importPath = [/data/user/0/com.connectme/files/restoreDirectory/Encrypted-ConnectMe-Wallet.wallet], encryptionKey = [f7b3b6720572f8aa], promise = [com.facebook.react.bridge.PromiseImpl@b4e229]
07-17 16:08:42.635 4336-4777/com.connectme D/JAVA_WRAPPER:VCX_JAVA: checkResult: 0
07-17 16:08:42.635 4336-4863/com.connectme I/vcx::api::wallet: vcx_wallet_import(command_handle: 4, path: ""/data/user/0/com.connectme/files/restoreDirectory/Encrypted-ConnectMe-Wallet.wallet"", backup_key: ****)
07-17 16:08:42.637 4336-4863/com.connectme V/indy::api::wallet: indy_import_wallet: >>> pool_name: 0xc80575d0, name: 0xc80575f0, storage_type: 0x0, config: 0x0, credentials: 0xc8127ea0, import_config: 0xc56c6b80
 indy_import_wallet: entities >>> pool_name: ""poolName"", name: ""walletName"", storage_type: None, config: None, credentials: ""\{\""key\"":\""walletKey\"",\""storage\"":\""\{}\""}"", import_config: ""\{\""key\"":\""f7b3b6720572f8aa\"",\""path\"":\""/data/user/0/com.connectme/files/restoreDirectory/Encrypted-ConnectMe-Wallet.wallet\""}""
 indy_import_wallet: <<< res: Success
07-17 16:08:42.637 4336-4814/com.connectme I/indy::commands: WalletCommand command received
07-17 16:08:42.637 4336-4814/com.connectme I/indy::commands::wallet: Import command received
07-17 16:08:42.637 4336-4814/com.connectme D/indy::commands::wallet: import >>> pool_name: ""poolName"", name: ""walletName"", storage_type: None, config: None, credentials: ""\{\""key\"":\""walletKey\"",\""storage\"":\""\{}\""}"", import_config: ""\{\""key\"":\""f7b3b6720572f8aa\"",\""path\"":\""/data/user/0/com.connectme/files/restoreDirectory/Encrypted-ConnectMe-Wallet.wallet\""}""
07-17 16:08:42.637 4336-4814/com.connectme V/indy::services::wallet: create_wallet >>> pool_name: ""poolName"", name: ""walletName"", storage_type: None, storage_config: None, credentials: ""\{\""key\"":\""walletKey\"",\""storage\"":\""\{}\""}""
07-17 16:08:42.637 4336-4814/com.connectme E/indy::errors::indy: Casting error to ErrorCode: Wallet with this name already exists: walletName",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,,,,No,,,No,,,"1|hzzivb:",,,,Unset,Unset,,,,,,,,,,Unset,,,,,,,,,,,viswa0269,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a Trustee running POOL_UPGRADE txn, I need to specify any package depending on indy-node, so that the package with the dependencies get upgraded",INDY-1491,31969,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,17/Jul/18 11:12 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"As of now POOL_UPGRADE txn upgrades IndyNode only with the dependencies. 
We need to be able to use the txn to upgrade the products based on IndyNode.

See a PoA in [https://docs.google.com/document/d/1_sVPtltsTZqP1ZK_wimrJuY8-Emc89TwYT51m0OZxF4/edit#heading=h.z1u0dzf62loi:]
 * Extend the POOL_UPGRADE txn with the (optional) package name (assume indy-node if not present)
 * get the list of dependencies for the specified package (apt-cache can be used on Ubuntu)

 * If it sees indy-node there, it get the list of dependencies for indy-node as well (use existing logic).

 * As an output, it creates a list of the packages with dependencies to be installed in the correct order.
For example,
[indy-plenum=1.4.4, indy-node=1.4.1, plugin1=1.0.1, plugin2=1.0.2, specified_package=2.0.0]",,,,,,,,,,,,,,,,,,,,,,,,INDY-1500,INDY-1499,,,,,,,,,IS-860,INDY-1579,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,,,,No,,,No,,,"1|hzwxvj:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,EV 18.16 Releasing 1.6,,,,,,,5.0,Unset,,,,,,,,,,,ashcherbakov,dsurnin,VladimirWork,,,,,,,,,,"24/Jul/18 6:57 PM;dsurnin;After team discussion we decided to add a target packet name in config - indy-node or sovrin. Current POOL_UPGRADE command syntax will not be modified.

PoA updated the same way ;;;","01/Aug/18 4:24 PM;ashcherbakov;We will also need a migration script that should set the config parameter for top-level package to Sovrin.;;;","01/Aug/18 11:27 PM;dsurnin;PRs
https://github.com/sovrin-foundation/sovrin/pull/77
https://github.com/hyperledger/indy-node/pull/843

for now pool_upgrade command will update packages starting from the top level package from the config parameter UPGRADE_ENTRY
for node default is ""indy-node""
for sovrin default is ""sovrin""

also sovrin pkg now depends from particular node version

for QA:
1
check that clean indy-node upgrade (without sovrin) is wrorking

2
install old sovrin
cmd pool_upgrade requires indy-node version
run pool_upgrade to upgrade indy-node to version >= 536
two case: manually install sovrin or in indy node config add UPGRADE_ENTRY = 'sovrin'
now pool_upgrade command requires sovrin package version
run pool_upgrade to latest and check that all the packages updated (plenum, node, sovrin)
      
;;;","03/Aug/18 10:07 PM;VladimirWork;Ticket is waiting for additional fixes.;;;","08/Aug/18 5:59 PM;dsurnin;New optional param ""package"" is added to upgrade command.
Package mentioned in ""package"" parameter must be installed and must has node as a dependency.
""version"" param should correspond to package being updated.
In case of ""indy-node"" upgrade any top level packages that has node as a dependency are not updated.
If param is omitted ""indy-node"" is assumed.
Old cli also modified to support new param.
PR: https://github.com/hyperledger/indy-node/pull/869;;;","10/Aug/18 7:01 PM;VladimirWork;Build Info:
sovrin 1.1.67
indy-node 1.5.555

Steps to Reproduce:
1. Send pool upgrade command with reinstall via old CLI:
{noformat}
send POOL_UPGRADE name=upgrade67cancel version=1.1.67 sha256=ed0a366b4ef36d40c055672a8b83679e99246fec71a706b4ae4cb7958feace3f action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pL
hPhv':'2018-08-10T10:00:00.258870+00:00','8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-08-10T10:05:00.258870+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya': '2018-08-10T10:10:00.258870+00:
00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA': '2018-08-10T10:15:00.258870+00:00'} timeout=10 force=False reinstall=True package=sovrin
Sending pool upgrade upgrade67cancel for version 1.1.67
{noformat}
2. Try to cancel it via old CLI:
{noformat}
send POOL_UPGRADE name=upgrade67cancel version=1.1.67 sha256=ed0a366b4ef36d40c055672a8b83679e99246fec71a706b4ae4cb7958feace3f action=cancel schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6p
LhPhv':'2018-08-10T10:00:00.258870+00:00','8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-08-10T10:05:00.258870+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya': '2018-08-10T10:10:00.258870+00
:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA': '2018-08-10T10:15:00.258870+00:00'} timeout=10 force=False reinstall=True package=sovrin
{noformat}

Actual Result:
{color:red}Pool upgrade failed: client request invalid: InvalidClientRequest('Version is not upgradable',){color}

Expected Results:
Pool upgrade should be cancelled.

Additioanl Info:
We have the same behavior with indy-node upgrade cancelling with the same version (e.g. 1.5.555 -> 1.5.555).;;;","10/Aug/18 8:27 PM;dsurnin;this it old cli issue 
PR
https://github.com/hyperledger/indy-node/pull/887;;;","11/Aug/18 7:50 PM;VladimirWork;Build Info:
indy-node 1.6.70
sovrin 1.1.13

Steps to Validate:
1. Send pool upgrade command with reinstall via old CLI:
{noformat}
send POOL_UPGRADE name=upgrade10_13_re version=1.1.13 sha256=ed0a366b4ef36d40c055672a8b83679e99246fec71a706b4ae4cb7958feace3f action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv':'2018-08-11T10:45:00.258870+00:00','8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-08-11T10:50:00.258870+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya': '2018-08-11T10:55:00.258870+00:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA': '2018-08-11T11:00:00.258870+00:00'} timeout=10 force=False reinstall=True package=sovrin
{noformat}

2. Try to cancel it via old CLI:
{noformat}
send POOL_UPGRADE name=upgrade10_13_re version=1.1.13 sha256=ed0a366b4ef36d40c055672a8b83679e99246fec71a706b4ae4cb7958feace3f action=cancel schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv':'2018-08-11T10:45:00.258870+00:00','8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-08-11T10:50:00.258870+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya': '2018-08-11T10:55:00.258870+00:00', '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA': '2018-08-11T11:00:00.258870+00:00'} timeout=10 force=False reinstall=True package=sovrin
{noformat}


Actual Result:
Pool upgrade is cancelled successfully.

Additional Info:
Regression testing of upgrade procedure also was performed in scope of this ticket against both indy-node and sovrin packages:
- force=False upgrades
- force=True upgrades
- reinstallation
- cancellation
- downgrading (forbidden)
- upgrade to nonexistent version (rollback)
- upgrade indy-node *without* sovrin installed
- upgrade indy-node *with* sovrin installed (rollback)
- upgrade logging
- archive backups;;;",,,,,,,,,,,,,,,,,
Create 1.6 Release,INDY-1492,31976,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,18/Jul/18 12:04 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,,,,No,,,No,,,"1|hzwxuv:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,,3.0,Unset,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"10/Aug/18 4:21 PM;ashcherbakov;Indy-Node RC 1.6.69 is issued.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Memory leaks profiling,INDY-1493,32008,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,zhigunenko.dsr,zhigunenko.dsr,18/Jul/18 6:58 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6,,,,0,,,,"_indy-node.service_ execution increases memory consumption, that leads to OOM node restart, that also increase other problems probability

*Acceptance Criteria*
* memory leak points have been found
* any problems found will be logged in JIRA as separate issues for independent prioritization
* load test shouldn't indicate stable memory consumption increase",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-223,,,,,INDY-1546,,,,,,,,,,,,,,,"18/Jul/18 10:53 PM;zhigunenko.dsr;after.png;https://jira.hyperledger.org/secure/attachment/15332/after.png","18/Jul/18 10:53 PM;zhigunenko.dsr;before.png;https://jira.hyperledger.org/secure/attachment/15331/before.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwxw7:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,,5.0,Unset,,,,,,,,,,,anikitinDSR,ashcherbakov,zhigunenko.dsr,,,,,,,,,,"18/Jul/18 10:48 PM;zhigunenko.dsr;||types ||   № objects ||   total size||
|                                          <class 'str |      630578 |     71.93 MB |
|                                         <class 'dict |      200976 |     55.78 MB |
|                                        <class 'tuple |      141700 |     13.78 MB |
|                                         <class 'list |       21904 |     11.25 MB |
|                <class 'indy_common.types.SafeRequest |      133809 |      7.15 MB |
|                                          <class 'int |      142333 |      4.83 MB |
|                                          <class 'set |        5363 |      2.00 MB |
|                                         <class 'type |        1895 |      1.91 MB |
|                                         <class 'code |       13584 |      1.87 MB |
|                      <class 'collections.OrderedDict |        1598 |      1.37 MB |
|            <class 'plenum.server.propagator.Requests |           1 |      1.18 MB |
|                 <class 'orderedset._orderedset.entry |       16130 |   1008.12 KB |
|                                      <class 'weakref |        5363 |    418.98 KB |
|            <class 'plenum.server.propagator.ReqState |        5754 |    314.67 KB |
|                <class 'plenum.common.request.Request |        5754 |    314.67 KB |
|                                  <class 'abc.ABCMeta |         274 |    281.45 KB |
|       <class 'sortedcontainers.sorteddict.SortedDict |          37 |    214.23 KB |
|                           <class 'typing.GenericMeta |         218 |    209.17 KB |
|                                   function (_remove) |        1512 |    200.81 KB |
|                            <class 'getset_descriptor |        2018 |    141.89 KB |
|                                  function (__init__) |        1054 |    139.98 KB |
|                                    <class 'frozenset |         250 |    112.69 KB |
|                 <class 'plenum.server.models.Commits |           9 |    108.98 KB |
|                <class 'plenum.server.models.Prepares |           9 |    108.98 KB |
|                           <class 'wrapper_descriptor |        1276 |     99.69 KB |
|                   <class 'builtin_function_or_method |        1260 |     88.59 KB |
|                            <class 'method_descriptor |        1231 |     86.55 KB |
|                             <class '_sre.SRE_Pattern |         196 |     84.66 KB |
|                          <class '_weakrefset.WeakSet |        1512 |     82.69 KB |
|                            <class 'collections.deque |         131 |     80.85 KB |
|                                     <class 'property |         981 |     76.64 KB |
|                             <class 'typing.UnionMeta |          71 |     70.45 KB |
|               <class 'intervaltree.interval.Interval |        1000 |     70.31 KB |
|         <class 'plenum.server.models.ThreePhaseVotes |         985 |     61.56 KB |
|                       <class 'intervaltree.node.Node |        1000 |     54.69 KB |
|                                       <class 'method |         823 |     51.44 KB |
|                 <class '_frozen_importlib.ModuleSpec |         852 |     46.59 KB |
|                                        <class 'float |        1775 |     41.60 KB |
|                                  function (__repr__) |         296 |     39.31 KB |
|                                <class 'enum.EnumMeta |          38 |     37.98 KB |
|                             <class 'typing.TupleMeta |          40 |     34.69 KB |
|                                         <class 'cell |         716 |     33.56 KB |
|  <class '_frozen_importlib_external.SourceFileLoader |         611 |     33.41 KB |
|                            <class 'member_descriptor |         457 |     32.13 KB |
|      plenum.common.messages.node_messages.PrePrepare |         492 |     26.91 KB |
|          plenum.common.messages.node_messages.Commit |         492 |     26.91 KB |
|         plenum.common.messages.node_messages.Prepare |         492 |     26.91 KB |
|                        <class '_ctypes.PyCSimpleType |          26 |     25.80 KB |
|                                        <class 'bytes |         429 |     25.00 KB |
|                                      <class 'StgDict |          41 |     23.86 KB |

Memory consumption after 100k writings
!before.png!  
Memory consumption after service restart
!after.png!;;;","31/Jul/18 7:25 PM;anikitinDSR;After several experiments was noticed that:
 # Memory usage of internal python's structure is not high. During load it was around 1Gb (string, dicts and list is in the top) with all RAM usage by python process is about 5-6 Gb
 # Also, in process ""start_indy_node"" we have threads with rocksdb and zeroMQ. Rocksdb use in-memory conception and store all data in RAM by blocks. In other words, under load, it can allocate too much RAM for internal usage.
 # ZeroMQ get data from socket buffers, store it into internal queue and return to python layer when it needed. Therefore, we need to increase count of extracted packages or limit size of internal zeroMQ queues.
 # Potentially, after allLedgersCaughtUp internal queue with preprepares, sentPreprepares, prepares, commit and etc will be cleaned only if replica isMaster or if not isPrimary. In other words, for backup replicas, in the case if this replica is primary, internal queues was not cleaned.;;;","02/Aug/18 6:05 PM;anikitinDSR;Status of last experiments:
 # In case with sending ""all txns (NYM, ATTRIB, SCHEMA, CLAIM_DEF, REVOC_REG_DEF, REVOC_ENTRY)"" we got ""out of memory"" errors after ~ 30k txns. RAM was not freed after stopping load test. Internal structures, like dicts, lists, str were taken ~ 2GB from ~7Gb for whole indy-node's process.
 # In case with sendin ""only revocations"" memory usage was similar as in step 1.
 # In case with sending ""all txns except revocations (REVOC_REG_DEF, REVOC_ENTRY, read/write)"" memory usage is acceptable. After about 30 k txns rss usage was ~400 Mb.

Therefore, we need to continue investigating memory usage problems in case with revocation transactions.;;;","02/Aug/18 6:10 PM;ashcherbakov;Further investigation will be continued in the scope of INDY-1546;;;",,,,,,,,,,,,,,,,,,,,,
Allow optional field in node-to-node and client-to-node,INDY-1494,32009,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,18/Jul/18 7:36 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"As of now the validation schema for all messages is strict and doesn't allow new (optional) fields to be included at the end.

As a long-term we should consider using protocolVersion, but for now it should be enough just to allow new (unknown) fields, so that old nodes don't break.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Unset,Unset,Unset,,INDY-1374,,,No,,Unset,No,,,"1|hzwxvb:",,,,Unset,Unset,EV 18.14 Monitoring/Stability,,,,,,,,2.0,Unset,,,,,,,,,,,ashcherbakov,,,,,,,,,,,,"18/Jul/18 10:33 PM;ashcherbakov;PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/816]
 * [https://github.com/hyperledger/indy-node/pull/826]

 

Validation will be done in the scope of INDY-1343 (just a confirmation that nothing is broken);;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_new_node_accepts_timestamp ,INDY-1495,32054,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,19/Jul/18 5:11 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/18 5:11 PM;ashcherbakov;test-result-plenum-3.ubuntu-24.txt.zip;https://jira.hyperledger.org/secure/attachment/15333/test-result-plenum-3.ubuntu-24.txt.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzj73:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Enable TRACK_CONNECTED_CLIENTS_NUM option,INDY-1496,32056,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,19/Jul/18 9:34 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"An ability to restart the listener socket to avoid too many open connections was implemented and basically verified in the scope of INDY-1431.

This is disabled by default (TRACK_CONNECTED_CLIENTS_NUM_ENABLED flag).

We need to enable the flag and continue testing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1241,,,No,,Unset,No,,,"1|hzzhmn:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,VladimirWork,,,,,,,,,,"21/Jul/18 12:56 AM;sergey-shilov;*Problem state / reason:*

Connections tracking and client stack restart are disabled by default. They need to be enabled by default.

Also see:
 https://jira.hyperledger.org/browse/INDY-1431?focusedCommentId=47234&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-47234

*Changes:*

Enabled connections tracking (_TRACK_CONNECTED_CLIENTS_NUM_ENABLED = True_) and client stack restart (_CLIENT_STACK_RESTART_ENABLED_ _= True_).

Also see:
 https://jira.hyperledger.org/browse/INDY-1431?focusedCommentId=47234&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-47234

*Committed into:*

    [https://github.com/hyperledger/indy-plenum/pull/830]
     [https://github.com/hyperledger/indy-node/pull/836]
     indy-node 1.5.516-master

*Risk factors:*

    Clients may not receive their replies.

*Risk:*

    Medium

*Recommendations for QA:*

Continue load testing with enabled by default connections tracking and client stack restart.;;;","28/Jul/18 11:39 PM;VladimirWork;Build Info:
 indy-node 1.5.526

Steps to Validate:
 1. Run continuous load testing below and above client connections limit.
 2. Check node logs and load test results.

Actual Results:
1. Node tracks client connections and restarts client stack after installation by default.
2. Pool works normally below client connections limit.
3. Amount of sent txns is always equal to succeeded + failed (due to PoolLedgerTimeout response) above client connections limit.
4. Amount of failed txns with enabled client stask restart case is less or equal to not enabled case:
{noformat}
limit: 10 load: 10 x 10 each 2 seconds (~30 peak clients amount)
on | Server Time: 2025 Sent: 24590 Succ: 24495 Failed: 95 Nacked: 0 Rejected: 0
off | Server Time: 2358 Sent: 26529 Succ: 26428 Failed: 101 Nacked: 0 Rejected: 0

limit: 10 load: 15 x 10 each 2 seconds (~50 peak clients amount)
on | Server Time: 1481 Sent: 17443 Succ: 15825 Failed: 1618 Nacked: 0 Rejected: 0
off | Server Time: 1360 Sent: 21946 Succ: 17561 Failed: 4385 Nacked: 0 Rejected: 0
{noformat}

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Re-send messages to disconnected remotes,INDY-1497,32058,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,19/Jul/18 11:18 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"If the remote (node) is not connected, we're trying to send it, but if re-connection will be needed (which means that the socket is re-created), the sent message will be lost.

This may be a cause of issues like the one found in the scope if INDY-1404 and INDY-1450 (when initial catchup can not be finished since some messages are not received).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwxtr:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,,,,,,,,,,,"31/Jul/18 4:20 PM;ashcherbakov;*Problem reason:*

If a message is sent to a node which hasn't yet replied with pong (that is connected), then there is a chance that the message will be lost and not re-sent. This may lead to losing messages, especially during initial catch-up.

*Fix:*
 * Stash messages sent to disconnected node and re-send them once we got both ping and pong from them (that is both nodes are being connected and known to each other).
 The queue for stashing is limited to avoid out of memory issues.
 * Stash and re-send ping/pongs received from not yet connected entities.

*PR:*

[https://github.com/hyperledger/indy-plenum/pull/844]

 

*Covered with tests:*

[stp_zmq/test/test_send_to_disconnected.py|https://github.com/hyperledger/indy-plenum/pull/844/files#diff-2dce90a7f03f12b73d021be239d2c382]

[stp_zmq/test/test_stashed_ping_pong.py|https://github.com/hyperledger/indy-plenum/pull/844/files#diff-0be12be3ae9c893fc7a6bb3c71fb7f9a]

 

*Build*

Indy-Node: 1.5.537

 

*Risk*:

Medium

 

*Recommendation for QA*:
 * Run load test
 * Check catchup during load;;;","03/Aug/18 4:01 AM;ozheregelya;Environment:
 indy-node 1.5.539
libindy 1.6.1~655

Steps to Validate:
 1. Set up the pool.
 2. Start medium load (like in INDY-1343).
 3. Create one new node and add it to the pool.
 => Wait for catch up.

Actual Results:
 Node successfully completed catch up.

Additional load testing will be performed in scope of INDY-1343.;;;",,,,,,,,,,,,,,,,,,,,,,,
Part of the nodes were not upgraded during forced upgrade 1.5.504 -> 1.5.514,INDY-1498,32060,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ozheregelya,ozheregelya,20/Jul/18 12:20 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.5,,,,0,,,,"*Steps to Reproduce:*
 1. Restore the pool from the snapshot: [https://s3-us-west-2.amazonaws.com/qanodelogs/backups/qa_large/node1_800K.7z]
 2. Run the load test.
 => After the load test ~75% of disc space were utilized.
 3. Try to upgrade the pool:
{code:java}
ledger pool-upgrade name=upgrade514 version=1.5.514 sha256=e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855 action=start schedule={""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-01-23T11:30:00.000000+00:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-01-23T11:35:00.000000+00:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-01-23T11:40:00.000000+00:00"",""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-01-23T11:45:00.000000+00:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-01-23T11:50:00.000000+00:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-01-23T11:55:00.000000+00:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-01-23T12:00:00.000000+00:00"",""98VysG35LxrutKTNXvhaztPFHnx5u9kHtT7PnUGqDa8x"":""2018-01-23T12:05:00.000000+00:00"",""6pfbFuX5tx7u3XKz8MNK4BJiHxvEcnGRBs1AQyNaiEQL"":""2018-01-23T12:10:00.000000+00:00"",""HaNW78ayPK4b8vTggD4smURBZw7icxJpjZvCMLdUueiN"":""2018-01-23T12:15:00.000000+00:00"",""2zUsJuF9suBy2iKkcgmm8uoMB6u5Dq2oHoRuchrZbj2N"":""2018-01-23T12:20:00.000000+00:00"",""BXV4SXKEJeYQ8XCRHgpw1Xume5ntqALsRhbUYcF85Mse"":""2018-01-23T12:25:00.000000+00:00"",""71WAtEevzz8aZr8baNJhQCUDLwRhM7LeaErSKNWWKxzn"":""2018-01-23T12:30:00.000000+00:00"",""FEUGMFWCSAM725vyH8JZnsitiNUy31NPhugVKb8zDpng"":""2018-01-23T12:35:00.000000+00:00"",""DPZ8GJ1NyNZGJMU6qQZVuBsumY1aVzvcV4FqQK9Y215x"":""2018-01-23T12:40:00.000000+00:00"",""FYDoBrDhfGuSwt39Sgd3DZETihpnXy6SzZBggyD9HMrD"":""2018-01-23T12:45:00.000000+00:00"",""EMNhsHNsEpuffxCmgC3fpwVj7LgwtSm3riSizCMN6MBo"":""2018-01-23T12:50:00.000000+00:00"",""HD1XnVG6jXqGdmFMDTdJk3AoChxaqTfa6zGLkyXTtHwH"":""2018-01-23T12:55:00.000000+00:00"",""DUGXi5vxRZcrDC8VPZFU6bpiHDMhnWic9tDaoDJv3Bj6"":""2018-01-23T13:00:00.000000+00:00"",""D7jphMASPQAD6UFvT2ULjEfYybCJVDzwvfG5ZWJoXa69"":""2018-01-23T13:05:00.000000+00:00"",""7vcRBffPvKuGQz4F1ThYAo3Ucq3rXgU62enf6d23u8KX"":""2018-01-23T13:10:00.000000+00:00"",""DfSoxVHbbdZrAmwTJcRqM2arwUSvK3L6PXjqWHGo58xD"":""2018-01-23T13:15:00.000000+00:00"",""FTBmYnhxVd8zXZFRzca5WFKh7taW9J573T8pXEWL8Wbb"":""2018-01-23T13:20:00.000000+00:00"",""EjZrHfLTBR38d67HasBxpyKRBvrPBJ5RiAMubPWXLxWr"":""2018-01-23T13:25:00.000000+00:00"",""koKn32jREPYR642DQsFftPoCkTf3XCPcfvc3x9RhRK7"":""2018-01-23T13:30:00.000000+00:00""} timeout=10 force=true{code}
*Actual Results:*
 Part of the nodes were not upgraded. There are no upgrade log on the not upgraded nodes. In the config ledger there are only pool upgrade txns and 12 in_progress txns. 12 of 25 nodes were upgraded.
 *Note that upgraded nodes used 100% of disc space and part of them was crashed due to ""no space left"".*

*Expected Results:*
 All nodes should be upgraded independently on failure of the rest nodes ( ? ).

Logs: https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1498",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzhof:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,spivachuk,,,,,,,,,,,"20/Jul/18 11:39 PM;spivachuk;The issue is the same as observed in INDY-1447 and must be fixed in scope of it. 13 of 25 nodes discarded {{POOL_UPGRADE}} request since treated the upgrade as already scheduled because had already written the corresponding transaction to their ledgers by the moment of receiving the request directly from the client.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Ledger upgrade transaction should specify a package name,INDY-1499,32070,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,20/Jul/18 5:04 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6,,,,0,,,,"*Story*
As an administrator of a node on an Indy Network, I want the upgrade transaction to specify my network specific package rather than generic Indy Node so that my network specific package can perform configuration and customization functions such as installing network specific plugins to the Plenum Ledger.

*Acceptance Criteria*
* The upgrade transaction should include a value containing the package that should be upgraded.
  Add a new (optional) parameter to POOL_UPGRADE txn with a package name. As of now this is IndyNode, and it should remain the default. https://github.com/hyperledger/indy-node/blob/master/docs/transactions.md#pool_upgrade

*Notes*
* The work for the Node upgrade script should use that value to call the platform specific package manager to install or update the package is tracked in INDY-1500.
* The Sovrin network wants their network upgrades to call the Sovrin package instead of Indy Node. The Sovrin package depends on Indy Node.
* There will be a new release of the Sovrin network package with each release of Indy Node.
",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1491,,,,,,,INDY-1500,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwk7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ledger upgrade script should accept a package name to update,INDY-1500,32073,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,20/Jul/18 5:50 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6,,,,0,,,,"*Story*
As an administrator of a node on an Indy Network, I want the upgrade transaction to specify my network specific package rather than generic Indy Node so that my network specific package can perform configuration and customization functions such as installing network specific plugins to the Plenum Ledger.

*Acceptance Criteria*
* The Node upgrade script should use that value to call the platform specific package manager (on Debian will be apt) to install or update the package.
  Enhance node_control_tool to update the specified package, not IndyNode (see https://github.com/hyperledger/indy-node/blob/master/indy_node/utils/node_control_tool.py and https://github.com/hyperledger/indy-node/blob/master/scripts/upgrade_indy_node_ubuntu1604.sh)

*Notes*
* The work to have the upgrade transaction include a value containing the package that should be upgraded is in INDY-1499
* The Sovrin network wants their network upgrades to call the Sovrin package instead of Indy Node. The Sovrin package depends on Indy Node.
* There will be a new release of the Sovrin network package with each release of Indy Node.",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1491,,INDY-1499,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwjz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Document the upgrade process,INDY-1501,32074,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,20/Jul/18 6:03 AM,09/Oct/19 5:36 PM,28/Oct/23 2:47 AM,09/Oct/19 5:36 PM,,,,,,0,,,,"*Acceptance Criteria*
A document exists in the Indy Node documentation folder that describes the architecture and goals of the current update system.

Key items to document:
* The system pushes updates out simultaneously to all nodes on the network.
* This allows Indy Node to be upgraded in unison, and avoids the need for backwards compatibility.
* Which ledger the upgrade transaction uses.
* That the upgrade transaction defines an arbitrary package so that systems building on top of Indy Node can install their own plugins. (INDY-1499)
* That Indy Node control tool updates that package using a system appropriate to the specific platform: currently Debian, but could be RedHat or Windows",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-792,,,No,,Unset,No,,,"1|hzzj9j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"20/Jul/18 5:55 PM;ashcherbakov;We already have these docs: [https://github.com/hyperledger/indy-node/blob/master/docs/pool-upgrade.md]
We need to  check and update them.;;;",,,,,,,,,,,,,,,,,,,,,,,,
tmp.log must have unique name,INDY-1502,32083,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,20/Jul/18 9:26 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"If indy-node.service restarts when logs rotate, archive hasn't created.
In case that node restarts again - new tmp file rewrite previous",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzhm7:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,zhigunenko.dsr,,,,,,,,,,,"23/Jul/18 9:59 PM;sergey.khoroshavin;*Problem reason:*
Temporary file is created by log rotation handler before compressing in order to avoid blocking. If node crashes or restarts during rotation compressed file is not created and data remains in uncompressed temporary file only. Since this temporary file always have same name, it can lead to loss of log data in case of multiple crashes during rotation.

*Changes:*
When logs are rotated temporary file is now named just like ordinary uncompressed rotated log would be. This both removes problem with loss of data and makes understanding where uncompressed log file belongs easier.

*PR:*
https://github.com/hyperledger/indy-plenum/pull/831

*Version:*
indy-plenum: 1.5.470-master

*Risk:*
Medium

*Risk factors:*
While log rotation logic is already covered by a number of tests some edge cases still might come up;;;","31/Jul/18 6:16 PM;zhigunenko.dsr;*Environment:*
indy-node 1.5.528
indy-plenum 1.5.480

*Actial results:*
Temporary file is now named just like ordinary log file

;;;",,,,,,,,,,,,,,,,,,,,,,,
Explore load test behavior in case of read requests and lots of connections,INDY-1503,32084,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,dsurnin,dsurnin,20/Jul/18 10:10 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"The tests with fresh versions of libindy, with new timeouts, re/connections, shows lots of timeout errors. We need to figure it out what can be improved on node side",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-833,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzhmv:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,ozheregelya,,,,,,,,,,,"31/Jul/18 1:35 AM;ozheregelya;The initial problem with timeout errors during connection to the pool was fixed after changing of timeouts on libindy side.;;;",,,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.5.68,INDY-1504,32088,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,VladimirWork,VladimirWork,20/Jul/18 11:11 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.5,,,31/Jul/18 12:00 AM,0,,,,"*Version Information*
 indy-node 1.5.68
 indy-plenum 1.5.48
 indy-anoncreds 1.0.11
 sovrin 1.1.12

*Major Fixes*
 INDY-1471 - Logs appears in old CLI
 INDY-1461 - Numerous blacklists under high load
 INDY-1460 - Pool stopped writing after 1114k txns (different view_no)
 INDY-1464 - AttributeError: 'NoneType' object has no attribute 'request' during load
 INDY-1406 - validator-info reading empty file
 INDY-1443 - validator-info -v --json does not produce valid JSON
 INDY-1459 - First Pre-Prepare message has incorrect state trie root right after view_change (on master replica)
 INDY-1455 - Pool cannot order transactions because Node set incorrect watermarks after it's restart
 INDY-1454 - Pool has stopped working due to several incomplete view changes
 INDY-1427 - Node crashes on _remove_stashed_checkpoints
 INDY-1360 - Out of memory during non-completed viewChange process (under load)
 INDY-1422 - Part of nodes continued ordering txns after `incorrect state trie` under load
 INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66
INDY-1519 - 1.3.62 -> 1.5.67 forced upgrade without one node in schedule was failed

*Changes and Additions*
 INDY-1431 - Implement periodic restart of client stack to allow new clients to connect
 INDY-1467 - Get rid of peersWithoutRemotes
 INDY-1462 - High Watermark on backup may be reset to 300
 INDY-1494 - Allow optional field in node-to-node and client-to-node
 INDY-1463 - Catchup during view change may last forever under the load
 INDY-1458 - Propagate Primary mode should not be set for already started view change
 INDY-1450 - Catchup needs to be finished during high load
 INDY-1416 - Include reviewed logging strings in Indy
 INDY-1483 - Benchmark performance impact of recorder tool
 INDY-1311 - Decrease amount of logging with INFO level
 INDY-1435 - Throughput measurements in monitor should be windowed
 INDY-1386 - Limit the number of requested PROPAGATES in MessageRequests
 INDY-1453 - Do not process any client requests during view change
 INDY-1452 - A node must send LEDGER_STATUS with correct last ordered 3PC after catch-up
 INDY-1385 - Fix calculation of prepared certificates during View Change
 INDY-1404 - Catchup should not be interrupted by external events

*Known Issues*
INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66
(!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]
(!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI for pool upgrade*.

(!) *Pool upgrade from indy-node 1.3.62 should be performed simultaneously for all nodes due to txn format changes.*
(!) *All indy-cli pools should be recreated with actual genesis files.*
(i) *For more details about txn format changes see INDY-1421.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzhmf:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,TechWritingWhiz,VladimirWork,,,,,,,,,,"21/Jul/18 1:02 AM;VladimirWork;FYI [~krw910];;;","24/Jul/18 3:31 AM;TechWritingWhiz;These are complete. The pull request is here: [https://github.com/sovrin-foundation/sovrin/pull/73];;;","28/Jul/18 12:24 AM;ozheregelya;[~TechWritingWhiz], New RC 1.5.68 was released because of INDY-1519. Ticket description was updated. So, I'll reopen this ticket to update release notes.;;;","31/Jul/18 4:32 AM;TechWritingWhiz;The pull request for the new additions are here: [https://github.com/sovrin-foundation/sovrin/pull/74]

 ;;;",,,,,,,,,,,,,,,,,,,,,
Stability Tests,INDY-1505,32090,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,21/Jul/18 1:27 AM,18/Aug/18 7:19 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-781,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-1,,Indy Stability Tests,To Do,No,,Unset,No,,,"1|hzzjav:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Two demotions results promotion the node,INDY-1506,32098,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ozheregelya,ozheregelya,21/Jul/18 3:49 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,TShirt_L,,,"*Steps to Reproduce:*
 1. Setup the pool of 4 nodes and 2 additional nodes.
 2. Add Node5 and Node6.
 3. Demote Node6 twice (see pool ledger in the folder with logs) and promote it back.
 !   Result of this demotion was not checked because second demotion and promotion was happened by mistake in the batch.
 4. Demote Node6 again. 
 5. Send several txns.
 => Txns were not written on Node6.
 6. Demote Node6 again, send txn.
 => Txns were not written on Node6 in few seconds after demotion.
 7. Try to demote Node6 as steward on Node5.
 => Error message appeared in the CLI, but after this all missed txns were written in Node6.

*Actual Results:*
 Node was promoted after two demotions.

*Expected Results:*
 Node should not be promoted after demotion.

Logs and captured snapshot: [https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1506|https://s3-us-west-2.amazonaws.com/qanodelogs/indy-1503]","AWS acceptance pool (4+2 nodes)
indy-node 1.5.67 (RC)
 libindy/indy-cli 1.5.0~2 (RC)",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1719,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzsgf:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,VladimirWork,,,,,,,,,,"05/Oct/18 12:01 AM;Derashe;PR: [https://github.com/hyperledger/indy-plenum/pull/922]

Recomendation for QA: 

Repeat reproduce steps and make sure that double demoted node does not have 'missed' txns

Version:
h4. [indy-node 1.6.627|https://github.com/hyperledger/indy-node/releases/tag/1.6.627-master]
h4. [indy-plenum 1.6.560|https://github.com/hyperledger/indy-plenum/releases/tag/1.6.562-master];;;","15/Oct/18 10:16 PM;VladimirWork;Build Info:
indy-node 1.6.633

Steps to Validate:
1. Setup the pool of 4 nodes and 2 additional nodes.
2. Add Node5 and Node6.
3. Demote Node6 twice (see pool ledger in the folder with logs) and promote it back.
4. Demote Node6 again. 
5. Send several txns.
6. Demote Node6 again, send txn.
7. Send several txns.

Actual Results:
Multiple node demotions (2..5) do not cause to this node promotion.;;;",,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_restart_to_same_view_with_killed_primary,INDY-1507,32130,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,23/Jul/18 10:10 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1376/artifact/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-3027.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/18 10:11 PM;sergey.khoroshavin;test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-3027.txt;https://jira.hyperledger.org/secure/attachment/15361/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-3027.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzhp3:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,"24/Jul/18 8:08 PM;sergey.khoroshavin;*PR:* 
https://github.com/hyperledger/indy-plenum/pull/834;;;",,,,,,,,,,,,,,,,,,,,,,,,
As a Steward I want to see upgrade timeout expiry in node's upgrade log,INDY-1508,32136,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,VladimirWork,VladimirWork,24/Jul/18 1:03 AM,13/Feb/19 10:01 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"As a Steward I want to see *upgrade timeout expiry* result in node's upgrade log in this and similar to this cases (INDY-1447):

_The upgrade was scheduled for 07/20 11:20. Node1 started the upgrade but failed to complete it at 11:30:33 because 300 seconds timeout for apt update command was exceeded. Before the moment of apt update timeout was exceeded, indy-node service had re-sent an upgrade message to indy-node-control service at 11:30:00 because Upgrader's 10 minute timeout for upgrade was exceeded._

Now we have the next entries in the upgrade log when we cannot start the upgrade due to slow network or unavailable indy-node-control service so it is hard to understand why we have too many scheduled and started entries but don't have failed/succeeded entries before 11:36:
{noformat}
2018-07-20 11:15:54.184776	scheduled	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:20:00.267384	started	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:20:14.022571	scheduled	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:20:14.025329	started	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:21:14.568795	scheduled	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:21:14.572383	started	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:30:00.286141	scheduled	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:30:00.286577	started	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:30:39.492017	scheduled	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:30:39.495043	started	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
2018-07-20 11:36:48.718960	failed	2018-07-20 11:20:00.258870+00:00	1.5.511	1532085354142718
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1447,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx4f:2rzs",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create a Hyperledger Project Enhancement (HYPE) for indy-test-automation repo,INDY-1509,32137,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,devin-fisher,ckochenower,ckochenower,24/Jul/18 1:28 AM,23/Aug/18 11:59 PM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Create a HYPE outlining the following:

# Proposal to rename [indy-post-install-automation|https://github.com/hyperledger/indy-post-install-automation] repo to indy-test-automation.
# Proposal to move the contents of the evernym/chaosindy public repo into indy-test-automation and include (document publicly) the intent and purpose of chaos experiments.
# Propose how indy-test-automation repo maintainers will handle tickets. [~esplinr] - can you please clarify what ""handle"" means in this context? I am guessing it means one or more of the following
## Who will be responsible for reviewing and merging pull requests?
## Who is responsible for fixing bugs found in tests when they are reported and no PR is included?
## Who is responsible for determining priority for bugs and enhancement requests?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjsn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"23/Aug/18 11:59 PM;ckochenower;[~devin-fisher] - Are you waiting for the HIPE to be merged before marking this issue as ""Done""?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Partial write to the ledger,INDY-1510,32138,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,ckochenower,ckochenower,24/Jul/18 2:13 AM,26/Jul/18 10:28 PM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Write a Chaos experiment to test if a pool can manage a partial write to the ledger.

TODO: Include detailed steps here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjsv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - View change subversion,INDY-1511,32139,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ckochenower,ckochenower,ckochenower,24/Jul/18 2:16 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,test-automation,,,0,,,,"Take down backup primaries (replicas) and then the primary.

Details from [~krw910]:
In a 10 node pool you will have a primary node indicated in the logs as :0 and three of what I call backup primaries. :1, :2, :3. Since we do round robin it would start the pool like the following:
Node 1:0 - Primary
Node 2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node 3:2 - Backup to node 2
Node 4:3 - Backup to node 3

Since you can lose 3 nodes in a 10 node pool the test scenario would be to lose 2 of the 3 backup nodes then take down node 1 which is the primary. The remaining backup node should become the primary after the view change. and as the nodes you took down come back online new backup nodes (in round robin fashion) are selected.",,,,,,,,,,,,,,,,,,,,,INDY-1540,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjt3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ckochenower,,,,,,,,,,,"27/Jul/18 2:57 AM;ckochenower;This experiment initially lead me and [~krw910] to believe that we had discovered a potential bug in context to view change.

The experiment ran as expected to the following point:
 # Discover who the primary and it's replicas are (Primary: Node1 Replicas: Node2, 3, 4)
 # Stop as many of the replicas as possible w/o falling out of consensus. min(3, validator_info['Pool_info']['f_value']) )
 # Stop the primary
 # Wait a reasonable amount of time for a view change to complete

While waiting for the view change to complete, each of the nodes reported the following as their master:

```Node    Master

Node1   Node1:0

Node10  None

Node2   Node1:0

Node3   Node1:0

Node4   None

Node5   None

Node6   None

Node7   None

Node8   None

Node9   None```

It appears that nodes 4 through 10 could not elect a new master even though Node4 is one of the replicas. I would have expected Node4 to quickly be elected as the master.

The state of the pool appeared to settle here for a long time (> 10 minutes), which led me to believe the experiment found a potential problem. The pool stayed in this state the entire time it took me to explain the problem to Kelly and begin writing up a JIRA issue detailing what we had found.

The pool recovered as I was writing up the JIRA issue. It took what felt like forever, but it nevertheless recovered.

I will run the experiment again and measure how long my 10 node pool takes to recover during this experiment. The plan will be to alter the experiment to wait long enough for the view change to complete under the aforementioned conditions.;;;","27/Jul/18 5:58 AM;ckochenower;I configured the experiment to check for a view change every 60 seconds up to 20 times (up to 20 minute wait for view change). The view change to Node4 as the master occurred after approximately 16 minutes when the experiment stopped replicas Node2, Node3 followed by primary Node1.
{code:java}
[2018-07-26 19:41:47 DEBUG] [primary:110] Check 16 of 20 if view change is complete

[2018-07-26 19:41:47 DEBUG] [primary:111] Stopped primary: Node1

[2018-07-26 19:41:47 DEBUG] [primary:112] Current primary: Node4

[2018-07-26 19:41:47 DEBUG] [primary:114] View change detected!

[2018-07-26 19:41:47 DEBUG] [node:148] start node: Node2

[2018-07-26 19:41:49 DEBUG] [node:148] start node: Node3

[2018-07-26 19:41:52 DEBUG] [node:148] start node: Node1

[2018-07-26 19:41:54 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 19:41:54 INFO] [hypothesis:149] Steady state hypothesis: Can write nym

[2018-07-26 19:41:54 INFO] [activity:158] Probe: can-write-nym

[2018-07-26 19:41:54 DEBUG] [write_nym:11] seed: 000000000000000000000000Trustee1 genesis_file: /home/ubuntu/chaosindy/pool_transactions_genesis pool_name: vcs_pool1 my_wallet_name: vcs_my_wallet1 their_wallet_name: vcs_their_wallet1 timeout: 60

[2018-07-26 19:41:54 DEBUG] [ledger_interaction:34] # 0. Set protocol version to 2

[2018-07-26 19:41:54 DEBUG] [ledger_interaction:41] # 1. Create ledger config from genesis txn file

[2018-07-26 19:41:54 DEBUG] [ledger_interaction:43] pool_name: vcs_pool1

[2018-07-26 19:41:54 DEBUG] [ledger_interaction:44] pool_config: {""genesis_txn"": ""/home/ubuntu/chaosindy/pool_transactions_genesis""}

[2018-07-26 19:41:54 DEBUG] [ledger_interaction:52] pool_config: {""genesis_txn"": ""/home/ubuntu/chaosindy/pool_transactions_genesis""}

[2018-07-26 19:42:10 DEBUG] [ledger_interaction:65] # 4. Create Their Wallet and Get Wallet Handle

[2018-07-26 19:42:20 DEBUG] [ledger_interaction:77] # 5. Create My DID

[2018-07-26 19:42:20 DEBUG] [ledger_interaction:80] # 6. Create Their DID from Trustee1 seed

[2018-07-26 19:42:20 DEBUG] [ledger_interaction:86] # 8. Prepare and send NYM transaction

[2018-07-26 19:42:21 DEBUG] [ledger_interaction:90] # 9. Prepare and send GET_NYM request

[2018-07-26 19:42:36 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 19:42:36 DEBUG] [hypothesis:172] allowed tolerance is True

[2018-07-26 19:42:36 INFO] [hypothesis:179] Steady state hypothesis is met!

[2018-07-26 19:42:36 INFO] [experiment:283] Let's rollback...

[2018-07-26 19:42:36 INFO] [rollback:28] Rollback: start-stopped-primary

[2018-07-26 19:42:36 INFO] [activity:158] Action: start-stopped-primary

[2018-07-26 19:42:36 DEBUG] [validator_info:22] Found 'chaos' process

[2018-07-26 19:42:36 DEBUG] [validator_info:34] subprocess pid: 27596 chaos pid: 27596

[2018-07-26 19:42:36 DEBUG] [validator_info:37] tempdir: /tmp/validator-info.27596

[2018-07-26 20:28:35 DEBUG] [node:161] start node: Node2

[2018-07-26 20:28:37 DEBUG] [node:161] start node: Node1

[2018-07-26 19:42:36 INFO] [rollback:28] Rollback: cleanup-validator-info

[2018-07-26 19:42:36 INFO] [activity:158] Action: cleanup-validator-info

[2018-07-26 19:42:36 DEBUG] [validator_info:22] Found 'chaos' process

[2018-07-26 19:42:36 DEBUG] [validator_info:34] subprocess pid: 27596 chaos pid: 27596

[2018-07-26 19:42:36 DEBUG] [validator_info:37] tempdir: /tmp/validator-info.27596

[2018-07-26 19:42:36 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 19:42:36 INFO] [experiment:254] Experiment ended with status: completed

[2018-07-26 19:42:36 DEBUG] [caching:42] Clearing activities cache

End printing captured results...



real	22m39.942s

user	1m4.476s

sys	0m7.708s{code}
I then configured the experiment to check for a view change every 60 seconds up to 20 times (up to 20 minute wait for view change), but told the experiment that f == 2 instead of 3. Doing so caused the experiment to shut down 1 replica instead of 2. Replicas Node3 and Node4 would remain up. Just Node2 would be stopped before stopping the master (Node1). The view change to Node3 as the master occurred after approximately 9 minutes when the experiment stopped replica Node2 followed by primary Node1.
{code:java}
[2018-07-26 20:27:51 DEBUG] [primary:110] Check 9 of 20 if view change is complete

[2018-07-26 20:27:51 DEBUG] [primary:111] Stopped primary: Node1

[2018-07-26 20:27:51 DEBUG] [primary:112] Current primary: Node3

[2018-07-26 20:27:51 DEBUG] [primary:114] View change detected!

[2018-07-26 20:27:51 DEBUG] [node:161] start node: Node2

[2018-07-26 20:27:53 DEBUG] [node:161] start node: Node1

[2018-07-26 20:27:56 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 20:27:56 INFO] [hypothesis:149] Steady state hypothesis: Can write nym

[2018-07-26 20:27:56 INFO] [activity:158] Probe: can-write-nym

[2018-07-26 20:27:56 DEBUG] [write_nym:11] seed: 000000000000000000000000Trustee1 genesis_file: /home/ubuntu/chaosindy/pool_transactions_genesis pool_name: vcs_pool1 my_wallet_name: vcs_my_wallet1 their_wallet_name: vcs_their_wallet1 timeout: 60

[2018-07-26 20:27:56 DEBUG] [ledger_interaction:34] # 0. Set protocol version to 2

[2018-07-26 20:27:56 DEBUG] [ledger_interaction:41] # 1. Create ledger config from genesis txn file

[2018-07-26 20:27:56 DEBUG] [ledger_interaction:43] pool_name: vcs_pool1

[2018-07-26 20:27:56 DEBUG] [ledger_interaction:44] pool_config: {""genesis_txn"": ""/home/ubuntu/chaosindy/pool_transactions_genesis""}

[2018-07-26 20:27:56 DEBUG] [ledger_interaction:52] pool_config: {""genesis_txn"": ""/home/ubuntu/chaosindy/pool_transactions_genesis""}

[2018-07-26 20:28:09 DEBUG] [ledger_interaction:65] # 4. Create Their Wallet and Get Wallet Handle

[2018-07-26 20:28:20 DEBUG] [ledger_interaction:77] # 5. Create My DID

[2018-07-26 20:28:20 DEBUG] [ledger_interaction:80] # 6. Create Their DID from Trustee1 seed

[2018-07-26 20:28:20 DEBUG] [ledger_interaction:86] # 8. Prepare and send NYM transaction

[2018-07-26 20:28:21 DEBUG] [ledger_interaction:90] # 9. Prepare and send GET_NYM request

[2018-07-26 20:28:35 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 20:28:35 DEBUG] [hypothesis:172] allowed tolerance is True

[2018-07-26 20:28:35 INFO] [hypothesis:179] Steady state hypothesis is met!

[2018-07-26 20:28:35 INFO] [experiment:283] Let's rollback...

[2018-07-26 20:28:35 INFO] [rollback:28] Rollback: start-stopped-primary

[2018-07-26 20:28:35 INFO] [activity:158] Action: start-stopped-primary

[2018-07-26 20:28:35 DEBUG] [validator_info:22] Found 'chaos' process

[2018-07-26 20:28:35 DEBUG] [validator_info:34] subprocess pid: 22564 chaos pid: 22564

[2018-07-26 20:28:35 DEBUG] [validator_info:37] tempdir: /tmp/validator-info.22564

[2018-07-26 20:28:35 DEBUG] [node:161] start node: Node2

[2018-07-26 20:28:37 DEBUG] [node:161] start node: Node1

[2018-07-26 20:28:39 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 20:28:39 INFO] [rollback:28] Rollback: cleanup-validator-info

[2018-07-26 20:28:39 INFO] [activity:158] Action: cleanup-validator-info

[2018-07-26 20:28:39 DEBUG] [validator_info:22] Found 'chaos' process

[2018-07-26 20:28:39 DEBUG] [validator_info:34] subprocess pid: 22564 chaos pid: 22564

[2018-07-26 20:28:39 DEBUG] [validator_info:37] tempdir: /tmp/validator-info.22564

[2018-07-26 20:28:39 DEBUG] [activity:175]   => succeeded with 'True'

[2018-07-26 20:28:39 INFO] [experiment:254] Experiment ended with status: completed

[2018-07-26 20:28:39 DEBUG] [caching:42] Clearing activities cache

End printing captured results...



real	13m21.365s

user	0m50.716s

sys	0m4.276s{code};;;","27/Jul/18 11:47 PM;ckochenower;Results of running this experiment in perpetuity overnight are as follows (message shared with the Berdyaev 6 team in Slack):

The experiment does the following:

1. Shut down indy-node on all but the last replica in round-robin order. No pauses/waits between shutting down each node. Each replica is stopped sequentially as fast as possible, which is effectively less than a few seconds apart.
 2. Shut down the master
 3. Check for a view change every 60 seconds up to 20 times (20 minute view change timeout)
 ...

After step 1 above, the cluster (n - f remaining nodes) begin reporting ""None"" as their master for approximately 8 minutes per stopped replica until the last replica is considered for role as master. In other words, in my 10 node cluster, the experiment starts with Node1 as the master and Nodes 2 - 4 as replicas. f = 3 and replica count = 4. If just the first replica is stopped (Node2) followed by the master (Node1), it takes approximately 8-9 minutes to complete a view change, during which time Nodes 3 - 10 report ""None"" as their master.

Taking down two replicas (Node 2 and 3) followed by the master (Node1) results in a time of 16 minutes to complete a view change. Deductive reasoning suggests that if a third replica was stopped, it will take an additional 8 to 9 minutes to complete a view change. I can't test that, because I only have 10 nodes and replica count is 4 (based on cluster size).

I ran this experiment in perpetuity (1000 times) last night starting around 5 PM. It ran 29 iterations of the experiment before a paramiko stacktrace caused the experiment to hang indefinitely:

```[2018-07-27 09:30:57 DEBUG] [execute:248] kwargs: \{""connect_kwargs"": null, ""connect_timeout"": 10}
 Process Process-93:
 Traceback (most recent call last):
 File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
 self.run()
 File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run
 self._target(*self._args, **self._kwargs)
 File ""/home/ubuntu/chaosindy/chaosindy/execute/execute.py"", line 262, in do_work
 as_sudo=as_sudo, **kwargs_dict)
 File ""/home/ubuntu/chaosindy/chaosindy/execute/execute.py"", line 205, in _parallel_execute_on_host
 rtn = c.sudo(action, hide=True)
 File ""<decorator-gen-4>"", line 2, in sudo
 File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/fabric-2.1.3-py3.5.egg/fabric/connection.py"", line 29, in opens
 self.open()
 File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/fabric-2.1.3-py3.5.egg/fabric/connection.py"", line 501, in open
 self.client.connect(**kwargs)
 File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 338, in connect
 retry_on_signal(lambda: sock.connect(addr))
 File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/util.py"", line 279, in retry_on_signal
 return function()
 File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 338, in <lambda>
 retry_on_signal(lambda: sock.connect(addr))
 socket.timeout: timed out
 [2018-07-27 09:31:00 DEBUG] [execute:243] Execute on host...
 [2018-07-27 09:31:00 DEBUG] [execute:244] host: Node10
 [2018-07-27 09:31:00 DEBUG] [execute:245] action: validator-info -v --json
 [2018-07-27 09:31:00 DEBUG] [execute:246] user: None
 [2018-07-27 09:31:00 DEBUG] [execute:247] as_sudo: True
 [2018-07-27 09:31:00 DEBUG] [execute:248] kwargs: \{""connect_kwargs"": null, ""connect_timeout"": 10}
 [2018-07-27 09:31:04 DEBUG] [execute:229] [P0] routine quits```

It took 10 hours 43 minutes to run through 29 iterations of the experiment after which a socket timeout inside paramiko caused the perpetual execution of the experiment to hang.

Indy Node did it's job 29 times w/o fail. Each iteration of the experiment resulting in 16 to 17 minutes to complete a view change.

corin.kochenower [8:41 AM]
 The math suggests that it takes approximately 22 minutes to run each iteration of the experiment. 16 to 17 minutes, of which, waiting for a view change. (edited);;;","27/Jul/18 11:52 PM;ckochenower;Results from perpetual execution of this experiment:

https://drive.google.com/file/d/1dbeJmAHm6s0I2g_8ZhtNGgbAqKMhgy8y/view?usp=sharing;;;","28/Jul/18 2:45 AM;ckochenower;[~krw910], [~esplinr], and [~stevetolman] - I had a quick look at the [View Change Hardening epic|https://jira.hyperledger.org/browse/INDY-1376] and found the following commit...

[https://github.com/hyperledger/indy-plenum/pull/717/files]

...which significantly increases view change related timeouts ( ""INDY-1341: Increase viewchange/catchup timeout to 5 minutes"" ). ""Simple Timeout fixes of the current View Change protocol"" as a temporary fix until current/open viewchange related tasks in the epic are completed. 

From the short descriptions on the following issues, I suspect the view change slowness experienced during this experiment may be resolved once the legacy to PBFT refactor is completed.
| |INDY-1468|Latency measurements in monitor should be windowed|!/secure/viewavatar?size=xsmall&avatarId=10318&avatarType=issuetype!|{color:#4a6785}TO TEST{color}|[Vladimir Shishkin|https://jira.hyperledger.org/secure/ViewProfile.jspa?name=VladimirWork]|[_Actions_|https://jira.hyperledger.org/rest/api/1.0/issues/31782/ActionsAndOperations]|
| |INDY-1383|Simplify current view change process|!/secure/viewavatar?size=xsmall&avatarId=10318&avatarType=issuetype!|{color:#4a6785}NEW{color}|[Sergey Khoroshavin|https://jira.hyperledger.org/secure/ViewProfile.jspa?name=sergey.khoroshavin]|[_Actions_|https://jira.hyperledger.org/rest/api/1.0/issues/30782/ActionsAndOperations]|
| |INDY-1344|Implement legacy viewchanger using design for PBFT view change|!/secure/viewavatar?size=xsmall&avatarId=10318&avatarType=issuetype!|{color:#4a6785}NEW{color}|_Unassigned_|[_Actions_|https://jira.hyperledger.org/rest/api/1.0/issues/30382/ActionsAndOperations]|
| |INDY-1345|Integrate refactored legacy viewchanger into current codebase|!/secure/viewavatar?size=xsmall&avatarId=10318&avatarType=issuetype!|{color:#4a6785}NEW{color}|_Unassigned_|[_Actions_|https://jira.hyperledger.org/rest/api/1.0/issues/30383/ActionsAndOperations]|
| |INDY-1346|Implement hybrid legacy/PBFT viewchanger|!/secure/viewavatar?size=xsmall&avatarId=10318&avatarType=issuetype!|{color:#4a6785}NEW{color}|_Unassigned_|

[~VladimirWork], [~sergey.khoroshavin], or [~ashcherbakov] - Can you confirm that the slowness experienced will likely be resolved with the legacy to PBFT viewchanger refactor?;;;","30/Jul/18 1:43 PM;ashcherbakov;[~ckochenower] Yes, the slowness is caused by the fact that we do not have PBFT View Change, and the PBFT View Change is the only correct fix.
We have to have this slowness, otherwise the chances that View Change will fail are quite high.

 ;;;",,,,,,,,,,,,,,,,,,,
Chaos Experiment - Competing Primaries,INDY-1512,32140,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,ckochenower,ckochenower,24/Jul/18 2:33 AM,26/Jul/18 10:28 PM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Attempt to create a competing primaries (a.k.a. split primary) scenario by forcing a view change over and over again (a.k.a. chasing the primary) while generating load.

The Berdyaev 6 team has observed this behavior while running the following experiment 20 to 40 times in a row: https://github.com/evernym/chaosindy/blob/master/experiments/force-view-change.json. Note that the force-view-change experiment has recently changed (mid-July) and the behavior has not been reproduced with the updated experiment. The initial version of the experiment asked each node who the primary is, calculates the majority response (i.e. the majority of validator nodes say Node1 is the primary), and stops that node assuming it is the primary. The initial implementation did not wait for all nodes to report the new primary before starting the stopped node (previous primary). As a result, the next iteration (up to 40 iterations) would execute and repeat the process. The new version that waits up to 360 seconds for a view change to complete before moving to the next iteration of the experiment does not induce a ""competing primaries"" scenario.

Perhaps the force-view-change experiment needs to be enhanced to work both ways and this task would be to invoke the old behavior 20 to 40 times and expect the pool to eventually recover.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjtb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Selection of backup primaries (replicas) that are not actually running,INDY-1513,32141,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ckochenower,ckochenower,ckochenower,24/Jul/18 2:35 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,test-automation,,,0,,,,"3 scenarios need to be simulated:

Scenario 1: Replica selection after forcing view change by shutting down indy-node on the master.
 # Discover which nodes are selected as master and replicas
 # Assuming round-robin selection of both the primary and replicas. For example, in a 10 node cluster (Node1 - Node10), Node1 would be selected as a primary (1st of 4 replicas), and Node2 through Node4 would be selected as replicas (backup primaries)
 # Shutdown Node5 followed by the master (Node1).
 # A view change should occur with Node2 becoming the master and Node3, Node4, and Node6 as replicas, because Node5 is down.

Scenario 2: Replica selection without forcing view change after making indy-node on one of the non-primary replicas unreachable (block node port or stop indy-node service)
 # Discover which nodes are selected as master and replicas
 # Assuming round-robin selection of both the primary and replicas. For example, in a 10 node cluster (Node1 - Node10), Node1 would be selected as a primary (1st of 4 replicas), and Node2 through Node4 would be selected as replicas (backup primaries)
 # Shutdown Node2
 # Once the master detects Node2 is unreachable, expect it to select Node5 as a replica to replace Node2 as one of it's replicas

Scenario 3: Replica selection without forcing view change after demoting one of the non-primary replicas
 # Discover which nodes are selected as master and replicas
 # Assuming round-robin selection of both the primary and replicas. For example, in a 10 node cluster (Node1 - Node10), Node1 would be selected as a primary (1st of 4 replicas), and Node2 through Node4 would be selected as replicas (backup primaries)
 # Demote Node2
 # Once the master detects Node2 has been demoted, expect it to select Node5 as a replica to replace Node2 as one of it's replicas",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Aug/18 4:22 AM;ckochenower;Screen Shot 2018-08-16 at 1.12.31 PM.png;https://jira.hyperledger.org/secure/attachment/15603/Screen+Shot+2018-08-16+at+1.12.31+PM.png","17/Aug/18 4:37 AM;ckochenower;Screen Shot 2018-08-16 at 1.37.46 PM.png;https://jira.hyperledger.org/secure/attachment/15605/Screen+Shot+2018-08-16+at+1.37.46+PM.png","17/Aug/18 4:47 AM;ckochenower;Screen Shot 2018-08-16 at 1.40.26 PM.png;https://jira.hyperledger.org/secure/attachment/15604/Screen+Shot+2018-08-16+at+1.40.26+PM.png","17/Aug/18 12:24 AM;ckochenower;Screen Shot 2018-08-16 at 8.57.56 AM.png;https://jira.hyperledger.org/secure/attachment/15597/Screen+Shot+2018-08-16+at+8.57.56+AM.png","17/Aug/18 12:55 AM;ckochenower;Screen Shot 2018-08-16 at 9.55.05 AM.png;https://jira.hyperledger.org/secure/attachment/15599/Screen+Shot+2018-08-16+at+9.55.05+AM.png","17/Aug/18 12:59 AM;ckochenower;Screen Shot 2018-08-16 at 9.59.11 AM.png;https://jira.hyperledger.org/secure/attachment/15598/Screen+Shot+2018-08-16+at+9.59.11+AM.png","18/Aug/18 6:50 AM;ckochenower;Screen Shot 2018-08-17 at 3.35.45 PM.png;https://jira.hyperledger.org/secure/attachment/15703/Screen+Shot+2018-08-17+at+3.35.45+PM.png","18/Aug/18 6:53 AM;ckochenower;Screen Shot 2018-08-17 at 3.49.43 PM.png;https://jira.hyperledger.org/secure/attachment/15704/Screen+Shot+2018-08-17+at+3.49.43+PM.png","18/Aug/18 7:03 AM;ckochenower;Screen Shot 2018-08-17 at 3.55.03 PM.png;https://jira.hyperledger.org/secure/attachment/15707/Screen+Shot+2018-08-17+at+3.55.03+PM.png","18/Aug/18 7:01 AM;ckochenower;Screen Shot 2018-08-17 at 3.55.03 PM.png;https://jira.hyperledger.org/secure/attachment/15706/Screen+Shot+2018-08-17+at+3.55.03+PM.png","18/Aug/18 6:55 AM;ckochenower;Screen Shot 2018-08-17 at 3.55.03 PM.png;https://jira.hyperledger.org/secure/attachment/15705/Screen+Shot+2018-08-17+at+3.55.03+PM.png","17/Aug/18 6:06 AM;ckochenower;Step1-DemoteNode10.png;https://jira.hyperledger.org/secure/attachment/15606/Step1-DemoteNode10.png","17/Aug/18 6:07 AM;ckochenower;Step10-Check-Indy-Node-Services.png;https://jira.hyperledger.org/secure/attachment/15615/Step10-Check-Indy-Node-Services.png","17/Aug/18 6:07 AM;ckochenower;Step11-StartNode2.png;https://jira.hyperledger.org/secure/attachment/15616/Step11-StartNode2.png","17/Aug/18 6:07 AM;ckochenower;Step12-StartNode3and5.png;https://jira.hyperledger.org/secure/attachment/15617/Step12-StartNode3and5.png","17/Aug/18 6:07 AM;ckochenower;Step2-DemoteNode4.png;https://jira.hyperledger.org/secure/attachment/15607/Step2-DemoteNode4.png","17/Aug/18 6:07 AM;ckochenower;Step3-StopNode1-Force-View-Change.png;https://jira.hyperledger.org/secure/attachment/15608/Step3-StopNode1-Force-View-Change.png","17/Aug/18 6:07 AM;ckochenower;Step4-PromoteNode4-Including-Restart.png;https://jira.hyperledger.org/secure/attachment/15609/Step4-PromoteNode4-Including-Restart.png","17/Aug/18 6:07 AM;ckochenower;Step5-PromoteNode10-Without-Restart.png;https://jira.hyperledger.org/secure/attachment/15610/Step5-PromoteNode10-Without-Restart.png","17/Aug/18 6:07 AM;ckochenower;Step6-StartNode1.png;https://jira.hyperledger.org/secure/attachment/15611/Step6-StartNode1.png","17/Aug/18 6:07 AM;ckochenower;Step7-RestartNode10.png;https://jira.hyperledger.org/secure/attachment/15612/Step7-RestartNode10.png","17/Aug/18 6:07 AM;ckochenower;Step8-StopNode3and5-No-Visable-Change.png;https://jira.hyperledger.org/secure/attachment/15613/Step8-StopNode3and5-No-Visable-Change.png","17/Aug/18 6:07 AM;ckochenower;Step9-StopNode2-Force-View-Change.png;https://jira.hyperledger.org/secure/attachment/15614/Step9-StopNode2-Force-View-Change.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjtj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"17/Aug/18 12:02 AM;ckochenower;[~krw910] - The description/requirements for this ticket made an invalid assumption about replica selection. The assumption being that if the indy-node service was stopped or made unreachable (i.e. block the node port) long enough, a new replica would be selected/elected. This invalid assumption makes scenarios 1 and 2 in the description uninteresting.

By design, nodes that, according to the pool ledger, are validator nodes (services = [""VALIDATOR""]) are eligible to play the role of replica if the indy-node service is running/reachable or not.

The third scenario, however, is interesting, because the cluster's f_value and Count_of_replicas (f_value + 1) changes when demoting one of the backup primaries (non-master replica), but the remaining 9 validator nodes do not elect a replacement.

The following screenshot depicts a 10 node cluster with Node1 as the primary and Node2, Node3 as backup primaries. Node1 and Node3-Node10 are validators and Node2 was demoted using indy-cli. The annotations in the screenshot make it very busy. Following the circled numbers 1 through 3 and the associated colors and arrows will help.

 !Screen Shot 2018-08-16 at 8.57.56 AM.png|thumbnail! 

Within a minute of demoting Node2, the remaining validator nodes began reporting f_value = 2 and Count_of_replicas = 3, which is good. However, Node4 was dropped as a replica instead of Node2.

Nothing changes if Node2 is both demoted and indy-node is stopped on Node2. In other words, a node can be demoted and effectively taken offline (service unreachable) and validator nodes continue to label it as a backup replica and validator info from Node2 continues to report the node's Mode as ""participating"".

I generated some load (7200 NYM transactions across 18 clients in batch sizes of 400 NYMs) with no change in state except the domain ledger txn count.

Stopping indy-node on the primary triggered a view change and Node2, even though it was next in line according to validator-info (backup replica 1 - R1 in above screenshot), as expected, was NOT selected as the new primary. Node3 became the new primary and Node4 and Node5 became backup primaries.

 !Screen Shot 2018-08-16 at 9.55.05 AM.png|thumbnail! 

Restarting indy-node on Node1 (still a validator node) and Node2 (still NOT a validator node) results in the following. Note that Node2's f_value and Count_of_replicas gets updated and it's Mode says ""starting"" indefinitely.

 !Screen Shot 2018-08-16 at 9.59.11 AM.png|thumbnail! 

Promoting Node2 using indy-cli while it is in ""starting"" mode causes the pool's f_value and Count_of_replicas values to increase by 1 on all nodes but Node2. Node2's mode changes to ""participating"", but it's f_value and Count_of_replicas values remain 2 and 3 respectively until Node2 is restarted. Starting or restarting indy-node on a node AFTER promoting the node (setting it's services to ""VALIDATOR"") is required in the ""promotion workflow"". I just thought you might be interested in the observed behavior.;;;","17/Aug/18 1:41 AM;ckochenower;I think that INDY-1541 and INDY-1560 cover Scenario 3 well enough and this issue can be closed as ""Done"". [~krw910] - please reopen this issue if you disagree.;;;","17/Aug/18 4:30 AM;ckochenower;I did not write a Chaos test to test round-robin selection of backup primaries when nodes are demoted. It seems to be more of a system test than a Chaos test. The results are depicted in the following screenshot show that demoted nodes are excluded from the round-robin selection of backup primaries.

 !Screen Shot 2018-08-16 at 1.12.31 PM.png|thumbnail! 

In a nutshell:

1. Start with 10 nodes, f_value = 3, Count_of_replicas = 4, master = Node1, backup replicas are Node2 - Node4.
2. Demote Node10 to cause  f_value = 2, and Count_of_replicas = 3, master = Node1, backup replicas are Node2 - Node3. Node4 is no longer needed as a backup replica.
3. Demote Node4 - Node4 would be the next node in round-robin to be selected as a backup replica. With 9 nodes in the pool, 3 more nodes would have to be demoted to reduce f_value/Count_of_replicas any further. Therefore, we don't have to worry about Count_of_replicas shrinking if we demote Node4.
4. f_value and Count_of_replicas remain 2 and 3 respectively.
5. Force a view change to cause Node2 (next backup primary in round-robin) to become the new master. Node3 and Node5 become the backup replicas. Node4 is excluded, because it has been demoted.
6. Promoting Node4 (including restart of indy-node) does not change the backup primaries. The master is still Node2 and the backup primaries remain Node3 and Node5.
7. Promoting Node10 back into the pool (including restart of indy-node) changes f_value to 3 and Count_of_replicas to 4.

I'm not sure if this is a problem, but Node4 is selected as the third backup primary for Node1 - 9 and Node6 is selected as the third backup primary for Node10. Node10's third backup primary remains Node6 even after restarting indy-node on Node10.

 !Screen Shot 2018-08-16 at 1.40.26 PM.png|thumbnail! 

[~krw910] - Is this a bug that needs to be reported? I guess I need to read up on the RBFT for the 3rd time to really know for myself.;;;","17/Aug/18 6:22 AM;ckochenower;[~krw910] wanted me to try the following:
Step 1 - Demote Node10
 !Step1-DemoteNode10.png|thumbnail! 
Step 2 - Demote Node4
 !Step2-DemoteNode4.png|thumbnail! 
Step 3 - Stop indy-node service on Node1 - force a view change
 !Step3-StopNode1-Force-View-Change.png|thumbnail! 
Step 4 - Promote Node4 (including restart of indy-node)
 !Step4-PromoteNode4-Including-Restart.png|thumbnail! 
Step 5 - Promote Node10 (without restart of indy-node)
 !Step5-PromoteNode10-Without-Restart.png|thumbnail! 
Step 6 - Start indy-node service on Node1 (old master)
 !Step6-StartNode1.png|thumbnail! 
Step 7 - Restart indy-node service on Node10
 !Step7-RestartNode10.png|thumbnail! 
Step 8 - Stop indy-node service on Node3 and Node5 - No visible change.
 !Step8-StopNode3and5-No-Visable-Change.png|thumbnail! 
Step 9 - Stop indy-node service on Node2 - force a view change
 !Step9-StopNode2-Force-View-Change.png|thumbnail! 
Step 10 - Check indy-node services state on each node. Inactive on Node2, Node3, and Node5
 !Step10-Check-Indy-Node-Services.png|thumbnail! 
Step 11 - Start indy-node service on Node2
 !Step11-StartNode2.png|thumbnail! 
Step 12 - Start indy-node service on Node3 and Node5
 !Step12-StartNode3and5.png|thumbnail!

The problem I see during this test scenario is that consensus cannot be reached who the new master will be. A majority of nodes (6 of 10) cannot come to consensus on who the new master will be. Node3 and 5 were stopped to force R3 (the third replica) to become the next primary and Node2 is stopped to force a view change. Because indy-node is not running on 3 of 10 nodes 6 of the 7 remaining nodes must agree who the new master is. The 7 nodes voting have the following votes:


||Alias||Vote||
|Node1|Node6|
|Node4|Node4|
|Node6|Node4|
|Node7|Node4|
|Node8|Node4|
|Node9|Node4|
|Node10|Node6|

5 vote Node4
2 vote Node6

Neither are a majority (>= 6 out of 10)

Because a new master cannot be chosen, the pool falls out of consensus.

Starting the indy-node service on Node2 allows the pool to come to consensus on who the new master is and Node4 is elected master.

Restarting indy-node on Node3 and Node5 brings all nodes back into sync with Node4 as the master and Node5, Node6, and Node7 as replica 1, 2, and 3 respectively.
 ;;;","18/Aug/18 7:03 AM;ckochenower;I am seeing some unexplained behavior promoting and demoting nodes w/o view change. The primary is never demoted, because that would trigger a view change.

I was repeating a variation of the test in the previous comment when I stumbled on to this behavior.

My notes:

Pool starts with the following configuration:
Node1 - Primary and replica :0
Node2 - Replica :1
Node3 - Replica :2
Node4 - Replica :3
Node5-10 - 
# Shrink the pool to 7 nodes
## Demote Node10 - this reduces f_value from 3 to 2 and Count_of_replicas from 4 to 3
## Demote Node9
## Demote Node8
# Force a view change - Stop indy-node on Node1
# Wait for view change and then start indy-node on Node1 - Note: starting Node1 after Demoting Node7 doesn't make a difference.
# Write Nym - Passed
# Demote Node7 - this reduces f_value from 2 to 1 and Count_of_replicas from 3 to 2
# Write Nym - Passed
# Stop Node3
# Write Nym - Fails with a timeout! - {color:#0747A6}TODO: log Jira issue stating that stopped or demoted backup replicas don't get supplanted until a view change. Stop indy-node on all backup replicas OR demote all backup replicas and try to write a NYM. The transaction will timeout with ""LoadClient_0 run_test error ErrorCode.PoolLedgerTimeout""{color}
# Start Node3
# Write Nym - Passed
# Promote Node7 (without restart of indy-node)
# Write Nym - Fails with a timeout! ""run_test error ErrorCode.PoolLedgerTimeout""
# Restart all nodes in the pool. This will cause a view change. Node1 will be selected with Node2 and Node3 as backup replicas.
# Stop indy-node service on Node3
# Write Nym - Fails with timeout! Should pass, because 6 of 7 nodes should participate in consensus. Only Node3 should be down at this point.
# Demote Node7
# Write Nym - Passed
# Demote Node3
# Write Nym - Failed!
# Force a view change - Stop indy-node on Node1
# Write Nym - Failed!
# Promote Node7 including restart of indy-node
# Write Nym - Failed!
# Promote Node8 including restart of indy-node
# Write Nym - Failed!
# Promote Node9 including restart of indy-node
# Write Nym - Passed
# Demote Node9
# Write Nym - Passed
# Demote Node8
# Write Nym - Passed
# Demote Node7
# Write Nym - Failed!
# Promote Node3 including indy-node restart
# Write Nym -Failed!
# Promote Node7 including indy-node restart
# Write Nym - Passed
# Demote Node7
# Write Nym - Passed
# Demote Node1
# Write Nym - Failed!
# Promote Node1
# Write Nym - Failed!

The following shows what should be an impossible state. There are only 6 validator nodes (Node7-Node10 are demoted), yet Node1-Node10 say f_value is 2 and Count_of_replicas is 3 when it should be 1 and 2 respectively.

 !Screen Shot 2018-08-17 at 3.35.45 PM.png|thumbnail! 

Restarting (`systemctl restart indy-node`) indy-node on the master (Node4) to force a view change resulted in the following:

 !Screen Shot 2018-08-17 at 3.49.43 PM.png|thumbnail! 

Stopping (`systemctl stop indy-node`) indy-node on the master (Node4) to force a view change resulted in the following. Note that some nodes continue to report f_value = 2 and Count_of_replicas = 3

 !Screen Shot 2018-08-17 at 3.55.03 PM.png|thumbnail! 

A `systemctl restart indy-node` on all nodes but the master (to avoid a view change) does NOT clear up the problem.

 !Screen Shot 2018-08-17 at 3.55.03 PM.png|thumbnail! 

Restarting the master (Node4) clears up the problem. In other words, Node4 had to be restarted before a Nym could be written to the ledger even though all other participating nodes (Node1-Node6 in the following screenshot) were reporting Node5 as the master.

 !Screen Shot 2018-08-17 at 3.55.03 PM.png|thumbnail! 

This is technically a split-master. I will see if I can reproduce this behavior next Monday.

;;;",,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Memory usage during catch up,INDY-1514,32142,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,ckochenower,ckochenower,24/Jul/18 2:41 AM,26/Jul/18 10:28 PM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Can we crash the service? Can we run out of memory?

[~krw910] - Can you provide more details?

[~ckochenower] guesses that this would be an external process that would utilize a great deal of memory after inducing catchup (i.e. stop node, generate load, start node) which should cause increased virtual memory usage, paging, and swapping perhaps to the point of ""thrashing"". Will the indy-node service crash or become completely unresponsive?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjtr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Recover after greater than F faults,INDY-1515,32143,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ckochenower,ckochenower,24/Jul/18 2:47 AM,28/Jul/18 12:20 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Auto-recover after F and F+1 faults.

[~krw910] - Can you provide more details? Any suggestions on what kind of faults and how to induce them?

The following experiment is a near-equivalent in terms of testing a cluster's ability to recover after falling out of consensus: https://github.com/evernym/chaosindy/blob/master/experiments/consensus-recovery.json
The pool falls out of consensus when more than a third of the nodes are stopped.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzjtz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,ozheregelya,,,,,,,,,,,"27/Jul/18 11:55 PM;ckochenower;[~ozheregelya] made the following clarifying remarks:

In these cases behavior for 'more than or equal F+1 nodes alive' differs from behavior for 'less than F+1 nodes alive':

1. More than F, but less than N - F nodes stopped / out of sync.
=> Pool can't write txns while nodes are stopped / out of sync. After starting of stopped nodes pool works and all nodes have ViewNo is the same as before.
2. More than or equal N - F nodes stopped / out of sync.
=> Pool can't write txns while nodes are stopped / out of sync. After simultaneous starting of stopped nodes pool works and all nodes have ViewNo 0.
For case 3 we had an issue INDY-1199. It's already fixed, but for now need to put option ENABLE_INCONSISTENCY_WATCHER_NETWORK=True to the /etc/indy/indy_config.py to enable restoring the pool after stopping of more than N - F nodes. This parameter will be set to the config by default soon.;;;","28/Jul/18 12:20 AM;ozheregelya;Sorry for my mistake in Case 1. View No may be changed if primary was stopped when more than N-F nodes were alive. So here should be 
1. More than F, but less than N - F nodes stopped / out of sync.
=> Pool can't write txns while nodes are stopped / out of sync. After starting of stopped nodes pool works and all nodes have ViewNo is -the same as before- not less than before.;;;",,,,,,,,,,,,,,,,,,,,,,,
"Chaos Experiment - With enough nodes out of sync, network is unrecoverable",INDY-1516,32144,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ckochenower,ckochenower,24/Jul/18 2:50 AM,17/Aug/18 2:14 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Can we get enough nodes out of sync with each other to the point the pool/network cannot recover and reach consensus?

[~krw910] - Any ideas how this may be accomplished? Steps?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzju7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Docker pool can't be built because of new python3-indy-crypto in sdk repo,INDY-1517,32154,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,sergey-shilov,ozheregelya,ozheregelya,24/Jul/18 7:32 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Steps to Reproduce:*
1. Run .../indy-node/environment/docker/pool/pool_start script.

*Actual Results:*
Following error appears:
{code:java}
The following packages have unmet dependencies:
 indy-node : Depends: indy-plenum (= 1.5.47) but it is not going to be installed
E: Unable to correct problems, you have held broken packages.{code}
because indy-plenum depends on python3-indy-crypto=0.4.1, but new version exists in sdk repo:
{code:java}
agent@agent04:~$ apt show python3-indy-crypto
Package: python3-indy-crypto
Version: 0.4.2~19
Priority: extra
Section: default
Maintainer: Hyperledger <hyperledger-indy@lists.hyperledger.org>
Installed-Size: 12.3 kB
Depends: libindy-crypto
Homepage: https://github.com/hyperledger/indy-crypto
License: MIT/Apache-2.0
Vendor: none
Download-Size: 5,608 B
APT-Sources: https://repo.sovrin.org/sdk/deb xenial/rc amd64 Packages
Description: This is the official wrapper for Hyperledger Indy Crypto library (https://www.hyperledger.org/projects).{code}
 

*Expected Results:*
Need to add installation of python3-indy-crypto=0.4.1 to the dockerfile.",indy-node 1.5.67,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxs7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,,,,,,,,,,,"26/Jul/18 5:55 PM;ashcherbakov;As a hotfix, it can be enough to add `python3-indy-crypto=0.4.1` to [https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/core.ubuntu.dockerfile#L23] before installing of indy-node.;;;","13/Aug/18 10:00 PM;ozheregelya;This issue will be permanently fixed in indy-node RC 1.6.70+. So, hotfix in the dockerfile is unnecessary. Ticket will be closed as 'won't fix'.;;;",,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on,INDY-1518,32157,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,sergey.khoroshavin,sergey.khoroshavin,24/Jul/18 9:14 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,https://jenkins.hyperledger.org/job/indy-plenum-verify-x86_64/1375/artifact/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-3016.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/18 9:16 PM;sergey.khoroshavin;test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on.txt.xz;https://jira.hyperledger.org/secure/attachment/15366/test_quorum_after_f_plus_2_nodes_including_primary_turned_off_and_later_on.txt.xz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzj7b:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,sergey.khoroshavin,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
1.3.62 -> 1.5.67 forced upgrade without one node in schedule was failed,INDY-1519,32163,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ozheregelya,VladimirWork,VladimirWork,25/Jul/18 12:25 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.5,,,,0,,,,"Build Info:
indy-node 1.3.62 -> 1.5.67

Steps to Reproduce:
1. Install pool of 7 nodes.
2. Schedule *forced* upgrade *with the 1st node missing* via *old* CLI on *future* time:
{noformat}
send POOL_UPGRADE name=upgrade67new version=1.5.67 sha256=ed0a366b4ef36d40c055672a8b83679e99246fec71a706b4ae4cb7958feace3f action=start schedule={'8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb':'2018-07-24T15:00:00.258870+00:00','DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya':'2018-07-24T15:00:00.258870+00:00','4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA':'2018-07-24T15:00:00.258870+00:00','4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe':'2018-07-24T15:00:00.258870+00:00','Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8':'2018-07-24T15:00:00.258870+00:00','BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW':'2018-07-24T15:00:00.258870+00:00'} timeout=10 force=True
{noformat}

Actual Results:
1. Upgrade was triggered *right after* scheduling (not in time).
2. 2nd..7th nodes were upgraded but node service was failed periodically with this error:
{noformat}
Jul 24 14:53:20 e7c7073a1401 env[7206]: Traceback (most recent call last):
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Jul 24 14:53:20 e7c7073a1401 env[7206]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Jul 24 14:53:20 e7c7073a1401 env[7206]:     looper.run()
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
Jul 24 14:53:20 e7c7073a1401 env[7206]:     return self.loop.run_until_complete(what)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Jul 24 14:53:20 e7c7073a1401 env[7206]:     return future.result()
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Jul 24 14:53:20 e7c7073a1401 env[7206]:     raise self._exception
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Jul 24 14:53:20 e7c7073a1401 env[7206]:     result = coro.send(None)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
Jul 24 14:53:20 e7c7073a1401 env[7206]:     await self.runOnceNicely()
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
Jul 24 14:53:20 e7c7073a1401 env[7206]:     msgsProcessed = await self.prodAllOnce()
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Jul 24 14:53:20 e7c7073a1401 env[7206]:     s += await n.prod(limit)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 298, in prod
Jul 24 14:53:20 e7c7073a1401 env[7206]:     c = await super().prod(limit)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1063, in prod
Jul 24 14:53:20 e7c7073a1401 env[7206]:     c += await self.serviceNodeMsgs(limit)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1097, in serviceNodeMsgs
Jul 24 14:53:20 e7c7073a1401 env[7206]:     await self.processNodeInBox()
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1658, in processNodeInBox
Jul 24 14:53:20 e7c7073a1401 env[7206]:     await self.nodeMsgRouter.handle(m)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 81, in handle
Jul 24 14:53:20 e7c7073a1401 env[7206]:     res = self.handleSync(msg)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
Jul 24 14:53:20 e7c7073a1401 env[7206]:     return self.getFunc(msg[0])(*msg)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_req_processor.py"", line 51, in process_message_rep
Jul 24 14:53:20 e7c7073a1401 env[7206]:     return handler.process(msg, frm)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 61, in process
Jul 24 14:53:20 e7c7073a1401 env[7206]:     valid_msg = self.create(msg.msg, **params)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/message_handlers.py"", line 76, in create
Jul 24 14:53:20 e7c7073a1401 env[7206]:     return LedgerStatus(**msg)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 101, in __init__
Jul 24 14:53:20 e7c7073a1401 env[7206]:     self.validate(input_as_dict)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 24, in validate
Jul 24 14:53:20 e7c7073a1401 env[7206]:     self._validate_fields_with_schema(dct, self.schema)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 35, in _validate_fields_with_schema
Jul 24 14:53:20 e7c7073a1401 env[7206]:     self._raise_missed_fields(*missed_required_fields)
Jul 24 14:53:20 e7c7073a1401 env[7206]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/messages/message_base.py"", line 57, in _raise_missed_fields
Jul 24 14:53:20 e7c7073a1401 env[7206]:     raise MissingProtocolVersionError(msg)
Jul 24 14:53:20 e7c7073a1401 env[7206]: plenum.common.exceptions.MissingProtocolVersionError: validation error [LedgerStatus]: missed fields - protocolVersion. Make sure that the latest LibIndy is used an
Jul 24 14:53:20 e7c7073a1401 systemd[1]: indy-node.service: Main process exited, code=exited, status=1/FAILURE
Jul 24 14:53:20 e7c7073a1401 systemd[1]: indy-node.service: Unit entered failed state.
Jul 24 14:53:20 e7c7073a1401 systemd[1]: indy-node.service: Failed with result 'exit-code'.
Jul 24 14:53:30 e7c7073a1401 systemd[1]: indy-node.service: Service hold-off time over, scheduling restart.
Jul 24 14:53:30 e7c7073a1401 systemd[1]: Stopped Indy Node.
-- Subject: Unit indy-node.service has finished shutting down
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/18 12:25 AM;VladimirWork;upgrade_failure.tar.gz;https://jira.hyperledger.org/secure/attachment/15367/upgrade_failure.tar.gz","25/Jul/18 5:49 PM;VladimirWork;upgrade_not_ok_1_5.tar.gz;https://jira.hyperledger.org/secure/attachment/15372/upgrade_not_ok_1_5.tar.gz","25/Jul/18 5:49 PM;VladimirWork;upgrade_ok_1_4.tar.gz;https://jira.hyperledger.org/secure/attachment/15371/upgrade_ok_1_4.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzhnr:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,spivachuk,VladimirWork,,,,,,,,,,"25/Jul/18 5:49 PM;VladimirWork;I've rechecked 1.3 -> 1.4 and 1.3 -> 1.5 upgrades against 7 nodes pool *without the 1st node in schedule*:

1. After *1.3 -> 1.4* upgrade nodes restart with `plenum.common.exceptions.MissingProtocolVersionError: validation error [LedgerStatus]` stacktrace one or two times and then works normally. [^upgrade_ok_1_4.tar.gz] 

2. After *1.3 -> 1.5* upgrade nodes restart constantly with the same stacktrace. [^upgrade_not_ok_1_5.tar.gz] ;;;","27/Jul/18 2:06 AM;spivachuk;*Problem reason:*
- Exceptions about an invalid structure of a replied message from a message reply were not handled. Due to this the upgraded nodes crashed when were performing catch-up and received message replies with {{LedgerStatuses}} in outdated format (without {{protocolVersion}} field) from the not upgraded node.

*Changes:*
- Added an integration test from [~Toktar] verifying catch-up in case one of nodes sends {{MessageReps}} with {{LedgerStatuses}} in outdated format.
- Added unit tests for some unhandled cases in message request / response handling logic.
- Added handling for these cases to message request / response processing logic.

*PRs:*
- Hotfix in stable:
-- https://github.com/hyperledger/indy-plenum/pull/840
-- https://github.com/hyperledger/indy-node/pull/842
- The same fix with additional corrections in names and comments in master:
-- https://github.com/hyperledger/indy-plenum/pull/841
-- https://github.com/hyperledger/indy-node/pull/845

*Version:*
- rc:
-- indy-node 1.5.68-rc
-- indy-plenum 1.5.48-rc
- master:
-- indy-node 1.5.525-master
-- indy-plenum 1.5.478-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_node_rejects_msg_reps_with_invalid_msg_structure}}
- {{test_node_rejects_msg_reps_with_mismatched_params}}
- {{test_node_catchup_with_new_ls_form}} (stable) / {{test_catchup_with_ledger_statuses_in_old_format_from_one_node}} (master);;;","27/Jul/18 8:02 PM;ozheregelya;*Environment:*
 indy-node 1.5.68 rc

*Steps to Validate:*
 1. Setup the docker pool with 1.3.62 version.
 2. Send upgrade transaction for all nodes exclude the first one.
 => All nodes exclude the first one were successfully upgraded.
 3. Check the 'systemctl status indy-node'.
 => No errors.
 4. Send txn.
 => Txn was written on all nodes exclude the first one.
 5. Upgrade the first node manually and send txn.

*Actual Results:*
 The first node was successfully upgraded, all missed txns were processed, new txns were written. 

*Additional Information:*
The fix was verified on indy-node 1.5.527 (master) version and works as well.;;;",,,,,,,,,,,,,,,,,,,,,,
Misleading bug on pool create and differing protocol-version .txn files.,INDY-1520,32169,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,EvelynEvergreene,EvelynEvergreene,EvelynEvergreene,25/Jul/18 4:50 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,1.5,1.5,libsovrin,,,0,,,,"eva@eva-VirtualBox:~/.indy_client$ indy-cli
indy> pool create sandbox gen_txn_file=/home/eva/.indy-cli/networks/sandbox/pool_transactions_sandbox_genesis
Pool config ""sandbox"" has been created
indy> pool connect sandbox
Pool ""sandbox"" does not exist.

eva@eva-VirtualBox:~/.indy_client/pool/sandbox$ cat sandbox.txn
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""australia"",""client_ip"":""52.64.96.160"",""client_port"":""9702"",""node_ip"":""52.64.96.160"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""UZH61eLH3JokEwjMWQoCMwB3PMD6zRBvG6NCv5yVwXz""},""metadata"":\{""from"":""3U8HUen8WcgpbnEz1etnai""},""type"":""0""},""txnMetadata"":\{""seqNo"":1,""txnId"":""c585f1decb986f7ff19b8d03deba346ab8a0494cc1e4d69ad9b8acb0dfbeab6f""},""ver"":""1""}
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""brazil"",""client_ip"":""54.233.203.241"",""client_port"":""9702"",""node_ip"":""54.233.203.241"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""2MHGDD2XpRJohQzsXu4FAANcmdypfNdpcqRbqnhkQsCq""},""metadata"":\{""from"":""G3knUCmDrWd1FJrRryuKTw""},""type"":""0""},""txnMetadata"":\{""seqNo"":2,""txnId"":""5c8f52ca28966103ff0aad98160bc8e978c9ca0285a2043a521481d11ed17506""},""ver"":""1""}
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""canada"",""client_ip"":""52.60.207.225"",""client_port"":""9702"",""node_ip"":""52.60.207.225"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""8NZ6tbcPN2NVvf2fVhZWqU11XModNudhbe15JSctCXab""},""metadata"":\{""from"":""22QmMyTEAbaF4VfL7LameE""},""type"":""0""},""txnMetadata"":\{""seqNo"":3,""txnId"":""408c7c5887a0f3905767754f424989b0089c14ac502d7f851d11b31ea2d1baa6""},""ver"":""1""}
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""england"",""client_ip"":""52.56.191.9"",""client_port"":""9702"",""node_ip"":""52.56.191.9"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""DNuLANU7f1QvW1esN3Sv9Eap9j14QuLiPeYzf28Nub4W""},""metadata"":\{""from"":""NYh3bcUeSsJJcxBE6TTmEr""},""type"":""0""},""txnMetadata"":\{""seqNo"":4,""txnId"":""d56d0ff69b62792a00a361fbf6e02e2a634a7a8da1c3e49d59e71e0f19c27875""},""ver"":""1""}
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""korea"",""client_ip"":""52.79.115.223"",""client_port"":""9702"",""node_ip"":""52.79.115.223"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""HCNuqUoXuK9GXGd2EULPaiMso2pJnxR6fCZpmRYbc7vM""},""metadata"":\{""from"":""U38UHML5A1BQ1mYh7tYXeu""},""type"":""0""},""txnMetadata"":\{""seqNo"":5,""txnId"":""76201e78aca720dbaf516d86d9342ad5b5d46f5badecf828eb9edfee8ab48a50""},""ver"":""1""}
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""singapore"",""client_ip"":""13.228.62.7"",""client_port"":""9702"",""node_ip"":""13.228.62.7"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""Dh99uW8jSNRBiRQ4JEMpGmJYvzmF35E6ibnmAAf7tbk8""},""metadata"":\{""from"":""HfXThVwhJB4o1Q1Fjr4yrC""},""type"":""0""},""txnMetadata"":\{""seqNo"":6,""txnId"":""51e2a46721d104d9148d85b617833e7745fdbd6795cb0b502a5b6ea31d33378e""},""ver"":""1""}
\{""reqSignature"":\{},""txn"":\{""data"":\{""data"":\{""alias"":""virginia"",""client_ip"":""34.225.215.131"",""client_port"":""9702"",""node_ip"":""34.225.215.131"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""EoGRm7eRADtHJRThMCrBXMUM2FpPRML19tNxDAG8YTP8""},""metadata"":\{""from"":""SPdfHq6rGcySFVjDX4iyCo""},""type"":""0""},""txnMetadata"":\{""seqNo"":7,""txnId"":""0a4992ea442b53e3dca861deac09a8d4987004a8483079b12861080ea4aa1b52""},""ver"":""1""}

When changed to the following it works:
\{""data"":\{""alias"":""australia"",""client_ip"":""52.64.96.160"",""client_port"":""9702"",""node_ip"":""52.64.96.160"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""UZH61eLH3JokEwjMWQoCMwB3PMD6zRBvG6NCv5yVwXz"",""identifier"":""3U8HUen8WcgpbnEz1etnai"",""txnId"":""c585f1decb986f7ff19b8d03deba346ab8a0494cc1e4d69ad9b8acb0dfbeab6f"",""type"":""0""}
\{""data"":\{""alias"":""brazil"",""client_ip"":""54.233.203.241"",""client_port"":""9702"",""node_ip"":""54.233.203.241"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""2MHGDD2XpRJohQzsXu4FAANcmdypfNdpcqRbqnhkQsCq"",""identifier"":""G3knUCmDrWd1FJrRryuKTw"",""txnId"":""5c8f52ca28966103ff0aad98160bc8e978c9ca0285a2043a521481d11ed17506"",""type"":""0""}
\{""data"":\{""alias"":""canada"",""client_ip"":""52.60.207.225"",""client_port"":""9702"",""node_ip"":""52.60.207.225"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""8NZ6tbcPN2NVvf2fVhZWqU11XModNudhbe15JSctCXab"",""identifier"":""22QmMyTEAbaF4VfL7LameE"",""txnId"":""408c7c5887a0f3905767754f424989b0089c14ac502d7f851d11b31ea2d1baa6"",""type"":""0""}
\{""data"":\{""alias"":""england"",""client_ip"":""52.56.191.9"",""client_port"":""9702"",""node_ip"":""52.56.191.9"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""DNuLANU7f1QvW1esN3Sv9Eap9j14QuLiPeYzf28Nub4W"",""identifier"":""NYh3bcUeSsJJcxBE6TTmEr"",""txnId"":""d56d0ff69b62792a00a361fbf6e02e2a634a7a8da1c3e49d59e71e0f19c27875"",""type"":""0""}
\{""data"":\{""alias"":""korea"",""client_ip"":""52.79.115.223"",""client_port"":""9702"",""node_ip"":""52.79.115.223"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""HCNuqUoXuK9GXGd2EULPaiMso2pJnxR6fCZpmRYbc7vM"",""identifier"":""U38UHML5A1BQ1mYh7tYXeu"",""txnId"":""76201e78aca720dbaf516d86d9342ad5b5d46f5badecf828eb9edfee8ab48a50"",""type"":""0""}
\{""data"":\{""alias"":""singapore"",""client_ip"":""13.228.62.7"",""client_port"":""9702"",""node_ip"":""13.228.62.7"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""Dh99uW8jSNRBiRQ4JEMpGmJYvzmF35E6ibnmAAf7tbk8"",""identifier"":""HfXThVwhJB4o1Q1Fjr4yrC"",""txnId"":""51e2a46721d104d9148d85b617833e7745fdbd6795cb0b502a5b6ea31d33378e"",""type"":""0""}
\{""data"":\{""alias"":""virginia"",""client_ip"":""34.225.215.131"",""client_port"":""9702"",""node_ip"":""34.225.215.131"",""node_port"":""9701"",""services"":[""VALIDATOR""]},""dest"":""EoGRm7eRADtHJRThMCrBXMUM2FpPRML19tNxDAG8YTP8"",""identifier"":""SPdfHq6rGcySFVjDX4iyCo"",""txnId"":""0a4992ea442b53e3dca861deac09a8d4987004a8483079b12861080ea4aa1b52"",""type"":""0""}",,,,,,,,,,INDY-1521,IS-837,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzjxr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,EvelynEvergreene,,,,,,,,,,,"25/Jul/18 3:41 PM;ashcherbakov;[~EvelynEvergreene]
What is the version of IndyNode on STN? It looks like STN hasn't been updated yet, so it can not work with the new format of genesis transactions.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Indy-cli not allowing re-connect to pool after exiting.,INDY-1521,32170,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,EvelynEvergreene,EvelynEvergreene,EvelynEvergreene,25/Jul/18 5:06 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,1.5,1.5,libsovrin,,,0,,,,"indy-cli 1.5.0 not allowing user to reconnect to pool.
{code:java}
// vagrant@indyCli2:~$ rm -r .indy_client/pool/stn
vagrant@indyCli2:~$ indy-cli
indy> pool create stn gen_txn_file=pool_transactions_STN_genesis
Pool config ""stn"" has been created
indy> pool connect stn protocol-version=1
Pool ""stn"" has been connected
pool(stn):indy> pool disconnect
Pool ""stn"" has been disconnected
indy> exit
Goodbye...
vagrant@indyCli2:~$ indy-cli
indy> pool connect stn protocol-version=1
Pool ""stn"" has not been connected.
indy>
{code}",,,,,,,,,,,,,,,,,INDY-1520,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzjxz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),krw910,,,,,,,,,,,,"25/Jul/18 5:13 AM;EvelynEvergreene;Disregard;;;","25/Jul/18 7:01 AM;krw910;Set back to Invalid since it is not fixed in INDY and the ticket for this issue is in IS-837 for the INDY-CLI. I did not want to give the impression this was fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,
"How-to ""Add new node to existing pool""",INDY-1522,32195,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,25/Jul/18 8:54 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,Documentation,,,"Adding new node to existing pool is not covered by documentation yet.
Current unique knowledge source is acceptance scenario",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-792,,,No,,Unset,No,,,"1|hzzj9r:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,zhigunenko.dsr,,,,,,,,,,,"20/Aug/18 4:36 PM;zhigunenko.dsr;PR: https://github.com/hyperledger/indy-node/pull/918;;;","27/Aug/18 6:43 PM;ozheregelya;[~zhigunenko.dsr], 
{quote} * {{target}} specifies public key from {{init_indy_node}} scrip{quote}
It could be not obvious for stewards that they should put not the node public key as is, but base58 of the node public key ('Verkey' field in output of init_indy_node).;;;","29/Aug/18 6:47 AM;ozheregelya;The issue described above was verified in updated PR. All is well.;;;","29/Aug/18 5:41 PM;zhigunenko.dsr;PR: https://github.com/hyperledger/indy-node/pull/921
Add ""How-To get base58"" ;;;","29/Aug/18 11:02 PM;zhigunenko.dsr;*Reason to Close:*
PR has been merged;;;",,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_restart_groups_4_of_7_wp_no_tm,INDY-1523,32221,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,26/Jul/18 3:25 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/18 3:25 PM;ashcherbakov;test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-3690.txt.zip;https://jira.hyperledger.org/secure/attachment/15382/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-3690.txt.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzipr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
One node was lagged behind others after normal load,INDY-1524,32230,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,,ozheregelya,ozheregelya,26/Jul/18 7:38 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,Stability,,,"*Steps to Reproduce:*
1. Run the load test with normal load (10 writes, 100 reads per sec) for 3 hours.
2. Wait for 1 hour.
3. Run one more load test with normal load (10 writes, 100 reads per sec) for 3 hours.

*Actual Results:*
After second round of the load test one node (Node 12) was lagged behind others after ""cannot allocate memory"" error in journalctl. Domain ledger size on all nodes: 252240, on Node12: 245715.
One View Change was happened, but it was happened in the beginning of the second test.

*Expected Results:*
Nodes should be restored after crashing and should participate in consensus.

Logs: s3://qanodelogs/lagged_node12_after_normal_load","indy-node 1.5.67 RC
libindy 1.5.0~648",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzk6v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,,,,,,,,,,,,"26/Jul/18 9:50 PM;ozheregelya;Node completed processing of missed txns after one night without load.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_reconnect_primary_and_not_primary,INDY-1525,32231,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,spivachuk,spivachuk,26/Jul/18 9:18 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,https://ci.evernym.com/view/Core/job/Indy-Plenum/job/indy-plenum-verify-x86_64/1955/artifact/test-result-plenum-1.ubuntu-09.txt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/18 9:22 PM;spivachuk;test_reconnect_primary_and_not_primary.ubuntu-09.zip;https://jira.hyperledger.org/secure/attachment/15387/test_reconnect_primary_and_not_primary.ubuntu-09.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzj7j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,spivachuk,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As an Indy Node Developer or Support Engineer, I need a tool(s) to monitor the health of a pool that will aide in root cause analysis of Indy Node faults/failures.",INDY-1526,32237,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ckochenower,ckochenower,26/Jul/18 11:49 PM,11/Oct/19 6:28 PM,28/Oct/23 2:47 AM,11/Oct/19 6:28 PM,,,validator-info,,,0,Ledger,,,"h1. Executive Summary

Periodically collect ""validator info"" from each node in a pool and persist it for use by analysis and/or monitoring tools. Data can be immediately fed to or read by monitoring/analysis tools. When faults/failures are experienced, the data will aide in root cause analysis.

The process/tool that performs this operation must be run by a Steward. No other entity is currently (as of July 25, 2018) privileged to obtain validator info. Therefore, tools and processes that access this data must enforce this data sharing authorization requirement for stored validator info. In other words, do NOT re-share data in raw or altered form with anyone else.

It is difficult to know where to start a root cause analysis on large pools (25+ nodes) under heavy load (??? - [~ashcherbakov] - what is considered heavy load?) and/or over long periods of time. Tools built on top of this persistent store of data will aide in identifying one or more starting points.

If use of the tool on the STN or Live pool are a concern, the tool will still add value to development and QA teams. QA teams are likely the only group to have resources sufficient to create large, highly transacted systems, but developers will hopefully find value and experience with the tool(s) in their own development environments.
h1. Considerations
h2. Data - Validator Info

Only Stewards are privileged to get validator info. Therefore the tool(s) are not intended for use by any other entity.

(possible future enhancement) [~danielhardman] offered the following suggestion (not verbatim) that may make validator info interesting/useful to other entities while allowing Stewards to control who sees what data:

Stewards collectively decide (TGB) what baseline validator info will be shared, but ultimately decide what, if any, additional information will be shared with whom.

Use the config ledger to define the baseline, pool-wide set of data/attributes validator-info (script) or indy-cli's `ledger get-validator-info` returns. Perhaps the set of baseline data should be configurable based on who is making the request, such as the public (no nym), Trustees, and Stewards, producing public, protected, and private views.

Enhance Indy Node configuration (i.e. /etc/indy/indy_config.py) to allow each Steward to include additional information beyond the baseline defined in the config ledger. If it is decided that a different set of data will be returned base on who is requesting it, this configuration option could allow Stewards to share data beyond the baseline up to the superset of validator info based on who is requesting the data.
h2. Storage

Devin Fisher suggested the use of a time series database (optimized for time series data) for storage and querying of data. Where the database is hosted is determined by the Steward collecting the data.
h2. Analysis

Validator info contains both ""Node_info"" and ""Pool_info"". Node_info will give the tools useful information about each node's internal indy-node process and ledger state such as catchup, replica, and view change status, config, domain, and pool ledger size, and performance metrics. The Pool_info provides useful information about each node's view of the pool such as, blacklisted nodes, quorums, reachable, unreachable, and suspicious nodes, and f value.",,,,,,,,,,INDY-1586,,,,,,,,,,,,,,,,,,,INDY-1278,,,,,INDY-1177,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-56,,,No,,Unset,No,,,"1|hzzk87:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,esplinr,,,,,,,,,,,"27/Jul/18 12:38 AM;ckochenower;[~esplinr] - I added this story to the ""Monitoring"" epic. Not sure if the ""Stability"" epic is more appropriate.;;;","11/Oct/19 6:28 PM;esplinr;This write-up helped to influence our thinking, and we extended validator-info to include much of this functionality. At this point Evernym, IBM, and the Sovrin Foundation have monitoring set up for the network based on those tools. Additional monitoring functionality should be implemented by the administrators of Indy networks. If we need tools in Indy core to assist, we will create separate issues to track the work.;;;",,,,,,,,,,,,,,,,,,,,,,,
Ledger permissions are configurable,INDY-1527,32240,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,esplinr,esplinr,27/Jul/18 12:13 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"*Story*
As a trustee responsible for an Indy Node network, I want to be able to update ledger permissions so that I can implement my governance rules specific to my network.

*Acceptance Criteria*
The ledger permissions available to each role should be configurable without code changes.

*Notes*
* The list of ledger roles and the list of permissions do not need to be configurable, but the mapping between them should be configurable.
* The current ledger roles and permissions should remain the same.",,,,,,,,,,INDY-1732,,,,,,,,,,,,,,,,,,,,,,,,INDY-1528,INDY-1704,INDY-1956,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001xnr",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"21/Jan/19 10:51 PM;ashcherbakov;This is already possible in code, and will be possible in config ledger once INDY-1732 is done.

[~esplinr]
What else is required here in addition to being able to change the rules in config ledger?;;;","31/Jan/19 6:21 PM;esplinr;I believe that once INDY-1732 is done, then this is just a testing task to ensure we meet the original use case.;;;","27/Feb/19 5:45 PM;ashcherbakov;Testting will be done in the scope of INDY-1995;;;",,,,,,,,,,,,,,,,,,,,,,
Trust anchor permission not needed for ledger writes,INDY-1528,32241,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,esplinr,esplinr,27/Jul/18 12:42 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"*Story*
 As a trustee of a network that has implemented a payment system, I want to use that payment system to manage writes (in particular credential definitions and schemas) to the ledger instead of requiring the use of trust anchors.

*Acceptance Criteria*
 * Create a configuration setting: ""writes-require-trust-anchor"", default to True
 * When the configuration is false, all users have permission to write to the ledger.
 * When the configuration is true, the Trust Anchor role is required to write.
 * The permission should affect all write transactions.
* Permissions should continue to enforce that only owners of existing transactions can edit them (no change to this behavior).
* The Trust Anchor role should continue to exist in case it is needed in the future.

*Notes*
 * Payment of fees during a write is enforced by plugins, and not part of the core ledger.
 * The work to make this configuration flexible across all permissions is being tracked in INDY-1527.
 * We assume that all nodes in a network set this property consistently. If n-f Stewards set this flag to not require TrustAnchor role, then it will not be required the same as with other consensus driven configuration.
* We decided against only removing the need for the Trust Anchor role for credential definitions and schema definitions, but still requiring it for writing nyms.
** Requiring a Trust Anchor in order to writing a nym transaction would help us to ensure that best practices are being used--personal data is not being written to the ledger (GDPR compliance).
** We decided that it is premature to enforce best practices regarding nyms early in the life of the ledger. We should wait to see our recommendations proved in practice before adopting inflexible policies.
** We decided that we should wait until users of Indy have created formal policies for on-boarding Trust Anchors.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1527,INDY-1957,,,,INDY-1554,INDY-1956,,,,,,,,,,,,,,"12/Aug/18 3:22 AM;ozheregelya;AS-03-01-identity-owner-anyone-can-write.batch;https://jira.hyperledger.org/secure/attachment/15534/AS-03-01-identity-owner-anyone-can-write.batch","12/Aug/18 3:22 AM;ozheregelya;AS-03-01-identity-owner.batch;https://jira.hyperledger.org/secure/attachment/15533/AS-03-01-identity-owner.batch","12/Aug/18 3:22 AM;ozheregelya;AS-03-01-steward.batch;https://jira.hyperledger.org/secure/attachment/15535/AS-03-01-steward.batch","12/Aug/18 3:22 AM;ozheregelya;AS-03-01-trust-anchor.batch;https://jira.hyperledger.org/secure/attachment/15536/AS-03-01-trust-anchor.batch","12/Aug/18 3:22 AM;ozheregelya;AS-03-01-trustee.batch;https://jira.hyperledger.org/secure/attachment/15537/AS-03-01-trustee.batch",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxv3:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,esplinr,ozheregelya,,,,,,,,,"27/Jul/18 12:51 AM;ashcherbakov;Options:
1) Use config ledger for permissions (a long-term option that solves everything)
2) Do the change in plugins, not in Indy code. It may be not so trivial and may require changes in both plugins and plugins support in general.
3) Have a flag (configuration) in config which defines whether trustAnchor role is required for Schema and ClaimDef. True by default. Custom products (with fees support) may set it to False during installation.
A possible vulnerability if n-f Stewards set this flag to not require TrustAnchor role (but on the other hand if n-f malicious Stewards collaborated, they can do a lot of other awful things and break the system at all);;;","01/Aug/18 8:28 PM;Derashe;Problem reason:
 * We need to have config switcher to choose wether client can write requests to domain ledger

Changes:
 * Created config flag which allows to choose wich permission mode you want to use
 * Class Authoriser was modified to support switchable permissions.

Committed into:
 * [https://github.com/hyperledger/indy-node/pull/854]
 * [https://github.com/sovrin-foundation/sovrin/pull/76]
 * https://github.com/hyperledger/indy-node/pull/860

Risk:
 * Low

Covered with tests:
 * 4 integraton tests at indy-node/ indy_client/test/cli
 * 4 unit tests to test authorizer module

Recommendations for QA: 
 * Try to run pool with default config. Check that client can not send requests to domain ledger.
 * Try to run pool with config flag ANYONE_CAN_WRITE set to TRUE. Check that client can send requests to domain ledger.;;;","01/Aug/18 10:58 PM;esplinr;Thank you for making fast progress on this issue. After discussion with the Sovrin Foundation, we changed the policy to not require a Trust Anchor for any write. We updated the description to reflect this change.;;;","03/Aug/18 6:03 PM;Derashe;[~esplinr], can you clarify some questions:
 * Can client send NYM transactions to create trust_anchor, steward, trustee? Or should he have permission to create only another client NYMs?
 * Should ATTRIB txn depends on config flag? Or should client be allowed to write ATTRIB in any case?
 * Should revocation txns also be depended on config flag?;;;","03/Aug/18 10:35 PM;esplinr;Q. Can client send NYM transactions to create trust_anchor, steward, trustee? Or should he have permission to create only another client NYMs?

A. No. The ability to create a trust_anchor, steward, or trustee should be restricted from clients.


Q. Should ATTRIB txn depends on config flag? Or should client be allowed to write ATTRIB in any case?

A. A client should only be able to assign ATTRIBs to a NYM if the client is the owner of the NYM.


Q. Should revocation txns also be depended on config flag?

A. Only the issuer of a credential should be able to revoke the credential.;;;","06/Aug/18 9:30 PM;ashcherbakov;[~esplinr]
 To summarize, we propose the following rules:

*ANYONE_CAN_WRITE=True*
 * NYM:
 ** NYMs for common user (no role) can be created by anyone
 ** NYMs for Trustees/Stewards/TrustAnchor are controlled by roles (see permissions table)
 ** Only owner can edit existing NYMs
 * ATTRIB:
 ** Only owners can create and edit ATTRIBs
 * CLAIM_DEF:
 ** Anyone can create new CLAIM_DEFS
 ** Only owners can edit existing CLAIM_DEFs
 * SCHEMA:
 ** Anyone can create new SCHEMA
 ** No one can edit SCHEMAs
 * REVOC_REG_DEF:
 ** Anyone can create new REVOC_REG_DEF
 ** Only owners can edit existing REVOC_REG_DEF
 * REVOC_REG_ENTRY:
 ** Only the owner of the corresponding REVOC_REG_DEF can create new REVOC_REG_ENTRY (will be done in the scope of INDY-1554)
 ** Only owners can edit existing REVOC_REG_ENTRY
 * NODE:
 ** No changes (only Stewards can create and edit)
 * POOL_UPGRADE:
 ** No changes (only Trustees can create)

*Default Indy-Node behaviour (ANYONE_CAN_WRITE=False)*:
 There are some deviations from the existing behaviour, will be addressed in a separate ticket INDY-1554

 * NYM:
 ** New NYMs can be created by Trustee/Steward/TrustAnchor only (see permissions table)
 ** Only owner can edit existing NYMs
 * ATTRIB:
 ** Only owners can create and edit ATTRIBs
 * CLAIM_DEF:
 ** Only Trustee/Steward/TrustAnchor can create new CLAIM_DEFS
 ** Only owners can edit existing CLAIM_DEFs
 * SCHEMA:
 ** Only Trustee/Steward/TrustAnchor can create new SCHEMA
 ** No one can edit SCHEMAs
 * REVOC_REG_DEF (will be done in the scope of INDY-1554):
 ** Only Trustee/Steward/TrustAnchor can create new REVOC_REG_DEF (will be done in the scope of INDY-1554)
 ** Only owners can edit existing REVOC_REG_DEF
 * REVOC_REG_ENTRY (will be done in the scope of INDY-1554):
 ** Only the owner of the corresponding REVOC_REG_DEF can create new REVOC_REG_ENTRY
 ** Only owners can edit existing REVOC_REG_ENTRY
 * NODE:
 ** Only Stewards can create and edit
 * POOL_UPGRADE:
 ** Only Trustees can create;;;","06/Aug/18 10:28 PM;ashcherbakov;We can update Acceptance Criteria if this looks OK;;;","06/Aug/18 11:10 PM;ashcherbakov;Part of the items affects existing behaviour (for Revocation txns). Will be fixed in the scope of a separate issued INDY-1554.;;;","07/Aug/18 9:43 PM;esplinr;These permissions look great. Thank you.;;;","08/Aug/18 12:01 AM;ashcherbakov;Build: 1.5.546;;;","11/Aug/18 8:24 PM;ozheregelya;{quote}*Acceptance Criteria*
 * Create a configuration setting: ""writes-require-trust-anchor"", default to True{quote}
Parameter name is ANYONE_CAN_WRITE. It has the opposite meaning, so default value is False and roles behavior is not changed by default.;;;","12/Aug/18 3:22 AM;ozheregelya;*Environment:*
 indy-node 1.6.558
 libindy|indy-cli 1.6.1~701

*Steps to Validate:*
 0. Make  sure that ANYONE_CAN_WRITE is False by default.
 1. Run batches from attachment on pool with ANYONE_CAN_WRITE=True.
 2. Run batches from attachment on pool with ANYONE_CAN_WRITE=False.
 3. Compare outputs.
 => Identity Owner can write nym, schema, cred_def with ANYONE_CAN_WRITE=True and can't do this with ANYONE_CAN_WRITE=False. For the rest roles behavior is the same in both of cases.
 4. Try to run _ledger node_ command as each user.
 => Only Steward can *add* new node and *edit* existing one (IP, porst, etc). Existing node can be *demoted* and *promoted* back by Steward of this node and by Trustee.
 5. Try to run _ledger pool-upgrade_ command as each user.
 => Only Trustee can run pool upgrade.

*Actual Results:*
 New parameter was added, it's set to false by default and works correctly if it was set to True.

Pull request with changes in batches for acceptance testing: [https://github.com/hyperledger/indy-node/pull/890]

*Additional Information:*
{quote} * NODE:
 ** Only Stewards can create and edit{quote}
[~ashcherbakov], note that Trustee also can send NODE for node demotion/promotion. As far as I understand, this behavior should not be changed.;;;",,,,,,,,,,,,,
Intermittent failure: test_6_nodes_pool_cannot_reach_quorum_with_2_disconnected ,INDY-1529,32302,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,anikitinDSR,anikitinDSR,27/Jul/18 5:46 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/18 5:47 PM;anikitinDSR;test-result-plenum-5.prd-ubuntu1604-indy-x86_64-4c-16g-76.txt;https://jira.hyperledger.org/secure/attachment/15400/test-result-plenum-5.prd-ubuntu1604-indy-x86_64-4c-16g-76.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzkav:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,esplinr,sergey.khoroshavin,,,,,,,,,,"10/Oct/18 9:16 PM;sergey.khoroshavin;*Triage*
All other intermittent test failures are reported as tasks, so making this task as well seems reasonable.;;;","11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Improve probes for existing experiments,INDY-1530,32347,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,ozheregelya,ozheregelya,27/Jul/18 11:51 PM,06/Nov/19 9:43 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,TODO for [~ozheregelya]: provide details.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzkjb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bind connection socket to NODE_IP,INDY-1531,32348,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,sergey-shilov,sergey-shilov,28/Jul/18 1:12 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,As a part of 2 NICs support we need to forward outgoing connection through NIC for node-to-node communication. Now the default NIC is used for that. To make it deterministic we need to bind connection socket to corresponding NIC explicitly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxwf:",,,,Unset,Unset,EV 18.15 Stability/Availabilit,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,"03/Aug/18 1:18 AM;sergey-shilov;In scope of this ticket we implemented bind-before-connect. But this is not enough as bind() does not control anything about the routing of transmitted packets. The kernel uses routes to choose interface for sending even if packet source address differ from address of interface.

There is a socket option SO_BINDTODEVICE that forces packets on the socket to only egress the bound interface, regardless of what the IP routing table would normally choose.

Seems like ZMQ supports binding to device as a part of connect call:
 [http://api.zeromq.org/4-1:zmq-tcp]
but in my experiments (using python wrapper) ZMQ tried to resolve interface name using DNS queries. Seems like it does not work, at least using python wrapper.

Also ZMQ library supports ZMQ_BINDTODEVICE option, but this option is not available in python wrapper.

But anyway, binding to device is a good solution for machines that are under our control. Much more better (and also native for networking) solution in our case is to recommend stewards to configure their routing tables so that they use dedicated interfaces for node-to-node communication.

BTW, very good doc regarding binding to address and binding to device:
 [https://codingrelic.geekhold.com/2009/10/code-snippet-sobindtodevice.html];;;","03/Aug/18 1:26 AM;sergey-shilov;*Problem state / reason:*

See ticket description.

*Changes:*

Added bind-before-connect to NODE_IP.

*Committed into:*

https://github.com/hyperledger/indy-plenum/pull/845
https://github.com/hyperledger/indy-node/pull/852
 indy-node 1.5.538-master

*Risk factors:*

Nothing is expected. Complete feature needs manual configuration of routing tables by stewards.

*Risk:*

Low

*NOTE:*

I couldn't implement bind to device using ZMQ (see previous comment).;;;",,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Retry if executor fails in paramiko,INDY-1532,32356,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ckochenower,ckochenower,28/Jul/18 6:29 AM,28/Jul/18 6:30 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Running Chaos experiments in perpetuity (1000x) is intended to test how resilient Indy Node is after repeated chaos experiments.

The last few times experiments have been run, Indy Node didn't fail any of the experiments, but paramiko failed with differing reasons. Paramiko is used by Python Fabric, which is used for remote SSH execution of commands that are run as part of Chaos experiments (i.e. stop/start indy-node service, block/unblock node port, get validator information, etc.)

Paramiko failures are described by the following stack traces. In both cases a simple retry (try-except-retry) may have been sufficient to keep the perpetual execution going. In both of the following cases, experiment execution appeared to hang instead of failing and returning. Perhaps that may have something to do with running within a screen session.
{code:java}
Traceback (most recent call last):
  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/ubuntu/chaosindy/chaosindy/execute/execute.py"", line 262, in do_work
    as_sudo=as_sudo, **kwargs_dict)
  File ""/home/ubuntu/chaosindy/chaosindy/execute/execute.py"", line 205, in _parallel_execute_on_host
    rtn = c.sudo(action, hide=True)
  File ""<decorator-gen-4>"", line 2, in sudo
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/fabric-2.1.3-py3.5.egg/fabric/connection.py"", line 29, in opens
    self.open()
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/fabric-2.1.3-py3.5.egg/fabric/connection.py"", line 501, in open
    self.client.connect(**kwargs)
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 424, in connect
    passphrase,
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 714, in _auth
    raise saved_exception
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 630, in _auth
    key_filename, pkey_class, passphrase,
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 551, in _key_from_filepath
    key = klass.from_private_key_file(key_path, password)
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/pkey.py"", line 206, in from_private_key_file
    key = cls(filename=filename, password=password)
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/ed25519key.py"", line 74, in __init__
    data = self._read_private_key(""OPENSSH"", f)
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/pkey.py"", line 289, in _read_private_key 
    raise SSHException('not a valid ' + tag + ' private key file')
paramiko.ssh_exception.SSHException: not a valid OPENSSH private key file {code}
{code:java}
[2018-07-27 09:30:57 DEBUG] [execute:248] kwargs: {""connect_kwargs"": null, ""connect_timeout"": 10}
Process Process-93:
Traceback (most recent call last):
  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 249, in _bootstrap
    self.run()
  File ""/usr/lib/python3.5/multiprocessing/process.py"", line 93, in run
    self._target(*self._args, **self._kwargs)
  File ""/home/ubuntu/chaosindy/chaosindy/execute/execute.py"", line 262, in do_work
    as_sudo=as_sudo, **kwargs_dict)
  File ""/home/ubuntu/chaosindy/chaosindy/execute/execute.py"", line 205, in _parallel_execute_on_host
    rtn = c.sudo(action, hide=True)
  File ""<decorator-gen-4>"", line 2, in sudo
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/fabric-2.1.3-py3.5.egg/fabric/connection.py"", line 29, in opens
    self.open()
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/fabric-2.1.3-py3.5.egg/fabric/connection.py"", line 501, in open
    self.client.connect(**kwargs)
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 338, in connect
    retry_on_signal(lambda: sock.connect(addr))
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/util.py"", line 279, in retry_on_signal
    return function()
  File ""/home/ubuntu/.venvs/chaostoolkit/lib/python3.5/site-packages/paramiko-2.4.1-py3.5.egg/paramiko/client.py"", line 338, in <lambda>
    retry_on_signal(lambda: sock.connect(addr))
socket.timeout: timed out
[2018-07-27 09:31:00 DEBUG] [execute:243] Execute on host...
[2018-07-27 09:31:00 DEBUG] [execute:244] host: Node10
[2018-07-27 09:31:00 DEBUG] [execute:245] action: validator-info -v --json
[2018-07-27 09:31:00 DEBUG] [execute:246] user: None
[2018-07-27 09:31:00 DEBUG] [execute:247] as_sudo: True
[2018-07-27 09:31:00 DEBUG] [execute:248] kwargs: {""connect_kwargs"": null, ""connect_timeout"": 10}
[2018-07-27 09:31:04 DEBUG] [execute:229] [P0] routine quits {code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzkk7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a developer, I don't want to worry about setting up an environment in which to replay a node recording",INDY-1533,32357,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ckochenower,ckochenower,28/Jul/18 6:39 AM,28/Jul/18 6:42 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"When nscapture is used to capture a node state archive, the archive contains sufficient information to reproduce a replay environment using the same python, pip module, and apt package versions.

Rather than leaving it up to a developer to create a VM/container manually in which to replay a recording, enhance nsreplay to provide an option to instantiate and replay a node inside a docker container. nsreplay would use the contents of the config and capture directories in the node state archive to produce an equivalent node execution environment inside a docker container.",,,,,,,,,,INDY-1534,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzkkf:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nscapture - collect pip and apt repo information,INDY-1534,32358,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,devin-fisher,ckochenower,ckochenower,28/Jul/18 6:42 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,test-automation,,,0,,,,Collect pip and apt repo configuration and store them in the nscapture node state archive. The replay environment should be configured to use the same set of repos during an nsreplay.,,,,,,,,,,,,,,,,,INDY-1533,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzkkn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"17/Aug/18 2:15 AM;ckochenower;[~devin-fisher] completed this task;;;",,,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Chase the master,INDY-1535,32359,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,,,ckochenower,ckochenower,28/Jul/18 6:47 AM,28/Jul/18 6:47 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Originally, the force-view-change experiment was designed to discover the master, stop it, and then continue to do so without waiting for a viewchange to complete. Running this version of force-view-change was successful in creating a split master condition, but stopped producing a split master condition once it was changed to wait for a viewchange up to a configurable timeout (360 seconds).

[~krw910] and [~ckochenower] think a ""chase the master"" as fast as possible experiment is useful. Therefore, this ticket tracks the need to recreate such an experiment and use it to see how indy-node does under this condition as we improve performance and stability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzkkv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After restarting 7 of 12 nodes, the STN is not coming to consensus",INDY-1536,32381,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,mgbailey,mgbailey,29/Jul/18 9:24 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,TShirt_M,,,"In the course of a test, 7 of the 12 nodes on the current STN had their indy-node service stopped for an hour.  After the service was restarted, an attempt was made to post a transaction to the network, which was unsuccessful.  The DID that I attempted to post was 7SoiEBoPkwDexrbb7eYFX6. Inspecting the logs of the 7 nodes that I have access to, I see that they all agree on the primary (australia).

I have asked the stewards of the remaining 5 nodes to restart their indy-node service as well. Meanwhile, information on the reason that we do not have consensus is needed.  Logs of the 7 nodes are attached.
{code:java}
mikebailey@rstn-seo-pe001:~$ sudo validator-info -v
[sudo] password for mikebailey:
2018-07-29 00:08:27,007 | DEBUG    | validator-info       ( 662) | main | Cmd line arguments: Namespace(basedir='/var/lib/indy/sandbox', json=False, log='/var/log/indy/validator-info.log', stdlog=False, verbose=True)
2018-07-29 00:08:27,008 | INFO     | validator-info       ( 699) | main | Reading file /var/lib/indy/sandbox/korea_info.json ...
2018-07-29 00:08:27,008 | DEBUG    | validator-info       ( 548) | get_stats_from_file | Data {'timestamp': 1532822889, 'bindings': {'client': {'protocol': 'tcp', 'port': 9702}, 'node': {'protocol': 'tcp', 'port': 9701}}, 'alias': 'korea', 'verkey': '5YZXWNmgadNSedFcEaFjbtNmGDzfe7WMvHh9mKFDawdnnG7h6oAgLkj', 'response-version': '0.0.1', 'metrics': {'average-per-second': {'read-transactions': 0.0, 'write-transactions': 0.0}, 'transaction-count': {'pool': 41, 'config': 522, 'ledger': 2522}, 'uptime': 1680}, 'did': 'HCNuqUoXuK9GXGd2EULPaiMso2pJnxR6fCZpmRYbc7vM', 'pool': {'total-count': 12, 'unreachable': {'list': [], 'count': 0}, 'reachable': {'list': ['RFCU', 'VeridiumIDC', 'amihan-sovrin', 'australia', 'brazil', 'canada', 'england', 'findentity', 'korea', 'singapore', 'trustscience-validator01', 'virginia'], 'count': 12}}, 'software': {'sovrin': '1.1.9', 'indy-node': '1.3.62'}}
2018-07-29 00:08:27,012 | DEBUG    | validator-info       ( 184) | shell_cmd | command 'ss -ln4 | sort -u | grep ':9701\s'': stdout 'b'tcp    LISTEN     0      100       *:9701                  *:*                  \n''
2018-07-29 00:08:27,013 | INFO     | validator-info       ( 258) | __init__ | Found the following bindings for target node with port 9701: [('tcp', '0.0.0.0/0')]
2018-07-29 00:08:27,017 | DEBUG    | validator-info       ( 184) | shell_cmd | command 'ss -ln4 | sort -u | grep ':9702\s'': stdout 'b'tcp    LISTEN     0      100       *:9702                  *:*                  \n''
2018-07-29 00:08:27,017 | INFO     | validator-info       ( 258) | __init__ | Found the following bindings for target client with port 9702: [('tcp', '0.0.0.0/0')]
Validator korea is running
Current time:     Sunday, July 29, 2018 12:08:09 AM
Validator DID:    HCNuqUoXuK9GXGd2EULPaiMso2pJnxR6fCZpmRYbc7vM
Verification Key: 5YZXWNmgadNSedFcEaFjbtNmGDzfe7WMvHh9mKFDawdnnG7h6oAgLkj
Node Port:        9701/tcp on 0.0.0.0/0
Client Port:      9702/tcp on 0.0.0.0/0
Metrics:
  Uptime: 28 minutes, 0 seconds
  Total Config Transactions:  522
  Total Ledger Transactions:  2522
  Total Pool Transactions:    41
  Read Transactions/Seconds:  0.00
  Write Transactions/Seconds: 0.00
Reachable Hosts:   12/12
  RFCU
  VeridiumIDC
  amihan-sovrin
  australia
  brazil
  canada
  england
  findentity
  korea
  singapore
  trustscience-validator01
  virginia
Unreachable Hosts: 0/12
Software Versions:
  indy-node: 1.3.62
  sovrin: 1.1.9
{code}","STN, running indy-node 1.3.62",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/18 9:24 AM;mgbailey;rstn-can-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15413/rstn-can-pe001_log.tgz","29/Jul/18 9:24 AM;mgbailey;rstn-lon-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15412/rstn-lon-pe001_log.tgz","29/Jul/18 9:24 AM;mgbailey;rstn-nva-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15411/rstn-nva-pe001_log.tgz","29/Jul/18 9:24 AM;mgbailey;rstn-sao-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15410/rstn-sao-pe001_log.tgz","29/Jul/18 9:24 AM;mgbailey;rstn-seo-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15409/rstn-seo-pe001_log.tgz","29/Jul/18 9:24 AM;mgbailey;rstn-sgp-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15408/rstn-sgp-pe001_log.tgz","29/Jul/18 9:24 AM;mgbailey;rstn-syd-pe001_log.tgz;https://jira.hyperledger.org/secure/attachment/15407/rstn-syd-pe001_log.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwkn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),mgbailey,,,,,,,,,,,,"30/Jul/18 10:52 AM;mgbailey;3 of the 5 external stewards have restarted the service on their node, and we have consensus again.  I am lowering the priority on this ticket.;;;","03/Aug/18 7:25 AM;mgbailey;[~Derashe] The logs on the nodes themselves are corrupt. The corruption is only during the time frame of concern, and the newer logs are fine.  I think we will just need to close this ticket out.;;;",,,,,,,,,,,,,,,,,,,,,,,
Indy's Promotion on the StackOverflow site,INDY-1537,32403,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,esplinr,zhigunenko.dsr,zhigunenko.dsr,30/Jul/18 6:48 PM,30/Jul/18 6:48 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"StackOverflow is ""default"" help for developers nowadays. Info coverage's improvement would help product recognizability and decrease user support costs via Rocketchat

First, improve [hyperledger-indy] tage like https://stackoverflow.com/tags/java/info
List would be continued...",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzktb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_no_propagated_future_view_change_while_view_change,INDY-1538,32405,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,30/Jul/18 9:16 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/18 9:16 PM;ashcherbakov;test-result-plenum-5.prd-ubuntu1604-indy-x86_64-4c-16g-430.txt.zip;https://jira.hyperledger.org/secure/attachment/15420/test-result-plenum-5.prd-ubuntu1604-indy-x86_64-4c-16g-430.txt.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzkpj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool has stopped to write txns,INDY-1539,32412,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,VladimirWork,VladimirWork,31/Jul/18 1:42 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,TShirt_L,,,"Build Info:
indy-node 1.5.526

Steps to Reproduce:
0. Install pool of 25 nodes.
1. Set client stack restart parameters:
{noformat}
MAX_CONNECTED_CLIENTS_NUM = 10
MIN_STACK_RESTART_TIMEOUT = 600  # seconds
CLIENT_STACK_RESTART_ENABLED = True
{noformat}
2. Run several load tests changing CLIENT_STACK_RESTART_ENABLED between True and False:
{noformat}
python3 perf.py -c 10 -n 10 -t 1 -g pool_transactions_genesis
python3 perf.py -c 20 -n 10 -t 1 -g pool_transactions_genesis
python3 perf.py -c 30 -n 10 -t 1 -g pool_transactions_genesis
python3 perf.py -c 10 -n 10 -t 2 -g pool_transactions_genesis
python3 perf.py -c 15 -n 10 -t 2 -g pool_transactions_genesis
{noformat}
3. Restart the whole pool several times.

Actual Results:
1. Pool has stopped to write txns with PoolLedgerTimeout error.
2. Pool is unable to perform catchup.
3. *17* of 25 nodes have *326349* txns written.
4. *8* of 25 nodes have *326359* txns written.
5. Validator-info shows all nodes as reachable.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1545,,,,,,,,,,,,,,,"31/Jul/18 8:31 PM;VladimirWork;jctl+info.tar.gz;https://jira.hyperledger.org/secure/attachment/15427/jctl%2Binfo.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzllr:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"31/Jul/18 7:10 PM;VladimirWork;All logs are in /home/ev/logs/1539/.;;;","01/Aug/18 5:48 PM;Toktar;View change started after 326349 transactions (0, 703). It had happened one by one. Mostly with reason ""26"" (primary disconnected), in the secondly time with reasons
 * ""21"" (pre-prepare has incorrect state root hash)
 * ""28"" (view change could not complete on time)
 * ""25"" (master degraded)

Incorrect state root hash was in the situation after view change because nodes had different transactions count (326349 and 326359). Primary had the lowest and had other root hash.;;;","01/Aug/18 10:01 PM;Toktar;Pre-prepare has incorrect state root hash happened because in joint of two checkpoints was cleaning a queue with preprepares. It was a reason for unordered transactions in correct view_no. As a result, _f_ nodes had other root hash and lost consensus.

Test plan:
 # Send request with write transaction.
 # Delay commits for this transaction in moment will have prepare sertificate .
 # Start checkpoint garbage cleaning and delay processing of checkpoints.
 # Start view change.
 # Cancel checkpoints delays.
 # Check cleaning was finish.
 # Cancel commits delays.
 # Check transaction ordered..
 # Check view change done

Fix in PR: https://github.com/hyperledger/indy-plenum/pull/854;;;",,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - View change subversion - Demote Replica,INDY-1540,32418,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ckochenower,ckochenower,ckochenower,31/Jul/18 4:32 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,test-automation,,,0,,,,"Demote a single backup primary (replica). It is important to note that this effectively reduces the consensus pool by 1 (i.e. from 10 to 9). The pool's ""f_value"" and ""Count_of_replicas"" are based on validator pool size. Expect the next node in the round robin to become a new replica.

Start a 10 validator node pool with the following Primary and Replica configuration:
 Node1:0 - Primary
 Node2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
 Node3:2 - Backup to node 2
 Node4:3 - Backup to node 3

Demote Node4 and expect Node5 to become a new replica.

Node1:0 - Primary
 Node2:1 - Next in line for primary (backup if Node1 goes down (causes view change) or a view change is requested)
 Node3:2 - Backup to Node2
 Node5:3 - Backup to Node3

If Node3 is demoted:

Demote Node3 and expect Node5 to become a new replica.

Node1:0 - Primary
 Node2:1 - Next in line for primary (backup if Node1 goes down (causes view change) or a view change is requested)
 Node4:2 - Backup to Node2
 Node5:3 - Backup to Node4

The experiment should be written in a way that it could be run over and over again (perpetual) w/o the need to reset the cluster. Therefore, ""Demote the last replica and expect the next node in round robin to be selected"" is more accurate.",,,,,,,,,,,,,,,,,,,,,INDY-1541,,INDY-1511,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Aug/18 1:57 AM;ckochenower;Screen Shot 2018-08-03 at 10.19.14 AM.png;https://jira.hyperledger.org/secure/attachment/15456/Screen+Shot+2018-08-03+at+10.19.14+AM.png","04/Aug/18 2:08 AM;ckochenower;Screen Shot 2018-08-03 at 10.22.09 AM.png;https://jira.hyperledger.org/secure/attachment/15459/Screen+Shot+2018-08-03+at+10.22.09+AM.png","04/Aug/18 1:41 AM;ckochenower;Screen Shot 2018-08-03 at 10.36.57 AM.png;https://jira.hyperledger.org/secure/attachment/15454/Screen+Shot+2018-08-03+at+10.36.57+AM.png","04/Aug/18 2:30 AM;ckochenower;Screen Shot 2018-08-03 at 11.21.56 AM.png;https://jira.hyperledger.org/secure/attachment/15460/Screen+Shot+2018-08-03+at+11.21.56+AM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzkvr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,krw910,,,,,,,,,,,"04/Aug/18 2:11 AM;ckochenower;*Summary*

Demoting a replica (backup primary) does not appear to trigger a reelection/selection of a replacement unless a view change occurs.

*Details*

f = number of nodes that can fail and still reach consensus

n= number of replicas

My 10 node cluster (f = 3, n = 4) has the following configuration according to each node in the pool. The state of replicas (:0 through :3) are highlighted (orange rectangle):

!Screen Shot 2018-08-03 at 10.36.57 AM.png|thumbnail!

The following was observed when demoting the last replica (Node4). Notice that ""Replicas 1 through 3"" changes to ""Replicas 1 through 2""?  ""Count_of_replicas"" reported by validator-info changes from 4 to 3. Please note that the master is replica "":0"". 

!Screen Shot 2018-08-03 at 10.19.14 AM.png|thumbnail!

It appears that demoting a replica does not immediately trigger the pool to select/elect a replacement. It also appears that the pool does not select/elect a replacement within a reasonable amount of time (minutes). I did not wait more than 10 minutes.

Promoting the demoted node (Node4) eventually returns the pool to it's original state. Once Node4 is promoted back into the pool, the nodes, immediately begin to report it as replica :3 as if it had never been demoted.

!Screen Shot 2018-08-03 at 10.22.09 AM.png|thumbnail!

Note that ""Count_of_replicas"" (""Replicas 1 through 2"") is still 2 (above) and does not change back to three (below) until the rest of the nodes report Node4 as replica :3.

!Screen Shot 2018-08-03 at 10.36.57 AM.png|thumbnail!

 ;;;","07/Aug/18 4:53 AM;ckochenower;Demoting a single replica does not produce a blast radius sufficient to observe anything interesting unless a view change is forced to occur. Closing this issue in favor of INDY-1541.;;;","08/Aug/18 12:15 AM;ckochenower;Simply demoting a replica doesn't result in a new replica being selected/elected. A view change is required to trigger the selection/election of a replacement. [~krw910] - Is this a problem? Perhaps the real problem is that a node can select itself as one of it's own replicas? 
The pool does not fall out of consensus and no negative side-effects are observed when the demoted node is promoted back into the pool. When promoted back into the pool, the node resumes it's role as a replica for other nodes w/o any negative side effects.;;;","08/Aug/18 11:11 PM;krw910;[~ckochenower] I think we are fine with the scenario.;;;",,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - View change subversion - Replica Down + Force View Change,INDY-1541,32419,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ckochenower,ckochenower,ckochenower,31/Jul/18 4:39 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,test-automation,,,0,,,,"Demote a single backup primary (replica) and the primary. Expect the next replica to become the primary and the next node in the round robin to become a new replica.

Start a 10 validator node pool with the following Primary and Replica configuration:
 Node1:0 - Primary
 Node2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
 Node3:2 - Backup to node 2
 Node4:3 - Backup to node 3

Take down Node4 and the primary (Node1) and expect Node2 to become the new primary and Node5 to become a new replica.

The result being:

 Node2:0 - Primary
 Node3:1 - Next in line for primary (backup if node 2 goes down or a view change is requested)
 Node5:2 - Backup to node 3
 Node6:3 - Backup to node 5",,,,,,,,,,,,,,,,,,,,,,,INDY-1540,,,,,,,,,,,INDY-1575,INDY-1577,INDY-1578,,,,,,,,,,,,,"07/Aug/18 5:17 AM;ckochenower;Screen Shot 2018-08-06 at 2.03.18 PM.png;https://jira.hyperledger.org/secure/attachment/15465/Screen+Shot+2018-08-06+at+2.03.18+PM.png","08/Aug/18 5:33 AM;ckochenower;Screen Shot 2018-08-07 at 2.13.14 PM.png;https://jira.hyperledger.org/secure/attachment/15470/Screen+Shot+2018-08-07+at+2.13.14+PM.png","08/Aug/18 5:37 AM;ckochenower;Screen Shot 2018-08-07 at 2.36.04 PM.png;https://jira.hyperledger.org/secure/attachment/15471/Screen+Shot+2018-08-07+at+2.36.04+PM.png","08/Aug/18 5:40 AM;ckochenower;Screen Shot 2018-08-07 at 2.40.13 PM.png;https://jira.hyperledger.org/secure/attachment/15472/Screen+Shot+2018-08-07+at+2.40.13+PM.png","10/Aug/18 12:16 AM;ckochenower;Screen Shot 2018-08-09 at 8.16.54 AM.png;https://jira.hyperledger.org/secure/attachment/15487/Screen+Shot+2018-08-09+at+8.16.54+AM.png","10/Aug/18 12:17 AM;ckochenower;Screen Shot 2018-08-09 at 8.48.03 AM.png;https://jira.hyperledger.org/secure/attachment/15489/Screen+Shot+2018-08-09+at+8.48.03+AM.png","10/Aug/18 12:16 AM;ckochenower;Screen Shot 2018-08-09 at 8.48.03 AM.png;https://jira.hyperledger.org/secure/attachment/15488/Screen+Shot+2018-08-09+at+8.48.03+AM.png","11/Aug/18 6:59 AM;ckochenower;Screen Shot 2018-08-10 at 3.56.27 PM.png;https://jira.hyperledger.org/secure/attachment/15514/Screen+Shot+2018-08-10+at+3.56.27+PM.png","11/Aug/18 7:03 AM;ckochenower;Screen Shot 2018-08-10 at 4.02.03 PM.png;https://jira.hyperledger.org/secure/attachment/15515/Screen+Shot+2018-08-10+at+4.02.03+PM.png","11/Aug/18 7:06 AM;ckochenower;Screen Shot 2018-08-10 at 4.04.57 PM.png;https://jira.hyperledger.org/secure/attachment/15516/Screen+Shot+2018-08-10+at+4.04.57+PM.png","11/Aug/18 7:13 AM;ckochenower;Screen Shot 2018-08-10 at 4.10.22 PM.png;https://jira.hyperledger.org/secure/attachment/15518/Screen+Shot+2018-08-10+at+4.10.22+PM.png","11/Aug/18 7:11 AM;ckochenower;Screen Shot 2018-08-10 at 4.10.22 PM.png;https://jira.hyperledger.org/secure/attachment/15517/Screen+Shot+2018-08-10+at+4.10.22+PM.png","11/Aug/18 7:18 AM;ckochenower;Screen Shot 2018-08-10 at 4.15.40 PM.png;https://jira.hyperledger.org/secure/attachment/15519/Screen+Shot+2018-08-10+at+4.15.40+PM.png","11/Aug/18 7:23 AM;ckochenower;Screen Shot 2018-08-10 at 4.19.35 PM.png;https://jira.hyperledger.org/secure/attachment/15520/Screen+Shot+2018-08-10+at+4.19.35+PM.png","10/Aug/18 1:08 AM;ckochenower;run-demote-replica.out;https://jira.hyperledger.org/secure/attachment/15490/run-demote-replica.out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzkvz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ckochenower,,,,,,,,,,,"07/Aug/18 5:19 AM;ckochenower;Results:

 !Screen Shot 2018-08-06 at 2.03.18 PM.png|thumbnail! 

The demoted node (Node4) is the only node that ended up with the expected state. I did not account for the fact that the number of replicas would be reduced from 4 to 3 when dropping the number of nodes participating in consensus was reduced from 10 to 9.;;;","08/Aug/18 5:41 AM;ckochenower;Results of demoting two replicas:

 !Screen Shot 2018-08-07 at 2.13.14 PM.png|thumbnail! 

*Experiment Steps:*
RC = node_info['Count_of_replicas']
F = validator_info['Pool_info']['f_value']
# Start with 10 nodes (RC = 4, F = 3) and steady state is met (can write an nym)
# Demote two of the replicas. Node4 and Node3 were demoted (RC = 3, F = 2)
# Force a view change by stopping indy-node on the master (Node1)
# Wait for a view change
# Start indy-node on the original master (Node1)
# Steady state is met
# Rollback Steps (best effort to undo changes made during experiment):
## Promote and restart Node3
## Promote and restart Node4
## Start indy-node on the original master (Node1)

Why are Node1 (original master) and Node4 still reporting RC = 3 and F = 2? Node1's indy-node service was started AFTER the view change. Both Node3 and Node4 promoted and indy-node service was restarted AFTER they were promoted. Why would Node3 report the correct replica count (RC) and f value (F) and Node4 would not?

Restarting Node4 a *second* time seems to improve the situation. RC = 4 and F = 3 with Node7 selected/elected as R3:

 !Screen Shot 2018-08-07 at 2.36.04 PM.png|thumbnail! 

Restarting Node1 seems to improve the situation. RC = 4 and F = 3 with Node7 selected/elected as R3. Note that the Node1 was the original master when the experiment began. It was stopped (indy-node service stopped) to force a view change and was started (indy-node service started) AFTER the view change was detected.

 !Screen Shot 2018-08-07 at 2.40.13 PM.png|thumbnail! 

[~spivachuk] or [~ashcherbakov] - Your feedback would be appreciated.;;;","08/Aug/18 9:11 AM;ckochenower;Running the demote-replica experiment described above for 5 iterations results in failures (ledger timeout when trying to write a nym) after at most 4 iterations. 



;;;","08/Aug/18 7:31 PM;ashcherbakov;[~ckochenower]
I think a ticket needs to be created for this issue.
Can it be that Node 1 and Node 4 were not stopped and re-started properly for the first time?;;;","09/Aug/18 12:19 AM;ckochenower;[~ashcherbakov] - I suppose it is possible. [stop_by_node_name|https://github.com/evernym/chaosindy/blob/master/chaosindy/actions/node.py#L128-L149] in chaosindy.actions.node, by default (""gracefully"" and ""force"" kwargs), will attempt to gracefully shutdown the indy-node service, but will then attempt to kill associated pids if the graceful attempt to shutdown indy-node times out after 30 seconds.

 I will dig through the experiment output to make sure indy-node was stopped and later started ""gracefully"" for both Node1 and Node4. If both nodes report that a `systemctl stop indy-node` fails or times out, we will have a potential explanation.;;;","09/Aug/18 1:14 AM;ckochenower;[~ashcherbakov] - The indy-node-control service is started when the indy-node service is started, but is not stopped when the indy-node service is stopped. Is this by design?

When a node is promoted, the node needs to be restarted as part of the promotion workflow. Does only the indy-node service need to be restarted or does the indy-node-control service also need to be restarted?;;;","09/Aug/18 3:46 AM;ckochenower;`systemctl stop <service>` does not wait until a unit terminates before it returns successful (return code 0). indy-node has been known to take up to a minute to gracefully stop. I will enhance the stop_by_node_name function to wait up to 60 seconds (poll every 6 seconds up to 10 times) for the pids associated with indy-node and indy-node-control to disappear.;;;","09/Aug/18 6:40 PM;ashcherbakov;{quote}Does only the indy-node service need to be restarted or does the indy-node-control service also need to be restarted?
{quote}
`indy-node` service is enough.
{quote}`systemctl stop <service>` does not wait until a unit terminates before it returns successful (return code 0). indy-node has been known to take up to a minute to gracefully stop. I will enhance the stop_by_node_name function to wait up to 60 seconds (poll every 6 seconds up to 10 times) for the pids associated with indy-node and indy-node-control to disappear.
{quote}
Ok, hopefully that should help.;;;","10/Aug/18 12:17 AM;ckochenower;Experiment results after making the aforementioned improvements to stop_by_node_name. suggest the ""stuck"" nodes are likely a node issue rather than an issue with the chaos experiment.

 !Screen Shot 2018-08-09 at 8.48.03 AM.png|thumbnail!

The following output is an extract from the full experiment output (  [^run-demote-replica.out] ).  It shows that nodes that are promoted by the experiment are definitely being restarted (guaranteed stop followed by start).

{code}
(chaostoolkit) ubuntu@KellyStableClientVirgina:~/chaosindy$ grep ""^*"" ./run-demote-replica.out 
********************** Experiment Iteration 1 of 1 ********************
*[2018-08-09 15:09:39 INFO] [experiment:188] Running experiment: Demote Replica
*[2018-08-09 15:09:39 INFO] [activity:158] Probe: can-write-nym
*[2018-08-09 15:09:47 INFO] [hypothesis:179] Steady state hypothesis is met!
*[2018-08-09 15:09:47 INFO] [activity:158] Action: demote-n-backup-primaries
*[2018-08-09 15:10:06 DEBUG] [node:767] Demoting Node4
*[2018-08-09 15:10:10 DEBUG] [node:767] Demoting Node3
*[2018-08-09 15:10:14 INFO] [activity:158] Action: force-a-view-change
*[2018-08-09 15:10:29 DEBUG] [node:162] stop node: Node1
*[2018-08-09 15:10:32 INFO] [activity:158] Action: start-stopped-primary-after-view-change
*[2018-08-09 15:10:48 DEBUG] [primary:146] Check 0 of 6 if view change is complete
*[2018-08-09 15:10:48 DEBUG] [primary:147] Former primary: Node1
*[2018-08-09 15:10:48 DEBUG] [primary:148] Current primary: Node1
*[2018-08-09 15:11:17 DEBUG] [primary:146] Check 1 of 6 if view change is complete
*[2018-08-09 15:11:17 DEBUG] [primary:147] Former primary: Node1
*[2018-08-09 15:11:17 DEBUG] [primary:148] Current primary: None
*[2018-08-09 15:11:47 DEBUG] [primary:146] Check 2 of 6 if view change is complete
*[2018-08-09 15:11:47 DEBUG] [primary:147] Former primary: Node1
*[2018-08-09 15:11:47 DEBUG] [primary:148] Current primary: Node2
*[2018-08-09 15:11:47 DEBUG] [primary:150] View change detected!
*[2018-08-09 15:11:48 INFO] [activity:150] Pausing before next activity for 60s...
*[2018-08-09 15:12:48 INFO] [activity:158] Probe: check-demoted-backup-primary-status
*[2018-08-09 15:13:04 INFO] [hypothesis:149] Steady state hypothesis: Can write a nym
*[2018-08-09 15:13:04 INFO] [activity:158] Probe: can-write-nym
*[2018-08-09 15:13:17 INFO] [hypothesis:179] Steady state hypothesis is met!
*[2018-08-09 15:13:17 INFO] [rollback:28] Rollback: promote-demoted-backup-primaries
*[2018-08-09 15:13:17 INFO] [activity:158] Action: promote-demoted-backup-primaries
*[2018-08-09 15:13:17 DEBUG] [node:821] Promoting Node4
*[2018-08-09 15:13:21 DEBUG] [node:831] Restart Node4
*[2018-08-09 15:13:21 DEBUG] [node:162] stop node: Node4
*[2018-08-09 15:13:21 DEBUG] [node:166] Attempting to stop indy-node service gracefully...
*[2018-08-09 15:13:24 DEBUG] [node:184] Ensuring node services are stopped: try 0...
*[2018-08-09 15:13:24 DEBUG] [node:149] Ensure indy-node/indy-node-control is stopped...
*[2018-08-09 15:13:26 DEBUG] [node:191] Node services guaranteed to be stopped.
*[2018-08-09 15:13:26 DEBUG] [node:213] start node: Node4
*[2018-08-09 15:13:28 DEBUG] [node:742] Setting Node3's services to >VALIDATOR<
*[2018-08-09 15:13:38 DEBUG] [node:831] Restart Node3
*[2018-08-09 15:13:38 DEBUG] [node:162] stop node: Node3
*[2018-08-09 15:13:38 DEBUG] [node:166] Attempting to stop indy-node service gracefully...
*[2018-08-09 15:13:39 DEBUG] [node:184] Ensuring node services are stopped: try 0...
*[2018-08-09 15:13:39 DEBUG] [node:149] Ensure indy-node/indy-node-control is stopped...
*[2018-08-09 15:13:40 DEBUG] [node:191] Node services guaranteed to be stopped.
*[2018-08-09 15:13:40 DEBUG] [node:213] start node: Node3
*[2018-08-09 15:13:41 INFO] [rollback:28] Rollback: start-stopped-primary
*[2018-08-09 15:13:41 INFO] [activity:158] Action: start-stopped-primary
*[2018-08-09 15:13:41 DEBUG] [node:213] start node: Node1
*[2018-08-09 15:13:43 INFO] [rollback:28] Rollback: cleanup
*[2018-08-09 15:13:43 INFO] [activity:158] Action: cleanup
{code}
;;;","10/Aug/18 3:55 PM;ashcherbakov;Can you please create a bug ticket for this? Can you please attach logs for demoted/promoted nodes as well as for at least 1 normal node?;;;","11/Aug/18 6:46 AM;ckochenower;[~ashcherbakov] - Yes, I will create a ticket. I would like to remove chaosindy from the equation and manually simulate the process to determine if it is a race condition of some sort. Either way, a ticket will be created.

I will do the following manually:

Steps 9-13 are likely where we may be getting a race condition. As each node is promoted, the pool's f_value and Count_of_replicas are potentially affected. Going from 8 to 9 nodes _f_ = 2 = (8-1)/3 vs. 9 to 10 nodes _f_ = 3 = (9-1)/3 changes f_value to 3 and Count_of_replicas to 4.

# Reset the pool. Delete all data directories and restart the nodes. The result should be 10 nodes, f_value = 3, Count_of_replicas=4, and 12 or so transactions on the domain ledger. Node1 will be the master, Node2-4 are the backup primaries (replicas).
# Demote Node4 using indy-cli
# Demote Node3 using indy-cli
# Stop indy-node on Node1 to force a view change
# Wait for the view change to complete
# Start indy-node on Node1
# Wait for Node1 to catchup
# Promote Node3 using indy-cli
# Restart indy-node on Node3
# Observe that f_value and Count_of_replicas are not changed (f_value = 2, Count_of_replicas = 3).
# Promote Node4 using indy-cli
# Restart indy-node on Node4
# Observe that f_value and Count_of_replicas were both increased by 1 (f_value = 3, Count_of_replicas = 4).

If we get different results, perhaps we are dealing with a race condition.;;;","11/Aug/18 8:56 AM;ckochenower;Results of manually simulating ""Demote Replica"" chaos experiment:

2. Demote Node4 using indy-cli
3. Demote Node3 using indy-cli

Following Node4/3 demotion: Node3 is still a replica for all nodes in the cluster. *Is this a problem?*

 !Screen Shot 2018-08-10 at 3.56.27 PM.png|thumbnail! 

4. Stop indy-node on Node1 to force a view change
5. Wait for the view change to complete

A replacement for Node3 as a replica only happens after a view change:
 !Screen Shot 2018-08-10 at 4.02.03 PM.png|thumbnail! 

6. Start indy-node on Node1
7. Wait for Node1 to catchup
 !Screen Shot 2018-08-10 at 4.04.57 PM.png|thumbnail! 

8. Promote Node3 using indy-cli
9. Restart indy-node on Node3

Node3 catches up with the rest of the participating nodes.
 !Screen Shot 2018-08-10 at 4.10.22 PM.png|thumbnail! 

10. Observe that f_value and Count_of_replicas are not changed (f_value = 2, Count_of_replicas = 3).
 !Screen Shot 2018-08-10 at 4.10.22 PM.png|thumbnail! 

11. Promote Node4 using indy-cli
After promoting Node4 and before restarting Node4, replicas are selected/elected.
 !Screen Shot 2018-08-10 at 4.15.40 PM.png|thumbnail! 

12. Restart indy-node on Node4
13. Observe that f_value and Count_of_replicas were both increased by 1 (f_value = 3, Count_of_replicas = 4).
 !Screen Shot 2018-08-10 at 4.19.35 PM.png|thumbnail! 

*Conclusion*: We appear to have a race condition causing the experiment to result in invalid/unexpected state. Chaos experiments are able to execute the above steps faster than I could manually.;;;","13/Aug/18 4:22 PM;ashcherbakov;> Following Node4/3 demotion: Node3 is still a replica for all nodes in the cluster. *Is this a problem?*
This is expected. As of now, re-election is done only during the view change, and view change is triggered only if master's primary is degraded or stopped. If one of backups' primaries is stopped/degraded/demoted - the view change will not be triggered. 
We may (and probably should) improve it in future, but as of now this is expected.;;;","14/Aug/18 12:01 AM;ckochenower;[~ashcherbakov] - I created [INDY-1577|https://jira.hyperledger.org/browse/INDY-1577] assuming a demoted node playing the role of replica was a problem. I will close the issue once you or [~spivachuk] respond to the two questions on the issue.;;;",,,,,,,,,,,
Validator Info must show committed and uncommitted roots for all states,INDY-1542,32432,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,31/Jul/18 5:44 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,validator-info,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzllz:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"02/Aug/18 10:34 PM;anikitinDSR;Reasons:
 * need to add committed and uncommitted roots displaying for state
 * some fixes, like fix HDD_used_by_node calculating, add limit for uncommitted txns from ledger, muppy memory-profiler

Changes:
 * add fixes from prevoius step

Version:
 * indy-node 1.5.538

Steps to validate:

Run load_test and check, that next fields are exist in output of 'validator-info -v':

- Committed_ledger_root_hashes

- Committed_state_root_hashes

- Uncommitted_ledger_root_hashes

- Uncommitted_ledger_txns

- Uncommitted_state_root_hashes

 ;;;","03/Aug/18 11:42 PM;VladimirWork;Build Info:
indy-node 1.5.541

Steps to Validate:
1. Run load test.
2. Check that new fields exist and have values.

Actual Results:
{noformat}
root@d0589e753342:/home/indy# validator-info -v | grep Committed_state_root_hashes -A 3   
     ""Committed_state_root_hashes"": 
          ""1"":            b'Fya9dkb2frkNLvBBmKWpmPx9xk32pHjCT215fbz9KpnF'
          ""2"":            b'DfNLmH4DAHTKv63YPFJzuRdeEtVwF5RtVnvKYHd8iLEA'
          ""0"":            b'H59VbR38tmmnyyDovb65XGB1jGv4Z8e4hbXcwurwDRBb'
root@d0589e753342:/home/indy# validator-info -v | grep Uncommitted_state_root_hashes -A 3
     ""Uncommitted_state_root_hashes"": 
          ""2"":            b'DfNLmH4DAHTKv63YPFJzuRdeEtVwF5RtVnvKYHd8iLEA'
          ""0"":            b'H59VbR38tmmnyyDovb65XGB1jGv4Z8e4hbXcwurwDRBb'
          ""1"":            b'Fya9dkb2frkNLvBBmKWpmPx9xk32pHjCT215fbz9KpnF'
root@d0589e753342:/home/indy# validator-info -v | grep Committed_ledger_root_hashes -A 3
     ""Committed_ledger_root_hashes"": 
          ""0"":            b'CfUGd4gvKyGT3SgCUEaxEDxrWd5CSh8f5iJsQ5dJmFSZ'
          ""2"":            b'GKot5hBsd81kMupNCXHaqbhv3huEbxAFMLnpcX2hniwn'
          ""1"":            b'58eP93wkMNvepaGhXiwKinDpkWWYmBGPhLjpApBjXbcg'
root@d0589e753342:/home/indy# validator-info -v | grep Uncommitted_ledger_root_hashes -A 3
     ""Uncommitted_ledger_root_hashes"": 
root@d0589e753342:/home/indy# validator-info -v | grep Uncommitted_ledger_txns -A 6
     ""Uncommitted_ledger_txns"": 
          ""2"":           
               ""Count"":        0         
          ""1"":           
               ""Count"":        0         
          ""0"":           
               ""Count"":        0   
{noformat}
;;;",,,,,,,,,,,,,,,,,,,,,,,
Node metrics should include additional info,INDY-1543,32433,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,31/Jul/18 6:21 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"*Acceptance criteria*
 We need to output the following additional statistics:
 * Number of messages for each type
 ** separately input and output
 ** separately master and backup
 * Monitor's statistics (latency, throughput) as used by view change
 * Number of requests for master instance
 * Validation and applying to state time",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwy2v:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,"16/Aug/18 10:57 PM;sergey.khoroshavin;Lots of new metrics added, PRs:
https://github.com/hyperledger/indy-plenum/pull/857
https://github.com/hyperledger/indy-plenum/pull/870
https://github.com/hyperledger/indy-plenum/pull/875
https://github.com/hyperledger/indy-node/pull/880
https://github.com/hyperledger/indy-node/pull/897;;;",,,,,,,,,,,,,,,,,,,,,,,,
View Change should not be triggered by re-sending Primary disconnected if Primary is not disconnected anymore,INDY-1544,32479,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,01/Aug/18 6:23 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6,,,,0,,,,"If a Primary was disconnected (probably for a short amount of time), we send INSTANCE_CHANGE.
This INSTANCE_CHANGE will be re-send every 60 seconds, even if Primary is already connected.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1580,,,,,,,,,,,,,,,,,,,,"11/Aug/18 2:53 AM;ozheregelya;Node1.7z;https://jira.hyperledger.org/secure/attachment/15505/Node1.7z","11/Aug/18 2:53 AM;ozheregelya;Node2.7z;https://jira.hyperledger.org/secure/attachment/15506/Node2.7z","11/Aug/18 2:53 AM;ozheregelya;Node3.7z;https://jira.hyperledger.org/secure/attachment/15507/Node3.7z","11/Aug/18 2:53 AM;ozheregelya;Node4.7z;https://jira.hyperledger.org/secure/attachment/15508/Node4.7z","11/Aug/18 2:53 AM;ozheregelya;Node5.7z;https://jira.hyperledger.org/secure/attachment/15509/Node5.7z","11/Aug/18 2:53 AM;ozheregelya;Node6.7z;https://jira.hyperledger.org/secure/attachment/15510/Node6.7z","11/Aug/18 2:53 AM;ozheregelya;Node7.7z;https://jira.hyperledger.org/secure/attachment/15511/Node7.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwkf:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,ozheregelya,,,,,,,,,,"03/Aug/18 9:14 PM;Derashe;Problem reason:
 * When node notice that primary disconnected, it starts to send INSTANCE_CHANGES every 60 seconds, even if Primary is already connected. We need to change it.

Changes:
 * Now, we check if primary node had reconnected before sending another INSTANCE_CHANGE

Committed into:
 * https://github.com/hyperledger/indy-plenum/pull/851

Risk:
 * Low

Covered with tests:
 * [plenum/test/view_change/test_if_primary_disconnected.py|https://github.com/hyperledger/indy-plenum/pull/851/files#diff-a35e383d9c6ae289b71d342ffb0e1f20]
 * [plenum/test/view_change/test_instance_change_if_needed.py|https://github.com/hyperledger/indy-plenum/pull/851/files#diff-a3c8948587705f1d966837e2a2fe4ad2];;;","10/Aug/18 5:22 PM;ashcherbakov;Recommendation for QA:
 * Disconnect 1 non-primary node from the Primary and re-connect back
 * Make sure that non-Primary sent only 1 INSTANCE_CHANGE message with Primary Disconnected, not more.;;;","11/Aug/18 2:54 AM;ozheregelya;*Environment:*
 indy-node 1.6.558
 libindy 1.6.1~655

*Reason for Reopen:*
 Non-Primary node which was disconnected from primary sends INSTANCE_CHANGE on each txn sending after re-connection back.

*Steps to Reproduce:*
 1. Setup the docker pool of 7 nodes.
 2. On one of the nodes (node 5) block the Primary (node 1) IP:
 iptables -A INPUT -s 10.0.0.2 -j DROP
 3. Check nodes logs.
 => Only 1 INSTANCE_CHANGE\{'viewNo': 1, 'reason': 25} was send by Node5, but one more INSTANCE_CHANGE\{'viewNo': 1, 'reason': 26} was send by Node7.
 4. Unblock the Primary IP:
 iptables -D INPUT -s 10.0.0.2 -j DROP
 5. Send several txns.

*Actual Results:*
 Node5 sends INSTANCE_CHANGE\{'viewNo': 1, 'reason': 25} after each txn sending even if it don't have network issues in connection to Primary. New txns are not written on Node5.

*Expected Results:*
 Node5 should work with Primary correctly, new txns should be written.

See logs in attachment.;;;","11/Aug/18 1:46 PM;ashcherbakov;[~ozheregelya]
reason:25 means 'Primary of master protocol instance degraded the performance'. This is expected, since backup instances order requests, and the master doesn't (since primary is blocked).

The Instance Change because of primary disconnected has code 26 (Primary of master protocol instance disconnected), it was sent only once as expected.

 ;;;","11/Aug/18 10:24 PM;ozheregelya;After internal discussion it was decided that the problem described above is not connected with fix of this ticket. So, it was moved to separated ticket (INDY-1580) and this ticket will be closed.;;;",,,,,,,,,,,,,,,,,,,,
GC by Checkpoints should not be triggered during View Change,INDY-1545,32481,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,01/Aug/18 9:30 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,"Checkpoints are processed during VC, so it may lead to garbage collection to be triggered, and PrePrepares cleared. As a result, the node will not be able to order till last prepared certificate, and the pool may end up with some nodes ahead",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1539,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzzll3:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,,"06/Aug/18 6:42 PM;Toktar;Problem reason:
 - A checkpoints finalize cleans necessary data from requests and 3pc queues in view change before catchup, because cleaning use current view_no but in view change it already new.

Changes:
 - Added bugfix for view_no in checkpoint cleaning in view change. Choose view_no from last_ordered_3pc

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/854]

Version:
 * indy-node 1.3.550 -master
 * indy-plenum 1.5.494 -master

Risk factors:
 - Problem with checkpoint finalize

Risk:
 - Low

Covered with tests:
 - [test_checkpoints_removal_in_view_change.py|https://github.com/hyperledger/indy-plenum/pull/854/files#diff-58ba738b95bc4480906b11ed3673151a]

Recommendations for QA:
 * Test with configurations from INDY-1539;;;","08/Aug/18 9:25 PM;ashcherbakov;Will be validated in the scope of other load testing and INDY-1343;;;",,,,,,,,,,,,,,,,,,,,,,,
Investigate MemoryLeak issues with Revocation transactions,INDY-1546,32496,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,02/Aug/18 5:31 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"As was found in INDY-1493, we do have some memory leak issues with Revocation transactions.

Need to investigate what is the cause.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1493,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwxtb:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,,"02/Aug/18 10:21 PM;anikitinDSR;Small comment:
Higher memory usage was catched during read revoc_reg_entry transactions.;;;","09/Aug/18 5:49 PM;anikitinDSR;After several experiments with writing/reading revoc_reg_def and revoc_reg_entry transactions was noticed, that (with enabled metrics):
 * dynamic validation for revocation transactions is spent about 15 ms vs 5 ms for other transactions. In that case, ordering process will reducing and all input queues will be overloaded.
 * internal memory usage for python's objects, like 'str', 'list' take up ~ 1/4 of all node's memory usage. 

Therefore, our main assumtion is that there is no memory leaks. Revocation transactions is a ""heavy"" transactions from validation's and memory usage side.;;;","09/Aug/18 6:08 PM;ashcherbakov;It looks like here we face a limitation of the pool that can not handle heavy load (more than 10 writes per sec) for a long time. If there is a heavy load for a long time, a lot of messages are stashed in transport queues, and it may lead to high memory consumption.;;;",,,,,,,,,,,,,,,,,,,,,,
"If the node is crashed with some uncommitted state persisted, it must recover its state and continue ordering",INDY-1547,32503,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,ashcherbakov,ashcherbakov,ashcherbakov,02/Aug/18 9:27 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6,,,,0,,,,"If a node was crashed during ordering, there will be some uncommitted state persisted in the state.

 

*Acceptance criteria*
Write a test checking that a node can continue ordering in this case.
Do a fix if needed.

 

*Note*

- Do it with NYM txns and in Indy-Node since there it's suspected that IdrCache may not be able to recover properly.

 

Take a look",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1477,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzllj:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"03/Aug/18 5:15 PM;ashcherbakov;It looks like there is no issue. The corresponding test was added: https://github.com/hyperledger/indy-node/pull/865;;;",,,,,,,,,,,,,,,,,,,,,,,,
read_ledger tool is not able to read the sovrin plugin ledger,INDY-1548,32515,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,krw910,krw910,03/Aug/18 4:06 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"The read_ledger tool is unable to find the sovrin payment plugin ledgers

The fix is to add the following to read_ledger where the accepted --type is located
{code}
    elif type_ == 'token':
        storage_name = config.tokenTransactionsFile
    elif type_ == 'sovtoken':
        storage_name = config.sovtokenTransactionsFile
{code}

You then need to add the following into the indy_config.py on the node in /etc/indy
{code}
tokenTransactionsFile = ""token_transactions""
sovtokenTransactionsFile = ""sovtoken_transactions""
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-984,,,No,,Unset,No,,,"1|hzwxqv:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,krw910,mgbailey,zhigunenko.dsr,,,,,,,,"07/Aug/18 5:13 AM;krw910;The sovtoken location is empty and what is in the token location may need to be moved into the sovtoken location. I am checking with that team.;;;","14/Aug/18 5:21 PM;ashcherbakov;As for a real fix, we need to provide a different solution.
We can not hardcore these values in Indy code.

The options:
 * Analyse the folder names and get the list of ledgers (have a look at folders with `_transaction` postfix)
 * Add a new config parameter with a list of all supported ledgers (it will be filled by Sovrin to include plugin ledgers).;;;","14/Aug/18 10:36 PM;Derashe;After internal discussion first option was chosen:

_Analyse the folder names and get the list of ledgers (have a look at folders with `_transaction` postfix)_

Committed into:
 * [https://github.com/hyperledger/indy-node/pull/895] 

Recommendations for QA: 

Test read_ledger script with pluggable ledgers

 ;;;","17/Aug/18 2:26 AM;zhigunenko.dsr;*Environment:*
indy-node 1.6.563

*Actual results:*
_read_ledger_ supports pluggable ledgers;;;",,,,,,,,,,,,,,,,,,,,,
Change default configs for better performance and stability,INDY-1549,32529,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,03/Aug/18 6:28 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.73,,,,0,,,,"As followed from INDY-1475, we need to change some default values for better stability and performance.

In particular:
 * Max3PCBatchSize = 1000
 * VIEW_CHANGE_TIMEOUT = 420
 * MSG_LEN_LIMIT = 128 * 1024
 * ZMQ_CLIENT_QUEUE_SIZE = 100",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1475,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwkv:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"15/Aug/18 10:27 PM;ashcherbakov;PR:
[https://github.com/hyperledger/indy-plenum/pull/853]
[https://github.com/hyperledger/indy-plenum/pull/855]


That was tested in the scope of testing 1.6 and INDY-1343.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Research how pool ledger size affect performance,INDY-1550,32533,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,zhigunenko.dsr,zhigunenko.dsr,03/Aug/18 9:15 PM,13/Feb/19 10:02 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"1. Write 10.000 pool transactions
2. Run load testing with setup that similar for load with known metrics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwx4f:2rzlo",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pool stopped writing under 20txns/sec load (with plugins),INDY-1551,32568,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,zhigunenko.dsr,zhigunenko.dsr,06/Aug/18 5:48 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6,,,,0,,,,"*Steps to Reproduce:*
1. Setup new pool with 25 nodes (AWS)
2. Install plugins on each node
3. Run load test
{code}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 22 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k schema
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 66 -n 1 -k attrib
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
python3 perf_processes.py -g pool_transactions_genesis -m t -c 200 -t 132 -n 1 -k cred_def
{code}

*Actual results:*
Pool stops writing after 120ktxns without any significant symptoms

Logs will be available on LogProcessor:20txns_plugins","indy-node 1.5.534
plugin 0.8.0+16.42",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzllb:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,zhigunenko.dsr,,,,,,,,,,,"08/Aug/18 5:55 PM;Toktar;Pool started a lot of view changes one by one. It leaded to ""out of memory"" in all nodes.
 Firstly the problem reason with view changes was a long catchup on some nodes:
 * 4 view change - nodes 11, 12, 16, 24, 3, 8
 * 5 view change - nodes 15, 17
 * 6 view change - nodes 14, 9, 4
 * 7 view change - nodes 14, 15, 4. 9

Via high load they spend more time for processing messages from queues, it leads to queue increase because nodes re-ask messages that wait processing in queues.
 Secondly this nodes sent instance changes and with the bug INDY-1555 nodes have a master degraded and starts new view.

This is the expected behavior under this high load conditions.;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As a Sovrin Foundation, I need to test and check that all nodes on the live pool are configured properly and participate in consensus",INDY-1552,32569,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,ashcherbakov,ashcherbakov,06/Aug/18 6:30 PM,15/Jul/19 3:35 PM,28/Oct/23 2:47 AM,13/Jul/19 6:50 AM,,,,,,0,,,,"There are some requirements for each Node in the pool (hardware, configuration, networking).
We need to make sure that every Steward has correct configuration and all Nodes participate in consensus.

*Acceptance criteria:*

We need to check the following for every Node and Steward in the live pool:
 * Hardware and min amount of RAM
 * Correct configuration of separate NICs including whitelisting and routing
 * Firewall settings to limit max number of clients
 * Correct BLS keys
 * Participates in consensus (on writing data)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxqf:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,mgbailey,,,,,,,,,,"16/Aug/18 1:27 AM;mgbailey;A spreadsheet where we track the networks' status running up to the upgrade is [here|https://docs.google.com/spreadsheets/d/1DGOINCDZymesU9ljNDfP_RiOvAci-5Y8rH77u2jibyI].  The only known issue at this time is that icenode does not have a BLS key configured.  ;;;","13/Jul/19 6:49 AM;esplinr;Lynn performed additional checks as part of the most recent upgrade. I'm calling this task ""done"".;;;","15/Jul/19 3:35 PM;ashcherbakov;[~esplinr] [~lbendixsen]
Can you please provide more details before closing this task? I find this task extremely important for production use cases. 
In particular, do ALL the nodes have correct firewall settings to limit max number of clients (https://github.com/hyperledger/indy-node/blob/master/docs/source/setup-iptables.md)?
Do all the nodes use separate NICs for client and node traffic? ;;;",,,,,,,,,,,,,,,,,,,,,,
Load script issues,INDY-1553,32570,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,06/Aug/18 7:14 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"1. When we run `python3 perf_processes.py -c 10 -n 1 -g pool_transactions_genesis -t 1` actual average outgoing load is about 6 txns per second (according to load script ""total"" and pool metrics). 10 average txns per second is expected in this case since client machine resources is enough to provide this outgoing load and 25 nodes pool throughput is more than 6 writing txns per second (actually the first txns were sent with this output rate but then load script output rate degraded).

2. It looks like current load script is still waiting for pool responses before new requests will be sent since increased nodes' network latency reduces outgoing load. For example:
`python3 perf_processes.py -c 25 -n 1 -g pool_transactions_genesis -t 1 -k ""{\""nym\"": 1, \""attrib\"": 1, \""cred_def\"": 1, \""schema\"": 1}""` loads pool with:

~10 average output rate txns/sec with close to 0 ms pool network latency
~ 8 average output rate txns/sec with 250..500 ms pool network latency
~ 4.5 average output rate txns/sec with 500..1000 ms pool network latency

The same output rate is expected in all this cases if load script just sends requests and doesn't wait for responses.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1654,INDY-1655,INDY-1656,,,,,,,,,,,,,"24/Aug/18 12:55 AM;VladimirWork;INDY-1553_revoc_reg_entry.PNG;https://jira.hyperledger.org/secure/attachment/15748/INDY-1553_revoc_reg_entry.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwwl3:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,NataliaDracheva,VladimirWork,,,,,,,,,,"10/Aug/18 12:41 AM;VladimirWork;We should check intervals between load script requests sending with default and reduced/increased libindy timeouts for network requests in scope of this ticket testing.;;;","16/Aug/18 9:27 PM;dsurnin;Implemented 4 sync modes: freeflow - old behavior; all - all client will send batch each tick; one - only one client will send a batch each tick; wair_resp - next req will be sent only after response received.
in modes all and one all clients will start sending only after all the clients established connection and done all the preload work.
refresh rate in seconds now
new output file name parameter to store all the output
load time parameter in sec to limit load time

PR
https://github.com/hyperledger/indy-node/pull/893;;;","25/Aug/18 12:08 AM;VladimirWork;1. We should implement a fast way to stop load script if 307 errors are occurred.
2. We should fix revoc_reg_entry txns sending (screenshot).;;;","27/Aug/18 10:09 PM;NataliaDracheva;Scenario: payments load testing.

During tokens minting the script produces hundreds of console errors ""Cannot generate request since no req data are available."" despite eventually all of the requests are sent successfully. This behavior is confusing and should be changed. The System should not display such errors.;;;","28/Aug/18 11:31 PM;VladimirWork;All issues found are reported as separate related tickets.;;;",,,,,,,,,,,,,,,,,,,,
Need to enhance write permissions for Revocation transactions,INDY-1554,32575,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,06/Aug/18 11:08 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.7.1,,,,0,,,,"As of now, there are no write permissions for REVOC_REG_DEF and REVOC_REG_ENTRY txns.

https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py

 *Accepatance criteria:*

Add tests and enhance permissions so that
 * if ANYONE_CAN_WRITE=True
 ** REVOC_REG_DEF:
 *** Anyone can create new REVOC_REG_DEF
 *** Only owners can edit existing REVOC_REG_DEF
 ** REVOC_REG_ENTRY:
 *** Only the owner of the corresponding REVOC_REG_DEF can create new REVOC_REG_ENTRY
 *** Only owners can edit existing REVOC_REG_ENTRY
 * if ANYONE_CAN_WRITE=False
 ** REVOC_REG_DEF:
 *** Only Trustee/Steward/TrustAnchor can create new REVOC_REG_DEF
 *** Only owners can edit existing REVOC_REG_DEF
 ** REVOC_REG_ENTRY:
 *** Only the owner of the corresponding REVOC_REG_DEF can create new REVOC_REG_ENTRY
 *** Only owners can edit existing REVOC_REG_ENTRY

Integration tests makes sense to do in a similar way as for NYM: https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1528,INDY-1957,INDY-2030,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00000o",,,,Unset,Unset,Ev-Node 19.05,Ev-Node 19.06,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,cam-parra,VladimirWork,,,,,,,,,,"01/Mar/19 6:01 AM;cam-parra;INDY1554 POA

Purpose

This will set write permissions for the `REVOC_REG_DEF` txn and setting it's properties correctly on the ledger.

Plan

1. Write integration tests in [indy_node/test/write_permission/test_revocation_write_permission.py]([https://github.com/hyperledger/indy-node/blob/master/indy_node/test/write_permission/test_revocation_write_permission.py])
2. Add rules for revocation using TDD
3. Update documentation
4. Integrate with old and new request handlers;;;","19/Mar/19 10:35 PM;cam-parra;Problem reason/description: 
Revocation did not have the correct permissions set.

Changes: 
auth_map.py modified

domain_request_handler modified 

PR:

 

[https://github.com/hyperledger/indy-node/pull/1192]

 

Version:
h1. #1204

 

Risk factors:
- ....

Risk:
- med

Covered with tests:
- https://github.com/hyperledger/indy-node/blob/master/indy_node/test/write_permission/test_revocation_write_permission.py

Recommendations for QA
- add edit revocation with different roles;;;","21/Mar/19 12:31 AM;VladimirWork;Build Info:
indy-node 1.6.868

Steps to Reproduce:
0. Install pool with ANYONE_CAN_WRITE=False (or with default settings).
1. Create REVOC_REG_DEF and REVOC_REG_ENTRY by default Trustee.
2. Add None role entity and another Trustee entity in the ledger.
3. Try to edit REVOC_REG_DEF and REVOC_REG_ENTRY from Step 1 by None role entity.
4. Try to edit REVOC_REG_DEF and REVOC_REG_ENTRY from Step 1 by another Trustee role entity.

Actual Results:
I've got correct error for REVOC_REG_ENTRY *but not for REVOC_REG_DEF (there is REPLYs for both entities)*:
3:
{noformat}
{""result"":{""txnMetadata"":{""seqNo"":17,""txnId"":""CQU4s9soCBJbkpk7vnQCJH:4:V4SGRU86Z58d6TV7PBUe6f:3:CL:12:cred_def_tag:CL_ACCUM:another_revoc_def_tag"",""txnTime"":1553094091},""ver"":""1"",""reqSignature"":{""values"":[{""value"":""3kz2TonGkEMrYy9rpSYYAKXBzahBepp27WMRN1mmSu7kNzAsqUmmJeH1DJXZ3LVfJdmUxmVFNCV1Rx4t9FFQvcKH"",""from"":""CQU4s9soCBJbkpk7vnQCJH""}],""type"":""ED25519""},""txn"":{""metadata"":{""digest"":""ca124e2d1b6d72b310fcba641966de6a02853428e48a38c276759af1b1cb5e47"",""from"":""CQU4s9soCBJbkpk7vnQCJH"",""reqId"":1553094090499436427},""type"":""113"",""protocolVersion"":2,""data"":{""credDefId"":""V4SGRU86Z58d6TV7PBUe6f:3:CL:12:cred_def_tag"",""tag"":""another_revoc_def_tag"",""revocDefType"":""CL_ACCUM"",""value"":{""tailsLocation"":""tails\/BHTDmteEQwRCkK6Q7hHw3XfjaSgFofKwQTBhqd3HojQF"",""issuanceType"":""ISSUANCE_BY_DEFAULT"",""publicKeys"":{""accumKey"":{""z"":""1 05070A9433CA190711130017BCD0F163F5EF329D4A015671B662F1B0B56FA062 1 0CAD8FA88443C713AA2B8E0DD5ECD548F7FC0B061235EE9CD59E7FE32559C287 1 193BA552717DE6FABEB3F9F6FABE6F7C7B18C596F3279A760112EE593E28A8D6 1 155B82B5A201AD94DBBF82099E6AEDCCABDA2963DFD3260FD29CD44B3C83361E 1 03D8CA2B0FB17AA63EB89482888BBD65CC59C0A9FCCC363766E0C3A7970677B4 1 0E15B47CE3028547E6F3195D42335360883E358DEFAE1AB30E8CB9C1DF7A204F 1 20EB8ADF41E96FA48DDB55DEE5169D334C9970C0359E36674D4A5B7FEA91CFBD 1 1004A467068E5EAC3A20D0F012F4B83AA435002686FA5A294039EFEB99169C5B 1 0E9095962DC2698EBF160D5F64EC1EFA2436F5F4E15312328AE296B9276212CB 1 08603E479E0E6E37C0AB8372B962D26FDF5883BF29BF50C06D142B4910C50937 1 0B65C41B05B47F549931F6BB59563E44201018C47EC800B847DFDC62C1C43AFA 1 1A2018EB6AA7437C7A51FA61B62E76C6A1DE337B45657095FF85A07A363FB885""}},""maxCredNum"":1,""tailsHash"":""BHTDmteEQwRCkK6Q7hHw3XfjaSgFofKwQTBhqd3HojQF""},""id"":""V4SGRU86Z58d6TV7PBUe6f:4:V4SGRU86Z58d6TV7PBUe6f:3:CL:12:cred_def_tag:CL_ACCUM:another_revoc_def_tag""}},""auditPath"":[""DejEdYJk3DZsx4rgW934C3epRiej2wRAVn8xCdSTnSPx""],""rootHash"":""5CLsSvCmWAo54MzhmdaV62cvzB2VXzbmKRbWARHMsaER""},""op"":""REPLY""}

{'identifier': 'CQU4s9soCBJbkpk7vnQCJH', 'reason': ""client request invalid: UnauthorizedClientRequest('None role can not edit REVOC_REG_ENTRY txn since only owner can modify it',)"", 'op': 'REJECT', 'reqId': 1553094091471307564}
{noformat}

4:
{noformat}
{""result"":{""ver"":""1"",""txn"":{""protocolVersion"":2,""data"":{""revocDefType"":""CL_ACCUM"",""id"":""V4SGRU86Z58d6TV7PBUe6f:4:V4SGRU86Z58d6TV7PBUe6f:3:CL:12:cred_def_tag:CL_ACCUM:another_revoc_def_tag"",""credDefId"":""V4SGRU86Z58d6TV7PBUe6f:3:CL:12:cred_def_tag"",""value"":{""tailsHash"":""BHTDmteEQwRCkK6Q7hHw3XfjaSgFofKwQTBhqd3HojQF"",""tailsLocation"":""tails\/BHTDmteEQwRCkK6Q7hHw3XfjaSgFofKwQTBhqd3HojQF"",""maxCredNum"":1,""issuanceType"":""ISSUANCE_BY_DEFAULT"",""publicKeys"":{""accumKey"":{""z"":""1 05070A9433CA190711130017BCD0F163F5EF329D4A015671B662F1B0B56FA062 1 0CAD8FA88443C713AA2B8E0DD5ECD548F7FC0B061235EE9CD59E7FE32559C287 1 193BA552717DE6FABEB3F9F6FABE6F7C7B18C596F3279A760112EE593E28A8D6 1 155B82B5A201AD94DBBF82099E6AEDCCABDA2963DFD3260FD29CD44B3C83361E 1 03D8CA2B0FB17AA63EB89482888BBD65CC59C0A9FCCC363766E0C3A7970677B4 1 0E15B47CE3028547E6F3195D42335360883E358DEFAE1AB30E8CB9C1DF7A204F 1 20EB8ADF41E96FA48DDB55DEE5169D334C9970C0359E36674D4A5B7FEA91CFBD 1 1004A467068E5EAC3A20D0F012F4B83AA435002686FA5A294039EFEB99169C5B 1 0E9095962DC2698EBF160D5F64EC1EFA2436F5F4E15312328AE296B9276212CB 1 08603E479E0E6E37C0AB8372B962D26FDF5883BF29BF50C06D142B4910C50937 1 0B65C41B05B47F549931F6BB59563E44201018C47EC800B847DFDC62C1C43AFA 1 1A2018EB6AA7437C7A51FA61B62E76C6A1DE337B45657095FF85A07A363FB885""}}},""tag"":""another_revoc_def_tag""},""type"":""113"",""metadata"":{""digest"":""cf9d1d805fbf56c6b9aef3a87daf63e66662d3753e7239dfbdacd88c039c8e5f"",""from"":""5nKYKaNC3CY5HPN7QGt7tn"",""reqId"":1553094092334199976}},""auditPath"":[""3yC6KAErj3iDqi2GgqWxoSKCvHwsqDap8f5apLomQzR3"",""DejEdYJk3DZsx4rgW934C3epRiej2wRAVn8xCdSTnSPx""],""reqSignature"":{""values"":[{""value"":""5skZQnMix8Z6gHVYygL5xnBTXWmp5HoD1iXcosWq115M3Pa949P6npaVz9zpByoxQF7iYuZU9YWUZKXjBA4LUJqu"",""from"":""5nKYKaNC3CY5HPN7QGt7tn""}],""type"":""ED25519""},""txnMetadata"":{""txnTime"":1553094093,""txnId"":""5nKYKaNC3CY5HPN7QGt7tn:4:V4SGRU86Z58d6TV7PBUe6f:3:CL:12:cred_def_tag:CL_ACCUM:another_revoc_def_tag"",""seqNo"":18},""rootHash"":""4zehkLiyTLS7RSWyu4jsfdQCWnp2GHi1TpCfP5aQMn8""},""op"":""REPLY""}

{'reqId': 1553094093520915925, 'reason': ""client request invalid: UnauthorizedClientRequest('TRUSTEE can not edit REVOC_REG_ENTRY txn since only owner can modify it',)"", 'op': 'REJECT', 'identifier': '5nKYKaNC3CY5HPN7QGt7tn'}
{noformat}

Expected Results:
There are should be similar REJECTs for both txn types.;;;","21/Mar/19 5:26 PM;VladimirWork;Build Info:
indy-node 1.6.868

Actual Results:
REVOC_REG_DEF and REVOC_REG_ENTRY permissions work as expected. There is an issue with ledger and client REVOC_REG_DEF_IDs difference - it will be fixed in scope of INDY-2030.;;;",,,,,,,,,,,,,,,,,,,,,
Monitor needs to be reset after the view change,INDY-1555,32577,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,06/Aug/18 11:14 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6,,,,0,,,,"As of now, monitor is reset when view change starts, so there can be more ViewChanges because of degradation, as ViewChange may take up to 5 mins, and degradation may be detected for 4 mins.

 

*Acceptance criteria:*
 * Add tests that no degradation can be detected during the view change
 * Reset monitor when view change ends (add unit tests for this)
 * Do not send InstanceChange because of degradation during view change (add unit tests for this)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzzlkf:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,,"09/Aug/18 5:31 PM;anikitinDSR;Reasons:
 * need to exclude false-positive ""master degradation"" view_change initiating after previous view_change procedure completed.

Changes:
 * reseting monitor stats after view_change complete

Versions:
 * indy-node: 1.5.552

Steps to validate:
 * covered by integration tests

 ;;;","10/Aug/18 5:26 PM;anikitinDSR;PR with unit tests:
[https://github.com/hyperledger/indy-plenum/pull/858]
Tests:
[https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change/test_reset_monitor_after_view_change.py]
https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change/test_checkPerformance.py;;;",,,,,,,,,,,,,,,,,,,,,,,
View Change may happen when load is stopped,INDY-1556,32578,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,spivachuk,ashcherbakov,ashcherbakov,06/Aug/18 11:16 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,,0,,,,We need to check that View Change doesn't happen when the load is stopped suddenly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzzlkv:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,spivachuk,,,,,,,,,,"06/Aug/18 11:29 PM;sergey.khoroshavin;During testing of INDY-1475 there was a moment of simultaneous load test agent crash and a view change. It might be just coincidence, but we'd better check it. Logs are located in `~/logs/1475` on internal log processing machine.;;;","11/Aug/18 6:42 AM;spivachuk;The view change to the view 3 was caused by going below the threshold ratio between the master and the average backup throughput.

In the view 2 there was load about 11 reqs/sec during several hours. Then at 20:59:50 the primary of the instance 3 (Node6) detected that it had lagged in master instance for 2 checkpoints from other nodes and so started a catch-up. Thus the primary of the instance 3 stopped to produce new 3PC-batches. At 21:11 the load was stopped and ordering in other instances was stopped. Then the primary of the instance 3 completed the catch-up at 21:16:07 and produced a 3PC-batch of maximum size (10000) of queued requests in the instance 3. When this 3PC-batch had been ordered by nodes in the pool, they detected that the ratio between the master throughput and the average backup throughput became below the threshold value. So the nodes sent INSTANCE_CHANGE messages with the next view_no 3 with the reason ""Primary of master protocol instance degraded the performance"". Then the nodes gathered quorums of these INSTANCE_CHANGE messages and performed a view change to the view 3.

Below there are messages related to sending of INSTANCE_CHANGE with view_no 3 from the log of one of the nodes:
{code:java}
2018-08-02 21:16:32.161000 | Node11:- | NOTIFICATION | monitor.py | MONITORING: Node11 master throughput ratio 0.051357800613378635 is lower than Delta 0.1.
2018-08-02 21:16:32.161000 | Node11:- | NOTIFICATION | node.py | Node11 master instance performance degraded
2018-08-02 21:16:32.161000 | Node11:- | NOTIFICATION | view_changer.py | Node11 sending instance with view_no = 3 and trying to start view change since performance of master instance degraded
2018-08-02 21:16:32.162000 | Node11:- | INFO | view_changer.py | MONITORING: Node11 metrics for monitor: Node11 Monitor metrics:: None
             Delta: 0.1
             Lambda: 240
             Omega: 20
             avg backup throughput: 10.265750156631821
             client avg request latencies: {0: {'V4SGRU86Z58d6TV7PBUe6f': 0.7126782652300148}, 1: {'V4SGRU86Z58d6TV7PBUe6f': 1.1258689948787894}, 2: {'V4SGRU86Z58d6TV7PBUe6f': 0.5550786061575981}, 3: {'V4SGRU86Z58d6TV7PBUe6f': 400.42709273566936}, 4: {'V4SGRU86Z58d6TV7PBUe6f': 0.6006979011295902}, 5: {'V4SGRU86Z58d6TV7PBUe6f': 0.7638420664324053}, 6: {'V4SGRU86Z58d6TV7PBUe6f': 0.9826751445647405}, 7: {'V4SGRU86Z58d6TV7PBUe6f': 1.0497014596251761}, 8: {'V4SGRU86Z58d6TV7PBUe6f': 0.8390144533225338}}
             instances started: [8251520.265940722, 8251520.266395832, 8251520.266789038, 8251520.267155353, 8251520.267602983, 8251520.267978794, 8251520.268382167, 8251520.268808046, 8251520.269186649]
             master request latencies: {}
             master throughput ratio: 0.051357800613378635
             master throughput: 0.5272263496910575
             ordered request counts: {0: 163875, 1: 163875, 2: 163875, 3: 163348, 4: 163875, 5: 163875, 6: 163875, 7: 163875, 8: 163875}
             ordered request durations: {0: 260343.76566756424, 1: 220833.90625354275, 2: 255480.54343646485, 3: 14945681.386451773, 4: 212150.0821664296, 5: 223643.44628215395, 6: 240507.8876771126, 7: 230157.68799525965, 8: 226719.50231549423}
             throughput: {0: 0.5272263496910575, 1: 0.5269553407300632, 2: 0.5286142685190659, 3: 78.43376998672488, 4: 0.5269196328292368, 5: 0.5271971235256135, 6: 0.5270468743104585, 7: 0.5272933082675938, 8: 0.5282047181476643}
             total requests: 171596
{code};;;","13/Aug/18 3:39 PM;ashcherbakov;So, the problems are not related to the fact that load script was stopped.

However, some issues with throughput calculations were found (that caused view change). This will be addressed in https://jira.hyperledger.org/browse/INDY-1565.;;;",,,,,,,,,,,,,,,,,,,,,,
OutOfMemory when catching up big ledgers,INDY-1557,32579,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,06/Aug/18 11:20 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.78,,,,0,,,,There is high risk of OutOfMemory (8 GB RAM) when catching up a huge pool.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwxmv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"06/Aug/18 11:39 PM;sergey.khoroshavin;During testing of INDY-1475 there were crashes due to out of memory, and they were triggered by received CATCHUP_REP messages. Logs are located in `~/logs/1475` on internal log processing machine.;;;","29/Aug/18 3:55 PM;ashcherbakov;The issue was partially fixed in the scope of INDY-1595. A follow-up issue is INDY-1657.
A proper catch-up of huge ledgers will be addressed in INDY-1242 and INDY-1657.;;;",,,,,,,,,,,,,,,,,,,,,,,
read_ledger (command line version) does not produce valid JSON,INDY-1558,32609,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,mgbailey,mgbailey,07/Aug/18 8:10 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"In previous versions, the output of the read_ledger CLI command produced valid JSON, at least for each transaction, if not across the entire array of transactions. In version 1.5 this has changed, requiring additional manual parsing of each line before attempting to parse it with a JSON parsing tool. For example, domain transactions now look like this:
{code:java}
1 {""reqSignature"":{},""txn"":{""data"":{""dest"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""0"",""verkey"":""~CoRER63DVYnWZtK8uAzNbx""},""metadata"":{},""type"":""1""},""txnMetadata"":{""seqNo"":1},""ver"":""1""}
2 {""reqSignature"":{},""txn"":{""data"":{""dest"":""Th7MpTaRZVRYnPiabds81Y"",""role"":""2"",""verkey"":""~7TYfekw4GUagBnBVCqPjiC""},""metadata"":{""from"":""V4SGRU86Z58d6TV7PBUe6f""},""type"":""1""},""txnMetadata"":{""seqNo"":2},""ver"":""1""}
3 {""reqSignature"":{},""txn"":{""data"":{""dest"":""EbP4aYNeTHL6q385GuVpRV"",""role"":""2"",""verkey"":""~RHGNtfvkgPEUQzQNtNxLNu""},""metadata"":{""from"":""V4SGRU86Z58d6TV7PBUe6f""},""type"":""1""},""txnMetadata"":{""seqNo"":3},""ver"":""1""}
{code}
The sequence number at the beginning of each line breaks JSON formatting, complicating parsing, and is redundant since this is also contained in the txnMetadata.

Please fix this by either removing the redundant line number at the beginning of the line, or by wrapping the transactions in brackets, as was done in the old version of indy, like this:
{code:java}
[1, {""reqSignature"":{},""txn"":{""data"":{""dest"":""V4SGRU86Z58d6TV7PBUe6f"",""role"":""0"",""verkey"":""~CoRER63DVYnWZtK8uAzNbx""},""metadata"":{},""type"":""1""},""txnMetadata"":{""seqNo"":1},""ver"":""1""}]
[2, {""reqSignature"":{},""txn"":{""data"":{""dest"":""Th7MpTaRZVRYnPiabds81Y"",""role"":""2"",""verkey"":""~7TYfekw4GUagBnBVCqPjiC""},""metadata"":{""from"":""V4SGRU86Z58d6TV7PBUe6f""},""type"":""1""},""txnMetadata"":{""seqNo"":2},""ver"":""1""}]
[3, {""reqSignature"":{},""txn"":{""data"":{""dest"":""EbP4aYNeTHL6q385GuVpRV"",""role"":""2"",""verkey"":""~RHGNtfvkgPEUQzQNtNxLNu""},""metadata"":{""from"":""V4SGRU86Z58d6TV7PBUe6f""},""type"":""1""},""txnMetadata"":{""seqNo"":3},""ver"":""1""}]
{code}",indy-node 1.5,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-984,,,No,,Unset,No,,,"1|hzzlmf:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,mgbailey,,,,,,,,,,,"14/Aug/18 7:37 PM;ashcherbakov;https://github.com/hyperledger/indy-node/pull/894;;;",,,,,,,,,,,,,,,,,,,,,,,,
"If the Node realizes that its state is not in sync with the Ledger, it needs to recover it from the Ledger",INDY-1559,32628,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,07/Aug/18 7:34 PM,08/Jan/20 5:09 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"There can be some situations where the state may be not in sync with the Ledger.
For example, when there is a crash after writing to the Ledger but before writing to the State.

*Acceptance criteria*
 * Track situation when state is not in sync with the Ledger (compare with the state from other nodes?)
 * Notify the Steward that the state is broken
 * Try to recover by removing the state and re-creating it from the Ledger.

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ibo",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,,,,,,,,,,,"20/Aug/18 4:28 PM;dsurnin;Possible solution
On start node checks two conditions
* if last service shutdown was unexpected (hardware faults, exceptions, etc)
* if node faced incorrect state error before shutdown 

If any of the conditions are True node should recreate state.

Also we can think of some cases/conditions to start from transaction log;;;",,,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - View change subversion - Increase/Decrease Replica Count,INDY-1560,32643,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ckochenower,ckochenower,ckochenower,08/Aug/18 12:37 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,test-automation,,,0,,,,"Demote/Promote a sufficient number of nodes to cause the number of replicas to decrease/increase by 1.

# Start a 10 validator node pool with the following Primary and Replica configuration:
f = f_value
n = Replica_count 
(f = 3, n = 4)
Node1:0 - Primary
Node2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node3:2 - Backup to Node2
Node4:3 - Backup to Node3
# Reduce n by 1
Reducing validator node count by 1 (9 validator nodes) results in f = 2 and n = 3.
Variants to consider:
## Demote the primary
Expected result:
Triggers a view change
(f = 2, n = 3)
Node2:0 - Primary
Node3:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node4:2 - Backup to Node3
## Demote a replica
Replica selection variants to consider. Replicas are [Node2, Node3, Node4]:
### forward - Node2
### reverse - Node4
### random - any(Node2 - Node4)
Expected result:
(f = 2, n = 3)
#### Forward
Node1:0 - Primary
Node3:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node4:2 - Backup to Node2
#### Reverse
Node1:0 - Primary
Node2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node3:2 - Backup to Node2
#### Random (lets assume Node3)
Node1:0 - Primary
Node2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node4:2 - Backup to Node2
## Demote a node that is not the primary and not a replica
(f = 2, n = 3)
Node1:0 - Primary
Node2:1 - Next in line for primary (backup if node 1 goes down or a view change is requested)
Node3:2 - Backup to Node2
# Increase n by 1 - Add/Promote enough nodes to increase n by 1. Three options for setting up for this experiment.
## Option 1: steady state = A nym can be written AND there are at least x demoted nodes where x is the number of nodes needed to move n to n + 1.
## Option 2: Use the ""Decrease n by 1"" experiment as setup for the ""Increase n by 1"" experiment. Modify the ""Decrease n by 1"" rollbacks that promote nodes demoted during the experiment so they take an optional parameter that tells them to skip the rollback. Doing so allows a ""Increase n by 1"" experiment to run immediately after execution of ""Decrease n by 1""
## Option 3: The method of the experiment demotes x nodes to reduce n by 1, waits up to a timeout for n to reduce by 1. Once n is reduced by 1, the method immediately promotes x nodes and waits up to a timeout for n to increase by 1.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1578,,,,,,,,,,,,,,,,,,,,"16/Aug/18 1:39 AM;ckochenower;Screen Shot 2018-08-14 at 5.41.19 PM.png;https://jira.hyperledger.org/secure/attachment/15589/Screen+Shot+2018-08-14+at+5.41.19+PM.png","16/Aug/18 1:40 AM;ckochenower;Screen Shot 2018-08-14 at 6.03.33 PM.png;https://jira.hyperledger.org/secure/attachment/15590/Screen+Shot+2018-08-14+at+6.03.33+PM.png","16/Aug/18 1:40 AM;ckochenower;Screen Shot 2018-08-14 at 6.08.11 PM.png;https://jira.hyperledger.org/secure/attachment/15591/Screen+Shot+2018-08-14+at+6.08.11+PM.png","15/Aug/18 12:31 AM;ckochenower;Screen Shot 2018-08-14 at 9.04.03 AM.png;https://jira.hyperledger.org/secure/attachment/15580/Screen+Shot+2018-08-14+at+9.04.03+AM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzlxr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"14/Aug/18 9:06 AM;ckochenower;Rather than taking the approach of demoting a sufficient number of nodes to decrease f_value by 1, I wrote the experiment to demote a sufficient number of nodes to get f_value down to a given value. The primary is removed from the list of nodes from which to choose the list of nodes to be demoted. The primary is then stopped to force a view change.

I ran the experiment with a target f_value of 1 an 2. Both experiments run with the experiment starting with 10 nodes and an f_value of 3.

*Target f_value of 2:*
To get f_value to 2, at least 1 of 10 nodes must be demoted to drop f_value to 2. If one node is demoted, N = 9 (instead of 10) and ((9 - 1) / 3) = 2. Running the experiment with the target f_value = 2, the experiment runs just fine, because demoting 1 node still allows the pool to come to consensus.

*Target f_value of 1:*
Running the experiment with an f_value = 1 demotes 4 nodes almost all at once. They are, in fact, demoted serially, but the demotion process is quite fast, which may not give the cluster time to elect a new primary before dropping out of consensus. The maximum number of validator nodes that participate in consensus when f_value is 1 is 6 ((6 - 1)/3) = 1. However, demoting 4 nodes causes a pool of 10 validator nodes to fall out of consensus and the experiment fails waiting for a view change.

*Conclusion:*
When the experiment attempts to demote a sufficient number of nodes to decrease the f_value by more than 1, the experiment fails, because the pool is unable to come to consensus . Restarting each of the participating 6 nodes does NOT appear to get the pool back into consensus.;;;","15/Aug/18 12:31 AM;ckochenower;Split master result following demotion of 5 of 10 nodes to reduce f_value from 3 to 1. Master was excluded from the 5 demoted nodes. The master was stopped to force view change. The 5 demoted nodes were then promoted as part of the rollback segment of the experiment in an attempt to bring the pool back to as close to a pre-experiment execution state as possible.
 !Screen Shot 2018-08-14 at 9.04.03 AM.png|thumbnail! ;;;","16/Aug/18 1:41 AM;ckochenower;It appears that we may have another race condition to consider.  The following is the state of the pool before demoting 4 nodes (Node10, Node9, Node8, and Node7 - none of which are master or replica) to drop f_value from 3 to 1.

 !Screen Shot 2018-08-14 at 5.41.19 PM.png|thumbnail! 

A view change was required (stop indy-node on Node1 - the master) to complete the process of dropping the f_value from 3 to 1. Is this a problem?

Node2 became the new master and Node3 became the one and only backup replica.

As part of the promotion process during the rollback segment of the Chaos experiment the demoted nodes were promoted and restarted in the following order: Node10 promoted, Node10 restarted, Node8 promoted, Node8 restarted, Node9 promoted, Node9 restarted, Node7 promoted, and Node7 restarted.

Finally, the original master Node1 was started, because it was stopped to force a view change.

The following was the result:

 !Screen Shot 2018-08-14 at 6.03.33 PM.png|thumbnail! 

A second restart (stop/start) of Node7 brings Node7 completely into sync with the other nodes. Note that Node7 was the last of the aforementioned 4 nodes to be promoted and when it was promoted, N became 10, which should cause f_value to change from 2 to 3 ((10 - 1)/3) = 3).

 !Screen Shot 2018-08-14 at 6.08.11 PM.png|thumbnail!;;;","16/Aug/18 3:13 AM;ckochenower;I think we can add one or more of the following exclude rules/features if/when we see the need, but I will not include them now unless someone strongly feels they should be included.

# ""exclude by role""
## Roles:
### master
### replica (master is technically a replica)
### backup replica/primary (all non-mater replicas)
### other (all non-master and non-replica nodes))
# ""exclude nodes list""
## Enumerate node aliases to exclude

This experiment has the following switches that can be combined to vary things sufficiently for now:

{code}
(chaostoolkit) ubuntu@KellyStableClientVirgina:~/chaosindy$ ./scripts/run-shrink-pool -h
Usage: ./scripts/run-shrink-pool
 required arguments: None
 optional arguments:
   -c|--cleanup
       Remove temporary files/directories created by the experiment?
       Default: Yes
       Valid Inputs (case insensitive): yes, y, 1, no, n, 0
   -d|--decrease-f-to
       Decrease tolerance for faulty nodes (f = (N - 1) / 3) to a
       given number.
       Default: 2
       Valid Input: Any positive number >= 1 and less than
                    ((N - 1) / 3) where N is the number of validator
                    nodes in the pool when the experiment begins.
   -e|--execution-count
       How many times to run the experiment.
       Default: 1
       Valid Input: Any positive number >= 1
   -g|--genesis-file
       Path to the target pool genesis transaction file.
       Default: /home/ubuntu/chaosindy/pool_transactions_genesis
   -h|--help
       Print script help/usage
   -n|--validator-nodes
       A JSON list of node names to include in the experiment. Usually
         the complete list from the genesis file. TODO: derive default
         from genesis file.
       Default: '[""Node1"", ""Node2"", ""Node3"", ""Node4"", ""Node5"", ""Node6"", ""Node7"", ""Node8"", ""Node9"", ""Node10""]'
   -o|--selection-order
       Order in which replicas are selected for demotion.
       Valid Input: 1 (FORWARD), 2 (REVERSE), 3 (RANDOM)
       Default: '2'
   -p|--pause-after
       How long to let the system reach a steady state (in seconds)
       after decreasing f_value
       Default: 60
       Valid Input: Any positive number >= 1
   -s|--set-services-timeout
       How long to wait (seconds) before timing out while promoting
       and demoting a node. A node's 'services' are changed between
       'VALIDATOR' and '' \(blank\) during demotion/promotion.
       Default: 60
       Valid Input: Any positive number >= 1
   -t|--write-nym-timeout
       How long to wait (seconds) before timing out while writing a NYM
         transaction.
       Default: 60
       Valid Input: Any positive number >= 1
   -s|--seed
       Seed to use to create DID/Verkey pair used to get validator info
         via indy-cli. Must be a Trustee or Steward seed.
       Default: 000000000000000000000000Trustee1
       Valid Input: A 32 byte string. See default above for an example.
{code};;;",,,,,,,,,,,,,,,,,,,,,
Nodes with 32Gb RAM were failed with OOM after 9 hours with 22thns/sec load,INDY-1561,32663,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ozheregelya,ozheregelya,08/Aug/18 6:49 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.73,,,,0,TShirt_L,,,"indy-node 1.5.543
 libindy 1.6.1~655

Steps to Reproduce:
 1. Setup the pool of 25 nodes with m4.2xlarge instances (32Gb RAM).
 2. Run the load test from 6 client instances (load from INDY-1343 x2):
{code:java}
python3 perf_processes.py -g pool_transactions_genesis -m t -c 334 -t 33 -n 1 -k nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 334 -t 33 -n 1 -k ""{\""schema\"": 1, \""attrib\"": 3}""
python3 perf_processes.py -g pool_transactions_genesis -m t -c 334 -t 132 -n 1 -k cred_def
---
python3 perf_processes.py -g pool_transactions_genesis -m t -c 334 -t 1.35 -n 1 -k get_nym
python3 perf_processes.py -g pool_transactions_genesis -m t -c 334 -t 3 -n 1 -k ""{\""get_schema\"": 1, \""get_attrib\"": 1}"" 
python3 perf_processes.py -g pool_transactions_genesis -m t -c 334 -t 6 -n 1 -k get_cred_def{code}
Actual Results:
 Part of nodes were lagged during load. Part of lagged nodes were failed with OOM after end of load.

Additional Information:
 Reading load was less than writing one. There were only 16 read requests per second, but expectation was that reading load will be greater that the writing one.  

Logs and metrics: s3://qanodelogs/load32RAM06aug18/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/load32RAM06aug18/ /home/ev/logs/1561/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/18 4:44 PM;Derashe;Figure_1.png;https://jira.hyperledger.org/secure/attachment/15570/Figure_1.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwmn:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,,,,,,,,,,,"14/Aug/18 5:05 PM;Derashe;Problem reason:

Pool stopped write txns under heavy load


Research:
 * Pool started lagging because of multiple view changes (50 view changes in 12 hours)
 ** view changes were called by ""master degraded"" reason, which was initiated because of low master throughput. In this particullar case master throughtput was low, because of uneven txn handling by different replicas. This can happen when backup replicas slow ordering txns for some time, and then trying to order plenty of them. This leads to ""thoughtput spikes"" which can cause view change.
 ** The reason why view change slow node is that sometimes (50% probability) view change can lasts for 5 minutes, stopping any other activity on node (this case described in https://jira.hyperledger.org/browse/INDY-1473).  
 ** The resolving for problem of ""throughput spikes"" is in progress now and will be solved in scope of another tickets (as https://jira.hyperledger.org/browse/INDY-1582).
 ** To lower the network load we can discard any client messages while view_change is in progress (https://jira.hyperledger.org/browse/INDY-1564).
 * There are 5 nodes that had out of memory error. 
 ** Every of these nodes had slowed ordering txns and got overflowed thier network queues
 ** 2 of these nodes slowed and started cathing up txns. And as a result of a continuing heavy load, nodes started making more catchups (the longest took ~1 hour) which leads to out of memory
 *** Catchup took so long because of node-to-node network stack crowded. On applied image (Figure_1), where ""y axis"" is timeline and ""x axis"" is requests we need to catch up, we can see that catchup replies came non sequentially and with a huge delays. Problem with overloaded node-to-node stack must be resolved in https://jira.hyperledger.org/browse/INDY-1472.  Also, problem with out of memory while catching up a big ledgers will be discovered in scope of https://jira.hyperledger.org/browse/INDY-1557. 
 ** 3 of these nodes lagged and could not make any view_change because of incorrect throughtput measurement and heavy network load which leads to out of memory error. There are a few tickets that must improve network stability: https://jira.hyperledger.org/browse/INDY-1549,  https://jira.hyperledger.org/browse/INDY-1569.

 

Recommendations for QA: 

Retest pool with this or similar config and load after fixes for throughtput spikes.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Part of the nodes are lagged after high load,INDY-1562,32674,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ozheregelya,ozheregelya,09/Aug/18 1:41 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"*Environment:*
 indy-node 1.5.543
 libindy 1.6.1~655
 AWS pool of 25 nodes with 32Gb RAM

*Steps to Reproduce:*
 1. Run load for INDY-1388:
{code:java}
2 clients:>>> python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 500 -g ~/stab_transactions_genesis_new -k ""[{\""nym\"": {\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}]""

8 clients:>>> python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 1125 -g ~/stab_transactions_genesis_new -k ""[{\""get_nym\"": {\""count\"": 9}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}]""{code}
=> Reading test was running ~2 hours, writing one was running ~30 minutes.
 2. After end of the test wait for processing of all written txns (more than 2-3 hours).
 => All nodes exclude Node16 wrote 127595 txns, Node 16 wrote 127490 txns.
 3. Write 1 txn.
 => All nodes wrote this txn exclude Node13, Node15 and Node16.
 4. Run small load test with medium load.
 => 190 txns were written by all nodes exclude Node13, Node15 and Node16. 

*Actual Results:*
 Nodes 13, 15 and 16 are not writing after load test. Note that in 5 hours after end of step 4 there are no updates. Nodes are in the same state.

*Logs and metrics:* 
 s3://qanodelogs/indy-1562/NodeXX/
 To get logs, run following command on log processor machine:
 aws s3 cp --recursive s3://qanodelogs/indy-1562/ /home/ev/logs/indy-1562/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzlm7:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ozheregelya,,,,,,,,,,,"10/Aug/18 7:32 PM;anikitinDSR;Logs and code investigation showed, that:
- view_change was not in progress, there is no catchup procedure.
- there is no any discarding in logs
- all backup replicas ordered all incoming request as master's replica on other nodes
- also, master replica received all checkpoint's messages that exclude some networks problems.

Other notes:
 # Lagged nodes sent InstanceChange messages for MasterDegraded reason. This issue will be investigated in the scope of INDY-1565.
 # Need to add more significant data in logging for case, when ""Unordered request was not found in prePrepares"". For example, will be helpful information about received Prepares and Commits.

Transactions was not ordered on nodes: Node13, Node15, Node16 because there is no received PrePrepares. There is 3 main reasons why:
 * Some networks problems
 * ""Silent"" discarding. Received Preprepare was discarded and there is no information in logs (on info level) about it
 * Was received quorum of CHECKPOINT messages and current checkpoint was stabilized (also remove Preprepares to checkpoint seqNoEnd)

Also, if node will reach 2 not stable checkpoints, then catchup will starting. In this case, after receiving next quorumed CHECKPOINTs lagged node should starting catchup.;;;",,,,,,,,,,,,,,,,,,,,,,,,
POA: Trust anchor writes preserve owner,INDY-1563,32680,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,09/Aug/18 3:28 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Story*
As a user of an Indy network that does not require fees for writes, I want my writes to the global ledger to be sponsored by a trust anchor who prevents spam, but I want to retain future control of my writes so that I can modify my writes or work with a different trust anchor in the future.

*Acceptance Criteria*
* Create a plan for effectively using trust anchors to sponsor writes to the ledger
* Get architectural review of the plan
* Create the necessary issues

Goals:
* A trust anchor can write a cred def that is owned by a third party
* The owner can modify that cred def
* A trust anchor can write a schema def that is owned by a third party
* The owner can modify that schema def",,,,,,,,,,,,,,,,,,,,,INDY-1708,,,,,,,,,,,,,INDY-1693,INDY-1694,INDY-1695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzo1r:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,brycecurtis,danielhardman,Derashe,devin-fisher,esplinr,gudkov,mgbailey,,,,,"10/Aug/18 4:44 PM;ashcherbakov;{quote}The value of the field will be a DID that will be set by the controller before it is written to the ledger.
{quote}
If the controller (Issuer) has a DID on the Ledger, then why can't he send the CLAIM_DEF by himself? Is it because he may be not a Trust Anchor?
 BTW there is a new feature ANYONE_CAN_WRITE which is False by default, but if set (by Indy-Node-based applications such as Sovrin) to True, then anyone can create new NYMs and CRED_DEFs, so there will be no need to send CRED_DEF by someone else.
{quote}In the current proof format, the schema sequence number is provided for each claim, but the cred def sequence number is not.
{quote}
Both proof request and proof contain references to both SCHEMA and CRED_DEF. Please note, that this is not seq_no, but an ID (in fact the key in the state), so verifier can retrieve SCHEMA and CLAID_DEF using GET_SCHEMA and GET_CLAIM_DEF requests.
 Please search for `indy_prover_create_proof` in [https://github.com/hyperledger/indy-sdk/tree/master/doc/design/002-anoncreds|https://github.com/hyperledger/indy-sdk/tree/master/doc/design/002-anoncreds,];;;","10/Aug/18 11:53 PM;mgbailey;[~ashcherbakov] thanks for the information on GET_CLAIM_DEF.  I will look into using that.

The need is for a proof to come from the processor, but appear to the verifier to come from the controller.  I have this working.  The additional need is that for audit purposes, in some circumstances the verifier must also be able to determine that the proof was actually sent by the processor.  Thus we need the additional field in the cred def where the controller is able to show that he has delegated proof issuance to the processor (who actually holds the private keys to the cred def).;;;","05/Sep/18 5:03 AM;mgbailey;Just to clarify, this story is not about spam prevention or fees in any way - it is about delegated authority. In this case the usual 3-party story (issuer, prover, and verifier) is modified into a 4-party story, where the issuer is split into 2, the controller and the processor. Both will need DIDs on the network, so that the verifier is able to see that the credential is from the controller, by means of the processor.
 If the controller is able to write a new cred def to the ledger sometime in the future, which updates it so that a different processor is designated, that would be a good modification to the concept.;;;","06/Sep/18 11:41 PM;Derashe;Taking into account possible approach and comments around this task, we've decided to make appropriate plan of attack. It includes described example with Controller (owner of txn) and Processor (mediator between owner and ledger)
 # *There are two possible approaches for delegating authority:*
 ** Controller doesn't have his did on ledger. In that case, Controller give full control over adding, changing, revoking of his transactions to Processor (including control over private keys) . Also Controller won't have possibility to change Processor to another. For now, node already have functionality that allows to process such a scenario.
 ** Controller have his did on ledger. In that case, when by some reason (Controller has inappropriate role or he don't want to deal with ledger credits/fees), Controller can't write txns in ledger by itself. But he can use Processor's service (who must have rights to write txns in ledger), in a way, that:
 *** Controller stays the only one who can add, change or revoke any of ""his"" data in the ledger
 *** In any time Controller can use another Processor's service
 *** Controller avoid credit/fees system
 # *The second approach seems more appropriate for distributed system, so changes below will be described with this approach as a precondition*. Assuming this, we've planned such a design:
 ** We are need to restructure few txns, such as: SCHEMA, CRED_DEF, REVOC_REG_DEF, REVOC_REG_ENTRY, ATTRIB(?) in a way, that owner of DID_1 can delegate this txns to be sent to the ledger by another owner DID_2, which has appropriate role. And DID_1 can choose which DID_2 he wants to interact with
 ** In the same time, DID_2 cannot modify this txn, and don't need any of private data of DID_1 to send txn
 ** In the same time, when txn is written in ledger, it must contain veritable information about DID_1 and DID_2, which was used during this txn. And that information must be availiable to be taken from ledger state.
 # *To satisfy such a requirements, we need to make few changes in request structure.*
 ** We are planning to do that backward compatible way. Actual protocol version for now is ""2"". We are planning make some changes in request, and upgrade protocol version for such a request to ""3"". But updated node will correctly handle both protocol versions (""2"", ""3""), because new functionality will be optional
 ** So, if we want to create request with new format we need to make such a changes:
 *** Add filed ""submitter"" to common request structure, this field will stand for did of Processor
 *** Use multisignature mechanism instead of solo signature, i.e. ""signature"" field will be replaced by ""signatures""
 ** To implement such functionality we need:
 *** SDK: Support new format of txn. Make an option to create requests in a new way:
 **** append submitter field
 **** set multisignature field instead of usual signature and sign it
 **** increment protocol version to 3
 *** Node: Make validation for correctly handling of two different version
 **** left same validation for protocol version 2
 **** add validation for new fields in protocol version 3, which described in SDK part
 # *How would this work*
 ** Controller, using sdk create the request he needs. He match that he wants to use some Processor for this and specify that Processor's DID and *sign that request.* After this noone can change data in this request without breaking the signature. Then he send this request to Processor he chose
 ** Processor get Controller's txn, if he agreed to write your txn to pool, he sign this txn too (in a multisignature field), and send it to the pool
 ** Controller ask ledger if his txn was written;;;","07/Sep/18 1:32 AM;mgbailey;Reading the above comment, I am missing some details for the primary use case, which is the processor issuing credentials in behalf of the controller. The key ledger transaction for this is the cred def, with its keys.  The processor must have the private keys for the cred def. How does the above satisfy this? 

I also don't see how the added complexity of multi-sig helps with this problem. The only signature needed on the ledger is that of the controller, since it is the authorizing party.

Are we changing it so that a controller can write a new cred def to the ledger for a given schema, in order to change or revoke a processor?;;;","07/Sep/18 2:06 AM;Derashe;[~mgbailey]

As far as i understand (correct me if i'm wrong) the key goal is that:
 * Only Controller must have full controll of his credentials and he can send it to Processor.
 * Only Processor can send txns in ledger 

Answering your questions:

>> The processor must have the private keys for the cred def. How does the above satisfy this? 

I thought that only Controller have controll over his private keys. If Processor have them, then he can write/edit anything by the name of Controller. Clarify pls, if i am wrong

>>I also don't see how the added complexity of multi-sig helps with this problem

You wrote: ""...verifier must also be able to determine that the proof was actually sent by the processor..."", so for correctly determining who was the Processor, we need his signature also.

>>Are we changing it so that a controller can write a new cred def to the ledger for a given schema, in order to change or revoke a processor?

I'm not sure that i understand you right. In the way, that we offered, you can choose any processor to process your messages. 

 ;;;","07/Sep/18 3:03 AM;mgbailey;The desire is not for the processor to be able to post transactions to the ledger, it is for the processor to be able to issue credentials to users under the authority of the controller. The prover and verifier should see that the credential came from the controller by means of the processor. In order for this to happen,
 # The processor must have the cred def private keys to be able to issue credentials
 # There must be a record of the delegation that is accessible by the verifier.
 # Ideally, the controller should be able to modify or revoke the delegation, perhaps by means of a new cred def written to the ledger.

 ;;;","07/Sep/18 3:28 PM;ashcherbakov;[~mgbailey] [~esplinr]
 I believe the use case you describe is not about Ledger transaction, but rather about crypto (HDKeys, DKMS, etc.). The current crypto doesn't support this use case.
 However, the proposed solution can support this use case, Artem will provide more details.;;;","07/Sep/18 7:49 PM;Derashe;[~mgbailey]

*For the last requirements comment we can suggest such a scenario to be implemented:*
 * controller make a deal with some processor and send him public and private keys, which controller will use for CRED_DEF
 * controller sending CRED_DEF txn with a new field ""processor"", which stands for processor's DID
 * after that, processor can issue new credentials for this CRED_DEF
 * controller sending usual REVOC_REG_DEF txn
 * after that, processor can send REVOC_REG_ENTRY thxns in case if he needs to revoke some credentials

In case if controller want to cancel delegation, he just send txn to change CRED_DEF's public keys and processor DID.

For implementing such case, we need change validation process:
 * For creating: 
 ** REVOC_REG_ENTRY can be created by identifier's and processor's DIDs of it's CRED_DEF
 * For editing:
 ** Still only identifier can edit CRED_DEF (including changing processor's did field) 
 ** REVOC_REG_ENTRY can be edited by identifier's and processor's DIDs of it's CRED_DEF

*For implementing feature that we've described earlier (about mediator):*

This feature can be actual when ANYONE_CAN_WRITE set to FALSE. 

This means that None role users won't be able to write new txns, but will be able to edit them. Than case, described above will looks like that (Assuming submitter is Trust Anchor, and controller/processor is None role) :
 * controller make a deal with some processor and send him public and private keys, which controller will use for CRED_DEF
 * controller creating CRED_DEF txn with a new field ""processor"" and a new field ""submitter"", also he adds his signature to multisignature field
 * controller send this txn to submitter
 * submitter send this txn to ledger
 * processor can issue new credentials for this CRED_DEF
 * controller send REVOC_REG_DEF txn with new field submmiter and multisig to submitter
 * submitter send this txn to ledger
 * controller/processor send REVOC_REG_ENTRY with new field submmiter and multisig to submitter
 * submitter send this txn to ledger
 * After this controller or processor can send REVOC_REG_ENTRY txns in ledger without submitter;;;","07/Sep/18 10:54 PM;mgbailey;I think we are on the right track. Thanks.;;;","07/Sep/18 11:27 PM;ashcherbakov;I think we can close the PoA ticket then and create a bunch of technical tasks to implement the feature.;;;","11/Sep/18 7:48 PM;gudkov;[~esplinr] [~danielhardman] I looks like significant change that can affect the all existing Indy-based software stack include Evernym and BcGov. Should we start more public procedure of these PoA approval like Hipe creation?;;;","12/Sep/18 1:02 AM;devin-fisher;{quote}controller make a deal with some processor and send him public and private keys, which controller will use for CRED_DEF
{quote}
This sounds like sharing keys. Sharing keys is almost always a bad idea. We can't just pass this off. Strong justification would be required to justify two idenpendent parties to both know the private keys.;;;","12/Sep/18 1:17 AM;danielhardman;I find this ticket title and the story in the first line of the comments to be incredibly confusing. I believe I understand Mike's intent–that it should be possible to issue credentials as a data processor (in the GDPR definition), where the ""owner"" of the cred def is a data controller (also in the GDPR definition). But I am totally confused about how that is supposed to relate to trust anchor status. I am agreeing with [~gudkov] that this needs HIPE, or at very least a doc. The comment stream does not clarify things for me.

The sentence that Devin called out is flat-out unacceptable; it violates a fundamental tenet of Indy security, which is that private keys are never shared. The controller doesn't send the processor private keys. Rather, the processor generates a key pair and tells the controller what the public key is.

 ;;;","14/Sep/18 10:40 PM;esplinr;The team produced a good POA for these requirements, and the resulting conversation has me questioning the requirements. I'm going to do more research with stakeholders in the Sovrin Foundation, and open a new ticket for us to clarify the design before we schedule work.;;;","15/Sep/18 12:14 AM;mgbailey;[~esplinr] please revert this ticket (or make a new one) to the original, very limited, ask: add a field to Cred Def where a controller can write a processor DID. This does not preclude later developments that are more comprehensive.;;;","02/Nov/18 7:20 AM;esplinr;The original ask is being tracked as INDY-1708. It was incorrect for me to conflate the two sets of requirements.;;;",,,,,,,,
Discard any client requests during view change,INDY-1564,32691,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,09/Aug/18 5:50 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"As of now, we don't process any client requests during view change (just not take them from the client's ZMQ queue while view change is in progress).

That may potentially lead to memory leaks. Most probably the client will get timeout for the request received during the view change, so it should not be a big issue to discard it.

*Acceptance criteria:*
 * Process client queue during the view change (`serviceClientMsgs` in node.py), but discard all requests (send NACK) with a message ""Discarding client request since View Change is in progress"".
 * Write tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwlr:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,zhigunenko.dsr,,,,,,,,,,"11/Aug/18 12:59 AM;Toktar;Problem reason:
 - In view change a lot of client requests lead to out_of_memory problem and clients receive responces timeout error.

Changes:
 - Discard any client requests during view change. 

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/866]

Version:
 * indy-node 1.6.558 -master
 * indy-plenum 1.6.499 -master

Risk factors:
 - Problem with client messages processing.

Risk:
 - Low

Covered with tests:
 - [test_client_req_during_view_change.py|https://github.com/hyperledger/indy-plenum/pull/866/files#diff-f71de0fc4d18eccb9054e0eef23f1976]

Recommendations for QA:
 * It can be load test with force view change. Messages sent in view change will have response with discarding message.;;;","16/Aug/18 9:07 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.558 -master
indy-plenum 1.6.499 -master

*Actual results:*
1)  [discard.7z|https://drive.google.com/file/d/1zdAVY2DYGvon6S-O0Gbxu09jAD_rf73o/view?usp=sharing] view change nadn't been started
1)  [discard2.7z|https://drive.google.com/file/d/1doR0rAwMhKfKetH1YGbKDa5UQa2zu-XR/view?usp=sharing] view change hadn't been finished
3) unexpected message _object has no attribute 'get'_;;;","20/Aug/18 4:31 PM;Toktar;Solve problems in new version 
indy-node 1.6.569 -master
indy-plenum 1.6.512 -master
[~zhigunenko.dsr] please retest it with a new version. Thank you!;;;","20/Aug/18 9:13 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.569
indy-plenum                1.6.512

*Steps to validate:*
1. Setup load test
2. Cause to view change (service stop)

*Actual results:*
Messages are nacked during long view change
Message ""object has no attribute 'get'"" does not appear;;;",,,,,,,,,,,,,,,,,,,,,
Improve throughput calculation to reduce a chance of false positive View Changes,INDY-1565,32693,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,09/Aug/18 5:54 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"During analysis of INDY-1556 and INDY-1562, we discovered that there can be cases where View Change may happen unexpectedly.

One of the problems is that we calculate the throughput based on requests number, while the trend is more for 3PC batches count. If we have 3PC batches with a big difference in requests count, we may see false positives because of spikes in the trend.

As of now, we use Exponential moving average for throughput calculation with constant window and constant alpha.



 

*Acceptance criteria:*
 * Write tests that there is no view change detected if we got the same amount of requests on all instances, but with a different distribution of ordering time:
 ** All instances ordered equally with some medium throughput; one instance didn't order for a some period, and then ordered all stashed requests in 1 huge 3PC batch
 ** All instances ordered equally with some medium throughput; one instance didn't order for a some period, and then ordered all stashed requests in multiple huge 3PC batches
 ** All instances didn't order for a long period of time, and then a small load started, and some backups started ordering earlier than master
 ** All instances didn't order for a long period of time, and then a huge load started, and some backups started ordering earlier than master
 ** All instances ordered equally with some medium throughput; and then all instances started ordering with a heavier load, but some non-master did it a bit earlier
 ** All instances ordered equally with some medium throughput; and then load stopped, leading to no more requests ordered
 ** Other test cases

 * Provide a theory for modification of the current algorithm.
 Some ideas:
 ** Fall back to initial-window mode if the current throughput value is almost zero.
 ** Do not trigger view change immediately, but wait for a sequence of trigger events (for example, backups noticed that master throughput degraded 5 times in a row).
 ** Use dynamic window and/or dynamic alpha for Exponential moving average
 *** consider the window doesn't contain zero intervals and extended till we have some non-zero values
 ** Use different moving average algorithm (not exponential) with a proper window
 *** for example, not exponential, but quadratic trend at the beginning of the window
 *** explore moving average with momentum
 *** consider digital filter synthesis (IIR or FIR) to obtain needed response and avoid ringing
 ** Include information about 3PC batch size into algorithm
 ** use moving average for 3PC batches, not requests. Consider a separate metric for requests
 *** beware that using moving average just for 3PC batches will allow malicious primary to drop requests unnoticed
 * Implement the modifications

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwlj:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,VladimirWork,,,,,,,,,,"28/Aug/18 12:46 AM;sergey-shilov;*Problem state / reason:*

Due to EMA features spikes in requests processing by replicas in case of low load (when empty windows occur) may cause false positive master degradation. For more details please see ticket description.

*Changes:*

Added a new strategy that deals with empty windows (windows without processed requests) using state machine. State machine allows to smooth out** spikes in requests processing by averaging the number of processed requests using empty windows.

*Committed into:*

https://github.com/hyperledger/indy-plenum/pull/872
https://github.com/hyperledger/indy-node/pull/916
 indy-node 1.6.580-master

*Risk factors:*

False positive master degradation in case of spikes of requests processing by master replica and non-master replicas in different windows due to EMA features.

*Risk:*

Medium

*Recommendations for QA:*

Use various load testing and see that no false positive master degradation occurred.;;;","30/Aug/18 5:51 PM;VladimirWork;Build Info:
indy-node 1.6.582 / 1.6.585

Steps to Validate:
1. Run load tests with continious/spike/DDOS load.
2. Check validator-info and logs for VCs and their reasons.

Actual Results:
There are no false-positive VCs occured during load testing.;;;",,,,,,,,,,,,,,,,,,,,,,,
Investigate why Looper and serviceActions take so long,INDY-1566,32694,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,09/Aug/18 5:57 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"One run of a looper may last for quite a long time (>5 secs, sometimes even 1000 secs) during the load. Also a lot of time is spent in `serviceAction` part.

*Acceptance criteria*
 * Check why looper may take so long time
 * Check why `serviceAction` part takes so long
 * Provide recommendations how to fix it if possible",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwxn3:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"29/Aug/18 4:26 PM;ashcherbakov;It looks like the spikes were caused by Validator Info dumps. It was fixed in INDY-1603.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Emulate non-smooth load,INDY-1567,32696,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,ashcherbakov,ashcherbakov,09/Aug/18 6:21 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"We need to check that the pool works as expected when the load is not smooth, but with spikes, and there are periods of time when there are no write load.

*Acceptance criteria*

Provide test results for the following cases:
 * Do a load of 10 writes per sec and 100 reads per sec for 10 minutes (all txn except revocation), then stop the load for 10 mins. Repeat multiple times.
 * Permanent read of 100 txns per sec. Do a load of 10 writes per sec for 10 minutes (all txns except revocation), then stop the load. Repeat multiple times.

Monitor how stable we are, and how often view change happens.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1666,,,,,,,,,,,,,,,"29/Aug/18 10:03 PM;NataliaDracheva;perf_spike_config.yml;https://jira.hyperledger.org/secure/attachment/15757/perf_spike_config.yml",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwwnz:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,,,,,,,,,,,"29/Aug/18 9:55 PM;NataliaDracheva;Scenario 1 (permanent reading background load):

2 hours

No view changes, no failures

Expected
|| ||Reading txns||Writing txns||
|Expected|720 000|36 000|
|Actual|707 900|35 637|
|Difference %|1.68|1.01|;;;","30/Aug/18 4:29 PM;NataliaDracheva;Scenario 2 (spike reading and writing load):

2 hours

No view changes, no failures

Expected
|| ||Reading txns||Writing txns||
|Expected|396 000|36 000|
|Actual|389 835|35 635|
|Difference %|1.55|1.01|;;;",,,,,,,,,,,,,,,,,,,,,,,
As a developer I need a simple tool to show graphical representation of some common metrics,INDY-1568,32700,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,09/Aug/18 11:55 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.73,,,,0,,,,"We need several aligned graphs:

1. Throughput
- client_stack_messages_processed_per_sec
- master_ordered_requests_count_per_sec

2. Latency
- avg_master_monitor_avg_latency
- avg_monitor_avg_latency

3. Queues
- avg_node_stack_messages_processed
- avg_client_stack_messages_processed

4. Looper (Y log scale)
- AVG_NODE_PROD_TIME
- AVG_SERVICE_REPLICAS_TIME
- AVG_SERVICE_NODE_MSGS_TIME
- AVG_SERVICE_CLIENT_MSGS_TIME
- AVG_SERVICE_ACTIONS_TIME
- AVG_SERVICE_VIEW_CHANGER_TIME
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/18 5:39 PM;sergey.khoroshavin;night.csv;https://jira.hyperledger.org/secure/attachment/15495/night.csv",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwnb:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZStack quotas should take into account size of received messages,INDY-1569,32702,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,sergey.khoroshavin,sergey.khoroshavin,10/Aug/18 12:44 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.73,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1241,,,No,,Unset,No,,,"1|hzwwnj:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,sergey.khoroshavin,,,,,,,,,,,"16/Aug/18 9:46 PM;anikitinDSR;Was added another limitation for getting messages from listener. For now we can take only limited count or limited size of received messages.

PR:
https://github.com/hyperledger/indy-plenum/pull/874;;;",,,,,,,,,,,,,,,,,,,,,,,,
Do not create strings for log messages we are not going to display,INDY-1570,32722,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,10/Aug/18 4:26 PM,11/Oct/19 9:07 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"Most log messages have an input as a message created using .format call.

It means that we create new strings even if the log message will not be displayed for the current log level. This may affect performance a lot.

Please use a standard logger's way to create parametrized log messages which will create new strings only if the log message will be displayed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwx4f:2rzlu",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pool with 32Gb RAM stopped writing after 20 hours of normal load,INDY-1571,32726,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ozheregelya,ozheregelya,10/Aug/18 7:31 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"indy-node 1.5.551
 libindy 1.6.1~655

Steps to Reproduce:
 1. Setup the pool of 25 nodes with 32Gb RAM.
 2. Run the load from INDY-1343 (without payments and revocations):
{code:java}
python3 perf_processes.py -g ~/ext_transactions_genesis_new -m t -c 167 -t 33 -n 1 -k nym
python3 perf_processes.py -g ~/ext_transactions_genesis_new -m t -c 167 -t 33 -n 1 -k ""{\""schema\"": 1, \""attrib\"": 3}""
python3 perf_processes.py -g ~/ext_transactions_genesis_new -m t -c 167 -t 132 -n 1 -k cred_def
---
python3 perf_processes.py -g ~/ext_transactions_genesis_new -m t -c 167 -t 1.35 -n 1 -k get_nym
python3 perf_processes.py -g ~/ext_transactions_genesis_new -m t -c 167 -t 3 -n 1 -k ""{\""get_schema\"": 1, \""get_attrib\"": 1}""
python3 perf_processes.py -g ~/ext_transactions_genesis_new -m t -c 167 -t 6 -n 1 -k get_cred_def{code}
Actual Results:
 Pool stopped working after ~20 hours of load.

Logs and metrics:

Logs and metrics: s3://qanodelogs/indy-1571/NodeXX/
 To get logs, run following command on log processor machine:

aws s3 cp --recursive s3://qanodelogs/indy-1571/ /home/ev/logs/indy-1571/

Looks like the same problem with not completed View Changes under load from INDY-1388:
s3://qanodelogs/indy-1571-ddos/NodeXX/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1603,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwxr3:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,,,,,,,,,,,"14/Aug/18 4:48 PM;ozheregelya;Looks like the same problem on pool with 8Gb RAM which was under load from INDY-1343 for 12 hours: s3://qanodelogs/indy-1571-8gb/NodeXX/
indy-node 1.6.70 RC.;;;","16/Aug/18 12:51 AM;Toktar;Nodes ordered 711192 transactions and stopped on (2, 11351).  A master degraded was a reason for first 3 view changes. After this was start step by step view change to 15 view because nodes can't end view change in time. 
The reason of this problem is a big load in validator info script. About 100 seconds for one looper iteration. 
Problem should be found in the validator info script and its long time of work.;;;","17/Aug/18 9:52 PM;Toktar;Node1 and node2 process already stable chenckpoints and can't found new missing checkpoints for catchup starting.
Logs should be analysed for found a reason of this problem.;;;","21/Aug/18 10:03 PM;Toktar;Node1 blacklisted node15 and node5 what caused a slow work. After this catchup was started and could not stopped because load was a very hight (in the end of catchup pull already ordered new transactions and catchup started again).

In this moment problem with node2 looks like a connection problem.;;;","29/Aug/18 11:33 PM;Toktar;Nodes were written at different speeds because of the large network latency and high load, so the hash roots of the repeated catchup_rep were different. On the further work of the node1 it did not affect.;;;",,,,,,,,,,,,,,,,,,,,
Explore memory leaks when a primary on one of backup instances is stopped,INDY-1572,32727,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,10/Aug/18 7:33 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"There is a high chance of memory leaks if one of the (non-master) primaries is stopped.

The corresponding instance will not be able to order, and requests will not be cleared till the next view change.

*Acceptance criteria*:

Perform the following tests and report results about memory consumption with memory checks enabled (in particular, the checks for memory usage of our internal queues and structures).
 * Disable view change. Stop a primary on 1 backup instance. Run load of 10 writes per second.
 * Disable view change. Stop a primary on multiple backup instances. Run load of 10 writes per second.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1248,INDY-1618,,,,,,,,,,,,,,,,,,,"21/Aug/18 11:43 PM;VladimirWork;ALL_UP.png;https://jira.hyperledger.org/secure/attachment/15722/ALL_UP.png","21/Aug/18 11:44 PM;VladimirWork;BACKUP_DOWN.png;https://jira.hyperledger.org/secure/attachment/15723/BACKUP_DOWN.png","21/Aug/18 11:44 PM;VladimirWork;BACKUP_DOWN_UNDER_LOAD.png;https://jira.hyperledger.org/secure/attachment/15724/BACKUP_DOWN_UNDER_LOAD.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzx16n:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"21/Aug/18 11:44 PM;VladimirWork;Build Info:
indy-node 1.6.565
_view change is disabled in all cases_

Actual Results:
*25/25 nodes are up:* We sustain *~20* NYM writes per second throughput and have low latency *(2~4)* during all test run. RAM consumption is *up to 600 MB* at the end of test run. *Over 100k* NYMs are written for 2 hours. !ALL_UP.png|thumbnail! 

*1..4 backup primaries are down:* Throughput is degraded *from ~20 to ~0* at the end of test run (clients get 307 from pool) and latency grows continiously during all test run *from ~0 to 40~60*. RAM consumption is *up to 1.5 GB* at the end of test run. *Less than 30k* NYMs are written for 1.5 hours. !BACKUP_DOWN.png|thumbnail! 

All this trends are confirmed by shutting down instance 1 primary under load: !BACKUP_DOWN_UNDER_LOAD.png|thumbnail! ;;;","22/Aug/18 12:37 AM;VladimirWork;So there are two main issues found:
- memory leaks (INDY-1248)
- throughput degradation (INDY-1618);;;",,,,,,,,,,,,,,,,,,,,,,,
test ledger restart issue,INDY-1573,32728,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,ashcherbakov,rajeshkalaria,rajeshkalaria,10/Aug/18 8:30 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"we were facing some weird issue in one of our test ledger (dev-integration ledger). 
 There was insufficient space on that vm for some time and ledger stopped responding and when we tried to restart (after making some space), when 3rd node starts, it gives error mentioned in attachment ([^dev_integration_stack_trace.txt])

 

*Some other helpful information:*

*node version=1.5.68*

read_ledger --node_name=Node1 --type=domain --count *=>* 1584

read_ledger --node_name=Node2 --type=domain --count *=>* 1584

read_ledger --node_name=Node3 --type=domain --count *=>* 1584

read_ledger --node_name=Node4 --type=domain --count *=>* 1584

 

read_ledger --node_name=Node1 --type=domain --frm=1 --to=1584 (output is attached): [^dev_integration_node1_txns.txt]

read_ledger --node_name=Node2 --type=domain --frm=1 --to=1584 (output is attached):  [^dev_integration_node2_txns.txt]

read_ledger --node_name=Node3 --type=domain --frm=1 --to=1584 (output is attached): [^dev_integration_node3_txns.txt]

read_ledger --node_name=Node4 --type=domain --frm=1 --to=1584 (output is attached): [^dev_integration_node4_txns.txt]

read_ledger --node_name=Node3 --type=pool (output is attached): [^dev_integration_pool_txns.txt]

read_ledger --node_name=Node3 --type=config (output is attached): [^dev_integration_configs_txns.txt]

 content of /var/lib/indy

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Aug/18 8:25 PM;rajeshkalaria;dev_integration_configs_txns.txt;https://jira.hyperledger.org/secure/attachment/15498/dev_integration_configs_txns.txt","10/Aug/18 10:45 PM;rajeshkalaria;dev_integration_node1_txns.txt;https://jira.hyperledger.org/secure/attachment/15503/dev_integration_node1_txns.txt","10/Aug/18 10:45 PM;rajeshkalaria;dev_integration_node2_txns.txt;https://jira.hyperledger.org/secure/attachment/15502/dev_integration_node2_txns.txt","10/Aug/18 8:22 PM;rajeshkalaria;dev_integration_node3_txns.txt;https://jira.hyperledger.org/secure/attachment/15500/dev_integration_node3_txns.txt","10/Aug/18 10:45 PM;rajeshkalaria;dev_integration_node4_txns.txt;https://jira.hyperledger.org/secure/attachment/15501/dev_integration_node4_txns.txt","10/Aug/18 8:23 PM;rajeshkalaria;dev_integration_pool_txns.txt;https://jira.hyperledger.org/secure/attachment/15499/dev_integration_pool_txns.txt","10/Aug/18 8:30 PM;rajeshkalaria;dev_integration_stack_trace.txt;https://jira.hyperledger.org/secure/attachment/15496/dev_integration_stack_trace.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxq7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,rajeshkalaria,,,,,,,,,,,"10/Aug/18 10:28 PM;ashcherbakov;According to logs, the issue may happen if there is a txn in old format (prior to 1.4) in the Ledger.

But the output of all ledgers shows that all txns in all Ledgers are in a valid new format (""txn"" is present everywhere).

Are we sure that the issue is reproduced for Node3? Maybe we should get the output of all ledgers for all nodes...;;;","10/Aug/18 10:56 PM;ashcherbakov;As one can see from the stacktrace, there is a missing ""txn"" field in a transaction got from the ledger.
But attached output shows that all domain transactions on all nodes have ""txn"" field... (can be checked by `grep -c`)

 ;;;","10/Aug/18 11:58 PM;ashcherbakov;This is the issue with the Plugins.;;;",,,,,,,,,,,,,,,,,,,,,,
Bad network pool has stopped to write txns under load,INDY-1574,32731,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,VladimirWork,VladimirWork,10/Aug/18 9:31 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,TShirt_S,,,"Build Info:
indy-node 1.5.548

Steps to Reproduce:
1. Install 25 nodes pool and fill ledger with 200k+ NYM txns.
2. Run 800 writing (4 machines with 200 threads from each) and 2000 reading clients (10 machines with 200 threads from each) that write/read 1 NYM txns each second.

Actual Results:
Nodes wrote 30..40k txns until the whole pool stopped writing and reading with PoolLedgerTimeout error. The most of nodes have 252k txns and the most lagged node has 222k txns written.",,,,,,,,,,INDY-1681,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/18 12:22 AM;VladimirWork;INDY-1574_OOM.PNG;https://jira.hyperledger.org/secure/attachment/15909/INDY-1574_OOM.PNG","03/Oct/18 6:50 PM;VladimirWork;INDY-1574_REQ_QUEUE_SIZES.PNG;https://jira.hyperledger.org/secure/attachment/16000/INDY-1574_REQ_QUEUE_SIZES.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwqn:",,,,Unset,Unset,EV 18.19,Ev 18.20,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,VladimirWork,,,,,,,,,,,"10/Aug/18 10:19 PM;VladimirWork;All logs/journals/node_info files are in evernymr33:/home/ev/logs/1574.tar.gz;;;","11/Sep/18 8:41 PM;Derashe;Problem reason:
 * Pool stopped writing

Research:
 * Pool doesn't write because it have repeatable restarts. Because nodes raises and fall because of OOM exception
 * Nodes became restart because of OOM, that was caused by stashing large amount of requests and affiliated client/node traffic. So node even can't exchange it's LEDGER_STATUS messages
 * Stashing of requests were caused by view_change, which was called because of primary degraded (case with throughtput master == 0, backup == 0.1)
 * We also can define problem about long view_change (~ 1 hour), which will lasts untill reason ""could not complete in time"", because pool cannot collect quorum of view_change_done because of 

Conclusion:
 * We can distinguish two problems:
 ** false positive view_changes
 ** OOM error
 * These problems should be fixed is scope of:
 ** https://jira.hyperledger.org/browse/INDY-1639
 ** https://jira.hyperledger.org/browse/INDY-1681 

 

Recomendadtion for QA:
 * Retest this case when above PRs will be merged;;;","03/Oct/18 12:22 AM;VladimirWork;Build Info:
indy-node 1.6.619

Steps to Reproduce:
0. Preload pool with 300k txns.
1. Run 20 writing and 200 reading txns/sec load test.
2. Check validator-info/logs/metrics.

Actual Results:
Pool stopped writing due to OOM (see screenshot).  !INDY-1574_OOM.PNG|thumbnail! ;;;","03/Oct/18 6:50 PM;VladimirWork;All logs and validator-infos are in ev@evernymr33:logs/1574_NEW.tar.gz. Metrics with request_sizes:  !INDY-1574_REQ_QUEUE_SIZES.PNG|thumbnail! ;;;","08/Oct/18 9:59 PM;Derashe;Results of the last load testing:
 * Pool went down because of OOM and could not restore consensus.
 * Memory started to grow because one of backup primary nodes restarted and could not work as a primary (seems to be a known issue)
 * This node(a) was restarted because of connection problems with one(b) of another backup primary nodes, which led to OOM just on this node(a). (no problems have seen for other nodes in pool). But in the same time replica with primary node(b) did not turned off on node(a), what is strange. After connection restored, node(a) cannot write any txns in this replica.;;;",,,,,,,,,,,,,,,,,,,,
validator-info has stale indy-node service information,INDY-1575,32741,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ckochenower,ckochenower,11/Aug/18 2:48 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,1.4,,test-automation,validator-info,,0,Ledger,,,"Troubleshooting issues discovered while running Indy Chaos experiments has revealed that validator-info['Extractions']['indy-node_service'] (and likely other 'Extractions') information are stale (out-of-date).

!Screen Shot 2018-08-10 at 11.19.02 AM.png|thumbnail! 

Steps to reproduce:

1. Login to a validator node
2. Run `sudo validator-info -v --json | python3 -m json.tool` and locate the ""Active"" line under Extractions > indy-node_status.
3. Run `sudo systemctl status indy-node` and locate the ""Active"" line.
4. Compare them. The datetime will be the same, but the human readable uptime (i.e. 51min ago) will be different, suggesting that indy-node is not regularly updating service details.

Repeat steps 2 through 4 above after stopping indy-node service on the node. Note that validator-info still says the service is active.","The output from `apt list --installed` and `pip3 freeze` are as follows:
{noformat}
ubuntu@kellyohio7:~$ sudo apt list --installed | grep indy
indy-anoncreds/xenial,now 1.0.32 amd64 [installed]
indy-node/xenial,now 1.4.497 amd64 [installed,upgradable to: 1.6.556]
indy-plenum/xenial,now 1.4.442 amd64 [installed,upgradable to: 1.6.497]
libindy-crypto/xenial,now 0.4.0 amd64 [installed]
python3-indy-crypto/xenial,now 0.4.1 amd64 [installed]

ubuntu@kellyohio7:~$ sudo pip3 freeze | grep indy
indy-anoncreds==1.0.32
indy-crypto==0.4.1
indy-node==1.4.497
indy-plenum==1.4.442
indy-plenum-dev==1.4
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1541,,,,,,,,,,,,,,,,,,,,"11/Aug/18 2:38 AM;ckochenower;Screen Shot 2018-08-10 at 11.19.02 AM.png;https://jira.hyperledger.org/secure/attachment/15504/Screen+Shot+2018-08-10+at+11.19.02+AM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzmc7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ckochenower,,,,,,,,,,,"11/Aug/18 3:02 PM;anikitinDSR;[~ckochenower], this bug was fixed in indy-node 1.5.541. For now, systemctl status is dumped with other jeneral info.
For example:
ubuntu@tokyoQALive:~$ sudo validator-info -v | grep Active
 Active: active (running) since Fri 2018-08-10 23:11:43 UTC; 6h ago
 Active: active (running) since Fri 2018-08-10 23:11:43 UTC; 6h ago

ubuntu@tokyoQALive:~$ systemctl status indy-node | grep Active
 Active: active (running) since Fri 2018-08-10 23:11:43 UTC; 6h ago;;;","14/Aug/18 1:46 AM;ckochenower;[~anikitinDSR] - Thank you for pointing out that an upgrade to 1.5.541 or later fixes this issue. I just completed an upgraded to the following versions and indy-node and indy-node-control systemctl information appears to be updating correctly.

{code}
ubuntu@kellysaopaulo1:~$ apt list --installed | grep indy
indy-anoncreds/xenial,now 1.0.32 amd64 [installed]
indy-node/xenial,now 1.6.561 amd64 [installed]
indy-plenum/xenial,now 1.6.501 amd64 [installed]
libindy/xenial,now 1.6.1 amd64 [installed]
libindy-crypto/xenial,now 0.4.3 amd64 [installed]
python3-indy-crypto/xenial,xenial,now 0.4.1 amd64 [installed,upgradable to: 0.4.3]
{code}

Closed and marked as done. ;;;",,,,,,,,,,,,,,,,,,,,,,,
Need to implement consequent mode in the load script,INDY-1576,32750,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ozheregelya,ozheregelya,11/Aug/18 7:57 AM,11/Aug/18 7:57 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,Need to implement new mode in the load script to be able to send new requests only after receiving answer for the previous one.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzme7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replicas that are demoted continue to be reported as replicas,INDY-1577,32751,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,ckochenower,ckochenower,ckochenower,11/Aug/18 9:06 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,test-automation,,,0,,,,"See details logged in the following comment on INDY-1541 for steps to reproduce:

https://jira.hyperledger.org/browse/INDY-1541?focusedCommentId=48747&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48747","The output from `apt list --installed` and `pip3 freeze` are as follows:
{noformat}
ubuntu@kellyohio7:~$ sudo apt list --installed | grep indy
indy-anoncreds/xenial,now 1.0.32 amd64 [installed]
indy-node/xenial,now 1.4.497 amd64 [installed,upgradable to: 1.6.556]
indy-plenum/xenial,now 1.4.442 amd64 [installed,upgradable to: 1.6.497]
libindy-crypto/xenial,now 0.4.0 amd64 [installed]
python3-indy-crypto/xenial,now 0.4.1 amd64 [installed]

ubuntu@kellyohio7:~$ sudo pip3 freeze | grep indy
indy-anoncreds==1.0.32
indy-crypto==0.4.1
indy-node==1.4.497
indy-plenum==1.4.442
indy-plenum-dev==1.4
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1541,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzmef:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"13/Aug/18 11:55 PM;ckochenower;[~ashcherbakov] says that demoted replicas are only replaced during a view change.

https://jira.hyperledger.org/browse/INDY-1541?focusedCommentId=48778&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-48778

I have observed demoted replicas being replaced following a view change.

[~ashcherbakov] or [~spivachuk],

This issue can be closed if this is expected behavior. However, it would be nice to have answers to the following before closing this issue:

1. If demoted replicas are not replaced until a view change occurs, are the demoted replicas expected to continue to participate in consensus on it's own protocol instance until the next view change? It would make sense that it would/could without any negative side effects. Doing so allows the demoted node to resume it's role as a replica and eliminates the need for catch up in the event that the demoted node is promoted back to ""VALIDATOR"" before a view change occurs (demote and promote before a view change).

2. I was thinking that setting a node's ""services"" to blank ("""") effectively ""blacklisted"" the node, but [~slafranca] says he believes that demotion and blacklisting are two different things. If so, is there a way to programmatically blacklist a node and how would you expect the pool to behave if a replica is ""blacklisted"" vs. ""demoted"".;;;","17/Aug/18 2:22 AM;ckochenower;I have discovered for myself in indy-cli documentation that setting a node's services to blank ("""") is effectively termed ""blacklisting"" a node.

The following is taken from the `ledger node help` output:

""services - (optional) Node type. One of: VALIDATOR, OBSERVER or empty in case of blacklisting node"";;;","17/Aug/18 2:25 AM;ckochenower;I am closing this issue and will send a direct slack message to the DSR team to get clarification on the following question:

{panel}
If demoted replicas are not replaced until a view change occurs, are the demoted replicas expected to continue to participate in consensus on it's own protocol instance until the next view change? It would make sense that it would/could without any negative side effects. Doing so allows the demoted node to resume it's role as a replica and eliminates the need for catch up in the event that the demoted node is promoted back to ""VALIDATOR"" before a view change occurs (demote and promote before a view change).
{panel};;;",,,,,,,,,,,,,,,,,,,,,,
Promotion Workflow - Potential race condition when restarting node after promotion,INDY-1578,32752,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,keichiri,ckochenower,ckochenower,11/Aug/18 9:28 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,test-automation,,,0,,,,"When running the Demote Replica Chaos experiment defined by INDY-1541, one or more nodes persistently get ""stuck"" even following the prescribed restart following node promotion.

[~ashcherbakov] asked that I log this issue and attach the following artifacts:

1.  Logs for demoted/promoted nodes
2. Logs for at least 1 normal node

Identical steps to those executed by the Demote Replica Chaos experiment were exercised manually and thus removed chaostoolkit/chaosindy from the equation and was unable to reproduce the problem. Perhaps there is a race condition caused by the programmatic execution of the steps faster than can be done manually?

Manual steps and their results are outlined in the following comment on INDY-1541:

https://jira.hyperledger.org/browse/INDY-1541?focusedCommentId=48747&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48747

The attached logs are for the following nodes:

Node1 - Was the master before indy-node was stopped to force view change. What you see in the screenshot is Node1's state following a indy-node service start on Node1 immediately AFTER promoting Node3 and Node4 (indy-node was restarted on Node3 and Node4 as prescribed by the ""node promotion workflow"")

Node4 - The demoted node that has the correct/expected state following promotion and indy-node service restart

Node3 - The demoted node that has the incorrect/unexpected state following promotion and indy-node service restart

Node2 - The replica that became the master after indy-node on Node1 was stopped.

","The output from `apt list --installed` and `pip3 freeze` are as follows:
{noformat}
ubuntu@kellyohio7:~$ sudo apt list --installed | grep indy
indy-anoncreds/xenial,now 1.0.32 amd64 [installed]
indy-node/xenial,now 1.4.497 amd64 [installed,upgradable to: 1.6.556]
indy-plenum/xenial,now 1.4.442 amd64 [installed,upgradable to: 1.6.497]
libindy-crypto/xenial,now 0.4.0 amd64 [installed]
python3-indy-crypto/xenial,now 0.4.1 amd64 [installed]

ubuntu@kellyohio7:~$ sudo pip3 freeze | grep indy
indy-anoncreds==1.0.32
indy-crypto==0.4.1
indy-node==1.4.497
indy-plenum==1.4.442
indy-plenum-dev==1.4
{noformat}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1541,,,,,INDY-1560,INDY-1640,,,,,,,,,,,,,,"11/Aug/18 9:27 AM;ckochenower;Node1.log;https://jira.hyperledger.org/secure/attachment/15524/Node1.log","11/Aug/18 9:28 AM;ckochenower;Node2.log;https://jira.hyperledger.org/secure/attachment/15523/Node2.log","11/Aug/18 9:28 AM;ckochenower;Node3.log;https://jira.hyperledger.org/secure/attachment/15522/Node3.log","11/Aug/18 9:28 AM;ckochenower;Node4.log;https://jira.hyperledger.org/secure/attachment/15521/Node4.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzx16f:",,,,Unset,Unset,EV 18.18 Service Pack 2,EV 18.19,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,keichiri,,,,,,,,,,,"11/Aug/18 9:31 AM;ckochenower;nscapture archives for each of the 10 nodes are available at https://drive.google.com/drive/folders/1-I_qJbtywjkuESQc7Mj-zHIqS_Vb1jgU?usp=sharing;;;","16/Aug/18 1:55 AM;ckochenower;A similar scenario exercised by the ""shrink pool"" Chaos experiment resulted in the same results. Attaching logs and/or nscapture archives from both experiments will clutter this issue.

The ""shrink pool"" experiment can reproduce the results documented [here|https://jira.hyperledger.org/browse/INDY-1560?focusedCommentId=48898&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-48898]. 

Please request logs and/or nscapture archives produced by the ""shrink pool"" experiment if you feel they would provide better/additional insight.;;;","16/Aug/18 8:10 AM;ckochenower;When a pool's f_value changes (and thus the Count_of_replica changes, because it is f+1), perhaps the node that was promoted and restarted doesn't immediately see the correct number of nodes, because the pool ledger is _eventually consistent_ and needs slightly more time to commit the change to the pool ledger?

I added a 10 second sleep between setting ""services"" equal to ""VALIDATOR"" and restarting the node and the node no longer needs a second restart to get into sync.

I will try reducing the sleep until I find an approximate minimum time to wait.;;;","26/Sep/18 8:45 AM;ckochenower;This is still a problem after upgrading to the following versions:;;;","26/Sep/18 8:45 AM;ckochenower;This is still a problem after upgrading to the following versions:

{code}
ubuntu@kellyseoul10:~$ apt list --installed | grep indy

WARNING: apt does not have a stable CLI interface. Use with caution in scripts.

indy-anoncreds/xenial,now 1.0.32 amd64 [installed]
indy-node/xenial,now 1.6.603 amd64 [installed,upgradable to: 1.6.613]
indy-plenum/xenial,now 1.6.539 amd64 [installed,upgradable to: 1.6.545]
libindy-crypto/xenial,now 0.4.3 amd64 [installed]
python3-indy-crypto/xenial,now 0.4.3 amd64 [installed]

ubuntu@kellyseoul10:~$ pip3 list installed | grep indy
indy-anoncreds (1.0.32)
indy-crypto (0.4.1)
indy-node (1.6.603)
indy-plenum (1.6.539)
indy-plenum-dev (1.4)
{code};;;","27/Sep/18 11:27 PM;keichiri;After long investigation, this problem was confirmed, and two different bugs contribute to it:
1. one of the nodes ends up with an incorrect replica count
2. one of the nodes has a replica with incorrect primaries

The bugs are in the plenum implementation

I confirmed the bug by writing the test to represent the mentioned scenario as accurately as possible. [~Derashe] knew where the issues might lay, so he confirmed it in code

Two separate tickets are created:
1. https://jira.hyperledger.org/browse/INDY-1719
2. https://jira.hyperledger.org/browse/INDY-1720;;;",,,,,,,,,,,,,,,,,,,
Incorrect upgrade error due to already used upgrade name,INDY-1579,32753,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,VladimirWork,VladimirWork,11/Aug/18 5:15 PM,12/Oct/18 4:19 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Build Info:
sovrin 1.1.67
indy-node 1.5.555

Steps to Reproduce:
1. Try to upgrade indy-node with sovrin package installed via old/new CLI:
{noformat}
indy@sandbox> new key with seed 000000000000000000000000Trustee1
Key created in wallet Default
DID for key is V4SGRU86Z58d6TV7PBUe6f
Verification key is ~CoRER63DVYnWZtK8uAzNbx
Current DID set to V4SGRU86Z58d6TV7PBUe6f
indy@sandbox> send POOL_UPGRADE name=upgrade556 version=1.5.556 sha256=ed0a366b4ef36d40c055672a8b83679e99246fec71a706b4ae4cb7958feace3f action=start schedule={'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv
':'2018-08-10T11:00:00.258870+00:00','8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb': '2018-08-10T11:00:00.258870+00:00', 'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya': '2018-08-10T11:00:00.258870+00:00',
 '4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA': '2018-08-10T11:00:00.258870+00:00'} timeout=10 force=True
Sending pool upgrade upgrade556 for version 1.5.556
Pool upgrade failed: client request invalid: UnauthorizedClientRequest('TRUSTEE cannot do POOL_UPGRADE',)
{noformat}

{noformat}
pool(p1):wallet(w1):did(V4S...e6f):indy> ledger pool-upgrade name=upgrade556 version=1.5.556 action=start sha256=f284bdc3c1c9e24a494e285cb387c69510f28de51c15bb93179d9c7f28705398 schedule={""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-08-10T10:45:00.000000+00:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-08-10T10:50:00.000000+00:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-08-10T10:55:00.000000+00:00"",""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-08-10T11:00:00.000000+00:00""} timeout=10 force=true
Transaction has been rejected: TRUSTEE cannot do POOL_UPGRADE
{noformat}

Actual Result:
{color:red}Pool upgrade failed: client request invalid: UnauthorizedClientRequest('TRUSTEE cannot do POOL_UPGRADE',) / Transaction has been rejected: TRUSTEE cannot do POOL_UPGRADE{color}

Expected Results:
There was another cancelled upgrade with the same name, so we should see error about already used upgrade name in this case.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1491,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwxkf:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node doesn't write txns after disconnection from the rest nodes,INDY-1580,32764,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ozheregelya,ozheregelya,11/Aug/18 10:20 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"*Environment:*
 indy-node 1.6.558
 libindy 1.6.1~655

*Steps to Reproduce:*
 1. Setup the docker pool of 7 nodes.
 2. On one of the nodes (node 5) block the Primary (node 1) IP:
 iptables -A INPUT -s 10.0.0.2 -j DROP
 3. Check nodes logs.
 => Only 1 INSTANCE_CHANGE\{'viewNo': 1, 'reason': 25} was send by Node5, but one more INSTANCE_CHANGE\{'viewNo': 1, 'reason': 26} was send by Node7.
 4. Unblock the Primary IP:
 iptables -D INPUT -s 10.0.0.2 -j DROP
 5. Send several txns.

*Actual Results:*
 Node5 sends INSTANCE_CHANGE\{'viewNo': 1, 'reason': 25} after each txn sending even if it don't have network issues in connection to Primary. New txns are not written on Node5.

*Expected Results:*
 Node5 should work with Primary correctly, new txns should be written.

Additional Information:
 After `iptables -D INPUT -s 10.0.0.2 -j DROP` primary node (node 1) was pinged from node which was disconnected before (node 5) and ping was successful, so simulated network issues were resolved.

See logs in attachment.

*UPD:*
After disconnection of Node5 from primary, Node5 was disconnected from the rest nodes in the pool. This behavior was caused by docker network behavior or iptables behavior in local network. So, this case should be re-tested on AWS pool in scope of confirmation testing of this ticket.

Summary was changed from ""Node doesn't write txns after disconnection from primary"" to ""Node doesn't write txns after disconnection from the rest nodes"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1544,,,,,,,,,,,,,,,"11/Aug/18 10:20 PM;ozheregelya;Node1.7z;https://jira.hyperledger.org/secure/attachment/15526/Node1.7z","11/Aug/18 10:20 PM;ozheregelya;Node2.7z;https://jira.hyperledger.org/secure/attachment/15527/Node2.7z","11/Aug/18 10:20 PM;ozheregelya;Node3.7z;https://jira.hyperledger.org/secure/attachment/15528/Node3.7z","11/Aug/18 10:20 PM;ozheregelya;Node4.7z;https://jira.hyperledger.org/secure/attachment/15529/Node4.7z","11/Aug/18 10:20 PM;ozheregelya;Node5.7z;https://jira.hyperledger.org/secure/attachment/15530/Node5.7z","11/Aug/18 10:20 PM;ozheregelya;Node6.7z;https://jira.hyperledger.org/secure/attachment/15531/Node6.7z","11/Aug/18 10:20 PM;ozheregelya;Node7.7z;https://jira.hyperledger.org/secure/attachment/15532/Node7.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwsf:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,,,,,,,,,,,"15/Aug/18 10:24 PM;Derashe;Problem reason:
 * Node5 stopped writing txns in it's ledger after removal of network issues.

Research:
 * While researching logs, we found that master replica of Node5 wasn't able to write txns, while backup replicas wrote txn as usual. That explains why did we send INSTANCE_CHANGE\{'viewNo': 1, 'reason': 25}
 * This situation can appear, when NodeX get new pre-prepare (0,3 for example), then because of connection troubles we've lost all prepare and commit messages for (0,3). But pool had quorum to order (0,3) and so to overtook NodeX. In that case NodeX will stay behid the pool till the nearest stable checkpoint, when it will catchup ledger and continue ordering with pool

 ;;;","27/Aug/18 7:52 PM;ozheregelya;INDY-1163 will be fixed in scope of this ticket, so it should be re-tested as well.;;;","29/Aug/18 2:34 AM;Derashe;Some tests were made to make sure that this behind node will catchup on closest stable checkpoint and pool will continue ordering. Also we checked, that in case of loosing quorum, pool will not breake and stop writing txns.

We could provide solution to resolve this issue, such as request missing messages again, but we didn't because of two reasons:
 * This may serioulsy affect node behaviuor and slow it down in some cases
 * Case that was reproduces in test environment with iptables is a bit syntetic. In real case, there is low probablility to reproduce this

Covered with tests:
 * [plenum/test/node_request/test_node_got_no_preprepare.py|https://github.com/hyperledger/indy-plenum/pull/894/files#diff-ecb7ed89c12ee93adde659056521eb71]
 * [plenum/test/node_request/test_node_got_only_preprepare.py|https://github.com/hyperledger/indy-plenum/pull/894/files#diff-d5e169d031dabc49cf0dc530532f16ce]

Commited info:
 * [https://github.com/hyperledger/indy-plenum/pull/894]

Conclusion:

That problem will be resolved and pool will continue stable work after achieving closest stable checkpoint

Recomendation for QA:
 * Reproduce case of this ticket and after unblocking primary, make pool order 300 batches (enought for stable checkpoint)
 * After these batches nodes should work correctly;;;","08/Sep/18 5:04 AM;ozheregelya;*Environment:*
indy-node 1.6.599

*Steps to Validate:*
1. Setup the docker pool of 7 nodes.
2. On one of the nodes (node 5) block the Primary (node 1) IP:
iptables -A INPUT -s 10.0.0.2 -j DROP
3. Check nodes logs.
=> Only 1 INSTANCE_CHANGE\{'viewNo': 1, 'reason': 25} was send by Node5, but one more INSTANCE_CHANGE\{'viewNo': 1, 'reason': 26} was send by Node7.
4. Unblock the Primary IP:
iptables -D INPUT -s 10.0.0.2 -j DROP
5. Run load test.

*Actual Results:*
Node 5 wrote all txns.

INDY-1163 was not reproduced.;;;",,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.70,INDY-1581,32765,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,VladimirWork,VladimirWork,11/Aug/18 10:31 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6,,,19/Aug/18 12:00 AM,0,Documentation,,,"*Version Information*
 indy-node 1.6.70
 indy-plenum 1.6.49
 indy-anoncreds 1.0.11
 sovrin 1.1.13

*Major Fixes*
 INDY-1473 - Several nodes (less than f) get ahead the rest ones under load
 INDY-1539 - Pool has stopped to write txns
 INDY-1497 - Re-send messages to disconnected remotes
 INDY-1478 - Pool stopped writing under 20txns/sec load
 INDY-1519 - 1.3.62 -> 1.5.67 forced upgrade without one node in schedule was failed
 INDY-1502 - tmp.log must have unique name
 INDY-1199 - A node need to hook up to a lower viewChange
 INDY-1470 - One of the nodes laged behind others after forced view changes
 INDY-1544 - View Change should not be triggered by re-sending Primary disconnected if Primary is not disconnected anymore

*Changes and Additions*
 INDY-1491 - As a Trustee running POOL_UPGRADE txn, I need to specify any package depending on indy-node, so that the package with the dependencies get upgraded
 INDY-1555 - Monitor needs to be reset after the view change
 INDY-1545 - GC by Checkpoints should not be triggered during View Change
 INDY-1542 - Validator Info must show committed and uncommitted roots for all states
 INDY-1475 - Explore timing and execution time
 INDY-1493 - Memory leaks profiling
 INDY-1531 - Bind connection socket to NODE_IP
 INDY-1496 - Enable TRACK_CONNECTED_CLIENTS_NUM option
 INDY-1378 - We should update revocation registry delta value during REG_ENTRY_REVOC writing
 INDY-1480 - Support latest SDK in Indy Plenum and Node
 INDY-1468 - Latency measurements in monitor should be windowed
 INDY-1528 - Trust anchor permission not needed for ledger writes

*Known Issues*
INDY-1517 - Docker pool can't be built because of new python3-indy-crypto in sdk repo
(i) The problem described in INDY-1517 will be fixed in the next release of indy-node. Workaround for this problem is to add python3-indy-crypto=0.4.1 to the list of packages to be installed.
 INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66
 (!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]
 (!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI for pool upgrade*.

(!) *Pool upgrade from indy-node 1.3.62 to indy-node 1.6.70 should be performed simultaneously for all nodes due to txn format changes.*
 (!) *There must be sovrin package upgrade to 1.1.13 version after indy-node package upgrade. You need to specify package=sovrin in pool-upgrade command to do this.*
 (!) *All indy-cli pools should be recreated with actual genesis files.*
 (i) *For more details about txn format changes see INDY-1421.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzlkn:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),TechWritingWhiz,VladimirWork,,,,,,,,,,,"11/Aug/18 10:35 PM;VladimirWork;FYI [~krw910];;;","18/Aug/18 6:30 AM;TechWritingWhiz;The pull request for these are here: https://github.com/sovrin-foundation/sovrin/pull/85

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Do not use average when calculating total throughput/latency of backups,INDY-1582,32781,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,13/Aug/18 4:57 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.73,,,,0,,,,"Currently we use a simple average when calculating the total throughput/latency of backupo instances to compare it with the master ones.

This may lead to problems of false positive View Changes when there is a spike on just one of a backups.

*Acceptance criteria*
 * Define a better way to calculate the average for backups. Consider median which is more robust to spikes. 
 * Write unit tests checking, in particular, that spikes should not trigger false positives.
 * Implement the changes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwlb:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"20/Aug/18 4:16 PM;anikitinDSR;Reasons:
 * need to change average calculating logic for total throughput/latency of backups

Changes:
 * added median calculating for backup's throughput/latency

PRs:
 * indy-plenum [https://github.com/hyperledger/indy-plenum/pull/871]
 * indy-node [https://github.com/hyperledger/indy-node/pull/899]

Versions:
 * indy-node: 1.6.568

Steps to validate:
 * in the next load_test experiments should decreased false-positive view_change triggering

 ;;;","24/Aug/18 5:47 PM;VladimirWork;Build Info:
indy-node 1.6.575

Steps to Validate:
1. Run load tests with close to max throughput for 8+ hours to force VCs.

Actual Results:
We have no unexpected VCs with all types of txn except revoc_reg_entry (will be fixed and checked in scope of INDY-1553) and payments (will be fixed in scope of INDY-1619).;;;",,,,,,,,,,,,,,,,,,,,,,,
Pool stopped writing after F change,INDY-1583,32783,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,13/Aug/18 5:24 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"*Steps to Reproduce:""
1) Make sure that Node1 is primary
2) stop service on Node2
3) stop service on Node3
4) send nym
5) demote Node6
6) start service on Node3
7) try to send nym

*Actual results:*
On step 7 transaction hasn't been written","Docker, 7 nodes
indy-node 1.6.70",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1163,,,,,INDY-1676,,,,,,,,,,,,,,,"13/Aug/18 6:32 PM;zhigunenko.dsr;Logs.7z;https://jira.hyperledger.org/secure/attachment/15544/Logs.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzx167:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,zhigunenko.dsr,,,,,,,,,,,"17/Aug/18 5:40 PM;Derashe;Problem reason:
 * After steps in description, pool stopped writing txns.

Research:
 * Pool stopped ordering because had no quorum for PREPARES for new txns. Because Node3 that've just connected cant send PREPARE.
 * Node3 can't send PREPARE because it discard PRE-PREPARE fir this txns.
 * It discard PRE-PREPARE because of reason ""PRE-PREPARE being sent to primary""
 * That behaviour is unpredictable and hard to inverstigate and it will be researched in scope of https://jira.hyperledger.org/browse/INDY-1163 

Changes:
 * For now, it was decided to remove this check as it seems to be pointless

Committed into:
 * https://github.com/hyperledger/indy-plenum/pull/878

Recommendations for QA: 
 * Retest this case with actual node;;;","20/Aug/18 5:50 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.569
indy-plenum 1.6.512

*Steps to Reproduce:*
1) Make sure that Node1 is primary
2) stop service on Node2
3) stop service on Node3
4) send nym
5) demote Node6
6) start service on Node3
7) try to send nym

*Actual results:*
Problem is still here

*Additional info:*
_plenum/server/replica.py_
Status can be occasionally equal to PP_CHECK_TO_PRIMARY;;;","27/Aug/18 8:48 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.580
indy-plenum 1.6.524

*Steps to Validate:*
1) Make sure that Node1 is primary
2) stop service on Node2
3) stop service on Node3
4) send nym
5) demote Node6
6) start service on Node3
7) try to send nym

*Actual results:*
NYM has been written;;;","27/Aug/18 10:31 PM;Derashe;Research:
 * During retesting was revealed that problem is about incorrect bls multisignature checking

Changes:
 * Fix for checking bls signature was added

Commited info: 
 * [https://github.com/hyperledger/indy-plenum/pull/890];;;",,,,,,,,,,,,,,,,,,,,,
Failed upgrade was re-scheduled and happened after manual upgrade,INDY-1584,32785,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ozheregelya,ozheregelya,13/Aug/18 7:11 PM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"*Environment:*
 indy-node 1.6.70

*Steps to Reproduce:*
 1. Setup the pool of 25 nodes with version 1.3.62.
 2. On one of the nodes change rc repo to master to get upgrade failed on this node (node 25 in attached logs).
 3. Schedule upgrade to 1.6.70 version.
 => Upgrade was happened on all nodes exclude Node25 (because the version was not fount in master repo) and Node7 (because of INDY-1447).
 4. Perform manual upgrade on nodes 25 and 7.

*Actual Results:*
 Upgrade log was rewritten, record about failed upgrade (at 14:35) was lost, new records about this upgrade appeared, upgrade was happened.

*Expected Results:*
 Upgrade log should not be affected by manual upgrade the node. Failed upgrade should not be re-scheduled and should not happen.

Some logs were collected after 1 day of load and they are too big for attachment. All logs are placed here: s3://qanodelogs/indy-1584/NodeXX.7z
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1584/ /home/ev/logs/indy-1584/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Aug/18 8:14 PM;ozheregelya;upgrade_Node1.7z;https://jira.hyperledger.org/secure/attachment/15546/upgrade_Node1.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node10.7z;https://jira.hyperledger.org/secure/attachment/15553/upgrade_Node10.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node11.7z;https://jira.hyperledger.org/secure/attachment/15554/upgrade_Node11.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node12.7z;https://jira.hyperledger.org/secure/attachment/15555/upgrade_Node12.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node13.7z;https://jira.hyperledger.org/secure/attachment/15556/upgrade_Node13.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node14.7z;https://jira.hyperledger.org/secure/attachment/15557/upgrade_Node14.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node15.7z;https://jira.hyperledger.org/secure/attachment/15558/upgrade_Node15.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node16.7z;https://jira.hyperledger.org/secure/attachment/15559/upgrade_Node16.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node17.7z;https://jira.hyperledger.org/secure/attachment/15560/upgrade_Node17.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node18.7z;https://jira.hyperledger.org/secure/attachment/15561/upgrade_Node18.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node19.7z;https://jira.hyperledger.org/secure/attachment/15562/upgrade_Node19.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node2.7z;https://jira.hyperledger.org/secure/attachment/15547/upgrade_Node2.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node20.7z;https://jira.hyperledger.org/secure/attachment/15563/upgrade_Node20.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node21.7z;https://jira.hyperledger.org/secure/attachment/15564/upgrade_Node21.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node22.7z;https://jira.hyperledger.org/secure/attachment/15565/upgrade_Node22.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node23.7z;https://jira.hyperledger.org/secure/attachment/15566/upgrade_Node23.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node24.7z;https://jira.hyperledger.org/secure/attachment/15567/upgrade_Node24.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node3.7z;https://jira.hyperledger.org/secure/attachment/15548/upgrade_Node3.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node4.7z;https://jira.hyperledger.org/secure/attachment/15549/upgrade_Node4.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node5.7z;https://jira.hyperledger.org/secure/attachment/15550/upgrade_Node5.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node6.7z;https://jira.hyperledger.org/secure/attachment/15551/upgrade_Node6.7z","13/Aug/18 8:14 PM;ozheregelya;upgrade_Node7.7z;https://jira.hyperledger.org/secure/attachment/15552/upgrade_Node7.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001ywbs",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,zhigunenko.dsr,,,,,,,,,,,"13/Dec/18 8:03 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.79 -> 1.6.80 (25 nodes)

*Steps to Validate:*
1) order pool-upgrade transactions (existing version, withiout force)
2) wait until the upgrade finish
3) stop services on first F-2 nodes (replica 0, replica 1, etc)
4) before the view change is over, send pool-upgrade transactions (non-existing version, force, in far future)

*Expected Results:*
1) non-existing version upgrade has been scheduled or ignored
2) no immediate upgrade

*Actual Results:*
1) non-existing version upgrade has been scheduled and cancelled immediately
2) after that the last successful(to 1.6.80) upgrade procedure has been repeated (with the same transaction ID);;;",,,,,,,,,,,,,,,,,,,,,,,,
"As Indy Node, I should not process and order client requests for which PoolTimeout was already received",INDY-1585,32791,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,13/Aug/18 10:05 PM,17/Aug/18 7:58 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"If the pool was busy and slow (that is wasn't able to get clients reqs from transport queues fast enough), so that we are sure clients received PoolLedgerTimeout for a request, then it doesn't make sense to start processing and ordering this request once it's finally got from the transport queue.

It can also solve the issue with a long ordering and processing of stashed (old) requests in the queue after the DDoS.

*Acceptance criteria:*
 * Check the time of request creation when get it from the queue

 * 
 ** We may need to extend request format to include timestamp there
 ** We may use  reqId is it's time-based
 * If the time is less than the PoolLedgerTImeout + some delta, then just discard the request. Do not send anything to the client.

 

*Concerns:*
 * PoolLedgerTimeout is currently configruable on the SDK side, so we may not know the exact time clients wait for requests
 * The client may have different timing than on the node side, that's why we need some delta when check it.
 * Can we use reqId to get client's timestamp?
 * What if request is re-sent with the same reqId multiple times? Should we reject before checking that the req with the given digest is already written to the ledger?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1241,,,No,,Unset,No,,,"1|hzwxpr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As a QA/Support engineer, Trustee, or Steward, I want a node's ""services"" included in validator-info",INDY-1586,32798,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ckochenower,ckochenower,14/Aug/18 2:16 AM,08/Jan/19 4:26 AM,28/Oct/23 2:47 AM,,,,test-automation,validator-info,,0,,,,"QA Engineers testing the ledger (i.e. Chaos experiments, system tests, etc.), Support Engineers troubleshooting ledger issues, and software engineers developing applications for Trustees and Stewards (i.e. ledger pool dashboard, diagnostic tools for operations) would benefit from having a node's ""services"" included in validator-info output.

The Chaos experiments being developed for indy-node provides sample code for reading a node's ""services"" from the pool ledger:

https://github.com/evernym/chaosindy/blob/master/chaosindy/ledger_interaction.py#L111-L243

It is recommended that the validator-info verbose json output (`validator-info -v --json`) include a node's services state (i.e. """", ""[OBSERVER]"", or ""[VALIDATOR]""). Perhaps under ""Node_info""? I would sort of expect ""Mode"" to be sufficient to indicate if a node's services are ""VALIDATOR"" (i.e. ""participating""), ""OBSERVER"" (i.e. ""observing""), """" (i.e. ""blacklisted""), or something like that. Not sure if I am using the term ""blacklisted"" correctly in this context.",,,,,,,,,,,,,,,,,INDY-1526,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Aug/18 2:56 AM;ckochenower;Screen Shot 2018-08-13 at 11.42.11 AM.png;https://jira.hyperledger.org/secure/attachment/15568/Screen+Shot+2018-08-13+at+11.42.11+AM.png","14/Aug/18 2:57 AM;ckochenower;Screen Shot 2018-08-13 at 11.48.10 AM.png;https://jira.hyperledger.org/secure/attachment/15569/Screen+Shot+2018-08-13+at+11.48.10+AM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzmm7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"14/Aug/18 2:57 AM;ckochenower;I currently have to run two separate scripts to troubleshoot/analyze pool state:

The following screen shots depict the output of both of the aforementioned scripts AFTER demoting Node4 (out of 10 nodes) in my test pool. The output on the left is a subset of data from validator-info. The output on the right is data provided by https://github.com/evernym/chaosindy/blob/master/chaosindy/ledger_interaction.py#L111-L243

Before restarting indy-node service on Node4:
 !Screen Shot 2018-08-13 at 11.42.11 AM.png|thumbnail! 

After restarting indy-node service on Node4:
 !Screen Shot 2018-08-13 at 11.48.10 AM.png|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,,,
We need to be able to specify different quotas for Client-to-node and Node-to-node stacks,INDY-1587,32809,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,14/Aug/18 4:38 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"As of now, we can specify LISTENER_QUOTA and REMOTE_QUOTA, but LISTENER_QUOTA is applied for both Node-to-Node and Client-to-Node stacks (REMOTE_QUOTA is only for client-to-client, that is old deprecated agents).

We need to be able to set different quotas for node and client stacks.

 

*Acceptance criteria*
 * Add new params to Plenum's config: NODE_TO_NODE_STACK_QUOTA, CLIENT_TO_NODE_STACK_QUOTA.
 * Pass the values of these params to Node and Client Stacks as values for Listener quota.
 * For now, let's keep the default values (100/100) for both",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1241,,,No,,Unset,No,,,"1|hzwwnr:",,,,Unset,Unset,EV 18.16 Releasing 1.6,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,,"16/Aug/18 9:47 PM;anikitinDSR;Was devided quota and size limitations for node and client stacks.
PR:
https://github.com/hyperledger/indy-plenum/pull/874;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As a Steward, I need to have a script that can generate Proof of possession for my BLS key, so that I can use this value in a NODE txn",INDY-1588,32810,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,14/Aug/18 5:52 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.73,,,,0,,,,"Each Steward must send a NODE txn with Proof of Possession of his BLS keys. So, we need a script that can help in generating the transaction.

*Acceptance criteria*
 * Create a script in Indy-node: `generate_bls_proof_of_posession`

 ** the script need to be installed with Indy Node
 ** Input for the script: no
 ** Output:
 *** BLS Public key in a form to be sent in NODE txn (base58-encoded)
 *** BLS Proof of possession for the Public Key in a form to be sent in NODE txn (base58-encoded)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwn3:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,zhigunenko.dsr,,,,,,,,,,"23/Aug/18 9:27 PM;Toktar;Problem reason:
 - Stewards need in a script for generating Proof of possession for the existing BLS key

Changes:
 - Added script [generate_bls_proof_of_possession|https://github.com/hyperledger/indy-node/pull/911/files#diff-6430731e9ad44c139b219482ce344760]. 

PR:
 - [https://github.com/hyperledger/indy-node/pull/911]
 - [https://github.com/hyperledger/indy-plenum/pull/886]

Version:
 * indy-node 1.6.576 -master
 * indy-plenum 1.6.518 -master

Risk factors:
 - Problem with [enable_bls|https://github.com/hyperledger/indy-node/pull/911/files#diff-89f14930e3da6bc0bb930ae80fe61618] script

Risk:
 - Low

Recommendations for QA:
 * Test [enable_bls|https://github.com/hyperledger/indy-node/pull/911/files#diff-89f14930e3da6bc0bb930ae80fe61618] for correct work
 * Test [generate_bls_proof_of_possession|https://github.com/hyperledger/indy-node/pull/911/files#diff-6430731e9ad44c139b219482ce344760] for correct work
 ** Start pool
 ** Start [generate_bls_proof_of_possession|https://github.com/hyperledger/indy-node/pull/911/files#diff-6430731e9ad44c139b219482ce344760]
 ** Send a NODE transaction with the old BLS Public key and new proof.;;;","28/Aug/18 10:51 PM;zhigunenko.dsr;*Environment:*
indy-anoncreds 1.0.32
indy-node 1.6.580
indy-plenum 1.6.524
libindy-crypto 0.4.3
python3-indy-crypto 0.4.3

*Steps to Reproduce:*
1) Create pool with 4 nodes
2) Add another 3 nodes

*Expected Results:*
Nodes have been added

*Actual Results:*
init_indy_node generates base58 incompatible keys

{code}
Transaction has been rejected: validation error [ClientNodeOperation]: should not contain the following chars ['0']
{code}
;;;","29/Aug/18 12:08 AM;Toktar;After generating keys via script init_indy_node verkey and public keys should be convert to base58 before sending it in transaction. ;;;","29/Aug/18 11:05 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.583
indy-plenum 1.6.527

*Steps to Validate:*
1) Create pool with 4 nodes
2) Add another node

*Actual Results:*
Node has been added
Pool can write after node adding

;;;",,,,,,,,,,,,,,,,,,,,,
A node should be able to participate in BLS multi-signature only if it has a valid proof of possession,INDY-1589,32811,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Aug/18 6:09 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"Once all Stewards send a NODE txn with proof of possession, we should allow a Node to participate in BLS multi-signature only if it has a BLS key with a proof of possession.

*Acceptance criteria*
 * Check that there is a valid proof of possession when creating a registry of BLS keys (see `BlsKeyRegister`)
 * Add tests that Nodes without PoP can not participate in BLS multi-signature.
 * The code may be disabled for a time being (until all Stewards provide a PoP).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwt3:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,,"30/Aug/18 7:56 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/901;;;","04/Sep/18 8:04 PM;Toktar;Problem reason:
 - When validating the bls signature, the key that the message was signed should be checked for proof of possession

Changes:
 - Added this check. If NODE transaction ordered a new bls key without proof of possession and parameter VALIDATE_BLS_SIGNATURE_WITHOUT_KEY_PROOF = False, the key will not be found and validation will not be pass.

PR:
 - [https://github.com/hyperledger/indy-node/pull/931]
 - [https://github.com/hyperledger/indy-plenum/pull/901]

Version:
 * indy-node 1.6.593 -master
 * indy-plenum 1.6.535 -master

Risk factors:
 - Problem with transactions ordering.

Risk:
 - Low

Covered with tests:
 * [test_sign_validation_for_key_proof_exist.py|https://github.com/hyperledger/indy-plenum/pull/901/files#diff-fc85142149f81d208e2e1f8a4ee7fc7d]

Recommendations for QA:
 * Send transaction with bls key and without proof of possession on the old version (before all tickets with bls proofs)
 * Upgrade pool for the last version
 * Send transaction and check it was successfully finish
 * Set in configs VALIDATE_BLS_SIGNATURE_WITHOUT_KEY_PROOF = False
 * Send transaction and check it wasn't successfully finish
 * Back to VALIDATE_BLS_SIGNATURE_WITHOUT_KEY_PROOF = True and check again successful ordering.;;;","11/Sep/18 10:55 PM;VladimirWork;Build Info:
indy-node 1.6.601

Steps to Validate:
1. Send transaction with bls key and without proof of possession on the 1.5.550 version (before all tickets with bls proofs).
2. Upgrade pool to the 1.6.601 version.
3. Send transaction and check it was successfully finish.
4. Set in configs VALIDATE_BLS_SIGNATURE_WITHOUT_KEY_PROOF = False.
5. Send transaction and check it wasn't successfully finish.
6. Back to VALIDATE_BLS_SIGNATURE_WITHOUT_KEY_PROOF = True.
7. Send transaction and check it was successfully finish.

Actual Results:
We get success at Steps 3,7 (so we handle txns without proofs of possession by default and with True in config) and get timeout at Step 5 (so we don't handle txns without proofs of possession with False in config).;;;",,,,,,,,,,,,,,,,,,,,,,
Explore indy-plenum and indy-node tests,INDY-1590,32813,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,14/Aug/18 7:20 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,We need to explore indy-plenum and indy-node tests to separate test logic from test implementation. Then we should improve our existing post-install automation tests and design new ones according to new test cases found during this exploration.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1434,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzmof:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run load with a lot of requests not passing static validation (NACKs),INDY-1592,32837,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,ashcherbakov,ashcherbakov,15/Aug/18 3:42 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/18 5:32 PM;NataliaDracheva;metrics.png;https://jira.hyperledger.org/secure/attachment/15787/metrics.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwxlb:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,,,,,,,,,,,"07/Sep/18 5:35 PM;NataliaDracheva;*Build version:*
indy-node: 1.6.598
indy-plenum: 1.6.538

*Test description:* 10 nyms from 1 client with a seed of a DID which is not in a ledger for 17 hours.

*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
1 machine X python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym -s=000000000000000000000NotTrustee3
{code}
*Expected results:* 
~606k nacks

*Actual results:* 
~591k nacks
View changes: 1 (code 26)

*Additional info* 
\\iserver\exchange\Evernym\INDY-1592\1592_17h_nacks

*Metrics:*

!metrics.png|thumbnail!;;;",,,,,,,,,,,,,,,,,,,,,,,,
Run load with a lot of requests not passing dynamic validation (Rejects),INDY-1593,32838,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,ashcherbakov,ashcherbakov,15/Aug/18 3:42 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1700,,,,,,,,,,,,,,,"07/Sep/18 5:21 PM;NataliaDracheva;metrics.png;https://jira.hyperledger.org/secure/attachment/15786/metrics.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwxlj:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,,,,,,,,,,,"07/Sep/18 5:28 PM;NataliaDracheva;*Build version:*
indy-node: 1.6.598
indy-plenum: 1.6.538

*Test description:* 10 nyms from 1 client with a seed of a DID which is in a ledger and has no role for 2 hours.

*Preconditions:*
1. As a Trustee, add a new did without a role in a ledger.

*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
1 machine X python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym -s=000000000000000000000000Trustee3
{code}

*Expected results:* 
72 k rejects (10 txns/sec * 120 min * 60 sec)

*Actual results:* 
13630 rejects
View changes: 0

*Additional info* 
\\iserver\exchange\Evernym\INDY-1593\Without dynamic quotas

*Metrics:*
 !metrics.png|thumbnail! ;;;","13/Sep/18 11:45 PM;NataliaDracheva;An investigation is done, the problems are described in INDY-1700.;;;",,,,,,,,,,,,,,,,,,,,,,,
"As a developer, I need a way to find a number of dids with role ""TRUSTEE""",INDY-1594,32840,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,KitHat,KitHat,15/Aug/18 6:45 PM,28/Sep/18 9:12 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,For some transactions (especially for transactions from plugins) we have a voting process. It requires us to have some percentage of users with specific role to sign this transaction. But we don't know how many dids with such role are there in the ledger.,,,,,,,,,,,,,,,,,INDY-1731,,,,,,,,,,,,,,,,,INDY-1704,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzmt3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,KitHat,lovesh,,,,,,,,,,"15/Aug/18 7:02 PM;lovesh;A simple solution is to iterate over keys of `IdrCache` (which is a key value store) and select all roles that are trustees; this check should be done at last step in dynamic validation (once we know each sender is a trustee). This is expensive but since such voting txns are rare it is not very bad.
A better solution is to create a new key in `IdrCache` that keeps a count (or list if needed) of all trustee roles. This is done in `onBatchCommitted` of `IdrCache` where we check that if a NYM is being given a trustee role or an existing trustee role is being revoked (the existing trustee dids can be kept in memory, initialised on node start and updated accordingly). There has to be a migration script written for existing ledgers that creates such a key with correct value. A caveat of adding this logic `onBatchCommitted` is that any uncommitted txns that are adding/removing trustee role will not be considered. A more thorough solution would be to add more logic in `set`, `get` and `batchRejected` too.;;;","16/Aug/18 5:51 PM;ashcherbakov;I think we can go with the option to have a new key in `idr_cache` contains the latest (committed) number of trustees and updated on `onBatchCommitted`.
I think that for the first version it may be enough to take into account only committed Trustee txns. In any case we can not expect any signatures and voting from the trustees being added, so it can be fine to not take them into account when checking multi-sig within a 3PC batch.
We can add a feature on re-calculating the number of Trustees for uncommitted state later.;;;",,,,,,,,,,,,,,,,,,,,,,,
Node can't catch up large ledger,INDY-1595,32841,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,15/Aug/18 6:48 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"*Environment:*
 indy-node 1.5.551

*Steps to Reproduce:*
 1. Restore pool from backup with large ledger (1,340,000 txns), keep one node clear (Node24 in attached logs).
 2. Start the pool exclude clear node.
 3. Initialize clear node and start it.
 => Domain ledger of clear node is 30txns.
 4. Wait several hours.

*Actual Results:*
 Clear node still has 30 txns in domain ledger, following messages appear in logs of the rest nodes:
{code:java}
2018-08-15 09:29:56,416|INFO|ledger_manager.py|Node1 received catchup request: CATCHUP_REQ{'seqNoStart': 1061257, 'ledgerId': 1, 'catchupTill': 1340508, 'seqNoEnd': 1117110} from Node24
2018-08-15 09:29:56,416|INFO|ledger_manager.py|Node1 generating consistency proof: 1117110 from 1340508
2018-08-15 09:30:12,778|WARNING|prepare_batch.py|Too many split steps were done 9. Batches were not created
2018-08-15 09:30:12,779|WARNING|prepare_batch.py|Too many split steps were done 9. Batches were not created
...... several hundreds of the same messages ......
2018-08-15 09:30:12,812|WARNING|prepare_batch.py|Too many split steps were done 9. Batches were not created
2018-08-15 09:30:12,812|WARNING|prepare_batch.py|Too many split steps were done 9. Batches were not created
2018-08-15 09:30:12,812|WARNING|prepare_batch.py|Too many split steps were done 9. Batches were not created
2018-08-15 09:30:12,812|ERROR|batched.py|Node1 cannot create batch(es) for Node24{code}
*Expected Results:*
 Clear node should complete catch up.

*Additional Information:*
* Pool restore was performed not quite right. Nodes 15, 18 and 25 were not started with the rest ones. 15 and 18 were started later (after steps to reproduce), 25 was not started at all.
* The same behavior for 730K txns in the ledger, when all nodes were restored correctly.

Logs: s3://qanodelogs/indy-1595
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1595/ /home/ev/logs/indy-1595/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1657,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwwmf:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ozheregelya,,,,,,,,,,,"21/Aug/18 9:44 PM;anikitinDSR;Reason:
 * need to change batch splitting logic

Changes:
 * For now, batch is spliting by max size.

Versions:
 * indy-node: 1.6.571

Steps to validate:
 * For now, clear node should complete a big catchup;;;","28/Aug/18 2:51 AM;ozheregelya;In case of pool with 8Gb RAM (standard QA pool) catch up of 600K txns still don't work. But now the reason is that the node fails with OOM. Re-run on node with 32Gb RAM.;;;","28/Aug/18 7:31 PM;ozheregelya;*Environment:*
 indy-node 1.6.578
 AWS pool of 24 nodes (8Gb) + 1 node (32Gb)

*Steps to Validate:*
 1. Set up the pool of 25 nodes and run load on it to get big ledger (600K txns).
 2. Stop and demote one of the nodes.
 3. Change instance type of demoted node to get 32Gb RAM.
 4. Re-promote node and start it.

*Actual Results:*
 Node completed catch up of 600K txns in 6 hours.

*Additional Information:*
 Ticket for catch-up improvements: INDY-1242.
Ticket for problems after catch up: INDY-1657.;;;",,,,,,,,,,,,,,,,,,,,,,
I want to limit the number of concurrent connections to indy_node in order to guard against DOS attacks,INDY-1596,32851,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,danielhardman,smithbk,smithbk,15/Aug/18 11:54 PM,15/Aug/18 11:59 PM,28/Oct/23 2:47 AM,,,,libsovrin,,,0,,,,"I want a configuration option to limit the number of concurrent connections which may be made to my indy_node process.  This is in order to guard against DOS attacks.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzmuv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),MichaelWang,smithbk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Automate running of Chaos experiments on a regular basis,INDY-1597,32857,,Task,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ckochenower,ckochenower,ckochenower,16/Aug/18 2:12 AM,28/Sep/18 7:58 AM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"Setup Chaos experiments to run on a regular basis. The frequency and which pools are to be experimented on is still to be determined.

Chaos experiments currently exist in the [chaosindy repo|https://github.com/evernym/chaosindy], but will soon move to the ""indy-test-automation"" repo.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzmvz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"30/Aug/18 7:29 AM;ckochenower;Any objections to this approach as a solution for INDY-1597 - Automate running of Chaos experiments on a regular basis? I'm not exactly sure this approach is possible, but I believe it is. [~krw910] and I discussed it and he says he likes it. I will proceed with a POC using a Jenkins Server hosted in the evernym-dev AWS account. If this new downstream jenkins job needs to be hosted elsewhere, I will need help moving it elsewhere. (edited)
https://drive.google.com/file/d/1tsYGnp-g6IJGxUKbIxX3Akpa_zEH335P/view?usp=sharing;;;","30/Aug/18 7:51 AM;ckochenower;The proposed approach will need the following added to the chaosindy module:

clean_pool
nscapture_pool
record_on
record_off

All other operations/primitives needed appear to already exist in the chaosindy module.

Perhaps the following or something similar would be needed as well:

upload_to_s3
notify_of_failure;;;","05/Sep/18 4:46 AM;ckochenower;Received feedback from Devin, Andrey K., Alex, and Sergey K. A ""Simplified Chaos Experiment Automation"" design is located here:

https://drive.google.com/file/d/12QM1-mq3Jf4s6lipJ7vCUWPu_IsHcbUb/view?usp=sharing

Please provide comments/feedback. I will get started on implementation assuming this draft is a good starting point.;;;","28/Sep/18 7:58 AM;ckochenower;A run.py script now exists in the chaosindy repo that allows CI/CD, QA, Dev workflows to run one or more chaos experiments. There are a number of TODOs documented in the README.md file that will make the run.py more useful. A pool reset (delete the ledger - start with a clean slate) is not yet available, but all experiments except the force-catchup-blocked-port experiment run back to back (one after the other) with no problems without ""resetting"" the pool between experiments.

{code}commit b241a8d3d7d26be9d6f02395d50cd31e5082011d
Author: Corin Kochenower <corin.kochenower@evernym.com>
Date:   Thu Sep 27 08:52:51 2018 -0600

    Instrumentation required to automate experiment execution

      - Add run.py script for batching/executing any/all experiments
      - Refactoring required due to changes in perf_processes load script

    Signed-off-by: Corin Kochenower <corin.kochenower@evernym.com>{code}

The following...

{code} ./run.py pool1 --exclude ""force-catchup-service-down,view-change-subversion""{code}

...produces the following output in the ""report"" file:

{code}(chaostoolkit) ubuntu@KellyStableClientVirgina:/tmp/None-2018-09-27T14:18:09.21690710oqluf7$ tail -f report
force-catchup-blocked-port: succeeded
demote-replica: succeeded
replica-selection: succeeded
force-view-change: succeeded
demote-promote: succeeded
consensus-recovery: succeeded
shrink-pool: succeeded{code};;;",,,,,,,,,,,,,,,,,,,,,
Improve the Indy Chaos documentation/README,INDY-1598,32858,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ckochenower,ckochenower,ckochenower,16/Aug/18 2:24 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,test-automation,,,0,,,,"Most of the contents in the following slides need to be condensed into one or more READMEs in the chaosindy (soon to be part of indy-test-automation) repo.

https://docs.google.com/presentation/d/1wSLUF9E4T8ekKMfU3tOEENj2dZQt42uDjTheSsUAJTI/edit?usp=sharing

Documentation (including recorded demos) need to be available for engineers responsible for or interested in contributing to Indy Chaos experiments.

As part of this effort, convert comments to Python Docstrings where appropriate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzmw7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use average latency for all clients when check the latency for View Change triggering,INDY-1599,32868,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,16/Aug/18 7:58 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"As of now, Monitor tracks latency for every client independently. This may make it more sensitive for triggering View Change, but on the other hand it means that
 * It means that we have a to have a huge map (if we have 1,000,000 clients, then it will take about 1GB of memory).
 * We clean the map only after view change
 * We are too sensitive to spikes for 1 client only.

Let's keep track of the average (for all clients) latency only.

*Acceptance criteria:*
 * Move monitoring of latency into a strategy
 * Move existing calculation of latency per client into a strategy
 * Create a new strategy for calculation of average latency for all clients
 ** As we use moving average, we can just take into account latencies for all reqs from all clients within the window
 * Use the new strategy by default
 * Write unit tests

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwrr:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,VladimirWork,,,,,,,,,,"11/Sep/18 12:56 AM;sergey-shilov;*Problem state / reason:*

As of now, Monitor tracks latency for every client independently. This may make it more sensitive for triggering View Change, but on the other hand it means that
 * It means that we have a to have a huge map (if we have 1,000,000 clients, then it will take about 1GB of memory).
 * We clean the map only after view change
 * We are too sensitive to spikes for 1 client only.

*Changes:*
 * Moved monitoring of latency into a strategy
 * Moved existing calculation of latency per client into a strategy
 * Created a new strategy for calculation of average latency for all clients.
 * Set the new strategy as a default
 * Wrote unit tests

*Committed into:*

        https://github.com/hyperledger/indy-plenum/pull/914
        https://github.com/hyperledger/indy-node/pull/939
        indy-node 1.6.600-master

*Risk factors:*

Less sensitive algorithm for master degradation detection.

*Risk:*

Low

 ;;;","14/Sep/18 10:39 PM;VladimirWork;Build Info:
indy-node 1.6.600

Step to Validate:
1. Run load test from 5 to 25 writing txns per second.
2. Check logs and validator-info for VCs by 25 reason (master degraded).

Actual Results:
There are no VCs by 25 reason under 20 txns/sec written.;;;",,,,,,,,,,,,,,,,,,,,,,,
"As an Indy user, I need to be able to get all Schemas for the given query criterias",INDY-1600,32870,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,16/Aug/18 8:14 PM,09/Oct/19 5:08 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"As of now, we have GET_SCHEMA call that can return a Schema for the given DID-name-version.

*Acceptance criteria:*
 * Create a new Request (LIST_SCHEMA) which can

 ** return all Schemas created by the given DID
 ** return all Schemas created by the given DID with the given name
 * The result must have a State Proof
 * See https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#list_schema",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2242,,,No,,Unset,No,,,"1|hzwy5r:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"As an Indy user, I need to be able to get all Claim Defs for the given query criterias",INDY-1601,32872,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,16/Aug/18 8:18 PM,09/Oct/19 5:08 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"As of now, we have GET_CLAIM_DEF call that can return a CLAIM_DEF for the given DID-schemaSeqNo-type-tag

*Acceptance criteria:*
 * Create a new Request (LIST_CLAIM_DEF) which can

 ** return all Claim Defs created by the given DID
 ** return all Claim Defs created by the given DID for the given type
 ** return all Claim Defs created by the given DID for the given type and the given Schema (Schema's seq no)
 * The result must have a State Proof
 * [See https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#list_schema|https://github.com/hyperledger/indy-node/blob/master/design/anoncreds.md#list_cred_def]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2242,,,No,,Unset,No,,,"1|hzwy5z:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Find out optimal message quotas ,INDY-1602,32875,,Task,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,sergey.khoroshavin,ashcherbakov,ashcherbakov,16/Aug/18 9:04 PM,13/Feb/19 10:02 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"*Acceptance criteria:*
 * Do more load testing and find out optimal message quotas after analysing metrics:
 ** NODE_TO_NODE_STACK_QUOTA
 ** CLIENT_TO_NODE_STACK_QUOTA
 ** NODE_TO_NODE_STACK_SIZE
 ** CLIENT_TO_NODE_STACK_SIZE",,,,,,,,,,INDY-1681,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/18 6:48 PM;NataliaDracheva;1602.png;https://jira.hyperledger.org/secure/attachment/15783/1602.png","07/Sep/18 9:30 PM;NataliaDracheva;1602_scenario1.png;https://jira.hyperledger.org/secure/attachment/15788/1602_scenario1.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwx4f:2rzl8",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,NataliaDracheva,ozheregelya,sergey.khoroshavin,,,,,,,,"01/Sep/18 7:59 AM;sergey.khoroshavin;*Problem reason*
Initially this issue was about finding some static message quotas, however after analyzing a number of metrics from load tests with different profiles it became apparent that static message quotas won't fit - when set too high they won't have desired effect, and when set too low they can kill performance in some situations. Next idea was to throttle client messages based on amount of node messages. However more metrics analysis showed that in some situations it won't help either. So, finally it was decided to try throttling client messages based on number of received, but still not ordered requests. Now it's implemented as a hard switch (receive or not) based on request queue size, but in future smarter strategies could be implemented, like smooth throttling client stack based on request queue size and smooth throttling of node stack based on time between looper runs. Code is written so that such strategies are easy implement.

*Changes*
New parameters added to config:
{code}
ENABLE_DYNAMIC_QUOTAS = False
MAX_REQUEST_QUEUE_SIZE = 1000
{code}
When _ENABLE_DYNAMIC_QUOTAS_ is set to True throttling of client messages is enabled based on unordered requests queue size. _MAX_REQUEST_QUEUE_SIZE_ defines queue length threshold after which client messages are throttled.

*Versions*
indy-node >= 1.6.591
indy-plenum >= 1.6.534

*PR*
 https://github.com/hyperledger/indy-plenum/pull/905

*Covered by tests*
https://github.com/hyperledger/indy-plenum/blob/a58ad6ea644a8bbdc5a22468be68d3ff489f4444/plenum/test/node/test_quota_control.py

*Risk*
Medium

*Risk factors*
Performance degradation might be observed when dynamic quotas are enabled, although this is unlikely.

*Recommendations for QA*
Run load test with writes of 40 NYMs per second for at least 30 minutes with enabled metrics and dynamic quota control. Observed ordering performance should be at least 20, probably 25 NYMs per second. Also metrics should show that average latency will stabilize after some time. If everything looks good it might be useful to repeat test with same parameters and _ZMQ_CLIENT_QUEUE_SIZE_ set to 10. If there is still no performance degradation it's recommended to run full-blown DDoS test (like in INDY-1388) to see if pool is stable.;;;","06/Sep/18 6:57 PM;NataliaDracheva;*Build Info:*

Indy-node 1.6.597
Indy-plenum 1.6.538


*OS/Platform:* Ubuntu 16.04

*Component:*
Indy-node

*Reason for Reopen:*
- avg throughput = 18 txns/sec;
- latency is not stable;
- 5 nodes became unreacheable.

*Steps to Reproduce:*
1. Ran ""perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym"" from 4 AWS instances for 90 minutes.

*Actual Results:*
315 success nyms, 5303 nack.

*Expected Results:*
54000 new nyms records in the domain ledger.

*Additional info:*
Logs and metrics are here: \\iserver\exchange\Evernym\INDY-1602\40_nyms_90_min
 !1602.png|thumbnail! ;;;","07/Sep/18 9:46 PM;NataliaDracheva;*Scenario 1.*
 *Build version:*
 indy-node: 1.6.598
 indy-plenum: 1.6.538
 *Test description:* 4 machines send 10 nym/sec per each machine for 2h 10 min
 *Steps to Reproduce:*
 1. Run perf_processes.py from 4 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym{code}

 *Expected results:* ordering: ~20-25, stable latency
 *Actual results:* ordering: ~21.24, stable latency
 *Additional info:* [https://s3.console.aws.amazon.com/s3/buckets/qanodelogs/indy-1602/?region=us-east-1]
 !1602_scenario1.png|thumbnail!;;;","07/Sep/18 10:10 PM;NataliaDracheva;*Scenario 2.*
*Build version:*
indy-node: 1.6.598
indy-plenum: 1.6.538
*Test description:* 4 machines send 10 nym/sec per each machine.
*Preconditions:* add ZMQ_CLIENT_QUEUE_SIZE = 10 to node config.
*Steps to Reproduce:*
1. Run perf_processes.py from 4 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym
{code}
*Expected results:* ordering: ~20-25, stable latency
*Actual results:* Load script crashed and returned command line.
{code:java}
Exception ignored in: <bound method Task.__del__ of <Task finished coro=<LoadClient.run_test() done, defined at perf_processes.py:1084> exception=AttributeError(""'NoneType' object has no attribute 'encode'"",)>>
Traceback (most recent call last):
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 93, in __del__
  File ""/usr/lib/python3.5/asyncio/futures.py"", line 215, in __del__
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1177, in call_exception_handler
  File ""/usr/lib/python3.5/logging/__init__.py"", line 1308, in error
  File ""/usr/lib/python3.5/logging/__init__.py"", line 1415, in _log
  File ""/usr/lib/python3.5/logging/__init__.py"", line 1425, in handle
  File ""/usr/lib/python3.5/logging/__init__.py"", line 1495, in callHandlers
  File ""/usr/lib/python3.5/logging/__init__.py"", line 855, in handle
  File ""/usr/lib/python3.5/logging/__init__.py"", line 986, in emit
  File ""/usr/lib/python3.5/logging/__init__.py"", line 908, in handleError
  File ""/usr/lib/python3.5/traceback.py"", line 100, in print_exception
  File ""/usr/lib/python3.5/traceback.py"", line 474, in __init__
  File ""/usr/lib/python3.5/traceback.py"", line 358, in extract
  File ""/usr/lib/python3.5/traceback.py"", line 282, in line
  File ""/usr/lib/python3.5/linecache.py"", line 16, in getline
  File ""/usr/lib/python3.5/linecache.py"", line 47, in getlines
  File ""/usr/lib/python3.5/linecache.py"", line 136, in updatecache
  File ""/usr/lib/python3.5/tokenize.py"", line 458, in open
  File ""/usr/lib/python3.5/encodings/__init__.py"", line 98, in search_function
  File ""<frozen importlib._bootstrap>"", line 969, in _find_and_load
  File ""<frozen importlib._bootstrap>"", line 954, in _find_and_load_unlocked
  File ""<frozen importlib._bootstrap>"", line 887, in _find_spec
TypeError: 'NoneType' object is not iterable
--- Logging error ---
{code};;;","10/Sep/18 9:18 PM;Derashe;Load tests satisfied requirements, that we've set for this ticket. But still, we can see OOM problem. We diagnose that memory graph very correlated with handling request count, which is too big. Dynamic quotas didn't work, because there are a few replicas who cannot order txns. This should be fixed in scope of https://jira.hyperledger.org/browse/INDY-1680. This ticket should be retested with these changes.;;;","11/Sep/18 8:22 PM;ozheregelya;UPD: It should be fixed in INDY-1681, not INDY-1680.;;;","18/Sep/18 10:24 PM;NataliaDracheva;*Scenario 2:*
*Build version:*
indy-node: 1.6.603
indy-plenum: 1.6.539
*Test description:* 4 machines send 10 nym/sec per each machine.
*Preconditions:* turn of dynamic quotas + set MAX_REQUEST_QUEUE_SIZE  = 1000 + add ZMQ_CLIENT_QUEUE_SIZE = 10 to node config.
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym
{code}
*Expected results:* ordering: ~20-25, stable latency
*Actual results:* ordering: 17 nyms/sec, unstable latency (x)
*Additional info:* Logs: ev@evernymr33:~/logs/1602_dq_zmq.zip;;;","20/Sep/18 6:17 PM;NataliaDracheva;*Scenario 1:*
*Build version:*
*indy-node: 1.6.603*
*indy-plenum: 1.6.539*
*Test description:* 4 machines send 10 nym/sec per each machine.
*Preconditions:* turn of dynamic quotas + set MAX_REQUEST_QUEUE_SIZE = 1000 to node config.
*Steps to Reproduce:*
1. Run perf_processes.py from 4 AWS agent with following parameters:

{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym
{code}

*Expected results:* ordering: ~20-25, stable latency
*Actual results:* ordering: 17 nyms/sec, stable latency 
Additional info: Logs: ev@evernymr33:~/logs/indy-1602/1602_scen1_603_754.zip;;;","22/Sep/18 12:36 AM;sergey.khoroshavin;{quote}
Scenario 2:
Actual results: ordering: 17 nyms/sec, unstable latency 
Additional info: Logs: ev@evernymr33:~/logs/1602_dq_zmq.zip
{quote}
According to metrics actual throughput during steady state is a bit below 20 nyms/sec

{quote}
Scenario 1:
Actual results: ordering: 17 nyms/sec, stable latency 
Additional info: Logs: ev@evernymr33:~/logs/indy-1602/1602_scen1_603_754.zip
{quote}
According to metrics actual throughput during steady state is a bit above 20 nyms/sec

Logs and metrics analysis of last two cases showed the following:
- reducing ZMQ client queue size have almost no effect on performance and memory consumption
- there is possible performance degradation due to dynamic quotas (without them in similar scenario we reached up to 25 nyms/sec)
- request queue doesn't grow indefinitely under load above pool capacity, although it still grows much higher than required threshold
- it turned out that throttling client stack is not enough - requests get to nodes through PROPAGATEs regardless of current incoming request queue size
- sometimes even when request queue size is above throshold client stack doesn't seem to be throttled
- there is suspicion than non-finalized requests might accumulate in input queue
- memory consumption continues to grow almost linearly despite request queue length not growing anymore (this finding might be worth mentioning in INDY-1688 as well)

Plan of attack:
- add more metrics to see internal queues size (like number of nonfinalized requests, requests tracked by monitor, etc) to further track memory issues
- add some logs into quota controller
- research possibility of throttling PROPAGATEs (in a different issue)
;;;","24/Sep/18 7:12 PM;sergey.khoroshavin;Additional testing is recommended.
*Version*
indy-node: 1.6.610
indy-plenum: 1.6.545
*Load*
40 NYMs per second
*Scenarios*
1) run load test of 40 NYMs per second for 30 minutes against AWS pool with default config
2) run load test of 40 NYMs per second for 90 minutes from same clients against same AWS pool with _ENABLE_DYNAMIC_QUOTAS = True_

This test is needed to:
* find out if enabling dynamic quotas really degrade performance
* find out relations between different internal queue sizes
* check whether non-finalized requests accumulate or not
* check whether monitor internal request queue grow unbounded leading to memory leak
;;;","25/Sep/18 1:45 AM;sergey.khoroshavin;Final conclusions (after a few more experiments):
* there is no performance degradation due to dynamic quotas or reduced ZMQ client queue size, so they can be enabled by default
* there is no visible effect on memory consumption with or without these options
* for some yet unknown reason node request queue is not affected by dynamic quotas (it's big, but it doesn't grow unbounded)
* for some yet unknown reason amount requests in request time tracker and node request queue can be totally different
* throttling client stack is not enough - PROPAGATEs should be throttled as well;;;",,,,,,,,,,,,,,
Validator Info may hang for a couple of minutes,INDY-1603,32876,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,16/Aug/18 9:23 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,validator-info,,,0,,,,"As was found in INDY-1571, validator info may hang for a couple of minutes which will make the whole node unavailable.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1571,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwpr:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,zhigunenko.dsr,,,,,,,,,,"22/Aug/18 10:07 PM;dsurnin;Some most time consuming parameters were removed from default dumping
such as
Memory profiler
Software info
Extractions from journalctl
Node's data folder size
Note however all these parameters are available via CLI get-validator-info, but validator_info script will not provide it.

https://github.com/hyperledger/indy-node/pull/908;;;","24/Aug/18 6:04 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.576

*Actual Results:*
_validator-info -v_ doesn't contain mentioned parameters
_ledger get-validator-info_ contains mentioned parameters;;;",,,,,,,,,,,,,,,,,,,,,,,
Intermittent failure: test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection,INDY-1604,32886,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,sergey.khoroshavin,sergey.khoroshavin,16/Aug/18 11:55 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/18 11:57 PM;sergey.khoroshavin;test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection.txt.xz;https://jira.hyperledger.org/secure/attachment/15596/test_disconnected_node_with_lagged_view_pulls_up_its_view_on_reconnection.txt.xz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzzmzj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,sergey.khoroshavin,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Load script became huge and and should be split into several files,INDY-1605,33001,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,dsurnin,dsurnin,17/Aug/18 5:44 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"Old load scripts are not supported now and should be deleted.

New load script should be split into several files.

*Acceptance criteria:*
 * Split the script into multiple parts
 * Cover with tests
 * Create a pypi package
 * Remove old (deprecated) load scripts",,,,,,,,,,,,,,,,,INDY-1665,INDY-1699,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwwp3:",,,,Unset,Unset,EV 18.18 Service Pack 2,EV 18.19,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,NataliaDracheva,ozheregelya,VladimirWork,,,,,,,,,"17/Aug/18 5:47 PM;dsurnin;Probably load script could be converted into a separate python package;;;","13/Sep/18 9:52 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/940;;;","18/Sep/18 7:05 PM;NataliaDracheva;*Scenario 1:*
*Build version:*
indy-node: 1.6.603
indy-plenum: 1.6.539
*Test description:* Test sending domain transactions without revocations by the new load script.
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -n=1 -k=""{\""schema\"": 1, \""cred_def\"": 1, \""nym\"": 1, \""attrib\"": 1, \""get_nym\"": 1, \""get_schema\"": 1, \""get_cred_def\"": 1, \""get_attrib\"": 2}"" --load_time=3600  -d=/home/me/Documents/logs -o=/home/me/Documents/logs/load_script.csv -c=1 -m=t -y=one --load_rate=10
{code}
*Expected results:* ~36k transactions sent and successfully handled
*Actual results:* ~36k transactions sent and successfully handled (/)
*Additional info:* logs: ev@evernymr33:~/logs/1605_domain_not_revocation.zip

*Scenario 2:*
*Build version:*
indy-node: 1.6.603
indy-plenum: 1.6.539
*Test description:* Test sending revocations by the new load script.
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -n=1 --load_time=600  -d=/home/me/Documents/logs -o=/home/me/Documents/logs/load_script.csv -c=10 -m=t -y=all --load_rate=0.5 -k=""{\""revoc_reg_def\"": 1, \""revoc_reg_entry\"": 1,\""get_revoc_reg_def\"": 1, \""get_revoc_reg_delta\"": 1}"" -b=1
{code}
*Expected results:* ~3k transactions sent and successfully handled
*Actual results:* ~1.5k transactions sent and successfully handled. As per discussion with developers, this is expected due to provided parameters. (/)
*Additional info:* logs: ev@evernymr33:~/logs/1605_revocations.zip;;;","18/Sep/18 7:05 PM;NataliaDracheva;Need to test the new load script with payments and fees.;;;","19/Sep/18 3:23 AM;ozheregelya;*Environment:*
 indy-node 1.6.598
 libindy 1.6.5~750

*Case 3:*
*Steps to Validate:*
 1. Run load test with -k fees_nym.
 2. Run load test with -k fees_schema.

*Actual Results:*
 Load script works. (/)

*Case 4:*
*Steps to Validate:*
 1. Run load test with -k payment.

*Actual Results:*
 Load script works. (/);;;","19/Sep/18 3:24 AM;ozheregelya;Waiting for merging fix of case
 * Create a pypi package;;;","21/Sep/18 11:38 PM;VladimirWork;Package can be installed successfully using `sudo pip3 install -e .` in ~/indy-node/scripts/performance/perf_load and script works. Pip package publishing will be done in scope of INDY-1699.;;;",,,,,,,,,,,,,,,,,,
Add tests for load script,INDY-1606,33002,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,dsurnin,dsurnin,dsurnin,17/Aug/18 5:46 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,Load script became huge and requires unit tests,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwxlr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,,,,,,,,,,,"30/Aug/18 10:13 PM;ashcherbakov;Needs to be done in the scope of INDY-1605;;;",,,,,,,,,,,,,,,,,,,,,,,,
Proof of stability under load,INDY-1607,33007,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,zhigunenko.dsr,zhigunenko.dsr,17/Aug/18 9:08 PM,08/Oct/19 9:03 PM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"We need to prove that it will be stable under conditions similar to production use.

*Acceptance Criteria*
Perform a test of an Indy network that has the following attributes:

* The ledger is pre-loaded with 1 million transactions
* Pool size is 25 nodes.
* 1K concurrent clients
* Over a 3 hour period induce a sustained throughput of 10 write transactions per second and 100 read transactions per second on average (from clients)
Write load is a mixture of:
** writing credentials schema (5%),
** writing credential definition (5%)
** revoke registry definition (5%)
** revoke registry update (5%)
** write DID to ledger (20%)
** write payment to ledger (45%)
** write attrib to ledger (15%)
Read load is a mixture of:
** read DID from ledger (45%)
** read credential schema (10%)
** read credential definition (10%)
** read revoke registry definition (10%)
** read revoke registry delta (10%)
** read attrib from ledger (10%)
** read payment balance from ledger (5%)
* Write response time should be less that 5 seconds (would also like a report of the average).
* Read response time should be less than 1 second (would also like a report of the average).
* Unless view change in progress, pool should write 10 txns/sec and read 100 txns/sec

*Main cases for research:*
1) mixed set (imitation of active production work) - 1000 clients / 10 writes / 100 reads
2) payments only (understanding of pluggable ledgers productivity) - 1000 clients / as much as possible writes (1 - 4 - 10) / as much as possible reads (up to 10 - 40 - 100)
3) not payment transactions (comparision with historical productivity data) - 1000 clients / 10 writes / 100 reads",,,,,,,,,,,,,,,,,,,,,INDY-1717,,,INDY-1002,INDY-307,,,,INDY-1343,INDY-2214,,,,INDY-1638,,,,,,,,,,,,,,,"27/Sep/18 8:21 PM;ozheregelya;Ext-26-09-18-mix.png;https://jira.hyperledger.org/secure/attachment/15882/Ext-26-09-18-mix.png","28/Sep/18 9:22 PM;ozheregelya;fees-node-13.png;https://jira.hyperledger.org/secure/attachment/15891/fees-node-13.png","28/Sep/18 9:22 PM;ozheregelya;no_fees-node-13.png;https://jira.hyperledger.org/secure/attachment/15892/no_fees-node-13.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-781,,,No,,Unset,No,,,"1|hzwwpj:",,,,Unset,Unset,EV 18.17 Service Pack,EV 18.18 Service Pack 2,EV 18.19,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),MichaelWang,ozheregelya,spivachuk,VladimirWork,zhigunenko.dsr,,,,,,,,"28/Aug/18 4:53 PM;zhigunenko.dsr;*Test run:* 1
*Environment:*
indy-node 1.6.71 (25 AWS nodes)
plugins 0.9.0+11.83

Plugin ledger size: 0k -> 40k
logLevel = INFO
STACK_COMPANION=0
*Goal:* understanding the maximum of payment ledger performance.
*Load:*
up to 250 processes of 
{code}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 1 -l 0.06000 -y one -k payment -o output &
{code}
*Results:*
Last ViewNo - {color:#008000}2{color}
Duration - {color:#008000}48+ hours{color}
Pool {color:#008000}in action{color}.
Transactions written: {color:#008000}40k+{color}
Throughput: write - up to {color:#008000}7 txns/sec{color}, but most of the time - less than 1 txns/sec.

*Additional Info:*
Load script generates new transactions only limited time, then closed. This is the reason of load decreasing.;;;","29/Aug/18 11:24 PM;zhigunenko.dsr;*Test run:* 2
*Environment:*
indy-node 1.6.71 (25 AWS nodes)
plugins 0.9.0+14.86

Domain ledger size: 159 -> 14841
Plugin ledger size: 370 -> 15198
logLevel = INFO
STACK_COMPANION=0
*Goal:* understanding pool behavior regarding ""txn+fee""
*Load:*
5 clients
{code:java}
python3.5 ~/indy-node/scripts/performance/perf_processes.py -g ~/indy-node/scripts/performance/pool_transactions_genesis -m t -n 1 -c 1 -l 0.50000 -y one -k ""{\""fees_nym\""}""
{code}
*Results:*
ViewChanges during load - {color:orange}10{color}
Duration - {color:#008000}3.5 hours{color}
Pool {color:#008000}in action{color}.
Transactions written: {color:#008000}14.5k+{color}
Throughput: write - up to {color:orange}2 txns/sec{color}

*Additional Info:*
Load script generates less transactions than pool can handle.;;;","31/Aug/18 7:35 PM;zhigunenko.dsr;*Test run:* 3
*Environment:*
indy-node 1.6.587 (25 AWS nodes)
plugins (master, custom build)

Domain ledger size: 40 -> 57
Plugin ledger size: 0 -> 19k+
logLevel = INFO
STACK_COMPANION=0
*Goal:* understanding the maximum of payment ledger performance.
*Load:*
5 clients
{code:java}
python3.5 ~/indy-node/scripts/performance/perf_processes.py -g ~/indy-node/scripts/performance/pool_transactions_genesis -m t -n 1 -c 1 -l 5.0000 -y one -k ""{\""payment\""}""
{code}
*Results:*
ViewChanges during load - {color:orange}10+{color}
Duration - {color:orange}30 min{color}
Pool {color:#008000}in action{color}.
Transactions written: {color:#008000}19k+{color}
Throughput: write - up to {color:green}12 txns/sec{color}, stable - {color:green}10txns/sec{color}

*Additional Info:*
There is no fix from INDY-1643;;;","06/Sep/18 5:48 PM;zhigunenko.dsr;*Load setup if fees enabled:*

||Transaction||Shares||Requests frequency||Total requests(3h)||
|cred_def|5|0.72|7776|
|revoc_reg_entry|5|0.72|7776|
|payment|20|2.86|31320|
|attrib|15|2.2|23760|
|schema+fee|5|0.72|7776|
|nym+fee|20|2.9|31320|
||Transaction||Shares||Requests frequency||Total requests(3h)||
|get_nym|45|45|486000|
|get_schema|10|10|108000|
|get_cred_def|10|10|108000|
|get_revoc_reg|10|10|108000|
|get_revoc_reg_delta|10|10|108000|
|get_attrib|10|10|108000|
|verify_payment|5|5|54000|

*Warning:*
""schema+fee"" must be launched as penultimate (at the only AWS instance)
""nym+fee"" must be launched as the latest (at the only AWS instance)

*How to work with table:*
_""Shares""_ is proportion from original criteria (INDY-1343) where payments were separated from domain transactions.  
If fee is enabled _""domain_txn+fee""_ means both _domain txn_ and corresponding _payment_.  
So if fee set for attrib you need to subtract corresponding shares from payment _(20 - 15 = 5)_.  
Next recalculate _""Requests frequency""_ (_-l_ for load script) according to the formula:  
_Frequency=Shares * (100 / SUM(Shares)) / 10_,  
there is _(100 / SUM(Shares))_ - percent distribution, and 10 is _""10 txns/sec""_  
_""Total requests(3h)""_ means how many transactions are expected during _3*60*60_ seconds.  
In order to avoid _""no generated requests""_ during test it needs to generate them before.  

*Example for 5 client instances:*
Each instance run 200 clients. Clients are distributed equally between transaction types. Each client prepares enough data for three-hour test. _""-b""_ is equal _""Total requests""_ divided by number of clients for corresponding transaction.
ALPHA:
{code}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k cred_def -c 50 -l 0.72 -b 156
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k revoc_reg_entry -c 50 -l 0.72 -b 156
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k ""{\""payment\"":{\""payment_addrs_count\"": 627,\""payment_method\"":\""null\"", \""plugin_lib\"": \""libnullpay.so\"", \""plugin_init_func\"":\""nullpay_init\""}}"" -c 50 -l 2.86
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k attrib -c 50 -l 2.2 -b 476
{code}

BRAVO:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k ""{\""fees_schema\"":{\""payment_addrs_count\"": 39,\""payment_method\"":\""null\"", \""plugin_lib\"": \""libnullpay.so\"", \""plugin_init_func\"":\""nullpay_init\""}}"" -c 200 -l 0.72
{code}

CHARLIE:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k ""{\""fees_nym\"":{\""payment_addrs_count\"": 157,\""payment_method\"":\""null\"", \""plugin_lib\"": \""libnullpay.so\"", \""plugin_init_func\"":\""nullpay_init\""}}"" -c 200 -l 2.9
{code}

DELTA:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k get_nym -c 100 -l 45 -b 4860 &
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k ""{\""verify_payment\"":{\""payment_addrs_count\"": 540,\""payment_method\"":\""null\"", \""plugin_lib\"": \""libnullpay.so\"", \""plugin_init_func\"":\""nullpay_init\""}}"" -c 100 -l 5 &
echo ""Fin""
{code}

ECHO:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k get_schema -c 40 -l 10 -b 2700
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k get_cred_def -c 40 -l 10 -b 2700
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k get_revoc_reg -c 40 -l 10 -b 2700
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k get_revoc_reg_delta -c 40 -l 10 -b 2700
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -y one -k get_attrib -c 40 -l 10 -b 2700
{code};;;","25/Sep/18 4:57 PM;VladimirWork;Testing should be continued after INDY-1665 confirmation testing.;;;","27/Sep/18 8:21 PM;ozheregelya;*Test run 4:* (Ext-26-09-18-mix)
 Environment:
 indy-node 1.6.610 + plugins
 libindy 1.6.6~750 + plugins
 load script 1.0.5

*Load from script:*
 2.5+2.5+2.5+2.5 txns/sec (started one-by-one).
{code:java}
perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext  ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 10 -b 1 -l 2.5
{code}
*Pool throughput:* 2.5 txns/sec during 3 hours run well. When the rest scripts started, View Change was happened and pool stopped ordering.
 *Txns written:* 122866 | 46315 (domain | sovtoken).

  !Ext-26-09-18-mix.png|thumbnail!
Logs and full metrics: s3://qanodelogs/INDY-1607-Ext-26-09-18-mix;;;","28/Sep/18 9:09 PM;spivachuk;*Investigation on Test Run 4:*
 Actually the pool continued ordering after the view had changed to view 1 (approximately at 21:13 on 09/26). But some nodes (*Node3, Node11, Node13, Node15, Node24, Node25*) were performing this view change for a long time, completed it later than other nodes (at 21:19 - 21:21) and *were not participating in ordering* after this. *The diagram* *attached* *to the previous comment is from Node13*, so the ordering on it stopped on the view change beginning at 21:13.;;;","28/Sep/18 9:43 PM;ozheregelya;*Environment:*
indy-node 1.6.615
 libindy 1.6.6~759
 load script 1.0.5

*Test run 5:* (Ext-27-09-18-mix-no-fees)
 *Load from script:* from 5 txns/sec to 10 txns/sec (smooth increasing of load, after that stable load with 10 txns/sec)
{code:java}
perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 5 -b 1 -l 1

perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 2 -b 1 -l 1

[3x]perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 2 -b 1 -l 1{code}
*Pool throughput:* pool processed 10 txns/sec during 6 hours, but note that there are less sovtoken txns than expected. That means that payment addresses stopped working by some reason in the load script and the load was without payment txns.
 *Txns written:* 501296 | 10020
 *!no_fees-node-13.png|thumbnail!*
 Logs and full metrics: s3://qanodelogs/INDY-1607-Ext-27-09-18-mix-no-fees

 

*Test run 6:* (Live-27-09-18-mix-fees)
 *Load from script:* from 5 txns/sec to 10 txns/sec (smooth increasing of load, after that stable load with 10 txns/sec)
{code:java}
perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 5 -b 1 -l 1

perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 2 -b 1 -l 1

[3x]perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 2 -b 1 -l 1{code}
*Pool throughput:* when the load become stable, pool processed 10 txns/sec during 2 hours. After that it failed with OOM and stopped ordering.
 *Txns written:* 11198 | 21163
 !fees-node-13.png|thumbnail!
 Logs and full metrics: s3://qanodelogs/INDY-1607-Live-27-09-18-mix-fees;;;","28/Sep/18 10:03 PM;ozheregelya;*Conclusion:*

Pool process load of 10 txns/sec with wanted mixture of txns with fees during 2 hours.

In both of cases (with fees and without fees) pool fails with OOM, so, this case may be re-tested with pool with 32Gb RAM in scope of INDY-1717 (according comment will be added to INDY-1717).

Logs for OOM issues will be added to INDY-1721 epic.

Problem from Test Run 4 will be moved to separate ticket.;;;",,,,,,,,,,,,,,,,
Some nodes can't write and catchup txns,INDY-1608,33009,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,VladimirWork,VladimirWork,17/Aug/18 9:51 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Build Info:
indy-node 1.6.565

Steps to Reproduce:
1. Install 25 nodes pool.
2. Disable VC `unsafe=set(['disable_view_change'])`.
3. Run 40 clients that send 1 NYM/sec to fill ledger with 4k txns.
4. Stop indy-node service on backup 1 primary (Node2).
5. Run 100 clients that send 1 NYM/sec load test.

Actual Results:
There are 4 nodes that don't write and don't catchup txns: 4, 12, 14, 15 (4239 txns in ledger).
Stopped Node2 has 4222 txns in ledger so it looks like bad nodes go forward the rest nodes at Step 3 (so the issue can be similar to INDY-1539 where we have >=f nodes that go forward).

Expected Results:
All nodes except stopped Node2 should have the same amount txns in ledger.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzn6n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"17/Aug/18 10:41 PM;VladimirWork;All logs/journalctls/validator-infos are in /home/ev/logs/1608.tar.gz.;;;","11/Oct/18 9:28 PM;ashcherbakov;INDY-1539 is DONE. Will be re-tested in the scope of the current load testing.;;;",,,,,,,,,,,,,,,,,,,,,,,
Chaos Experiment - Repeated Demotion and Promotion of Nodes,INDY-1609,33022,,Task,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,ckochenower,ckochenower,ckochenower,18/Aug/18 7:19 AM,23/Aug/18 11:52 PM,28/Oct/23 2:47 AM,,,,test-automation,,,0,,,,"It has been observed that a pool falls out of consensus when nodes are demoted and promoted numerous times w/o a view change, which implies the master is never demoted.

See comments on https://jira.hyperledger.org/browse/INDY-1513 for details. This issue was created to create a chaos experiment to produce an automated way of proving this behavior exists.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-903,INDY-1640,,,,,,,,,,,,,,"23/Aug/18 4:35 AM;ckochenower;Screen Shot 2018-08-22 at 1.34.39 PM.png;https://jira.hyperledger.org/secure/attachment/15735/Screen+Shot+2018-08-22+at+1.34.39+PM.png","23/Aug/18 2:16 AM;ckochenower;Screen Shot 2018-08-22 at 11.13.08 AM.png;https://jira.hyperledger.org/secure/attachment/15731/Screen+Shot+2018-08-22+at+11.13.08+AM.png","23/Aug/18 2:16 AM;ckochenower;Screen Shot 2018-08-22 at 11.13.20 AM.png;https://jira.hyperledger.org/secure/attachment/15730/Screen+Shot+2018-08-22+at+11.13.20+AM.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1505,,,No,,Unset,No,,,"1|hzzn9j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ckochenower,,,,,,,,,,,,"22/Aug/18 8:51 AM;ckochenower;Note to self:

Running the following command will randomly (\-o 3) demote and promote (timeout after 120 seconds of trying to change the node's services to blank) one node (\-d 1) at a time. However, the primary will be excluded (\-\-include\-primary NO) when the random node is selected. Doing so prevents a view change from being triggered.

{code}./scripts/run-demote-promote --include-primary NO --include-backup-primaries YES --include-other-nodes YES -d 1 -s 120 -o 3 -e 1000 2>&1 > /home/ubuntu/run-demote-promote.out{code};;;","23/Aug/18 2:14 AM;ckochenower;I ran the following script in a detached (screen) terminal last night:;;;","23/Aug/18 2:14 AM;ckochenower;I ran the following script in a detached (screen) terminal last night and the pool fell out of consensus in 6 iterations of the experiment and remained out of consensus for over 17 hours:

{code}
./scripts/run-demote-promote --include-primary NO --include-backup-primaries YES --include-other-nodes YES -d 1 -s 120 -o 3 -e 1000 2>&1 > /home/ubuntu/run-demote-promote.out
{code}

 !Screen Shot 2018-08-22 at 11.13.08 AM.png|thumbnail! 

 !Screen Shot 2018-08-22 at 11.13.20 AM.png|thumbnail! 

;;;","23/Aug/18 4:36 AM;ckochenower;I ran the aforementioned run-demote-promote script a second time with the same parameters and the pool fell out of consensus in 8 iterations of the experiment.

 !Screen Shot 2018-08-22 at 1.34.39 PM.png|thumbnail! 

I will create a ticket (INDY-1640) and attach node logs, the experiment log, and nscapture archives (minutes of recording vs hours).
;;;",,,,,,,,,,,,,,,,,,,,,
Logs aren't archiving on default config with loglevel=0,INDY-1612,33059,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,zhigunenko.dsr,zhigunenko.dsr,21/Aug/18 4:09 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.78,,,,0,,,,"*Steps to Reproduce:*
1. Run load test 1000 clients /10 writes / 100 reads

indy_config.py
{code:java}
#Current network
# Disable stdout logging
enableStdOutLogging = False
# Directory to store ledger.
LEDGER_DIR = '/var/lib/indy'
# Directory to store logs.
LOG_DIR = '/var/log/indy'
# Directory to store keys.
KEYS_DIR = '/var/lib/indy'
# Directory to store genesis transactions files.
GENESIS_DIR = '/var/lib/indy'
# Directory to store backups.
BACKUP_DIR = '/var/lib/indy/backup'
# Directory to store plugins.
PLUGINS_DIR = '/var/lib/indy/plugins'
# Directory to store node info.
NODE_INFO_DIR = '/var/lib/indy'
NETWORK_NAME = 'sandbox'
ENABLED_PLUGINS=[]
ENABLED_PLUGINS.append('')
ENABLED_PLUGINS.append('')
CLIENT_TO_NODE_STACK_SIZE = 20 * 1024 * 1024 * 1024
NODE_TO_NODE_STACK_SIZE = 20 * 1024 * 1024 * 1024
METRICS_COLLECTOR_TYPE ='kv'
logLevel = 0
{code}

*Actual Results:*
Logs aren't archived and and filled all disk space",indy-node 1.6.565,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzngv:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,zhigunenko.dsr,,,,,,,,,,,"22/Aug/18 9:13 PM;sergey.khoroshavin;It seems like it was because of crashes during rotation due to insufficient RAM. Why node ate so much RAM is a different question, but AFAIK we already have issues about potential memory leaks, so this is out of scope of for this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Research Tendermint protocol,INDY-1613,33062,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,21/Aug/18 6:31 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Find out if Tendermint protocol looks good for Indy and is better than RBFT
Timebox the effort to 2 engineering days.

Acceptance Criteria
* Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community. Discussion should include:
** Pros and Cons
** Characteristics of current production implementations of the protocol
** Rough estimate of work required to bring implement Tendermint in Plenum
* A recommendation on whether to research more or kill the proposal
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwxmf:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"04/Oct/18 11:16 PM;ashcherbakov;* *Description:*

 * Very similar to PBFT with a new leader in every round
 * no explicit view change, leader is changed every round
 * uses locking to simplify conflict resolution


 * Used as consensus engine in multi-blockchain platform Cosmos

 * *Pros:*

 * The protocol has formal verification proofs
 * Has multiple implementations
 * The main implementation is pretty advanced with many tricks that greatly helps with performance and stability (more on this in next section)
 * The protocol has been tested with Jepsen, probably the most comprehensive and popular toolkit used to test distributed systems
 * A new leader every round, so stewards can be awarded tokens based on number of requests they put into ordering


 * *Cons:*
 * Only 1 batch in flight, so potentially worse performance

 * *Links:*

 * [https://allquantor.at/blockchainbib/pdf/buchman2016tendermint.pdf]

 

More details in [https://docs.google.com/presentation/d/1WCwyFMkKJFjaUGlysp49Pitjcv6TIayEMKzCaKzUf_g/edit#slide=id.g4245334205_0_989]

The protocol looks good, and can be considered as one of the main candidates if we write Plenum from scratch.
As for applying it to existing code base, this may be more tricky.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Research Tendermint implementation,INDY-1614,33064,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Aug/18 7:33 PM,05/Nov/19 4:33 AM,28/Oct/23 2:47 AM,,,2.0,,,,0,,,,"[https://github.com/tendermint/tendermint]

We need to find out how stable Tendermint implementation is .
 Timebox the effort to 4 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** Characteristics of current deployments in production usage
 ** Current project roadmap
 ** Health of the open source community / level of investment
 ** Rough estimate of work required to bring Tendermint to the same level as Plenum
 ** Rough understanding of performance
 ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Tendermint
 * A recommendation on whether to research more or kill the proposal
 * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

 

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",,,,,,,,,,,,,,,,,,,,,INDY-1617,INDY-2279,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwx4f:2rzma",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Evaluate alternatives to RBFT,INDY-1615,33065,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,21/Aug/18 7:35 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Consider other BFT-based algorithms which can be better than RBFT.
Timebox the effort to 3 engineering days.

*Acceptance Criteria*
* Brief summary of findings evaluating the top alternatives to RBFT for review by Richard, Nathan, Daniel, Jason, and the broader community.
* Each alternative should cover
** Characteristics of current implementations of the protocol in production usage
** Pros and Cons
** Rough estimate of work required
* Current recommendations (given the limited research done to date)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwxmn:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"04/Oct/18 11:11 PM;ashcherbakov;The results are in [https://docs.google.com/presentation/d/1WCwyFMkKJFjaUGlysp49Pitjcv6TIayEMKzCaKzUf_g/edit#slide=id.p]

 *Problems with RBFT:*
 * Very chatty protocol
 * Propagation phase is a bottleneck
 * Requires more resources than other protocols (CPU, Memory)
 * RBFT’s focus is protecting against performance degradation of the primary, but this is not our highest priority in Sovrin
 ** We can incentivize Stewards to provide good performance by paying them in tokens.
 As a result, the probability of such malicious actions is lower than in a common abstract case, and such rare situations can hopefully be caught by active monitoring systems.
 * RBFT doesn’t work well if Consensus (ordering phase) is partially combined with execution (uncommitted state) as done in Plenum, since there is nothing to execute by backup Replicas.

*Observations:*
 * Observation 1: Poor primary performance is not our biggest concern
 * Observation 2: Biggest current bottleneck is request propagation
 * Observation 3: Protocols with MAC are more performant, but more difficult

*Protocols:*
 * PBFT
 * Aardvark
 * Spinning
 * Tendermint
 * Stellar

*Conclusion*
 * We should use some ideas from Aardvark (like separate NICs and regular view changes)
 * If we improve existing Plenum code base, then the easiest options are (in order of difficulty):
 ** PBFT
 ** Aardvark
 ** Tendermint
 ** Spinning
 * If we start from scratch, then we should consider Tendermint or Spinning

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Run load tests just with one Replica,INDY-1616,33066,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Aug/18 7:39 PM,25/Oct/19 9:09 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Patch the code to have one (master) replica only and compare performance with the normal case. Needs to be done after INDY-1649 is fixed.

*Acceptance criteria:*
 * Patch the code (should not go to production)
 * Run load tests with patched code to write NYMs only
 * Run load tests with patched code in a mixed mode

 ",,,,,,,,,,INDY-1649,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125u:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Research Sawtooth,INDY-1617,33067,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,esplinr,21/Aug/18 8:41 PM,09/Sep/19 10:07 PM,28/Oct/23 2:47 AM,,,2.0,,,,0,,,,"Evaluate if Sawtooth could be made into a viable replacement for Indy.
 Timebox the effort to 3 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** The state of the project's community
 ** The current roadmap
 ** Rough estimate of work required to bring Sawtooth to the same level as Plenum
 ** A recommendation on whether to research more or kill the proposal
 ** Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",,,,,,,,,,,,,,,,,,,,,,,INDY-1614,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwx4f:2rzmc",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throughput is degrading if backup primary is stopped,INDY-1618,33072,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,22/Aug/18 12:32 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"Build Info:
indy-node 1.6.565

Steps to Reproduce:
1. Install pool of 25 nodes.
2. Disable view changes and enable metrics.
3. Run load test ~100 NYMs/sec.
4. Stop the backup primary (1 instance for example) after 30 minutes.
5. Wait another 30 minutes and check throughput and latency metrics.

Actual Results:
Throughput is degraded and latency is dramatically increased if backup primary is stopped.

Expected Results:
Throughput and latency should be about the same with backup primary stopped.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1572,,,,,,,,,,,,,,,"22/Aug/18 12:37 AM;VladimirWork;BACKUP_DOWN_UNDER_LOAD.png;https://jira.hyperledger.org/secure/attachment/15725/BACKUP_DOWN_UNDER_LOAD.png","07/Sep/18 11:29 PM;VladimirWork;Inked1618_LI.jpg;https://jira.hyperledger.org/secure/attachment/15790/Inked1618_LI.jpg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwlz:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"22/Aug/18 1:08 AM;VladimirWork;All logs/journalctls/validator-infos are in /home/ev/logs/1618.tar.gz.;;;","30/Aug/18 9:18 PM;Toktar;When Node2 (backup primary) was switch off other nodes (12,13,14,15,25) fell behind and had a latency up to 20 minutes. But slow nodes catchuped all lost transactions after 40 minutes.
Fix in the task https://jira.hyperledger.org/browse/INDY-1660 should fix this problem. Please, retest it again with version indy-node >= 1.6.586 (indy-plenum >= 1.6.529);;;","07/Sep/18 11:29 PM;VladimirWork;Build Info:
indy-node 1.6.599

Steps to Validate:
1. Run load test to provide ~20 txns written at pool side.
2. Stop primaries for 1st, 2nd, 3rd, 4th backup instances consecutively.
3. Start primaries for 1st, 2nd, 3rd, 4th backup instances consecutively.
4. Check throughput and latency using metrics.

Actual Results:
Throughput and latency are not degraded during backup primaries stopping. !Inked1618_LI.jpg|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,
Pool stopped writing after several View Changes under load with payments,INDY-1619,33074,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,ozheregelya,ozheregelya,22/Aug/18 1:10 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"*Environment:*
 indy-node 1.5.555
 sovtoken 0.8.0+20.46
 sovtokenfees 0.8.0+20.46

*Steps to Reproduce:*
 1. Run following load tests: 
{code:java}
READ
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 1690 -g ~/stab_transactions_genesis_new -k ""[{\""get_nym\"": {\""count\"": 9}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}]""
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 1800 -g ~/stab_transactions_genesis_new -k ""[{\""get_revoc_reg_def\"":{\""count\"": 2}}, {\""get_revoc_reg_delta\"":{\""count\"": 2}}]""
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 450 -g ~/stab_transactions_genesis_new -k verify_payment
WRITE 
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 450 -g ~/stab_transactions_genesis_new -k ""[{\""nym\"": {\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}]""
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 100 -g ~/stab_transactions_genesis_new -k ""[{\""revoc_reg_def\"":{\""count\"": 1}}, {\""revoc_reg_entry\"":{\""count\"": 1}}]""
python3.5 perf_processes.py -m t -n 1 -t 0.0001 -c 225 -g ~/stab_transactions_genesis_new -k ""[{\""payment\"":{\""count\"": 9}}]""
{code}
2. Stop load test after ~12 hours.

*Actual Results:*
 Pool stopped writing. About 8 View Changes were happened on part of nodes. There are a lot of messages like ""NodeXX:0 is getting requests but still does not have a primary so the replica will not process the request until a primary is chosen"" on all of nodes. Only 50K txns written.

*Expected Results:*
 Pool should work.

Logs: s3://qanodelogs/indy-1619/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1619/ /home/ev/logs/indy-1619/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwrb:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,,,,,,,,,,,"24/Aug/18 9:29 PM;Toktar;After long View Change, nodes 10,11,12 started catchup and couldn't finish it because other nodes ordering with a high load. They stashed a lot of transactions and nodes restarted after 1 hour with out of memory.
 In this time happened 9,10,11 view changes and pool tried to make 10-12 nodes primary but couldn't because nodes restarted and pool was forced to start a new view change again and again. It took about 30 min and in this time all nodes had stashing all client messages. After this all nodes restarted with out of memory problem.

It problem was solved with next ways:
 * Discarding client requests in view change
 * Fix bug with a lot of View Changes because of master degraded. ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fail to update pool ledger if no connection to a node in the genesis file in old cli,INDY-1620,33085,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,22/Aug/18 7:30 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"This failure looks like it may be related to [https://evernym.atlassian.net/browse/EN-755,] so it is valuable to diagnose even though it is in the old cli.
Older versions of indy-node like 1.3.X were able to handle the case where a node in the genesis file were not available.  The catch-up in the old cli would time out and pool data updates would be requested from another node. This does not appear to happen correctly in newer versions such as 1.5.515.

In this case if a node is unreachable by a client, the pool is not updated, and the following is repeatedly printed to the terminal:
{code:java}
9uwBGTkFRnh9h3VkDKtVnESKTmTgHLjFzWXyWL3WjuvS reconnecting to danubeC:HA(host='128.130.204.35', port='9722')
9uwBGTkFRnh9h3VkDKtVnESKTmTgHLjFzWXyWL3WjuvS reconnecting to danubeC:HA(host='128.130.204.35', port='9722')
9uwBGTkFRnh9h3VkDKtVnESKTmTgHLjFzWXyWL3WjuvS reconnecting to danubeC:HA(host='128.130.204.35', port='9722')
...{code}
The pool catch-up does not occur.

cli logs are attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 7:30 AM;mgbailey;indy.log.txt;https://jira.hyperledger.org/secure/attachment/15727/indy.log.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwsv:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,mgbailey,,,,,,,,,,"28/Aug/18 9:24 PM;ashcherbakov;[~mgbailey]
Are you talking about old CLI (from Sovrin/Indy-Node package), or about libindy-based one?;;;","29/Aug/18 12:00 AM;mgbailey;This is the old indy-node based cli. However I am concerned that it may reflect on catch-up more generally, where there is a genesis node that is not reachable.;;;","30/Aug/18 8:13 PM;ashcherbakov;I believe Old CLI is already deprecated. Can we recommend moving to the new (libindy-based) CLI? Will it solve the issue?

[~esplinr] [~mgbailey];;;","31/Aug/18 9:41 PM;Derashe;Problem reason:
 * Problem that is happening in the attached log caused by using indy-node older than 1.3.446 and using updated client 
 * That is because updated client support protocolVersion filed in LEDGER_STATUS message and old indy-nod not. So that you can't catchup with this client

Solution:
 * For newer node use updated client
 * For elder node use downgraded client

 ;;;","04/Sep/18 11:03 PM;mgbailey;thanks for looking into this. We can close the ticket.;;;",,,,,,,,,,,,,,,,,,,,
Unable to demote node in STN,INDY-1621,33086,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ozheregelya,mgbailey,mgbailey,22/Aug/18 8:21 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"A steward with a node on the STN (version 1.3.62) is ready to have his node promoted to the live network. As a preliminary to this, his node must be demoted on the STN. Unfortunately this functionality, which we have used many times before, is not working this time. I have tried it with both the new (indy-cli) and the old (indy) cli's. Both give the same error:
{code:java}
pool(stn):wallet(stn_wallet):did(6fe...zSJ):indy> ledger node target=8Tqj57DbizpjWQCHvybtKNqKFgfw2bjJbPZrhHDoRoND alias=trustscience-validator01 services=
Transaction has been rejected: existing data has conflicts with request data {'alias': 'trustscience-validator01', 'services': []}
{code}
{code:java}
indy@sandbox> send NODE dest=8Tqj57DbizpjWQCHvybtKNqKFgfw2bjJbPZrhHDoRoND data={'alias': 'trustscience-validator01', 'services': []} Sending node request for node DID 8Tqj57DbizpjWQCHvybtKNqKFgfw2bjJbPZrhHDoRoND by 6feBTywcmJUriqqnGc1zSJ (request id: 1534888273867130) Node request failed with error: client request invalid: UnauthorizedClientRequest(""existing data has conflicts with request data {'alias': 'trustscience-validator01', 'services': []}"",)
{code}
Attached is a log from one of the STN nodes, and the pool ledger.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Aug/18 8:20 AM;mgbailey;koreaa_log.tgz;https://jira.hyperledger.org/secure/attachment/15728/koreaa_log.tgz","22/Aug/18 8:18 AM;mgbailey;pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/15729/pool_ledger.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwrz:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,mgbailey,ozheregelya,,,,,,,,,,"30/Aug/18 6:06 PM;Derashe;[~mgbailey] [~ashcherbakov]

Problem reason:
 * We can't demote node because there are another node ""valNode01"" with same ip address: 54.214.176.123

Consequences: 
 * Now we have two nodes in the pool (""valNode01"" and ""trustscience-validator01""), that cannot be updated (including demotion, promotion)
 * This is a bug and it will be fixed in scope of this ticket 
 * That situation was caused by adding ""trustscience-validator01"" node with ips and ports similar to ""valNode01"" ips and ports

Commited info: 
 * [https://github.com/hyperledger/indy-plenum/pull/904]

Rec for QA:

test on indy node ver >= 1.6.596

Try to reproduce some cases (they should fail):
 * add new node with name similar to existing one
 * add new node with host address (node/client) similar to existing one
 * change existing node's alias address to another exisiting node's alias
 * change existing node's host address to another exisiting node's HA;;;","10/Sep/18 11:38 PM;Derashe;There is fix for demote and node txn validation in 1.6.596 node version. 

We can demote ""trustscience-validator01"" node only if we change it's ip:port pair or ""valNode01""'s ip:port pair;;;","11/Sep/18 12:16 AM;mgbailey;[~Derashe] Is it possible to write a transaction to the ledger that would change the valNode01 ip:port data? What versions of indy will allow this? Only 1.6.596 and newer? Who (steward and/or trustee) can make this change? I would expect only the steward can, but I'm just checking.;;;","11/Sep/18 2:42 AM;Derashe;[~mgbailey] 
 * Yes, it is possible to change valNode01 ip:port data
 * It is possible with version that you have right now on the STN
 * Only steward of the node can change it's ip:port

All in all, you have two options to demote ""trustscience-validator01"":
 * first: You need both stewards to do this
 ** Change valNode01 ip:port
 ** Demote trustscience-validator01
 * second: You need one steward (trustscience-validator01)
 ** Change trustscience-validator01 ip:port
 ** Demote trustscience-validator01

Hope, one of this would help.;;;","11/Sep/18 4:50 AM;mgbailey;Thanks, [~Derashe] for the potential work-arounds. I will ask the steward to give them a try.;;;","12/Sep/18 5:23 AM;ozheregelya;[~mgbailey],
Update of the valNode01 is possible on both of 1.3.62 and future RC. Behavior of NODE transactions was not changed. Only STEWARD can update node data and both of STEWARD and TRUSTEE can demote node.;;;","12/Sep/18 5:36 AM;ozheregelya;Environment:
indy-node 1.3.375 (master analog of version in STN and SLN) -> 1.6.601

Steps to Validate:
1. Set up the pool with two nodes with the same IPs and ports.
2. Change IPs and/or ports on one of these nodes.
=> Node can be demoted (old CLI).
3. Add one more node with the same IPs and ports for next steps.
4. Upgrade the pool to the latest master version.
5. Try to demote one of these node.
=> Node can be demoted (new CLI).
6. Try to update any node to get the same IPs and ports on two nodes.
7. Try to add new node to get the same IPs and ports on two nodes.
=> Situation from description is impossible on newer versions.

Actual Results:
Situation from description is impossible in future. Nodes with the same IPs can be demoted.;;;","12/Sep/18 4:26 PM;Derashe;Yes, I'm sorry. [~ozheregelya] is right. [~mgbailey]

Work-arounds can be processed easier way:
 * first: You need only valNode01 steward to do this
 ** valNode01 change valNode01's ip:port
 ** Trustee demote trustscience-validator01 
 * second: You need only trustscience-validator01 steward to do this
 ** trustscience-validator01 change trustscience-validator01's ip:port
 ** Trustee demote trustscience-validator01

 ;;;",,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to create and manage a number of AWS instances,INDY-1622,33094,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,andkononykhin,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:13 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,devops,,,"Additional requirements:
- groups should be named
- instances of group should be created in different AWS regions
- there should be easy way to generate ssh config to access instances in pool
- there should be easy way to access all instances in group as a whole using Ansible

There are two major options how this can be implemented:
1. Tool that creates pool should outputs ssh config and ansible inventory file that could be used to access group instances without requiring access to AWS API.
2. Tool that creates pool just tags instances somehow, so Ansible dynamic or in-memory inventory could be used, which can be easier for further automation.
",,,,,,,,,,,,,,,,,INDY-1632,,,,,,,,,,,,,,,,,INDY-1782,INDY-1783,INDY-1784,INDY-1785,INDY-1786,INDY-1787,INDY-1788,INDY-1789,INDY-1790,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxi7:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,sergey.khoroshavin,,,,,,,,,,,"15/Oct/18 7:20 PM;sergey.khoroshavin;There is a PoC that satisfies most important requirements: https://github.com/hyperledger/indy-node/pull/979
However there is still a lot of room for improvement:
* molecule integration tests were not updated to check that instances are created in different regions
* molecule integration tests use boto, migration to boto3 is highly recommended
* module integration tests are dirty (they don't delete test security groups during teardown)
* there is no support for creating spot instances
* different groups in same namespace create separate inventory dirs (having one inventory dir per namespace would be highly preferrable)
* aws_manage role doesn't register nodes in in-memory inventory (which prevents combining aws_manage and pool_install in one playbook)
* AMI search criteria is hardcoded (it uses latest Ubuntu 16.04 image from Canonical)
* there is no support for scaling existing instance types (for example from small to large and back)
* some of documentation is not up to date;;;","26/Oct/18 6:26 PM;andkononykhin;As it was mentioned in previous comment the task is completed in general. There were also mentioned a set of things to improve: separate tasks will be created.;;;",,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to install and configure Indy Node (possibly with plugins) on a number of instances,INDY-1623,33096,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:16 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwvjo:i",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ozheregelya,sergey.khoroshavin,,,,,,,,,,"10/Dec/18 9:18 PM;andkononykhin;*PoA*:
 * add API (variable) to pass list of plugin packages
 * add API (variable) to pass role for plugins installation/configuration if it requires more complex routine than just packages installation;;;","11/Dec/18 12:39 AM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1086;;;","11/Dec/18 8:30 PM;andkononykhin;*Problem reason*:

Need an API to install plugins for both node and client parts.

*Changes*:
 * plugins as a role to manage plugins installation, provides two ways:
 ** plugins_packages variable as a list of packages
 ** plugins_role variable as a name of separate role for more complex installation scenarios
 * improves indy repositories configuration routine to reduce copy-paste

 *Committed into*:

[https://github.com/hyperledger/indy-node/pull/1086]

*Risk factors*:

Nothing is expected.

*Risk*:

Low
  

*Covered with tests*:
 * updated/added molecule tests for all relevant roles (plugins, indy_node, indy_cli, perf_scripts, indy_repo)

*Recommendations for QA*:
 * case 1: plugins installed as a list of packages
 ** create namespace using _namespace-config.py_ script with the following parameters
{noformat}
--nodes.plugins_packages gosu 'jq=1.5+dfsg-1' --clients.plugins_packages tree{noformat}

 * 
 ** run provision and configure playbooks
 ** ensure that specified packages are installed on clients (_gosu_ and _jq_) and nodes (_tree_) and have proper versions (_jq_)
 ** destroy

 * case 2: plugins installed using special ansible role (check that custom role is executed)
 ** create simple role
 *** mkdir -p test_roles/plugins_test/tasks
 *** add the following content (or any other, up to you)
{noformat}
---
- name: Ensure test file exists
  copy:
    content: """"
    dest: ""/tmp/{{inventory_hostname}}.test""
    force: no
...
{noformat}

 * 
 ** create namespace using namespace-config.py script with the following parameters (optionally, you may try different roles for nodes and clients)
{noformat}
 --nodes.plugins_role plugins_test --clients.plugins_role plugins_test{noformat}

 ** run provision playbook
 ** run configuration playbook with re-defined path where ansible searches for roles: 
{noformat}
ANSIBLE_ROLES_PATH=./test_roles ansible-playbook ... configure.yml{noformat}

 ** check that for both clients and nodes plugins_test role has been executed: there should be an empty file */tmp/*.test* on each host
 ** destroy;;;","12/Dec/18 7:53 AM;ozheregelya;*Steps to Validate:*
Case 1:
1. Initialize inventory with specified plugins to install them from repo (with version and without it).
2. Run provision and configure playbooks.
3. Make sure that plugins were installed. 
4. Run destroy playbook.
Case 2:
1. Initialize inventory with specified role for plugins installation.
2. Run provision and configure playbooks.
3. Make sure that plugins were installed. 
4. Run destroy playbook.

*Actual Results:*
Both ways of plugins installation work.;;;",,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to quickly adjust configuration of Indy Node on a number of instances,INDY-1624,33097,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:17 PM,21/Sep/18 4:48 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxnj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to preload indy node pool with a large number of transactions,INDY-1625,33099,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:21 PM,28/Nov/18 3:28 AM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"Probably this should be ""backup/restore"" workflow",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxnb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,sergey.khoroshavin,,,,,,,,,,,"28/Nov/18 3:28 AM;ozheregelya;We have two cases of backups usage:
 # Fill the ledger with any data for load testing (restoration of domain ledger would be enough);
 # Restore backup of Persistent pool for upgrade testing.

In Case 2 we need all ledgers, and we need the same IPs as pool had in the moment of backup creation. So, for this use case we will need an ability to add elastic IPs to nodes.;;;",,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to install CLI and load scripts pointing to previously configured pool,INDY-1626,33100,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:22 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,"*Acceptance criteria*
* _inventory_init.py_ script should prepare group vars file where number of node and client instances are separately configurable
* _pool_create.yml_ playbook should provision both node and client instance according to inventory group vars
* _pool_install.yml_ playbook should configure both node and client instances according to inventory group vars
* client instances should have load script installed as well as pool genesis transaction pointing at nodes
* _pool_destroy.yml_ playbook should destroy both node and client instances
* _pool_*.yml_ playbooks probably should be renamed to better reflect that they manage not only nodes, but clients also (probably just _create.yml_, _configure.yml_, _destroy.yml_)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1788,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx9z:",,,,Unset,Unset,Ev 18.23,EV 18.24,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ozheregelya,sergey.khoroshavin,,,,,,,,,,"12/Nov/18 9:28 PM;andkononykhin;*PoA*:
 # improve inventory-init.py to deal with two levels of variables: roles and playbooks
 # improve roles
 ## check that aws_manage is agnostic to hosts roles (node, client)
 ## add role to install client tools with necessary environment (e.g. _client_install_)
 *** indy-cli
 *** load scripts
 ## add role to configure client hosts (e.g. _client_configure_)
 *** genesis transactions (either generated or grabbed from one of node hosts)
 ## (optionally) refactor roles to move common node/client parts to separate roles
 # improve playbooks:
 ## rename existent playbooks (likely to be as suggested in description: _create.yml, configure.yml, destroy.yml_)
 ## extend them to deal with client hosts as well
 # update docs

*Tests*: all tests are performed by molecule
 * indy-cli is installed
 * performance scripts are installed
 * pool txns file exists on client
 * indy-cli can connect to pool
 * performance scripts can connect to pool;;;","30/Nov/18 6:13 PM;andkononykhin;*Problem reason*:

There was no way to provision and configure clients machine using pool automation ansible scripts.

*Changes*:
 * improved roles:
 ** added role *indy_cli* to install and configure indy-cli
 ** added role *perf_scripts* to install and configure performance scripts
 ** added role *indy_node* as a combination of former roles node_install and pool_install
 * improved playbooks:
 ** *provision.yml* and *destroy.yml* to create and destroy hosts  respectively
 ** *configure.yml* to configure hosts
 ** implemented logic of separate variables scopes for clients and nodes provisioning
 * improved inventory-init.py script:
 ** renamed to *namespace-config.py* since for now it is more oriented to config namespace directory where inventory is only one of sub-directories
 ** added support to config clients and nodes provisioning separately
 ** improved inventory directory structure
 ** improved command line API
 * improved tests to cover new functionality

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1041|https://github.com/hyperledger/indy-node/pull/1007]

*Risk factors*:

Nothing is expected.

*Risk*:

Low
  

*Covered with tests*:
 * molecule tests in all roles
 * test for stateful_set.py ansible module

*Recommendations for QA*:
 # check *python* *scripts/namespace-config.py --help*, please notice:
 ** vars for *localhosts* group impact provisioning parameters for both nodes and clients
 ** vars for *aws_clients_provisioner* and *aws_clients_provisioner*  hosts impact provisioning for clients and nodes respectively
 ** vars for *clients* and *nodes*  groups impact configuration for clients and nodes respectively
 # initiallize namespace directory: *{{python ./scripts/namespace-config.py indy1626 --localhosts.aws_regions us-east-1 us-west-2 --aws_clients_provisioner.aws_instance_count 2 --aws_clients_provisioner.aws_ec2_type t3.micro --aws_nodes_provis}}*{{*ioner.aws_instance_count 5 --aws_nodes_provisioner.aws_ec2_type t3.large*}}. This should create namespace directory *indy1626* with inventory sub-directory *indy1626/inventory* configured to provision five t2.large nodes and two t3.micro clients instances across two listed aws regions (_us-east-1_ and _us-west-2_). Optionally you may want to use the following options to configure clients and nodes:
 ** indy-cli: --clients.indy_cli_ver and --clients.indy_cli_libindy_ver
 ** performace scrtips: --clients.perf_scripts_ver (expected values: branch, tag or commit SHA1 in indy-node repo), --clients.perf_scripts_python3_indy_ver (in pip) and --clients.perf_scripts_libindy_ver
 ** indy-node: --nodes.indy_node_ver, --nodes.indy_plenum_ver, --nodes.python_indy_crypto_ver, --nodes.libindy_crypto_ver
 # run *{{ansible-playbook -i indy1626/inventory provision.yml}}*. This should perform creation of nodes and clients. Ensure:
 ** nodes and clients number, regions distribution, ec2 instance types are as expected
 ** ec2 security groups and key pairs are created for both clients and nodes
 ** all instances have proper tags: _Project=Indy-PA_, _Namespace=indy1626_ and _Group_ vakues are clients or nodes
 ** there is a *indy1626/ssh* directory with ssh settings to access created hosts
 ** there are inventory files *clients.yml* and *nodes.yml* in *indy1626/inventory/* directory
 # check that you are able to ssh there:
 ** {{cd indy1626/ssh && ssh -F config.nodes indy1626_nodes1 whoami && cd -}}
 ** {{cd indy1626/ssh && ssh -F config.clients indy1626_clients2 whoami && cd -}}
 # run *{{ansible-playbook -i indy1626/inventory configure.yml}}* and ensure:
 ** (locally) you have pool genesis transactions file in *indy1626/pool* directory
 ** (locally) the file is also available on clients:
 ***  {{cd indy1626/ssh && ssh -F config.clients indy1626_clients1 ""ls -la \$HOME/pool_transactions_genesis"" && cd -}}
 ** (clients) ensure that indy-cli tool is available and already configured to connect to running pool (which is named *indy-pool*):
 *** {{cd indy1626/ssh &&  ssh -F config.clients indy1626_clients2 ""echo 'pool connect indy-pool' | indy-cli"" && cd -}}
 ** (clients) ensure that perf-scripts are installed (in python virtual env), can be run and can connect to pool:
 *** {{cd indy1626/ssh && ssh -F config.clients indy1626_clients1 ""\$HOME/perf_venv/bin/perf_processes.py --help"" && cd -}}
 *** {{cd indy1626/ssh && ssh -F config.clients indy1626_clients1 ""\$HOME/perf_venv/bin/perf_spike_load.py --help"" && cd -}}
 *** {{cd indy1626/ssh && ssh -F config.clients indy1626_clients1 ""\$HOME/perf_venv/bin/perf_processes.py --test_conn -g \$HOME/pool_transactions_genesis"" && cd -}} (for now it just silently exits with exit code 0 if everything is fine)
 ** (clients, nodes) perform other checks to ensure that pool is working properly and client tools are able to connect and communicate with it
 # run *ansible-playbook -i indy1626/inventory destroy.yml* and ensure:
 ** all aws ec2 resources have been destroyed (ec2 instances, security groups, key pairs)
 ** *indy1626/inventory* and *indy1626/ssh* directories don't include any information about removed hosts

You can repeat all the above steps with the following changes:
 * each ansible-playbook can be run several times: second and further runs should not do any changes (green ansible logs)
 * you can configure each ansible-playbook command to perform operations only on/for nodes or clients using *--tags(-t)* option (e.g. _-t clients_);;;","07/Dec/18 3:16 AM;ozheregelya;*Steps to Validate:*
 # check *python* *scripts/namespace-config.py --help*
*=>* vars for *localhosts* group impact provisioning parameters for both nodes and clients
*=>* vars for *aws_clients_provisioner* and *aws_clients_provisioner*  hosts impact provisioning for clients and nodes respectively
*=>* vars for *clients* and *nodes*  groups impact configuration for clients and nodes respectively
 # initiallize namespace directory: *{{python ./scripts/namespace-config.py indy1626 --localhosts.aws_regions us-east-1 us-west-2 --aws_clients_provisioner.aws_instance_count 2 --aws_clients_provisioner.aws_ec2_type t3.micro --aws_nodes_provis}}*{{*ioner.aws_instance_count 5 --aws_nodes_provisioner.aws_ec2_type t3.large*}}. 
*=>* namespace directory *indy1626* with inventory sub-directory *indy1626/inventory* created
 # run *{{ansible-playbook -i indy1626/inventory provision.yml}}*. This should perform creation of nodes and clients. Ensure:
*=>* 5 nodes and 2 clients created in two regions, with instance types t3.large for nodes and t3.micro for clients.
*=>* ec2 security groups and key pairs are created for both clients and nodes
*=>* all instances have proper tags: _Project=Indy-PA_, _Namespace=indy1626_ and _Group_ values are clients or nodes
*=>* there is a *indy1626/ssh* directory with ssh settings to access created hosts
*=>* there are inventory files *clients.yml* and *nodes.yml* in *indy1626/inventory/* directory
 # check that you are able to ssh
*=>* both nodes and clients are accessible via ssh
 # run *{{ansible-playbook -i indy1626/inventory configure.yml}}* and ensure:
*=>* genesis file available on host and client machines.
*=>* indy-cli tool is available and already configured to connect to running pool
*=>* perf-scripts are installed in python virtual env on client machines
*=>* pool work correctly under load test from one of clients 
 # run *ansible-playbook -i indy1626/inventory destroy.yml* 
*=>* all aws ec2 resources have been destroyed
*=> indy1626/inventory* and *indy1626/ssh* directories don't include any information about removed hosts
 # repeat all the above steps and run them only for part of instances
=> *tags* work, second run does nothing.

*Actual Results:*

Pool automation playbooks are awesome.;;;",,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to run load test from several agent instances,INDY-1627,33101,,Story,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,andkononykhin,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:23 PM,13/Feb/19 9:50 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxgb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to add/remove nodes from already configured pool,INDY-1628,33102,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:24 PM,21/Sep/18 4:47 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxnz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),CryptoCaveat,sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to add/remove load test agents from group,INDY-1629,33103,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:25 PM,21/Sep/18 4:47 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxo7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to upload all data from group of nodes to S3 bucket,INDY-1630,33104,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:29 PM,07/Dec/18 9:38 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx4f:3",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,sergey.khoroshavin,,,,,,,,,,,"28/Nov/18 3:14 AM;ozheregelya;Following data need to be uploaded from node:
 * Node info: /var/lib/indy/sandbox/node*.json
 * Node logs: /var/log/indy/sandbox/*.log*
 * journalctl output
 * Metrics: /var/lib/indy/sandbox/data/Node\{{ node_num }}/metrics_db/
 * Validator-info history /var/lib/indy/sandbox/node\{{node_num}}_info_db/
 * Node config: /etc/indy/indy_config.py
 * Last validator-info output;;;",,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to download from S3 bucket all or some parts of data from specific experiment,INDY-1631,33105,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:31 PM,07/Dec/18 9:38 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx4f:3i",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to give access to testing pool to other team members,INDY-1632,33107,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:34 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,This is closely coupled with INDY-1622.,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxov:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,"09/Nov/18 11:29 PM;sergey.khoroshavin;This was already done as part of INDY-1622;;;",,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need an easy way to clean pool data without destroying AWS instance,INDY-1633,33108,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:35 PM,21/Sep/18 4:45 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxp3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need a way to create repeatable and fully automated load test (or other experiment),INDY-1634,33109,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,andkononykhin,sergey.khoroshavin,sergey.khoroshavin,22/Aug/18 6:38 PM,07/Dec/18 9:38 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx4f:4",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
indy-plenum/indy-node: decrease code smell,INDY-1635,33124,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,zhigunenko.dsr,zhigunenko.dsr,22/Aug/18 9:58 PM,08/Sep/18 12:49 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Superficial scrutineering via SonarQube revealed quite a lot pieces of strange code.
It may be just typos, but also potential causes of complicated bugs.
Easy solution is just to replace these code, but it can just to hide a symptoms of structure problems.

*Plan of attack:*
1. Do scrutineering again
2. Prepare separated PR's for each type of code smell (because of potentially big amount of files per PR or merge conflicts)
3. Check that replaced code is not a symptom of tricky problem

If it prove itself, it also good to add it to CI",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzznnb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,"23/Aug/18 11:32 PM;zhigunenko.dsr;Error types:
* Naming conventions
* Too many parameters
* Commented out code
* Too complex functions
* Collapsible ""if"" statements
* Unused local variables
* Identical expressions on both sides of operator
** https://github.com/hyperledger/indy-plenum/pull/829/files
* Suspicious nested blocks
* Two 'if' branches identical
** https://github.com/hyperledger/indy-plenum/pull/913
* Resembling methods and field names
* ""pass"" is needless;;;",,,,,,,,,,,,,,,,,,,,,,,,
Load script needs to be able to set transactions with FEEs,INDY-1636,33125,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,ashcherbakov,ashcherbakov,22/Aug/18 9:59 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"*Acceptance criteria:*
 * Need to be able to apply FEEs to all Domain transactions supported by load script.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzk8f:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,ozheregelya,,,,,,,,,,"30/Aug/18 3:36 PM;dsurnin;There are two txns with fees implemented: nym and schema.
 It is enough to start testing. The other fees txns will be implemented in scope of INDY-1665

PR
 [https://github.com/hyperledger/indy-node/pull/919];;;","30/Aug/18 5:18 PM;ozheregelya;Will be tested in scope of INDY-1661.;;;",,,,,,,,,,,,,,,,,,,,,,,
Make validator info as a historical data,INDY-1637,33127,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,sergey-shilov,sergey-shilov,22/Aug/18 10:51 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,validator-info,,,0,,,,"For now validator-info dumps state of node structures into single file, so that we can just check node state at the time of the last dump.

It is very useful to have ability to get historical data of validator-info to track node's state transition in time.

Proposed solution:
 * Add ability to save validator-info data in RocksDB storage in form
 ** key = timestamp
 ** value = json_data
 * Add ability to switch between file dump (current implementation, just last state) and RocksDB (historical data)
 * Change validator-info console app so that it checks config and finds out which data source to use. Several additional parameters may be added in case of RocksDB used, i.e. number of last records to return, time range etc.

Of course RocksDB with historical data is not for production usage.

Also for now there is no any synchronisation between validator-info writer and reader of file, it should be fixed too. Fortunately, RocksDB supports read-only mode, so there is no need to care about synchronisation here.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-56,,,No,,Unset,No,,,"1|hzwws7:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,zhigunenko.dsr,,,,,,,,,,"05/Sep/18 5:26 PM;sergey-shilov;*Problem state / reason:*

Validator info tool dumped just the last node state. It would be better for debugging to have the history of node states.

*Changes:*

Now validator info tool dumps node state into RocksDB in addition to single file. This functionality is not supposed to be used in production and disabled by default. To enable dumping of validator-info into DB in addition to file specify the following in indy config file (/etc/indy/...):
 VALIDATOR_INFO_USE_DB {color:#f92672}= {color}{color:#66d9ef}True{color}

Also implemented *validator-info-history* tool to read data from RocksDB storages, see usage.

*Committed into:*

https://github.com/hyperledger/indy-plenum/pull/910
 [https://github.com/hyperledger/indy-node/pull/929]
 [https://github.com/hyperledger/indy-node/pull/933]
 indy-node 1.6.597-master

*Risk factors:*

Nothing is expected.

*Risk:*

Low

*Recommendations for QA:*

Current implementation of *validator-info-history* tool is not final. Please try all features described in usage and provide your feedback what should be changed/improved for ease of use.
 Also try to use *validator-info-history* to read from several storages got from several nodes. A subset of nodes can be specified in command line parameters.;;;","05/Sep/18 11:04 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.597
indy-plenum 1.6.538

*Steps to Validate:*
1) {color:#00875A}validator-info-history --help{color}

2) {color:#00875A}validator-info-history -v -n -1 {color}
3) {color:#00875A}validator-info-history -v -s --names Node8{color}
4) {color:#00875A}validator-info-history -v -n 1 --names Node1,Node3{color}
5) {color:#00875A}validator-info-history -v -n 2 -s{color}
6) {color:#00875A}validator-info-history -v -n 99{color}
7) {color:#DE350B}validator-info-history -v --frm 1 --to 1536154610{color} - shows nothing, but in case --frm 1536154608 --to 1536154610 there is single record with timestamp=1536154609
8) {color:#00875A}validator-info-history -v --frm 1536154610 --to 0.1{color} - expected exception ""invalid int value""
9) {color:#FF8B00}validator-info-history -v --frm 1536154615 --to 1536154610{color} - AssertionError is thrown
10) {color:#FF8B00}validator-info-history -v --frm 1536154610 --to 1536154605{color} - AssertionError is thrown
11) {color:#00875A}validator-info-history --fields Pool{color} - expected ""Incorrect fields path""
12) {color:#00875A}validator-info-history --fields Pool_info{color}
13) {color:#00875A}validator-info-history --json --names Node1,Node3 --frm 1536154608 --to 1536154612 --fields Hardware,Pool_info.f_value{color}

14) {color:#00875A}validator-info{color}
15) {color:#00875A}validator-info -v{color}
16) {color:#00875A}CLI get-validator-info command{color}

*Additionl info:*
[~sergey-shilov]
1) with ""v"" and without ""json"" it would be better if timestamp would be human-readable, e.g. '%Y-%m-%d %H:%M:%S'

;;;","07/Sep/18 4:26 PM;ashcherbakov;Fixed in [https://github.com/hyperledger/indy-node/pull/936] (Indy Node 1.6.599);;;","07/Sep/18 6:23 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.599
indy-plenum                1.6.538

*Steps to Validate:*
1) {color:#00875A}validator-info-history -v --frm 1 --to 1536309570{color}
2) {color:#00875A}validator-info-history -v --frm 1536309569 --to 1536309570{color}
3) {color:#00875A}validator-info-history -v --frm 1536309569 --to 0{color} - expected Error Message
4) {color:#00875A}validator-info-history --json --names Node1 --frm 1536309569 --to 1636309569 --fields Hardware,Pool_info.f_value{color};;;","11/Sep/18 6:32 PM;sergey-shilov;To enable dumping of validator-info into DB in addition to file specify the following in indy config file (/etc/indy/...):
VALIDATOR_INFO_USE_DB {color:#f92672}= {color}{color:#66d9ef}True{color};;;",,,,,,,,,,,,,,,,,,,,
Numerous view changes without load,INDY-1638,33133,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,zhigunenko.dsr,zhigunenko.dsr,23/Aug/18 12:17 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"*Steps to Reproduce:*
1. Setup load: 10 writes / 100 reads per sec (no plugins)
2. Stop load when view changes started

*Actual results:*
Pool doing 27 view change one after another during 2+hours

*Expected results:*
Pool stopped change view_no as fast as possible after load stop

*Logs:* logprocessor:~/logs/viewno_chain/","indy-node 1.6.571
indy-plenum 1.6.514 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1607,INDY-1717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzznh3:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,zhigunenko.dsr,,,,,,,,,,"24/Aug/18 5:24 PM;ashcherbakov;Will be fixed in https://jira.hyperledger.org/browse/INDY-1642;;;","24/Aug/18 5:27 PM;anikitinDSR;After logs and code analizing was noticed, that we have a error with request forming during PREPREPARE building. When primary is building a PREPREPARE request, it making a dynamic validation and create 2 list of valid and invalid requests. After this, reqIdr list will be formed as simple concatenating valid and invalid lists. Then, node which received this PREPREPARE will trying to validating list of reqIdrs durinng dynamic validation, but order of requests on primary and other nodes would be different. In case of revocation , when we need to apply transactions in a special order (one by one), it would be very critical. Therefore, during PREPREPARE validation other nodes, in result, will have less or more rejected request (by dynamic validation reason) and will raise suspicious on primary and send an InstanceChange message.
Ticket with fix would be INDY-1642;;;",,,,,,,,,,,,,,,,,,,,,,,
A new strategy to reduce a risk of false positive view changes,INDY-1639,33135,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ashcherbakov,ashcherbakov,23/Aug/18 12:19 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.78,,,,0,,,,"We need to implement a pluggable strategy to trigger the view change as follows:
 * Measure how many requests are ordered (on each instance) on a given window MEASUREMENT_INTERVAL
 * If the number of ordered requests on master is less than the average (or median) on backup instances to some Delta during DEGRADADED_INTERVALS_COUNT windows in a row, then trigger View Change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1675,,,,,,,,,,,,,,,"08/Sep/18 12:34 AM;VladimirWork;INDY-1639_TIMELINE.PNG;https://jira.hyperledger.org/secure/attachment/15791/INDY-1639_TIMELINE.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwm7:",,,,Unset,Unset,EV 18.17 Service Pack,EV 18.18 Service Pack 2,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,sergey.khoroshavin,Toktar,VladimirWork,,,,,,,,"29/Aug/18 5:03 PM;sergey.khoroshavin;*Problem reason:*
Spikes during request ordering on different instances can cause false positive view changes.

*Changes:*
New strategy is implemented that takes into account only difference between number of transactions ordered on master and backup instances.  The following parameters were added to config:
{code}
ACC_MONITOR_ENABLED = False
ACC_MONITOR_TXN_DELTA_K = 100
ACC_MONITOR_TIMEOUT = 300
ACC_MONITOR_INPUT_RATE_REACTION_HALF_TIME = 300
{code}

New strategy is used only when _ACC_MONITOR_ENABLED_ is set to True. In this case if number of txns ordered by any instance is more than ordered by master by more than _ACC_MONITOR_TXN_DELTA_K_ * input request rate per second then monitor will enter alerted state. If monitor is alerted for more than _ACC_MONITOR_TIMEOUT_ seconds it will fire master degradation event. Input request rate is averaged using moving average with reaction half time of _ACC_MONITOR_INPUT_RATE_REACTION_HALF_TIME_

*PR:*
https://github.com/hyperledger/indy-plenum/pull/891

*Version:*
indy-plenum: >= 1.6.525-master
indy-node: >=  1.6.581-master

*Risk:*
None when strategy is disabled
Medium when strategy is enabled

*Risk factors:*
As this strategy is new there could be yet unknown conditions leading to false positive or missed view changes.

*Covered with tests:*
https://github.com/skhoroshavin/indy-plenum/blob/37ec03327555c236394643963cda705b54615049/plenum/test/monitoring/test_acc_monitor_strategy.py
https://github.com/skhoroshavin/indy-plenum/blob/37ec03327555c236394643963cda705b54615049/plenum/test/monitoring/test_moving_average.py

*Recommendations for QA:*
Set _ACC_MONITOR_ENABLED_ to True, run different load tests - there should be now view changes. Run load test which makes pool work at almost maximum performance (for example, write 20 NYMs per second) and slow down master primary node (using traffic shaping to delay packets or using stress tool to eat CPU) - view change should happen in 5-10 minutes.;;;","01/Sep/18 12:14 AM;VladimirWork;Build Info:
indy-node 1.6.586

Steps to Reproduce:
1. Install pool of 25 nodes and force 10 VCs (by primary shutdown/demote) to set viewNo to 10.
2. Set `ACC_MONITOR_ENABLED = True`.
3. Run load test to write 20 NYMs/sec.
4. Stop load test from previous step.
5. Run load test to write 28 NYMs/sec.
6. Stop load test from previous step.
7. Restart all nodes to reset view from 10 to 0 (there are no VCs forced by load at this moment).
8. Run load test to write 20 NYMs/sec.
9. Check viewNo at all nodes after 30-40 minutes.

Actual Results:
10 nodes are at view 1. 15 nodes are at view 0. Pool doesn't write any txns due to `Transaction has been rejected: Client request is discarded since view change is in progress` *for more than 1 hour*.

Expected Results:
1. It's unclear is this VC was false-positive or not - if it is - we should avoid false-positive VCs using this strategy.
2. VC should be completed in any case successfully and at all nodes.;;;","01/Sep/18 12:57 AM;VladimirWork;All logs and validator dumps are in /home/evernym/logs/1639.tar.gz.;;;","04/Sep/18 4:59 PM;Derashe;Problem reason: 
 * During the testing, after we restarted pool, it was not able to write txns and we had viewchange to 1 view_no.

Research:
 * Viewchange was caused by slow primary after restart
 * Primary was slow because of:
 ** processing and stashing new requests (stashing because of empty view_no) over 50k reqs
 ** generating consistency_proof for nodes that was behind befor restart

Conclusion:
 * That was not false positive view_change, and if we wait more, pool will continue ordering, so we need to retest this case
 * We need to research behaviour of node restart under heavy load and with behind nodes as a separate ticket (https://jira.hyperledger.org/browse/INDY-1675)

 ;;;","08/Sep/18 12:39 AM;VladimirWork;Build Info:
indy-node 1.6.599

Steps to Reproduce:
1. Run load test to provide ~20 txns written at pool side with ACC_MONITOR_ENABLE = True.
2. Stop primaries for 1st, 2nd, 3rd, 4th backup instances consecutively.
3. Start primaries for 1st, 2nd, 3rd, 4th backup instances consecutively.
4. Check throughput and latency using metrics.

Actual Results:
We have 22 nodes at view 5 and 3 nodes at view 0. *Pool has stopped ordering txns after all VCs*. The first VC performed after Step 3 because of 'reason': 25 (PRIMARY_DEGRADED) and all the next because of 'reason': 28 (INSTANCE_CHANGE_TIMEOUT) and we have the next metrics dynamics (1 - switching off 2,3,4,5 nodes; 2 - switching on 2,3,4,5 nodes; 3 - instance change from 0 to 1 view due to 25 reason):  !INDY-1639_TIMELINE.PNG|thumbnail! 

Expected Results:
VC due to master degraded reason looks strange with new strategy enabled. Pool should continue working after any amount of VCs.;;;","10/Sep/18 7:58 PM;VladimirWork;All logs and validator dumps are in /home/evernym/logs/1639newissue.tar.gz.;;;","12/Sep/18 6:56 PM;Toktar;First View Change was expected because master instance ordering slower than backup instance 7. Size of butches on the master was smaller and with near speeds of ordering batch speed of transaction ordering was slower. Moreover, in this time happened a stabilization of checkpoint on the master.
2-5 view changes were  expected too because  in tests were stopped  primary nodes for 1st, 2nd, 3rd, 4th backup instances and these nodes can't be a primary for master instance. 
Problem with out of memory because of untreated requests queue will fix in tasks from epic Switch off replicas.;;;",,,,,,,,,,,,,,,,,,
Repeated demotion and promotion of non-primary nodes result in eventual consensus failure,INDY-1640,33146,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ckochenower,ckochenower,23/Aug/18 7:07 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,1.6.73,1.6.79,test-automation,,,0,TShirt_L,,,"Running an experiment that does the following over and over again (demote and promote random non-primary node) causes a pool of 10 nodes to fall out of consensus in approximately 6 to 8 iterations:

Configuration (reset the pool):
- 10 node pool - Node1 through Node10
- Node1 is the primary
- Node2, Node3, and Node4 are initially the backup primaries
- f_value = 3
- Count_of_replicas = 4

Steps:
1. Guarantee that the pool is in consensus by writing a NYM to the domain ledger (a.k.a. steady state hypothesis). Allow up to 60 seconds for the operation (write to the domain ledger) to complete.
2. Pick a non-primary node at random and demote it (set the node's services attribute to blank (""""). Expect this operation to succeed. Allow up to 120 seconds for the operation (write to the pool ledger) to complete.

Configuration at this point:
- 9 node pool. Node1 (the primary) is guaranteed to still be in the pool. The remaining 8 nodes depends on what node was randomly selected to be demoted.
- Node1 is the primary
- The set of 2 backup primaries depends on what node was randomly selected and demoted, because one of the backup primaries could have been selected.
- f_value = 2
- Count_of_replicas = 3

3. Check if the pool is still in consensus by writing a NYM to the domain ledger (a.k.a. steady state hypothesis). Allow up to 60 seconds for the operation (write to the domain ledger) to complete.
4. Promote the demoted node (a.k.a. rollback). Allow up to 20 seconds for the operation (write to the domain ledger) to complete.
5. Sleep 10 seconds before restarting the node being promoted.
6. Stop the indy-node service on the node being promoted.
7. Make sure the indy_node service is stopped on the node being promoted
8. Start the indy-node service on the node being promoted

Configuration at this point:
- 10 node pool - Node1 through Node10
- Node1 is the primary
- Node2, Node3, and Node4 are likely still backup primaries, because backup primaries (replicas) are not changed unless a view change happens and a view change was not forced by stopping or demoting the primary (Node1). However, a view change could happen for other reasons. Therefore, the set of backup primaries could vary.
- f_value = 3
- Count_of_replicas = 4

Repeating the above steps over and over again cause the pool to fall out of consensus within 6 to 8 iterations.

nscapture archives from each of the 10 nodes are located [here|https://drive.google.com/open?id=1xRcegwFchSmUHcWm7Ux65AzGRc-W2GcW].

If the log processor tool will be used to analyze this issue, the logs from each node can be found in the nscapture archive within the archive's log directory.","$ pip3 freeze | grep indy
python3-indy==1.5.0.dev620

$ sudo apt list --installed | grep indy
indy-cli/xenial,now 1.6.1~701 amd64 [installed,upgradable to: 1.6.2~713]
libindy/xenial,now 1.6.1~701 amd64 [installed,upgradable to: 1.6.2~713]
libindy-crypto/xenial,now 0.4.1~53 amd64 [installed,upgradable to: 0.4.3]",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1609,INDY-1578,INDY-1719,INDY-1720,,INDY-1676,,,,,,,,,,,,,,,"23/Aug/18 11:33 PM;ckochenower;Screen Shot 2018-08-23 at 8.33.11 AM.png;https://jira.hyperledger.org/secure/attachment/15747/Screen+Shot+2018-08-23+at+8.33.11+AM.png","23/Aug/18 7:20 AM;ckochenower;run-demote-promote.out;https://jira.hyperledger.org/secure/attachment/15736/run-demote-promote.out",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwx67:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ckochenower,Derashe,,,,,,,,,,"23/Aug/18 7:29 AM;ckochenower;The STDOUT from the ""demote-promote"" Chaos experiment located [here|https://github.com/evernym/chaosindy/blob/master/experiments/demote-promote.json] is attached:

 [^run-demote-promote.out] ;;;","23/Aug/18 7:59 AM;ckochenower;I am rerunning the experiment with a 60 second pause/sleep between demoting and promoting the randomly selected node. Doing so ensures that the other nodes have time to see that the node was demoted and to reduce their f_value and Count_of_replicas values (because we are going from 10 to 9 nodes each time) before the demoted node is brought back into the pool.;;;","23/Aug/18 11:29 PM;ckochenower;Adding a 60 second pause/sleep between demoting and then promoting the node allows the experiment to run w/o failure through the night. The experiment successfully ran 316 times.

*Therefore, it appears that the pool can be compromised if the nodes are allowed to exit and then reenter the pool too quickly.*

The pool starts with 10 transactions on the Pool Ledger. Each iteration of the experiment increases the Pool Ledger by 2; one demote transaction and one promote transaction. With 316 successful iterations of the experiment, I would have expected the Pool Ledger Size (PLS) to be much larger than 109. It should have been closer to 316 * 2 + 10 = 642. I'm not sure how to explain the transaction count of 109.

 !Screen Shot 2018-08-23 at 8.33.11 AM.png|thumbnail! 

As a side note, it appears that even though the primary is guaranteed to be excluded from the list of nodes from which to randomly select a node for demotion, the pool experienced at least one view change. The experiment started with Node1 as the primary. Node2 was the primary after 316 iterations of the experiment.;;;","28/Sep/18 12:50 AM;ashcherbakov;The issue may be the same as addressed in INDY-1578, INDY-1719, INDY-1720;;;","09/Nov/18 9:37 PM;Derashe;During research, Janko reproduced the above situation in test environment. In this case we were able to distinguish some bugs, find them in code and create related tickets ( INDY-1719, INDY-1720).

So problems that were described in this ticket will be solved in scope of them and this ticket can be closed.;;;",,,,,,,,,,,,,,,,,,,,
Test Pool Automation,INDY-1641,33158,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,23/Aug/18 8:29 PM,15/Aug/19 9:43 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,We need scripts for efficient dealing with AWS Pools in order to do load testing there and get results.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-1,,Test Pool Automation,To Do,No,,Unset,No,,,"1|hzzntj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3PC Batch should preserve the order of requests when applying PrePrepare on non-primary,INDY-1642,33195,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,24/Aug/18 5:23 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"Primary doesn't preserve the order of requests it applied them, so it may lead to different results of dynamic validation and Suspicious Exception on non-primaries.

It may lead to a lot of View Changes.

*Example:*
 * When master primary creates a PrePrepare it iterates through requests in some order, let's say [R1, R2, R3, R4, R5]
 * Let's assume that request R1 and R2 can be created only after R3, so they fail dynamic validation.
 * As a result, a list of valid requests is [R3, R4, R5], and a list of invalid is [R1, R2].
 * When PrePrepare is created, it get a list of reqs as valid + invalid, that is [R3, R4, R5, R1, R2], which is not equal to the initial list the master primary iterated through.
 * Non-primary replicas will apply PrePrepares in the order as specified in PrePrepare, that is [R3, R4, R5, R1, R2], and for it R1 and R2 will pass dynamic validation since R3 is already created.
 * So, non-primaries will raise Suspicious code and trigger view change.

 

*Acceptance criteria:*
 * Preserve the order of requests for master primary in PrePrepare
 * Have a bitmask to mark invalid requests
 * Extend dynamic validation to eack the bitmask
 * Enhance creation of Ordered requests to get only the valid ones",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1643,INDY-1672,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwtb:",,,,Unset,Unset,EV 18.17 Service Pack,EV 18.18 Service Pack 2,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,zhigunenko.dsr,,,,,,,,,,"30/Aug/18 4:14 PM;anikitinDSR;Reason:
 * need to change discarded logic in PREPREPARE building and take into account the order of requests while doing dynamic validation

Changes:
 * as of now, discarded field in PREPREPARE is a bit mask of reqIdrs, when 0 is a valid request and 1 - is not

Version:
 * indy-node 1.6.590

Recomendation for QA (as in INDY-1638):
 - run load_test for revocation transactions and check that there is not chain of view_changes by reason ""got one of primary suspicions codes 7"" ;;;","03/Sep/18 6:22 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.590
indy-plenum 1.6.533

*Steps to Validate:*
1) prepare pool with 25 nodes
2) run load test: 1000 clients, 10 revocation writes per sec, 100 revocation reads per sec

*Actual results:*
There is no chain of view_changes by reason ""got one of primary suspicions codes 7""

*Additional Info:*
Performance issue detected. Moved to INDY-1672;;;",,,,,,,,,,,,,,,,,,,,,,,
Monitor needs to take into account requests not passing dynamic validation when trigerring view change,INDY-1643,33197,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ashcherbakov,ashcherbakov,24/Aug/18 6:16 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.73,,,,0,,,,"When Monitor calculates throughput and latency for master, it takes into account only valid requests, that is requests passing dynamic validation. But non-master instances don't have dynamic validation, so all requests are taken into account there.

It may lead to false-positive View Changes since backup instances will think that master degraded, while it just had a lot of invalid requests.

*Acceptance criteria:*
 * Include information about all requests into Ordered message
 * Pass all requests into Monitor
 * Take only valid requests when processing Ordered message on Node.

Can be done together with INDY-1642 to take into account a new PrePrepare format.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1642,,,,,INDY-1653,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzzo1j:",,,,Unset,Unset,EV 18.17 Service Pack,EV 18.18 Service Pack 2,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,ozheregelya,,,,,,,,,,"30/Aug/18 4:07 PM;anikitinDSR;Reason:
 * need to take into account for monitoring requests, which not passing dynamic validation

Changes:
 * added invalid requests into request oredring procedure for monitoring

Version:
 * indy-node: 1.6.590

Recomendation for QA:
 * setup pool
 * send not valid requests (for example revocation)
 * check metrics section in 'validator-info -v' output that all replicas have the same ordered requests count;;;","04/Sep/18 7:56 AM;ozheregelya;Environment:
 indy-node 1.6.591

Steps to Validate:
 1. Set up the pool.
 2. Run load test with valid sender seed.
 3. Run one more load test with not existing seed as sender seed.
 4. Run one more load test with identity owner seed as sender seed.

Actual Results:
 Node1: 
{code:java}
 ""ordered request counts"": ""1"": 37430 ""0"": 37428 {code}
Node2:
{code:java}
 ""ordered request counts"": ""0"": 37220 ""1"": 37222 {code}
Node3:
{code:java}
 ""ordered request counts"": ""1"": 37356 ""0"": 37354 {code}
Node4:
{code:java}
 ""ordered request counts"": ""1"": 37238 ""0"": 37237 {code}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Find out maximal pool performance numbers for writes only,INDY-1644,33263,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ozheregelya,ozheregelya,28/Aug/18 4:06 AM,08/Oct/19 9:14 PM,28/Oct/23 2:47 AM,08/Oct/19 9:14 PM,,,,,,0,,,,"Find out maximal pool performance for writes only:
 * Only NYMs
 * Payments only
 * Mixture mode as in INDY-1343
 * NYMs with FEEs
 * Mixture mode with FEEs (for the txns we support FEEs in the load script)

Run strategy:
 * Consider have metrics enabled and increase the load starting from a small number. If according to metrics everything is fine, increase the load further.

Results:
|NO FEES|Initial load|Version| Result|
|nym|10|1.6.738| 33|
|attrib|10|1.6.738| 30|
|schema|8|1.6.747| 23.5|
|cred_def|6|1.6.747| 9|
|revoc_reg_def|5|1.6.753| 20|
|revoc_reg_entry|5| | |
|payment|5|1.6.738 | 20|
|production load|10|1.6.747| 15|
|FEES|Initial load|Version| Result|
|nym|10|1.6.738 | 20|
|attrib|8|1.6.738| 17.7|
|schema|8|1.6.747| 17.5|
|cred_def|3|1.6.752| 8|
|revoc_reg_def|3|1.6.761| 14|
|revoc_reg_entry|3| | |
|payment|3|1.6.738| 20|
|production load|3|1.6.747| 11|",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwx4n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,,,,,,,,,,,,"30/Nov/18 4:23 AM;ozheregelya;*PoA:*

Need to find out maximal pool performance for each type of transactions. 
 Load should run using [spike load test|https://github.com/hyperledger/indy-node/blob/master/scripts/performance/perf_load/perf_spike_load.py]. 
 Initial load rate (see spreadsheet in description) was chosen as load which expected to be successfully processed by pool.
 Load should increase by 1 txn/sec per 0.5 hour.
 Load test should be stopped when client will consistently give timeout errors.
 Maximal pool performance will be obtained from node metrics and load script statistic, as performance in the last period when performance of the pool was equal to load rate produced by the load script.

Spreadsheet in the description will be used to track results.

Note that testing of cases with fees will not make sense until fix of ST-497/INDY-1874.;;;","12/Feb/19 12:38 AM;ozheregelya;Current results:
||txn type|| without fees || with fees ||
|nym| 33 | 20 |
|attrib| 30 |  17.7 |
|schema| 23.5 | 17.5 |
|cred_def| 9 | 8 |
|revoc_reg_def| 20 | 14 |
|payment| 20| 20 |
|production load (INDY-1343)| 15 | 11 |;;;",,,,,,,,,,,,,,,,,,,,,,,
read_ledger passes incorrectly formatted stdout and breaks convention,INDY-1645,33266,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,evalovelac3,evalovelac3,28/Aug/18 6:29 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.73,,,,0,common-components,,,"read_ledger not passing correct stdout, this breaks several scripts that make use of this functionality and prevents technical operations from progressing on current projects.
{code:java}
// vagrant@validator01:/usr/local/bin$ sudo read_ledger --type pool --count 2018-08-27 20:44:29,359|INFO|notifier_plugin_manager.py|Found notifier plugins: [] 
4
 
{code}
 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzo27:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,evalovelac3,mgbailey,VladimirWork,,,,,,,,,"28/Aug/18 6:32 AM;mgbailey;Specifically, once again we have logging information going to stdout on one of the tools that we depend on to give standardized output.  Once again, this is a bug.;;;","28/Aug/18 6:41 AM;evalovelac3;{code:java}
// vagrant@validator01:/usr/local/bin$ sudo dpkg -l | grep indy
hi  indy-anoncreds                     1.0.32                                     amd64        Anonymous credentials
hi  indy-node                          1.6.580                                    amd64        Indy node
hi  indy-plenum                        1.6.524                                    amd64        Plenum Byzantine Fault Tolerant Protocol
hi  libindy-crypto                     0.4.3                                      amd64        This is the shared crypto libirary for Hyperledger Indy components.
hi  python3-indy-crypto                0.4.3                                      amd64        This is the official wrapper for Hyperledger Indy Crypto library (https://www.hyperledger.org/projects).
{code};;;","28/Aug/18 6:46 AM;evalovelac3;https://github.com/hyperledger/indy-node/blob/master/scripts/read_ledger;;;","28/Aug/18 6:46 AM;evalovelac3;Breaks https://github.com/hyperledger/indy-node/blob/7cefc77279bb1910ca55bb96e640c9fe60ae605e/scripts/current_validators#L84;;;","29/Aug/18 2:32 AM;Derashe;Problem reason:
 * That problem was caused by incorrect config dependency 
 * That problem fixed by setting classes name instead of classes itself

Commit info:
 * [https://github.com/hyperledger/indy-plenum/pull/900|https://github.com/hyperledger/indy-plenum/pull/899]

Rec for QA:
 * Run any of read_ledger script and check if log message appears (indy-node 584 ver);;;","30/Aug/18 7:16 PM;VladimirWork;Build Info:
indy-node 1.6.585

Steps to Validate:
1. Run all used indy-node scripts (read_ledger/validator-info/init/generate/enable_bls) to check output.

Actual Results:
There are no node's logs in scripts' output.;;;",,,,,,,,,,,,,,,,,,,
Metrics: timing and transaction types,INDY-1646,33274,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,sergey.khoroshavin,zhigunenko.dsr,zhigunenko.dsr,28/Aug/18 5:22 PM,29/Aug/18 3:51 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"As Pool maintainer I need to understand real profile of loading during pool work

*Acceptance criteria:*
- Metrics or CSV output allow to create transaction time distribution graph (average and median frequency between two requests)
- Metrics or CSV output allow to create transaction type distribution graph (at least each type percentage)
- Metrics or CSV output allow to match frequency distribution to type distribution

*Expected Result:*
%timestamp% | %incoming_request_type_from_client%",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxlz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SMART info in validotor_info output,INDY-1647,33277,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,28/Aug/18 6:11 PM,08/Jan/19 4:26 AM,28/Oct/23 2:47 AM,,,,validator-info,,,0,,,,"To help stewards with monitoring node's state and hardware health we could include hdd\ssd SMART info to the validator_info log.
Also it is possible to make some rough predictions about disk health based on SMART info.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzojj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stewards gui app,INDY-1648,33278,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,28/Aug/18 6:52 PM,28/Aug/18 7:19 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"To help stewards monitor node state we could provide GUI application with node status, pool status, latest log events, validator info, etc.
Based on current statistic app could make some analysis and prediction about node state or possible hardware issues.
It could remind stewards to make some preventive/prophylactic activity like making backup, etc. Other stewards related info.
With community growth app can show latest community news, events.
It could be desktop or web or mobile app.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzojr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Do not re-verify signature for Propagates with already verified requests ,INDY-1649,33281,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ashcherbakov,ashcherbakov,28/Aug/18 9:41 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"In order to improve performance, we can avoid redundant verification of request signatures in Propagates for which we already verified signature",,,,,,,,,,,,,,,,,INDY-1616,INDY-1707,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwwrj:",,,,Unset,Unset,EV 18.18 Service Pack 2,EV 18.19,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,ozheregelya,zhigunenko.dsr,,,,,,,,,"26/Sep/18 10:47 PM;anikitinDSR;Reasons:
 * need to decrease counts of signature verifying for Propagates messages

Changes:
 * created map in ReqAuthenticator class for already verified requests.
 * added logic for ""do not re-verify signature for Propagates messages"" using a map from previous point

Versions:
 * indy-node 1.6.614

Recommendations for QA:
 * need to run load_test for general regression purposes;;;","27/Sep/18 12:53 AM;zhigunenko.dsr;*Steps to Validate:*
1. Run load test - 10 writes per second, no payments, no fees
2. Run load test - 10 writes per second, + payments, + fees;;;","28/Sep/18 9:04 PM;ozheregelya;*Environment:*
indy-node 1.6.615
libindy 1.6.6~759

*Steps to Validate:*
1. Run load test with normal load (10 mixed writes/sec).
2. Run load test with high load (40 nyms/sec).

*Actual Results:*
No regression found in case of normal load (see results in Ext-27-09-18-mix-no-fees, Live-27-09-18-mix-fees).
Pool performance was improved (see results in Stab-27-09-18-Memory).;;;",,,,,,,,,,,,,,,,,,,,,,
Research performance of static validation against message schema,INDY-1650,33282,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,28/Aug/18 9:45 PM,08/Jan/20 5:11 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Each message (node and client one) is validated against Message Schema in the scope of static validation.

According to metrics, it takes quite a lot of time.

*Acceptance criteria*
 * Come with a plan (PoA) on how to improve performance.
 One of the following options is possible:

 * 
 ** Everything is fine, and validation is slow just because of a huge number of messages
 ** There is an obvious issue in the code
 ** A fundamental change to the approach is proposed
 * Create a ticket for implementation of the proposed solution",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41isn",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support Incremental creation of 3PC batches,INDY-1651,33283,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,28/Aug/18 9:48 PM,13/Feb/19 10:02 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"As of now each PrePrepare defines 1 3PC batch. Ordering each batch requires sending and processing lots of 3PC messages, so the more requests we put into batch, the better throughput we achieve (theoretically). Unfortunatelly now dynamic validation is performed when PrePrepare is created or received, which means that we need to perform dynamic validation of all requests in a batch at once, and for batches consisting of 1000s requests it means pause of tens seconds during which node is not responsive. Furthermore, this validation takes place on primary replica, and only after it is finished here it can start on non-primary.

Proposed solution is to allow multiple PrePrepares define 1 3PC batch,  so that validation of each PrePrepare is relatively fast and can overlap between primary and non-primary nodes reducing total latency. This will allow much bigger batches which in turn can significantly reduce workload per request, leading to higher throughput.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwx4f:2rzlc",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
[Design] Fix node release numbers,INDY-1652,33287,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,esplinr,esplinr,28/Aug/18 10:12 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,"*Story*
 As an administrator of a node for an Indy network, I want to clearly understand the scope of changes in each release and map that release to version numbers in JIRA and the release notes.

*Goals:*
 * Version numbers use 4 values, generally following semver: major release, minor release, service pack, and build.
 * The build number increases with each build, starting with zero on each service pack.
 * The major, minor, and service pack numbers should be the same for all artifacts in the release: debian, android, etc. The build number can float.
 * Next release needs to be one higher than the current stable build.

*Acceptance criteria:*
 * Design/PoA 
 * WBS and technical tasks
 * Review of Design",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1992,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-766,,,No,,Unset,No,,,"1|hzwvif:000006r",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,danielhardman,esplinr,,,,,,,,,"31/Aug/18 1:35 AM;andkononykhin;[~esplinr] Hello Richard. This task might become an epic since it might impact not only version scheme and git tags routine but the whole development and release workflows. I am going to accumulate my thoughts about versioning and releasing and post a kind of a design here in a few days.;;;","07/Sep/18 11:13 PM;andkononykhin;[~esplinr][~ashcherbakov][~gudkov][~Sergey.Kupryushin][~sergey.minaev][~andrey.goncharov]
The design https://docs.google.com/document/d/1rVTbda_T57JJF2PzpEjmr-SlQ4do5qZXMPfFf1Ao3qQ/edit#heading=h.76gmzqydqqme;;;","15/Sep/18 6:38 AM;danielhardman;Andrey: I appreciate your effort here, but I cannot consume a 21-page document with a table of contents and so forth. It's simply too overwhelming. Here's what would help me to make useful decisions:
 # What are the key differences between what we did before and what you want to do now?
 # Why?

I don't want to close this ticket until our entire organization is aligned.;;;","15/Sep/18 2:11 PM;andkononykhin;[~danielhardman] I agree: a kind of summary is missed there, I will add that to the document at the top.

>What are the key differences between what we did before and what you want to do now?

There are a lot differences between projects, I will mostly keep in mind two documents (slides and docs) that describe ""approved"" approach plus current indy node way which I am aware of most of all. ;;;","18/Sep/18 11:02 PM;andkononykhin;[~danielhardman] The design has been updated.;;;","08/Feb/19 10:45 PM;ashcherbakov;The Design has been done.
Application of the Design (or some principles from this design) to the issue from the ticket will be addressed in INDY-1992;;;",,,,,,,,,,,,,,,,,,,
View changes happen when all responses should be rejected during load testing scenario,INDY-1653,33290,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,NataliaDracheva,NataliaDracheva,28/Aug/18 11:22 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,load,,,"*Environment*

Indy-node 1.6.71 (stable)

*Steps to Reproduce:*

1. Add a user without a role to a ledger, seed = 000000000000000000000NotTrustee1.

2. Execute perf_spike_load.py script with the attached config file (update genesis file in config) on a pool of 25 nodes.

3. Wait until the script is executed (1 hour).

*Actual Results:*
There were View Changes and a lot of 307 errors (timeout)

*Additional info:*
Logs are in logs/indy-1653 folder on evernymr33 server.
In order to get logs:
{code:java}
pscp ev@evernymr33:logs/indy-1653/Indy-1653.zip <dest folder on local>
{code}",Indy-node 1.6.71,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1643,,,,,,,,,,,,,,,,,,,,"11/Sep/18 7:12 PM;NataliaDracheva;1653_retest_config.yml;https://jira.hyperledger.org/secure/attachment/15800/1653_retest_config.yml","28/Aug/18 11:22 PM;NataliaDracheva;perf_spike_load_config.yml;https://jira.hyperledger.org/secure/attachment/15751/perf_spike_load_config.yml","28/Aug/18 11:22 PM;NataliaDracheva;validator_info.txt;https://jira.hyperledger.org/secure/attachment/15752/validator_info.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwq7:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,,,,,,,,,,,"29/Aug/18 3:46 PM;ashcherbakov;This may be caused by the issued being fixed in the scope of https://jira.hyperledger.org/browse/INDY-1643;;;","05/Sep/18 11:18 PM;ashcherbakov;Let's re-test this since INDY-1643 is done;;;","11/Sep/18 7:10 PM;NataliaDracheva;*Scenario 1:*
*Build version:* 
indy-node: 1.6.599
indy-plenum: 1.6.538
*Test description:* Produce spike load of transactions which should be rejected.
*Preconditions:* Add a did without role to the ledger (000000000000000000000000Trustee3)
*Steps to Reproduce:*
1. Run perf_spike_load.py from 1 AWS agent (config file for the script:  [^1653_retest_config.yml] ).
*Expected results:* No view changes during test execution.
*Actual results:* No view changes during test execution. (/)
*Additional info:* Errors in console were because of incorrect -b parameter which should be pre-calculated as TotalTransactions/NumberOfClients, it does not affect test results.
*Logs:* https://s3.console.aws.amazon.com/s3/buckets/qanodelogs/indy-1653?region=us-east-1;;;",,,,,,,,,,,,,,,,,,,,,,
As a QA I want to stop load script fast when errors 307 are occured,INDY-1654,33291,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,VladimirWork,VladimirWork,28/Aug/18 11:30 PM,09/Oct/19 6:24 PM,28/Oct/23 2:47 AM,09/Oct/19 6:24 PM,,,,,,0,,,,As a QA I want to stop load script fast when errors 307 are occured because it is impossible to do this via Ctrl+C or Ctrl+D.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwxl3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"09/Oct/19 6:24 PM;ashcherbakov;Not an issue anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix revocation txns in load script,INDY-1655,33292,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,VladimirWork,VladimirWork,28/Aug/18 11:30 PM,09/Oct/19 6:25 PM,28/Oct/23 2:47 AM,09/Oct/19 6:25 PM,,,,,,0,,,,We should send revocation txns to ledger strictly after credential definition txn result getting since now it causes to errors during revoctaion txns sending.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwxkv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"09/Oct/19 6:25 PM;ashcherbakov;Not an issue anymore;;;",,,,,,,,,,,,,,,,,,,,,,,,
Incorrect load script behaviour,INDY-1656,33293,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,VladimirWork,VladimirWork,28/Aug/18 11:30 PM,09/Oct/19 6:20 PM,28/Oct/23 2:47 AM,09/Oct/19 6:20 PM,,,,,,0,,,,"Overview:
During tokens minting the script produces hundreds of console errors ""Cannot generate request since no req data are available."" despite eventually all of the requests are sent successfully. This behavior is confusing and should be changed.

Expected Results:
The System should not display such errors.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1553,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwxk7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"09/Oct/19 6:20 PM;ashcherbakov;We've already fixed this;;;",,,,,,,,,,,,,,,,,,,,,,,,
Node can't write after catch up of large ledger.,INDY-1657,33296,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,ozheregelya,ozheregelya,29/Aug/18 12:33 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,TShirt_M,,,"*Environment:*
 indy-node 1.6.578

*Steps to Reproduce:*
 1. Perform catch up of large ledger (600K txns) on one of the nodes of pool (Node25 in this case).
 2. Send several NYMs.
 => Node25 doesn't write txns.
 3. Restart the Node25.
 => It processed missed txns.
 4. Send several NYMs.
 => Node25 doesn't write txns again.

*Actual Results:*
 Node after catch up can't write txns.

*Expected Results:*
 Node after catch up should work same as other ones.

*Additional Information:*
Pool restart didn't help.

Logs: s3://qanodelogs/indy-1657/NodeXX/
To get logs, run following command on log processor machine: 
aws s3 cp --recursive s3://qanodelogs/indy-1657/ /home/ev/logs/indy-1657/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1595,,,,,INDY-1711,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwqf:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,spivachuk,,,,,,,,,,,"19/Sep/18 1:56 AM;spivachuk;Node25 stopped ordering because it detected an *incorrect state trie root hash* at 2018-08-28 12:32:27 when it tried to apply the first 3PC-batch after the huge catch-up. However, Node25 was still able to catch up missed transactions because catch-up process does not verify matching of states.

The merkle tree root hashes before applying the transaction that caused state mismatch and after applying it (before reverting) were the same on Node25 as on other nodes. But the state trie root hashes on Node 25 were different from the hashes on other nodes both before reverting and after reverting the transaction.

There were no logged state root hashes on Node25 before attempts to apply 3PC-batches that were done after the huge catch-up because state root hashes are logged during ordering only _but not during or after a catch-up and not on a node initialization_.

However, we see in logs of Node25 that there were multiple attempts to catch up the domain ledger. They were interrupted by multiple restarts of this node until the last attempt succeeded at 2018-08-27 22:57:18 to catch up the domain ledger till its end. We don't have systemd journal of this node for the period of these restarts, so we don't know the reasons for these restarts. During the successfully completed last attempt of catch-up the node caught up ~162K transactions of ~633K total number caught up. So other transactions were caught up during the previous interrupted attempts of catch-up. Not all the shutdowns of the node on these restarts were graceful, some of them were abnormal as it is seen from the logs (for these abnormal shutdowns there are no log messages about the looper shutdown). We suppose that for some transaction added to the ledger (during a catch-up) the state was not updated due to an interruption by such an abnormal shutdown of the node. But to determine the cause of the issue for sure, we have added logging of states to node initialization and catch-up and will re-test the case.

Logging of states have been enhanced in scope of the following PRs:
 - [https://github.com/hyperledger/indy-plenum/pull/916]
 - [https://github.com/hyperledger/indy-node/pull/947]

The version containing these changes:
 - indy-node 1.6.608-master
 - indy plenum 1.6.543-master;;;","21/Sep/18 7:39 PM;ozheregelya;*Environment:*
 indy-node 1.6.608
 indy plenum 1.6.543

*Steps to Reproduce:*
 1. Setup the pool of 24 nodes.
 2. Run load test to get ~600K txns in domain ledger.
 3. Add 25th node.
 ~ First time node was added by mistake with wrong configuration. After that it was demoted from the pool, stopped cleaned up, initialized again with right configuration and promoted back.
 4. Wait for the end of catch up.
 => Node completed catch up successfully. All nodes have 607129 in the ledger.
 5. Write 1 NYM using indy-cli.
 => NYM was written on all nodes exclude 25th.
 6. Run load test.

*Actual Results:*
 NYM was not written by 25th node after first catch up (step 5).
 After load test and second catch up of 25th node all nodes exclude 25th have 616280 txns. 25th node has 613330.

*Expected Result:*
 Node should write after catch up.

Logs and metrics: s3://qanodelogs/INDY-1657-additional-logging/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/INDY-1657-additional-logging/ /home/ev/logs/INDY-1657-additional-logging/;;;","23/Sep/18 6:38 AM;spivachuk;After retesting the scenario with the enhanced set of cases when the state trie root hash is logged, we have got the same symptoms as when testing the scenario for the first time. Now we see that the reasons of the spontaneous restarts of the node during a huge catch-up are the node process falls caused by the Python *call stack overflow*. The state root of Node25 diverged from the state root of other nodes in the pool after the first such fall of Node25. All these call stack overflows were caused by a big depth of {{LedgerManager._processCatchupReplies}} recursion but the overflows themselves occurred deeper - in functions called from {{DomainRequestHandler.updateState}}. Thus the state was not updated completely at those moments.

The call stack overflows can be seen in the systemd journal of Node25. There are error tracebacks for all the node process falls. For all these tracebacks *the call stack depth is 1000* which is the *default* value of *the maximum depth of the Python interpreter stack*.

For example, here is the traceback of the first fall of the node process on Node25:

{code}
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]: Traceback (most recent call last):
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     looper.run()

...

Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 516, in processCatchupRep
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     ledgerId, ledger, txns_already_rcvd_in_catchup)
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 554, in _processCatchupReplies
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     catchUpReplies[toBeProcessed:])
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 554, in _processCatchupReplies
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     catchUpReplies[toBeProcessed:])
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 554, in _processCatchupReplies
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     catchUpReplies[toBeProcessed:])

...

Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 554, in _processCatchupReplies
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     catchUpReplies[toBeProcessed:])
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 554, in _processCatchupReplies
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     catchUpReplies[toBeProcessed:])
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 550, in _processCatchupReplies
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     ledgerInfo, txn)
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger_manager.py"", line 571, in _add_txn
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     ledgerInfo.postTxnAddedToLedgerClbk(ledgerId, txn)
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1985, in postTxnFromCatchupAddedToLedger
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     rh.updateState([txn], isCommitted=True)
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/domain_req_handler.py"", line 69, in updateState
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     self._updateStateWithSingleTxn(txn, isCommitted=isCommitted)

...

Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:   File ""/usr/lib/python3.5/_weakrefset.py"", line 75, in __contains__
Sep 20 19:12:47 saopauloQALive25.qatest.evernym.com env[2223]:     return wr in self.data
{code}

The recursive calls of {{LedgerManager._processCatchupReplies}} took the most part of the call stack.

Created INDY-1711 about this issue.;;;",,,,,,,,,,,,,,,,,,,,,,
Avoid computing the aggregate verkey (BLS) during processing of PrePrepare,INDY-1658,33316,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,lovesh,lovesh,29/Aug/18 8:15 PM,10/Oct/18 9:08 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Indy-crypto should expose API to compute aggregate verkey by acceppting a list of verkeys and provide an API to verify multi-sig using aggreated verkey. Thus the node can cache different aggregated verkeys. eg, in a 4 node pool, Node4 will cache 
1 for Node1, Node2 and Node3, 
1 for Node1 and Node2, 
1 for Node2 and Node 3 and 
1 for Node1 and Node3",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzzopr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),lovesh,sergey.khoroshavin,,,,,,,,,,,"10/Oct/18 9:08 PM;sergey.khoroshavin;*Triage*
In fact this seems more like a performance feature request, not a bug.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Performance of monitor should be improved,INDY-1659,33328,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,29/Aug/18 11:24 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Under high load when requests are received faster than they are ordered _monitor.requestOrdered_ starts taking significant portion of time, which leads to even slower ordering. This should be fixed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzoqn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"30/Aug/18 12:23 AM;sergey.khoroshavin;*PR* https://github.com/hyperledger/indy-plenum/pull/902;;;","30/Aug/18 4:40 PM;ashcherbakov;Duplicates INDY-1660;;;",,,,,,,,,,,,,,,,,,,,,,,
Performance of monitor should be improved,INDY-1660,33342,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,30/Aug/18 4:33 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Under high load when requests are received faster than they are ordered _monitor.requestOrdered_ starts taking significant portion of time, which leads to even slower ordering. This should be fixed.


There is a place in the monitor where we have quadratic asymptotic for 3PC batch size because we iterate twice.

We need to fix this, which can give us a benefit in performance, especially in case of DDoS where we may have huge 3PC batches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzzotr:",,,,Unset,Unset,EV 18.17 Service Pack,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"30/Aug/18 4:33 PM;ashcherbakov;Fixed in https://github.com/hyperledger/indy-plenum/pull/902;;;","30/Aug/18 5:39 PM;sergey.khoroshavin;*Problem reason*
There is a place in the monitor where we have quadratic asymptotic for 3PC batch size. It also gets worse with growing number of queued but still unordered requests.

*Changes*
Instead of iterating through all queued requests there is now request cache based on hash map so loop over queued requests is replaced with O(1) lookup which brings total asymptotic back to linear with 3PC batch size.

*PR*
https://github.com/hyperledger/indy-plenum/pull/902

*Versions*
indy-node >= 1.6.586
indy-plenum >= 1.6.529

*Risk*
Low

*Covered by tests*
https://github.com/hyperledger/indy-plenum/blob/4cdafcd1188e32b9d8c647d9b36705bf2726e747/plenum/test/monitoring/test_request_time_tracker.py

*Recommendations for QA*
Run load test against 25-node AWS pool which tries to write 40 NYMs per second. Make sure that at least 20 NYMs per second are ordered and this performance is sustained for at least half an hour.;;;",,,,,,,,,,,,,,,,,,,,,,,
Test domain transactions with FEEs,INDY-1661,33343,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,30/Aug/18 4:41 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.78,,,,0,,,,"We need to test stability and performance of the Ledger with FEEs enabled.

The load script was extended in the scope of INDY-1636 to support FEEs (FEEs support there will be continued in the scope of INDY-1605, but this is enough to start testing).

*Acceptance criteria*
 * Run load tests with FEEs enabled

 ** NYM write only
 *** 5 writes per sec
 *** 10 writes per sec
 *** 15 writes per sec
 ** NYM read and write
 *** 10 writes and 100 reads per sec
 ** NYM and SCHEMA
 *** 10 writes and 100 reads per sec
 * Check for Stability issues
 * Check for the number of view changes
 * Metrics output
 * Performance values",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/18 7:10 PM;ozheregelya;Ext-18-09-18-nym-fees-nym-read.png;https://jira.hyperledger.org/secure/attachment/15862/Ext-18-09-18-nym-fees-nym-read.png","26/Sep/18 3:32 AM;ozheregelya;Ext-25-09-18-nym-schema-fees-nym.png;https://jira.hyperledger.org/secure/attachment/15867/Ext-25-09-18-nym-schema-fees-nym.png","19/Sep/18 4:18 AM;ozheregelya;NYMs-fees-15-per-sec.png;https://jira.hyperledger.org/secure/attachment/15831/NYMs-fees-15-per-sec.png","03/Sep/18 11:43 PM;zhigunenko.dsr;nyms_15.png;https://jira.hyperledger.org/secure/attachment/15770/nyms_15.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwwof:",,,,Unset,Unset,EV 18.18 Service Pack 2,EV 18.19,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,zhigunenko.dsr,,,,,,,,,,"03/Sep/18 11:44 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.587
indy-plenum 1.6.529
server-plugin 0.9.1
server-plugin 0.9.1+5.87

*Test 1:*
Incoming: 15 writes (nym+fee) per sec
Ordered: 6 txns per sec
No view changes
!nyms_15.png!

*Test 2:*
Incoming: 9 writes (nym+fee) per sec
Ordered: 6-7 txns per sec
Single view change right after load start

*Test 3:*
Incoming: 7-8 writes (nym+fee) per sec
Ordered: {color:#008000}*7 txns per sec*{color}
No view changes;;;","03/Sep/18 11:54 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.587
indy-plenum 1.6.529
server-plugin 0.9.1
client-plugin 0.9.1+5.87

*Test 4:*
Incoming: 15 writes (schema) per sec
Ordered: up to 5 writes (schema + fee) per sec (but it isn't looks like a limit);;;","07/Sep/18 11:16 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.598
indy-plenum 1.6.538
server-plugin 0.9.2
client-plugin 0.9.2+1.90

*Test 5:*
Incoming: 10-15 writes (nym+fee) per sec
Ordered: 7 txns per sec
7 view changes (during 10 minutes right after script ran out of money);;;","19/Sep/18 4:26 AM;ozheregelya;*NYM write only (done)*
 * 5 writes per sec
 ** Pool processes 5 writes/sec well. No View Changes happened.
 * 10 writes per sec
 ** Pool processed 10 writes/sec for 24 hours and reached 737K txns in the ledger. Then it failed with OOM. No View Changes happened.
Test case ID in the 'Load and Performance' spreadsheet: Ext-14-09-18-nym-fees
 * 15 writes per sec
 ** Pool can't stable process 15 nyms with fees per sec. It can do in for some time (less than an hour), but then it become slower and can't reach 15 writes/sec until the load will be cancelled. But when the load was stopped, pool was able to reach up to 25 writes/sec. 
Pool worked with 15 NYMs with fees per sec during ~4 hours and processed all requests during ~4.5 hours. 158K txns were written. No View Changes happened.
!NYMs-fees-15-per-sec.png|thumbnail!
Test case ID in the 'Load and Performance' spreadsheet: Ext-18-09-18-nym-fees;;;","26/Sep/18 3:34 AM;ozheregelya;*Mix of read and write txns:*
 * NYMs writing + reading:
 ** Pool can process 10 nyms with fees and ~1000 reads without fees for 8 hours. 273771 txns were written. Pool was failed with OOM. 1 View Change was completed (reason 26).
 !Ext-18-09-18-nym-fees-nym-read.png|thumbnail!
 * NYMs and SCHEMA writing + mixed reading:
 ** Pool can process 10 nyms and schemas with fees and ~1000 reads without fees for 6 hours. 226641 txns were written. Pool was failed with OOM. 2 View Changes were completed (reason 25 and 26).
!Ext-25-09-18-nym-schema-fees-nym.png|thumbnail!

*Conclusion:*

Newly implemented txns with fees basically work and satisfy acceptance criteria from INDY-1343 (10 writes/sec for at least 3 hours). But in accordance with previous results it looks like performance and stability under load with fees worse than without fees:
|| ||Txns written before OOM
 (10 txns/sec load)||Pool throughput||
|NYMs|at least 419389|at least 20 nyms/sec|
|NYMs+fees|273739|14 nyms+fees/sec (with input load 15 txns/sec)|

Ticket for exploration of performance and stability degradation: INDY-1713.;;;",,,,,,,,,,,,,,,,,,,,
Load with unacceptable payments,INDY-1662,33352,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,30/Aug/18 9:15 PM,30/Mar/19 5:39 AM,28/Oct/23 2:47 AM,30/Mar/19 5:39 AM,,,,,,0,,,,"*Possible cases:*
1) input hasn't been set totally
2) inexistent payment-source
3) insufficient funds
4) extra funds

*Possible realization:*
CLI batch file",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1352,,,No,,Unset,No,,,"1|hzzovb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,zhigunenko.dsr,,,,,,,,,,,"28/Sep/18 6:48 AM;esplinr;I moved this payment specific testing to another team. TOK-445;;;",,,,,,,,,,,,,,,,,,,,,,,,
Find out maximal pool performance numbers for read only,INDY-1663,33353,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,30/Aug/18 10:08 PM,08/Oct/19 9:15 PM,28/Oct/23 2:47 AM,08/Oct/19 9:15 PM,,,,,,0,,,,"Find out maximal pool performance for reads only:
 * Only NYMs
 * Mixture mode as in INDY-1343
 * Payments only",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwx4v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,zhigunenko.dsr,,,,,,,,,,,"29/Nov/18 5:24 PM;zhigunenko.dsr;1) get_nym
indy-node 1.6.700 (25 nodes)
domain ledger - 110k, payment ledger - 154k

fact performance (total from 25 nodes): up to 7300 read/sec (potentially up to 8200 and more in case of bigger client number);;;",,,,,,,,,,,,,,,,,,,,,,,,
Find out maximal pool performance numbers for mixed mode (read and write),INDY-1664,33354,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,30/Aug/18 10:09 PM,09/Oct/19 6:14 PM,28/Oct/23 2:47 AM,09/Oct/19 6:14 PM,,,,,,0,,,,"Find out maximal pool performance for a mix of of writs and reads:
 * Mixture mode as in INDY-1343
 * Mixture mode with FEEs (for the txns we support FEEs in the load script)

Run strategy:
 * Consider have metrics enabled and increase the load starting from a small number. If according to metrics everything is fine, increase the load further.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzwxjz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"09/Oct/19 6:14 PM;ashcherbakov;We have https://jira.hyperledger.org/browse/INDY-2214 instead;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support all FEEs txns in the load script,INDY-1665,33355,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,30/Aug/18 10:16 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"As of now, we support FEEs for NYM and SCHEMA only. We need to support it for all Domain txns.

This needs to be dome after improvements in INDY-1605",,,,,,,,,,INDY-1605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwwon:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,VladimirWork,,,,,,,,,,"24/Sep/18 9:47 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/950;;;","26/Sep/18 8:03 PM;VladimirWork;INDY-1378 issue is appeared again with revoc_reg_entry txns with `-b 1`:
{noformat}
LoadClient_0 : LoadClient_0 generate req error ErrorCode.AnoncredsRevocationRegistryFullError

DONE At 2018-09-25 17:56:15.800983
Time 291.31 Clients 0/1 Sent: 65 Succ: 61 Failed: 2 Nacked: 0 Rejected: 3
Task exception was never retrieved
future: <Task finished coro=<LoadClient.gen_signed_req() done, defined at /usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py:167> exception=IndyError(<ErrorCode.AnoncredsRevocationRegistryFullError: 400>,)>
Traceback (most recent call last):
 File ""/usr/lib/python3.5/asyncio/tasks.py"", line 241, in _step
   result = coro.throw(exc)
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py"", line 178, in gen_signed_req
   raise e
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py"", line 171, in gen_signed_req
   req_data, req = await self._req_generator.generate_request(self._test_did)
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_req_gen.py"", line 81, in generate_request
   raise ex
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_req_gen.py"", line 77, in generate_request
   req = await self._gen_req(submit_did, req_data)
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_req_gen_revoc.py"", line 133, in _gen_req
   self._default_revoc_reg_def_id, self._blob_storage_reader_cfg_handle)
 File ""/usr/local/lib/python3.5/dist-packages/indy/anoncreds.py"", line 325, in issuer_create_credential
   issuer_create_credential.cb)
 File ""/usr/lib/python3.5/asyncio/futures.py"", line 361, in __iter__
   yield self  # This tells Task to wait for completion.
 File ""/usr/lib/python3.5/asyncio/tasks.py"", line 296, in _wakeup
   future.result()
 File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
   raise self._exception
indy.error.IndyError: ErrorCode.AnoncredsRevocationRegistryFullError
Task was destroyed but it is pending!
task: <Task pending coro=<LoadClient.gen_signed_req() running at /usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py:180> wait_for=<Future pending cb=[Task._wakeup()]> cb=[LoadClient.check_batch_avail()]>
{noformat}
;;;","27/Sep/18 7:54 PM;VladimirWork;Build Info:
indy-perf-load 1.0.5

Steps to Validate:
1. Run all writing txn types with fees separately with various load rate and client count.
2. Run mix of all writing txn types with fees together with various load rate and client count.

Actual Results:
Txns with fees are written to both ledgers successfully. All issues found are reported as separate tickets (TOK-425, TOK-449, INDY-1718).;;;",,,,,,,,,,,,,,,,,,,,,,
Write load scripts emulating non-smooth incrementing load ,INDY-1666,33356,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,ashcherbakov,ashcherbakov,30/Aug/18 10:24 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"We created an extension to load scripts emulating non-smooth load in the scope of INDY-1567.

*Acceptance criteria*

We need to improve the script to support the following patterns:
 * Permanently incrementing load
 * Permanently incrementing load with spikes",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1567,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwwtj:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,,,,,,,,,,,"05/Sep/18 11:36 PM;NataliaDracheva;Change reason: 
  - The previous version of spike script did not allow a user to configure gradual load.

  Changes: 
  - Added a possibility to provide initial and final rate, time of one step. Additional load rate per step is calculated automatically.
  - Added a possibility to see the configured load profile before execution, for this purpose the script should be launched with -g=True argument.  

  PR:
  - https://github.com/hyperledger/indy-node/pull/935


  Version:
  - master, build 626

  Risk factors:
  - Some configurations may lead to not desired load rate, please check graph before load testing in complicated cases.

  Risk:
  - Low

  Recommendations for QA
  Configurations which will raise errors:
* Final load rate less than initial load rate
* Difference between initial and final load rate divided to test/spike time should not be less than 1.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Extend load scripts emulating non-smooth load according to the changes in the core script,INDY-1667,33357,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,ashcherbakov,ashcherbakov,30/Aug/18 10:26 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"The core load script functionality will be changed in the scope of INDY-1605.

We need to support it in the scripts created in INDY-1567 and INDY-1666.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwwr3:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,,,,,,,,,,,"28/Sep/18 8:48 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/957;;;",,,,,,,,,,,,,,,,,,,,,,,,
Create Service Pack Release 1.6.73,INDY-1668,33368,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,31/Aug/18 12:25 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxcn:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,,,,,,,,,,,"05/Sep/18 9:48 PM;ozheregelya;Release has been approved.
Release notes: INDY-1679.
Results of acceptance testing are placed [here|https://docs.google.com/spreadsheets/d/1OVjua8JMwW7RhBWsdd9vSfGvJkgxWxKhEjB3T0yil1U/edit#gid=0] (column '1.6.73').;;;",,,,,,,,,,,,,,,,,,,,,,,,
validator-info periodically returns invalid JSON,INDY-1669,33384,,Bug,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dfarns,dfarns,31/Aug/18 5:21 AM,27/Mar/20 10:09 PM,28/Oct/23 2:47 AM,,,1.16.0,validator-info,,,0,,,,"Periodically running this command, results in non-JSON output and an error:
{code}
danfarnsworth@qstn-irl-qm001:~$ sudo validator-info --json
File /var/lib/indy/sandbox/node5_info.json has an invalid json format
{""Configuration"": {""Genesis_txns"": {""pool_txns"": [""{\""data\"":{\""alias\"":\""Node1\"",\""blskey\"":\""4N8aUNHSgjQVgkpm8nhNEfDf6txHznoYREg9kirmJrkivgL4oSEimFF6nsQ6M41QvhM2Z33nves5vfSn9n1UwNFJBYtWVnHYMATn76vLuL3zU88KyeAYcHfsih3He6UHcXDxcaecHVz6jhCYz1P2UZn2bDVruL5wXpehgBfBaLKm3Ba\
"",\""client_ip\"":\""54.233.136.74\"",\""client_port\"":9702,\""node_ip\"":\""54.233.136.74\"",\""node_port\"":9701,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv\"",\""identifier\"":\""Th7MpTaRZVRYnPiabds81Y\"",\""txnId\"":\""fea82e10e894419fe2bea7d96
296a6d46f50f93f9eeda954ec461b2ed2950b62\"",\""type\"":\""0\""}\r"", ""{\""data\"":{\""alias\"":\""Node2\"",\""blskey\"":\""37rAPpXVoxzKhz7d9gkUe52XuXryuLXoM6P6LbWDB7LSbG62Lsb33sfG7zqS8TK1MXwuCHj1FKNzVpsnafmqLG1vXN88rt38mNFs9TENzm4QHdBzsvCuoBnPH7rpYYDo9DZNJePaDvRvqJKByCabubJz3XXKbEeshzpz
4Ma5QYpJqjk\"",\""client_ip\"":\""13.228.112.216\"",\""client_port\"":9704,\""node_ip\"":\""13.228.112.216\"",\""node_port\"":9703,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb\"",\""identifier\"":\""EbP4aYNeTHL6q385GuVpRV\"",\""txnId\"":\""1ac8aece2a1
8ced660fef8694b61aac3af08ba875ce3026a160acbc3a3af35fc\"",\""type\"":\""0\""}\r"", ""{\""data\"":{\""alias\"":\""Node3\"",\""blskey\"":\""3WFpdbg7C5cnLYZwFZevJqhubkFALBfCBBok15GdrKMUhUjGsk3jV6QKj6MZgEubF7oqCafxNdkm7eswgA4sdKTRc82tLGzZBd6vNqU8dupzup6uYUf32KTHTPQbuUM8Yk4QFXjEf2Usu2TJcNkdgp
yeUSX42u5LqdDDpNSWUK5deC5\"",\""client_ip\"":\""13.54.146.111\"",\""client_port\"":9706,\""node_ip\"":\""13.54.146.111\"",\""node_port\"":9705,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya\"",\""identifier\"":\""4cU41vWW82ArfxJxHkzXPG\"",\""txnId\"":\
""7e9f355dffa78ed24668f0e0e369fd8c224076571c51e2ea8be5f26479edebe4\"",\""type\"":\""0\""}\r"", ""{\""data\"":{\""alias\"":\""Node4\"",\""blskey\"":\""2zN3bHM1m4rLz54MJHYSwvqzPchYp8jkHswveCLAEJVcX6Mm1wHQD1SkPYMzUDTZvWvhuE6VNAkK3KxVeEmsanSmvjVkReDeBEMxeDaayjcZjFGPydyey1qxBHmTvAnBKoPydvuTAq
x5f7YNNRAdeLmUi99gERUU7TD8KfAa6MpQ9bw\"",\""client_ip\"":\""13.113.117.92\"",\""client_port\"":9708,\""node_ip\"":\""13.113.117.92\"",\""node_port\"":9707,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA\"",\""identifier\"":\""TWwCRQRZ2ZHMJFn9TzLp7W\""
,\""txnId\"":\""aa5e817d7cc626170eca175822029339a444eb0ee8f0bd20d3b0b76e566fb008\"",\""type\"":\""0\""}\r"", ""{\""data\"":{\""alias\"":\""Node5\"",\""blskey\"":\""2JSLkTGhnG3ZzGoeuZufc7V1kF5wxHqTuSUbaudhwRJzsGZupNHs5igohLnsdcYG7kFj1JGC5aV2JuiJtDtHPKBeGw24ZmBJ44YYaqfCMi5ywNyP42aSjMkvjtHrGS
7oVoFbP4aG4aRaKZL3UZbbGcnGTK5kfacmBNKdPSQDyXGCoxB\"",\""client_ip\"":\""52.209.67.38\"",\""client_port\"":9710,\""node_ip\"":\""52.209.67.38\"",\""node_port\"":9709,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe\"",\""identifier\"":\""92PMXtzRGuTAhA
K5xPbwqq\"",\""txnId\"":\""5abef8bc27d85d53753c5b6ed0cd2e197998c21513a379bfcf44d9c7a73c3a7e\"",\""type\"":\""0\""}\r"", ""{\""data\"":{\""alias\"":\""Node6\"",\""blskey\"":\""3D5JAwAhjW5gik1ogKrnQaVrHY94e8E56iA5UifXjjYypMm2LifLiaRtgWJPiFA6uv2EiGy4MYByZ88Rmi8K3mUvb9TZeR9sdLBxsTdqrikeenac8ZVN
kdCaFmGWcw8xVGqgv9cs574YDj7nuLHbJUDXN17J2fzQiD83iVQVQHW1RuU\"",\""client_ip\"":\""35.170.106.44\"",\""client_port\"":9712,\""node_ip\"":\""35.170.106.44\"",\""node_port\"":9711,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8\"",\""identifier\"":\""Ha
N1iLFgVfM31ssY4obfYN\"",\""txnId\"":\""a23059dc16aaf4513f97ca91f272235e809f8bda8c40f6688b88615a2c318ff8\"",\""type\"":\""0\""}\r"", ""{\""data\"":{\""alias\"":\""Node7\"",\""blskey\"":\""4ahBpE7gVEhW2evVgS69EJeSyciwbbby67iQj4htsgdtCxxXsEHMS6oKVeEQvrBBgncHfAddQyTt7ZF1PcfMX1Gu3xsgnzBDcLzPBz6Z
doXwi3uDPEoDZHXeDp1AFj8cidhfBWzY1FfKZMvh1HYQX8zZWMw579pYs3SyNoWLNdsNd8Q\"",\""client_ip\"":\""52.60.212.231\"",\""client_port\"":9714,\""node_ip\"":\""52.60.212.231\"",\""node_port\"":9713,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW\"",\""ident
ifier\"":\""BgJMUfWjWZBDAsu251dtrF\"",\""txnId\"":\""e5f11aa7ec7091ca6c31a826eec885da7fcaa47611d03fdc3562b48247f179cf\"",\""type\"":\""0\""}\r"", """"], ""domain_txns"": [""{\""dest\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""0\"",\""type\"":\""1\"",\""verkey\"":\""~CoRER63DVYnWZtK8uAzNbx\""}\r"", ""{\""
dest\"":\""Th7MpTaRZVRYnPiabds81Y\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~7TYfekw4GUagBnBVCqPjiC\""}\r"", ""{\""dest\"":\""EbP4aYNeTHL6q385GuVpRV\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":
\""~RHGNtfvkgPEUQzQNtNxLNu\""}\r"", ""{\""dest\"":\""4cU41vWW82ArfxJxHkzXPG\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~EMoPA6HrpiExVihsVfxD3H\""}\r"", ""{\""dest\"":\""TWwCRQRZ2ZHMJFn9TzLp7W\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""ro
le\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~UhP7K35SAXbix1kCQV4Upx\""}\r"", ""{\""dest\"":\""92PMXtzRGuTAhAK5xPbwqq\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~7zT6QFx9vRjtHUTmfLxEfz\""}\r"", ""{\""dest\"":\""HaN1iLFgVfM31ssY4obfYN\"",\""identif
ier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~MLaCVL1C6uA4zMHPbagpuG\""}\r"", ""{\""dest\"":\""BgJMUfWjWZBDAsu251dtrF\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~GgF8cUaDwpHF8CsVk8pkQR\""}\r"", ""{\""dest\""
:\""7JhapNNMLnwkbiC2ZmPZSE\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""type\"":\""1\"",\""verkey\"":\""~LgpYPrzkB6awcHMTPZ9TVn\""}\r"", """"]}, ""indy-node.service"": [""[Unit]"", ""Description=Indy Node"", ""Requires=indy-node-control.service"", """", ""[Service]"", ""EnvironmentFile=/etc/in
dy/indy.env"", ""ExecStart=/usr/bin/env python3 -O /usr/local/bin/start_indy_node ${NODE_NAME} ${NODE_IP} ${NODE_PORT} ${NODE_CLIENT_IP} ${NODE_CLIENT_PORT}"", ""User=indy"", ""Group=indy"", ""Restart=on-failure"", ""RestartSec=10"", ""StartLimitBurst=10"", ""StartLimitInterval=200"", 
""TimeoutSec=300"", ""LimitNOFILE=65536:131072"", """", ""[Install]"", ""WantedBy=multi-user.target"", """"], ""indy.env"": [""NODE_NAME=Node5"", ""NODE_PORT=9709"", ""NODE_CLIENT_PORT=9710"", """", ""CLIENT_CONNECTIONS_LIMIT=10000"", ""NODE_IP=0.0.0.0"", ""NODE_CLIENT_IP=0.0.0.0"", """"], ""indy-node
-control.service"": [""[Unit]"", ""Description=Service for upgrade of existing Indy Node and other operations"", ""#Requires=indy.service"", ""#After=indy.service"", ""After=network.target"", """", ""[Service]"", ""Type=simple"", ""EnvironmentFile=/etc/indy/node_control.conf"", ""ExecStart=
/usr/bin/env python3 -O /usr/local/bin/start_node_control_tool $TEST_MODE --hold-ext ${HOLD_EXT}"", ""Restart=on-failure"", ""RestartSec=10"", ""StartLimitBurst=10"", ""StartLimitInterval=200"", ""TimeoutSec=300"", """", ""[Install]"", ""WantedBy=multi-user.target"", """"], ""node_control.c
onf"": [""# Uncomment this to run agent in test mode:"", ""#TEST_MODE=--test"", """", ""TEST_MODE="", ""HOLD_EXT=\""sovrin \"""", """"], ""Config"": {""User_config"": [], ""Main_config"": [""tokenTransactionsFile = \""token_transactions\"""", ""sovtokenTransactionsFile = \""sovtoken_transactions\""
"", ""enableStdOutLogging=False"", ""baseDir = '/var/lib/indy'"", ""NODE_BASE_DATA_DIR = baseDir"", ""LOG_DIR = '/var/log/indy'"", ""BACKUP_DIR = '/var/lib/indy/backup'"", ""CLI_BASE_DIR = '~/.indy-cli/'"", ""CLI_NETWORK_DIR = '~/.indy-cli/networks'"", ""NODE_BASE_DATA_DIR = baseDir"", ""
NETWORK_NAME = 'sandbox'"", ""# Directory to store ledger."", ""LEDGER_DIR = '/var/lib/indy'"", """", ""# Directory to store logs."", ""LOG_DIR = '/var/log/indy'"", """", ""# Directory to store keys."", ""KEYS_DIR = '/var/lib/indy'"", """", ""# Directory to store genesis transactions files.
"", ""GENESIS_DIR = '/var/lib/indy'"", """", ""# Directory to store backups."", ""BACKUP_DIR = '/var/lib/indy/backup'"", """", ""# Directory to store plugins."", ""PLUGINS_DIR = '/var/lib/indy/plugins'"", """", ""# Directory to store node info."", ""NODE_INFO_DIR = '/var/lib/indy'"", """", """",
 ""ENABLED_PLUGINS=[]"", """", """", """", """", """", """", """", """", """", """", ""ANYONE_CAN_WRITE = False"", """", ""UPGRADE_ENTRY = 'sovrin'"", """", """", """", ""ENABLED_PLUGINS.append('sovtoken')"", """", ""ENABLED_PLUGINS.append('sovtokenfees')"", """"], ""Network_config"": []}, ""iptables_config"": []}}
 {code}

Notice that first line:
{code}
File /var/lib/indy/sandbox/node5_info.json has an invalid json format
{code}

I tried running a check on all the nodes in our QA validation pool, and if I got that error, then parsing that file right after getting the error, and was able to parse as valid json:
*NOTE*: The madsu command below is a tool I wrote that loops through a list of hosts, ssh's to them and executes commands/scripts. The execution can be done sequentially or in parallel, while also prompting for a sudo password one time (if the command needs it), instead of for each host. The example below executes my bash script, against all of the hosts in my {{qa-stn}} list all in parallel.

The script, being executed, in a more nicely formatted way is:
{code}
if sudo validator-info --json 2>&1 | grep -q ""invalid json format"" ; then
    cat /var/lib/indy/sandbox/*_info.json | python -m json.tool
else
    echo Good
fi
{code}

Checking all nodes in my pool, and try to parse the contents of the {{_info.json}} if the error was detected

{code}
danf@dans-dell-evernym:~/git_repos/gitlab/Ops-tools/madsu> madsu -l qa-stn -c 'if sudo validator-info --json 2>&1 | grep -q ""invalid json format"" ; then cat /var/lib/indy/sandbox/*_info.json | python -m json.tool ; else echo Good ;fi' -p 30
Detected 'sudo' commands in bash or sh shells
qstn-nva-qm001.qa.evernym.com: Good
qstn-can-qm001.qa.evernym.com: Good
qstn-irl-qm001.qa.evernym.com: Good
qstn-tky-qm001.qa.evernym.com: Good
qstn-syd-qm001.qa.evernym.com: Good
qstn-sao-qm001.qa.evernym.com: {
qstn-sao-qm001.qa.evernym.com:     ""Configuration"": {
qstn-sao-qm001.qa.evernym.com:         ""Config"": {
qstn-sao-qm001.qa.evernym.com:             ""Main_config"": [
qstn-sao-qm001.qa.evernym.com:                 ""tokenTransactionsFile = \""token_transactions\"""",
qstn-sao-qm001.qa.evernym.com:                 ""enableStdOutLogging=False"",
qstn-sao-qm001.qa.evernym.com:                 ""baseDir = '/var/lib/indy'"",
qstn-sao-qm001.qa.evernym.com:                 ""NODE_BASE_DATA_DIR = baseDir"",
qstn-sao-qm001.qa.evernym.com:                 ""LOG_DIR = '/var/log/indy'"",
qstn-sao-qm001.qa.evernym.com:                 ""BACKUP_DIR = '/var/lib/indy/backup'"",
qstn-sao-qm001.qa.evernym.com:                 ""CLI_BASE_DIR = '~/.indy-cli/'"",
qstn-sao-qm001.qa.evernym.com:                 ""CLI_NETWORK_DIR = '~/.indy-cli/networks'"",
qstn-sao-qm001.qa.evernym.com:                 ""NODE_BASE_DATA_DIR = baseDir"",
qstn-sao-qm001.qa.evernym.com:                 ""NETWORK_NAME = 'sandbox'"",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store ledger."",
qstn-sao-qm001.qa.evernym.com:                 ""LEDGER_DIR = '/var/lib/indy'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store logs."",
qstn-sao-qm001.qa.evernym.com:                 ""LOG_DIR = '/var/log/indy'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store keys."",
qstn-sao-qm001.qa.evernym.com:                 ""KEYS_DIR = '/var/lib/indy'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store genesis transactions files."",
qstn-sao-qm001.qa.evernym.com:                 ""GENESIS_DIR = '/var/lib/indy'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store backups."",
qstn-sao-qm001.qa.evernym.com:                 ""BACKUP_DIR = '/var/lib/indy/backup'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store plugins."",
qstn-sao-qm001.qa.evernym.com:                 ""PLUGINS_DIR = '/var/lib/indy/plugins'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""# Directory to store node info."",
qstn-sao-qm001.qa.evernym.com:                 ""NODE_INFO_DIR = '/var/lib/indy'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""ENABLED_PLUGINS=[]"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""ANYONE_CAN_WRITE = False"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""UPGRADE_ENTRY = 'sovrin'"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""ENABLED_PLUGINS.append('sovtoken')"",
qstn-sao-qm001.qa.evernym.com:                 """",
qstn-sao-qm001.qa.evernym.com:                 ""ENABLED_PLUGINS.append('sovtokenfees')"",
qstn-sao-qm001.qa.evernym.com:                 """"
qstn-sao-qm001.qa.evernym.com:             ],
qstn-sao-qm001.qa.evernym.com:             ""Network_config"": [],
qstn-sao-qm001.qa.evernym.com:             ""User_config"": []
qstn-sao-qm001.qa.evernym.com:         },
qstn-sao-qm001.qa.evernym.com:         ""Genesis_txns"": {
qstn-sao-qm001.qa.evernym.com:             ""domain_txns"": [
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""0\"",\""type\"":\""1\"",\""verkey\"":\""~CoRER63DVYnWZtK8uAzNbx\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""Th7MpTaRZVRYnPiabds81Y\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~7TYfekw4GUagBnBVCqPjiC\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""EbP4aYNeTHL6q385GuVpRV\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~RHGNtfvkgPEUQzQNtNxLNu\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""4cU41vWW82ArfxJxHkzXPG\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~EMoPA6HrpiExVihsVfxD3H\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""TWwCRQRZ2ZHMJFn9TzLp7W\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~UhP7K35SAXbix1kCQV4Upx\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""92PMXtzRGuTAhAK5xPbwqq\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~7zT6QFx9vRjtHUTmfLxEfz\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""HaN1iLFgVfM31ssY4obfYN\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~MLaCVL1C6uA4zMHPbagpuG\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""BgJMUfWjWZBDAsu251dtrF\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""role\"":\""2\"",\""type\"":\""1\"",\""verkey\"":\""~GgF8cUaDwpHF8CsVk8pkQR\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""dest\"":\""7JhapNNMLnwkbiC2ZmPZSE\"",\""identifier\"":\""V4SGRU86Z58d6TV7PBUe6f\"",\""type\"":\""1\"",\""verkey\"":\""~LgpYPrzkB6awcHMTPZ9TVn\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 """"
qstn-sao-qm001.qa.evernym.com:             ],
qstn-sao-qm001.qa.evernym.com:             ""pool_txns"": [
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node1\"",\""blskey\"":\""4N8aUNHSgjQVgkpm8nhNEfDf6txHznoYREg9kirmJrkivgL4oSEimFF6nsQ6M41QvhM2Z33nves5vfSn9n1UwNFJBYtWVnHYMATn76vLuL3zU88KyeAYcHfsih3He6UHcXDxcaecHVz6jhCYz1P2UZn2bDVruL5wXpehgBfBaLKm3Ba\"",\""client_ip\"":\""54.233.136.74\"",\""client_port\"":9702,\""node_ip\"":\""54.233.136.74\"",\""node_port\"":9701,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv\"",\""identifier\"":\""Th7MpTaRZVRYnPiabds81Y\"",\""txnId\"":\""fea82e10e894419fe2bea7d96296a6d46f50f93f9eeda954ec461b2ed2950b62\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node2\"",\""blskey\"":\""37rAPpXVoxzKhz7d9gkUe52XuXryuLXoM6P6LbWDB7LSbG62Lsb33sfG7zqS8TK1MXwuCHj1FKNzVpsnafmqLG1vXN88rt38mNFs9TENzm4QHdBzsvCuoBnPH7rpYYDo9DZNJePaDvRvqJKByCabubJz3XXKbEeshzpz4Ma5QYpJqjk\"",\""client_ip\"":\""13.228.112.216\"",\""client_port\"":9704,\""node_ip\"":\""13.228.112.216\"",\""node_port\"":9703,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb\"",\""identifier\"":\""EbP4aYNeTHL6q385GuVpRV\"",\""txnId\"":\""1ac8aece2a18ced660fef8694b61aac3af08ba875ce3026a160acbc3a3af35fc\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node3\"",\""blskey\"":\""3WFpdbg7C5cnLYZwFZevJqhubkFALBfCBBok15GdrKMUhUjGsk3jV6QKj6MZgEubF7oqCafxNdkm7eswgA4sdKTRc82tLGzZBd6vNqU8dupzup6uYUf32KTHTPQbuUM8Yk4QFXjEf2Usu2TJcNkdgpyeUSX42u5LqdDDpNSWUK5deC5\"",\""client_ip\"":\""13.54.146.111\"",\""client_port\"":9706,\""node_ip\"":\""13.54.146.111\"",\""node_port\"":9705,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya\"",\""identifier\"":\""4cU41vWW82ArfxJxHkzXPG\"",\""txnId\"":\""7e9f355dffa78ed24668f0e0e369fd8c224076571c51e2ea8be5f26479edebe4\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node4\"",\""blskey\"":\""2zN3bHM1m4rLz54MJHYSwvqzPchYp8jkHswveCLAEJVcX6Mm1wHQD1SkPYMzUDTZvWvhuE6VNAkK3KxVeEmsanSmvjVkReDeBEMxeDaayjcZjFGPydyey1qxBHmTvAnBKoPydvuTAqx5f7YNNRAdeLmUi99gERUU7TD8KfAa6MpQ9bw\"",\""client_ip\"":\""13.113.117.92\"",\""client_port\"":9708,\""node_ip\"":\""13.113.117.92\"",\""node_port\"":9707,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA\"",\""identifier\"":\""TWwCRQRZ2ZHMJFn9TzLp7W\"",\""txnId\"":\""aa5e817d7cc626170eca175822029339a444eb0ee8f0bd20d3b0b76e566fb008\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node5\"",\""blskey\"":\""2JSLkTGhnG3ZzGoeuZufc7V1kF5wxHqTuSUbaudhwRJzsGZupNHs5igohLnsdcYG7kFj1JGC5aV2JuiJtDtHPKBeGw24ZmBJ44YYaqfCMi5ywNyP42aSjMkvjtHrGS7oVoFbP4aG4aRaKZL3UZbbGcnGTK5kfacmBNKdPSQDyXGCoxB\"",\""client_ip\"":\""52.209.67.38\"",\""client_port\"":9710,\""node_ip\"":\""52.209.67.38\"",\""node_port\"":9709,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe\"",\""identifier\"":\""92PMXtzRGuTAhAK5xPbwqq\"",\""txnId\"":\""5abef8bc27d85d53753c5b6ed0cd2e197998c21513a379bfcf44d9c7a73c3a7e\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node6\"",\""blskey\"":\""3D5JAwAhjW5gik1ogKrnQaVrHY94e8E56iA5UifXjjYypMm2LifLiaRtgWJPiFA6uv2EiGy4MYByZ88Rmi8K3mUvb9TZeR9sdLBxsTdqrikeenac8ZVNkdCaFmGWcw8xVGqgv9cs574YDj7nuLHbJUDXN17J2fzQiD83iVQVQHW1RuU\"",\""client_ip\"":\""35.170.106.44\"",\""client_port\"":9712,\""node_ip\"":\""35.170.106.44\"",\""node_port\"":9711,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8\"",\""identifier\"":\""HaN1iLFgVfM31ssY4obfYN\"",\""txnId\"":\""a23059dc16aaf4513f97ca91f272235e809f8bda8c40f6688b88615a2c318ff8\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 ""{\""data\"":{\""alias\"":\""Node7\"",\""blskey\"":\""4ahBpE7gVEhW2evVgS69EJeSyciwbbby67iQj4htsgdtCxxXsEHMS6oKVeEQvrBBgncHfAddQyTt7ZF1PcfMX1Gu3xsgnzBDcLzPBz6ZdoXwi3uDPEoDZHXeDp1AFj8cidhfBWzY1FfKZMvh1HYQX8zZWMw579pYs3SyNoWLNdsNd8Q\"",\""client_ip\"":\""52.60.212.231\"",\""client_port\"":9714,\""node_ip\"":\""52.60.212.231\"",\""node_port\"":9713,\""services\"":[\""VALIDATOR\""]},\""dest\"":\""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW\"",\""identifier\"":\""BgJMUfWjWZBDAsu251dtrF\"",\""txnId\"":\""e5f11aa7ec7091ca6c31a826eec885da7fcaa47611d03fdc3562b48247f179cf\"",\""type\"":\""0\""}\r"",
qstn-sao-qm001.qa.evernym.com:                 """"
qstn-sao-qm001.qa.evernym.com:             ]
qstn-sao-qm001.qa.evernym.com:         },
qstn-sao-qm001.qa.evernym.com:         ""indy-node-control.service"": [
qstn-sao-qm001.qa.evernym.com:             ""[Unit]"",
qstn-sao-qm001.qa.evernym.com:             ""Description=Service for upgrade of existing Indy Node and other operations"",
qstn-sao-qm001.qa.evernym.com:             ""#Requires=indy.service"",
qstn-sao-qm001.qa.evernym.com:             ""#After=indy.service"",
qstn-sao-qm001.qa.evernym.com:             ""After=network.target"",
qstn-sao-qm001.qa.evernym.com:             """",
qstn-sao-qm001.qa.evernym.com:             ""[Service]"",
qstn-sao-qm001.qa.evernym.com:             ""Type=simple"",
qstn-sao-qm001.qa.evernym.com:             ""EnvironmentFile=/etc/indy/node_control.conf"",
qstn-sao-qm001.qa.evernym.com:             ""ExecStart=/usr/bin/env python3 -O /usr/local/bin/start_node_control_tool $TEST_MODE --hold-ext ${HOLD_EXT}"",
qstn-sao-qm001.qa.evernym.com:             ""Restart=on-failure"",
qstn-sao-qm001.qa.evernym.com:             ""RestartSec=10"",
qstn-sao-qm001.qa.evernym.com:             ""StartLimitBurst=10"",
qstn-sao-qm001.qa.evernym.com:             ""StartLimitInterval=200"",
qstn-sao-qm001.qa.evernym.com:             ""TimeoutSec=300"",
qstn-sao-qm001.qa.evernym.com:             """",
qstn-sao-qm001.qa.evernym.com:             ""[Install]"",
qstn-sao-qm001.qa.evernym.com:             ""WantedBy=multi-user.target"",
qstn-sao-qm001.qa.evernym.com:             """"
qstn-sao-qm001.qa.evernym.com:         ],
qstn-sao-qm001.qa.evernym.com:         ""indy-node.service"": [
qstn-sao-qm001.qa.evernym.com:             ""[Unit]"",
qstn-sao-qm001.qa.evernym.com:             ""Description=Indy Node"",
qstn-sao-qm001.qa.evernym.com:             ""Requires=indy-node-control.service"",
qstn-sao-qm001.qa.evernym.com:             """",
qstn-sao-qm001.qa.evernym.com:             ""[Service]"",
qstn-sao-qm001.qa.evernym.com:             ""EnvironmentFile=/etc/indy/indy.env"",
qstn-sao-qm001.qa.evernym.com:             ""ExecStart=/usr/bin/env python3 -O /usr/local/bin/start_indy_node ${NODE_NAME} ${NODE_IP} ${NODE_PORT} ${NODE_CLIENT_IP} ${NODE_CLIENT_PORT}"",
qstn-sao-qm001.qa.evernym.com:             ""User=indy"",
qstn-sao-qm001.qa.evernym.com:             ""Group=indy"",
qstn-sao-qm001.qa.evernym.com:             ""Restart=on-failure"",
qstn-sao-qm001.qa.evernym.com:             ""RestartSec=10"",
qstn-sao-qm001.qa.evernym.com:             ""StartLimitBurst=10"",
qstn-sao-qm001.qa.evernym.com:             ""StartLimitInterval=200"",
qstn-sao-qm001.qa.evernym.com:             ""TimeoutSec=300"",
qstn-sao-qm001.qa.evernym.com:             ""LimitNOFILE=65536:131072"",
qstn-sao-qm001.qa.evernym.com:             """",
qstn-sao-qm001.qa.evernym.com:             ""[Install]"",
qstn-sao-qm001.qa.evernym.com:             ""WantedBy=multi-user.target"",
qstn-sao-qm001.qa.evernym.com:             """"
qstn-sao-qm001.qa.evernym.com:         ],
qstn-sao-qm001.qa.evernym.com:         ""indy.env"": [
qstn-sao-qm001.qa.evernym.com:             ""NODE_NAME=Node1"",
qstn-sao-qm001.qa.evernym.com:             ""NODE_PORT=9701"",
qstn-sao-qm001.qa.evernym.com:             ""NODE_CLIENT_PORT=9702"",
qstn-sao-qm001.qa.evernym.com:             """",
qstn-sao-qm001.qa.evernym.com:             ""CLIENT_CONNECTIONS_LIMIT=10000"",
qstn-sao-qm001.qa.evernym.com:             ""NODE_IP=0.0.0.0"",
qstn-sao-qm001.qa.evernym.com:             ""NODE_CLIENT_IP=0.0.0.0"",
qstn-sao-qm001.qa.evernym.com:             """"
qstn-sao-qm001.qa.evernym.com:         ],
qstn-sao-qm001.qa.evernym.com:         ""iptables_config"": [],
qstn-sao-qm001.qa.evernym.com:         ""node_control.conf"": [
qstn-sao-qm001.qa.evernym.com:             ""# Uncomment this to run agent in test mode:"",
qstn-sao-qm001.qa.evernym.com:             ""#TEST_MODE=--test"",
qstn-sao-qm001.qa.evernym.com:             """",
qstn-sao-qm001.qa.evernym.com:             ""TEST_MODE="",
qstn-sao-qm001.qa.evernym.com:             ""HOLD_EXT=\""sovrin \"""",
qstn-sao-qm001.qa.evernym.com:             """"
qstn-sao-qm001.qa.evernym.com:         ]
qstn-sao-qm001.qa.evernym.com:     }
qstn-sao-qm001.qa.evernym.com: }
qstn-sgp-qm001.qa.evernym.com: Good
{code}

 I'm not sure what is actually causing the error, but something somewhere is not happy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-984,,,No,,Unset,No,,,"1|hzwvif:00001ywbr",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dfarns,,,,,,,,,,,"31/Aug/18 3:36 PM;ashcherbakov;[~dfarns] What is the version of IndyNode on that Pool? Can you please make sure that this is the same version on all machines?;;;","31/Aug/18 10:39 PM;dfarns;{code:java}
danf@dans-dell-evernym:~/git_repos/gitlab/Ops-tools/madsu> madsu -l 'qa-stn' -c 'dpkg -l | grep indy-node' -p 20
qstn-can-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node
qstn-nva-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node
qstn-tky-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node
qstn-irl-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node
qstn-syd-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node
qstn-sgp-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node
qstn-sao-qm001.qa.evernym.com: hi  indy-node                        1.6.71                                     amd64        Indy node {code};;;",,,,,,,,,,,,,,,,,,,,,,,
Whole pool was restarted with OOM after high load ,INDY-1670,33404,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,ozheregelya,ozheregelya,31/Aug/18 10:37 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"*Environment:*
 indy-node 1.6.588

*Steps to Validate:*
 1. Set up the pool of 25 nodes.
 2. Run load with 70 writing txns and 750 reading txns/sec.

*Actual Results:*
 Whole pool was restarted with OOM after 12K txns written.

*Expected Results:*
 Need to figure out what was changed since 1.5.543 version because on this version we were able to write up to 112K txns.

*Logs and metrics:* 
s3://qanodelogs/indy-1670/NodeXX/
To get logs, run following command on log processor machine: 
aws s3 cp --recursive s3://qanodelogs/indy-1670/ /home/ev/logs/indy-1670/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzo1z:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,Toktar,,,,,,,,,,"04/Sep/18 9:43 PM;ozheregelya;Similar problem with normal load (10 writes/sec): s3://qanodelogs/indy-1670-normal-load/NodeXX/
 Note that before normal load there was short time high load, and before high load pool was working with low load ~1 day.;;;","06/Sep/18 8:03 PM;Toktar;After analyzing logs, two main problems were identified:

1) Often View changes with ""master degraded"". Problem with a long latency will fix in a new strategy in the task INDY-1639

2) One of the problems with the current implementation of View Change is different prepared certificates on different nodes. It's a known problem in PBFT.
 F nodes can have a larger preperd certificate for two reasons:
  - Network delays. All nodes sent prepare messages, but some be in time to collect a quorum of them, some are not, as a result, a minority of nodes can have a higher or lower certificate.
  - All messages from all nodes can reach us, but at the time of collecting a prepare certificate they are still in the queue, and we have not processed them yet.
 It's a known problem in PBFT and can be solve via change a logic of View Change. As a hot fix we can processing the maximum from the node stack before setting a prepare certificate.;;;","06/Sep/18 10:13 PM;ashcherbakov;One of the reasons why prepared certificates may be so different on different nodes in the pool was identified: this is because we may have different number of messaged queued in Node-to-Node message queue. This will be addressed in https://jira.hyperledger.org/browse/INDY-1687;;;",,,,,,,,,,,,,,,,,,,,,,
Need to find a way to handle OOM more safely for node.,INDY-1671,33413,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ozheregelya,ozheregelya,01/Sep/18 6:55 AM,15/Oct/18 11:52 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Node can stop working after restarts because of OOM. Need to find a way to manage situation with lack of memory more safely for node.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzp53:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,ozheregelya,,,,,,,,,,,"15/Oct/18 11:52 PM;esplinr;When the node service fails due to OOM, it automatically restarts. This could recreate the same issue that caused it to fail previously.

A few ways we can improve the situation:
* Notify the admin that we are failing due to OOM
* Devise a way to change the system on the reload so as not to fail;;;",,,,,,,,,,,,,,,,,,,,,,,,
Throughput critically decreases without causing view_change,INDY-1672,33435,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,zhigunenko.dsr,zhigunenko.dsr,03/Sep/18 6:16 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"1*Steps to Reproduce:*
 1) Prepare pool with 25 nodes
 2) run load test from 3 instances:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym
{code}
*Expected Results:*
 Pool handles about 18 txns/sec or more. View change happens if needed

*Actual Results:*
 Throughput slows down to 0. View change hasn't happen

*Additional Info:*
 !1.png!
 Possibly caused by fix from INDY-1642
 Logs (+metrics + other stats) are available here: _logProcessor:~/logs/throughput_decreasing_","indy-anoncreds 1.0.32
indy-node 1.6.590
indy-plenum 1.6.533
libindy-crypto 0.4.0
python3-indy-crypto 0.4.3",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1642,,,,,,,,,,,,,,,,,,,,"03/Sep/18 6:15 PM;zhigunenko.dsr;1.png;https://jira.hyperledger.org/secure/attachment/15769/1.png","04/Sep/18 10:36 PM;NataliaDracheva;1672_Master_586.png;https://jira.hyperledger.org/secure/attachment/15775/1672_Master_586.png","04/Sep/18 10:32 PM;NataliaDracheva;1672_RC_73.png;https://jira.hyperledger.org/secure/attachment/15774/1672_RC_73.png","04/Sep/18 10:42 PM;NataliaDracheva;1672_RC_get_metrics_output.txt;https://jira.hyperledger.org/secure/attachment/15776/1672_RC_get_metrics_output.txt","04/Sep/18 10:42 PM;NataliaDracheva;1672_RC_metrics.csv;https://jira.hyperledger.org/secure/attachment/15777/1672_RC_metrics.csv","12/Nov/18 2:16 PM;seokm0;image.png;https://jira.hyperledger.org/secure/attachment/16252/image.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwtr:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,zhigunenko.dsr,,,,,,,,,,"04/Sep/18 8:17 PM;ashcherbakov;It looks like this is caused by huge transport batches, so the looper process them for ~10 secs, and hence everything is slowed down.

Number of messages in one transport batch:
    11520 samples, 1539857.00 None, 1.00/133.67/360.00 min/avg/max, 113.10 stddev

Node message size, bytes:
    Outgoing: 11503 samples, 614338081.00 None, 2.00/53406.77/131070.00 min/avg/max, 42032.58 stddev
    Incoming: 9841 samples, 387978683.00 None, 2.00/39424.72/131072.00 min/avg/max, 40372.12 stddev

NODE_PROD_TIME : 41957 samples, 2995.69 seconds, 2.52/71.40/27386.67 ms min/avg/max, 811.04 stddev

SERVICE_NODE_MSGS_TIME : 41958 samples, 1987.87 seconds, 0.53/47.38/17537.82 ms min/avg/max, 515.21 stddev

SERVICE_NODE_STACK_TIME : 41958 samples, 1509.35 seconds, 0.34/35.97/14055.15 ms min/avg/max, 387.47 stddev

PROCESS_NODE_INBOX_TIME : 41958 samples, 475.67 seconds, 0.00/11.34/3542.65 ms min/avg/max, 130.58 stddev

FLUSH_OUTBOXES_TIME : 41957 samples, 525.71 seconds, 0.01/12.53/21056.10 ms min/avg/max, 307.95 stddev

UNPACK_BATCH_TIME : 9268 samples, 1412.31 seconds, 0.41/152.39/2482.74 ms min/avg/max, 183.60 stddev

 

 

Next Steps:
 * Test and apply fixes from INDY-1602
 * Check whether we create transport batches efficiently (there was a recent change which changed the algorithm)

 

 ;;;","04/Sep/18 9:55 PM;ashcherbakov;Results of comparing performance of the Batch creation from Propagate msgs (since Propagate is the most frequent one):
 * Batch Size = 10:
 ** Current way: 0.0002
 ** Old way: 0.00005
 * Batch Size = 300:
 ** Current way: 0.03 sec
 ** Old way: 0.0003 sec
 * Batch Size = 1000:
 ** Current way: 0.33
 ** Old way: 0.0008;;;","04/Sep/18 10:37 PM;NataliaDracheva;*Test run 1:*
 *Environment:*
indy-node 1.6.73 (RC)
indy-plenum 1.6.51

*Steps to Validate:*
1) prepare pool with 25 nodes
2) run load test from 3 instances::
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym
{code}
*Actual results:*
Pool handles about 20 txns/sec for 30 minutes without any issues. View change happens if needed (has happened once per test)
 !1672_RC_73.png|thumbnail! 

*Test run 2:*
 *Environment:*
indy-node 1.6.586 (Master)
indy-plenum 1.6.529.dev714

*Steps to Validate:*
1) prepare pool with 25 nodes
2) run load test from 3 instances::
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 10.00000 -y one -k nym
{code}
*Actual results:*
Pool handles about 20 txns/sec for 120 minutes without any issues. View change happens if needed.
 !1672_Master_586.png|thumbnail! 


*metrics: \\iserver\exchange\Evernym\INDY-1672*;;;","04/Sep/18 10:44 PM;ashcherbakov;I think the ticket can be closed. The issue will be addressed in the following tickets:
 * INDY-1602 (dynamic quotas to prevent huge batches)
 * INDY-1677 (to fix batch creation performance)
 * INDY-1649 (fix performance of propagate signature verification, which should also prevent huge batches);;;",,,,,,,,,,,,,,,,,,,,,
Need to call a PRE_SEND_REPLY hook when request is already successfully written to ledger,INDY-1673,33441,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,KitHat,KitHat,03/Sep/18 8:45 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,blocked,,,"If some request writes transaction for example to domain ledger and to some plugged one, it will be returned as two transactions one inside another in some special field. However, when we send the same request the second time, ledger will send the response from transaction log only from domain ledger, disregarding the plugged one. It can be solved with the call to the PRE_SEND_REPLY hook.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwqv:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,KitHat,VladimirWork,,,,,,,,,,"12/Sep/18 3:47 AM;Derashe;Problem reason:
 * We need to implement pre_send_reply hook for:
 ** On reply on write request
 ** On reply on write already written request
 ** On reply on get txn request

Changes: 
 * hook impemented

Covered with tests:
 * [plenum/test/plugin/test_hooks.py|https://github.com/hyperledger/indy-plenum/pull/915/files#diff-53a9a325c794e016117c74a02d63277d]

Commit info:
 * [https://github.com/hyperledger/indy-plenum/pull/915]

Recomendation for QA: 
 * Install some plugins whitch affect pre_send_reply_hook and check if they work as follows in 3 cases described in problem reason;;;","18/Sep/18 10:28 PM;VladimirWork;Not in master and no fixes on plugin side.;;;","18/Sep/18 10:39 PM;Derashe;Last indy-node ver. 1.6.607 include this fix;;;","19/Sep/18 5:49 PM;VladimirWork;There is no corresponding changes in the sovtoken plugins and they cannot be added before this ticket changes will be pushed to the stable so acceptance criteria for this ticket is just that we can install and run 1.6.607 pool and write/read all txn types from it ((/)).;;;",,,,,,,,,,,,,,,,,,,,,
Need to account fields from PLUGIN_CLIENT_REQUEST_FIELDS when calculating digest,INDY-1674,33442,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,KitHat,KitHat,03/Sep/18 8:51 PM,17/May/19 4:54 PM,28/Oct/23 2:47 AM,12/Apr/19 10:03 PM,,1.7.1,,,,0,,,,"We need to account PLUGIN_CLIENT_REQUEST_FIELDS in digest calculation.

For fields from PLUGIN_CLIENT_REQUEST_FIELDS we might call some special method that will provide structure that won't change if we will shuffle elements in the request field. It will be implemented in a place where these fields are defined.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1915,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2093,,,No,,Unset,No,,,"1|hzwvif:000006r20di",,,,Unset,Unset,Ev-Node 19.07,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,KitHat,,,,,,,,,,"12/Apr/19 10:03 PM;ashcherbakov;Validation will be done in the scope of INDY-2046;;;","12/Apr/19 11:25 PM;Derashe;Problem reason/description: 
- We need to include plugin's fields into digest calculation. Otherwise, two txns with different plugin's fields can be distinguished by node as one.

Changes: 
- Plugin's fields included in digest calculation

PR:
- [https://github.com/hyperledger/indy-plenum/pull/1162]

Version:
- will be marked later

Risk factors:
- no

Risk:
- Low

Covered with tests:
- [https://github.com/hyperledger/indy-plenum/pull/1162/files#diff-c785cd3e9c42a73c8ad18c5fdb8c8b7f]

Recommendations for QA
 * build one txn with plugin fields set. 
 * build another one, by modifying prev txn's plugin field
 * send these two txns at same time/with a time space. Ensure that second rejected;;;",,,,,,,,,,,,,,,,,,,,,,,
Investigate behavior of a pool that is restarted while under load,INDY-1675,33453,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,esplinr,Derashe,Derashe,04/Sep/18 6:18 PM,09/Oct/19 6:13 PM,28/Oct/23 2:47 AM,09/Oct/19 6:13 PM,,,,,,0,Research-and-Architecture,,,"Investigate restarting a pool under load:
 * Will it achieve consensus and begin processing transactions normally?
 * How long will this take?

We need to reproduce case:
 * pool was under load (load similar to https://jira.hyperledger.org/browse/INDY-1639 decription in comments) 
 * make sure that few nodes slowed ordering (probably stress would help)
 * restart a pool
 * make a normal load ( ~20 txn/s)

Then, check if pool will start ordering and how soon will it happen",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1639,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|hzzpbb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,,"09/Oct/19 6:13 PM;ashcherbakov;We tested this, also this is a frequent check in acceptance criteria of different tickets;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool cannot order transaction when N-F nodes are active,INDY-1676,33460,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,04/Sep/18 9:43 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"*Steps to Reproduce:""
1) Make sure that Node1 is primary
2) stop service on Node2
3) stop service on Node3
4) send nym
5) demote Node6
6) start service on Node3
7) try to send nym
8) promote Node6
9) start all stopped services to restore pool
10) stop service on Node2
11) stop service on Node3
12) send nym

*Actual results:*
Pool cannot write nym on step 12 despite 5 of 7 nodes are available.

*Expected Results:*","indy-node 1.6.73
indy-plenum 1.6.51
Docker pool (7 nodes)",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1583,INDY-1640,,,,,,,,,,,,,,,,,,,"04/Sep/18 9:43 PM;zhigunenko.dsr;sandbox.7z;https://jira.hyperledger.org/secure/attachment/15772/sandbox.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzpcn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,"07/Sep/18 4:49 PM;zhigunenko.dsr;*Reason to Close:*
Acceptance criteria 04-04 updated: Node promotion requires indy-node service restart;;;",,,,,,,,,,,,,,,,,,,,,,,,
Improve performance when creating transport batches,INDY-1677,33464,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,04/Sep/18 10:42 PM,20/Dec/19 11:20 PM,28/Oct/23 2:47 AM,20/Dec/19 11:20 PM,,1.12.2,,,,0,,,,"The way how transport Batches are created was changed recently (INDY-1595) to support catch-up of big ledgers.

However, it turned out that this new way is much slower than the previous one.

Results of comparing performance of the Batch creation from Propagate msgs (since Propagate is the most frequent one):
 * Batch Size = 10:
 ** Current way: 0.0002
 ** Old way: 0.00005
 * Batch Size = 300:
 ** Current way: 0.03 sec
 ** Old way: 0.0003 sec
 * Batch Size = 1000:
 ** Current way: 0.33
 ** Old way: 0.0008

It may be quite critical if we have a lot of huge batches (like in INDY-1672).

 

*Acceptance criteria:*
 * Fix creation of batches to be as fast as the previous approach
 * Make sure that the new approach still supports catchup of huge ledgers

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2294,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41lk",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,,"18/Sep/18 12:01 AM;anikitinDSR;For now, we are creating a batch incrementally. In the other words, there is a queue of outgoing messages and batch is created by adding message one by one, serializing and checking that serialized couple of messages less then LIMIT constant (128 Kb for now).
Therefore, the possible way to improve this method is to change serializing method (not checking for each message for example);;;","20/Nov/19 11:16 PM;ashcherbakov;We are hoping to just close this ticket of INDY-2294 is done.;;;","20/Dec/19 11:20 PM;ashcherbakov;As we disabled transport batches in INDY-2294, it's OK to close the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,
Record to restart_log isn't guaranteed even if restart is successful,INDY-1678,33465,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,zhigunenko.dsr,zhigunenko.dsr,04/Sep/18 11:25 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"Logs on the different nodes are not the same

Node1:
{code}
2018-09-04 13:40:44.170320	scheduled	2018-09-04 13:42:14.170070+00:00
2018-09-04 13:41:00.314054	scheduled	2018-09-04 13:42:30.313758+00:00
2018-09-04 13:41:14.804230	scheduled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:41:22.563852	cancelled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:55:34.731103	scheduled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.564277	cancelled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.564602	scheduled	2018-09-04 13:57:00+00:00
2018-09-04 13:57:00.003011	started	2018-09-04 13:57:00+00:00
2018-09-04 13:57:18.584291	succeeded	2018-09-04 13:57:00+00:00
{code}

Node2:
{code}
2018-09-04 13:41:15.171029	scheduled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:41:22.570075	cancelled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:42:57.835480	scheduled	2018-09-04 13:44:27.835307+00:00
2018-09-04 13:55:34.717679	scheduled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.578721	cancelled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.579104	scheduled	2018-09-04 13:57:00+00:00
2018-09-04 13:57:00.007226	started	2018-09-04 13:57:00+00:00
2018-09-04 13:57:18.533908	succeeded	2018-09-04 13:57:00+00:00
{code}

Node3:
{code}
2018-09-04 13:41:14.677642	scheduled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:41:22.580352	cancelled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:55:34.700403	scheduled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.577801	cancelled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.578303	scheduled	2018-09-04 13:57:00+00:00
2018-09-04 13:57:00.001587	started	2018-09-04 13:57:00+00:00
2018-09-04 13:57:17.226459	succeeded	2018-09-04 13:57:00+00:00
{code}

Node4
{code:java}
2018-09-04 13:41:14.967964	scheduled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:41:22.574571	cancelled	2020-01-25 12:49:05.258870+00:00
2018-09-04 13:55:34.735593	scheduled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.573813	cancelled	2018-09-04 14:44:00+00:00
2018-09-04 13:55:49.574195	scheduled	2018-09-04 13:57:00+00:00
2018-09-04 13:57:00.013251	started	2018-09-04 13:57:00+00:00
2018-09-04 13:57:18.406559	succeeded	2018-09-04 13:57:00+00:00
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzpdb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,Toktar,zhigunenko.dsr,,,,,,,,,,"15/Oct/18 11:41 PM;esplinr;[~zhigunenko.dsr] Can you provide more information about how you are triggering the restart, and why it is a bad thing that the logs are not equal?;;;","07/Nov/18 8:25 PM;Toktar;Only Node2 has different log. It has one more record:
{code:java}
2018-09-04 13:42:57.835480 scheduled 2018-09-04 13:44:27.835307+00:00
{code}
[~zhigunenko.dsr] Is the restart command was send in this time?;;;","07/Nov/18 9:40 PM;Toktar;Is not reproduced.;;;",,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.73,INDY-1679,33470,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,VladimirWork,VladimirWork,05/Sep/18 12:31 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Version Information*
indy-node 1.6.73 
 indy-plenum 1.6.51 
 indy-anoncreds 1.0.11 
 sovrin 1.1.17 

*Major Fixes*
INDY-1583 - Pool stopped writing after F change
INDY-1645 - read_ledger passes incorrectly formatted stdout and breaks convention
INDY-1595 - Node can't catch up large ledger
INDY-1603 - Validator Info may hang for a couple of minutes
INDY-1548 - read_ledger tool is not able to read the sovrin plugin ledger

*Changes and Additions*
INDY-1642 - 3PC Batch should preserve the order of requests when applying PrePrepare on non-primary
INDY-1643 - Monitor needs to take into account requests not passing dynamic validation when trigerring view change
INDY-1565 - Improve throughput calculation to reduce a chance of false positive View Changes
INDY-1660 - Performance of monitor should be improved
INDY-1588 - As a Steward, I need to have a script that can generate Proof of possession for my BLS key, so that I can use this value in a NODE txn
INDY-1389 - Support Proof of Possession for BLS keys
INDY-1582 - Do not use average when calculating total throughput/latency of backups
INDY-1564 - Discard any client requests during view change
INDY-1568 - As a developer I need a simple tool to show graphical representation of some common metrics
INDY-1549 - Change default configs for better performance and stability

*Known Issues*
INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66
(!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]
(!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI for pool upgrade*.

(!) *Pool upgrade from indy-node 1.3.62 to indy-node 1.6.73 should be performed simultaneously for all nodes due to txn format changes.*
(!) *There must be sovrin package upgrade to 1.1.17 version after indy-node package upgrade. You need to specify package=sovrin in pool-upgrade command to do this.* 
(!) *All indy-cli pools should be recreated with actual genesis files.*
(i) *For more details about txn format changes see INDY-1421.*
(i) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzpef:",,,,Unset,Unset,EV 18.18 Service Pack 2,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),TechWritingWhiz,VladimirWork,,,,,,,,,,,"05/Sep/18 12:36 AM;VladimirWork;FYI [~krw910];;;","06/Sep/18 5:48 AM;TechWritingWhiz;The pull request for this is here: https://github.com/sovrin-foundation/sovrin/pull/94;;;",,,,,,,,,,,,,,,,,,,,,,,
Ability to switch off (remove) replicas with no changes of F value ,INDY-1680,33501,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Toktar,Toktar,05/Sep/18 5:40 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,GA-0,,,"If a backup primary node is malicious (for example, disconnected), all replicas on this backup instance store all new requests and other replicas can't remove already ordered messages.
 For solve this problem we should have ability to switch off (remove) replicas with no changes value F:

 

*Acceptance criteria*
 - add function to remove a replica (and cleaning client requests for this replica)
 - It should not lead to change of F value
 - re-evaluate requests queue once replica is removed to check if we can clear some requests
 - correctly process messages for switched off replicas
 ** discarding them may be fine for this version
 - make sure monitor works as expected
 - add tests for check this function and check the correct work of other systems (like a monitor, requests removing)",,,,,,,,,,,,,,,,,INDY-1681,INDY-1685,,,,,,,,,,,INDY-1248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1684,,,No,,Unset,No,,,"1|hzwwpb:",,,,Unset,Unset,EV 18.18 Service Pack 2,EV 18.19,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,VladimirWork,,,,,,,,,,"18/Sep/18 7:05 PM;Toktar;Problem reason:
 - If a backup primary node is malicious (for example, disconnected), all replicas on this backup instance store all new requests and other replicas can't remove already ordered messages.

Changes:
 - add function for remove a replica (and cleaning client requests for this replica)
 - restore the number of replicas during the view change
 - add tests for check this function and check the correct work of other systems
 - correctly process messages for switched off replicas
 - change a replicas list format in node.py to a SortedDict
 - change a replicas list format in monitors to a dict

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/912]
 * [https://github.com/hyperledger/indy-node/pull/945]

Version:
 * indy-node 1.6.605 -master
 * indy-plenum 1.6.542 -master

Risk factors:
 - False positive view changes

Risk:
 - Low

Covered with tests:
 * [test_replica_removing.py|https://github.com/hyperledger/indy-plenum/pull/912/files#diff-8a9b6445373e3a3399539077f1fc7ef8]

Recommendations for QA:
 * Load test with usual load (~20txns/sec). Pool has no view changes with a master degraded during few hours.;;;","20/Sep/18 12:25 AM;VladimirWork;Build Info:
indy-node 1.6.607

Steps to Validate:
1. Run continuous load tests for 1..20 writing txns pers second.
2. Check logs and validator-info for VCs and their reasons.

Actual Results:
There are no master degraded VCs for 24 hours under load (1m+ txns in ledger).;;;",,,,,,,,,,,,,,,,,,,,,,,
Switch off a replica that stopped because disconnected from a backup primary,INDY-1681,33502,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,Toktar,Toktar,05/Sep/18 6:08 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,GA-0,,,"If a backup primary node is disconnected, all replicas on this backup instance store all new requests and other replicas can't remove already ordered messages.
 For solve this problem we should detect that a backup primary node was disconnected for a long constant time and switch off the replica with this primary. (task: INDY-1680)

*Acceptance criteria:*
 - Implement an abstract strategy to detect malicious backup primaries
 - Implement a strategy which detects malicious backup primaries by disconnection
 ** We need to have a tolerance time we wait before reporting disconnection (like being disconnected for 10 secs in a row)
 - switch off a replica (a code in INDY-1680) once strategy detects malicious.
 - make sure that all replicas are switch on after a View Change
 - add tests

 - testing performance changes (shouldn't be worse) after disconnect a backup primary node
 - testing memory consumption (should be better)",,,,,,,,,,INDY-1680,,,,,,,INDY-1682,INDY-1685,INDY-1602,INDY-1574,,,,,,,,,,,,,,INDY-1248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1684,,,No,,Unset,No,,,"1|hzwwpz:",,,,Unset,Unset,EV 18.19,Ev 18.20,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),NataliaDracheva,spivachuk,Toktar,,,,,,,,,,"22/Sep/18 12:15 AM;Toktar;The implementation of this task allows a situation when the nodes will have a different number of instances. In this case when the instance contains an insufficient number of replicas for the transactions ordering, there will be an error out_of_memory.
This problem will be fixed in  tasks INDY-1682,  INDY-1683.;;;","29/Sep/18 6:43 AM;spivachuk;*Changes:*
- Implemented removal of a backup replica on its primary loss.
- Added tests for removal of a backup replica on its primary loss.
- Reworked the tests of the replica removal mechanism.
- Fixed an issue with lack of freeing ordered requests on a replica removal.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/925
- https://github.com/hyperledger/indy-plenum/pull/931
- https://github.com/hyperledger/indy-node/pull/958

*Version:*
- indy-node 1.6.618-master
- indy-plenum 1.6.553-master;;;","05/Oct/18 5:06 PM;NataliaDracheva;*Scenario 1:*
*Build version:*
indy-node: 1.6.618
indy-plenum: 1.6.553
*Test description:* Verify if pool keeps writing after all replicas switching off and forced View Change.
*Steps to Validate:*
1. Run perf_processes.py from 1 AWS agent with the following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 20 -l 10 -y one -k nym
{code}
2. 10 mins later ssh to Node2 and do 'sudo systemctl stop indy-node'.
3. Do the same to all node up to Node8 (including) every 10 mins.
=> Instances are turned off, pool keeps writing, no VC by master degraded, no OOM.
4. Turn on all nodes.
5. Turn off Node1.
~/logs/1681/618-553
*Expected results:* VC happens, all replicas start, pool keeps writing, all replicas catch up data.
*Actual results:* VC happens, all replicas start, pool keeps writing, all replicas catch up data. (/)
*Additional info:* ~/logs/1681/618-553

*Scenario 2:*
*Build version:*
indy-node: 1.6.622
indy-plenum: 1.6.554
*Test description:* 
*Steps to Validate:*
1. Run perf_processes.py from 1 AWS agent with the following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 20 -l 10 -y one -k nym
{code}
=> Instance8 is removed automatically as f = 7 now.
2. Turn off node9.
3. Wait 1.5 minutes.
*Expected results:* No errors in logs, pool keeps writing, all other nodes are reachable.
*Actual results:* No errors in logs, pool keeps writing, all other nodes are reachable. (/)
*Additional info:* ~/logs/1681/additional;;;",,,,,,,,,,,,,,,,,,,,,,
Improve the switch off replica logic to collecting all reasons (not only disconnected primary),INDY-1682,33504,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Toktar,Toktar,05/Sep/18 6:31 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,GA-0,,,"A backup replica should be switch off not only with disconnected backup primary (task: INDY-1681) but with backup instance degradation. It shouldn't send BACKUP_INSTANCE_FAULTY but should collect its own messages with this type.

*Acceptance criteria:*
 - Implement a new strategy (see INDY-1681) which will check for performance degradation (the same way as master does) in addition to disconnections
 - switch off a backup replica when it has more then M its BACKUP_INSTANCE_FAULTY messages in K time.
 - add tests to check correct replica switch off
 - testing performance changes (shouldn't be worse) after disconnect or degraded a backup primary node.
 - testing memory consumption (should be better in case a backup primary node degraded)",,,,,,,,,,INDY-1681,,,,,,,INDY-1683,,,,,,,,,,,,INDY-1248,,,,,,,,,,,,,,,,,,,,"26/Oct/18 6:40 PM;VladimirWork;Figure_25th+max_replica_request_queues_backup.png;https://jira.hyperledger.org/secure/attachment/16176/Figure_25th%2Bmax_replica_request_queues_backup.png","26/Oct/18 6:40 PM;VladimirWork;Figure_5th+max_replica_request_queues_backup.png;https://jira.hyperledger.org/secure/attachment/16175/Figure_5th%2Bmax_replica_request_queues_backup.png","26/Oct/18 12:40 AM;VladimirWork;Figure_node25_90k.png;https://jira.hyperledger.org/secure/attachment/16172/Figure_node25_90k.png","26/Oct/18 12:40 AM;VladimirWork;Figure_node5_300k.png;https://jira.hyperledger.org/secure/attachment/16171/Figure_node5_300k.png","25/Oct/18 5:39 PM;VladimirWork;INDY-1682-all-txns-fees.PNG;https://jira.hyperledger.org/secure/attachment/16168/INDY-1682-all-txns-fees.PNG","25/Oct/18 5:36 PM;VladimirWork;load_test_results.tar.gz;https://jira.hyperledger.org/secure/attachment/16167/load_test_results.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1684,,,No,,Unset,No,,,"1|hzwxin:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"24/Oct/18 5:05 PM;Toktar;*Problem reason:*
If a backup primary node is disconnected, all replicas on this backup instance store all new requests and other replicas can't remove already ordered messages.

*Changes:*
 * Added configuration parameters
REPLICAS_REMOVING_WITH_DEGRADATION = ""local"" (None/local/quorum)
REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = ""local""(None/local/quorum). 
With ""local"" when backup replica degradation will found, replicas will removed
 * Was switched on parameter ACC_MONITOR_ENABLED

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/948]
 * [https://github.com/hyperledger/indy-node/pull/996]


*Version:*

indy-node 1.6.643 -master
indy-plenum 1.6.570 -master


*Risk factors:*
 * This feature should solve memory leak problem but not in all cases.
 * A problem with master degraded View Change can be found.

*Risk:*

Low

*Covered with tests:*

[test_replica_removing_with_backup_degraded.py|https://github.com/hyperledger/indy-plenum/pull/948/files#diff-4f7376795ddaf94804fd3efc795afe2d]

*Recommendations for QA:*

Test this feature in scope of a load testing.;;;","25/Oct/18 5:39 PM;VladimirWork;Build Info:
indy-node 1.6.643
indy-perf-load 1.0.16

Steps to Reproduce:
1. Run load test with half of target load rate (5 writing txns per second) with all txn types (except revoc_reg_entry) and fees for all txns (config.yml):
{noformat}
clients: 250
genesis_path: ~/live_transactions_genesis
seed: [000000000000000000000000Trustee1, 000000000000000000000000Trustee2, 000000000000000000000000Trustee3, 000000000000000000000000Trustee4]
batch_size: 1
buff_req: 100
req_kind: '[{""nym"":{""count"": 4}}, {""schema"":{""count"": 1}}, {""attrib"":{""count"": 3}}, {""cred_def"":{""count"": 1}}, {""revoc_reg_def"":{""count"": 1}}, {""payment"":{""count"": 9}}]'
ext_set: '{""payment_addrs_count"":100,""addr_mint_limit"":10000,""payment_method"":""sov"",""plugin_lib"":""libsovtoken.so"",""plugin_init"":""sovtoken_init"",""trustees_num"":4, ""mint_by"": 100, ""set_fees"":{""1"":1,""100"":1,""101"":1,""102"":1,""113"":1,""10001"":1}}'
mode: t
load_rate: 0.01
sync_mode: freeflow
load_time: 0
{noformat}
{code:java}
x2 client machines
{code}
2. Check metrics and logs.

Actual Results:
We have nearly expected throughput only during initial ~15 minutes then it falls close to zero. !INDY-1682-all-txns-fees.PNG|thumbnail! 
After 1.5 hours of load test run we have more than 2k txns in token ledger and only 150 txns in domain ledger.
So it looks like we have some issues with txns ordering or with load script output rate. FYI [~Toktar] [~dsurnin]

Logs/journals/info from all nodes: ev@evernymr33:logs/1682_1.tar.gz
Load script full output:  [^load_test_results.tar.gz] ;;;","26/Oct/18 12:41 AM;VladimirWork;Build Info:
indy-node 1.6.643
indy-perf-load 1.0.16

Steps to Reproduce:
1. Run writing load test with ~40 NYMs/second without fees.
2. Check metrics, logs and nodes' ledgers.

Actual Results:
1. We have 19 writing nodes with ~300k txns in ledger and 6 (4, 7, 8, 9, 22, 25) broken with 90..100k txns in ledger (unable to write or catch up) at the end of load test run.
2. 5th node (good) is on view 2 with RAM consumption growing up to ~7 GB: !Figure_node5_300k.png|thumbnail! 
3. 25th node (bad after 13:00) is on view 1 with OOM near 14:00: !Figure_node25_90k.png|thumbnail! 

Logs and all info: ev@evernymr33:logs/1682_2.tar.gz;;;","26/Oct/18 6:40 PM;VladimirWork;The same figures with `max_replica_request_queues_backup`.  !Figure_5th+max_replica_request_queues_backup.png|thumbnail!  !Figure_25th+max_replica_request_queues_backup.png|thumbnail! ;;;","26/Oct/18 7:01 PM;VladimirWork;Development and bugfixing will be continued in scope of INDY-1685.;;;",,,,,,,,,,,,,,,,,,,,
Improve the switch off replica logic to do it after InstanceChange quorum,INDY-1683,33506,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Toktar,Toktar,05/Sep/18 7:04 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,GA-0,,,"A backup replica should be switch off after quorum of INSTANCE_CHANGE messages on backup instance.

*Acceptance criteria:*
 - A strategy done in INDY-1682 should result in sending a special INSTANCE_CHANGE about instance change on backup instance X
 - Once node got a quorum (n-f) of instance changes on backup instance X, it switches off the replica (INDY-1680)
 - add tests to check correct replica switch off

 - testing perfomance changes (shouldn't be worse then in task INDY-1682) after disconnect or degraded a backup primary node.

 - testing memory consumption (should be better then in task INDY-1682)",,,,,,,,,,INDY-1682,,,,,,,,,,,,,,,,,,,INDY-1248,,,,,,,,,,,,,,,,,,,,"09/Nov/18 6:46 PM;VladimirWork;1683_1.png;https://jira.hyperledger.org/secure/attachment/16241/1683_1.png","09/Nov/18 6:46 PM;VladimirWork;1683_16.png;https://jira.hyperledger.org/secure/attachment/16244/1683_16.png","09/Nov/18 6:46 PM;VladimirWork;1683_6.png;https://jira.hyperledger.org/secure/attachment/16242/1683_6.png","09/Nov/18 6:48 PM;VladimirWork;1683_8.png;https://jira.hyperledger.org/secure/attachment/16245/1683_8.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1684,,,No,,Unset,No,,,"1|hzwww7:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,,"25/Oct/18 5:17 PM;ashcherbakov;I think we should use `quorum` strategy for degradation and `local` strategy for disconnection;;;","06/Nov/18 6:44 PM;Toktar;If we will remove replicas only with quorum of messages, nodes are not protected from memory leaks. 
 I think we should switch on quorum strategy for degradation and ""local"" for disconnected, but change it. In new ""quorum"" strategy, node send  BackupInstanceFaulty message with every problem detection. Replica will removed if 
 * node has a quorum of message from different nodes
 * node has a quorum of own messages;;;","07/Nov/18 6:41 PM;Toktar;Problem reason:
 -  With ""local"" strategy of replicas removing, different nodes can have different count of nodes. 

Changes:
 - Add a new ""quorum"" strategy for removing performance degraded replica with quorum(f-1) own messages or quorum of messages from other nodes.
 - Add tests

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/954]
 * [https://github.com/hyperledger/indy-node/pull/1014]

Version:
 * indy-node 1.6.661 -master
 * indy-plenum 1.6.583  -master

Risk factors:
 - All replicas will be removed or no one replicas will not be removed and will happen memory leak.

Risk:
 - Low

Covered with tests:
 * [test_instance_faulty_processor.py|https://github.com/hyperledger/indy-plenum/pull/954/files#diff-e1a005e8a7b3eef109858ae5550ce196]

Recommendations for QA:
 * Load test with 40 NUM/sec for 4 hours.;;;","09/Nov/18 6:46 PM;VladimirWork;Build Info:
indy-node 1.6.669

Steps to Validate:
1. Run 20 nyms/sec for 15 hours.
2. Check metrics and logs for OOM reasons and replicas removal at any node.

Actual Results:
Pool works normally. Only 8th node breaks due to OOM and corresponding replica was removed successfully at 03:30 to avoid OOM at all other nodes.
 !1683_1.png|thumbnail!  !1683_6.png|thumbnail!  !1683_8.png|thumbnail!  !1683_16.png|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,
Switch off backup replicas which primary nodes are malicious ,INDY-1684,33508,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,Toktar,Toktar,05/Sep/18 7:16 PM,25/Oct/19 9:14 PM,28/Oct/23 2:47 AM,09/Oct/19 6:57 PM,,,,,,0,,,,"We have a problem with backup instances thouse primaries are stopped. In this case primary nodes:
 * replicas stashing messages and can't use it and dynamicly free memory
 * replicas store all new requests and other replicas can't remove already oredered messages.

In RBFT we need in f+1 instances because if f primary (backup and master) nodes will malicious, one node will correct and pool will do view changes until the correct node becomes the primary.
If primary on backup malicious, the whole backup instance malicious and we don't need in this throughput metrics. And we can remove it.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-5,,Switch off replicas,Done,No,,Unset,No,,,"1|hzzpm7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Start View Change if more than M replicas switched off.,INDY-1685,33509,,Task,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Toktar,Toktar,05/Sep/18 7:52 PM,25/Oct/19 9:17 PM,28/Oct/23 2:47 AM,,,,,,,0,GA-0,,,"If more than M (f/2) replicas switched off, start View Change and return switched off replicas to work mode",,,,,,,,,,INDY-1681,INDY-1680,,,,,,,,,,,,,,,,,,INDY-1248,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125u:y",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,,"09/Oct/18 8:29 PM;Toktar;https://github.com/hyperledger/indy-plenum/pull/938;;;",,,,,,,,,,,,,,,,,,,,,,,,
Check if ZMQ connections and queues affect memory consumption,INDY-1686,33547,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,sergey-shilov,VladimirWork,VladimirWork,06/Sep/18 9:52 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"As of now we limit ZMQ message queues, but this limit is per-connection (there is a queue for every connection)

*Acceptance criteria:*
 - create tests (can be without Plenum, just isolated one based on ZMQ) emulating SDK client load more than server can process
 ** for example 40 requests per sec from client, and 20 reads per sec by listener
 ** client need to create a new connection for every 5 requests
 ** create tests for Python and C implementation of ZMQ
 - compare memory consumption
 ** compare Python and C
 ** analyse how ZMQ deals with a lot of connection (how it reads requests, and releases queues for each connections)
 ** check if and when memory is released once load is stopped.
 - check how Monitor affect memory consumption

Compare memory usage in each case",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwxif:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,VladimirWork,,,,,,,,,,"13/Oct/18 12:50 AM;sergey-shilov;While looking for possibility of monitoring of ZMQ inner queues I found a feature of ZMQ called MonitoredQueue. MonitoredQueue is a kind of ZeroMQ device. In fact it is a combination of three ZMQ sockets that is something like a proxy. Using this proxy you can monitor all incoming/outgoing traffic.

Some docs and examples for that:
https://pyzmq.readthedocs.io/en/latest/devices.html
https://learning-0mq-with-pyzmq.readthedocs.io/en/latest/pyzmq/pyzmqdevices/monitorqueue.html

In fact this is not that I've been looking for as it can not be used for monitoring of inner ZMQ queues. Also I had a trouble with combination of this proxy with Router socket, I got an issue with one more added ident to a message. But anyway, may be it is useful information for future investigation.;;;","25/Oct/18 1:33 AM;sergey-shilov;During working on this ticket we did various tests with ZMQ-based sample servers written in C and in Python (libzmq-4.1.4). During these tests we used a sample ZMQ-based client written in Python which emulates libindy's behaviour.

*Results*
 * There is no big difference between C and Python servers in memory consumption under equal load (the server written in Python consumes slightly more memory, but it's expected)
 * ZMQ is a very big memory consumer as:
 ** it creates a separate queue for each connection
 ** it reads from these queues in a Round-Robin manner (when recv() for listener is called), so these queues are kept in memory for a long time in case of big number of connections (note our receive quotas for client stack)
 ** these inner queues are not cleaned if corresponding connection socket has been closed from the client side (for example by time out), such queue is kept in memory until the last packet is read
 ** under high load the memory grows fast, but it is freed very slow even after load stop, looks like there is some additional memory management
 ** during one of long tests of high load with C-server when clients throughput exceeds server's throughput (40 reqs/seq vs 25 reqs/sec) memory usage grew up to ~12GB VSZ and ~5.5GB RSS. Then load was stopped and we let server read all messages from queues. When no messages left (i.e. non-blocking recv() finished with EAGAIN) we observed that the C-server still keeps ~4GB VSZ and ~1.5GB RSS. After some idle time and small and short load the C-server abruptly reduced memory consumption to ~250MB VSZ and 64MB RSS.
* Monitor does not affect memory consumption.

*Conclusion*
 Seems like ZMQ library is not designed for high load with big numbers of connections. The inner queues management and such long keeping of memory by ZMQ library may cause very large memory consumption under long high load.
 For now we consider restart of client stack depending on memory consumption by indy-node process and reducing of allowed number of connections (INDY-1777), these options are under research now.;;;",,,,,,,,,,,,,,,,,,,,,,,
We need to take as much as possible Node-to-Node messages from queues before starting View Change,INDY-1687,33548,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,06/Sep/18 10:07 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"The current View Change protocol is based on catch-up and last prepared certificate calculated by each node individually before starting the View Change.

The idea is the following:
 * Each nodes calculates the maximum 3PC seqNo for a quorum of received prepares.
 * Once we have a quorum (n-f) Prepares, other nodes should also have this (in an ideal perfect world). So, we should have n-f nodes at the same state, while other f nodes can catchup to this state.

However, in reality this approach doesn't work in 100% cases and may lead to some nodes go beyond others because
 * The network is async and unpredictable, so all nodes may send Prepares, some nodes may receive them, but some not
 * *There is a message queue with node-to-node messages, and messages are grabbed from there by small portions to avoid looper iterations taking too much time*

*The problem:*
 * It's possible that a Node has enough Prepares to set a higher prepared certificate, but they are still queued when View Change  is started, and prepared certificate is older than can be.
 * So, if we have fast and slow nodes, for fast nodes we may have higher prepared certificate than for slow ones because fast nodes process message queue faster. This may lead to a situation where fat nodes go beyond others, and we lost consistency.

*Acceptance criteria:*
 * Provide a configurable strategy that can process as much as possible messages form Node-to-Node message queue before starting the View Change and setting last prepared certificate.

 ** We would like to grab all messages
 ** However, there is a risk of infinite grabbing if there is intensive load
 ** So, probably we still need to have some limits, or say ZMQ to not process any incoming messages while we are flushing the queue.
 * Test the strategy and enable it by default if everything is fine",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/18 7:43 PM;VladimirWork;INDY-1687-NODE-16.PNG;https://jira.hyperledger.org/secure/attachment/16240/INDY-1687-NODE-16.PNG","08/Nov/18 7:43 PM;VladimirWork;INDY-1687-NODE-6.PNG;https://jira.hyperledger.org/secure/attachment/16239/INDY-1687-NODE-6.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwxbj:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"29/Oct/18 6:28 PM;ashcherbakov;PoA:

We need to make sure that there is a chance for 3PC messages to be processed before continue of paused View Change.
 Also it's more safe to process only 3PC messaged for the current view (to update last_pp_certificate).
 So, the following can be done:
 1) New messages: `ViewChangeStarted` and `ViewChangeContinued`
 2) New strategy with the following methods:
 - `prepare_view_change`
 - `on_view_change_started`
 - `on_view_change_continued`

3) `ViewChanger` calls strategy's `prepare_view_change` on `startViewChange` and exits.
 4) `prepare_view_change`:
 - extends `Node`'s router to call `on_view_change_started` for `ViewChangeStarted` and `on_view_change_continued` for `ViewChangeContinued`
 - extends Master Replica's router for `ViewChangeStarted` to add `ViewChangeContinued` to node's inBox
 - appends `ViewChangeStarted` to node's inBox

5) `on_view_change_started`:
 - reads more messaged from nodestack by calling `self.nodestack.service(limit=None, quota=EXTENDED_QUOTA_MULTIPLIER * self.quota_control.node_quota)`, where `EXTENDED_QUOTA_MULTIPLIER =10`
 - pops all messages from node's inBox and stashes them in `stashedNodeInNox`
 - iterate through `stashedNodeInNox` and pop all 3PC (PrePrepare, Prepare, Commit) messages where viewNo=currentViewNo
 - pass all such 3PC messaged to Replica (by calling `node.nodeMsgRouter.handle(m)`
 - pass `ViewChangeStarted` to master replica's nodeInBox.

6) `on_view_change_continued`:
 - call `node.view_changer.startViewChange(continue=True)`
 - add all stashed messages from `stashedNodeInNox` to the head(!!!) of node's inBox;;;","07/Nov/18 4:38 PM;anikitinDSR;Some comments for PoA:
 * `prepare_view_change` method should not pass ViewChangeStartMessage into master replica's inBox queue, because there is no steps for processing this message on replica's side
 * `on_view_change_started` method is a handler for processing ViewChangeStartMessage on nodeMsgRouter's side. Also, after all msgs routines like calling nodestack.service, pops all not 3PC msgs into stashedNodeInBox queue this method should append ViewChangeContinueMessage to master replica's inBox queue.;;;","07/Nov/18 7:32 PM;anikitinDSR;Reasons:
 * need to take as much as possible Node-toNode messages from queues before starting ViewChange

Changes:
 * Added strategy for making some prepration steps before starting ViewChange as described in PoA.
 * Added unit and integration tests for this strategy

Versions:
 * indy-plenum: 1.6.582
 * indy-node: 1.6.660

Recomendation for QA:
- As of now, this strategy is disabled by default. For activating this need to add next lines into indy config:

`

from plenum.common.constants import PreVCStrategies
PRE_VC_STRATEGY = PreVCStrategies.VC_START_MSG_STRATEGY

`

- Run load test with forced view_change (for example each 30 minutes) and check, that pool works fine (ledgers are the same on all nodes and there is no any blacklisted nodes) after several VC circles.

 ;;;","08/Nov/18 7:43 PM;VladimirWork; !INDY-1687-NODE-6.PNG|thumbnail!  !INDY-1687-NODE-16.PNG|thumbnail! ;;;","08/Nov/18 11:05 PM;VladimirWork;[~Toktar] [~anikitinDSR] all logs/journals/metrics are in ev@evernymr33:logs/1687.7z;;;","09/Nov/18 5:05 PM;ashcherbakov;The rest of the testing and enabling the strategy will be done in the scope of INDY-1835.;;;",,,,,,,,,,,,,,,,,,,
Investigate Out of memory issues with the current load testing,INDY-1688,33549,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ashcherbakov,ashcherbakov,06/Sep/18 10:15 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"* Check results of the latest load testing where we fail with out of memory
 * Check if it depends on txn type and load

 

*Acceptance criteria*
 # Run load tests with different txn types and different load
 # Find out if txn type affects memory issues
 # Find out if load affects memory issues and what load we can handle without issues
 # Find out if the number of unprocessed requests affects memory issues
 # Create necessary bugs or do simple fixes if needed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/18 9:46 PM;ozheregelya;1111.png;https://jira.hyperledger.org/secure/attachment/15789/1111.png","10/Sep/18 10:13 PM;ozheregelya;Ext-07-09-18-mix-no-pay.png;https://jira.hyperledger.org/secure/attachment/15797/Ext-07-09-18-mix-no-pay.png","17/Sep/18 8:08 PM;ozheregelya;Live-14-09-18-mix-no-pay.png;https://jira.hyperledger.org/secure/attachment/15812/Live-14-09-18-mix-no-pay.png","28/Sep/18 4:16 PM;Derashe;image-2018-09-28-10-17-09-638.png;https://jira.hyperledger.org/secure/attachment/15888/image-2018-09-28-10-17-09-638.png","26/Sep/18 6:25 PM;Derashe;metrics.png;https://jira.hyperledger.org/secure/attachment/15869/metrics.png","21/Sep/18 4:25 PM;Derashe;screenshot-1.png;https://jira.hyperledger.org/secure/attachment/15849/screenshot-1.png","27/Sep/18 12:59 AM;Derashe;screenshot-2.png;https://jira.hyperledger.org/secure/attachment/15875/screenshot-2.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwov:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,ozheregelya,,,,,,,,,,"06/Sep/18 10:19 PM;ozheregelya;*Test 1:*
Steps to Reproduce:
1. Setup the pool of 25 nodes.
2. Run load test with 10 writes/sec and ~1000 reads/sec.
{code:java}
[2 machines X ]python3.5 perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -c 50 -l 0.1 -y freeflow -k ""[{\""nym\"": {\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""payment\"":{\""payment_method\"":\""sov\"", \""plugin_lib\"": \""libsovtoken.so\"", \""plugin_init_func\"":\""sovtoken_init\""}}]""
[8 machines X ] python3.5 perf_processes.py -m t -n 1 -c 1125 -l 0.1 -g ~/ext_transactions_genesis -k ""[{\""get_nym\"": {\""count\"": 9}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}]""{code}
=> Reading was failed after several thousands of requests because of problem in load script.
=> Pool was failed with OOM several times and was broken after the first one. 172347 txns were written on all of the nodes.
3. Restart the pool.
=> Pool works after restart.
4. Run the same test once more.
=> Pool was failed with OOM several times and was broken after the first one. ~335414 txns were written.

Actual Results:
Similar test with only NYMs
{code:java}
[2 machines X ]python3.5 perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -c 50 -l 0.1 -y freeflow{code}
showed that in case of load with all txns pool fails earlier than with only NYMs.

Logs and metrics: s3://qanodelogs/indy-1670-normal-load-2/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1670-normal-load-2/ /home/ev/logs/indy-1670-normal-load-2/;;;","07/Sep/18 6:05 PM;ashcherbakov;There are quite interesting results in INDY-1592, where all incoming requests were intentionally rejected with NACKs (that is during static validation, so that they didn't go to 3PC and consensus). There is no memory leak there at all, and memory consumption is only about 1.6 GB.
It means that there is probably no memory leak because of ZMQ or metrics or logs.

Most probably memory leaks are caused by intensive ordering.;;;","07/Sep/18 9:54 PM;ozheregelya;*Test 2:* NYMs only (400K txns written)
Strange memory usage during normal load (10 writes/sec, 1000 reads/sec) with only NYMs requests:

!1111.png|thumbnail!
 At some time memory started growing up faster than before.

Logs and metrics: s3://qanodelogs/indy-1688-normal-load-nyms-strange-memory-usage/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1688-normal-load-nyms-strange-memory-usage/ /home/ev/logs/indy-1688-normal-load-nyms-strange-memory-usage/;;;","10/Sep/18 10:25 PM;ozheregelya;*Test 3:* domain txns only, no revocations, no payments.
 1638329 txns written. Pool was broken after 45 hours.
 !Ext-07-09-18-mix-no-pay.png|thumbnail!

At some time memory started growing up faster than before, same as in case with NYMs.

Logs and metrics: s3://qanodelogs/indy-1688-Ext-07-09-18-mix-no-pay/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1688-Ext-07-09-18-mix-no-pay/ /home/ev/logs/indy-1688-Ext-07-09-18-mix-no-pay/;;;","17/Sep/18 8:08 PM;ozheregelya;Test 4: domain txns only, no revocations, no payments (same as Test 3, but with new metrics).
2156723 txns were written. Pool was broken after 60 hours.
!Live-14-09-18-mix-no-pay.png|thumbnail!
 Logs and metrics: s3://qanodelogs/indy-1688-Live-14-09-18-mix-no-pay/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1688-Live-14-09-18-mix-no-pay/ /home/ev/logs/indy-1688-Live-14-09-18-mix-no-pay/;;;","17/Sep/18 10:52 PM;Derashe;*Results of testing:* 

The above tests showed us similar result: Pool stably write txns until some moment, when memory start growing rapidly and crush the node and the pool. That seems to happen on every node. After researches, we understand, that memory grow correlate with grow of stored requests. We've assumed, that they are not deleting because some backup instance stopping ordering. That is seems to be true, because in every test, we've seen stable work of all backups and right before ""memory explosion"", one or few replicas stopped ordering.

*Ticket continuation:*

So, we need to implement such a logic, that will detect if replica degraded and turn it off.

Replica deletion mechanism will be implemented in https://jira.hyperledger.org/browse/INDY-1681.

Detect mechanism will be implemented and tested in scope of this ticket, cause it will be part of investigation.;;;","20/Sep/18 6:42 PM;Derashe;After discussions with team, we've designed few ways to fix this problem:
 * turn instance off
 * make view_change for every instance separately
 * deleting of old stashed requests

As for now, we've decided to use first option (which already implemented) to resolve one of possible causes of request stashing - backup primary network disconnecting https://jira.hyperledger.org/browse/INDY-1681. That will work for simplest case . For case when primary degraded we need to implement complex detection logic. That will be done in scope of another ticket;;;","24/Sep/18 8:10 PM;Derashe;Also, we've found another reason why requests could stash so much. For now, we clear requests queue only when achieving stable checkpoint. That means that we clear queue when we get at least 100 batches (or even more). 1 batch can contain up to 1000 txns. So, in the wost case we can get more than 100*1000 txns stashed. To resolve that, we can change node behaviour to clear txn from queue right after it was ordered on every replica.;;;","25/Sep/18 7:55 AM;ashcherbakov;[~Derashe] 
In Test4 we can see that requests queue doesn't grow (1000 requests all the time), but we still see OutOfMemory.;;;","25/Sep/18 3:17 PM;Derashe;[~ashcherbakov]

In that case, most probably, memory timeline shifted to the left. That was a known bug and it was fixed in [https://github.com/hyperledger/indy-node/pull/944];;;","26/Sep/18 6:59 PM;Derashe;We diagnose, that if we have load under 20 tps, we are processing every txn in time and we have no memory explosion. But when we raising load up to 30 and more, we can't process txns in time and we start collecting them in our internal queues. This cause memory explosion and inexplicable node behaviour, which can be seen on image below

!metrics.png|thumbnail!

There are:
 * According to node's logic, requests queue must be higher than moniotor queue. Because requests queue cleaning only with stable checkpoint and monitor queue cleaning smoothly.
 * Monitor's queue seems to add new requests and delete ordered once smoothly at the beginning ~(14:42 - 14:47) but then monitor stopped deleting them and grow equally with requests queue. 
 * Latency not stable, while we are ordering requests;;;","27/Sep/18 1:31 AM;Derashe;Upward problem was fixed in [https://github.com/hyperledger/indy-plenum/pull/924.]

But still we have OOM problem:

!screenshot-2.png|thumbnail!

We can try to debug this problem is such a way: after 30 minutes load test of 40 nym/s stop load for some time (10-20 minutes) and after that resume load for 30 minutes.

Purpose here is to look at memory consumption after stopping load:
 * if memory will stay on the same level. That means, that we need to search for some stashing collections in node's code
 * if memory will fall down to adequate values, that means that we need to find safe way to block memory explosion (i.e. block processing of a new requests) in some moment. Also need more ways to optimise node's work (such as https://jira.hyperledger.org/browse/INDY-1649);;;","27/Sep/18 2:07 AM;ashcherbakov;In this test we can see that the expected load was 40 txns per sec, while we order about 25-30. So, the rest is probably stashed, and ZMQ queues are growing. That nay explain the memory growth, and is probably expected.
What we can also test, is to restrict ZMQ internal queues to some small value, and see if this will help.;;;","28/Sep/18 4:20 PM;Derashe;After testing described case we've get such a graph:

!image-2018-09-28-10-17-09-638.png|thumbnail!

We can see that memory stayed on the same level after stopping load. That means that problem is about some collection in code that start grow rapidly, when we get unordered requests.;;;","28/Sep/18 9:52 PM;Derashe;According to acceptance criteria, final result are:
 * There are two discovered cases of memory leak: backup primary disconnecting/degraded and unordered requests storing
 * Unordered requests significantly affect memory
 * In case of ordering everything in time, we can see no ""memory explosion""
 * Txn type does not affect memory consumption directly, but more complex txns lower our throughput, so we get more unordered txns (with the same load) and stashing unordered requests faster.
 * Bug to discover and fix internal collection stashing is created https://jira.hyperledger.org/browse/INDY-1723;;;",,,,,,,,,,
Gather metrics from different profiling tools,INDY-1689,33553,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,06/Sep/18 10:36 PM,15/Oct/18 11:39 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"We should make several tests with different profiling tools to gather reports and compare with our metrics to have more details on node performance 

Examples of profiling tools that could be used
https://github.com/benfred/py-spy",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzzpsv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Create CHANGELOG.md,INDY-1690,33554,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Toktar,esplinr,esplinr,06/Sep/18 10:38 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"There should be a CHANGELOG.md file in the root of the Indy Node git repo.

It should contain brief release notes for the previous releases, back to 1.1.37.

Release notes should contain:
* A summary of major changes
* Brief notes about upgrades and behavior
* Any known issues that are important to communicate

The top of the changelog should include this explanation (with links):
""Details about what is included in this release can be found by browsing the fix version in the Hyperledger Issue Tracker and the tag in GitHub.""

Format should be similar to the CHANGELOG.md in the indy-skd.

Contents can come from the Sovrin release notes that accompanied each Indy Node release:
https://github.com/sovrin-foundation/sovrin/blob/master/release-notes.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzsfz:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,Toktar,,,,,,,,,,,"09/Oct/18 8:29 PM;Toktar;PR: https://github.com/hyperledger/indy-node/pull/966;;;",,,,,,,,,,,,,,,,,,,,,,,,
Plenum 2.0 Protocol Research,INDY-1691,33565,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,07/Sep/18 4:23 PM,13/Mar/20 10:46 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Evaluating alternatives to Plenum: other protocols and implementations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-7,,Plenum 2.0 Protocol Research,To Do,No,,Unset,No,,,"1|hzzpuv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,MichaelWang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Test automation architectural items,INDY-1692,33612,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,10/Sep/18 7:21 PM,11/Oct/19 9:02 PM,28/Oct/23 2:47 AM,11/Oct/19 9:02 PM,,,,,,0,,,,"- what test environment do we have?
- what tests do we continuously run?
- what reporting do we expect?
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxhz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"11/Oct/19 9:02 PM;ashcherbakov;I believe we've don this analysis;;;",,,,,,,,,,,,,,,,,,,,,,,,
Implement new authorisation table,INDY-1693,33617,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,Derashe,Derashe,10/Sep/18 10:45 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"As a part of INDY-1563 we need to implement complex auhtorization logic, so we need new authorization table for this.",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1999,INDY-1732,INDY-1563,INDY-1708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzq4n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),danielhardman,Derashe,esplinr,,,,,,,,,,"12/Sep/18 1:18 AM;danielhardman;I do not agree with the POA in INDY-1563, so I am against moving forward with implementation until the design is clarified.;;;","18/Feb/19 11:49 PM;esplinr;This issue is superseded by the work to move permissions to the config ledger (INDY-1732) and appropriately handle the delegation of write permissions (INDY-1999).;;;",,,,,,,,,,,,,,,,,,,,,,,
Implement validation for delegating txn to another DID,INDY-1694,33618,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,Derashe,Derashe,10/Sep/18 10:50 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"As a part of INDY-1563  we need to support adding txn, created by identifier and sended in ledger by submitter.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1999,,INDY-1563,INDY-1708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzq4v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),danielhardman,Derashe,esplinr,,,,,,,,,,"12/Sep/18 1:18 AM;danielhardman;I do not agree with the POA in INDY-1563, so I am against moving forward with implementation until the design is clarified.;;;","18/Feb/19 11:52 PM;esplinr;We will be tracking this work against the use case in INDY-1999, which will result in a different organization of tickets.;;;",,,,,,,,,,,,,,,,,,,,,,,
Implement validation for delegating process of issuing credentials to processor,INDY-1695,33619,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,Derashe,Derashe,10/Sep/18 10:53 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"As a part of INDY-1563  we need to support new version of CRED_DEF, which will contain processor field, which stand for DID of a identity, who can write REVOC_REG_DEF for this CRED_DEF",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1999,,INDY-1563,INDY-1708,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzq53:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),danielhardman,Derashe,esplinr,,,,,,,,,,"12/Sep/18 1:19 AM;danielhardman;I do not agree with the POA in INDY-1563, so I am against moving forward with implementation until the design is clarified.;;;","18/Feb/19 11:52 PM;esplinr;We will be tracking this work against the use case in INDY-1999, which will result in a different organization of tickets.;;;",,,,,,,,,,,,,,,,,,,,,,,
35 view changes were happened during 10 minutes after nodes failure because of invalid request,INDY-1696,33681,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,ozheregelya,ozheregelya,12/Sep/18 6:58 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,TShirt_S,,,"*Environment:*
 indy-node 1.6.598

*Steps to Reproduce:*
 1. Setup the pool.
 2. Run small load test.
 3. Keep pool without any load for long time (~10 hours).
 4. Run load test:
{code:java}
python3.5 perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -c 10 -l 1 -y freeflow -k ""[{\""fees_nym\"":{\""count\"": 4, \""payment_addrs_count\"": 100,\""payment_method\"":\""sov\"", \""plugin_lib\"": \""libsovtoken.so\"", \""plugin_init_func\"":\""sovtoken_init\""}}, {\""fees_schema\"":{\""count\"": 1, \""payment_addrs_count\"": 100,\""payment_method\"":\""sov\"", \""plugin_lib\"": \""libsovtoken.so\"", \""plugin_init_func\"":\""sovtoken_init\""}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""revoc_reg_entry\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9, \""payment_addrs_count\"": 100,\""payment_method\"":\""sov\"", \""plugin_lib\"": \""libsovtoken.so\"", \""plugin_init_func\"":\""sovtoken_init\""}}]"" -b 200{code}
=> Load test was failed because of TOK-425, nodes were crashed.
 5. Run one more load test with NYMs only (without fees).
 => NACKs were received for each request because of View Change, part of the nodes were failed because of the same reason.

*Actual Results:*
 36 View Changes were happened during 10 minutes right after start of load test.
 Nodes 3 and 4 had ViewNo 0 after 27 and 28.
 Node 7 had ViewNo 33.
 Nodes 2, 5, 6, 8, 9 had ViewNo 35.
 The rest ones stopped view changing on ViewNo 36.

Logs and metrics: s3://qanodelogs/indy-1696/NodeXX/
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1696/ /home/ev/logs/indy-1696/

*Acceptance criteria:*

Check whether this is a known issue or not. Most probably this ticket is related to plugins throwing exceptions INDY-1698 and cyclic view change issues.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzsfj:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,zhigunenko.dsr,,,,,,,,,,"01/Oct/18 7:32 PM;Toktar;View Changes happened because all nodes restarted with code problem in /sovtokenfees/static_fee_req_handler.py
Please, re-test it with the last version of sovtokenfees.
{code:java}
File ""/usr/local/lib/python3.5/dist-packages/sovtokenfees/static_fee_req_handler.py"", line 103, in can_pay_fees
outputs = request.fees[1]
AttributeError: 'Request' object has no attribute 'fees'
{code}
PS: I think, this task can be close and open in the token project;;;","01/Oct/18 11:10 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.619
indy-plenum                1.6.553
plugins (master)

*Steps to Validate:*
1) prepare pool and set fee for nym
2) send nym via CLI
3) send nyms via load script

*Actual Results:*
Requests failed without node and plugins harming;;;",,,,,,,,,,,,,,,,,,,,,,,
Define how node deals with exceptions thrown by plugins,INDY-1698,33711,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,devin-fisher,devin-fisher,13/Sep/18 6:57 AM,11/Oct/19 9:00 PM,28/Oct/23 2:47 AM,11/Oct/19 8:52 PM,,1.13.0,,,,0,,,,"Observed:

An AttributeError was thrown during the execution of PRE_DYNAMIC_VALIDATION hook. This exception was not caught and surfaced all the way out to start_indy_node. Effectively crashing the node.

processReqDuringBatch in indy-plenum/plenum/server/replica.py catches InvalidClientMessageException and UnknownIdentifier but don't deal with more generic exceptions

 

Expected:
A pooly written hook should not be able to crash the node. At least not so easily.

 

This issue was obsured in work done at Evernym. If this ticket is worked on by an Evernym employee, [https://evernym.atlassian.net/browse/TOK-425 |https://evernym.atlassian.net/browse/TOK-425,] could provide additional context.

*Acceptance Criteria:*
* Produce a document that defines:
** the contract between Indy Node and Plugins
** description of how Indy Node will respond when a plugin does not fulfill the contract
* Write tests that ensure that Indy Node fulfills this contract.
* Ensure the contract is properly implemented in Plenum's code base",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywbyc",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,devin-fisher,esplinr,krw910,sergey.khoroshavin,sergey-shilov,,,,,,,"13/Sep/18 6:58 AM;krw910;Changing this to Highest priority because it causes a crash
;;;","19/Sep/18 10:16 PM;sergey-shilov;[~esplinr] [~danielhardman] [~krw910] [~devin-fisher] [~KitHat] [~ashcherbakov] [~sergey.khoroshavin] [~steptan]

Hello everyone.

Seems like this is such a critical problem for the indy-node that there is no any regulation of plugins behaviour.

On the node side we can not catch everything when some plugin's function is called. As a plugin is a black box for the node then exception catching code would look like 'except Exception', i.e. the most basic exception class. Ok, we can catch it, but how to handle this? For example, if PRE_DYNAMIC_VALIDATION hook raises some exception, there may be at least three possible ways:
 * Send out 'Reject' reply (but the reason is unknown or may look very strange)
 * Silently drop
 * The worst case: the plugin entered some failed state and all further requests causes raising of exception and only node process restart helps, so continue working does not make sense

But catching of basic exception does not provide any information which way to choose.

On my opinion the solution is some regulative doc (let's call it contract) that the indy-node provides for plugins. This contract will have the list of supported hooks provided by the indy-node, for each hook we can specify:
 * What exceptions are caught by indy-node (i.e. expected exceptions) and node's reaction on each type of exception (for example, send out 'Reject').
 * All other exceptions causes node's process restart (may be not immediate to have the ability to gracefully finalise some node's structures, for example flush out boxes, but anyway node's process will be restarted).

Each *valid* plugin should follow this contract, otherwise working capacity is not guaranteed.

That is my proposal, any ideas on that?;;;","20/Sep/18 6:13 PM;sergey.khoroshavin;{quote} All other exceptions causes node's process restart (may be *not immediate to have the ability to gracefully finalise some node's structures, for example flush out boxes*, but anyway node's process will be restarted). {quote}

Just wanted to elaborate on why this is important. During tests with buggy plugins we sometimes encountered the following situation:
- primary started ordering request by sending PREPREPARE and modifying uncommited state
- other nodes agreed, sent PREPARE and modified their uncommited state as well
- prepare certificate was reached on _n-f-1_ nodes and they sent COMMITs
- other _f+1_ nodes received those _n-f-1_ COMMITs as well as enough PREPAREs to reach certificate and send their own COMMIT
- now these _f+1_ nodes have _n-f_ COMMITs (including their own) so they can finish ordering of batch, and they have their own COMMIT in outbox
- _f+1_ nodes execute plugin commit hook which crash node due to some bug, so _f+1_ COMMITs are lost
- we end up with _f+1_ restarted nodes with clean consensus state (zero viewNo and ppSeqNo, no messages) and _n-f-1_ nodes with unfinished batch for which they will never get enough COMMITs
- so pool cannot order anymore until restart of at least _n-2f_ nodes with dirty state

The same situation will happen if we just restart node on unexpected plugin exception without flushing outboxes.;;;","21/Sep/18 4:12 AM;devin-fisher;[~sergey-shilov] I very much agree with you proposal to document the contract between plenum and the plugin. But I think some strategy for how plenum will protect itself in the event that the plugin don't follow the contract is still needed and is the main thing that this ticket should be driving us towards. 

 ;;;","11/Oct/19 8:52 PM;ashcherbakov;We get rid of hooks now and use Configurable request Handlers;;;","11/Oct/19 9:00 PM;esplinr;This issue makes two requests:
 * Indy Node should protect itself from plugins that are behaving badly. Our design for plugins does not allow this, as plugins can create additional ledgers that need to be managed on the audit ledger, and so if the plugin behaves badly consensus should stop until the problem is resolved. Plugins need to be tested to the same level as the rest of the ledger. However, moving from plugin hooks to plugin interfaces should make it easier to create well behaved plugins. 
 * We should document the interface. We have created the following documents:
[https://github.com/hyperledger/indy-plenum/blob/master/docs/source/plugins.md]
[https://github.com/hyperledger/indy-plenum/blob/master/docs/source/request_handling.md
]If people need more documentation, we should create new issues to guide the effort.;;;",,,,,,,,,,,,,,,,,,,
Invent some mechanism to publish load script package to pypi,INDY-1699,33723,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,13/Sep/18 9:56 PM,26/Sep/18 11:35 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,explore,,"Load script were converted to python package. For now it is in node repository and is not publishing to pypi.
We should have a mechanism to publish load script package to pypi",,,,,,,,,,INDY-1605,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxjb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Requests queue is not cleared in case of reject-nym transactions.,INDY-1700,33731,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,NataliaDracheva,NataliaDracheva,NataliaDracheva,13/Sep/18 11:45 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.78,,,,0,TShirt_M,,,"*Build version:*
indy-node: 1.6.601
indy-plenum: 1.6.539
*Test description:* 1 machine sends 5 nyms/sec to the pool by a user which has no role in the ledger.
*Preconditions:* Add a DID without role to the ledger (000000000000000000000000Trustee3)
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 5 -y one -k nym -s=000000000000000000000000Trustee3 --load_time=3600 -b=180 -d=~/Documents/logs -o=~/Documents/logs/load_script.csv
{code}
*Expected results:* 18k rejects in logs 
*Actual results:* Test was stopped after 35-40 minutes, there were 9795 rejected nyms, 100 failed and 23 nacked because of a View Change.
*Additional info:* 
Logs: evernymr33 ~/logs/indy-1593/
*Priority justification:*
When the test is executed and there is no additional load afterwards, the requests stay in queue and can not be released so it increases the load on nodes and may lead to memory overflow (DDOS attack from users which are registered in ledger and don't have enough privileges to write transactions/don't have enough tokens for fees)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1593,,,,,,,,,,,,,,,,,,,,"26/Sep/18 7:28 PM;NataliaDracheva;metrics.png;https://jira.hyperledger.org/secure/attachment/15870/metrics.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzsf3:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),NataliaDracheva,spivachuk,,,,,,,,,,,"24/Sep/18 7:11 PM;spivachuk;*Problem reason:*
- Rejected requests (i.e. requests which failed dynamic validation) were not marked as executed when 3PC-batches containing them were ordered. This prevented removal of rejected requests from Propagator.requests collection on checkpoint stabilization.

*Changes:*
- Modified the test verifying removal of requests from Propagator.requests collection on checkpoint stabilization: now removal of rejected requests is also verified. Corrected a related test helper-function and tests using it.
- Added marking rejected requests as executed along with valid requests on ordering of 3PC-batches. This fixed the issue with lack of removing rejected requests from Propagator.requests collection on checkpoint stabilization.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/920
- https://github.com/hyperledger/indy-node/pull/949

*Version:*
- indy-node 1.6.610-master
- indy-plenum 1.6.544-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_request_older_than_stable_checkpoint_removed}};;;","26/Sep/18 7:29 PM;NataliaDracheva;*Scenario 1:*
 *Build version:*
indy-node: 1.6.610
indy-plenum: 1.6.545
 *Test description:* 5 reject-nyms/sec are sent from one machine.
 *Preconditions:* Add a user without role to the ledger and send requests as the user.
 *Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 200 -l 5 -y one -k nym -s=000000000000000000000000Trustee3 --load_time=3600 -b=180 -o=load_script.csv
{code}
*Expected results:* Requests queues size do not grow infinitely.
 *Actual results:* Requests queues size do not grow infinitely. (/)
 *Additional info:* ~/logs/1700_confirmation_610_545.zip
 !metrics.png|thumbnail!;;;",,,,,,,,,,,,,,,,,,,,,,,
Review and  fix not pinned dependencies in indy-plenum,INDY-1701,33742,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,14/Sep/18 12:51 AM,30/Oct/19 6:23 AM,28/Oct/23 2:47 AM,30/Oct/19 6:23 AM,,,,,,0,devops,,,"The task reflects recent discussion with Daniel about the list of things that should be presented in projects.
 Need to use strict dependencies instead of omitted ('latest') ones.

*Acceptance Criteria*
 * List the dependencies than need pinning
 * For each dependency, briefly evaluate whether the project has traditionally broken during minor upgrades
 ** Review previous release notes
 ** Review our previous experience depending on the package
 * Make a proposal for how to pin
 ** Weigh the QA risk of floating packages
 ** Compare with the risk of us missing a service pack security fix or breaking when ported to other platforms (Fedora)
 * Get approval from Daniel
 * Implement the change
 * If a dependency currently has an upgrade available or that will be available soon, create an issue for testing the upgrade.

*Notes*
 * We are pinning to the currently known working versions. Upgrading the packages is not required.
 * We don't want to pin packages used exclusively for testing. They don't impact QA, and we can pin them later if they cause a build breakage and fixing them is too big of a burden.
 * We are guessing that Plenum has less than 3 not pinned dependencies.

*Questions*
 * What is our plan for upgrading package dependencies in the future?
 ** Can we have development packages that always use the latest dependencies so that we identify problems quickly?
 ** How often should we schedule work to upgrade the dependencies?",,,,,,,,,,,,,,,,,INDY-1702,INDY-1703,,,INDY-1702,,,,,,,,INDY-2196,,,,,INDY-1702,INDY-1703,INDY-1090,INDY-1440,INDY-1706,INDY-2021,INDY-2196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxg7:i",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,danielhardman,esplinr,,,,,,,,,,"14/Sep/18 9:50 PM;andkononykhin;[~esplinr] [~ashcherbakov] [~sergey-shilov] prioritize please;;;","21/Sep/18 12:41 AM;danielhardman;I am happy to see these tickets. Thank you for taking my request to address pinning so seriously.

I'd like to clarify what I consider to be the success criteria here. My first major goal is to eliminate the phenomenon where a component below X in the stack can break X unexpectedly, because X consumes the latest version of its dependence, and ""latest"" can change without warning. We have repeatedly experienced this among the layers of our own stack. My second major goal is to improve reproducibility of builds; if you build against ""latest"" of something today, and do the same thing 6 months from now, you might not have the same binary at all.

How we pin (to an exact version or to a fuzzy version) should be governed by pragmatism and human wisdom. If we are dealing with dependencies that are known to exhibit very stable and predictable behaviors with respect to semver, then I think it would be to our advantage to use fuzzy pinning (e.g., pin to a version >= N, rathern than version = N). For example, if we have a dependency on iptables, pinning to an exact version would be very undesirable, since it prevents the system from receiving upgrades and security patches, violates sysadmin expectations, and creates unnecessary brittleness. Distro iptables packages use semver reliably, so we can predict whether an upgrade will remain compatible. I suspect that most other third-party dependencies we have are similar to iptables in this respect, in that their semver contract is reliable and fuzzy pinning is preferred.

On the other hand, if we are dealing with components that change unpredictably, and that do not give us confidence about semver conformance, then perhaps an exact pin is merited. This might be the case with our own software components, for example. Perhaps in 6 months our semver conformance will be better, but right now, many things in many of our layers can change without warning...

Expectations of sysadmins is a good indicator of whether we're making wise pinning decisions. They need to be able to patch a system using normal workflows, and not to be hampered or puzzled or shocked by constraints we create. I think the handful of packages that we write could have pretty strong version constraints, but we don't want to prevent them from picking up security patches from openssl or other system-relevant libraries and tools.;;;","02/Oct/18 9:25 PM;andkononykhin;[~danielhardman] [~esplinr] [~ashcherbakov] [~gudkov] [~tharmon] [~Sergey.Kupryushin] have finalized design doc:   https://docs.google.com/document/d/17larJHUcr5LoDrxTOMw71vKhAfIGt8nO51yVfQ87Lhg/edit#
I will wait for a feedback before any implementation.;;;","30/Oct/19 6:23 AM;esplinr;We made a lot of progress on this effort. Our current thinking about dependency management is in this comment:

https://jira.hyperledger.org/browse/INDY-2196?focusedCommentId=65169&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-65169;;;",,,,,,,,,,,,,,,,,,,,,
Review and  fix not pinned dependencies in indy-node,INDY-1702,33743,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,14/Sep/18 12:52 AM,30/Oct/19 6:23 AM,28/Oct/23 2:47 AM,30/Oct/19 6:23 AM,,,,,,0,devops,,,"The task reflects recent discussion with Daniel about the list of things that should be presented in projects.
Need to use strict dependencies instead of omitted ('latest') ones.

* Acceptance Criteria* 
* List the dependencies than need pinning
* For each dependency, briefly evaluate whether the project has traditionally broken during minor upgrades
** Review previous release notes
** Review our previous experience depending on the package
* Make a proposal for how to pin
** Weigh the QA risk of floating packages
** Compare with the risk of us missing a service pack security fix or breaking when ported to other platforms (Fedora)
** Get approval from Daniel
* Implement the change
* If a dependency currently has an upgrade available or that will be available soon, create an issue for testing the upgrade.

*Notes*
* We are pinning to the currently known working versions. Upgrading the packages is not required.
* We don't want to pin packages used exclusively for testing. They don't impact QA, and we can pin them later if they cause a build breakage and fixing them is too big of a burden.
* We are guessing that Node has less than 3 dependencies.",,,,,,,,,,INDY-1701,,,,,,,,,,,INDY-1703,,INDY-1701,,,,,,INDY-1701,INDY-1703,INDY-2196,,,INDY-1440,INDY-1706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxg8:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,danielhardman,esplinr,,,,,,,,,,"14/Sep/18 9:50 PM;andkononykhin;[~esplinr] [~ashcherbakov] [~sergey-shilov] prioritize please;;;","21/Sep/18 12:42 AM;danielhardman;Just repeating a comment that I also made in INDY-1701:

I am happy to see these tickets. Thank you for taking my request to address pinning so seriously.

I'd like to clarify what I consider to be the success criteria here. My first major goal is to eliminate the phenomenon where a component below X in the stack can break X unexpectedly, because X consumes the latest version of its dependence, and ""latest"" can change without warning. We have repeatedly experienced this among the layers of our own stack. My second major goal is to improve reproducibility of builds; if you build against ""latest"" of something today, and do the same thing 6 months from now, you might not have the same binary at all.

How we pin (to an exact version or to a fuzzy version) should be governed by pragmatism and human wisdom. If we are dealing with dependencies that are known to exhibit very stable and predictable behaviors with respect to semver, then I think it would be to our advantage to use fuzzy pinning (e.g., pin to a version >= N, rathern than version = N). For example, if we have a dependency on iptables, pinning to an exact version would be very undesirable, since it prevents the system from receiving upgrades and security patches, violates sysadmin expectations, and creates unnecessary brittleness. Distro iptables packages use semver reliably, so we can predict whether an upgrade will remain compatible. I suspect that most other third-party dependencies we have are similar to iptables in this respect, in that their semver contract is reliable and fuzzy pinning is preferred.

On the other hand, if we are dealing with components that change unpredictably, and that do not give us confidence about semver conformance, then perhaps an exact pin is merited. This might be the case with our own software components, for example. Perhaps in 6 months our semver conformance will be better, but right now, many things in many of our layers can change without warning...

Expectations of sysadmins is a good indicator of whether we're making wise pinning decisions. They need to be able to patch a system using normal workflows, and not to be hampered or puzzled or shocked by constraints we create. I think the handful of packages that we write could have pretty strong version constraints, but we don't want to prevent them from picking up security patches from openssl or other system-relevant libraries and tools.;;;","30/Oct/19 6:23 AM;esplinr;We made a lot of progress on this effort. Our current thinking about dependency management is in this comment:

https://jira.hyperledger.org/browse/INDY-2196?focusedCommentId=65169&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-65169;;;",,,,,,,,,,,,,,,,,,,,,,
Review and  fix not pinned dependencies in indy-anoncreds,INDY-1703,33744,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,andkononykhin,andkononykhin,14/Sep/18 12:52 AM,11/Oct/19 9:01 PM,28/Oct/23 2:47 AM,11/Oct/19 9:01 PM,,,,,,0,devops,,,"The task reflects recent discussion with Daniel about the list of things that should be presented in projects.
Need to use strict dependencies instead of omitted ('latest') ones.

* Acceptance Criteria* 
* List the dependencies than need pinning
* For each dependency, briefly evaluate whether the project has traditionally broken during minor upgrades
** Review previous release notes
** Review our previous experience depending on the package
* Make a proposal for how to pin
** Weigh the QA risk of floating packages
** Compare with the risk of us missing a service pack security fix or breaking when ported to other platforms (Fedora)
** Get approval from Daniel
* Implement the change
* If a dependency currently has an upgrade available or that will be available soon, create an issue for testing the upgrade.

*Notes*
* We are pinning to the currently known working versions. Upgrading the packages is not required.
* We don't want to pin packages used exclusively for testing. They don't impact QA, and we can pin them later if they cause a build breakage and fixing them is too big of a burden.
* We are guessing that Indy-Anoncreds has less than 5 dependencies.",,,,,,,,,,INDY-1701,,,,,,,,,,,,,INDY-1702,,,,,,INDY-1701,,,,,INDY-1702,INDY-1440,INDY-1706,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxg8:i",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,danielhardman,,,,,,,,,,"14/Sep/18 9:50 PM;andkononykhin;[~esplinr] [~ashcherbakov] [~sergey-shilov] prioritize please;;;","21/Sep/18 12:42 AM;danielhardman;Just repeating a comment I made in INDY-1701:

I am happy to see these tickets. Thank you for taking my request to address pinning so seriously.

I'd like to clarify what I consider to be the success criteria here. My first major goal is to eliminate the phenomenon where a component below X in the stack can break X unexpectedly, because X consumes the latest version of its dependence, and ""latest"" can change without warning. We have repeatedly experienced this among the layers of our own stack. My second major goal is to improve reproducibility of builds; if you build against ""latest"" of something today, and do the same thing 6 months from now, you might not have the same binary at all.

How we pin (to an exact version or to a fuzzy version) should be governed by pragmatism and human wisdom. If we are dealing with dependencies that are known to exhibit very stable and predictable behaviors with respect to semver, then I think it would be to our advantage to use fuzzy pinning (e.g., pin to a version >= N, rathern than version = N). For example, if we have a dependency on iptables, pinning to an exact version would be very undesirable, since it prevents the system from receiving upgrades and security patches, violates sysadmin expectations, and creates unnecessary brittleness. Distro iptables packages use semver reliably, so we can predict whether an upgrade will remain compatible. I suspect that most other third-party dependencies we have are similar to iptables in this respect, in that their semver contract is reliable and fuzzy pinning is preferred.

On the other hand, if we are dealing with components that change unpredictably, and that do not give us confidence about semver conformance, then perhaps an exact pin is merited. This might be the case with our own software components, for example. Perhaps in 6 months our semver conformance will be better, but right now, many things in many of our layers can change without warning...

Expectations of sysadmins is a good indicator of whether we're making wise pinning decisions. They need to be able to patch a system using normal workflows, and not to be hampered or puzzled or shocked by constraints we create. I think the handful of packages that we write could have pretty strong version constraints, but we don't want to prevent them from picking up security patches from openssl or other system-relevant libraries and tools.;;;","11/Oct/19 9:01 PM;ashcherbakov;indy-anoncreds is deprecated;;;",,,,,,,,,,,,,,,,,,,,,,
POA: Require multiple signatures for important transactions,INDY-1704,33757,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,esplinr,esplinr,14/Sep/18 7:29 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"*Story*
As a trustee of an Indy Network, the network should require other trustees to sign administrative transactions I want to sponsor so that I am confident that no single trustee can abuse the network.

*Acceptance Criteria*
* All transactions that require a trustee role should require multiple trustee signatures. Specifically:
** Add a trustee
** Remove a trustee
** Add a steward
** Revoke the role of steward
** Demote a consensus node
** Send an upgrade transaction to the ledger
* The number of signatures required should be specified on the configuration ledger
* Create a Plan of Attack for how we would address this problem.
* Create a HIPE describing how we want to handle trustee permissions.
* Create an Epic and issues for doing the work.

*Notes*
* In the future, it would be useful to specify the number of signatures as a ratio of total trustees, but this is not required for this story. (Depends on INDY-1594)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1594,INDY-1527,INDY-1728,INDY-1729,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwwo7:",,,,Unset,Unset,EV 18.19,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,esplinr,sergey.khoroshavin,sergey-shilov,,,,,,,,"18/Sep/18 10:41 PM;sergey.khoroshavin;[~esplinr]
I have several questions regarding specifying required number of signatures in configuration ledger:
* Do you want to specify only number of signatures (with affected transactions essentially hardcoded) or you want to specify required number of signatures per transaction type? First case is much simpler to implement, but I'm afraid it may cause problems in future when more requirements are added. Second one will essentially require moving _auth_map_ to config ledger which is quite big task, but it's much more flexible.
* Do we have plans to make configurable number of stewards to approve some transactions as well, or it's just for trustees?
* It seems like we should have some defaults when no such transaction is written to configuration ledger (I don't think we can afford rewriting SLN ledgers). If so, what are those defaults?
* As for specifying ratio of total trustees - while actual implementation is blocked by INDY-1594 I believe we should add this to protocol upfront. Probably two numbers - absolute and relative number of trustees, taking maximum of them when deciding on final number. Does it seem reasonable?
;;;","20/Sep/18 9:32 PM;esplinr;My suggested responses:
1. The number of signatures should be specified per transaction type, so that some transactions can have a higher trust requirement than others.
2. This should also apply to steward transactions so that the ledger can be configured such that one steward cannot control the ledger.
3. The default should be one signature is required to approve the transaction. This is to make it easy for people running development and test ledgers.
4. It does seem prudent to include in the transaction specification the ability to declare a ratio of trustees or stewards. Rather than having two numbers, and having a rule for which is valid, I was thinking there would be a single number and an indicator whether it was a percentage ratio or an absolute number. That's simply how I was thinking about it; I would consider the architects' opinion on how it should be implemented (and whether to postpone specifying the percentage ratio) to be a more authoritative opinion.;;;","20/Sep/18 11:49 PM;sergey-shilov;I just want to comment that it is not fully correct to use *""transaction""* and *""transaction type""* words. Add a trustee, suspend a trustee, add a steward, revoke the role of steward - these are all the *same* transaction type. They are all *actions*, *not transaction types*. So identifying of these actions is a topic for discussion.

For now we can say that configuration ledger will store something like that:
 * _<action1>: <required number of sigs>_
 * _<action2>: <required number of sigs>_
 * _<action3>: <required number of sigs>_

But for now it is not clear what the _actionN_ is.;;;","21/Sep/18 4:21 AM;sergey.khoroshavin;[~sergey-shilov]
{quote} They are all actions, not transaction types. So identifying of these actions is a topic for discussion. {quote}
I believe these actions are already clearly defined in _auth_map_, so it all boils down to moving it out of code into config ledger (so it's really configurable) and adding support for configuring required number of signatures. Please correct me if I'm wrong.;;;","25/Sep/18 8:03 AM;esplinr;Thank you [~sergey-shilov] for the clarification. The approach discussed by [~sergey.khoroshavin] does appear to meet the requirement.

Sergey K. also mentioned that using two numbers (both absolute and percentage) has the advantage of allowing logic such as ""33% of trustees but not less than 2"". That is an interesting advantage that it is good to consider. I worry about the added complexity in implementation and user configuration, but the rule ""if both number are specified use whichever is bigger"" is simple enough  to communicate. I like the approach.;;;","26/Sep/18 2:19 AM;sergey-shilov;*Plan of Attack*

 

*Requirement*

According to the story the network should require several trustees to sign administrative transactions to do some actions that are important for the network. Otherwise the transaction whould be rejected due to insufficient number of signatures. Also the quorum of trustees should be configurable for different kinds of actions.

 

*What to do*
 Current implementation of the indy-node does not support such functionality. To address this issue we need to do several consequence steps:
 * find out the structure of ""actions"" and their definition
 * apply them for incoming transactions as a part of authorisation
 * store them in config ledger to make them configurable

 

*How to do*

For now we have a static structure called auth_map. This structure defines permissions for various actions. So we can use this data structure as a base for new rules definition, but most probably it will require some changes and extensions. Another issue is the fact that auth_map structure is static, so we should find a solution how to move it to the config ledger.

Moving the auth_map to config ledger requires significant work as we should define corresponding transactions and so on. It would be more efficient for whole stack to implement a solution with static rules first.

At the first step we can use absolute numbers of required signatures. Further we can use a strategy of combination of absolute numbers and percentage to implement a rules like ""33% of trustees but not less than 2"", but using of percentage requires INDY-1594 complete.

Since we plan to move the auth_map structure to the config ledger as the last step there is the question of atomicity of adding and modification of the rules for all administrative actions is still under discussion. Possible solutions:
 # each rule can be added/modified by separate transaction;
 # all rules are added/modified by single transaction, i.e. this transaction contains a list of all rules for administrative actions.

I think, the first point is better.

One important thing regarding the client side: the indy-node will expect an administrative transaction *sent with the same source DID (action initiator) and signed by required number of trustees*. Otherwise the transaction will be rejected.

 

*Technical tasks:*

So our current plan is:
 # Modify and extend the auth_map structure to store the authorization rules for administrative actions.
 # Define the authorization rules for administrative actions with quorums as constants (hard-coded constants for absolute number of signers).
 # Modify applying of the rules stored if the auth_map according to changes made by steps 1 and 2.
 # Support a strategy to specify per-cent of signers instead of absolute number
 # Move the auth_map structure to the config ledger.

 ;;;",,,,,,,,,,,,,,,,,,,
Primary node votes for view change against itself.,INDY-1705,33940,,Bug,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,sergey-shilov,NataliaDracheva,NataliaDracheva,20/Sep/18 6:48 PM,28/Sep/18 2:43 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*dBuild version:*
 indy-node: 1.6.603
 indy-plenum: 1.6.539
 libindy & wrapper: 1.6.1~754
 *Steps to Reproduce:*
 1. Turned on dynamic quotes in indy config.
 2. Ran perf_processes.py from 4 AWS agent with following parameters:
{code:java}
python3.5 perf_processes.py -g ~/indy-node/scripts/performance/pool_transactions_genesis -m t -n 1 -c 200 -l 10 -y one -k nym --out=out.txt --load_time=7200
{code}
*Expected results:* If there is something wrong with a primary node, other nodes vote for View Change.
 *Actual results:* When ViewNo was 0, primary (Node1) voted for IC by 26th reason.
 *Additional info:*
 Validator-info-history on the 1st node:
 ""View_change_status"":
 ""IC_queue"":
 ""1"":
 ""Message"": INSTANCE_CHANGE

{'viewNo': 1, 'reason': 26}

""Vouters"":
 Node1
 logs: ev@evernymr33:~/logs/indy-1602/1602_scen1_603_754.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxjj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explore and choose python dependency manager,INDY-1706,33941,,Task,In Progress,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,andkononykhin,andkononykhin,andkononykhin,20/Sep/18 7:05 PM,30/Jul/19 11:12 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"Indy projects should have more predicable and stable list of dependencies to make reproducible builds possible. We need some kind of lock file for all dependency tree to be sure that we can reproduce the env where we work well. But there is no such PEP that introduces thus no standard way here.

Keeping all that in mind I see several workarounds:
* adapt legacy pip+virtualenv and use setup.py / requirements.txt for that purpose (hard to manage)
* use custom solutions provided by newly appeared  (but already popular and widely discussed) dependency managers which provide promising improvements here
** [pipenv|https://github.com/pypa/pipenv/]
** [poetry|https://github.com/sdispater/poetry]
** [hatch|https://github.com/ofek/hatch]
** ... (? other)

*Note*. Possibly it's not a time to do a final decision now since mentioned tools have a set of drawbacks no one is accepted by major part python community.

Useful articles:
* [Python's New Package Landscape|http://andrewsforge.com/article/python-new-package-landscape/]
* [Pipenv: promises a lot, delivers very little|https://chriswarrick.com/blog/2018/07/17/pipenv-promises-a-lot-delivers-very-little/]

Also these docs should be taken into account to make a decision:
* [PEP 517|https://www.python.org/dev/peps/pep-0517/]
* [PEP 518|https://www.python.org/dev/peps/pep-0518/]
",,,,,,,,,,,,,,,,,INDY-1440,,,,,,,,,,,,INDY-1701,INDY-1702,INDY-1703,,,INDY-2196,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxg9:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI command 'did rotate-key' doesn't work in some cases,INDY-1707,33952,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,zhigunenko.dsr,ozheregelya,ozheregelya,21/Sep/18 2:25 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,TShirt_S,,,"*Steps to Reproduce:*
1. Create several NYMs in the wallet and send them to the ledger.
2. Try to run 'did rotate-key' for any of them.

*Actual Results:*
Error message appears:
{code:java}
Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(0, 1)
{code}
New verkey was updated in the domain ledger, but it was not updated in the wallet. So, DID can't be used anymore.

*Expected Result:*
Verkey should be rotated in both of ledger and wallet.


*Additional Information:*
The issue was noticed during running this instruction: https://docs.google.com/document/d/1QOfPsylBnSNa9Ke19IoXLdPbKvbXjdhYraRi4JDX8iA/edit

Logs and nscapture: s3://qanodelogs/TOK-434/NodeXX/
or: [https://drive.google.com/open?id=1KnymbWO9N8Dq-OY9Qun29brLtv8wdsEd]

*WORKAROUND (how to get DID working again after this problem with rotation):*
Use seed for key rotation. This case it will be possible to change verkey in the wallet manually using
{code:java}
did new did=<current DID> seed=<seed from failed rotate-key command>{code}",indy-node 1.6.73,,,,,,,,,INDY-1649,,,,,,,,,,,,,,,,,,,,,,,,IS-1022,,,,,,,,,,,,,,,"01/Oct/18 7:34 PM;zhigunenko.dsr;Node1.log;https://jira.hyperledger.org/secure/attachment/15897/Node1.log","01/Oct/18 7:34 PM;zhigunenko.dsr;Node2.log;https://jira.hyperledger.org/secure/attachment/15898/Node2.log","01/Oct/18 7:34 PM;zhigunenko.dsr;Node3.log;https://jira.hyperledger.org/secure/attachment/15899/Node3.log","01/Oct/18 7:34 PM;zhigunenko.dsr;Node4.log;https://jira.hyperledger.org/secure/attachment/15900/Node4.log","01/Oct/18 7:34 PM;zhigunenko.dsr;indy_cli.log;https://jira.hyperledger.org/secure/attachment/15901/indy_cli.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzsfr:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,sergey.khoroshavin,zhigunenko.dsr,,,,,,,,,"21/Sep/18 7:50 PM;sergey.khoroshavin;This bug is caused by fundamental problem - request authentication accesses uncommitted state outside of ordering. This looks like:
* client send request rotate keys to pool
* this request reaches some (in fact one is enough) nodes, they verify it against current uncommitted state and propagate to other nodes
* some propagates reach other nodes and are also authenticated against current uncommitted state (client requests are still in flight)
* propagates achieve quorum, so primary sends preprepare, and non-primary receive it, which modifies their uncommitted state (and client requests are still in flight)
* client requests finally reach nodes, but state is already updated with new key, so they fail authentication and REQNACKs are sent
* other propagates with this request also reach nodes, and also fail authentication leading to nodes raising suspicion on each other
* ordering finishes successfully (so keys in ledger are updated), but client thinks that request was rejected and key was not updated

The most bullet-proof solution to this problem is to move authentication into dynamic validation. I'm not going to propose this now, because it will greatly simplify DDoS attacks (lots of invalid requests will get to propagate phase), but I'm leaving it for future reference in case all other solutions fail to solve the problem.

Much more practical solution is following:
* implement INDY-1649, which is basically authentication cache
* probably move check if request is already ordered BEFORE authentication

This will lead to following scenario:
* client send request rotate keys to pool
* this request reaches some (in fact one is enough) nodes, they verify it against current uncommitted state, store the fact that request is already successfully verified in cache and propagate it to other nodes
* some propagates reach other nodes and are also authenticated against current uncommitted state, updating auth cache (client requests are still in flight)
* propagates achieve quorum, so primary sends preprepare, and non-primary receive it, which modifies their uncommitted state, but auth cache is intact (and client requests are still in flight)
* client requests finally reach nodes, and due to auth cache are successfully authenticated
* other propagates with this request also reach nodes, and auth cache shows they are correctly authenticated
* ordering finishes successfully (so keys in ledger are updated), and client knows it

Still, there is edge case: if we clean auth cache upon ordering request (and we want to do that, otherwise memory requirements are unbounded) client might be unlucky enough for it's request to reach the node when auth cache is already cleared, resulting in REQNACK. However, this is possible only when node already ordered request, which means it already sent REPLY to client (this happens on successful ordering even if given node didn't receive anything from client), so in worst case client will receive REPLY followed by REQNACK, which is confusing, but client still can figure out that key was updated. To battle this we need to move check if request is already ordered before authentication, in this case node will send two identical REPLYs. However implications of this second change are probably better be researched a bit more.

;;;","28/Sep/18 2:48 AM;ashcherbakov;Needs to be re-tested as INDY-1649 is done;;;","01/Oct/18 7:34 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.619
indy-plenum 1.6.553
indy-cli 1.6.6~759

*Steps to Reproduce:*
1. Create Docker pool with 4 nodes
2. Connect to pool and rotate default Trustee key many times
3. run [traffic_shaper.sh|https://docs.google.com/document/d/1swLE5FwYY0tQXWpcge7KNQ5oosiz4P57Iz9843e7uHY/edit#] with default latencies
4. try to rotate key few times
5. try to send NYM

*Expected results:*
CLI guarantees consistency between keys in ledger and in local storage

*Actual Results:*
On step 2 keys successfully changed in ledger
On step 4 keys successfully changed in CLI, but ledger response falls in timeout
On step 5 error appears: _Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(0, 1)_;;;","03/Oct/18 11:12 PM;zhigunenko.dsr;*Reason to Close:*
Cause has located on CLI side. Moved to IS-1022;;;",,,,,,,,,,,,,,,,,,,,,
POA: Delegate issuance to a processor,INDY-1708,33958,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,,mgbailey,esplinr,21/Sep/18 5:12 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,EV-CS,,,"*Story*
As an organization issuing claims on an Indy network, I want to be able to delegate the issuing of credentials to a third party so that I benefit from them simplifying my business processes. Verifiers of those credentials should see that they came from me as the controller, but should also be able to identify the processor for audit purposes.

*Requirements*
* The controller should be able to change processors by updating the credential definition, but the history of processors should be retained.

*Acceptance Criteria*
* Create a plan
* Get architectural review of the plan from stakeholders including Evernym and the Sovrin Foundation
* Create the necessary issues

*Possible Approaches*

_Add issuer field to cred def transactions_
When using delegation, where one company (the processor) issues credentials for another (the controller), to the verifier the claims appear to have come from the controller.  This is fine, but we also need to be able to tell that the processor was involved with issuing the claim. 

To do this, please add an additional field to cred def transactions called ""issuer"".  The value of the field will be a DID that will be set by the controller before it is written to the ledger.

In addition it will be necessary for the verifier to be able to determine what cred def was used to make the claim in the proof. In the current proof format, the schema sequence number is provided for each claim, but the cred def sequence number is not. Other than parsing the ledger to find the cred def with a particular ""ref"" and ""identifier"", which would be very inefficient, there is not an apparent way for the verifier to find the correct cred def. If indeed there currently is no better way, then please also add a cred def sequence number to the claims in a proof.

It will then be the responsibility of the verifier to retrieve the cred def from the ledger to determine the processor, when needed. Two very key customers currently need this capability, with more to follow.

The additional field in the cred def allows the controller to show that he has delegated proof issuance to the processor who actually holds the private keys to the cred def.

*Notes*
This story introduces a new participant in credential verification. The 3-party story (issuer, prover, and verifier) is modified into a 4-party story, where the issuer is split into 2, the controller and the processor. Both will need DIDs on the network, so that the verifier is able to see that the credential is from the controller, by means of the processor.",,,,,,,,,,,,,,,,,,,,,,,INDY-1563,,,,,,,,,,,INDY-1693,INDY-1694,INDY-1695,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxiv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,brycecurtis,danielhardman,Derashe,devin-fisher,esplinr,gudkov,mgbailey,,,,,"02/Nov/18 7:48 AM;esplinr;We discussed this at some length. The use case of a credential issuer contracting with a processor to issue the credentials is an important one which we want to be successful. However, we don't think the approach in this story is the best way to achieve that goal.

We want the ledger to be as dumb as possible, and is not the right place to record the relationship between an issuer and a processor. Recording this information on the ledger complicates the system and introduces an additional (though probably small) risk of correlation.

Delegation is a key management issue, and should be governed off of the ledger by legally enforceable contracts between the verifier and the processor. The contract should enforce the terms of the relationship, and the issuer is responsible for maintaining the records necessary to audit the issuance of their credentials.

However, we recognize that the ledger could help enforce these relationships. We will continue to investigate the right way to do this, and when we have a plan we will raise separate issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Faulty primary can order and write already ordered and written request.,INDY-1709,33979,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,Derashe,Derashe,21/Sep/18 9:02 PM,01/Aug/19 9:58 PM,28/Oct/23 2:47 AM,17/May/19 5:57 PM,,,,,,0,,,,"If primary will include digest of already ordered request, then this pre-prepare will not be considered as faulty one and ""repeated"" request will be ordered by every node again.

This is bad because:
 * Faulty primary node can save some txn and when state changed, this node can revert state to saved txn
 * Primary can degrade performance by not ordering new client requests and only re-ordering old already ordered txns",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001ywbt",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,,"21/Sep/18 9:06 PM;Derashe;Test that reproduce this problem: https://github.com/hyperledger/indy-node/pull/948;;;","17/May/19 5:57 PM;ashcherbakov;Fixed in the scope of https://jira.hyperledger.org/browse/INDY-1757;;;",,,,,,,,,,,,,,,,,,,,,,,
"When installing indy-node, installer should place an apt hold on libindy-crypto",INDY-1710,33990,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,mgbailey,mgbailey,22/Sep/18 5:40 AM,24/Jul/19 3:28 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Stewards are reporting that libindy-crypto is not ""held"" in apt, meaning that it will likely be inadvertently upgraded in the course of routine maintenance. I will try to mitigate this by emailing instructions to existing stewards to correct the lapse.

To help future stewards, please have the package installer add this to the list of packages that are held in apt.",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1323,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwqr4:i",,,,Unset,Unset,CommunityContribution,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,mgbailey,,,,,,,,,,,"25/Sep/18 7:42 PM;andkononykhin;Seems as a duplicate of INDY-1323;;;",,,,,,,,,,,,,,,,,,,,,,,,
Call stack overflow during catch-up of large ledger,INDY-1711,34000,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,spivachuk,spivachuk,23/Sep/18 6:51 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,TShirt_M,,,The recursion in {{LedgerManager._processCatchupReplies}} method causes the call stack overflow during catch-up of a large ledger. This results in the node process fall and also may result in corruption of a state (see the comments to INDY-1657 for details).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1657,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzsfb:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),NataliaDracheva,ozheregelya,spivachuk,Toktar,zhigunenko.dsr,,,,,,,,"05/Oct/18 9:25 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/936;;;","10/Oct/18 11:21 PM;Toktar;Problem reason:
 - The recursion in LedgerManager._processCatchupReplies method causes the call stack overflow during catch-up of a large ledger.

Changes:
 - Changed a recursion to a cycle in  LedgerManager._processCatchupReplies method
 - Added a unit tests for CatchupRep logic.

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/936]
 * [https://github.com/hyperledger/indy-node/pull/971]

Version:
 * indy-node 1.6.627 -master
 * indy-plenum 1.6.560 -master

Risk factors:
 - Catch-up process

Risk:
 - Low

Covered with tests:
 * [test_process_catchup_replies.py|https://github.com/hyperledger/indy-plenum/pull/936/files#diff-7623a00740392088721dd40ae96a2d58]

Recommendations for QA:
 * Test catch-up of a large ledger.;;;","11/Oct/18 5:07 PM;zhigunenko.dsr;*Steps to Validate:*
1) Setup docker / AWS pool with 4 nodes
2) Stop the third node
3) Write 300-500k txns
4) Set network delay on node1 and node2
5) Start and catchup node3;;;","18/Oct/18 5:55 PM;ozheregelya;*Steps to Validate* (correction with regard to INDY-1657)*:*

1. Setup AWS pool (because testing in docker will not make sense because OOM in docker will happen faster than in case of AWS pool).
2. Fill the ledger using load script, or restore it form backup on all nodes exclude one (ledger size should be at least 600K because initial problems with large ledger catch up were noticed on ~600K ledger, but for ~300K catch up worked).
3. Prepare node for catch up.
3.1. Stop the instance, change instance type to m4.2xlarge, start the instance (it is expected that catch up will fail because of OOM on node with 8GB RAM).
3.2. Make sure that node is clear (no data in ledgers).
3.3. Make sure that genesis files are correct.
3.4 (optional). Add additional logging for catch up by changing 'trace' to 'info' here:
https://github.com/hyperledger/indy-plenum/pull/916/files#diff-d2bf9d811236a80877dac9c803bd2a0aR1975
3.5. Make sure that log level is INFO.
4. Start the node which was prepared to catch up.
5. Wait until the end of catch up.
6. Write several txns, make sure that they were written by all nodes including catched up node.
7. Run small load test (100-1000 txns) without high load. Make sure that all nodes have wrote all txns.;;;","23/Oct/18 12:14 AM;NataliaDracheva;*Scenario 1:*
*Build version:* 
indy-node: 1.6.639
indy-plenum: 1.6.568
*Test description:* Test catch up.
*Preconditions:* Node 24 and Node 25 are demoted, the pool is filled with 500k transactions (nyms)
*Steps to Validate:*
1. Promote Node 24 and restart its indy-node service.
=> Node 24 successfully finished a catch up of 500k transactions.
2. Fill ledger with 200k more transactions.
3. Promote Node 25 and restart its indy-node service.
*Expected results:* Both demoted nodes could finish a catch up successfully.
*Actual results:* Both demoted nodes could finish a catch up successfully.
*Additional info:* ~/logs/indy-1711/success/;;;",,,,,,,,,,,,,,,,,,,,
As a dev/QA I want to see metrics online as load test goes on,INDY-1712,34033,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,25/Sep/18 12:02 AM,25/Sep/18 12:03 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"One possible solution is to use Prometheus monitoring system (probably with Grafana dashboard), export could be done using [official client library|https://github.com/prometheus/client_python] which is under Apache 2.0 license.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzs8v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Explore how fees affect node stability and performance,INDY-1713,34043,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ozheregelya,ozheregelya,25/Sep/18 4:29 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"As we can see in test results in INDY-1661, our stability and performance in case of load with fees significantly degrade in comparison with the same transactions types without fees. Examples from test results:
|| ||Txns written before OOM
 (10 txns/sec load)||Pool throughput||
|NYMs|at least 419389|at least 20 nyms/sec|
|NYMs+fees|273739|14 nyms+fees/sec (with input load 15 txns/sec)|

*Logs and metrics:*
 Test case ID: Ext-25-09-18-nym-schema-fees-nym
 Load: 10 writes/sec (5 nyms with fees + 5 schemas with fees), ~1000 mixed reads/sec.
 Metrics visualization:
 !Ext-25-09-18-nym-schema-fees-nym.png|thumbnail!
 AWS s3 link: s3://qanodelogs/INDY-1713-nym-schem
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/INDY-1713-nym-schem/ /home/ev/logs/INDY-1713-nym-schem/

 

*Acceptance criteria*
 * Find out how much time request validation takes for every txn type with FEEs and without",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Sep/18 3:44 AM;ozheregelya;Ext-25-09-18-nym-schema-fees-nym.png;https://jira.hyperledger.org/secure/attachment/15868/Ext-25-09-18-nym-schema-fees-nym.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-781,,,No,,Unset,No,,,"1|hzwx7z:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,VladimirWork,,,,,,,,,,,"12/Nov/18 5:02 PM;VladimirWork;PoA:
Run load tests with all txn types with fees and compare requests processing time / RAM consumption / VC rate *with and without* fees:
||TXN_TYPE||WITH_FEES (ms)||WITHOUT_FEES (ms)||
|NYM|14.4|2.5|
|ATTRIB|16.4|4.1|
|SCHEMA|14|4.5|
|CRED_DEF|14.7|5.2|
|REVOC_REG_DEF|15|5.1|
|REVOC_REG_ENTRY|14.1|9.1|
|PAYMENT|14.7|10.5|;;;","19/Nov/18 11:38 PM;VladimirWork;Actual Results:
We have some delta time for all txn types so we should investigate reasons of slower validation fees txns.;;;",,,,,,,,,,,,,,,,,,,,,,,
Assist EV QA,INDY-1714,34064,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,25/Sep/18 7:36 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"The Evernym Indy Node team will be assisting other Evernym teams with QA tasks.

*Acceptance Criteria*
* Each engineer will spend one day running QA scripts provided by the Evernym QA team.
* Instructions on what to test, and how to report the results will be provided by [~krw910].
* Part of the day should be spent doing structured QA, and part should be spent doing related exploratory testing. 
* Also provide feedback on how to improve the testing instructions and activities.",,,,,,,,,,,,,,,,,,,,,,,IS-1011,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx6n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),0.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Triage old bug reports,INDY-1715,34066,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,25/Sep/18 7:54 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"The Evernym Indy Node team will be closing old bug reports that are unlikely to be reproducible on the current release.

*Acceptance Criteria*
 Overall:
 * Each engineer will spend one day of the sprint in this activity.
 * The goal is to review at least 8 issues (less than 1 hour per issue).

For each issue:
 * Read the issue and the comments.
 * Briefly investigate the issue using the current code base (not the code base where the issue was found).
 * Close the issue as ""Done"" if:
 ** It appears to be something that we tested as part of our most recent release, and we didn't see the issue.
 ** You try to reproduce the issue, and didn't see the issue.
 ** A corresponding task/story exists (or created by the engineer) that addresses the same issue. Make sure that the task/story is linked to the bug.
 * Keep the issue open if you are able to reproduce it or suspect that it is a real issue after your brief investigation.
 * Include a comment documenting:
 ** why you closed the issue,
 ** or what you found during your investigation, including your current environment, symptoms observed, and steps to remediate.
 * Create any related tickets that your investigation shows are necessary.
 * If you think any of these issues should be immediately addressed, please escalate to [~ashcherbakov] and [~esplinr]

*Notes*
 * Close the issue as ""Done"" if you are at least 80% confident that the issue does not exist in the latest code base. The goal is to separate important issues from unimportant issues, and real issues will surface again at some point in the future.
 * If during your investigation you see that you can fix the issue quickly (only a couple of hours), proceed to fix the issue.",,,,,,,,,,,,,,,,,,,,,INDY-1734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzsen:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),0.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compare rocksdb pool performance with leveldb one,INDY-1716,34069,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,VladimirWork,VladimirWork,25/Sep/18 5:23 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,explore,,,"*PoA:*

*Txn types:*
1. All types excluding payments and no fees.
2. All types including payments and with fees.

*Load:*
1. 5 writing / 50 reading.
2. 10 writing / 100 reading.
3. 20 writing / 200 reading.

*DB:*
1. LevelDB.
2. RocksDB.

*Additional Cases:*
1. SSD storage.
2. Magnetic storage.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzshb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,VladimirWork,,,,,,,,,,,"15/Oct/18 11:35 PM;esplinr;We have completed our transition to RocksDB, and during that process did enough testing to be satisfied that it is faster than LevelDB. We don't feel like we need further investigation into LevelDB.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Measure performance improvements,INDY-1717,34111,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,zhigunenko.dsr,esplinr,26/Sep/18 11:15 PM,23/Aug/19 9:59 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"In recent sprints, we made a number of changes that should improve performance. We want to measure and report on the results of these changes.

*Acceptance Criteria*
Perform a test of an Indy network that has the following attributes:

* The ledger is pre-loaded with 1 million transactions
* Pool size is 25 nodes.
* 1K concurrent clients
* Over a 3 hour period induce a sustained throughput of 10 write transactions per second and 100 read transactions per second on average (from clients)
Write load is a mixture of:
** writing credentials schema (5%),
** writing credential definition (5%)
** revoke registry definition (5%)
** revoke registry update (5%)
** write DID to ledger (20%)
** write payment to ledger (45%)
** write attrib to ledger (15%)
Read load is a mixture of:
** read DID from ledger (45%)
** read credential schema (10%)
** read credential definition (10%)
** read revoke registry definition (10%)
** read revoke registry delta (10%)
** read attrib from ledger (10%)
** read payment balance from ledger (5%)
* Write response time should be less that 5 seconds (would also like a report of the average).
* Read response time should be less than 1 second (would also like a report of the average).
* Unless view change in progress, pool should write 10 txns/sec and read 100 txns/sec
* Fees should be defined for all write transaction types.",,,,,,,,,,INDY-1737,,,,,,,,,,,,,INDY-1607,,,,,,INDY-1343,INDY-2214,,,,INDY-1638,,,,,,,,,,,,,,,"11/Oct/18 11:22 PM;VladimirWork;10nyms+fees.png;https://jira.hyperledger.org/secure/attachment/16128/10nyms%2Bfees.png","11/Oct/18 11:22 PM;VladimirWork;10nyms.png;https://jira.hyperledger.org/secure/attachment/16127/10nyms.png","09/Oct/18 5:51 PM;VladimirWork;10payment+fees.png;https://jira.hyperledger.org/secure/attachment/16112/10payment%2Bfees.png","09/Oct/18 5:51 PM;VladimirWork;10payment.png;https://jira.hyperledger.org/secure/attachment/16111/10payment.png","11/Oct/18 11:22 PM;VladimirWork;1nyms+fees.png;https://jira.hyperledger.org/secure/attachment/16126/1nyms%2Bfees.png","11/Oct/18 11:22 PM;VladimirWork;1nyms.png;https://jira.hyperledger.org/secure/attachment/16125/1nyms.png","09/Oct/18 5:51 PM;VladimirWork;1payment+fees.png;https://jira.hyperledger.org/secure/attachment/16110/1payment%2Bfees.png","09/Oct/18 5:51 PM;VladimirWork;1payment.png;https://jira.hyperledger.org/secure/attachment/16109/1payment.png","23/Oct/18 7:48 PM;ozheregelya;attrib_no_fees_10_sec.png;https://jira.hyperledger.org/secure/attachment/16159/attrib_no_fees_10_sec.png","24/Oct/18 7:44 PM;ozheregelya;indy_1717_attr1_nofees.png;https://jira.hyperledger.org/secure/attachment/16161/indy_1717_attr1_nofees.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-781,,,No,,Unset,No,,,"1|hzwxbb:",,,,Unset,Unset,Ev 18.20,Ev 18.23,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,ozheregelya,VladimirWork,zhigunenko.dsr,,,,,,,,,"01/Oct/18 12:54 AM;ozheregelya;As was noticed in INDY-1607, pool fails with OOM in case of load with payments txns after several hours. So, it make sense to try the same load on pool with 32Gb RAM.;;;","01/Oct/18 3:54 AM;ozheregelya;There is a problem with load script: INDY-1737. Load script can't re-use payment address for the came client. So, we need to wait until fix of INDY-1737, or run load script more tricky for payments.;;;","09/Oct/18 5:51 PM;VladimirWork;PoA:
1. Sustain *1 / 10* NYM writing *with / without* fees for 3+ hours. (/) !1nyms.png|thumbnail!  !1nyms+fees.png|thumbnail!  !10nyms.png|thumbnail!  !10nyms+fees.png|thumbnail! 
2. Sustain *1 / 10* ATTRIB writing *with / without* fees for 3+ hours.
3. Sustain *1 / 10* SCHEMA writing *with / without* fees for 3+ hours.
4. Sustain *1 / 10* CRED_DEF writing *with / without* fees for 3+ hours.
5. Sustain *1 / 10* REVOC_REG_DEF writing *with / without* fees for 3+ hours.
6. Sustain *1 / 10* REVOC_REG_ENTRY writing *with / without* fees for 3+ hours.
7. Sustain *1 / 10* PAYMENT writing *with / without* fees for 3+ hours.  (/) !1payment.png|thumbnail!  !1payment+fees.png|thumbnail!  !10payment.png|thumbnail!  !10payment+fees.png|thumbnail! 
*8. Sustain production writing/reading mix for 3+ hours.*;;;","23/Oct/18 7:50 PM;ozheregelya;2. Sustain *1 / 10* ATTRIB writing *with / without* fees for 3+ hours.
 * 1 ATTRIB without fees. Test ID: Live-23-10-18-attr-1-no-fees
!indy_1717_attr1_nofees.png|thumbnail!  
 * 10 ATTRIB without fees. Test ID: Live-22-10-18-attr-10-no-fees
   !attrib_no_fees_10_sec.png|thumbnail!;;;","22/Nov/18 11:48 PM;ozheregelya;Following load testing was performed:
 * Production load without fees (Live-20-11-18-production-mix-no-fees): 
 ** 270K txns were written during ~10 hours of load. Pool was in consensus after the end of load, but several nodes were lagged: INDY-1865 was created for investigation of this problem.
 * Production load with fees (Live-20-11-18-production-mix-fees):
 ** 30K | 75K txns were written during ~2 hours of load. Pool was broken after the load: INDY-1867 was created for investigation of this problem.

Since we have some problems with production load with fees, it will not make a lot of sense before investigation and fixing of INDY-1867. Not tested cases from PoA of this ticket will be covered in scope of INDY-1644. So, this ticket is moved to Done.;;;",,,,,,,,,,,,,,,,,,,,
AnoncredsRevocationRegistryFullError during load test run with revoc_reg_entry txns,INDY-1718,34116,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,VladimirWork,VladimirWork,27/Sep/18 12:35 AM,09/Oct/19 6:23 PM,28/Oct/23 2:47 AM,09/Oct/19 6:23 PM,,,,,,0,,,,"Build Info:
sovrin 1.1.24 (+stable plugins)
indy-node 1.6.73
libindy 1.6.6

Steps to Reproduce:
1. Run revoc_reg_entry load test with fees:
{code:java}
perf_processes.py -g genesis -c 1 -n 1 -l 1 -k revoc_reg_entry -y one -b 1 -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext  ""{\""payment_addrs_count\"":300,\""addr_mint_limit\"":1000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""114\"":1}}""
{code}

Actual Results:
{noformat}
Time 291.31 Clients 0/1 Sent: 65 Succ: 61 Failed: 2 Nacked: 0 Rejected: 3
Task exception was never retrieved
future: <Task finished coro=<LoadClient.gen_signed_req() done, defined at /usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py:167> exception=IndyError(<ErrorCode.AnoncredsRevocationRegistryFullError: 400>,)>
Traceback (most recent call last):
 File ""/usr/lib/python3.5/asyncio/tasks.py"", line 241, in _step
   result = coro.throw(exc)
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py"", line 178, in gen_signed_req
   raise e
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py"", line 171, in gen_signed_req
   req_data, req = await self._req_generator.generate_request(self._test_did)
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_req_gen.py"", line 81, in generate_request
   raise ex
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_req_gen.py"", line 77, in generate_request
   req = await self._gen_req(submit_did, req_data)
 File ""/usr/local/lib/python3.5/dist-packages/perf_load/perf_req_gen_revoc.py"", line 133, in _gen_req
   self._default_revoc_reg_def_id, self._blob_storage_reader_cfg_handle)
 File ""/usr/local/lib/python3.5/dist-packages/indy/anoncreds.py"", line 325, in issuer_create_credential
   issuer_create_credential.cb)
 File ""/usr/lib/python3.5/asyncio/futures.py"", line 361, in __iter__
   yield self  # This tells Task to wait for completion.
 File ""/usr/lib/python3.5/asyncio/tasks.py"", line 296, in _wakeup
   future.result()
 File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
   raise self._exception
indy.error.IndyError: ErrorCode.AnoncredsRevocationRegistryFullError
Task was destroyed but it is pending!
task: <Task pending coro=<LoadClient.gen_signed_req() running at /usr/local/lib/python3.5/dist-packages/perf_load/perf_client.py:180> wait_for=<Future pending cb=[Task._wakeup()]> cb=[LoadClient.check_batch_avail()]>
{noformat}

Expected Results:
There should be no errors.

Additional Info:
There was the same issue fixed in INDY-1378.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1378,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzwxkn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"09/Oct/19 6:23 PM;ashcherbakov;This is essential issue related to ordering of transactions in a non-sync mode. There is no issue with a sync mode.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Node Service Changed function does not take into account the node running it,INDY-1719,34148,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,keichiri,keichiri,27/Sep/18 10:53 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,TShirt_S,,,"In function nodeServicesChanged in pool_manager.py, information is not updated if the node executing the function is the node whose services have been changed.

This was observed in https://jira.hyperledger.org/browse/INDY-1578

Steps to reproduce:
1. setup a running pool
2. turn off one node
3. demote the node to not be validator
4. turn on the node and connect it to others
5. promote it to be validator

This way it will happen during the promotion. It is easy to modify the scenario so it happens during demotion (dont turn off the node)

That leads to replicas state not being adjusted, and incorrent replicas state observed on that node",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1640,INDY-1506,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzsg7:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,keichiri,zhigunenko.dsr,,,,,,,,,"28/Sep/18 3:23 AM;ashcherbakov;https://github.com/hyperledger/indy-plenum/pull/922;;;","05/Oct/18 12:24 AM;Derashe;Recomendation for QA: 
 * Start a 10 nodes pool
 * Demote one node (non-primary)
 * Make a view_change
 * Promote demoted node
 * Get validator-info from promoted node. It must tell that node has 4 replicas. And Reachable_nodes_count must be 10.
 * Convey validator-info from every node to me, please

Version:
h4. [indy-node 1.6.627|https://github.com/hyperledger/indy-node/releases/tag/1.6.627-master]
h4. [indy-plenum 1.6.560|https://github.com/hyperledger/indy-plenum/releases/tag/1.6.562-master];;;","11/Oct/18 6:16 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.627
indy-plenum                1.6.560

*Steps to Validate:*
1) Start pool with 10 Docker nodes
2) Demote node6 (non-primary)
3) Restart primary node1 for view chandge
4) Promote node6

*Actual results:*
Both logs and validator-info from promoted node tell that node has 4 replicas;;;",,,,,,,,,,,,,,,,,,,,,,
View Change processing - replica ends up with incorrect primaries,INDY-1720,34151,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,keichiri,keichiri,27/Sep/18 11:26 PM,09/Apr/19 10:46 PM,28/Oct/23 2:47 AM,09/Apr/19 10:44 PM,,1.7.1,,,,0,TShirt_S,,,"It seems that during the processing of view change, node registry which is not updated is being used. The node registry being used is valid at the moment of previous view change, but does not take into account update since then. [~Derashe] found the problem. This leads to node ending up with incorrect primaries.

This was found in https://jira.hyperledger.org/browse/INDY-1578

Steps to reproduce:
1. start pool of nodes

2. turn off and demote 2 nodes

3. force a view change (restart primary)

4. promote 2 nodes

 

Simple explanation  - if a node has been demoted, view has been changed, and promoting this node caused adding new replica, then this node would choose incorrect primaries on new replica",,,,,,,,,,INDY-1946,INDY-2025,,,,,,,,,,,,,,,,,,,,,,,INDY-1640,INDY-1917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:000006r20c6",,,,Unset,Unset,Ev-Node 19.07,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,keichiri,ozheregelya,VladimirWork,,,,,,,,"11/Dec/18 1:46 AM;ozheregelya;Similar issue was reproduced on docker pool with indy-node 1.6.725.

*Steps to Reproduce:*
1. Setup the docker pool of 6 nodes.
2. Demote and stop primary (Node1).
=> Node2 become primary.
3. Run load test.
=> One more View Change was happened during load test, new primary is Node4.
4. Promote demoted node back.
=> Node1 doesn't write, it can't complete View Change and stays on ViewNo 0 and sands INSTANCE_CHANGE\{'viewNo': 3, 'reason': 26}
5. Restart Node4 to initiate View Change.
=> View Change started, but it can't be completed for a long. Node4 selected Node3 as primary after restart for previous ViewNo 2.

*Actual Results:*
Pool can't write after described steps.

*Expected Results:*
Node should be in consistent state with the pool after demotion and promotion back.
Primary should be in consistent state with the pool after restart.

Logs: s3://qanodelogs/indy-1720
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1720/ /home/ev/logs/indy-1720/;;;","13/Feb/19 11:14 PM;Derashe;*Plan of attack:*

As for now, we have mechanism, which allows newly connected nodes set same primaries, that are on other pool. 
 * When view_change completed, we are saving LedgerInfo, which includes seq_no of every ledger. Nodes pass this info to newly connected nodes so they could calculate replica's primaries based on this data.
 * In case when, addition of a new node causes addition of a new replica in pool, other nodes recalculate their primaries again despite LedgerInfo data.
 * That cause inconsistency of primaries of replicas that had just connected and replicas that adjust their primaries because of new replica.

As a hotfix we are suggesting to update stored LedgerInfo at the moment when we adjust replicas and selecting new primaries. In this case, we will send updated LedgerInfo to newly connected replicas, and everything will be in sync.

As a long term solution, we can try to merge functionality of storing current primaries with audit ledger.;;;","15/Feb/19 7:00 PM;Derashe;Earlier, we used to transfer ViewChangeDone message in CurrentState. We did it to force newly connected node to make view_change with pool set at moment of this vew_change. Problem here is that we did not repeat every pool action (demote, promote) that happened after view change. So we need to fix and replace this functionality. 

*Mention:* Fix will affect only nodes when they are in catchup process. In-sync nodes behaviour would not be touched.

 

*Feature #1.* Add new field in audit txn. This field can be :
 * seq_no of audit txn, which was the last in previous_view (0 in 0 view) (this option is more generic)

      or
 * seq_no of pool ledger txn, which was the last in previous view (0 in 0 view)

Point of keeping this value is to aknowledge at which point of time view_change happened.

*Feature #2.* Write audit txn every time we finish view_change.

*We have number of options, described below.*

*Option #1. (Require feature #1, #2)*

After we finished catching up all ledgers, we will take pool state at moment of last view_change we will calculate primaries for this moment. After that, we will sequentially go through pool ledger and apply every txn to our temporary primaries. When node process last pool txn, it will apply these primaries to current pool state.

*Option #2. (Require feature #1**, #2**)*

While we are in sync, we can persist primaries in local storage (persist set of primaries, pool ledger seq_no, view_no). 

In case if node had been disconnected/demoted and it need catchup. We will first catch up audit ledger. We will acknowledge current view_no.
 * If current view_no differs from persisted view_no, we can flush persisted primaries set, we would not need it. After that we are starting pool ledger catchup. Using postRecvTxnFromCatchup function, which will handle every txn sequentially, we can insert functionality that works as follows:
 ** if pool txn seq_no < seq_no from audit ledger, than do usual addition without primary selection
 ** if pool txn seq_no == seq_no from audit ledger, than flush current primaries, recalculate replicas count, and recalculate primaries for pool, considering audit view_no as current view_no.
 ** if pool txn seq_no > seq_no from audit ledger, than do usual addition with primary selection.
 * if current view_no == persisted view_no, than next caughtup txn must be == local persisted seq_no + 1 (to make this consistent, we need to persist primaries in executer - in moment of commitment). In this case, we can just continue do usual addition with primary selection, as pool will be consistent.

*Option #3.*

We can store current primaries in audit ledger. So when new node will connect and caughtup all ledgers, it will just apply primaries from last audit txn.

New field in audit txn would be ordered array of node's names. Because names can uniquely identify node, we can determenisticly set primary for every replica. This field will be modified every time we call select primaries (view_change, demotion, promotion).

Concerns:
 * We need to handle situation, when pool started view_change to view 3 (for example). But in the same time node caughtup audit ledger with last view_no 2. After that, this node will reject pp-s with 3-rd view. 
 ** The good thing here is that if this node get behind few checkpoints, it will start catchup, so it will get audit txns with new view. But this case need to be tested;;;","28/Mar/19 4:15 PM;ashcherbakov;This will be fixed by INDY-1945 and INDY-2025;;;","09/Apr/19 10:43 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev888

Steps to Validate:
1. Setup the docker pool of 7 nodes.
2. Demote and stop primary (Node1).
3. Run load test.
4. Promote demoted node back.
5. Restart new primary to initiate View Change.
6. Run one more load test.
7. Check all ledgers count and ViewNo at all nodes.

Actual Results:
All ledgers are in sync after the second load test. ViewNo is the same at the all nodes and VC status is not in progress. Pool is in consensus.;;;",,,,,,,,,,,,,,,,,,,,
Memory Leak Issues,INDY-1721,34158,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,28/Sep/18 12:22 AM,09/Oct/19 7:01 PM,28/Oct/23 2:47 AM,09/Oct/19 6:59 PM,,,,,,0,,,,"* Check results of the latest load testing where we fail with out of memory
 * Check if it depends on
 ** txn type
 ** logging
 ** metrics
 ** ZMQ
 * Try more profiling

 

*Plan of attack*
 # Run more load tests with different txn types and metrics
 # Run python profiler to check if there are any leaks in our main code
 # Limit ZMQ watermarks to some small number
 # Check if number of client connections affects memory consumption
 # Try to reset ZMQ listener stacks to see if memory usage decreases
 # Limit RocksDB memory usage in config
 # Use leveldb instead of RocksDB for all storages
 # Use file storage instead of LevelDB/RocksDB
 # Valgrind",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-14,,Memory Leak Issues,Done,No,,Unset,No,,,"1|hzzsxb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.6.74 Release,INDY-1722,34160,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,28/Sep/18 12:38 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"Task Overview
Perform all necessary tasks in order to release

Acceptance Criteria
* Code is tested
* Release is tagged
* Release notes are drafted
* Release notes are reviewed by Docs and Product Management
* A new release of Indy-Plenum is part of this release",,,,,,,,,,,,,,,,,,,,,INDY-1726,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzsgn:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"09/Oct/18 1:15 AM;VladimirWork;Indy-node cases are passed. There will be another acceptance testing of Sovrin package as soon as RC plugins will be ready.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Find out if there are memory leaks in interanl Plenum structures and queues,INDY-1723,34161,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Derashe,ashcherbakov,ashcherbakov,28/Sep/18 12:40 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.79,,,,0,,,,"*Acceptance criteria:*
 * Investigate the code base and find out if there are some queues or data structure that can grow and not GCed

 ** have a look at the case when the load is more than node can handle, and a lot of node-to-node messages may be stashed
 ** take into account results obtained in INDY-1688
 * Run python profiler to check if there are any leaks in our main code",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/18 5:03 PM;Derashe;image-2018-10-16-11-05-27-789.png;https://jira.hyperledger.org/secure/attachment/16141/image-2018-10-16-11-05-27-789.png","17/Oct/18 11:47 PM;Derashe;image-2018-10-17-17-49-34-832.png;https://jira.hyperledger.org/secure/attachment/16146/image-2018-10-17-17-49-34-832.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzzsev:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,,"28/Sep/18 4:23 PM;Derashe;Some metrics added to find stashing collection(s) https://github.com/hyperledger/indy-plenum/pull/929;;;","10/Oct/18 6:01 PM;Derashe;Metrics of length of collections did not give us the results. New metrics added, which measuring most 'heavy' collections 

https://github.com/hyperledger/indy-plenum/pull/934;;;","12/Oct/18 8:13 PM;Derashe;Another day, another metrics(

[https://github.com/hyperledger/indy-plenum/pull/945];;;","12/Oct/18 11:07 PM;Derashe;Test node with [1.6.632 next case:|https://github.com/hyperledger/indy-node/releases/tag/1.6.632-master]

for i in range(6):

    run 10 minutes load of 40 nym/s

    stop load for 5 minutes

run 30 minutes load of 40 nym/s

stop load and wait for 10 minutes

dump logs, metrics and validator info;;;","16/Oct/18 5:05 PM;Derashe;!image-2018-10-16-11-05-27-789.png|thumbnail!

Here's the results of upper load testing. As we can see internal structures weight does not affect memory much.;;;","16/Oct/18 10:55 PM;Derashe;During researching leaks, we've tested some cases locally ([https://github.com/hyperledger/indy-node/pull/984)]

We used such a scenario for testing on a node:
 * order n txns
 * send n txns and hold them unordered
 * order these txns
 * call_gc
 * send n txns and hold them unordered again
 * order these txns again
 * call_gc again

 ;;;","17/Oct/18 11:49 PM;Derashe;We've profiled local tests with a objgraph and when we tried to profile every dict in node's pool, we've got such a results:

!image-2018-10-17-17-49-34-832.png|thumbnail!

As you can see, we are not getting more than 150 MBs. But in the same time RSS of the python process was around 1 Gb. ;;;","17/Oct/18 11:56 PM;Derashe;We've profiled Nodes with a pympler and it also showed us that when we are stashing not more that 60 Mbs when we have over 6k txns stashed. So we can be pretty sure, that there is no leak in our node code

Also, we've probably could reproduce memory leak in test environment. So probable direction for researching is to debug local test for something out of scope node's collections (like zmq or kv storages);;;",,,,,,,,,,,,,,,,,
Check how RocksDB affects memory omsumption,INDY-1724,34163,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,28/Sep/18 1:56 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.79,,,,0,,,,"*Acceptance criteria*
 * Run a test with NYMs
 * run the same test limiting RocksDB memory usage in config
 * run the same test with leveldb instead of RocksDB for all storages
 * add metrics for RocksDB and analyze them
 * run the same test with file storages instead of RocksDB for all storages (may be a separate task)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1769,INDY-1774,,,,,,,,,,,,,,"25/Oct/18 1:11 AM;sergey.khoroshavin;Screenshot from 2018-10-09 13-13-32.png;https://jira.hyperledger.org/secure/attachment/16165/Screenshot+from+2018-10-09+13-13-32.png","25/Oct/18 1:11 AM;sergey.khoroshavin;Screenshot from 2018-10-24 18-30-26.png;https://jira.hyperledger.org/secure/attachment/16166/Screenshot+from+2018-10-24+18-30-26.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwxcf:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,Toktar,,,,,,,,,,"04/Oct/18 12:15 AM;Toktar;In this task was found a bug in Restarter class.
{code:java}
stab_node8: Oct 01 10:03:49   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/restarter.py"", line 55, in _update_action_log_for_started_action
stab_node8: Oct 01 10:03:49     self._notifier.sendMessageUponNodeRestartComplete(
stab_node8: Oct 01 10:03:49 AttributeError: 'PluginManager' object has no attribute 'sendMessageUponNodeRestartComplete'
{code}
Fixed in PR https://github.com/hyperledger/indy-node/pull/960;;;","25/Oct/18 2:08 AM;sergey.khoroshavin;*Scope limiting*
This investigation is concerned with case of increasing memory usage during load that node can sustain (so queues are in more or less in steady state)

*Limiting buffer sizes*
Node use default RocksDB configuration and according to documentation defaults already have very tight limits.

*RocksDB vs LevelDB*
 !Screenshot from 2018-10-09 13-13-32.png|thumbnail! 
Test was conducted with half nodes running RocksDB and another half LevelDB. It can be seen that memory consuption is rougly the same. This indicates that they either:
* have same memory consumption OR
* are not major memory consumers

*RocksDB metrics*
 !Screenshot from 2018-10-24 18-30-26.png|thumbnail! 
This was part of other test with much higher load (hence much bigger memory consumption), but nodes already had some RocksDB metrics enabled. Interesting thing here are RocksDB memtable usage metrics. It can be seen that one database steadily increases it's memtable size until 70 Mb at which point it gives memory back and then process is repeated. Some other databases have much slower growth which doesn't cut off, but on the other hand they don't reach 70 Mb, so if test was run for more time we might see same cut off at 70 Mb. It can also explain visible jaggies in memory consumption of RocksDB vs LevelDB run. Unfortunately most interesting metric (block cache usage) is not available yet (it requires patching  RocksDB python wrapper).

*RocksDB vs file storage*
This is work in progress which will be finished in scope of INDY-1769

*Preliminary conclusions*
During sustained load memory usage grows and it might be RocksDB, but this growth is most likely have an upper bound. To prove this relatively long (probably at least 2-3 days) sustainable load test is needed.;;;","25/Oct/18 3:51 PM;ashcherbakov;1) INDY-1774 is created to run a long tests to check memory consumption (prove a theory from the comment above)

2) INDY-1769 is created to continue testing with just file storage enabled;;;",,,,,,,,,,,,,,,,,,,,,,
Memory profiling with Valgrind,INDY-1725,34164,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,28/Sep/18 2:20 AM,09/Oct/19 6:59 PM,28/Oct/23 2:47 AM,09/Oct/19 6:59 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwxj3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"09/Oct/19 6:59 PM;ashcherbakov;We've already done it. We don't see memory issues now.;;;",,,,,,,,,,,,,,,,,,,,,,,,
1.6.79 Release,INDY-1726,34178,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,esplinr,28/Sep/18 5:31 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"Task Overview
Perform all necessary tasks in order to release

Acceptance Criteria
* Code is tested
* Release is tagged
* Release notes are drafted
* Release notes are reviewed by Docs and Product Management
* A new release of Indy-Plenum is part of this release",,,,,,,,,,,,,,,,,,,,,,,INDY-1722,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwzb:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,VladimirWork,,,,,,,,,,"06/Dec/18 8:28 PM;VladimirWork;RC 1.6.79 has been approved to stable.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Configurable Auth Rules,INDY-1727,34189,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey-shilov,sergey-shilov,28/Sep/18 6:41 PM,01/Oct/20 8:21 AM,28/Oct/23 2:47 AM,09/Oct/19 7:08 PM,,,,,,0,,,,"There are important and potentially dangerous actions in Indy which require a quorum of signers (Trustees, Stewards, etc.).

For example, as a trustee of an Indy Network, the network should require other trustees to sign administrative transactions I want to sponsor so that I am confident that no single trustee can abuse the network. All transactions that require a trustee role should require multiple trustee signatures. Specifically:
 * Add a trustee
 * Remove a trustee
 * Add a steward
 * Revoke the role of steward
 * Demote a consensus node
 * Send an upgrade transaction to the ledger

This feature can be applied not only for trustees and adopted for any role supported by the indy-node.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-4,,Configurable Auth Rules,Done,No,,Unset,No,,,"1|hzzt2v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PoA: Modify and extend the auth_map structure to store the authorization rules for administrative actions,INDY-1728,34191,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,sergey-shilov,sergey-shilov,28/Sep/18 6:59 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"Current implementation of the _auth_map_ structure does not support multi-signature quorums for actions. In order to support such functionality we need to modify and extend it to implement the rules like:

_<action>: [<who can do this>: <required number of signatures>]_
i.e.
_<action1>: [<role1>: <quorum1>, <role2>: <quorum2>]_

Of course the format may be different, it's just a concept.",,,,,,,,,,,,,,,,,INDY-1729,,,,,,,,,,,,,,,,,INDY-1704,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwx0v:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,sergey-shilov,,,,,,,,,,"26/Nov/18 8:46 PM;anikitinDSR;h2. *Plan of attack*

*Requirement*

According to the task description we need to modify and extend auth_map structure which will support all existing rules and multi-signature features too. Also, in this case we should modify all auth's tests and write some new.

 

*What to do*

Summarizing requirements and task's description we need to implement the next steps:
 * find a ""framework like"" solution for existing rules and multi-signature features
 * implement this solution
 * fix old auth's tests according with new approach of auth rules working

*How to do*

We can divide rules by actions, like:
 * rules for adding       (adding new steward, or node)
 * rules for removing  (remove steward)
 * rules for editing       (demote node)

Therefore, the next classes of rules can be implemented:
 * AbstractRule(description, txn_type, rule_id, default_auth_сonstraint)
 * RuleAdd(description, txn_type, rule_id, default_auth_сonstraint)
 * RuleRemove(description, txn_type, rule_id, default_auth_сonstraint)
 * RuleEdit(description, txn_type, rule_id, field, prev_value, new_value, default_auth_сonstraint)

 

_""txn_type""_ is a type of transaction, like NYM, NODE, SCHEMA, etc.

_""field""_ is a field into transaction which can be modified. If it's empty, that means, that all of fields can be modified.

_""prev_value""_ means that sender want to change current value to other or add new transaction (adding new steward for example). The possible choices for this fields are: _empty, '*'*, string. ""Empty""_ means that sender can add this transaction. '' - changing existing transcation does not depend on current stored value. If this field is some other string, then sender can change this value only if current is _string._ For example, we can change field ""services"" to value ""[]"" only if current value is ""[VALIDATOR]"". In that case _prev_value_ should be _""_[VALIDATOR]_""_

_""new_value""_ is similar with _prev_value,_ but it can be _empty._

""_rule_id_"" may be as a string, like: ""txnType_orderOfRule"". For example, for ""Adding new steward"" it's look like: ""1_2"", where 1 is a NYM constant and 2 is a order of rule.

""_auth_constraint_"" is a list of roles with required count of signatures. For example, ['TRUSTEE': 3, 'STEWARD': 7].

*For storing auth_constraint in the ledger the next format is proposed*:

key:        _rule_id_
 value:     _auth_constraint_

*For choosing which constraints will be used, we need to implement the next policies*:
 * AbstratcAuthPolicy
            with get_auth_constraint method
 * LocalPolicy
 * ConfigLedgerPolicy

LocalPolicy uses default auth_constraints from source code.
ConfigLedgerPolicy uses auth_constraints from config ledger.

Summarizing previous suggestions about policy and auth rules, we need to implement the main class for applying authorization procedure:

Authorizer(auth_map, policy):
    def authorize(txn_type, role, old_value, new_value):
             rule = self.find_rule(txn_type, role, old_value, new_value)
             auth_constraint = self.policy.get_auth_constraint(rule.rule_id)
             return self._do_authorize(role, old_value, new_value, rule, auth_сonstraint)

   def find_rule(txn_type, role, old_value, new_value) -> AbstractRule
   def _do_authorize(role, old_value, new_value, rule, auth_сonstraint) -> bool ( use policy.get_auth_constraint)

 ;;;","05/Dec/18 12:55 AM;ashcherbakov;I think we may need to do the following to support extension of validation by plugins:

1) *AuthAction*
 * AbstractAuthAction(txn_tye)
 ** def get_action_id()
 * AuthActionAdd(txn_type, field=None, value=None)
 ** action_id: ADD_(txnType|*)_(field|_*_)_(value|*)
 * AuthActionEdit(txn_type, field, old_value, new_value)
 ** action_id: EDIT_(txnType|*)_(field|_*_)_(old_value|*)_(new_value|*)

2) *AuthConstraint* (they don't have a code for validation, just definition of constraints):
 * AbstractAuthConstraint
 ** def get_id()
 * AuthConstraint(role, sig_count, metadata)
 ** id: ROLE
 * AuthConstraintAnd(auth_constraints)
 ** id: AND
 * AuthConstraintOr(auth_constraints)
 ** id: OR

3) *AuthConstraintParser* (for future)
 * Support a simple grammar with OR only:
 ** action_id1:\{role1, sig_count1, metadata1} || action_id1:\{role1, sig_count1, metadata1}
 * Can use a simple split based on || for now
 * Consider using _antlr_ and more complex grammar for future

3) *AuthMap*
 * static map
 * has the following structure:
 ** action_id -> default_auth_constraint
 ** actiomn_id may comtain * meaning 'any value'
 * allows to add new auth rules by Plugins

4) *AuthConstraintStrategy*
 * AbstractAuthConstraintStrategy
 ** def get_auth_constraint(action_id)
 ** def find_auth_constraint_key(action_id, action_ids)
 *** finds a key in auth_map matching the given action_id
 *** gets an action_id from the list of ids matching the given action_id
 *** takes into account * (any value)
 * LocalAuthConstraintStrategy
 ** gets default rules from  the auth map
 * ConfigLedgerAuthConstraintStrategy
 ** get from the config ledger
 ** uses AuthConstraintParser to parse the constraint
 ** TBD

5) *Authorizers* 
 * Authorizer interface
 ** def authorize(req, auth_action, auth_constraint)

 * Plugins can implement their own authorizers
 * There are a number of default Authorizers:
 ** *RolesAuthorizer*
 *** has access to `idr_cache`
 *** def authorize(req, auth_action, auth_constraint: AuthConstraint)
 **** gets the role from the request
 **** may need to take either a real submitter role, or a trusted submitter (TBD)
 **** gets the number of signatures for this role from the request
 **** checks if the role is valid
 **** checks if the number of signatures is valid
 ** *CompositeAuthorizer(authorizers*)
 *** def register_authorizer(authorizer, auth_consraint_id=ROLE)
 **** auth_constrauint_id -> List[Authorizer]
 *** def authorize(req, auth_action, auth_constraint: AuthConstraint)
 **** for authorizer in authorizers.get(auth_constraint.id):
 ***** if not authorizer.authorize((req, auth_action , auth_constraint) - raise Exceptionwith validation error
 ** *AndAuthorizer*(*CompositeAuthorizer*)
 *** def authorize(req, auth_action, auth_constraint: AuthConstraintAnd)
 **** for authorizer in auth_constraint.authorizers:
 ***** if not super().authorize() - raise Exceptionwith validation error
 ** *OrAuthorizer*(*CompoisteAuthorizer*)
 *** def authorize(req, auth_action, auth_constraint: AuthConstraintOr)
 **** for authorizer in auth_constraint.authorizers:
 ***** if not super().authorize() - continue
 **** If all False - raise Exception with description

6) *WriteRequestValidator(**CompositeAuthorizer**)*
 * Has a number of registered authorizers (plugins can register custom ones)
 * Has the following registered by default:
 ** ROLE -> [RolesAuthorizer]
 ** AND -> [AndAuthorizer]
 ** OR -> [OrAuthorizer]

 * Has *AuthConstraintStrategy* (either local or config ledger)
 * def validate(req, auth_actions: List[AuthAction]):
 ** for auth_action in auth_actions:
 *** action_id = auth_action .get_action_id()
 *** auth_constraint = auth_constr_strategy.get_auth_constraint(action_id)
 *** super().authorize(req, auth_action, auth_constraint)

 7) Changes in ReqHandlers:
 * A req handler validate method define what AuthActions are being done, and creates corresponding Action instances
 ** Example for promotion with changes of IP address:
 *** actions = [AuthActionEdit(NODE, ""services"", [], [VALIDATOR],   AuthActionEdit(NODE, ""node_ip"", ""ip1"", ""ip2"")]
 * A req handler  calls WriteRequestValidator.authorize(req, actions)

 ;;;","06/Dec/18 3:48 PM;ashcherbakov;The PoA is created and implementation is started in the scope of https://github.com/hyperledger/indy-node/pull/1060;;;","06/Dec/18 3:48 PM;ashcherbakov;Implementation will be continue in the scope of INDY-1729, INDY-1730.;;;",,,,,,,,,,,,,,,,,,,,,
Implement the authorization rules for administrative actions with quorums as constants (hard-coded constants for absolute number of signers),INDY-1729,34193,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey-shilov,sergey-shilov,28/Sep/18 8:27 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.83,,,,0,,,,"Implement authorization rule with constant quorums according to _auth_map_ modification/extension made in scope of INDY-1728.



The current rules can be the same as before (see `auth_map.md`).

 ",,,,,,,,,,INDY-1728,,,,,,,INDY-1730,INDY-1731,INDY-1962,,,,,,,,,,,,,,,INDY-1704,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvin:",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Modify applying of the rules stored in the auth_map according to changes made by INDY-1728 and INDY-1729,INDY-1730,34195,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,sergey-shilov,sergey-shilov,28/Sep/18 8:55 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,Since we have modified/extended _auth_map_ we need to implement applying of  authorization rules that define quorums for important actions (implemented in scope of INDY-1729) and modify applying of other rules stored in the _auth_map_ as they may be changed in scope of INDY-1728.,,,,,,,,,,INDY-1729,,,,,,,INDY-1731,INDY-1732,,,,,,,,,,,,,,,,INDY-1939,INDY-1969,INDY-1970,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvio:",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,"22/Dec/18 1:51 AM;sergey-shilov;*Problem state / reason:*

According to the description of INDY-1727 we need to modify and extend auth_map structure which will support all existing rules and multi-signature features too. Also, in this case we should modify all auth's tests and write some new.

*Changes:*

* Implemented a new framework-like authorization approach
* Defined ""actions"":
** AbstractAuthAction
** AuthActionAdd
** AuthActionEdit
* Defined actions IDs as a combination of action prefix, TXN type, field to be modified, old and new values
* Defined _AuthConstraint_  which defines a role and number of signatures for doing action
* Made authentication as a strategy (local (static rules) or config ledger, local is a default)
* Moved current auth rules to new ""actions""
* Integrate new auth_map to poolHandler and moved write_req_validator to node code
* Defined authMap with current actions and new administrative actions (for now 1 signature is required).

Details of proposed and implemented solution can be found here:
https://jira.hyperledger.org/browse/INDY-1728?focusedCommentId=54287&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-54287

*Committed into:*

    https://github.com/hyperledger/indy-node/pull/1100
    indy-node 1.6.743-master

*Risk factors:*

    May be some inconsistency in authorisation rules, QA validation is needed.

*Risk:*

    Medium

*Recommendations for QA:*

Check that authorisation works as expected (adding Trustee, Steward etc., add/edit Schema/ClaimDef, nodes actions and so on).;;;",,,,,,,,,,,,,,,,,,,,,,,,
Support a strategy to specify per-cent of signers instead of absolute number,INDY-1731,34197,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey-shilov,sergey-shilov,28/Sep/18 9:11 PM,09/Oct/19 7:06 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"As a result of INDY-1730 we have a rules that define absolute quorums for administrative actions. In scope of this ticket we have to implement a strategy of combination of absolute numbers and percentage to implement a rules like ""33% of trustees but not less than 2"", but using of percentage requires INDY-1594 complete.

*Notes*
* It is expected that requiring a percentage of users will be implemented after the capability of requiring a specific threshold of signatures and recording the rules on the config ledger is already implemented.
* We do not yet have an efficient mechanism for getting from the ledger the total number of potential signers with a specific role. That will have to be done as part of this work.",,,,,,,,,,INDY-1730,INDY-1594,INDY-1729,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxg7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PoA: Move the auth_map structure to the config ledger,INDY-1732,34198,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,sergey-shilov,sergey-shilov,28/Sep/18 9:28 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.7.1,,,,0,,,,"In order to make the _auth_map_ structure configurable we need to move it to the config ledger. Here we should:
 - define a data format for storing in the config ledger
 - define a new transactions for adding/modifying of authorization rules
 - create required set of genesis transactions to be able to bootstrap
 - modify applying of the rules stored in the config ledger according to new format

There is the question of atomicity of adding and modification of the rules for all administrative actions. Possible solutions:
 - each rule can be added/modified by separate transaction;
 - all rules are added/modified by single transaction, i.e. this transaction contains a list of all rules for administrative actions for atomic update.

I think, the first point is better.",,,,,,,,,,INDY-1730,,,,,,,INDY-1527,INDY-1962,,,,,,INDY-1693,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:000005y",,,,Unset,Unset,Ev-Node 19.04,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,sergey-shilov,,,,,,,,,,,"20/Feb/19 1:35 AM;anikitinDSR;h2. Plan of attack
h3. Needed changes and features:
 * Storing auth rules in config ledger
 * Using rules from config ledger (get rules from state, not from source code defined auth_map structure)
 * New transaction for adding/modifying current rules
 * genesis transaction (for bootstrap purposes)
 * Modifying docs (about new transaction into config ledger and genesis file)
 * Need to create task for indy-sdk to support new write transaction

h3. Storing auth rules in config ledger.

Storing auth rules in config ledger consists of:
 * Define a format to storing. What's the ""key"" and what's the ""value""
 * Make serialization and deserialization methods (for put and get actions)
 * Make some cache. Because we use auth rules for validation all of requests, we can use a cache (like idrCache for NYM roles). But it can be implemented as the next step of integration in a separate task.
 * Make a couple of tests, unit and integration, for checking storing and catchup/revert cases.

h4. Format to storing.

During new auth_map implementation, was assumed, that rule_id should be used as a key for storing into ledger. Serialized instance of corresponded auth_constraint class should be a value. All the rules have a unique rule_id represantion. In other words, rule_id as a key will give unique couple of pair ""key-value"" for storing into ledger. Value represantion should be serializable and deserializable. I propose, that json-like serialization is the most comman way for the value storing.
h4. Serialization/deserialization

As i noticed before, for value i propose a simple json serialization and also we should add method for making corresponded auth_constraint object from serialized value. We should create some ""compositor"" class, which can make AuthConstraint object from serialized value. In sum for this prupose we should add:
 * value serialization
 * procedure for making AuthConstraint from serialized value
 * put pair of rule_id/AuthConstraint into cache (optional for now)

h3. Modify applying rule from config_ledger

As of no, we use LocalAuthStrategy for getting corresponded auth constraints by default and also we have a template for ConfigLedgerAuthStrategy which is empty now. In this step we can implement corresponded method (get_auth_constraint and _find_auth_constraint_key) and move to it by changing ""authPolicy"" parameter in config file. We should pass state of ConfigLedger directly to auth validator for request validation with actual uncommitted auth rules. In the future we can pass some cache with uncommitted state. Steps to validate:
 * Find corresponded rule_id in state/cache for incoming action_id
 * Get uncommitted auth constraint from config ledger's state
 * Make AuthConstraint
 * Using general workflow for checking auth constraint do request validation

h3. New transaction for adding/modyfing current rules

For this step, we should add 2 new types of transactions for ConfigReqHandler. Adding new transaction consists of:
 * Add new client message type with static validation if needed
 * Add dynamic validation. It mens, that we should check, that user can write this transaction. For initial case we can use some ""genesis"" rule or rules from corresponded genesis file and then do it as in previous step
 * Add write method with corresponded serialization.

h3. Genesis transaction

This step should be discussed
h3. Modifying docs

Add information about new write transactions to ConfigLedger and genesis file.
h3. Tests

All of changes should be covered by unit and integration tests.

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,
GitHub prerelease builds should be clearly marked,INDY-1733,34212,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,esplinr,esplinr,29/Sep/18 4:25 AM,14/Jun/19 8:39 PM,28/Oct/23 2:47 AM,14/Jun/19 8:39 PM,,1.9.0,,,,0,devops,,,"*Problem*
In the Indy Node repository in GitHub, development releases appear alongside the stable releases.

*Acceptance Criteria*
Builds that have not been approved by QA are marked in GitHub as ""pre-release"".",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1992,,,,,,,,,,,,,,,,,,,,"09/Feb/19 12:44 AM;andkononykhin;master_as_no_release_collapsed.jpeg;https://jira.hyperledger.org/secure/attachment/16749/master_as_no_release_collapsed.jpeg","09/Feb/19 12:44 AM;andkononykhin;master_as_pre_release.jpeg;https://jira.hyperledger.org/secure/attachment/16750/master_as_pre_release.jpeg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609m",,,,Unset,Unset,Ev-Node 19.12,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,esplinr,,,,,,,,,,"04/Oct/18 9:25 PM;andkononykhin;{quote}In the Indy Node repository in GitHub, development releases appear alongside the stable releases.
{quote}
and release candidates as well (a.k.a. pre-releases)
 e.g.: [pre-release for 1.6.70|https://github.com/hyperledger/indy-node/tree/1.6.70-rc] and [stable release 1.6.70|https://github.com/hyperledger/indy-node/tree/1.6.70-stable]
{quote}Builds that have not been approved by QA are marked in GitHub as ""pre-release"".
{quote}
Current release candidate processing logic among other steps includes: tagging pre-release as X.Y.Z-rc -> ... QA approval -> ... tagging stable release X.Y.Z-stable
 If QA denies RC no stable tag is created. And it might be treated as an indicator of such QA decision.
 e.g. here is [pre-release for 1.6.69|https://github.com/hyperledger/indy-node/tree/1.6.69-rc] but no tag 1.6.69-stable

[~esplinr] Do you think we should change anything in that logic? Seems it provides all necessary information that is mentioned in acceptance criteria. Moreover a lot of discussions are happening these days regarding development and release processes and things may change drastically soon.

One thing that might be worth to improve is to include more details to the [ci-cd doc|https://github.com/hyperledger/indy-node/blob/master/docs/ci-cd.md] to clarify mentioned logic.;;;","08/Feb/19 11:50 PM;andkononykhin;Some exploration results:
 * GithHub release is a kind of addition over git's tags: GitHub adds some metadata like release notes, release name, attachments
 * for now GitHub shows each tag in Releases tab anyway (regardless of release metadata)  treating it as release point (placeholder), so each tag is visible in both Releases and Tags tabs
 * GitHub agrees that showing all tags in Release tab might be confusing and an internal ticket regarding that has been created ([https://github.community/t5/How-to-use-Git-and-GitHub/Tag-without-release/td-p/6255)]
 * (one more related conversation: [https://stackoverflow.com/questions/28496319/github-a-tag-but-not-a-release])

Thus, for now we can't make non-release tags invisible in Release tab: development tags will be anyway presented in Releases tab alongside tags for releases it would be still confusing.

But we can try satisfy the acceptance criteria in any case.

*PoA*

+Option 1+
 * create GitHub releases for x-stable tags and pre-releases for *-rc tags
 * leave *-master tags as is (just tags, no GitHub release metadata)

pros
 * all release and pre-release tags would be properly marked
 * *-master (dev tags) would be partly collapsed !master_as_no_release_collapsed.jpeg|thumbnail!

cons
 * simple tags (no GitHub release metadata) would be still visible in Releases tab
 * pre-releases would be still visible as well

+Option 2+
 * create GitHub releases for x-stable tags and pre-releases for *-rc tags and *-master

pros
 * *-master tags will be marked as pre-releases as well

cons
 * *-master pre-releases would  occupy a lot of area in UI  !master_as_pre_release.jpeg|thumbnail!
 * it's not accurate to mark *-master as pre-release since it's actually a dev version (ones might call them `pre-pre-release`)
 * simple tags (no GitHub release metadata) would be still visible in Releases tab
 * pre-releases would be still visible as well

+Option 3+
 * create Releases only *-stable tags
 * remove all current *-master tags and stop pushing them from CD pipeline

pros:
 * more clean UI Releases tab:  real releases are shown as expected, pre-releases are presented as just one line (but not marked), dev tags are not presented, thus don't ""pollute"" the UI

cons:
 * might be less convenient to associate artifacts (e.g. debian and pypi packages) with the source code but I don't see any valuable cases where it is really needed (artifacts for now includes metadata with commit sha)
 * I believe that can't be done before INDY-1992 since it requires revision of current workflows and version management

Seems the option 3 is best one. There might be variants of it (e.g. create pre-release for *rc as well) but the main idea is to get rid of dev tags.

[~esplinr] [~ashcherbakov] What do you think?;;;","12/Feb/19 1:53 AM;ashcherbakov;I vote for Option 3.;;;","02/Apr/19 9:03 PM;andkononykhin;The task was partly done in scope of INDY-2019:
 * master tags were removed from indy-plenum and indy-node
 * CD pipeline stopped publishing new master tasg
 * rc tags are not published for now as well but it might be changed in future

What is left:
 * (using GitHub REST API) add changelog info for releases as GitHub release metadata as a part of CD pipeline
 * (optionally) push tags for pre-releases and mark them as GitHub pre-releases via GitHub REST API;;;","10/Jun/19 4:39 PM;ashcherbakov;I think the following needs to be done as a minimum to complete the task:
 * Remove all RCs in GitHub release tab (both plenum and node)
 * Add changelog (manually) to every release in plenum and node.;;;","14/Jun/19 12:42 AM;andkononykhin;The following has been done:
 * rc tags - removed in both [indy-plenum|https://github.com/hyperledger/indy-plenum/tags] and [indy-node|https://github.com/hyperledger/indy-node/tags]
 * release notes - attached to [releases|https://github.com/hyperledger/indy-node/releases] for indy-node only
 * release notes - fixed in few places in indy-node and sovrin, PRs:
 ** [https://github.com/hyperledger/indy-node/pull/1347]
 ** https://github.com/sovrin-foundation/sovrin/pull/177;;;","14/Jun/19 8:39 PM;ashcherbakov;Release notes for sovrin token plugins have also been added (for the latest release only).;;;",,,,,,,,,,,,,,,,,,
Triage old bug reports 2,INDY-1734,34217,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,29/Sep/18 8:28 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"The Evernym Indy Node team will be closing old bug reports that are unlikely to be reproducible on the current release.

*Acceptance Criteria*
 Overall:
 * Each engineer will spend one day of the sprint in this activity.
 * The goal is to review at least 8 issues (less than 1 hour per issue).

For each issue:
 * Read the issue and the comments.
 * Briefly investigate the issue using the current code base (not the code base where the issue was found).
 * Close the issue as ""Done"" if:
 ** It appears to be something that we tested as part of our most recent release, and we didn't see the issue.
 ** You try to reproduce the issue, and didn't see the issue.
 ** A corresponding task/story exists (or created by the engineer) that addresses the same issue. Make sure that the task/story is linked to the bug.
 * Keep the issue open if you are able to reproduce it or suspect that it is a real issue after your brief investigation.
 * Include a comment documenting:
 ** why you closed the issue,
 ** or what you found during your investigation, including your current environment, symptoms observed, and steps to remediate.
 * Create any related tickets that your investigation shows are necessary.
 * If you think any of these issues should be immediately addressed, please escalate to [~ashcherbakov] and [~esplinr]

*Notes*
 * Close the issue as ""Done"" if you are at least 80% confident that the issue does not exist in the latest code base. The goal is to separate important issues from unimportant issues, and real issues will surface again at some point in the future.
 * If during your investigation you see that you can fix the issue quickly (only a couple of hours), proceed to fix the issue.",,,,,,,,,,,,,,,,,,,,,INDY-1735,,INDY-1715,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzw4f:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),0.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Review old issues,INDY-1735,34218,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,29/Sep/18 8:28 AM,06/Dec/18 3:56 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"The Evernym Indy Node team will be closing old issues and bug reports that are no longer needed or are unlikely to be reproducible on the current release.

*Acceptance Criteria*
 Overall:
 * Each engineer will spend one day of the sprint in this activity.
 * The goal is to review at least 8 issues (less than 1 hour per issue).

For each issue:
 * Read the issue and the comments.
 * Briefly investigate the issue using the current code base (not the code base where the issue was found).
 * Close the issue as ""Done"" if:
 ** It appears to be something that we tested as part of our most recent release, and we didn't see the issue.
 ** You try to reproduce the issue, and didn't see the issue.
 ** A corresponding task/story exists (or created by the engineer) that addresses the same issue. Make sure that the task/story is linked to the bug.
 * Keep the issue open if you are able to reproduce it or suspect that it is a real issue after your brief investigation.
 * Include a comment documenting:
 ** why you closed the issue,
 ** or what you found during your investigation, including your current environment, symptoms observed, and steps to remediate.
 * Create any related tickets that your investigation shows are necessary.
 * If you think any of these issues should be immediately addressed, please escalate to [~ashcherbakov] and [~esplinr]

*Notes*
 * Close the issue as ""Done"" if you are at least 80% confident that the issue does not exist in the latest code base. The goal is to separate important issues from unimportant issues, and real issues will surface again at some point in the future.
 * If during your investigation you see that you can fix the issue quickly (only a couple of hours), proceed to fix the issue.",,,,,,,,,,,,,,,,,,,,,,,INDY-1734,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxe4:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),0.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Part of nodes stopped ordering during load test with payments,INDY-1736,34224,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,spivachuk,ozheregelya,ozheregelya,01/Oct/18 1:28 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"*Environment:*
 indy-node 1.6.610
 libindy 1.6.5~750

*Steps to Reproduce:*
 1. One-b-one run following load from 4 different machines:
{code:java}
perf_processes.py -g ~/ext_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext  ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 10 -b 1 -l 2.5
{code}
*Actual Results:*
 Part of nodes stopped ordering. 
 From [~spivachuk] comment in INDY-1607:

{quote}Actually the pool continued ordering after the view had changed to view 1 (approximately at 21:13 on 09/26). But some nodes (*Node3, Node11, Node13, Node15, Node24, Node25*) were performing this view change for a long time, completed it later than other nodes (at 21:19 - 21:21) and *were not participating in ordering* after this. *The diagram* *attached* *to the previous comment is from Node13*, so the ordering on it stopped on the view change beginning at 21:13.{quote}

Logs and metrics:
 s3://qanodelogs/INDY-1607-Ext-26-09-18-mix",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1739,INDY-1740,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzt8f:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,spivachuk,,,,,,,,,,,"03/Oct/18 2:04 AM;spivachuk;As we see in the logs, *Node3* (one of the nodes which were performing the view change to view 1 longer than most nodes in the pool) started a catch-up because of a checkpoints lag in less than 2 minutes after the view change 1 completion. Then on reception of some next {{Checkpoint}} message *Node3* *started another catch-up* caused by a checkpoints lag - {color:#d04437}*right during the first catch-up being in progress*{color}. When a node starts a catch-up, it resets {{LedgerInfos}} for all the ledgers. *Thus the new catch-up abandoned the previous incomplete one.* Then again and again on reception of {{Checkpoint}} messages *Node3* started new catch-ups abandoning previous incomplete ones. None of these catch-up was completed. So *Node3* was in permanent catch-up phase and did not participate in ordering due to this. The other nodes which were performing the view change to view 1 during a long time showed the same behavior. All these nodes - *Node3*, *Node11*, *Node13*, *Node15*, *Node24*, *Node25* - were in permanent catch-up phase and did not participate in ordering due to this during the period since they started a catch-up on a checkpoints lag (21:20 - 21:23) (after the view change 1 completion) until they were restarted (22:41 - 22:47). Created INDY-1739 for fixing this issue.

Also we have found repeated requests for ledger statuses in logs. As it has turned out, scheduled re-asking for ledger statuses and maximal consistency proofs is not actually canceled when sufficient count of messages are gathered. Created INDY-1740 for fixing this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Load script can't re-use payment addresses for the same client in case of load with 'payment' txns,INDY-1737,34225,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,NataliaDracheva,ozheregelya,ozheregelya,01/Oct/18 3:31 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"*Environment:*
indy-node 1.6.614
libindy 1.6.6~759
load script 1.0.3, 1.0.5

*Steps to Reproduce:*
1. Run load test with payments (with fees):
{code:java}
perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k payment -c 5 -b 1 -l 1{code}
=> (payment_addrs_count * -c = 500) txns written, after that only 'Cannot generate request since no req data are available.' errors appear.
2. Run the same load with NYMs (with fees):
{code:java}
perf_processes.py -g ~/live_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000
000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_l
imit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees
\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k nym -c 5 -b 1 -l 1{code}
=> more than (payment_addrs_count * -c = 500) txns were successfully written.

*Actual Results:*
Load script can't re-use payment addresses in case of load with 'payment' txns.

*Expected Results:*
Load script should re-use payment addresses independently on txns types.",,,,,,,,,,,,,,,,,INDY-1717,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/18 7:13 PM;ozheregelya;out.7x;https://jira.hyperledger.org/secure/attachment/16142/out.7x",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzt8n:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,dsurnin,NataliaDracheva,ozheregelya,,,,,,,,,"12/Oct/18 2:16 PM;anikitinDSR;Reasons:
 * need to reuse payment addresses instead of generating count_of_payment_addresses == count_of_requests

Changes:
 * reuse payment addresses, but it works only for wait_resp sync option
 * for other sync mode old way will be used

Versions:
 * 1.0.14

PRs:
 * [https://github.com/hyperledger/indy-node/pull/965]

Recomendation for QA:
 * as of now version for testing available as tar archive in load_script_dev_qa channel. After testing and merging PR pypi package will be created.

 

 ;;;","13/Oct/18 12:37 AM;NataliaDracheva;*Scenario 1:*
*Build version:*
indy-node: 1.6.631
indy-plenum: 1.6.564
*Test description:* 5 payments + fees/sec, mode = wait_resp
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with the following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -y wait_resp -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k payment -c 5 -b 1 -l 1
{code}

*Expected results:* 
The number of transactions in sovtoken ledger is: clients * load rate * test time in seconds. 
Test time = 1170 s - 100 s for minting = 1070 s;
load rate = 1;
clients = 5.
Expected count of transactions in sovtoken ledger = 5350
*Actual results:*
Transactions in sovtoken ledger = 5079 (/)

*Scenario 2:*
*Build version:*
indy-node: 1.6.631
indy-plenum: 1.6.564
*Test description:* 20 payments + fees/sec, mode = wait_resp
*Preconditions:*
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with the following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -y wait_resp -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k payment -c 5 -b 1 -l 4
{code}
*Expected results:* 
The number of transactions in sovtoken ledger is: clients * load rate * test time in seconds. 
Test time = 830 s - 100 s for minting = 730 s;
load rate = 4;
clients = 5.
Expected count of transactions in sovtoken ledger = 14 600
*Actual results:*
Transactions in sovtoken ledger = 3589 (x)
*Additional info:* No errors in load script logs.

*Scenario 3:*
*Build version:*
indy-node: 1.6.631
indy-plenum: 1.6.564
*Test description:* 5 nyms + fees/sec, mode = wait_resp
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with the following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -y wait_resp -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k nym -c 5 -b 1 -l 1
{code}
*Expected results:*
The number of transactions in sovtoken ledger is: clients * load rate * test time in seconds. 
Test time = 730 s - 100 s for minting = 630 s;
load rate = 1;
clients = 5.
Expected count of transactions in sovtoken ledger = 3150
Expected count of transactions in domain ledger = 3150
*Actual results:*
Transactions in sovtoken ledger = 500 (x)
Transactions in domain ledger = 500 (x)
Other transactions are rejected (534) because of a lack of money and failed (13) because of the pool timeout.

*Scenario 4:*
*Build version:*
indy-node: 1.6.631
indy-plenum: 1.6.564
*Test description:* 5 nyms + fees/sec, mode = freeflow
*Steps to Reproduce:*
1. Run perf_processes.py from 1 AWS agent with the following parameters:
{code:java}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -y wait_resp -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k nym -c 5 -b 1 -l 1
{code}
*Expected results:*
The number of transactions in sovtoken ledger is: clients * load rate * test time in seconds. 
Test time = 2462 s - 100 s for minting = 2362 s;
load rate = 1;
clients = 5.
Expected count of transactions in sovtoken ledger = 11810
Expected count of transactions in domain ledger = 11810
*Actual results:*
Transactions in sovtoken ledger = 500 (x)
Transactions in domain ledger = 500 (x)
Other transactions are rejected (9620) because of a lack of money and failed (1042) because of the pool timeout.

Logs: s3://qanodelogs/indy-1737/load_script_14;;;","15/Oct/18 9:54 PM;dsurnin;it should be retested with version 1.0.13
also please note since fees and payments use the same addresses initial address count should be increased

it is possible to change mint strategy to have lots of txos for each address with 1 atom instead of 1 txo with all atoms - it will significantly increase number of concurrent reqs, but probably init time will be longer.;;;","16/Oct/18 7:14 PM;ozheregelya;The issue is reproduces on 1.0.13 version. Results of load script run (step 1 from Steps to Reproduce): [^out.7x];;;","19/Oct/18 12:06 AM;dsurnin;payment responses were not parsed.

added response parsing and new parameter to ext config - mint_by

PR
https://github.com/hyperledger/indy-node/pull/990

new script version is 16;;;","19/Oct/18 12:57 AM;NataliaDracheva;*Scenario 1:*
*Build version:*
indy-node: 1.6.638
indy-plenum: 1.6.566
*Test description:* Run 5 payments + fees/sec
*Steps to Reproduce:*
1. Run perf_processes.py from 1 client with the following parameters:
{code:java}
perf_processes.py -g  .indy-cli/networks/sandbox/pool_transactions_genesis -m t -n 1 -y freeflow -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":100,\""addr_mint_limit\"":1000000,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4,\""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k payment -c 5 -b 1 -l 1 -d=/home/indy/logs -o=load_script_output.txt
{code}
*Expected results:* the script keeps sending requests after 500 (5*100) transactions are sent
*Actual results:* the script keeps sending requests after 500 (5*100) transactions are sent
*Additional info:* Rechecked scenario with nyms + fees, works fine as well.;;;",,,,,,,,,,,,,,,,,,,
In case of load with 'payment' txns pool fails with OOM much more earlier than without them,INDY-1738,34226,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,ozheregelya,ozheregelya,01/Oct/18 3:42 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"*Environment:*
 indy-node 1.6.614
 libindy 1.6.5~759
 load script 1.0.5

See more details in *Test run 5* and  *Test run 6* from INDY-1607:
 https://jira.hyperledger.org/browse/INDY-1607?focusedCommentId=51348&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-51348



*Test Run 5:*
 There are no fees set in the pool. Load was increased from 5 to 10 txns/sec.
 Only 10K payments txns were written because of problem with load script (INDY-1737).
 The last payment txn was written at 09/27/2018 @ 9:05pm (UTC), first OOM failure was near 23:00. Pool was working during 5 hours before first OOM (in previous tests without payments it was able to work up to 50 hours).
 s3://qanodelogs/INDY-1607-Live-27-09-18-mix-no-fees

*Test Run 6:*
 There were the same load as in Test Run 5 and the same problem with load script was reproduced. When 10K txns were written to domain ledger and ~20K txns to sovtoken ledger were written, load test stopped writing valid requests with fees and started to write txns without fees (which should be rejected). According to metrics, right after end of valid requests, pool failed with OOM.
 s3://qanodelogs/INDY-1607-Live-27-09-18-mix-fees",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzzt8v:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ozheregelya,Toktar,,,,,,,,,,"11/Oct/18 5:15 PM;anikitinDSR;About case ""Test Run 5"":

First OOM failure was at 22:03 on Node6 by reason from INDY-1739. Therefore, this backup replica was not participating in order process and other nodes near midnight was downed by OOM reason too.
Another note, there is a lot of reject messages (all from 20:00) because of problem with load script (INDY-1737);;;","11/Oct/18 9:27 PM;Toktar;Found problem are solved by fix in tasks:  INDY-1739 and  INDY-1681;;;",,,,,,,,,,,,,,,,,,,,,,,
Catch-up can be abandoned by another catch-up,INDY-1739,34277,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,zhigunenko.dsr,spivachuk,spivachuk,03/Oct/18 3:17 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,A catch-up can be abandoned by another catch-up. Under load this can result in a permanent catch-up phase when on incoming {{Checkpoint}} messages new catch-ups are triggered and abandon previous incomplete ones. See comments to INDY-1736 for details.,,,,,,,,,,,,,,,,,INDY-1744,,,,,,,,,,,,INDY-1736,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzzthr:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),spivachuk,zhigunenko.dsr,,,,,,,,,,,"10/Oct/18 10:57 PM;spivachuk;*Problem reason:*
- {{Node.start_catchup}} method did nothing only in case the node was in ledger statuses gathering phase of catchup. In other phases of catchup this method allowed to start a new catchup.

*Changes:*
- Wrote a test verifying that a catchup cannot be started if another catchup is in progress.
- Fixed a bug with ability to start a catchup when another catchup is in progress.
- Made a minor correction in test helpers.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/939
- https://github.com/hyperledger/indy-node/pull/969

*Version:*
- indy-node 1.6.626-master
- indy-plenum 1.6.558-master

*Risk factors:*
- Catchup phase in scope of view change.

*Risk:*
- Low

*Covered with tests:*
- {{test_catchup_not_triggered_if_another_in_progress}}

*Recommendations for QA*
- Please test a catchup of a node in a pool under load in AWS.;;;","12/Oct/18 12:00 AM;spivachuk;*Changes:*
- Added unit tests verifying ability to start catchup in different node modes.
- Reduced execution time of the integration test verifying that a catchup cannot be started if another catchup is in progress.
- Made a minor correction in an existing test.

*PR:*
- https://github.com/hyperledger/indy-plenum/pull/943;;;","13/Oct/18 1:07 AM;zhigunenko.dsr;*Environment:*
indy-node                        1.6.629
indy-plenum                      1.6.561

*Steps to Reproduce:*
1) Setup AWS pool with 25 nodes
2) Stop one non-primary node
3) Run load test (10txns/sec)
4) After 50k txns start node

*Actual Results:*
Catching-up node cannot become equal by checkpoints. Pool lost it's consensus after  291k txns

*Expected Results:*
Node catch ups others and joins to ordering process;;;","13/Oct/18 7:36 AM;spivachuk;As we can see in logs, all the nodes in the pool fell due to *out-of-memory* many times. Eventually this resulted in *loss of consensus* (after ~290K written transactions). As to *Node13 which performed catch-up*, it also fell due to *out-of-memory* many times. However, it successfully done the 1st catch-up at 11:43 (up to 62689 transactions in total) on start after being off during ~50K transactions. Then it started the 2nd catch-up because the pool went farther while the node was performing the 1st catch-up. This catch-up was interrupted by a fall due to out-of-memory. After restart Node13 successfully done the 3rd catch-up at 13:47 (up to 136670 transactions in total). Then it started the 4th catch-up because the pool went farther again but it did not complete it because of an out-of-memory fall. All the further attempts of catch-ups after restarts were interrupted by out-of-memory falls.

*So the observed issues are related to the memory leaks (see INDY-1721) and do not look related to the fix in catch-up.* It makes sense to re-run the scenario with reduced load. Please re-test the fix with load *5 txns/sec* and catch-up after *50K txns*.;;;","16/Oct/18 8:42 PM;zhigunenko.dsr;*Environment:*
indy-node                        1.6.633

*Steps to Reproduce:*
1) Setup AWS pool with 25 nodes
2) Stop one non-primary node (13th)
3) Run load test (5txns/sec)
4) After 50k txns start node

*Actual Results:*
13th node orders only 113k txns versus 120k txns on other nodes. Catchup couldn't finished even after load test stop.

*Expected Results:*
Node catch ups others and joins to ordering process

*Additional Info:*
~/logs/INDY-1739-5nyms/;;;","19/Oct/18 2:59 AM;spivachuk;Node13 completed its last catch-up at 18:22. After that it received 7 generations of farther checkpoint messages (from {{\{viewNo: 2, seqNoStart: 10601, seqNoEnd: 10700\}}} to {{\{viewNo: 2, seqNoStart: 11201, seqNoEnd: 11300\}}}) from other nodes. But it did not start a next catch-up because it *had not gathered a quorum* of messages for any of these checkpoint generations. For each of these checkpoint generations *Node13* received *{color:#de350b}15{color}* messages while the quorum was *16*. At the same time, for example, *Node12* received *{color:#00875a}21{color}* messages for each of these checkpoint generations. However, Node13 reported that it was connected to all the other nodes _except for Node7_ and all these nodes reported that they were connected to Node13. Also no nodes blacklisted any others. The logs are of INFO level, so the reason, why Node13 did not receive checkpoint messages from the nodes from which Node12 received them, are unclear _(except for Node7 - Node13 was disconnected from it)_.

Please re-test the scenario one more time.;;;","19/Oct/18 10:31 PM;zhigunenko.dsr;*Environment:*
indy-node                        1.6.639        
indy-plenum                      1.6.568      

*Steps to Validate:*
1) Setup AWS pool with 25 nodes
2) Stop one non-primary node (22th, North America)
3) Run load test (5txns/sec)
4) After 50k txns start node

*Actual Results:*
Node catch ups others and joins to ordering process at the 60к txns;;;",,,,,,,,,,,,,,,,,,
Re-asking for ledger statuses and maximal consistency proofs is not canceled,INDY-1740,34278,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,spivachuk,spivachuk,03/Oct/18 3:27 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,It has been found that scheduled re-asking for ledger statuses and maximal consistency proofs is not actually canceled when sufficient count of messages are gathered (see comments to INDY-1736). This must be fixed in scope of this ticket.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1736,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzthz:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),spivachuk,zhigunenko.dsr,,,,,,,,,,,"04/Oct/18 3:04 AM;spivachuk;*Problem reason:*
 - {{HasActionQueue._cancel}} method was called from {{LedgerManager._cancel_request_ledger_statuses_and_consistency_proofs}} incorrectly. The ID of the action which must be canceled was passed to {{HasActionQueue._cancel}} as the sole positional argument. Thus it was taken as the first parameter of the formal parameter list which is {{Callable}} rather than the concrete action ID.

*Changes:*
 - Corrected the test of cancellation of scheduled re-asking for ledger statuses and maximal consistency proofs on reception of sufficient count of messages.
 - Fixed cancellation of scheduled re-asking for ledger statuses and maximal consistency proofs in {{LedgerManager._cancel_request_ledger_statuses_and_consistency_proofs}} method.

*PRs:*
 - [https://github.com/hyperledger/indy-plenum/pull/933]
 - [https://github.com/hyperledger/indy-node/pull/961]

*Version:*
 - indy-node 1.6.621-master
 - indy-plenum 1.6.554-master

*Risk factors:*
 - Nothing is expected.

*Risk:*
 - Low

*Covered with tests:*
 - {{test_cancel_request_cp_and_ls_after_catchup}};;;","05/Oct/18 12:20 AM;zhigunenko.dsr;*Environment:*
indy-node 1.6.620

*Step to Reproduce:*
1. Install pool of 4 nodes.
2. Run load test for ~5k NYMs.
3. Add 5th node.
4. Run load test for ~5k NYMs.
5. Restart 3st node to change the primary (at that moment)
6. Stop 3st node.
7. Run load test for ~25k NYMs.
8. Add 6th node (without start)
9. Start service at 6th node and start 3st node simultaneously to check their catchup under load.

*Actual Results:*
After start nodes 3 and 6 didn't complete catchup and so didn't complete primary propogation view change.
Catchup runs once and cannot finished. During first 5 minutes Node6 received only one LedgerStatus and one Consistency Proof.

view_no = 2 before step 5
view_no = 3 right after step 6
view_no = 5 before step 9

Node 6 has been added in 2018-10-04 14:06:28

*Additional info:*
Logs are in _indy-1740-recheck/_;;;","05/Oct/18 8:18 PM;spivachuk;As we can see in logs, *under load there were issues with connections between nodes in the Docker pool*. This hindered the new node (*Node6*) and the restarted node (*Node3*) from catch-up completion:

Here are the moments of connections establishment on *Node6*:
{code:java}
2018-10-04 14:07:06.074000 | Node6:- | INFO | looper.py | Starting up indy-node
2018-10-04 14:07:28.173000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node2
2018-10-04 14:10:51.111000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node4
2018-10-04 14:14:43.281000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node1
2018-10-04 14:15:58.011000 | Node6:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node6 now connected to Node5
{code}
Here are the moments of connections establishment on *Node3* after restart:
{code:java}
2018-10-04 14:06:48.006000 | Node3:- | INFO | looper.py | Starting up indy-node
2018-10-04 14:07:18.967000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node2
2018-10-04 14:07:33.915000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node5
2018-10-04 14:09:37.851000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node1
2018-10-04 14:09:37.851000 | Node3:- | NOTIFICATION | keep_in_touch.py | CONNECTION: Node3 now connected to Node4
{code}
Please retest the scenario on AWS pool.;;;","05/Oct/18 9:43 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.623

*Step to Validate:*
1. Install pool of 4 nodes on AWS.
2. Run load test for ~5k NYMs.
3. Add 5th node.
4. Run load test for ~5k NYMs.
5. Restart 2st node to change the primary (at that moment)
6. Stop 2st node.
7. Run load test for ~25k NYMs.
8. Add 6th node (without start)
9. Start service at 6th node and start 2st node simultaneously to check their catchup under load.

*Actual Results:*
Both nodes finished their catchup successfully.;;;",,,,,,,,,,,,,,,,,,,,,
Bug in calling notifier methods in Restarter,INDY-1741,34420,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,NataliaDracheva,Toktar,Toktar,04/Oct/18 5:25 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,TShirt_S,,,"In the task INDY-1724 was found a bug in Restarter class. It calls sendMessageUponNodeRestartComplete() which is not implemented.
{code:java}
stab_node8: Oct 01 10:03:49   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/restarter.py"", line 55, in _update_action_log_for_started_action
stab_node8: Oct 01 10:03:49     self._notifier.sendMessageUponNodeRestartComplete(
stab_node8: Oct 01 10:03:49 AttributeError: 'PluginManager' object has no attribute 'sendMessageUponNodeRestartComplete'
{code}
*Acceptance criteria:*
 - Method calls removed
 - A test according to TDD",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwvz:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),NataliaDracheva,Toktar,,,,,,,,,,,"04/Oct/18 5:27 PM;Toktar;Fixed in PR [https://github.com/hyperledger/indy-node/pull/960];;;","04/Oct/18 7:38 PM;Toktar;Problem reason:
 - Restarter class contains calls of unimplemmented methods.

Changes:
 - Removed sendMessageUponNodeRestartComplete() and sendMessageUponNodeRestartFail() calls in Restarter class.

PR:
 * [https://github.com/hyperledger/indy-node/pull/960]

Version:
 * indy-node 1.6.622 -master
 * indy-plenum 1.6.553 -master

Risk factors:
 - No risk

Covered with tests:
 * [test_pool_restart.py|https://github.com/hyperledger/indy-node/pull/960/files#diff-5f88942e85e13268c921603c686868ac]

Recommendations for QA:
 * Test restart command for a future and check that pool can ordered any transaction (NYM for example) after it.;;;","04/Oct/18 10:21 PM;NataliaDracheva;*Scenario 1:*
*Build version:* 
indy-node: 1.6.622
indy-plenum: 1.6.554
*Test description:* Verify if pool keeps writing when scheduled restart happens.
*Preconditions:*
*Steps to Reproduce:*
# Schedule a pool restart in near future:
{code:java}

did new seed=000000000000000000000000Trustee1

did use V4SGRU86Z58d6TV7PBUe6f

ledger pool-restart action=start datetime=2018-09-21T14:44:00.000000+00:00
{code}
# Once restart happens, send nym and get-nym by indy-cli.

*Expected results:* Pool writes the nym to the ledger and sends nym details when get-nym is requested.
*Actual results:* Pool writes the nym to the ledger and sends nym details when get-nym is requested. (/)

*Scenario 2:*
*Build version:* 
indy-node: 1.6.622
indy-plenum: 1.6.554
*Test description:* Verify if pool keeps writing when scheduled restart happens.
*Preconditions:*
*Steps to Reproduce:*
# Schedule a pool restart somewhen in future. 
# Schedule a pool restart in near future
# Once restart happens, send nym and get-nym by indy-cli.

*Expected results:* Pool writes the nym to the ledger and sends nym details when get-nym is requested.
*Actual results:* Pool writes the nym to the ledger and sends nym details when get-nym is requested. (/)

*Scenario 3:*
*Build version:* 
indy-node: 1.6.622
indy-plenum: 1.6.554
*Test description:* Verify if pool keeps writing when scheduled restart cancelled.
*Preconditions:*
*Steps to Reproduce:*
# Schedule a pool restart somewhen in future. 
# Cancel the pool restart.
# Send nym and get-nym by indy-cli.

*Expected results:* Pool writes the nym to the ledger and sends nym details when get-nym is requested.
*Actual results:* Pool writes the nym to the ledger and sends nym details when get-nym is requested. (/);;;",,,,,,,,,,,,,,,,,,,,,,
Review overrides of Ubuntu debs,INDY-1742,34425,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,esplinr,esplinr,04/Oct/18 9:42 PM,24/Jul/19 3:28 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,devops,,,"Indy-Node depends on overrides of default Ubuntu debs. They are stored at repo.sovrin.org

For each package please briefly explain:
* Why we override the deb shipped with Ubuntu 16.04
* In what version of Indy Node the package was overridden
* When the override was last refreshed
* If there are any security fixes that would cause us to do an immediate refresh
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwqr5:",,,,Unset,Unset,CommunityContribution,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,esplinr,,,,,,,,,,,"05/Oct/18 11:43 PM;andkononykhin;This comment holds the progress and will be updated.

Common Why reason: either package or its necessary version is missed in Ubuntu official repositories.

How security updates are checked:
* ubuntu 16.04: apt cache for xenial-updates and xenial-security
* upstream: release notes review (searching for security related words)


*Indy-plenum*
||package||version constraint||upstream||when first||when last refreshed||latest in upstream||security fixes in ubuntu 16.04||security fixes in the upstream||
|rocksdb|5.8.8|GitHub|[22-03-2018 (1.2.287-master, 1.2.40-stable)|https://github.com/hyperledger/indy-plenum/commit/8734243b08cf056d408d488343cc72540bbf72e1]|never|5.15.10|not found|[not found|https://github.com/facebook/rocksdb/blob/master/HISTORY.md]|
|ioflo|1.5.4|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|1.7.5|not found|[not found|https://github.com/ioflo/ioflo/blob/master/ChangeLog.md]|
|orderedset|2.0|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|2.0.1|not found|[not found|https://github.com/simonpercivall/orderedset/blob/master/HISTORY.rst]|
|base58|1.0.0|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|[27-04-2018 (1.2.340-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/9a003327e96a8eff1b5170059354fa1e62033df9]|1.0.2|not found|[not found|https://github.com/keis/base58/blob/master/CHANGELOG.md]|
|prompt-toolkit|0.57|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|2.0.5|not found|[not found|https://github.com/jonathanslenders/python-prompt-toolkit]|
|rlp|0.5.1|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|1.0.3|not found|not found (release notes are not maintained)|
|sha3|0.2.1|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|0.2.1|not found|not found (release notes are not maintained)|
|libnacl|1.6.1|PyPI|[20-02-2018 (1.2.251-master, 1.2.40-stable)|https://github.com/hyperledger/indy-plenum/commit/794e8e5a5fd559d2561148f6993c9d2e98ae72d3]|never|1.6.1|not found|[not found|https://libnacl.readthedocs.io/en/latest/topics/releases/index.html]|
|six|1.11.0|PyPI|[20-02-2018 (1.2.251-master, 1.2.40-stable)|https://github.com/hyperledger/indy-plenum/commit/794e8e5a5fd559d2561148f6993c9d2e98ae72d3]|never|1.11.0|not found|[not found|//github.com/benjaminp/six/blob/master/CHANGES]|
|portalocker|0.5.7|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|1.2.1|not found|[not found|https://github.com/WoLpH/portalocker/blob/develop/CHANGELOG.rst]|
|sortedcontainers|1.5.7|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|2.0.5|not found|[not found|https://github.com/grantjenks/python-sortedcontainers/blob/master/HISTORY.rst]|
|setuptools|38.5.2|PyPI|[21-03-2018 (1.2.280-master, 1.2.40-stable)|https://github.com/hyperledger/indy-plenum/commit/b656455505231d6749417117b2bddb21321c9dbf]|never|40.4.3|not found|[not found|https://github.com/pypa/setuptools/blob/master/CHANGES.rst]|
|python-dateutil|2.6.1|PyPI|[25-04-2018 (1.2.344-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/047a8bf6005ef89a16400ddc2bb2411b3baaa23a]|never|2.7.3|not found|[not found|https://dateutil.readthedocs.io/en/stable/changelog.html]|
|semver|2.7.9|PyPI|[26-04-2018 (1.2.335-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/409db149f708536f5ec3a94eb6510ebe717f81fb]|never|2.8.1|not found|[not found|https://github.com/k-bx/python-semver/blob/master/CHANGELOG]|
|pygments|2.2.0|PyPI|[26-04-2018 (1.2.335-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/409db149f708536f5ec3a94eb6510ebe717f81fb]|[27-04-2018 (1.2.342-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/b38204b6fb785acbe6acd80f6ea28c1da39b231e]|2.2.0|not found|[not found|https://bitbucket.org/birkenfeld/pygments-main/src/tip/CHANGES?fileviewer=file-view-default]|
|psutil|5.4.3|PyPI|[26-04-2018 (1.2.335-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/409db149f708536f5ec3a94eb6510ebe717f81fb]|never|5.4.7|not found|[not found|https://github.com/giampaolo/psutil/blob/master/HISTORY.rst]|
|pyzmq|17.0.0|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|[26-04-2018 (1.2.335-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/409db149f708536f5ec3a94eb6510ebe717f81fb]|17.1.2|not found|[not found|https://pyzmq.readthedocs.io/en/latest/changelog.html]|
|intervaltree|2.1.0|PyPI|[14-07-2017 (0.4.59-master, 1.0.21-stable)|https://github.com/hyperledger/indy-plenum/commit/7f91331df706e08877ce25ff4bee8ffadef694c4]|never|2.1.0|not found|[not found|https://github.com/chaimleib/intervaltree/blob/master/CHANGELOG.md]|
|jsonpickle|0.9.6|PyPI|[26-04-2018 (1.2.336-master, 1.4.45-stable)|https://github.com/hyperledger/indy-plenum/commit/3c9849e895f7309f7bcd869fe42f0612c0be2d4d]|never|1.0|not found|[not found|https://jsonpickle.github.io/#change-log]|
|python-rocksdb|0.6.9|PyPI|[21-03-2018 (1.2.280-master, 1.2.40-stable)|https://github.com/hyperledger/indy-plenum/commit/b656455505231d6749417117b2bddb21321c9dbf]|never|0.6.9|not found|not found (release notes are not maintained)|
|pympler|0.5|PyPI|[31-07-2018 (1.5.486-master, 1.6.49-stable)|https://github.com/hyperledger/indy-plenum/commit/37c80b26aef57c63a4d719f82c7dd9e169199f74]|never|0.6|not found|[not found|https://github.com/pympler/pympler/blob/master/CHANGELOG.md]|

*indy-anoncreds*
||package||version||upstream||when first||when last refreshed||latest in upstream||security fixes in upstream or ubuntu||security fixes in the upstream||
|base58|latest (1.0.0 is used)|PyPI|[17-07-2017 (0.4.16-master, 1.0.10-stable)|https://github.com/hyperledger/indy-anoncreds/commit/4f09ade4e5de2a6dc4c029805e192cf07ac8f341]|never|1.0.2|not found|[not found|https://github.com/keis/base58/blob/master/CHANGELOG.md]|
|Charm-Crypto|latest (0.43 is used but packaged as 0.0.0)|PyPI|[17-07-2017 (0.4.16-master, 1.0.10-stable)|https://github.com/hyperledger/indy-anoncreds/commit/4f09ade4e5de2a6dc4c029805e192cf07ac8f341]|never|0.43|not found|[not found|https://github.com/JHUISI/charm/blob/dev/CHANGELOG]|

*indy-node*
||package||version||upstream||when first||when last refreshed||latest in upstream||security fixes in upstream or ubuntu||security fixes in the upstream||
|timeout-decorator|0.4.0|PyPI|[17-07-2017 (0.4.41-master, 1.1.33-stable)|https://github.com/hyperledger/indy-node/commit/e89a063ffcd42126c4bd190d0b79ea78f3630151]|[27-04-2018 (1.3.390-master, 1.4.66-stable)|https://github.com/hyperledger/indy-node/commit/fec61e7f2327807933a2ccd9f81d69ac5097ea47]|0.4.0|not found|not found (release notes are not maintained)|;;;","10/Oct/18 12:47 AM;andkononykhin;[~esplinr][~tharmon] The list is completed. Please, consider to review and close the task.;;;",,,,,,,,,,,,,,,,,,,,,,,
Include the needed version of libindy-crypto with the package dependencies,INDY-1743,34460,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,anikitinDSR,mgbailey,mgbailey,06/Oct/18 2:29 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.78,,,,0,,,,"We currently have various diverse versions of libindy-crypto installed on validator nodes. This did not change with the recent upgrade, since there is apparently no version information for this package in the other packages' dependencies.

I am unsure of the severity of this issue, but I have been told by [~MikeLodder] that the version of this package matters. ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzti7:",,,,Unset,Unset,Ev 18.20,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,esplinr,mgbailey,,,,,,,,,"09/Oct/18 10:23 PM;esplinr;We confirmed that the current release of Indy Plenum depends on Indy Crypto 0.4.3 in the Setup.py used to generate the Debian package. We quickly checked the master branch to confirm that the Sovrin package is correctly pinned to Indy Node which is pinned to Indy Plenum.

Things to check:
* Can we identify which machines are running Indy Crypto that is not version 0.4.3? What versions are they running?
* Where there warnings during the upgrade process on those machines?
* Are their sources list correctly defined?
* Retest that the upgrade procedure updates Indy Crypto properly.

[~mgbailey] Can you assist with the first three questions while we work on the last one?;;;","09/Oct/18 11:22 PM;mgbailey;indy-plenum v.1.6.51 depends on python3-indy-crypto v.0.4.3, which depends on libindy-crypto (no version). So no, libindy-crypto is not pinned to a version.;;;","10/Oct/18 5:42 PM;ashcherbakov;Yes, this is an issue, and very critical.
The proposed solution:
1) A ticket for IS team (who build python3-indy-crypto) to pin libindy-crypto correctly when building python3-indy-crypto deb package.

2) Hotfix to Sovrin for the current release to fix the issue with the next Upgrade.

 ;;;","11/Oct/18 12:39 AM;mgbailey;For the TestNet, I suggest manual remediation until this is fixed. Since there are few stewards on that network, I can easily ask them to run: ""sudo apt install libindy-crypto=0.4.3"".

Since the MainNet is still at indy-node 1.3, soon to upgrade to 1.6, I am not sure if we should do the same there.;;;","11/Oct/18 10:03 PM;anikitinDSR;New rc version of indy-plenum (1.6.53) depends on python3-indy-crypto package that pin corresponding version of libindy-crypto version. In the other words, while installing new version of indy-plenum, corresponded package of libindy-crypto will be installed automatically.;;;",,,,,,,,,,,,,,,,,,,,
Stashed checkpoints should be checked on catchup completion,INDY-1744,34499,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,spivachuk,spivachuk,09/Oct/18 5:56 PM,11/Oct/19 9:05 PM,28/Oct/23 2:47 AM,11/Oct/19 9:05 PM,,1.13.0,,,,0,,,,"On a catchup completion the node should check the stashed checkpoints to decide if a new catchup is needed. Otherwise, with INDY-1739 fixed, the node may stay lagged after a catch-up completion until it receives a next future checkpoint message from some node which triggers stashed checkpoints check.",,,,,,,,,,INDY-1739,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001ywby6",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,spivachuk,,,,,,,,,,,"11/Oct/19 9:05 PM;ashcherbakov;Since we stash all checkpoints during the catchup and unstash it in  historical order when catchup is finished, we already have the desired behaviour;;;",,,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.78,INDY-1745,34518,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,10/Oct/18 1:18 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,Documentation,,,"*Version Information*
 indy-node 1.6.78
 indy-plenum 1.6.53
 indy-anoncreds 1.0.11

*Major Fixes*
 INDY-1740 - Re-asking for ledger statuses and maximal consistency proofs is not canceled
 INDY-1741 - Bug in calling notifier methods in Restarter
 INDY-1696 - 35 view changes were happened during 10 minutes after nodes failure because of invalid request
 INDY-1700 - Requests queue is not cleared in case of reject-nym transactions.
 INDY-1672 - Throughput critically decreases without causing view_change
 INDY-1595 - Node can't catch up large ledger
 INDY-1621 - Unable to demote node in STN
 INDY-1653 - View changes happen when all responses should be rejected during load testing scenario
 INDY-1580 - Node doesn't write txns after disconnection from the rest nodes
 INDY-1618 - Throughput is degrading if backup primary is stopped

*Changes and Additions*
 INDY-1681 - Switch off a replica that stopped because disconnected from a backup primary
 INDY-1667 - Extend load scripts emulating non-smooth load according to the changes in the core script
 INDY-1607 - Proof of stability under load
 INDY-1688 - Investigate Out of memory issues with the current load testing
 INDY-1649 - Do not re-verify signature for Propagates with already verified requests
 INDY-1704 - POA: Require multiple signatures for important transactions
 INDY-1665 - Support all FEEs txns in the load script
 INDY-1661 - Test domain transactions with FEEs
 INDY-1642 - 3PC Batch should preserve the order of requests when applying PrePrepare on non-primary
 INDY-1680 - Ability to switch off (remove) replicas with no changes of F value
 INDY-1589 - A node should be able to participate in BLS multi-signature only if it has a valid proof of posession
 INDY-1637 - Make validator info as a hystorical data

*Known Issues*
INDY-1447 - Upgrade failed on pool from 1.3.62 to 1.4.66
 (!) Note that INDY-1447 was fixed in indy-node 1.5.68, but it still presents in indy-node 1.3.62 and 1.4.66 code. So, *some of the nodes may not to be upgraded during simultaneous pool-upgrade*. If this problem will appear, stewards should perform manual upgrade of indy-node in accordance with this instruction: [https://docs.google.com/document/d/1vUvbioL5OsmZMSkwRcu0p0jdttJO5VS8K3GhDLdNaoI]
 (!) To reduce the risk of reproducing INDY-1447, it is *recommended to use old CLI* for pool upgrade from indy-node *1.3.62*.

(!) *Pool upgrade from indy-node 1.3.62 to indy-node 1.6.78 should be performed simultaneously for all nodes due to txn format changes.*
 (!) *All indy-cli pools should be recreated with actual genesis files.*
 (i) *For more details about txn format changes see INDY-1421.*


 (!) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxbz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),krw910,ozheregelya,TechWritingWhiz,VladimirWork,,,,,,,,,"11/Oct/18 5:23 AM;TechWritingWhiz;This is complete and the pull request is here: https://github.com/sovrin-foundation/sovrin/pull/103

 ;;;","17/Oct/18 9:32 PM;ozheregelya;[~TechWritingWhiz], FYI.
*Known Issues* section was updated for case of Live network upgrade.;;;","18/Oct/18 4:26 AM;TechWritingWhiz;The additional information has been added to the release notes. The pull request is here: [https://github.com/sovrin-foundation/sovrin/pull/114]

 

 ;;;","18/Oct/18 11:55 PM;krw910;[~VladimirWork] It looks like Misty already has this completed based off the earlier comments. If this is complete you can close this ticket.;;;",,,,,,,,,,,,,,,,,,,,,
"Failed upgrade ""sovrin"" to version 1.1.26 ",INDY-1746,34532,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,,zhigunenko.dsr,zhigunenko.dsr,10/Oct/18 5:01 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Steps to Reproduce:*
1. Setup Docker pool with 15 containers and indy-node                  1.6.73
2. Install sovrin 1.1.6
3. Via CLI: 

{code:java}
ledger pool-upgrade name=upgrade-1 version=1.1.17 action=start sha256=f284bdc3c1c9e24a494e285cb387c69510f28de51c15bb93179d9c7f28705398 schedule={whole pool} package=sovrin force=true
{code}

4. Via CLI: 
{code:java}
ledger pool-upgrade name=upgrade-2 version=1.1.26 action=start sha256=f284bdc3c1c9e24a494e285cb387c69510f28de51c15bb93179d9c7f28705398 schedule={""Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv"":""2018-10-09T15:20:00.258870+00:00"",""8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb"":""2018-10-09T15:25:00.258870+00:00"",""DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya"":""2018-10-09T15:30:00.258870+00:00"",""4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA"":""2018-10-09T15:35:00.258870+00:00"",""4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe"":""2018-10-09T15:40:00.258870+00:00"",""Cv1Ehj43DDM5ttNBmC6VPpEfwXWwfGktHwjDJsTV5Fz8"":""2018-10-09T15:45:00.258870+00:00"",""BM8dTooz5uykCbYSAAFwKNkYfT4koomBHsSWHTDtkjhW"":""2018-10-09T15:50:00.258870+00:00"",""98VysG35LxrutKTNXvhaztPFHnx5u9kHtT7PnUGqDa8x"":""2018-10-09T15:55:00.258870+00:00"",""6pfbFuX5tx7u3XKz8MNK4BJiHxvEcnGRBs1AQyNaiEQL"":""2018-10-09T16:00:00.258870+00:00"",""HaNW78ayPK4b8vTggD4smURBZw7icxJpjZvCMLdUueiN"":""2018-10-09T16:05:00.258870+00:00"",""2zUsJuF9suBy2iKkcgmm8uoMB6u5Dq2oHoRuchrZbj2N"":""2018-10-09T16:10:00.258870+00:00"",""BXV4SXKEJeYQ8XCRHgpw1Xume5ntqALsRhbUYcF85Mse"":""2018-10-09T16:15:00.258870+00:00"",""71WAtEevzz8aZr8baNJhQCUDLwRhM7LeaErSKNWWKxzn"":""2018-10-09T16:20:00.258870+00:00"",""FEUGMFWCSAM725vyH8JZnsitiNUy31NPhugVKb8zDpng"":""2018-10-09T16:25:00.258870+00:00"",""DPZ8GJ1NyNZGJMU6qQZVuBsumY1aVzvcV4FqQK9Y215x"":""2018-10-09T16:30:00.258870+00:00""} package=sovrin
{code}

*Expected Results:*
Packages upgraded successfully
Additional ledger has been created
Writing to additional ledger is succesful without manual node/pool restart

*Actual Results:*
{code:java}
2018-10-09 15:28:34,318|ERROR|upgrader.py|Node Node1 failed upgrade 153909818157787019332 to version 1.1.26 scheduled on 2018-10-09 15:20:00.258870+00:00 because of unknown reason
2018-10-09 15:28:34,319|ERROR|upgrader.py|This problem may have external reasons, check syslog for more information
2018-10-09 15:28:34,319|INFO|notifier_plugin_manager.py|Found notifier plugins: []
2018-10-09 15:28:34,320|DEBUG|restarter.py|Node Node1 has no restart events
2018-10-09 15:28:34,320|INFO|node.py|Node1 found state to be empty, recreating from ledger
2018-10-09 15:28:34,351|INFO|motor.py|Node1 changing status from stopped to starting
{code}

*Additional Info:*
[Logs|https://drive.google.com/open?id=1NhJc3082jpPaEICjHC5mWVo3KmOrGH7P]",sovrin 1.1.17,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzud3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,"10/Oct/18 7:20 PM;zhigunenko.dsr;*Reason to Close:*
Wrong repo branch for apt;;;",,,,,,,,,,,,,,,,,,,,,,,,
Find out why max node prod time increases during long load test,INDY-1747,34563,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,10/Oct/18 5:49 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"Some observations:
* growth is same for loads of 1 NYM/sec and 10 NYMs/sec (so it doesn't depend on number of transactions written, although it's likely to depend on number of batches ordered)
* growth is same when using RocksDB and LevelDB as storage
* maximum node-to-node message size grows as well, but not as fast
* _average_ node prod time doesn't seem to be affected, which means spikes are in fact rare

In my opinion prime suspect here is cyclic garbage collector, but more research is needed.

*Acceptance criteria*
- find out why max prod time grows
- find out if this growth has upper bound
- create issues for actual fix
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Oct/18 5:49 PM;sergey.khoroshavin;Screenshot from 2018-10-09 13-13-32.png;https://jira.hyperledger.org/secure/attachment/16120/Screenshot+from+2018-10-09+13-13-32.png","10/Oct/18 5:55 PM;sergey.khoroshavin;Screenshot from 2018-10-10 11-54-45.png;https://jira.hyperledger.org/secure/attachment/16121/Screenshot+from+2018-10-10+11-54-45.png","16/Nov/18 12:11 AM;sergey.khoroshavin;Screenshot from 2018-11-15 16-08-36.png;https://jira.hyperledger.org/secure/attachment/16272/Screenshot+from+2018-11-15+16-08-36.png","20/Nov/18 1:27 AM;sergey.khoroshavin;Screenshot from 2018-11-19 19-20-54.png;https://jira.hyperledger.org/secure/attachment/16280/Screenshot+from+2018-11-19+19-20-54.png","15/Nov/18 6:09 PM;sergey.khoroshavin;Screenshot_2018-11-15_10-08-11.png;https://jira.hyperledger.org/secure/attachment/16271/Screenshot_2018-11-15_10-08-11.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwx7j:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"08/Nov/18 4:53 PM;ashcherbakov;The main suspicion here is that it's caused by GC calls which take more and more time.

*Ideas for investigation:*
 * Add GC-related metrics and track the number of objects
 * Run GC in debug mode
 * Try to disable GC (reference one) `{{gc.disable()}}` and check memory and performance
 * Call `{{sys._debugmallocstats()}}` to output more statistics;;;","10/Nov/18 1:33 AM;sergey.khoroshavin;*PoA*
* Add GC-related metrics (actually done)
* Run load test with metrics enabled (also done - number of objects tracked by GC is steadily increasing)
* Write small experimental scripts to figure out GC behavior
** Check growth patterns of _gc.garbage_
** Check _sys._debugmallocstats()_
* add more GC-related metrics to plenum as a result of experiments on small scripts and run more load tests in docker
* check most popular object types in _gc.get_objects()_ and _gc.garbage_ lists
* run load test in docker with some GC disabled on some nodes to check if we really have a problem with cyclic references
;;;","15/Nov/18 6:11 PM;sergey.khoroshavin;Load test in docker with additional metrics and statistics of most popular object types:

 !Screenshot_2018-11-15_10-08-11.png|thumbnail! 

{code}
207c4b5a2e87f:/home/indy# cat /var/log/indy/sandbox/Node2.log | grep garbage -A 2
2018-11-14 17:26:58,691|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 17:26:58,691|INFO|node.py|    <class 'function'>: 14998
2018-11-14 17:26:58,692|INFO|node.py|    <class 'dict'>: 11464
--
2018-11-14 18:26:58,947|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 18:26:58,947|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 27795
2018-11-14 18:26:58,947|INFO|node.py|    <class 'dict'>: 15156
--
2018-11-14 19:26:59,264|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 19:26:59,265|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 30112
2018-11-14 19:26:59,265|INFO|node.py|    <class 'dict'>: 15719
--
2018-11-14 20:26:59,546|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 20:26:59,547|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 55246
2018-11-14 20:26:59,547|INFO|node.py|    <class 'dict'>: 15551
--
2018-11-14 21:26:59,863|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 21:26:59,863|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 87169
2018-11-14 21:26:59,863|INFO|node.py|    <class 'function'>: 14998
--
2018-11-14 22:27:00,214|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 22:27:00,215|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 90586
2018-11-14 22:27:00,215|INFO|node.py|    <class 'dict'>: 15098
--
2018-11-14 23:27:00,579|INFO|node.py|Top objects tracked by garbage collector:
2018-11-14 23:27:00,580|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 64413
2018-11-14 23:27:00,580|INFO|node.py|    <class 'dict'>: 15937
--
2018-11-15 00:27:00,907|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 00:27:00,908|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 74832
2018-11-15 00:27:00,908|INFO|node.py|    <class 'function'>: 14998
--
2018-11-15 01:27:01,288|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 01:27:01,289|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 86350
2018-11-15 01:27:01,289|INFO|node.py|    <class 'dict'>: 17518
--
2018-11-15 02:27:01,642|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 02:27:01,643|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 96967
2018-11-15 02:27:01,644|INFO|node.py|    <class 'function'>: 14998
--
2018-11-15 03:27:01,955|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 03:27:01,955|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 71280
2018-11-15 03:27:01,955|INFO|node.py|    <class 'dict'>: 16518
--
2018-11-15 04:27:02,347|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 04:27:02,347|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 156374
2018-11-15 04:27:02,348|INFO|node.py|    <class 'dict'>: 17048
--
2018-11-15 05:27:02,752|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 05:27:02,752|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 170481
2018-11-15 05:27:02,752|INFO|node.py|    <class 'function'>: 14998
--
2018-11-15 06:27:03,161|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 06:27:03,162|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 139272
2018-11-15 06:27:03,162|INFO|node.py|    <class 'function'>: 14998
--
2018-11-15 06:37:03,246|INFO|node.py|Top objects tracked by garbage collector:
2018-11-15 06:37:03,247|INFO|node.py|    <class 'orderedset._orderedset.entry'>: 186832
2018-11-15 06:37:03,247|INFO|node.py|    <class 'function'>: 14998
{code};;;","15/Nov/18 6:16 PM;sergey.khoroshavin;It can be seen that:
* number of live objects grows steadily
* Gen2 collection collects lots of objects, but list of live objects doesn't shrink as much
* the only type of object that seem to leak is _orderedset._orderedset.entry_

*Updated PoA*
* add metrics for all OrderedSet-based collections (there aren't many) to check if they are cleared
* try to use standard OrderedDict instead of 3rd party OrderedSet (written in Cython) to check if OrderedSet is leaky;;;","16/Nov/18 12:15 AM;sergey.khoroshavin; !Screenshot from 2018-11-15 16-08-36.png|thumbnail! 
It turned out that problem was in _Replica.ordered_ collection which was not cleared until view change, so this is a real memory leak.  However, fix is quite easy, PR is already in progress: https://github.com/hyperledger/indy-plenum/pull/979;;;","20/Nov/18 1:37 AM;sergey.khoroshavin; !Screenshot from 2018-11-19 19-20-54.png|thumbnail! 
Results of load test after fix was applied. It can be  seen that number of objects tracked by GC is no longer grows, actually now it matches number of requests in queue. Also GC gen2 events no longer become more frequent, and _max_prod_time_ seems stable. So it seems like now there are no leaks in python code under sustainable load.

*Problem description*
It turned out that _Replica.ordered_ collection was not cleared until view change. This led to ever increasing number live objects, which in the long run significantly degraded GC performance. Also it was a memory leak

*Solution*
_Replica.ordered_ was basically a set containing contiguous ranges of integers. Replacing it with _IntervalList_ with automatically merging ranges reduced storage requirements from O(N) to O(1) (where N is number of pp_seq_nos tracked).

*Versions*
Indy-node: 1.6.684
Indy-plenum: 1.6.594

*PR*
https://github.com/hyperledger/indy-plenum/pull/979

*Covered by tests*
https://github.com/hyperledger/indy-plenum/pull/979/files#diff-4eecd62b97800f596d964dc888f71cd4

*Risk*
Low
;;;",,,,,,,,,,,,,,,,,,,
Failed to upgrade from Sovrin 1.1.24 to Sovrin 1.1.26,INDY-1748,34564,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,VladimirWork,VladimirWork,VladimirWork,10/Oct/18 7:51 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.78,,,,0,,,,"Build Info:
sovrin 1.1.24
indy-node 1.6.73
plugins 0.9.3~

Steps to Reproduce:
1. Install sovrin 1.1.24 (0.9.3~ plugins dependency) pool.
2. Try to upgrade it to sovrin 1.1.26 (0.9.4~ plugins dependency) using POOL_UPGRADE command.
3. Check package versions and journalctl.

Actual Result:
Upgrade is failed.
{noformat}
Oct 10 10:23:02 290bca03d347 env[62]: + echo 'Try to donwload indy version indy-anoncreds=1.0.11 python3-indy-crypto=0.4.3 indy-plenum=1.6.52 indy-node=1.6.74 sovrin=1.1.26'
Oct 10 10:23:02 290bca03d347 env[62]: Try to donwload indy version indy-anoncreds=1.0.11 python3-indy-crypto=0.4.3 indy-plenum=1.6.52 indy-node=1.6.74 sovrin=1.1.26
Oct 10 10:23:02 290bca03d347 env[62]: + apt-get -y update
Oct 10 10:23:02 290bca03d347 env[62]: Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Oct 10 10:23:03 290bca03d347 env[62]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 10 10:23:03 290bca03d347 env[62]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 10 10:23:03 290bca03d347 env[62]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 10 10:23:04 290bca03d347 env[62]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 10 10:23:04 290bca03d347 env[62]: Fetched 107 kB in 1s (75.7 kB/s)
Oct 10 10:23:05 290bca03d347 env[62]: Reading package lists...
Oct 10 10:23:06 290bca03d347 env[62]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-anoncreds=1.0.11 python3-indy-crypto=0.4.3 indy-plenum=1.6.52 indy-node=1.6.74 sovrin=1.1.26
Oct 10 10:23:07 290bca03d347 env[62]: Reading package lists...
Oct 10 10:23:07 290bca03d347 env[62]: Building dependency tree...
Oct 10 10:23:07 290bca03d347 env[62]: Reading state information...
Oct 10 10:23:07 290bca03d347 env[62]: indy-anoncreds is already the newest version (1.0.11).
Oct 10 10:23:07 290bca03d347 env[62]: python3-indy-crypto is already the newest version (0.4.3).
Oct 10 10:23:07 290bca03d347 env[62]: Some packages could not be installed. This may mean that you have
Oct 10 10:23:07 290bca03d347 env[62]: requested an impossible situation or if you are using the unstable
Oct 10 10:23:07 290bca03d347 env[62]: distribution that some required packages have not yet been created
Oct 10 10:23:07 290bca03d347 env[62]: or been moved out of Incoming.
Oct 10 10:23:07 290bca03d347 env[62]: The following information may help to resolve the situation:
Oct 10 10:23:07 290bca03d347 env[62]: The following packages have unmet dependencies:
Oct 10 10:23:07 290bca03d347 env[62]:  sovrin : Depends: sovtoken (= 0.9.4+1.59) but 0.9.3+12.58 is to be installed
Oct 10 10:23:07 290bca03d347 env[62]:           Depends: sovtokenfees (= 0.9.4+1.59) but 0.9.3+13.58 is to be installed
Oct 10 10:23:07 290bca03d347 env[62]:  sovtoken : Depends: indy-plenum (= 1.6.51) but 1.6.52 is to be installed
Oct 10 10:23:07 290bca03d347 env[62]: E: Error, pkgProblemResolver::Resolve generated breaks, this may be caused by held packages.
Oct 10 10:23:07 290bca03d347 env[62]: + ret=100
Oct 10 10:23:07 290bca03d347 env[62]: + '[' 100 -ne 0 ']'
Oct 10 10:23:07 290bca03d347 env[62]: + echo 'Failed to obtain indy-anoncreds=1.0.11 python3-indy-crypto=0.4.3 indy-plenum=1.6.52 indy-node=1.6.74 sovrin=1.1.26'
Oct 10 10:23:07 290bca03d347 env[62]: Failed to obtain indy-anoncreds=1.0.11 python3-indy-crypto=0.4.3 indy-plenum=1.6.52 indy-node=1.6.74 sovrin=1.1.26
Oct 10 10:23:07 290bca03d347 env[62]: + exit 1
Oct 10 10:23:07 290bca03d347 env[62]: Upgrade from 1.1.24 to 1.1.26 failed: upgrade script failed, exit code is 1
Oct 10 10:23:07 290bca03d347 env[62]: Trying to rollback to the previous version upgrade script failed, exit code is 1
{noformat}

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/18 7:38 PM;VladimirWork;1748.tar.gz;https://jira.hyperledger.org/secure/attachment/16139/1748.tar.gz","10/Oct/18 7:50 PM;VladimirWork;journal;https://jira.hyperledger.org/secure/attachment/16122/journal",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzuin:",,,,Unset,Unset,Ev 18.20,Ev 18.21,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,dsurnin,VladimirWork,,,,,,,,,"10/Oct/18 11:15 PM;anikitinDSR;After logs and code analizing was noticed the next problem:

All the token plugin's related packages have not expected version representation, like sovtoken (= 0.9.4+1.59). Node_control_tool is parsing output of apt command for package to update and expect, that version will be in SemVer style, like 0.9.4. But for now, packages sovtoken and sovtokenfees will be not included in deps list and will be not used in apt-get install command. Also this packages are held.
For solving it we can:

1) Fix it in node_control_tool side. But this fix will not be included in currently running node_control_tool.

2) Fix version representation for sovtoken and sovtokenfees packages.;;;","11/Oct/18 9:24 PM;anikitinDSR;Reasons:
 * need to fix upgrade procedure from sovrin with plugins to newest version with plugins

Changes:
 * fix dependency tree building

Versions:
 * indy-node: rc 1.6.75
 * indy-plenum: rc 1.6.53
 * sovrin: rc 1.1.27

Reqcomendations for QA:
 * upgrade sovrin package from version with plugins to sovrin version 1.1.27;;;","12/Oct/18 7:27 PM;VladimirWork;Build Info:
sovrin=1.1.24 indy-node=1.6.73 indy-plenum=1.6.51 python3-indy-crypto=0.4.3 libindy-crypto=0.4.3 sovtoken=0.9.3+12.58 sovtokenfees=0.9.3+13.58

Steps to Reproduce:
1. Install sovrin 1.1.24 (0.9.3~ plugins dependency) pool.
2. Try to upgrade it to sovrin 1.1.27 (0.9.5~ plugins dependency) using POOL_UPGRADE command.
3. Check package versions and journalctl.

Actual Result:
Upgrade is failed.
{noformat}
Oct 12 10:00:00 e08efcd808a0 env[72]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 12 10:00:12 e08efcd808a0 env[72]: + deps='sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.27'
Oct 12 10:00:12 e08efcd808a0 env[72]: + '[' -z 'sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.27' ']'
Oct 12 10:00:12 e08efcd808a0 env[72]: + echo 'Try to donwload indy version sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.
Oct 12 10:00:12 e08efcd808a0 env[72]: Try to donwload indy version sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.27
Oct 12 10:00:12 e08efcd808a0 env[72]: + apt-get -y update
Oct 12 10:00:12 e08efcd808a0 env[72]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 12 10:00:12 e08efcd808a0 env[72]: Hit:2 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 12 10:00:12 e08efcd808a0 env[72]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 12 10:00:12 e08efcd808a0 env[72]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 12 10:00:13 e08efcd808a0 env[72]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 12 10:00:15 e08efcd808a0 env[72]: Reading package lists...
Oct 12 10:00:15 e08efcd808a0 env[72]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.27
Oct 12 10:00:16 e08efcd808a0 env[72]: Reading package lists...
Oct 12 10:00:16 e08efcd808a0 env[72]: Building dependency tree...
Oct 12 10:00:16 e08efcd808a0 env[72]: Reading state information...
Oct 12 10:00:16 e08efcd808a0 env[72]: indy-anoncreds is already the newest version (1.0.11).
Oct 12 10:00:16 e08efcd808a0 env[72]: Some packages could not be installed. This may mean that you have
Oct 12 10:00:16 e08efcd808a0 env[72]: requested an impossible situation or if you are using the unstable
Oct 12 10:00:16 e08efcd808a0 env[72]: distribution that some required packages have not yet been created
Oct 12 10:00:16 e08efcd808a0 env[72]: or been moved out of Incoming.
Oct 12 10:00:16 e08efcd808a0 env[72]: The following information may help to resolve the situation:
Oct 12 10:00:16 e08efcd808a0 env[72]: The following packages have unmet dependencies:
Oct 12 10:00:16 e08efcd808a0 env[72]:  python3-indy-crypto : Depends: libindy-crypto (= 0.4.5) but 0.4.3 is to be installed
Oct 12 10:00:16 e08efcd808a0 env[72]: E: Unable to correct problems, you have held broken packages.
Oct 12 10:00:16 e08efcd808a0 env[72]: + ret=100
Oct 12 10:00:16 e08efcd808a0 env[72]: + '[' 100 -ne 0 ']'
Oct 12 10:00:16 e08efcd808a0 env[72]: + echo 'Failed to obtain sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.27'
Oct 12 10:00:16 e08efcd808a0 env[72]: Failed to obtain sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.75 sovrin=1.1.27
Oct 12 10:00:16 e08efcd808a0 env[72]: + exit 1
Oct 12 10:00:16 e08efcd808a0 env[72]: Upgrade from 1.1.24 to 1.1.27 failed: upgrade script failed, exit code is 1
Oct 12 10:00:16 e08efcd808a0 env[72]: Trying to rollback to the previous version upgrade script failed, exit code is 1
{noformat}
;;;","15/Oct/18 7:35 PM;VladimirWork;Build Info:
sovrin=1.1.24 indy-node=1.6.73 indy-plenum=1.6.51 python3-indy-crypto=0.4.3 libindy-crypto=0.4.3 sovtoken=0.9.3+12.58 sovtokenfees=0.9.3+13.58

Steps to Reproduce:
1. Install sovrin 1.1.24 (0.9.3~ plugins dependency) pool.
2. Try to upgrade it to sovrin 1.1.29 (0.9.5~ plugins dependency) using POOL_UPGRADE command with *force=False*.
3. Try to upgrade it to sovrin 1.1.29 (0.9.5~ plugins dependency) using POOL_UPGRADE command with *force=True*.
4. Check package versions and journalctl.

Actual Result:
1. Upgrade with *force=False* is not triggered - there is no upgrade log created and no entries in journalctl about upgrade (but it is present in config ledger).
2. Upgrade with *force=True* is triggered successfully but is failed:
{noformat}
Oct 15 10:16:58 410be396e052 env[74]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 15 10:17:13 410be396e052 env[74]: + deps='sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.76 sovrin=1.1.29'
Oct 15 10:17:13 410be396e052 env[74]: + '[' -z 'sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.76 sovrin=1.1.29' ']'
Oct 15 10:17:13 410be396e052 env[74]: + echo 'Try to donwload indy version sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.76 sovrin=1.1.
Oct 15 10:17:13 410be396e052 env[74]: Try to donwload indy version sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.76 sovrin=1.1.29
Oct 15 10:17:13 410be396e052 env[74]: + apt-get -y update
Oct 15 10:17:13 410be396e052 env[74]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 15 10:17:13 410be396e052 env[74]: Hit:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 15 10:17:13 410be396e052 env[74]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Oct 15 10:17:13 410be396e052 env[74]: Hit:4 http://security.ubuntu.com/ubuntu xenial-security InRelease
Oct 15 10:17:14 410be396e052 env[74]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 15 10:17:16 410be396e052 env[74]: Reading package lists...
Oct 15 10:17:16 410be396e052 env[74]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5
Oct 15 10:17:17 410be396e052 env[74]: Reading package lists...
Oct 15 10:17:17 410be396e052 env[74]: Building dependency tree...
Oct 15 10:17:17 410be396e052 env[74]: Reading state information...
Oct 15 10:17:17 410be396e052 env[74]: indy-anoncreds is already the newest version (1.0.11).
Oct 15 10:17:17 410be396e052 env[74]: Some packages could not be installed. This may mean that you have
Oct 15 10:17:17 410be396e052 env[74]: requested an impossible situation or if you are using the unstable
Oct 15 10:17:17 410be396e052 env[74]: distribution that some required packages have not yet been created
Oct 15 10:17:17 410be396e052 env[74]: or been moved out of Incoming.
Oct 15 10:17:17 410be396e052 env[74]: The following information may help to resolve the situation:
Oct 15 10:17:17 410be396e052 env[74]: The following packages have unmet dependencies:
Oct 15 10:17:18 410be396e052 env[74]:  python3-indy-crypto : Depends: libindy-crypto (= 0.4.5) but 0.4.3 is to be installed
Oct 15 10:17:18 410be396e052 env[74]:  sovrin : Depends: libindy-crypto (= 0.4.5) but 0.4.3 is to be installed
Oct 15 10:17:18 410be396e052 env[74]: E: Unable to correct problems, you have held broken packages.
Oct 15 10:17:18 410be396e052 env[74]: + ret=100
Oct 15 10:17:18 410be396e052 env[74]: + '[' 100 -ne 0 ']'
Oct 15 10:17:18 410be396e052 env[74]: + echo 'Failed to obtain sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.76 sovrin=1.1.29'
Oct 15 10:17:18 410be396e052 env[74]: Failed to obtain sovtokenfees=0.9.5 sovtoken=0.9.5 indy-anoncreds=1.0.11 python3-indy-crypto=0.4.5 indy-plenum=1.6.53 indy-node=1.6.76 sovrin=1.1.29
Oct 15 10:17:18 410be396e052 env[74]: + exit 1
Oct 15 10:17:18 410be396e052 env[74]: Upgrade from 1.1.24 to 1.1.29 failed: upgrade script failed, exit code is 1
Oct 15 10:17:18 410be396e052 env[74]: Trying to rollback to the previous version upgrade script failed, exit code is 1
{noformat}
;;;","15/Oct/18 9:44 PM;ashcherbakov;*PoA for developers:*
 # Create a new RC of IndyNode=1.6.77 with the same code, but pointing to the previous Plenum (1.6.52) which depends on python3-indy-crypto=0.4.3
 # Create a new RC of IndyNode=1.6.78 with the same code, but pointing to Plenum (1.6.53) which depends on python3-indy-crypto=0.4.5
 # Create a new RC of Sovrin=1.1.30 depending on Indy-Node=1.6.78 and latest plugins (0.9.5)

*PoA for QA:*
 # Test STN Upgrade:
 ** Install sovrin=1.1.24 (emulate different versions of libindy-crypto < 0.4.3)
 ** Update libindy-crypto manually to 0.4.5
 ** Run POOL_UPGRADE for sovrin=1.1.30 (non-forced)
 # Test SLN Upgrade:
 ** Install old sovrin and node (indy-node=1.3.x)
 ** Run POOL_UPGRADE for indy-node=1.6.77 (forced)
 ** Run POOL_UPGRADE for sovrin=1.1.30 (non-forced)

*Upgrade Workflow*
 * STN Upgrade:
 ** Update libindy-crypto manually to 0.4.5
 ** Run POOL_UPGRADE for sovrin=1.1.30 (non-forced)
 * SLN Upgrade:
 ** Run POOL_UPGRADE for indy-node=1.6.77 (forced)
 *** will fix Upgrade procedure, but version of IndyCrypto may be different on nodes
 ** Run POOL_UPGRADE for sovrin=1.1.30 (non-forced)
 *** will fix IndyCrypto versions, and install Sovrin with Token plugins;;;","16/Oct/18 8:20 PM;dsurnin;Non forced upgrade was not handled correctly because config_req_handler were replaced with StaticFeesReqHandler by Sovrin Plugins.
 Method in plenum/node.py
{code:java}
    def commitAndSendReplies(self, ledger_id, ppTime, reqs: List[Request],
                             stateRoot, txnRoot) -> List:
        logger.trace('{} going to commit and send replies to client'.format(self))
*!!*        reqHandler = self.get_req_handler(ledger_id)
*!!*        committedTxns = reqHandler.commit(len(reqs), stateRoot, txnRoot, ppTime)
        self.execute_hook(NodeHooks.POST_BATCH_COMMITTED, ledger_id=ledger_id,
                          pp_time=ppTime, committed_txns=committedTxns,
                          state_root=stateRoot, txn_root=txnRoot)
        self.updateSeqNoMap(committedTxns, ledger_id)
        updated_committed_txns = list(map(self.update_txn_with_extra_data, committedTxns))
        self.hook_pre_send_reply(updated_committed_txns, ppTime)
        self.sendRepliesToClients(updated_committed_txns, ppTime)
        self.hook_post_send_reply(updated_committed_txns, ppTime)
        return committedTxns
{code}
indy_node has 3 default ledgers 0 - pool; 1 - domain; 2 - config
 two marked lines show error - we select req handler by index 2 (which is config in terms of indy_node), but in case of plugins we have the following
{code:java}
ledger_id 2
reqHandler <sovtokenfees.static_fee_req_handler.StaticFeesReqHandler object at 0x7fcd8a4019b0>
self.ledger_to_req_handler {0: <indy_node.server.pool_req_handler.PoolRequestHandler object at 0x7fcd8e9bf048>, 1: <indy_node.server.domain_req_handler.DomainReqHandler object at 0x7fcd8e9bf780>, 2: <sovtokenfees.static_fee_req_handler.StaticFeesReqHandler object at 0x7fcd8a4019b0>, 1001: <sovtoken.token_req_handler.TokenReqHandler object at 0x7fcd8a401940>}
{code}
so the commit method of wrong req handler is called and upgrade transaction is not handled correctly.;;;","16/Oct/18 8:23 PM;ashcherbakov;Plugins override ConfigRegHandler incorrectly (using a parent class from Plenum, not from IndyNode)
 If we want to fix it, we will need:
 * Make plugins depend on Indy-Node, not Plenum
 * Change dependencies in Sovrin (depend on Plugins only, not IndyNode?)
 * Use correct parent class (from Indy-Node)
 * re-issue Sovrin RCs

There is a workaround: use forced (force=True) Upgrade for STN.

If we don't fix the issue now, then
 # POOL_CONFIG txn doesn't work (since it's incorrectly overriden by Plugins)
 # We will have to do forced Upgrades until the issue in plugins is fixed. So, if we use the latest RC for upgrade of the Main Net, then since the issue is still there, the next upgrade (after the Upgrade to 1.6) needs to forced as well.

If we want to do the fix right now, then there is a risk that we will have to change dependency tree, and re-test Plugins.

 ;;;","17/Oct/18 12:45 AM;VladimirWork;Test STN Upgrade: (/)
Install sovrin=1.1.24 (emulate different versions of libindy-crypto < 0.4.3)
Update libindy-crypto manually to 0.4.5
Run POOL_UPGRADE for sovrin=1.1.30 (-non-forced- forced)

Test SLN Upgrade: (x)
Install old sovrin and node (indy-node=1.3.x)
Run POOL_UPGRADE for indy-node=1.6.77 (forced)
Run POOL_UPGRADE for sovrin=1.1.30 (-non-forced- forced)

{noformat}
Oct 16 15:20:05 a839cb478e79 env[3205]: + deps='libindy-crypto=0.4.5 sovrin=1.1.30'
Oct 16 15:20:05 a839cb478e79 env[3205]: + '[' -z 'libindy-crypto=0.4.5 sovrin=1.1.30' ']'
Oct 16 15:20:05 a839cb478e79 env[3205]: + echo 'Try to donwload indy version libindy-crypto=0.4.5 sovrin=1.1.30'
Oct 16 15:20:05 a839cb478e79 env[3205]: Try to donwload indy version libindy-crypto=0.4.5 sovrin=1.1.30
Oct 16 15:20:05 a839cb478e79 env[3205]: + apt-get -y update
Oct 16 15:20:06 a839cb478e79 env[3205]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 16 15:20:06 a839cb478e79 env[3205]: Hit:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 16 15:20:06 a839cb478e79 env[3205]: Get:3 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Oct 16 15:20:06 a839cb478e79 env[3205]: Get:4 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Oct 16 15:20:07 a839cb478e79 env[3205]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 16 15:20:07 a839cb478e79 env[3205]: Fetched 214 kB in 1s (163 kB/s)
Oct 16 15:20:08 a839cb478e79 env[3205]: Reading package lists...
Oct 16 15:20:08 a839cb478e79 env[3205]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install libindy-crypto=0.4.5 sovrin=1.1.30
Oct 16 15:20:09 a839cb478e79 env[3205]: Reading package lists...
Oct 16 15:20:09 a839cb478e79 env[3205]: Building dependency tree...
Oct 16 15:20:09 a839cb478e79 env[3205]: Reading state information...
Oct 16 15:20:09 a839cb478e79 env[3205]: libindy-crypto is already the newest version (0.4.5).
Oct 16 15:20:09 a839cb478e79 env[3205]: Some packages could not be installed. This may mean that you have
Oct 16 15:20:09 a839cb478e79 env[3205]: requested an impossible situation or if you are using the unstable
Oct 16 15:20:09 a839cb478e79 env[3205]: distribution that some required packages have not yet been created
Oct 16 15:20:09 a839cb478e79 env[3205]: or been moved out of Incoming.
Oct 16 15:20:09 a839cb478e79 env[3205]: The following information may help to resolve the situation:
Oct 16 15:20:09 a839cb478e79 env[3205]: The following packages have unmet dependencies:
Oct 16 15:20:09 a839cb478e79 env[3205]:  sovrin : Depends: indy-node (= 1.6.78) but 1.6.77 is to be installed
Oct 16 15:20:09 a839cb478e79 env[3205]:           Depends: sovtoken (= 0.9.5) but it is not going to be installed
Oct 16 15:20:09 a839cb478e79 env[3205]:           Depends: sovtokenfees (= 0.9.5) but it is not going to be installed
Oct 16 15:20:09 a839cb478e79 env[3205]: E: Unable to correct problems, you have held broken packages.
Oct 16 15:20:09 a839cb478e79 env[3205]: + ret=100
Oct 16 15:20:09 a839cb478e79 env[3205]: + '[' 100 -ne 0 ']'
Oct 16 15:20:09 a839cb478e79 env[3205]: + echo 'Failed to obtain libindy-crypto=0.4.5 sovrin=1.1.30'
{noformat}
;;;","17/Oct/18 4:05 PM;ashcherbakov;This is caused by the following:
 * sovrin 1.1.0 depends on the latest indy-node (without explicit Indy-node version pin)
 * When upgrade procedure traverses dependencies, it doesn't take indy-node from Sovrin since there is no explicit version pin.
 * So, the upgrade commands look like `apt-get install libindy-crypto=0.4.5 sovrin=1.1.30`
 * Sovrin-1.1.30 depends on indy-node 1.6.78; indy-node 1.6.77 is installed
 * Since indy-node is on hold, and sovrin depends on a newer version of indy-node, update fails

A separate ticket is created to address the issue: INDY-1764

As a workaround for the MainNet Upgrade we can do the following ([~ozheregelya] verified this):
 * Run POOL_UPGRADE for indy-node=1.6.78 (forced)
 * Run POOL_UPGRADE for sovrin=1.1.30 (forced)

 ;;;","17/Oct/18 9:42 PM;VladimirWork;Build Info:
indy-anoncreds=1.0.11 indy-plenum=1.2.42 indy-node=1.3.62 sovrin=1.1.10 python3-base58=0.2.4 python3-indy-crypto=0.4.1 libindy-crypto=0.4.0

Steps to Validate:
1. Upgrade 1.3.62 (1.1.10) pool to 1.6.78 version simultaneously with force=True.
2. Check all package versions and send all txn types to the ledger.
3. Upgrade 1.6.78 (1.1.10) pool to 1.1.30 version simultaneously with force=True.
4. Check all package versions and send all txn types to the ledger.

Actual Results:
Both upgrades was passed successfully. Pool works normally.;;;",,,,,,,,,,,,,,,
Validator info tool should show windowed throughput statistics,INDY-1749,34565,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,10/Oct/18 9:00 PM,08/Jan/19 4:26 AM,28/Oct/23 2:47 AM,,,,validator-info,,,0,,,,"We already have windowed throughput measurements for write requests in monitor. It might be sensible to reuse this code for read throughput measurements as well and to use it in validator_info to avoid confusion (INDY-930). 

*Things to be determined:*
* are we okay with windowed throughput measurements as implemented in monitor
* should we use same window size for read and write request throughput measurements
* should we use same window size for monitor and validator info
* should we output used window size in validator info (it can be configurable and different stewards can potentially set according to their preferences)

It would be nice to have input from [~krw910].",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-930,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzuiv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use standard tools to parallelize test execution,INDY-1750,34566,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,10/Oct/18 9:28 PM,10/Oct/18 9:28 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,Pytest-xdist looks like a good candidate.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-380,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzuj3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid redundant static validation during signature verification,INDY-1753,34601,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,NataliaDracheva,ashcherbakov,ashcherbakov,11/Oct/18 9:12 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"As of now we don't re-validate signatures in Requests came in Propagates. 
However, it lead to a lot of creation of Request (SafeRequest class) instances, which implies static (schema) validation. This process is quite expensive and affects performance a lot.
We need to avoid static (schema) validation for a Request if it has been already perfromed.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1768,,,,,,,,,,,,,,,,,,,,"24/Oct/18 11:27 PM;NataliaDracheva;indy_cli.log;https://jira.hyperledger.org/secure/attachment/16163/indy_cli.log",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwxc7:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,spivachuk,,,,,,,,,,"22/Oct/18 11:00 PM;spivachuk;*Issue #1*

The structure of a write request incoming in a Propagate or in a MessageRep with a Propagate is validated against the schema several times. This is redundant and time-consuming.

*Issue #2*

Now static validation ({{Node.doStaticValidation}}) is always performed for a write request received from a client and is never performed for a write request received from a node in a Propagate or a MessageRep with a Propagate. It is an issue because if a request is received only from nodes in Propagates and / or MessageReps with Propagates then static validation is not done. Static validation must be performed one time for a write request received from a client and all its Propagates received from nodes and this validation must be performed when the node sees the request for the first time.

*Issue #3*

Static validation ({{Node.doStaticValidation}}) is not performed and the signature is not verified for a write request received in a MessageRep with a Propagate.;;;","22/Oct/18 11:01 PM;spivachuk;*PoA for Issue #1:*

Replace constructing of a SafeRequest instance with constructing of a Request instance in {{Node.verifySignature}}, {{Node.processPropagate}} and {{PropagateHandler.create}} methods to eliminate redundant validations of the request structure. (The request structure is validated earlier - in scope of constructing the Propagate instance.);;;","23/Oct/18 2:00 AM;spivachuk;*PoA for Issues #2 and #3:*

Now we perform schema validation, static validation ({{Node.doStaticValidation}}) and signature verification when processing incoming messages from ZStack level. This makes it impossible to use {{Propagator.requests}} map for skipping validation of already validated requests. This is so because a node handles ZStack queues and inBox queues sequentially. Thus multiple ZStack-level messages representing the same client request can be processed in one pass while the corresponding typed messages being created are just enqueued to inBox and not processed synchronously. So {{Propagator.requests}} map will not be populated at this step (it is populated when typed messages from inBox are processed). Now, to avoid repeated signature verification, such the workflow makes us to maintain additional map of authenticated requests ({{ReqAuthenticator}} maintains this map).

If we move static validation ({{Node.doStaticValidation}}) and signature verification from the step of processing incoming ZStack-level messages to the step of processing typed inBox-level messages, then we will be able to ensure single-time static validation / signature verification (on the first reception of a request either from a client or as a Propagate from a node) with use of {{Propagator.requests}} map only. No additional maps of validated requests will be needed. By the way, this will fix the issue with a lack of static validation and signature verification for a request received in a MessageRep with a Propagate.

The plan for this fix is as follows:
* Perform only validation against a schema in {{Node.handleOneClientMsg}} and {{Node.handleOneNodeMsg}} methods.
* In {{Node.processClientRequest}}, {{Node.processNodeRequest}} and {{Node.processPropagate}} methods:
*# Perform static validation ({{Node.doStaticValidation}}) and signature verification if in {{Propagator.requests}} there is no ReqState for the request digest or if there is a ReqState for this digest but the request signature in it is different.
*# If both static validation and signature verification succeed (this may happen only in case there is no ReqState for the request digest in {{Propagator.requests}}) then add the ReqState for the request digest to {{Propagator.requests}}.
* Remove the additional map of authenticated requests and its usage from {{ReqAuthenticator}} class.;;;","23/Oct/18 11:57 PM;spivachuk;Created INDY-1768 for fixing *Issues #2 and #3*.;;;","24/Oct/18 12:51 AM;spivachuk;*Changes:*
 - Made changes according to PoA for *Issue #1*.

*PRs:*
 - [https://github.com/hyperledger/indy-plenum/pull/953]
 - [https://github.com/hyperledger/indy-node/pull/994]

*Version:*
 - indy-node 1.6.642-master
 - indy-plenum 1.6.569-master

*Risk factors:*
 - Nothing is expected.

*Risk:*
 - Low

*Covered with tests:*
 - {{test_schema_validation_for_request_from_client}}
 - {{test_request_schema_validation_for_propagate}};;;","24/Oct/18 11:28 PM;NataliaDracheva;*Scenario 1:*
*Build version:* 
indy-node: 1.6.645
indy-plenum: 1.6.570
*Test description:* Verify the static validation cases.
*Preconditions:* Docker pool of 4 nodes.
*Steps to validate:*
# Run indy-cli.
# Connect to the pool.
# Send requests to the pool:
* Send a valid write request (e.g. nym).
* Send a valid custom request.
* Send invalid custom requests:
** with an unexpected field;
** without a mandatory field;
** with a typo in a field name;
** with a typo in a field value. 
** without a signature where it is required (e.g. for nym request).

*Expected results:* Pool responds with expected success replies in case of correct requests and throws errors when invalid requests are sent.
*Actual results:* Pool responds with expected success replies in case of correct requests and throws errors when invalid requests are sent. (/)
*Additional info:*  [^indy_cli.log] 
Scheduled a performance test for this ticket.;;;",,,,,,,,,,,,,,,,,,,
Function plenum.common.util.randomString does not return fully random result,INDY-1754,34606,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,Toktar,gdshaw,gdshaw,12/Oct/18 12:43 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.7.1,plenum,,,0,TShirt_S,,,"There is an off-by-one error in the function plenum.common.util.randomString which means that returned values containing an odd number of hex digits cannot end with an 'F'.

This occurs because the final digit is obtained by evaluating the expression randombytes_uniform(15). The intent was presumably to obtain a number between 0 and 15 inclusive, however the function interprets its argument as an exclusive upper bound, therefore the value 15 can never be generated.

To test this, Nettitude generated 100,000 values using randomString each with 3 hex digits, then counted the number of occurrences of each value. As expected, no results ending in an 'F' were observed.

The effect is to reduce the entropy of generated values by approximately 0.1 bits compared to a random number generator with a uniform distribution. It is very unlikely that this would ever make the difference between a vulnerability being exploitable or not, however a fix is recommended on the grounds that the code does not behave as expected.","Testing was performed against libnacl 1.6.1, however Nettitude would expect the outcome to be the same regardless of the environment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000002c",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,gdshaw,Toktar,,,,,,,,,,"12/Oct/18 12:51 AM;gdshaw;For information there is a similar error in indy-node/scripts/performance/perf_load/perf_utils.py, however this is presumably only used for testing.;;;","26/Nov/18 11:42 PM;esplinr;This function is currently only used in tests.;;;","12/Feb/19 9:29 PM;Toktar;""F"" can't be achieved because randomString() can generate strings only with lowercase letters.
Added test that 'f' at the end of the generated string can be achieved - [test_common_util.py|https://github.com/hyperledger/indy-plenum/pull/1086/files#diff-10e9cc2ab963467441012f6b8415d52b];;;",,,,,,,,,,,,,,,,,,,,,,
Research Exonum implementation,INDY-1755,34618,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,12/Oct/18 4:54 PM,09/Sep/19 10:07 PM,28/Oct/23 2:47 AM,,,2.0,,,,0,,,,"[https://github.com/exonum/exonum]

We need to find out how stable Exonum Implementation is.
 Timebox the effort to 4 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** Characteristics of current deployments in production usage
 ** Current project roadmap
 ** Health of the open source community / level of investment
 ** Rough estimate of work required to bring Exonum to the same level as Plenum
 ** Rough understanding of performance
 ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Exonum
 * A recommendation on whether to research more or kill the proposal
 * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwx4f:2rzm8",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,RyanWest,,,,,,,,,,,"08/Nov/18 3:30 PM;RyanWest;*Research report copied over from #ledger_2_0 Slack channel:*

Exonum is like Tendermint’s protocol but with a few tweaks, all written in Rust. It initially looked great, with good documentation and runs well on my computer, but I learned a few unfortunate things when looking through their chat:

I came across a comment on their chat (on Gitter, similar to Rocket.Chat) saying that a reasonable upper bound “for the number of validators would be around 8-15. 16 nodes is maximum for anchoring service”.

And also this: “We’re aiming at private networks, so more than 16 validators will not bring much benefits, but will drastically decrease network performance.” I’m under the impression that we’re hoping to have 25-30+ validator nodes, so that might be an issue. Of course, @alexander.shcherbakov pointed out to me earlier that “this is rather a question whether PBFT-like protocols (in general) are suitable for Sovrin if Sovrin wants to have 25+ Nodes.”

Another comment from their chat: “Additionally, you cannot add new validators easily hence public blockchain is nearly impossible.”

*Responses:*

alexander.shcherbakov
I agree with what Sergey said below. There is no actualy limitations for the number of nodes, this is just a question of performance (throughput VS latency).
Another question is whether Exonum is well-tested on 25 node pools, because our experience showed that behavior in case of 25 nodes and in case of 9 nodes is very different.


alexander.shcherbakov 
Private blockchain is a goal for almost all existing frameworks (Tendermint may be the only exception), so this can also be not a real limitation.
But we need to check what features a framework has to support a public one (for example, some analogue of Observer nodes out of the box).
The architecture (client-to-node communication in particular) and scalability is important here. Also the question of deployment is very important.


alexander.shcherbakov
So, I still think that Exonum is a good candidate and worth further researches.

 ;;;","08/Nov/18 3:31 PM;RyanWest;*Relevant comment by Sergey Khoroshavin*
- number of validator nodes - yes, documentation states that number of validators should normally be in 8-15 range (https://exonum.com/doc/advanced/network/), however it's not clear for which case this assumption is made. Exonum uses tendermint protocol under the hood, and there was a thesis from one of major tendermint contributors which included test results of as many as 64 nodes. Probably developers had some specific use case in mind with some performance requirements which prohibited use of more than 15 nodes. Sovrin most likely have very different performance requirements, so number of nodes will be different. I don't see any hard cap here.
- concerning ""you cannot add new validators easily"" - in fact there is whole chapter in documentation on how to change pool configuration including validator set (https://exonum.com/doc/advanced/configuration-updater/), and there is corresponding source code in their repo (https://github.com/exonum/exonum/tree/master/services/configuration);;;",,,,,,,,,,,,,,,,,,,,,,,
Extend Load Script with GET_TXN,INDY-1756,34619,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,dsurnin,ashcherbakov,ashcherbakov,12/Oct/18 5:22 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"As of now, GET_TXN doesn't support State (Audit) Proofs, and requires to be sent to all nodes and waiting for f+1 equal replies.

GET_TXN may be used a lot in some cases, so we need to evaluate max performance the pool can handle with GET_TXN.

*Acceptance criteria*
 * Extend load script to be able to send GET_TXN
 * It needs to be possible to use GET_TXN to get UTXO (the most common case)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1368,,,No,,Unset,No,,,"1|hzzx5z:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,,,,,,,,,,,"08/Nov/18 9:29 PM;dsurnin;Support of get_txn PR
https://github.com/hyperledger/indy-node/pull/1021

It is possible to specify the ledger to send req to and range of seq_no to find.
""ledger"" - id of the ledger, default is ""DOMAIN""
""min_seq_no"" - min seq_no to req, default is 0
""max_seq_no"" - max seq_no to req, default is 10000000
""rand_seq"" - whether to generate seq_no from min to max randomly or sequentially, default is true
full parameter specification is 
-k ""{\""get_txn\"":{\""min_seq_no\"":200,\""max_seq_no\"":300,\""rand_seq\"":false}}"";;;","09/Nov/18 5:09 PM;ashcherbakov;Will be validated in the scope of INDY-1829;;;",,,,,,,,,,,,,,,,,,,,,,,
Need to track same transactions with different multi-signatures,INDY-1757,34621,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,12/Oct/18 6:05 PM,17/May/19 4:54 PM,28/Oct/23 2:47 AM,12/Apr/19 3:38 PM,,1.7.1,,,,0,,,,"As of now transaction's digest is used to uniquely identify the transaction.

Signature (as well as multi-signature) is not part of the digest.
This is not a problem for a common (single) signature, but nay cause issues in case of multi-signature:
 * There is a transaction signed by Trustees 1,2,3, and by Trustees 3,4,5.
 * Both Trustee 1 and Trustee 4 send this txn
 * Since multi-signature is not part of the digest, the pool will not be able to distinguish if this is txn signed by 1,2,3, or by 3,4,5.
 * Some nodes in the pool may select for ordering a txn signed by 1,2,3, and some signed by 3,4,5. Since multi-sig is part of the txn in the Ledger, it will lead to different txn root hashes and different ledgers, so the pool may lost consensus.",,,,,,,,,,,,,,,,,,,,,INDY-1915,,,,,,,,INDY-2055,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2093,,,No,,Unset,No,,,"1|hzwvif:000006r20d",,,,Unset,Unset,Ev-Node 19.07,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,,"03/Apr/19 10:46 PM;Derashe;Taking into account changes, described in PoA of https://jira.hyperledger.org/browse/INDY-1915

***** - places, where payload digest need to be used

We need to consider txn handling in a few txn states:
 * not recieved txn
 ** problem case: client send two txns (*A* and *B*) with same payload_digest but different digest. If we make nodes reject second txn with the same payload digest, that may cause inconsistency (that can happen, because we cannot guarantee which txn will be recieved earlier, so there is a chance, that part of nodes will get txn *A* first, while other nodes will get *B* first. So they could not agree on PrePrepare). Hence, we need to reject it only if it is already ordered (in seq_no_db storage) *****

 * finalized txn
 ** problem, described in previous point, can cause master node to include both of this txns in one batch. To handle this situation, while creating 3pc batch, we must check, that if there is an uncommitted txn with the same payload digest in seq_no_db *OR* in uncommitted txns, we must reject this txn *****. Same kind of check need to be done in processing of PrePrepare
 * applied txn
 ** nodes must consistently agree on txns that will be committed and rejected
 * written txn
 ** no additional checks needed

Using this solution, two main problems can be resolved:
 * different nodes cannot consider same txns with different signatures as one, because txn identified by digest, which contains signatures
 * one txn with modyfied signatures cannot be written to the ledger as a new one, cause we check for payload digest in seq_no_db and uncommitted txns;;;","04/Apr/19 4:54 PM;Derashe;Note: As far, as we use _serialize_msg_for_signing_ to build digest of request, so messages will be deterministically sorted.;;;","05/Apr/19 6:01 PM;Derashe;Problem reason/description:
 - additional validation needed to use txns with multisignature

Changes:
 - validation added

PR:
 - [https://github.com/hyperledger/indy-plenum/pull/1150]

Version:
 - 
h1. 892 dev

Risk factors:
 - additional validation may degrade the performance

Risk:
 - Med

Covered with tests:
 - [https://github.com/hyperledger/indy-plenum/pull/1150/files#diff-3e2e1565d65a1423b14b30a7c460973e]

Recommendations for QA
 - try to send single request twice with different multisignatures, ensure that only one will be written;;;","09/Apr/19 5:06 PM;Derashe;After internal discussion, we've made a decision to operate with request's digests as follows:
 * static validation
 ** if payload_digest in seq_no_db and full_digest in seq_no_db => REPLY
 ** if payload_digest in seq_no_db and full_digest *NOT* in seq_no_db => REQNACK
 * propagate
 ** if payload_digest in seq_no_db => do not process it
 * creation of pre_prepare
 ** if payload_digest in seq_no_db or payload_digest in uncommitted => ignore it
 * processing of pre_prepare
 ** if request not finalized, check by full_digest
 *** if present => suspicious primary
 *** if not present => request for it
 ** if request finalized, do dynamic validation
 *** if payload_digest in seq_no_db or payload_digest in uncommitted => suspicious primary;;;","11/Apr/19 10:09 PM;ashcherbakov;Validation will be continued in the scope of INDY-2046;;;",,,,,,,,,,,,,,,,,,,,
Static Analysis Tools,INDY-1758,34624,,Epic,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,12/Oct/18 8:05 PM,11/Oct/19 10:27 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-6,,Static Analysis Tools,To Do,No,,Unset,No,,,"1|hzzutr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use persisted last_pp_seq_no for recovery of backup primaries,INDY-1759,34625,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,12/Oct/18 8:27 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"As of now, if master primary crashed (stopped) and re-started, it will get last_pp_seq_no from other Nodes and continue correct ordering.

For backup primaries this information is not propagated, so backup primary will start sending PRE-PREPARES from 1, and other nodes will reject it. It may eventually cause a memory leak, since ordering on this backup is stopped for some time.

*Acceptance criteria*
 * Create a new key-value DB for persisting current states
 * Persist last pp_seq_no for each Instance in the pool
 * Use persisted last_pp_seq_no as a new pp_seq_no for a Replica if it's a primary in one of the Instances (including Master?)",,,,,,,,,,,,,,,,,INDY-1779,,,,,,,,,,,,,,,,,INDY-1873,,,,,,,,,,,,,,,"23/Nov/18 12:14 AM;VladimirWork;1759.tar.gz;https://jira.hyperledger.org/secure/attachment/16309/1759.tar.gz","23/Nov/18 12:14 AM;VladimirWork;INDY-1759.PNG;https://jira.hyperledger.org/secure/attachment/16308/INDY-1759.PNG","23/Nov/18 6:08 PM;VladimirWork;INDY-1759_ok_1.PNG;https://jira.hyperledger.org/secure/attachment/16310/INDY-1759_ok_1.PNG","23/Nov/18 6:08 PM;VladimirWork;INDY-1759_ok_2.PNG;https://jira.hyperledger.org/secure/attachment/16311/INDY-1759_ok_2.PNG","23/Nov/18 6:08 PM;VladimirWork;INDY-1759_ok_primaries.PNG;https://jira.hyperledger.org/secure/attachment/16312/INDY-1759_ok_primaries.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwwn:",,,,Unset,Unset,Ev 18.22,Ev 18.23,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,spivachuk,VladimirWork,,,,,,,,,"16/Oct/18 7:43 PM;Derashe;Our goals are:
 * to persist view_no:inst_id:seq_no every time primary replica send pre-prepare (def send3PCBatch).
 ** I think we need to include this code in _def send3PCBatch_ after _sendPrePrepare_ call. 
 * to restore this state.
 ** I think we need to include this code after we finished view_change and catch_up (which calls when node starts) so we dont overwrite restored state. 
 ** Probable place for it is in _def send3PCBatch_ before _create3PCBatch_ call.

Logic for restoring state:
 * If we are trying to send PP with seq_no == 0, we need to check persisted data. We need to look for an info with current view_no. 
 ** if we don't have, than this is first 3pc message in this view and we're ok with it 
 ** if we have, than it means, that we've restarted and we need to restore value (we can set _lastPrePrepareSeqNo_ value to persisted seq_no value)
 * We need to take into account case of restarting of whole pool. In that case valid 3pc key is (0,0). In that case we don't need to restore state and we need to prune persisted data. We can insert this logic in _allLedgersCaughtUp_ function, after we set mode to synced. Because we can send PP only when synced.

 

 

 ;;;","25/Oct/18 6:31 PM;spivachuk;*PoA:*

Since a node can have at most one primary at a time, the node need to store only one entry (or zero if currently there is no primary replica on the node). This entry (triad) must contain primary replica index (inst_id), current view_no and pp_seq_no of the last sent PrePrepare.

The rules for using this triad must be as follows:
 * A primary replica must persist the triad each time after sending PrePrepare to other nodes.
 * The node must clear the persisted triad on completion of a regular (not propagate primary) view change.
 * A backup primary replica must restore {{lastPrePrepareSeqNo}} and adjust {{last_ordered_3pc}} using pp_seq_no from the persisted triad on completion of propagate primary from view 0 by CurrentStates in case the persisted triad exists, this replica index equals to inst_id from the persisted triad and the new view_no equals to view_no from the persisted triad.
 * Additionally, the node must clear the triad on completion of catch-up if master's {{last_caught_up_3PC}} == (0, 0) since it is a convincing indication that the whole pool has been restarted. This should prevent faulty restoration of pp_seq_no in case of the whole pool restart when a node with some backup primary is restarted with some lag after other nodes and so receives CurrentStates instead of not wrapped ViewChangeDones.

If after a node with some backup primary had been stopped, then the rest of the pool was restarted, later made several view changes and the stopped node is started again when view_no from its persisted triad coincides with the current view_no of the pool then pp_seq_no will be restored from the persisted triad while actually it is different view and the other replicas in this instance have not ordered any 3PC-batches (because the primary was absent until now). To workaround this issue we can adjust {{last_ordered_3pc}} on non-primary backup replicas on PrePrepare reception in case the replica haven't ordered any 3PC-batches in this view yet. (Currently we already do a similar adjustment of {{last_ordered_3pc}} on non-primary backup replicas but now we do it on gathering a prepared certificate rather than just on reception of a PrePrepare.);;;","25/Oct/18 7:02 PM;ashcherbakov;WBS:
 # Find out a storage where we save (inst_id, view_no, last_pp_seq_no). Maybe use `idr_cache` for this?
 # Extends `_send3PCBatch_ `: if the Replica is a primary in a backup instance - persist (inst_id, view_no, last_pp_seq_no) in the cache
 # Clear (inst_id, view_no, last_pp_seq_no) at the end of a _regular_ view change _(not a propagate primary)_.
 # When propagate primary to viewNo=X is finished (`on_propagate_primary_done` in Replica): if this is a primary in a backup instance, and we have persisted (inst_id, view_no, last_pp_seq_no) where viewNo=X, then restore `{{lastPrePrepareSeqNo}} ` and `{{last_ordered_3pc}} ` accordingly, or clear persisted info.
 # [Create a separate ticket for this] If backup Repica sees a PrePrepare (A,B) with a gap (let's say (0, 100)), and it hasn't yet ordered anything in this view, then set `{{last_ordered_3pc}} ` to (A, B-1) and accept PrePrepare.

 ;;;","25/Oct/18 8:49 PM;spivachuk;Created INDY-1779 for the last item from the previous comment.;;;","20/Nov/18 11:42 PM;spivachuk;*Changes:*
- Implemented restoration of {{lastPrePrepareSeqNo}} on a backup primary after restart.
- Made some renaming in the code related to view change to make it clearer.
- Corrected {{sdk_send_batches_of_random}} and {{sdk_send_batches_of_random_and_check}} test helper functions.
- Wrote tests for {{lastPrePrepareSeqNo}} restoration feature.
- Corrected existing tests that restarted node instances which had been stopped previously. Now a new node instance is created for restarting a node.
- Made advancing {{last_ordered_3pc}} on a backup replica up to master's {{last_ordered_3pc}} on completion of a regular view change (not propagate primary) conditional: now it is made only if the backup replica is not primary.
- Updated tests in {{test_no_propagate_request_on_different_last_ordered_before_vc}} module according to the change in the logic of advancing {{last_ordered_3pc}} on backup replicas on a regular view change.
- Fixed intermittent failures in tests in {{test_no_propagate_request_on_different_last_ordered_before_vc}} module.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/971
- https://github.com/hyperledger/indy-node/pull/1043

*Version:*
- indy-node 1.6.687-master
- indy-plenum 1.6.596-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- Tests in {{test_last_sent_pp_store_helper}} module
- {{test_backup_primary_restores_pp_seq_no_if_view_is_same}}
- {{test_node_erases_last_sent_pp_key_on_view_change}}
- {{test_node_erases_last_sent_pp_key_on_pool_restart}}
- {{test_node_erases_last_sent_pp_key_on_propagate_primary_after_pool_restart}}

*Recommendations for QA:*

Please test the following scenarios:

*I*
1. Start a pool of 4 nodes.
2. Ensure that the view is 0.
3. Send some write requests.
4. Ensure that the batches are ordered in the instance 1.
5. Restart the node which is the primary in the instance 1.
6. Send 1 write request.
7. Verify that the new batch is ordered in the instance 1.

*II*
1. Start a pool of 4 nodes.
2. Stop the master's primary.
3. Wait for a view change is started and completed.
4. Ensure that the view is 1.
5. Send some write requests.
6. Ensure that the batches are ordered in the instance 1.
7. Restart the node which is the primary in the instance 1.
8. Send 1 write request.
9. Verify that the new batch is ordered in the instance 1.

*III*
1. Start a pool of 7 nodes.
2. Send some write requests.
3. Ensure that the batches are ordered in the instances 1 and 2.
4. Stop the primary of the instance 2.
5. Stop the master's primary.
6. Wait for a view change is started and completed.
7. Start the last stopped node.
8. Start the first stopped node and ensure that it is the primary of the instance 1 now.
9. Send 1 write request.
10. Verify that the new batches are ordered in the instances 1 and 2.

*IV*
1. Start a pool of 4 nodes.
2. Send some write requests.
3. Ensure that the batches are ordered in the instance 1.
4. Stop all the nodes in the pool.
5. Start all the nodes in the pool.
6. Send 1 write request.
7. Verify that the new batch is ordered in the instance 1.;;;","23/Nov/18 12:15 AM;VladimirWork;Build Info:
indy-node 1.6.694

Steps to Reproduce (III):
1. Start a pool of 7 nodes.
2. Send some write requests.
3. Ensure that the batches are ordered in the instances 1 and 2.
4. Stop the primary of the instance 2 (3rd node).
5. Stop the master's primary (1st node).
6. Wait for a view change is started and completed.
7. Start the last stopped node (1st node).
8. Start the first stopped node (3rd node) and ensure that it is the primary of the instance 1 now.
9. Send 1 write request.
10. Verify that the new batches are ordered in the instances 1 and 2.

Actual Results:
There are 3 primaries (0, 1, 2) on the 1st and 3rd node and only 2 primaries (0, 2) on the other 5 nodes.
There are 4 txns ordered in the 2nd instance and only 3 txns ordered in the 1st instance.
 !INDY-1759.PNG|thumbnail! 
 [^1759.tar.gz] ;;;","23/Nov/18 1:09 AM;spivachuk;The behavior described in the previous comment is caused by the replica removal feature. Please re-test Scenario III with the disabled replica removal feature. To disable it, add the following parameters to the config of each node:
{code:java}
REPLICAS_REMOVING_WITH_DEGRADATION = None
REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None
{code};;;","23/Nov/18 6:09 PM;VladimirWork;Build Info:
indy-node 1.6.694

Steps to Validate (III):
0. Set `REPLICAS_REMOVING_WITH_DEGRADATION = None` and `REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None` in the pool config.
1. Start a pool of 7 nodes.
2. Send some write requests.
3. Ensure that the batches are ordered in the instances 1 and 2.
4. Stop the primary of the instance 2 (3rd node).
5. Stop the master's primary (1st node).
6. Wait for a view change is started and completed.
7. Start the last stopped node (1st node).
8. Start the first stopped node (3rd node) and ensure that it is the primary of the instance 1 now.
9. Send 1 write request.
10. Verify that the new batches are ordered in the instances 1 and 2.

Actual Results:
All nodes have the same amount of instances and the same primaries for them. All instances have ordered all requests successfully. !INDY-1759_ok_1.PNG|thumbnail!  !INDY-1759_ok_2.PNG|thumbnail!  !INDY-1759_ok_primaries.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,
View change with large number (JSON parser),INDY-1760,34628,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,esplinr,esplinr,12/Oct/18 10:00 PM,08/Nov/18 7:03 PM,28/Oct/23 2:47 AM,,1.6.73,,plenum,,,0,,,,https://github.com/hyperledger/indy-plenum/issues/940,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzxzr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,esplinr,,,,,,,,,,,"08/Nov/18 7:03 PM;dsurnin;The link from the description was deleted

[~esplinr]
could you please update the description?;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix failure when publishing to GitHub during Sovrin RC creation,INDY-1761,34632,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,12/Oct/18 11:32 PM,17/May/19 4:34 PM,28/Oct/23 2:47 AM,17/May/19 4:34 PM,,,,,,0,devops,,,"If one tries to create a Sovrin RC, then the deb package will be successfully published to RC deb repo for QA validation.
 However, the next step in the pipeline (publishing to GitHub) will fail, since evernym-ci doesn't have permissions in GitHub anymore.
 As a result, the next steps in the pipeline for QA approval and copying RC deb to Stable are not available.
 One will have to copy the deb package from RC to Stable manually for this release.

*Acceptance criteria:*
 * Either find a way to publish to GitHub correctly or remove this step",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw96a",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"17/May/19 4:34 PM;ashcherbakov;Moved to https://sovrin.atlassian.net/browse/ST-570;;;",,,,,,,,,,,,,,,,,,,,,,,,
Change dependency building for upgrade procedure,INDY-1762,34697,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,anikitinDSR,anikitinDSR,16/Oct/18 5:08 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"As of now, we have the next dependency building procedure for sovrin and indy-node packages:
 * DEPS list in node_control_tool source code file
 * PACKAGES_TO_HOLD list also located in node_control_tool source file
 * Calculating deps list for current installed package (upgrade_entry)
 * Using calculating deps and deps from source's code list as a filter for recursive dep list building.

*There is 2 main problems for this approach*:
 * Manually hardcoded list of DEPS in source code. Upgrade procedure is performed by previous version of upgraded package. In this case there is no any way to add new deps into source code list (DEPS list)
 * Deps tree, performed by using calculated dep list as a filter do not taking into account holded packages. Holded packages must be included into deps list

*Possible changes:*
 * Use new version of upgrade_entry's package and build all dependency tree for it. 
 * Get all holded package for current environment
 * Get versions from calculated whole dep list for packages which in holded lists.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1764,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-738,,,No,,Unset,No,,,"1|hzzv9b:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,dsurnin,ozheregelya,,,,,,,,,,"24/Oct/18 6:33 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/987

Now we are going to get all dependencies of the new version of the upgraded packet. This list will be filtered according to system hold list. Final list will be passed to upgrade script.
PACKAGES_TO_HOLD config were moved from src to config file;;;","26/Oct/18 7:16 AM;ozheregelya;*Environment:*
indy-node 1.6.645+

*Steps to Validate:*
1. Setup the pool with old version (analog of current stable version).
2. Perform pool upgrade to 1.6.645 version.
3. Perform pool upgrade to 1.6.646 version.
4. Check results of upgrade.
5. Check hold of packages.

*Actual Results:*
Following cases of upgrade work:
--- forced upgrade
--- not forced upgrade
--- upgrade indy-node to indy-node
--- upgrade indy-node to sovrin
--- upgrade sovrin to sovrin

Following packages are hold:
--- indy-anoncreds
--- indy-node
--- indy-plenum
--- libindy-crypto
--- python3-indy-crypto
--- sovrin

Additional Information:
Some errors appear in journalctl. Ticket for this issue: INDY-1781.;;;",,,,,,,,,,,,,,,,,,,,,,,
Verify Upgrade of the MainNet to the latest IndyNode,INDY-1763,34730,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,17/Oct/18 3:28 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.78,,,,0,,,,"We need to check that Upgrade of the MainNet works correctly to the latest version of IndyNode.

*Acceptance citeria*
 * Check that POOL_UPGRADE of MainNet works from 1.3.62 to 1.6.78 (force=True)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/18 12:39 AM;VladimirWork;info_journals.7z;https://jira.hyperledger.org/secure/attachment/16147/info_journals.7z","18/Oct/18 12:38 AM;VladimirWork;logs.7z;https://jira.hyperledger.org/secure/attachment/16148/logs.7z",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzvg7:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,Toktar,VladimirWork,,,,,,,,,"17/Oct/18 9:44 PM;VladimirWork;Build Info (Docker, 15 nodes):
indy-anoncreds=1.0.11 indy-plenum=1.2.42 indy-node=1.3.62 sovrin=1.1.10 python3-base58=0.2.4 python3-indy-crypto=0.4.1 libindy-crypto=0.4.0

Steps to Validate:
1. Upgrade 1.3.62 (1.1.10) pool to 1.6.78 version simultaneously with force=True.
2. Check all package versions and send all txn types to the ledger.
3. Upgrade 1.6.78 (1.1.10) pool to 1.1.30 version simultaneously with force=True.
4. Check all package versions and send all txn types to the ledger.

Actual Results:
Both upgrades was passed successfully. Pool works normally.;;;","18/Oct/18 12:33 AM;VladimirWork;Build Info (AWS, 25 nodes):
indy-anoncreds=1.0.11 indy-plenum=1.2.42 indy-node=1.3.62 sovrin=1.1.10 python3-base58=0.2.4 python3-indy-crypto=0.4.1 libindy-crypto=0.4.0

Steps to Reproduce:
1. Upgrade 1.3.62 (1.1.10) pool to 1.6.78 version simultaneously with force=True.
2. Check all package versions and send all txn types to the ledger.

Actual Results:
There is PoolLedgerTimeout responce for each client request.;;;","18/Oct/18 8:26 PM;Toktar;In case with pool on the AWS, after upgrade all nodes were connected but lots of nodes don't receive messages that primary sent. There are no problem with backup primaries and backup instanses order transactions correctly.
After restart pool, it works correctly.

Please, re-test it on the AWS.;;;","19/Oct/18 12:20 AM;ozheregelya;Initial case was re-tested and problem was not reproduced.;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade to latest Sovrin doesn't work if it depends on a newer IndyNode,INDY-1764,34732,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ashcherbakov,ashcherbakov,17/Oct/18 3:56 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"If we have installed old Sovrin (not dependent on concrete version of IndyNode) and indy-node version X, and trying to send POOL_UPGRADE to the latest Sovrin depending on indy-node version > X, then upgrade will fail, since sovrin's dependencies are not processed.
 *Example:*
 Have Sovrin 1.1.10, and Indy-Node 1.6.77.
 Trying to Upgrade to sovrin 1.1.30 (depending on 1.6.78)

*Actual behaviour:*

Upgrade fails (see INDY-1748 for details):
{code:java}
sovrin 1.1.10 indy-node 1.6.77 -> sovrin 1.1.30:
Oct 16 15:20:05 a839cb478e79 env[3205]: + deps='libindy-crypto=0.4.5 sovrin=1.1.30'
Oct 16 15:20:05 a839cb478e79 env[3205]: + '[' -z 'libindy-crypto=0.4.5 sovrin=1.1.30' ']'
Oct 16 15:20:05 a839cb478e79 env[3205]: + echo 'Try to donwload indy version libindy-crypto=0.4.5 sovrin=1.1.30'
Oct 16 15:20:05 a839cb478e79 env[3205]: Try to donwload indy version libindy-crypto=0.4.5 sovrin=1.1.30
Oct 16 15:20:05 a839cb478e79 env[3205]: + apt-get -y update
Oct 16 15:20:06 a839cb478e79 env[3205]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 16 15:20:06 a839cb478e79 env[3205]: Hit:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Oct 16 15:20:06 a839cb478e79 env[3205]: Get:3 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Oct 16 15:20:06 a839cb478e79 env[3205]: Get:4 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Oct 16 15:20:07 a839cb478e79 env[3205]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 16 15:20:07 a839cb478e79 env[3205]: Fetched 214 kB in 1s (163 kB/s)
Oct 16 15:20:08 a839cb478e79 env[3205]: Reading package lists...
Oct 16 15:20:08 a839cb478e79 env[3205]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install libindy-crypto=0.4.5 sovrin=1.1.30
Oct 16 15:20:09 a839cb478e79 env[3205]: Reading package lists...
Oct 16 15:20:09 a839cb478e79 env[3205]: Building dependency tree...
Oct 16 15:20:09 a839cb478e79 env[3205]: Reading state information...
Oct 16 15:20:09 a839cb478e79 env[3205]: libindy-crypto is already the newest version (0.4.5).
Oct 16 15:20:09 a839cb478e79 env[3205]: Some packages could not be installed. This may mean that you have
Oct 16 15:20:09 a839cb478e79 env[3205]: requested an impossible situation or if you are using the unstable
Oct 16 15:20:09 a839cb478e79 env[3205]: distribution that some required packages have not yet been created
Oct 16 15:20:09 a839cb478e79 env[3205]: or been moved out of Incoming.
Oct 16 15:20:09 a839cb478e79 env[3205]: The following information may help to resolve the situation:
Oct 16 15:20:09 a839cb478e79 env[3205]: The following packages have unmet dependencies:
Oct 16 15:20:09 a839cb478e79 env[3205]:  sovrin : Depends: indy-node (= 1.6.78) but 1.6.77 is to be installed
Oct 16 15:20:09 a839cb478e79 env[3205]:           Depends: sovtoken (= 0.9.5) but it is not going to be installed
Oct 16 15:20:09 a839cb478e79 env[3205]:           Depends: sovtokenfees (= 0.9.5) but it is not going to be installed
Oct 16 15:20:09 a839cb478e79 env[3205]: E: Unable to correct problems, you have held broken packages.
Oct 16 15:20:09 a839cb478e79 env[3205]: + ret=100
Oct 16 15:20:09 a839cb478e79 env[3205]: + '[' 100 -ne 0 ']'
Oct 16 15:20:09 a839cb478e79 env[3205]: + echo 'Failed to obtain libindy-crypto=0.4.5 sovrin=1.1.30' 
{code}
*Expected behaviour*

Upgrade is successful

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1762,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-738,,,No,,Unset,No,,,"1|hzzvgf:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,esplinr,ozheregelya,,,,,,,,,"17/Oct/18 11:17 PM;esplinr;The workaround is straightforward; issue two upgrade transactions: one to upgrade indy-node and then one to upgrade sovrin.;;;","24/Oct/18 6:34 PM;dsurnin;It should be fixed in scope of [INDY-1762|https://jira.hyperledger.org/browse/INDY-1762]
;;;","26/Oct/18 7:16 AM;ozheregelya;*Environment:*
indy-node 1.6.645+

*Steps to Validate:*
1. Setup the pool with old version (analog of current stable version).
2. Perform pool upgrade to 1.6.645 version.
3. Perform pool upgrade to 1.6.646 version.
4. Check results of upgrade.
5. Check hold of packages.

*Actual Results:*
Following cases of upgrade work:
--- forced upgrade
--- not forced upgrade
--- upgrade indy-node to indy-node
--- upgrade indy-node to sovrin
--- upgrade sovrin to sovrin

Following packages are hold:
--- indy-anoncreds
--- indy-node
--- indy-plenum
--- libindy-crypto
--- python3-indy-crypto
--- sovrin

Additional Information:
Some errors appear in journalctl. Ticket for this issue: INDY-1781.;;;",,,,,,,,,,,,,,,,,,,,,,
RequestQueue in Replica doesn't clear after View Change,INDY-1765,34734,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,Toktar,Toktar,17/Oct/18 6:09 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,TShirt_S,,,"RequestQueue in Replica class doesn't clear in method _gc() after View Change. In case when  one replica ordered less transactions then others but has pre-prepares and prepares for unordered requests, after View Change requests collection will cleared and requestsQueue in a slow replica will not.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/18 11:57 PM;VladimirWork;1765.png;https://jira.hyperledger.org/secure/attachment/16185/1765.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwwf:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"18/Oct/18 10:17 PM;Toktar;Problem reason:
 - RequestQueue in Replica doesn't clear after View Change if one instance was a bit slower than other.

Changes:
 - Added cleaning of requestQueues in Replica in gc() that call in View Change and after checkpoint stabilization.
 - Add test

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/950]
 * [https://github.com/hyperledger/indy-node/pull/989]

Version:
 * indy-node 1.6.639 -master
 * indy-plenum 1.6.568  -master

Risk factors:
 - No risk

Risk:
 - Low

Covered with tests:
 * [test_replica_clear_collections_after_view_change.py|https://github.com/hyperledger/indy-plenum/pull/950/files#diff-2b820e16ae4145eca38c13c980b4f921]

Recommendations for QA:
 * Check in a load test (may be for an other task) with 20 NYMs in second.;;;","26/Oct/18 6:39 PM;VladimirWork;All testing results are in INDY-1682.;;;","29/Oct/18 11:57 PM;VladimirWork;Build Info:
indy-node 1.6.643
indy-perf-load 1.0.16

Steps to Validate:
1. Run writing load test with ~40 NYMs/second without fees.
2. Check metrics, logs and nodes' ledgers.

Actual Results:
RAM consumption at all instances is equal. Size of requests' queues is the same as expected.  !1765.png|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,
Need to add system tests to the indy-node repository,INDY-1766,34767,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Sergey.Kupryushin,Sergey.Kupryushin,18/Oct/18 9:02 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,test-automation,,,0,qa,,,"PR created, but some checks failed

https://github.com/hyperledger/indy-node/pull/988

https://ci.evernym.com/job/Indy-Node/job/indy-node-verify-x86_64/1461/execution/node/17/log/",,,,,,,,,,,,,,,,,INDY-505,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzvnr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Sergey.Kupryushin,VladimirWork,,,,,,,,,,,"23/Oct/18 12:06 AM;VladimirWork;PR fixes are made but github works too bad to check and merge them.;;;","24/Oct/18 11:06 PM;VladimirWork;Test are here: https://github.com/hyperledger/indy-test-automation/tree/master/system;;;",,,,,,,,,,,,,,,,,,,,,,,
"Node mustn't ignore ""pool-restart"" from Trustee due to ""View change in progress""",INDY-1767,34819,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,zhigunenko.dsr,zhigunenko.dsr,22/Oct/18 9:09 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Steps to Validate:*
1) provoke view change on large pool
2) run ""pool-restart"" via CLI

*Actual Results:*
Node ignores restart command if view change in progress

*Expected Results:*
Node restarts itself anyway
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/18 9:18 PM;NataliaDracheva;indy_cli.log;https://jira.hyperledger.org/secure/attachment/16155/indy_cli.log","22/Oct/18 9:23 PM;NataliaDracheva;restart_rejected_because_of_VC.png;https://jira.hyperledger.org/secure/attachment/16156/restart_rejected_because_of_VC.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzvyn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,NataliaDracheva,zhigunenko.dsr,,,,,,,,,,"22/Oct/18 9:21 PM;NataliaDracheva;A few notes:
1. The pool was under a huge load (100 nims/sec) and stopped writing. 
2. In indy-cli I connected to the pool and ran ""ledger pool-restart action=start"" as a Trustee.
3. There are no records about restarting in node journalctl log and restart log.
 ;;;","13/Dec/18 4:59 PM;ashcherbakov;Fixed in the scope of INDY-1896;;;",,,,,,,,,,,,,,,,,,,,,,,
Skip static validation and signature verification for requests that were already received and validated earlier,INDY-1768,34833,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,spivachuk,spivachuk,23/Oct/18 11:29 PM,13/Feb/19 10:04 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,"Now static validation ({{Node.doStaticValidation}}) is always performed for a write request received from a client and is never performed for a write request received from a node in a Propagate or a MessageRep with a Propagate. It is an issue because if a request is received only from nodes in Propagates and / or MessageReps with Propagates then static validation is not done. Static validation must be performed one time for a write request received from a client and all its Propagates received from nodes and this validation must be performed when the node sees the request for the first time.

Also static validation ({{Node.doStaticValidation}}) is not performed and the signature is not verified for a write request received in a MessageRep with a Propagate.

*PoA:*

Now we perform schema validation, static validation ({{Node.doStaticValidation}}) and signature verification when processing incoming messages from ZStack level. This makes it impossible to use {{Propagator.requests}} map for skipping validation of already validated requests. This is so because a node handles ZStack queues and inBox queues sequentially. Thus multiple ZStack-level messages representing the same client request can be processed in one pass while the corresponding typed messages being created are just enqueued to inBox and not processed synchronously. So {{Propagator.requests}} map will not be populated at this step (it is populated when typed messages from inBox are processed). Now, to avoid repeated signature verification, such the workflow makes us to maintain additional map of authenticated requests ({{ReqAuthenticator}} maintains this map).

If we move static validation ({{Node.doStaticValidation}}) and signature verification from the step of processing incoming ZStack-level messages to the step of processing typed inBox-level messages, then we will be able to ensure single-time static validation / signature verification (on the first reception of a request either from a client or as a Propagate from a node) with use of {{Propagator.requests}} map only. No additional maps of validated requests will be needed. By the way, this will fix the issue with a lack of static validation and signature verification for a request received in a MessageRep with a Propagate.

The plan for this fix is as follows:
* Perform only validation against a schema in {{Node.handleOneClientMsg}} and {{Node.handleOneNodeMsg}} methods.
* In {{Node.processClientRequest}}, {{Node.processNodeRequest}} and {{Node.processPropagate}} methods:
*# Perform static validation ({{Node.doStaticValidation}}) and signature verification if in {{Propagator.requests}} there is no ReqState for the request digest or if there is a ReqState for this digest but the request signature in it is different.
*# If both static validation and signature verification succeed (this may happen only in case there is no ReqState for the request digest in {{Propagator.requests}}) then add the ReqState for the request digest to {{Propagator.requests}}.
* Remove the additional map of authenticated requests and its usage from {{ReqAuthenticator}} class.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1753,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwx4f:2rzlr",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),spivachuk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Run load tests with file storages,INDY-1769,34844,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,24/Oct/18 6:08 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"Acceptance Criteria:
 * Adapt code to support file storages for all DBs
 * Run load tests using file storage
 * Analyse memory consumption",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1724,,,,,,,,,,,,,,,,,,,,"31/Oct/18 10:10 PM;sergey.khoroshavin;Screenshot from 2018-10-31 14-02-41.png;https://jira.hyperledger.org/secure/attachment/16191/Screenshot+from+2018-10-31+14-02-41.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwwv:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"26/Oct/18 1:19 AM;sergey.khoroshavin;Test should be done with following options in _indy_config.py_ on some of nodes:
{code:python}
hashStore = {
    ""type"": ""file""
}
stateTsStorage = 5
domainStateStorage = 5
poolStateStorage = 5
configStateStorage = 5
reqIdToTxnStorage = 5
stateSignatureStorage = 5
transactionLogDefaultStorage = 5
configStateStorage = 5
idrCacheStorage = 5
attrStorage = 5
METRICS_KV_STORAGE = 5
{code}

Required load is 1 NYM per second with duration of at least 2 hours.;;;","26/Oct/18 11:18 PM;sergey.khoroshavin;*Minimum verions:*
indy-node: 1.6.647
indy-plenum: 1.6.573;;;","06/Nov/18 11:09 PM;sergey.khoroshavin; !Screenshot from 2018-10-31 14-02-41.png|thumbnail! 
Test was done in docker with load of 3 NYMs per second. It can be seen that with file storage:
* memory consumption is really low (around 100 Mb) compared to similar runs with RocksDB storage 
* memory consumption increases mostly in sync with finalized request queue
* memory usage didn't decrease when request queue was cleared, however it stayed constant until request queue size caught up to previous maximum value

Given all these things it seems safe to assume that:
* there were no unexpected memory consumers in this test
* internal request queue doesn't release memory when cleared, however this memory is reused, so this is not a leak
* *increasing memory consumption during sustainable loads is due to RocksDB internal structures*;;;",,,,,,,,,,,,,,,,,,,,,,
Test ZMQ Memory Consumption with restricted number of client connections,INDY-1770,34852,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,24/Oct/18 8:30 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.

Plenum does (be default) 100 reads in one looper run.

We need to test if situation with memory consumption improves when we restrict the number of client connections.

*Acceptance criteria*
 * Run load test with restricting maximum number of connections
 ** Configure a pool with maximum number of incoming client connections set to 200
 ** set MAX_CONNECTED_CLIENTS_NUM = 100
 ** Run two write only tests: 10 NYMs per sec and 100 NYMs per sec.
 ** Run a test with write and read: 10 write NYMs per sec and 100 read NYMs per sec from 1000 clients
 ** Run a test with write and read: 100 write NYMs per sec and 1000 read NYMs per sec from 1000 clients
 ** Run more experiments if needed
 * Monitor and analyze memory consumption in every case
 * Check how many client requests were processed, and how many rejected because the pool was busy (due to timeout). Do it for the both read and write reqs
 * Decide if we should reduce MAX_CONNECTED_CLIENTS_NUM  in production

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/18 6:47 PM;VladimirWork;1770_100w_1000r_node1.PNG;https://jira.hyperledger.org/secure/attachment/16266/1770_100w_1000r_node1.PNG","14/Nov/18 6:47 PM;VladimirWork;1770_100w_1000r_node13.PNG;https://jira.hyperledger.org/secure/attachment/16267/1770_100w_1000r_node13.PNG","14/Nov/18 6:47 PM;VladimirWork;1770_100w_1000r_node25.PNG;https://jira.hyperledger.org/secure/attachment/16268/1770_100w_1000r_node25.PNG","14/Nov/18 6:18 PM;VladimirWork;1770_100w_node1.PNG;https://jira.hyperledger.org/secure/attachment/16261/1770_100w_node1.PNG","14/Nov/18 6:18 PM;VladimirWork;1770_100w_node25.PNG;https://jira.hyperledger.org/secure/attachment/16262/1770_100w_node25.PNG","14/Nov/18 6:23 PM;VladimirWork;1770_10w_100r_node1.PNG;https://jira.hyperledger.org/secure/attachment/16263/1770_10w_100r_node1.PNG","14/Nov/18 6:23 PM;VladimirWork;1770_10w_100r_node13.PNG;https://jira.hyperledger.org/secure/attachment/16264/1770_10w_100r_node13.PNG","14/Nov/18 6:23 PM;VladimirWork;1770_10w_100r_node25.PNG;https://jira.hyperledger.org/secure/attachment/16265/1770_10w_100r_node25.PNG","14/Nov/18 6:17 PM;VladimirWork;1770_10w_node15.PNG;https://jira.hyperledger.org/secure/attachment/16260/1770_10w_node15.PNG","14/Nov/18 6:17 PM;VladimirWork;1770_10w_node5.PNG;https://jira.hyperledger.org/secure/attachment/16259/1770_10w_node5.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1777,,,No,,Unset,No,,,"1|hzwx7b:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,VladimirWork,,,,,,,,,,"14/Nov/18 6:48 PM;VladimirWork;Test run 1 (10 writes/sec / indy-node 1.6.648): !1770_10w_node5.PNG|thumbnail!  !1770_10w_node15.PNG|thumbnail! 

Test run 2 (100 writes/sec / indy-node 1.6.673): !1770_100w_node1.PNG|thumbnail!  !1770_100w_node25.PNG|thumbnail! 
Logs and metrics: ev@evernymr33:logs/1770_12_11_2018_metrics.tar.gz ev@evernymr33:logs/1770_12_11_2018.tar.gz

Test run 3 (10 writes/sec + 100 reads / indy-node 1.6.673): !1770_10w_100r_node1.PNG|thumbnail!  !1770_10w_100r_node13.PNG|thumbnail!  !1770_10w_100r_node25.PNG|thumbnail! 
Logs and metrics: ev@evernymr33:logs/1770_13_11_2018_10_100.7z

Test run 4 (100 writes/sec + 1000 reads / indy-node 1.6.673): !1770_100w_1000r_node1.PNG|thumbnail!  !1770_100w_1000r_node13.PNG|thumbnail!  !1770_100w_1000r_node25.PNG|thumbnail! 
Logs and metrics: ev@evernymr33:logs/1770_14_11_2018_100_1000.7z
Clients output: ev@evernymr33:logs/client_output_reads_1770_100_1000_case.tar.gz ev@evernymr33:logs/client_output_writes_1770_100_1000_case.tar.gz;;;","15/Nov/18 10:31 PM;anikitinDSR;After small investigation of logs from the last load_test launching (1770_14_11_2018_100_1000.7z) was noticed, that there are a lot of errors, which described in INDY-1849 (2600), that means that all of this batches was discarded. Before giving some recommendation to stewards about connection's limitation i propose, that we should retest this situation when hot fix of INDY-1849 wuold be added into plenum's part.;;;","16/Nov/18 9:53 PM;anikitinDSR;As i can see in client's output (client_output_reads_1770_100_1000_case) we had 3843 rejected requests by ""Client request is discarded since view change is in progress"" reason, 1 request had a ""Pool Ledger timeout"" and other requests (494842) was successfully read and returned a REPLY to client. 

Also about write requests:
It was a 542635 successfully ""write"" requests, 653214 with ErrorCode.PoolLedgerTimeout error and 101378 nacked by ""Client request is discarded since view change"" reason.;;;","22/Nov/18 6:50 PM;VladimirWork;Build Info:
indy-node 1.6.694

Steps to Validate:
1. Set MAX_CONNECTED_CLIENTS_NUM = 200 and 300 connection limit in the iptables.
2. Run production load test (writes + reads).

Actual Results:
22 from 25 nodes have ~283k txns in domain ledger and ~262k txn in token ledger and almost all of 1.5m reads were successful (263 nacked/rejected and 1 failed).
All logs are in ev@evernymr33:logs/1770_22_11_2018.tar.gz;;;","22/Nov/18 6:59 PM;ashcherbakov;Recommendation for Stewards:
 * restrict incoming client connections in firewall (iptables) to *500*
 * set *MAX_CONNECTED_CLIENTS_NUM = 400*;;;","22/Nov/18 7:13 PM;anikitinDSR;After the last load_test launching (with MAX_CONNECTED_CLIENTS_NUM = 200 and 300 connection limit in the iptables), the next results was got:

* For read transactions there was a 1453754 successfull and 264 failed requests.
* For write transactions there was a 539230 successfull and 1104 failed.
After this load_test launches we assume that the most usable limitations from indy-node side is MAX_CONNECTED_CLIENTS_NUM=400 and 500 for iptables.;;;",,,,,,,,,,,,,,,,,,,
As a dev I need to be able to perform tests on docker,INDY-1771,34853,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,andkononykhin,andkononykhin,24/Oct/18 8:31 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,"Currently tests are performed using molecule and its vagrant driver only. It makes sense to have docker based way as a default to speed up development since docker containers are much lighter than vagrant VMs. Also it might be useful for future integration into CI pipelines.

Acceptance criteria:
 * tests can be run on docker containers as a default scenario
 * tests are passed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzw3z:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,,"30/Oct/18 7:03 PM;andkononykhin;Problem reason:
 * pool automation tests used vagrant as a driver for molecule, which is quite slow if we need to re-provision hosts. It would be nice to have docker as alternative (possibly as default one).

Changes:
 * improved bootstrap routine: renamed common role to ansible_bootstrap and leave there only bootstrapping logic
 * added docker as default molecule engine for all configuration roles (ansible_bootstrap, node_install, pool_install)

Committed into:

[https://github.com/hyperledger/indy-node/pull/995]

Risk factors:
 * Nothing is expected.

Risk:
 * Low

 

Covered with tests:
 * existent tests were extended a bit, no new functionality in role to test

Recommendations for QA:
 * do installation steps described in docs
 * for each of the roles *ansible_bootstrap, node_install, pool_install* do *molecule test --all* do:
 ** *molecule destroy && molecule lint && molecule converge && molecule converge && molecule destroy* (yes, converge two times) to test with docker driver
 ** *molecule destroy -s vagrant && molecule lint -s vagrant && molecule converge -s vagrant && molecule converge -s vagrant && molecule destroy -s vagrant* to test with vagrant driver;;;","31/Oct/18 10:28 PM;VladimirWork;Checked against docker.;;;","02/Nov/18 9:27 PM;VladimirWork;Checked against vagrant.;;;","02/Nov/18 9:29 PM;VladimirWork;Steps to Validate:
1. Do installation steps described in docs.
2. For each of the roles ansible_bootstrap, node_install, pool_install do molecule test --all do:
2.1. `molecule destroy && molecule lint && molecule converge && molecule converge && molecule destroy` (yes, converge two times) to test with docker driver.
2.2. `molecule destroy -s vagrant && molecule lint -s vagrant && molecule converge -s vagrant && molecule converge -s vagrant && molecule destroy -s vagrant` to test with vagrant driver.

Actual Results:
All actions were performed without any errors.;;;",,,,,,,,,,,,,,,,,,,,,
Check why backup instances stop ordering so often,INDY-1772,34855,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,ashcherbakov,ashcherbakov,24/Oct/18 10:29 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"A lot of load tests showed that backup instances stop ordering. This, in particular, can lead to out-of-memory issues (since requests queue can not be cleared).

*Acceptance criteria*
 * Find out the reason
 * Create tickets for fixes if needed

*Additional information*:
Logs for the tests mentioned above: ~/logs/indy-1711/success.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1795,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwxb:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,spivachuk,,,,,,,,,,,"26/Oct/18 6:36 PM;spivachuk;In the logs {{indy-1711/success}} the instances 3, 4 and 6 stopped ordering 3PC-batches because their *primaries were restarted and so lost {{lastPrePrepareSeqNo}} counter value and started numbering of pp_seq_nos from 1 again* while other replicas expected continuous numbering of pp_seq_nos. The primaries in the instances 3 (Node4), 4 (Node5) and 6 (Node7) were restarted on 10/19 at 18:10, 18:07 and 18:23 correspondingly. Node4 and Node5 fell due to out-of-memory and so were restarted. As to Node7, there is no systemd journal for that period but most probably the reason of its restart is the same.;;;","30/Oct/18 10:02 PM;spivachuk;In the logs {{indy-1574-new}} *the instance 6 stopped ordering 3PC-batches because its primary - Node8 - was restarted and so lost {{lastPrePrepareSeqNo}} counter value and started numbering of pp_seq_nos from 1 again while other replicas expected continuous numbering of pp_seq_nos.*

*Node8 was restarted because it fell due to out-of-memory on 10/02 at 01:55. This out-of-memory was most likely caused by that the replica Node8:7 stopped ordering 3PC-batches. The last batch which it ordered was (1, 9136). It was on 10/01 at 16:52:10.* However, the primary of the instance 7 - Node9 - did not fall and was not restarted at that period, it did not stopped ordering, it did not report about disconnection from Node8 and Node8 did not report about disconnection from it. There was a pause in receiving Checkpoints by Node8 from Node9 from 16:52:08 till 16:55:41 but after it Node8 continued receiving Checkpoints from Node9 in all the instances. Moreover, there was no gap in incoming Checkpoints from Node9 in any instance. *Most likely Node8:7 did not receive Commits for the batch (1, 9137) from some replicas and so did not gather the quorum of Commits for this batch. Since missing of Commits is not a trigger for requesting missed 3PC-messages, Node8:7 stopped ordering.* However, due to Node8:7 continued to receive Checkpoints, it moved the watermarks on gathering the quorum of messages for each next Checkpoint generation (starting from 2 quorumed generations of stashed checkpoints). *But backup replicas {color:#de350b}do nothing other than moving the watermarks{color} when detecting a lag in Checkpoints. So they cannot resume ordering.* This leads to growth of the queues of request keys and pending 3PC-messages on the replica and growth of request map on the node. This can eventually result in a node fall due to out-of-memory. We see such a fall on Node8 at 01:55.

So ordering in the instance 6, where Node8 was the primary at that moment, was stopped all over the pool. In turn, stop of ordering in an instance lead to growth of memory consumption on all the nodes in the pool and can eventually result in their falls due to out-of-memory (as it is specified in the ticket description).;;;","30/Oct/18 10:59 PM;spivachuk;Created INDY-1795 for adding adjustment of {{last_ordered_3pc}} and performing GC to the logic of handling a lag in checkpoints on a backup replica.;;;",,,,,,,,,,,,,,,,,,,,,,
Find out why validation of PrePrepares with Fees takes so long,INDY-1773,34857,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,24/Oct/18 10:41 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.83,,,,0,,,,"During the recent tests it was found, that dynamic validation of PrePrepares with fees take a lot of time (100 ms vs 2.5 ms), which led to high latency on master and hence a View Change.

We need to check what is the cause of such behaviour.

Logs can be found in s3://qanodelogs/indy-1717/live-23-10-18-production-mix",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzzvgn:",,,,Unset,Unset,Ev 18.21,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,,"25/Oct/18 12:19 AM;anikitinDSR;After log and metrics analizing was noticed, that request processing time (which includes validation and state applying steps) was:

REQUEST_PROCESSING_TIME : 147 samples, 16.79 seconds, 1.04/114.24/603.50 ms min/avg/max

for time from 2018-10-23 17:01:00 to 2018-10-23 17:01:20. 

All requests was written to ledger 2 and 1001. Therefore need to investigate validation process of token's related requests.;;;","25/Oct/18 12:29 AM;ashcherbakov;Created [https://evernym.atlassian.net/browse/TOK-486] for this.;;;",,,,,,,,,,,,,,,,,,,,,,,
Do a long test with a load pool can handle,INDY-1774,34874,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,25/Oct/18 3:48 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"We can see that memory is slightly increasing during a load that pool can handle.
 We suspect that there can be a limit where it's cut off, but in order to prove it we need to run a long test.

Increasing memory consumption may be caused by RocksDB (see comments in INDY-1724).

*Acceptance Criteria:*
 * Run load script with 1-10 NYMs per second for 2-3 days.
 * Check graphs for memory consumption

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1724,,,,,INDY-1821,,,,,,,,,,,,,,,"29/Oct/18 8:39 PM;VladimirWork;Figure_node15_ram8.png;https://jira.hyperledger.org/secure/attachment/16183/Figure_node15_ram8.png","29/Oct/18 8:39 PM;VladimirWork;Figure_node20_ram4.png;https://jira.hyperledger.org/secure/attachment/16184/Figure_node20_ram4.png","29/Oct/18 8:39 PM;VladimirWork;Figure_node5_ram8.png;https://jira.hyperledger.org/secure/attachment/16182/Figure_node5_ram8.png","06/Nov/18 9:01 PM;sergey.khoroshavin;Screenshot from 2018-11-06 14-18-03.png;https://jira.hyperledger.org/secure/attachment/16223/Screenshot+from+2018-11-06+14-18-03.png","06/Nov/18 7:15 PM;VladimirWork;node15.png;https://jira.hyperledger.org/secure/attachment/16222/node15.png","06/Nov/18 7:15 PM;VladimirWork;node5.png;https://jira.hyperledger.org/secure/attachment/16221/node5.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwxj:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,,"26/Oct/18 1:35 AM;sergey.khoroshavin;It would be great to reduce RAM on some (3 is enough) nodes to 4 Gb to see whether it affects memory consumption. Their IDs should be in 15-25 range.

Also this test can be combined with INDY-1769, but in this case number of nodes with file storages should be really small (3 is enough) and their IDs should be in 15-25 range. This requirement is due to fast performance degradation of file storages with increasing size of ledger, so these nodes shouldn't be primaries and their number should be less than _f_ so they won't affect performance of other nodes. Nodes with file storages shouldn't overlap with nodes with reduced RAM.

Also during previous tests it was seen that there is almost no difference in memory consumption between 1 NYM/sec and 10 NYMs/sec, so if this test is really combined with INDY-1769 it make sense to run with just 1 NYM/sec so that performance degradation of file storage won't become apparent too fast.;;;","26/Oct/18 6:32 PM;ashcherbakov;In order to efficiently run load, we can combine multiple checks and run the following test for weekends:

Prepare Pool:
 * 25 Nodes
 * 4 GB RAM on Nodes 20, 21 and 22
 * 8 GB RAM on other Nodes

Configure Indy-Node:
 * Install Indy-Node 1.6.678
 * Enable metrics
 * Use file Storage (see INDY-1769) on Nodes 23 and 24

Run load test:
 * 1 NYM per second;;;","27/Oct/18 12:39 AM;VladimirWork;Load test has been started at 18:40 10/26 and stopped at 12:00 10/29.;;;","29/Oct/18 9:08 PM;VladimirWork;Actual Results:
Pool has written 230k txns successfully and was in consensus at the end of load test. 23 and 24 nodes don't order any txn but catch up them from other nodes.
 !Figure_node5_ram8.png|thumbnail!  !Figure_node15_ram8.png|thumbnail!  !Figure_node20_ram4.png|thumbnail! 

All logs and journals are in ev@evernymr33:logs/1774_1.tar.gz;;;","03/Nov/18 12:17 AM;VladimirWork;Load {noformat}nohup perf_processes.py -g live_transactions_genesis -c 10 -n 1 -l 10 -k nym -y one -b 100 &{noformat} has been started at 18:15 11/02 and stopped at 10:15 11/06.;;;","06/Nov/18 7:19 PM;VladimirWork;Actual Results:
Pool has written 3.15m txns successfully and was in consensus at the end of load test. 
 !node5.png|thumbnail!  !node15.png|thumbnail! 

All logs and journals are in ev@evernymr33:logs/1774_2.tar.gz
Metrics are in metrics_1774_2.7z;;;","06/Nov/18 9:01 PM;sergey.khoroshavin;Load 
{code}
perf_processes.py -g pool_transactions_genesis -m t -n 1 -c 1 -l 10 -y one -k nym
{code}
was run against 4-node docker pool for 6 days.
 !Screenshot from 2018-11-06 14-18-03.png|thumbnail! ;;;",,,,,,,,,,,,,,,,,,
Get information about how many client connections is usually in progress,INDY-1775,34876,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,ashcherbakov,ashcherbakov,25/Oct/18 6:06 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.

Plenum does (be default) 100 reads in one looper run.

We need to test if situation with memory consumption improves when we restrict the number of client connections.

*Acceptance criteria*
 * Get information about how many client connections is usually in progress:
 ** Include metrics to track current number of open client connections (a PR)
 ** Run two tests with metrics enabled: 10 NYMs per sec and 100 NYMs per sec. Compare results and how many connections are used.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1777,,,No,,Unset,No,,,"1|hzwwxz:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,,,,,,,,,,,"02/Nov/18 10:28 PM;sergey-shilov;*Results*

10 NYMs/sec: ~10 simultaneous connections
100 NYMs/sec: ~1300 simultaneous connections, then client stack is restarted;;;",,,,,,,,,,,,,,,,,,,,,,,,
Test ZMQ Memory Consumption with restarting of listener on every X connections,INDY-1776,34877,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,ashcherbakov,ashcherbakov,25/Oct/18 6:07 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.

Plenum does (be default) 100 reads in one looper run.

We need to test if situation with memory consumption improves when we restrict the number of client connections.

*Acceptance criteria*
 * Run test with restarting of listener on every 1000 connections
 ** Run Python tests (created in the scope of INDY-1686) restarting listener 
 ** Run C tests (created in the scope of INDY-1686) restarting listener
 ** Run Indy load tests
 *** set MAX_CONNECTED_CLIENTS_NUM = 100
 *** Run two tests: 10 NYMs per sec and 100 NYMs per sec
 ** Compare results",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/18 12:02 AM;sergey-shilov;100_nyms_sec_without_iptables.png;https://jira.hyperledger.org/secure/attachment/16228/100_nyms_sec_without_iptables.png","07/Nov/18 12:02 AM;sergey-shilov;10_nyms_sec_without_iptables.png;https://jira.hyperledger.org/secure/attachment/16227/10_nyms_sec_without_iptables.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1777,,,No,,Unset,No,,,"1|hzwwxr:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,,,,,,,,,,,"29/Oct/18 4:54 PM;sergey-shilov;We did the test with listener recreation in case of exceeded memory usage threshold.

*Test set up*

We used ZMQ-based sample server written in Python. As an RSS threshold we specified 500MB, overall clients throughput exceeded server's throughput (40 reqs/seq vs 25 reqs/sec).
The listener socket is recreated when RSS threshold exceeded, but not often than once per 60 sec.

*Results*

When the RSS threshold was reached the listener socket was recreated the first time. Memory was not released. All further test under load (which lasted about three days) the listener socket was recreated each 60 seconds due to reached threshold, memory was not released but did not grow at the same time and was pretty stable: ~500MB (RSS) and ~1GB (VSZ).

*Conclusion*

The test showed that there is no sense to recreate listener socket to solve memory problems as such strategy leads to cyclic listener restart without memory releasing. We already have the logic to restart listener stack to avoid file descriptors exhaustion, but seems like this is not the solution for memory issues.

Also I think that this test covers the first described test in Acceptance criteria options (Python test with restart on each 1000 connections), but instead of connections the threshold is set for memory usage that makes the test more reasonable.;;;","01/Nov/18 1:29 AM;sergey-shilov;*Test set up*

The same test was done for the equivalent implementation in C. Test set up is the same as described by the comment (the difference is that ZMQ-based sample server written in C):
https://jira.hyperledger.org/browse/INDY-1776?focusedCommentId=52725&page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-52725

*Results*

Each time when 500MB RSS was reached the listener socket was recreated. Each time this event occurred the RSS fault down to 180-300 MB.

*Conclusion*

Described two tests with Python-based and C-based server implementations shown very different results. Seems like the solution with restarting of listener socket based on RSS threshold would be very good for C implementation where there is no any additional memory management. But since we have Python-based implementation this solution is useless as RSS doesn't go down under threshold and we always have the ""red flag"" once the threshold is reached.;;;","03/Nov/18 1:11 AM;sergey-shilov;We ran two indy load tests: 10 NYMs per sec and 100 NYMs per sec (MAX_CONNECTED_CLIENTS_NUM = 100, without iptables).

*Results*
 * *10 NYMs per sec:* we did not reach connections limit and all transactions were written
 * *100 NYMs per sec:* connections limit was reached, the client stack was restarted several times, memory grew and the node process was killed by OOM killer. During whole test we read maximum allowed size of batches per looper iteration from node-to-node queues, so it seems like so high memory consumption is caused by very big node-to-node traffic;;;","07/Nov/18 12:03 AM;sergey-shilov;*Pictures for 10 NYMs/sec and 100 NYMs/sec*
 !10_nyms_sec_without_iptables.png|thumbnail!  !100_nyms_sec_without_iptables.png|thumbnail! ;;;","07/Nov/18 12:22 AM;sergey-shilov;*Conclusion*

Our investigation shows that client stack restart without limitation of number of client connections by firewall (iptables) does not help to avoid OOM during DDoS scenarios like 100 NYMs/sec.
High load with restricted number of clients connections using iptables will be done in scope of INDY-1770.;;;",,,,,,,,,,,,,,,,,,,,
ZMQ Memory Consumption ,INDY-1777,34878,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,25/Oct/18 6:09 PM,09/Oct/19 12:42 AM,28/Oct/23 2:47 AM,25/Jul/19 5:59 AM,,,,,,0,,,,"As was understood in INDY-1686, ZMQ may consume a lot of memory if there are a lot of simultaneous client requests, since it uses round-robin to read client queues.

Plenum does (be default) 100 reads in one looper run.

We need to test if situation with memory consumption improves when we restrict the number of client connections.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-13,,ZMQ Memory Consumption ,Done,No,,Unset,No,,,"1|hzzw9z:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Research R3 Corda,INDY-1778,34881,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,25/Oct/18 6:28 PM,09/Sep/19 10:07 PM,28/Oct/23 2:47 AM,,,2.0,,,,1,,,,"Evaluate if R3 Corda with BFT consensus (BFT Smart) could be made into a viable replacement for Indy.
 Timebox the effort to 3 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** The state of the project's community
 ** The current roadmap
 ** Rough estimate of work required to bring R3 Corda-based solution to the same level as Plenum
 ** A recommendation on whether to research more or kill the proposal
 ** Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path

There is already a couple of PoC of integration Indy SDk with Corda-based pool (see, for example, [https://github.com/Luxoft/cordentity])
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwx4f:2rzmf",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,remmeier,sergey.khoroshavin,,,,,,,,,,"31/Oct/18 11:02 PM;sergey.khoroshavin;Brief research showed that:
* Corda architecture (and purpose) looks a bit like Fabric
** private blockchain
** more focused on data ownership than consensus
* BFT consensus is optional (implemented as [notary|https://github.com/corda/corda/blob/master/docs/source/key-concepts-notaries.rst] nodes)
* all notary implementations (both CFT and BFT) are currently [experimental|https://github.com/corda/corda/tree/master/experimental];;;",,,,,,,,,,,,,,,,,,,,,,,,
Adjust last_ordered_3pc on non-primary backup replica upon reception of PrePrepare if there are no ordered batches in this view,INDY-1779,34882,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,spivachuk,spivachuk,25/Oct/18 8:48 PM,25/Oct/19 9:09 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Adjust {{last_ordered_3pc}} and update the watermarks on a non-primary backup replica upon reception of PrePrepare if the replica has not ordered any 3PC-batches in this view yet. This will be a workaround for a case of faulty restoration of pp_seq_no on the backup's primary when view_no from the persisted triad (inst_id, view_no, pp_seq_no) coincides with view_no of the current pool view but actually the latter is a different view, not the view which the persisted triad relates to. (See comments to INDY-1759 for details.)

Currently we already do a similar adjustment of {{last_ordered_3pc}} on a non-primary backup replica but we do it on gathering a prepared certificate rather than just on reception of a PrePrepare. This was implemented for proceeding ordering on non-primary backup replicas after a catch-up. Loosing the condition for {{last_ordered_3pc}} adjustment will not break ability of a non-primary backup replica to proceed ordering after a catch-up.",,,,,,,,,,INDY-1759,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2251,,,No,,Unset,No,,,"1|i0125u:i",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),spivachuk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clear Requests queue periodically,INDY-1780,34886,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,ashcherbakov,ashcherbakov,25/Oct/18 11:35 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"As of now, there is a shared Requests queue for all Replica. A request is removed from the queue only when it's ordered on all Replicas.

So, if at least one Replica doesn't order a request, it will stay there forever.

There are a couple of strategies implemented that fixed it in most of the cases (see for example INDY-1684 and INDY-1759). So, if  one of the Backup Instances doesn't order, or order too slow, we can detect it and remove the replica (allowing requests to be cleared).

However, there are cases where some requests may still be not cleared (until view change happens):

1) Request queue is different on nodes, so a Node has requests which are not present (by some reason) on one of the primaries, so it's never ordered on this instance and hence never removed from the queue.

2) A malicious primary on one of the backup instances doesn't order some of the request (in a way that backup instance performance is not changed significantly, otherwise it will be detected by others because of the strategy from INDY-1684). So, some requests will not be cleared on all nodes.

 

*Acceptance criteria*
 * Define a strategy of removing outdated requests from the queue periodically.
 Possible options:
 ** Assign a timestamp for each request when it goes to the queue. Schedule a timer (let's say every X minutes) which will clear all requests staying in the queue for more than X minutes
 ** Clear all requests for a stable checkpoint on master (once it's ordered by master some time ago, it's unlikely that it's still not ordered by backups; so this rather means that backups doesn't order or order maliciously)
 * It may make sense to implement it as a strategy disabled by default in config, and enabled once it's validated by QA

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwwx3:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey-shilov,,,,,,,,,,,"30/Oct/18 10:15 PM;sergey-shilov;*Plan of Attack*

Items of Requests queue are wrapped requests objects with additional meta data (class _ReqState_). Such item is created during adding of request to Requests queue. We can extend this meta data with _created_ts_ field.
Also there should be a periodic task that clears all requests staying in the queue for more than X minutes (configurable) by comparison of _created_ts_ field with current time stamp.
As a second step we can implement clearing of all requests for a stable checkpoint on master, but it's discussable.

All these actions may be enabled or disabled using the node's configuration.;;;","31/Oct/18 12:30 AM;sergey-shilov;For now the main question is the phase of request processing when deletion of request object does not lead to significant side effects, i.e. traffic amplification (lots of message requests), losing of seqno that leads to stashing of everything and so on.

Another one option is a limit for Requests queue size. But this solution should also be reviewed for side effects.;;;","01/Nov/18 12:47 AM;sergey-shilov;As a result of the several discussions about the solution we plan to:
* add the first time out for the request's propagates quorum phase that starts when request is added to the Requests queue, when the time out is spent the request is dropped from the Requests queue. As such request is not forwarded to the replicas then the dropping of the request is safe operation on this phase.
* add the second time out for the request's ordering phase (pre-prepare, prepare, commit), when the time out is spent the request is dropped from the Requests queue. This operation is not safe as the KeyError exception may occur during request handling in replicas, so modification of the code base is needed to handle such situation. Some critical points when the second time out is spent and the request is dropped:
** Pre-prepare:
*** Sending by the primary: pre-prepare is not sent as request was dropped
*** Receiving by the non-primary: original request is re-asked from other nodes using the MessageRequest (current implementation)
** Prepare/commit/ordered: original request is not needed for the ledger as it is applied to uncommitted state on the pre-prepare stage, it is needed just to send out the reply to the client, so it is enough to handle the KeyError gracefully or to tell the replicas that request does not exist any more when request is dropped (implementation is needed)

Both described time outs are supposed to be very long (hours), we can treat this rather as some rear ""sanity check"" then some regular process.;;;","09/Nov/18 9:27 PM;sergey-shilov;Draft implementation of dropping of outdated requests is done as an optional strategy that is disabled by default.
https://github.com/hyperledger/indy-plenum/pull/973
Further implementation and investigation of possible side affects will be done in scope of INDY-1836, so this ticket may be closed.;;;",,,,,,,,,,,,,,,,,,,,,
Upgrade from 1.6.645+ version result errors about packages versions in journalctl,INDY-1781,34897,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ozheregelya,ozheregelya,26/Oct/18 7:02 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"*Steps to Reproduce:*
1. Set up docker pool with 1.6.645+ indy-node.
2. Run ledger pool-upgrade command (doesn't matter if it will be for sovrin or for indy-node).

*Actual Results:*
Following errors appear in journalctl:
{code:java}
Oct 25 17:44:17 f56585ade2de systemd[1]: Started Service for upgrade of existing Indy Node and other operations.
Oct 25 17:50:03 f56585ade2de systemd[1]: Starting Cleanup of Temporary Directories...
Oct 25 17:50:03 f56585ade2de env[1616]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Oct 25 17:50:03 f56585ade2de systemd[1]: Started Cleanup of Temporary Directories.
Oct 25 17:51:39 f56585ade2de env[1616]: E: Version '3.6' for 'python3' was not found
Oct 25 17:51:39 f56585ade2de env[1616]: E: No packages found
Oct 25 17:51:50 f56585ade2de env[1616]: E: Version '3.5~' for 'python3' was not found
Oct 25 17:51:50 f56585ade2de env[1616]: E: No packages found
Oct 25 17:52:02 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 17:52:02 f56585ade2de env[1616]: E: No packages found
Oct 25 17:52:38 f56585ade2de env[1616]: E: Version '1.9.0' for 'python3-six' was not found
Oct 25 17:52:38 f56585ade2de env[1616]: E: No packages found
Oct 25 17:53:03 f56585ade2de env[1616]: E: Version '3.3.2-2~' for 'python3' was not found
Oct 25 17:53:03 f56585ade2de env[1616]: E: No packages found
Oct 25 17:53:15 f56585ade2de env[1616]: E: Version '3.3.2-2~' for 'python3' was not found
Oct 25 17:53:15 f56585ade2de env[1616]: E: No packages found
Oct 25 17:54:13 f56585ade2de env[1616]: E: Version '3.6' for 'python3' was not found
Oct 25 17:54:13 f56585ade2de env[1616]: E: No packages found
Oct 25 17:54:24 f56585ade2de env[1616]: E: Version '3.5~' for 'python3' was not found
Oct 25 17:54:24 f56585ade2de env[1616]: E: No packages found
Oct 25 17:54:37 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 17:54:37 f56585ade2de env[1616]: E: No packages found
Oct 25 17:54:49 f56585ade2de env[1616]: E: Version '1:3.0' for 'libgcc1' was not found
Oct 25 17:54:49 f56585ade2de env[1616]: E: No packages found
Oct 25 17:55:13 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 17:55:13 f56585ade2de env[1616]: E: No packages found
Oct 25 17:55:24 f56585ade2de env[1616]: E: Version '1:4.1.1' for 'libgcc1' was not found
Oct 25 17:55:24 f56585ade2de env[1616]: E: No packages found
Oct 25 17:55:48 f56585ade2de env[1616]: E: Version '5.2' for 'libstdc++6' was not found
Oct 25 17:55:48 f56585ade2de env[1616]: E: No packages found
Oct 25 17:56:00 f56585ade2de env[1616]: E: Version '5.2' for 'libstdc++6' was not found
Oct 25 17:56:00 f56585ade2de env[1616]: E: No packages found
Oct 25 17:57:32 f56585ade2de env[1616]: E: Version '10.0.0' for 'python3-pip' was not found
Oct 25 17:57:32 f56585ade2de env[1616]: E: No packages found
Oct 25 17:59:13 f56585ade2de env[1616]: E: Version '3.3.2-2~' for 'python3' was not found
Oct 25 17:59:13 f56585ade2de env[1616]: E: No packages found
Oct 25 17:59:38 f56585ade2de env[1616]: E: Version '3.6' for 'python3' was not found
Oct 25 17:59:38 f56585ade2de env[1616]: E: No packages found
Oct 25 17:59:50 f56585ade2de env[1616]: E: Version '3.5~' for 'python3' was not found
Oct 25 17:59:50 f56585ade2de env[1616]: E: No packages found
Oct 25 18:00:03 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:00:03 f56585ade2de env[1616]: E: No packages found
Oct 25 18:01:08 f56585ade2de env[1616]: E: Version '25' for 'python3-setuptools' was not found
Oct 25 18:01:08 f56585ade2de env[1616]: E: No packages found
Oct 25 18:01:32 f56585ade2de env[1616]: E: Version '1.5' for 'python3-six' was not found
Oct 25 18:01:32 f56585ade2de env[1616]: E: No packages found
Oct 25 18:02:57 f56585ade2de env[1616]: E: Version '3.3.2-2~' for 'python3' was not found
Oct 25 18:02:57 f56585ade2de env[1616]: E: No packages found
Oct 25 18:03:12 f56585ade2de env[1616]: E: Version '1.5.5' for 'python3-pyparsing' was not found
Oct 25 18:03:12 f56585ade2de env[1616]: E: No packages found
Oct 25 18:03:40 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:03:40 f56585ade2de env[1616]: E: No packages found
Oct 25 18:04:22 f56585ade2de env[1616]: E: Version '2.4' for 'libc6' was not found
Oct 25 18:04:22 f56585ade2de env[1616]: E: No packages found
Oct 25 18:04:33 f56585ade2de env[1616]: E: Version '3.6' for 'python3' was not found
Oct 25 18:04:33 f56585ade2de env[1616]: E: No packages found
Oct 25 18:04:48 f56585ade2de env[1616]: E: Version '3.5~' for 'python3' was not found
Oct 25 18:04:48 f56585ade2de env[1616]: E: No packages found
Oct 25 18:05:48 f56585ade2de env[1616]: E: Version '1.0.1-11' for 'libpam-runtime' was not found
Oct 25 18:05:48 f56585ade2de env[1616]: E: No packages found
Oct 25 18:06:00 f56585ade2de env[1616]: E: Version '3.2-14' for 'lsb-base' was not found
Oct 25 18:06:00 f56585ade2de env[1616]: E: No packages found
Oct 25 18:06:12 f56585ade2de env[1616]: E: Version '1.18~' for 'init-system-helpers' was not found
Oct 25 18:06:12 f56585ade2de env[1616]: E: No packages found
Oct 25 18:06:24 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:06:24 f56585ade2de env[1616]: E: No packages found
Oct 25 18:06:36 f56585ade2de env[1616]: E: Version '0.99.7.1' for 'libpam0g' was not found
Oct 25 18:06:36 f56585ade2de env[1616]: E: No packages found
Oct 25 18:06:48 f56585ade2de env[1616]: E: Version '2.1.9' for 'libselinux1' was not found
Oct 25 18:06:48 f56585ade2de env[1616]: E: No packages found
Oct 25 18:07:25 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:07:25 f56585ade2de env[1616]: E: No packages found
Oct 25 18:07:38 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:07:38 f56585ade2de env[1616]: E: No packages found
Oct 25 18:08:03 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:08:03 f56585ade2de env[1616]: E: No packages found
Oct 25 18:08:28 f56585ade2de env[1616]: E: Version '2.14' for 'libc6' was not found
Oct 25 18:08:28 f56585ade2de env[1616]: E: No packages found
Oct 25 18:08:41 f56585ade2de env[1616]: + deps='libsodium18 iptables at indy-anoncreds=1.0.32 libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-plenum=1.6.571 indy-node=1.6.646 sovrin=1.1.82'
Oct 25 18:08:41 f56585ade2de env[1616]: + '[' -z 'libsodium18 iptables at indy-anoncreds=1.0.32 libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-plenum=1.6.571 indy-node=1.6.646 sovrin=1.1.82' ']'
Oct 25 18:08:41 f56585ade2de env[1616]: + echo 'Try to donwload indy version libsodium18 iptables at indy-anoncreds=1.0.32 libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-plenum=1.6.571 indy-node=1.6.646 sovrin=1.1.82'
Oct 25 18:08:41 f56585ade2de env[1616]: Try to donwload indy version libsodium18 iptables at indy-anoncreds=1.0.32 libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-plenum=1.6.571 indy-node=1.6.646 sovrin=1.1.82
Oct 25 18:08:41 f56585ade2de env[1616]: + apt-get -y update
Oct 25 18:08:42 f56585ade2de env[1616]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Oct 25 18:08:42 f56585ade2de env[1616]: Get:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Oct 25 18:08:42 f56585ade2de env[1616]: Get:3 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Oct 25 18:08:42 f56585ade2de env[1616]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Oct 25 18:08:44 f56585ade2de env[1616]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Oct 25 18:08:44 f56585ade2de env[1616]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Oct 25 18:08:50 f56585ade2de env[1616]: Fetched 323 kB in 8s (36.2 kB/s)
Oct 25 18:09:03 f56585ade2de env[1616]: Reading package lists...
Oct 25 18:09:03 f56585ade2de env[1616]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install libsodium18 iptables at indy-anoncreds=1.0.32 libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-plenum=1.6.571
Oct 25 18:09:15 f56585ade2de env[1616]: Reading package lists...
Oct 25 18:09:18 f56585ade2de env[1616]: Building dependency tree...
Oct 25 18:09:18 f56585ade2de env[1616]: Reading state information...
Oct 25 18:09:20 f56585ade2de env[1616]: at is already the newest version (3.1.18-2ubuntu1).
Oct 25 18:09:20 f56585ade2de env[1616]: iptables is already the newest version (1.6.0-2ubuntu3).
Oct 25 18:09:20 f56585ade2de env[1616]: libsodium18 is already the newest version (1.0.8-5).
Oct 25 18:09:20 f56585ade2de env[1616]: indy-anoncreds is already the newest version (1.0.32).
Oct 25 18:09:20 f56585ade2de env[1616]: indy-node is already the newest version (1.6.646).
Oct 25 18:09:20 f56585ade2de env[1616]: indy-plenum is already the newest version (1.6.571).
Oct 25 18:09:20 f56585ade2de env[1616]: libindy-crypto is already the newest version (0.4.5).
Oct 25 18:09:20 f56585ade2de env[1616]: python3-indy-crypto is already the newest version (0.4.5).
Oct 25 18:09:20 f56585ade2de env[1616]: The following held packages will be changed:
Oct 25 18:09:21 f56585ade2de env[1616]:   sovrin
Oct 25 18:09:21 f56585ade2de env[1616]: The following packages will be upgraded:
Oct 25 18:09:21 f56585ade2de env[1616]:   sovrin
Oct 25 18:09:23 f56585ade2de env[1616]: 1 upgraded, 0 newly installed, 0 to remove and 11 not upgraded.
Oct 25 18:09:23 f56585ade2de env[1616]: Need to get 10.4 kB of archives.
Oct 25 18:09:23 f56585ade2de env[1616]: After this operation, 0 B of additional disk space will be used.
Oct 25 18:09:23 f56585ade2de env[1616]: Get:1 https://repo.sovrin.org/deb xenial/master amd64 sovrin amd64 1.1.82 [10.4 kB]
Oct 25 18:09:23 f56585ade2de env[1616]: Fetched 10.4 kB in 1s (5805 B/s)
Oct 25 18:09:23 f56585ade2de env[1616]: Download complete and in download only mode
Oct 25 18:09:23 f56585ade2de env[1616]: + ret=0
Oct 25 18:09:23 f56585ade2de env[1616]: + '[' 0 -ne 0 ']'
Oct 25 18:10:28 f56585ade2de env[1616]: + '[' '' = '' ']'
Oct 25 18:10:28 f56585ade2de env[1616]: + systemctl daemon-reload
Oct 25 18:10:28 f56585ade2de systemd[1]: Reloading.
Oct 25 18:10:28 f56585ade2de env[1616]: + systemctl reset-failed
Oct 25 18:10:28 f56585ade2de env[1616]: + echo 'Starting indy-node'
Oct 25 18:10:28 f56585ade2de env[1616]: Starting indy-node
Oct 25 18:10:28 f56585ade2de env[1616]: + systemctl start indy-node
Oct 25 18:10:28 f56585ade2de systemd[1]: Started Indy Node.
Oct 25 18:10:28 f56585ade2de env[1616]: + echo 'Restarting agent'
Oct 25 18:10:28 f56585ade2de env[1616]: Restarting agent
Oct 25 18:10:28 f56585ade2de env[1616]: + systemctl restart indy-node-control
Oct 25 18:10:28 f56585ade2de systemd[1]: Stopping Indy Node...
Oct 25 18:10:28 f56585ade2de systemd[1]: Stopping Service for upgrade of existing Indy Node and other operations...
Oct 25 18:10:28 f56585ade2de systemd[1]: Stopped Service for upgrade of existing Indy Node and other operations.
Oct 25 18:10:28 f56585ade2de systemd[1]: Stopped Indy Node.
Oct 25 18:10:29 f56585ade2de systemd[1]: Started Indy Node.
Oct 25 18:10:29 f56585ade2de systemd[1]: Started Service for upgrade of existing Indy Node and other operations.
Oct 25 18:49:16 f56585ade2de kernel: e1000: enp0s3 NIC Link is Down
Oct 25 18:49:20 f56585ade2de kernel: e1000: enp0s3 NIC Link is Up 1000 Mbps Full Duplex, Flow Control: RX
Oct 25 20:38:00 f56585ade2de kernel: 29:38:00.239382 timesync vgsvcTimeSyncWorker: Radical host time change: 6 549 453 000 000ns (HostNow=1 540 499 880 865 000 000 ns HostLast=1 540 493 331 412 000 000 ns)
Oct 25 20:38:00 f56585ade2de systemd[1]: Time has been changed
Oct 25 20:38:10 f56585ade2de kernel: 29:38:10.241236 timesync vgsvcTimeSyncWorker: Radical guest time change: 6 527 607 954 000ns (GuestNow=1 540 499 890 872 181 000 ns GuestLast=1 540 493 363 264 227 000 ns fSetTimeLastLoop=true ){code}
Upgrade happens later than it was scheduled.

*Expected Results:*
Upgrade should not result errors.",indy-node 1.6.645+,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzw47:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,ozheregelya,,,,,,,,,,"26/Oct/18 6:56 PM;ashcherbakov;[~ozheregelya] [~dsurnin] What Python version is used? It looks like it uses Python 3.6 instead of 3.5?

 ;;;","26/Oct/18 11:27 PM;dsurnin;Those error messages are printed by apt-cache during the dependency tree building.
For example
* indy-node=1.6.647 depends on
indy-plenum=1.6.573, indy-anoncreds (= 1.0.32), python3-dateutil, python3-timeout-decorator (= 0.4.0), at, iptables, libsodium18

* indy-plenum=1.6.573 depends on
python3-ujson=1.33-1build1, python3-jsonpickle=0.9.6, python3-prompt-toolkit (= 0.57-1), python3-pygments (= 2.2.0), python3-rlp (= 0.5.1), python3-sha3 (= 0.2.1), python3-leveldb, python3-ioflo (= 1.5.4), python3-semver (= 2.7.9), python3-base58 (= 1.0.0), python3-orderedset (= 2.0), python3-sortedcontainers (= 1.5.7), python3-psutil (= 5.4.3), python3-pip (<< 10.0.0), python3-portalocker (= 0.5.7), python3-pyzmq (= 17.0.0), python3-libnacl (= 1.6.1), python3-six (= 1.11.0), python3-psutil (= 5.4.3), python3-intervaltree (= 2.1.0), python3-msgpack (= 0.4.6-1build1), python3-indy-crypto (= 0.4.5), python3-rocksdb (= 0.6.9), python3-dateutil (= 2.6.1), python3-pympler (= 0.5)

* python3-ujson=1.33-1build1 depends on
python3 (<< 3.6), python3 (>= 3.5~), libc6 (>= 2.14)

During the processing of non-strict dependencies of python3-ujson node-control-tool tries to request info about python3=3.6, python3=3.5~, libc6=2.14 but since we use different versions of these packages apt generates error messages.

Those error messages should be considered as warnings.

Delay in upgrade start connected with dependency tree building - it is made by several calls to apt.;;;","07/Nov/18 4:04 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/1004

node versions 655 and 656 contain the fix
node 658 has new plenum;;;","07/Nov/18 11:45 PM;ozheregelya;*Environment:*
indy-node 1.6.631 -> 1.6.656 -> 1.6.658
docker pool of 10 nodes

*Reason for Rejection:*
Upgrade 1.6.656 -> 1.6.658 was not happened.

*Steps to Reproduce:*
1. Setup the pool of 10 nodes with 1.6.631.
2. Upgrade it to 1.6.656.
=> Upgrade was successfully completed.
3. Upgrade it to 1.6.658.

*Actual Results:*
Pool was not upgraded, following errors appear in journalctl:
{code:java}
Nov 07 12:26:15 77b37e7c01e6 env[1610]: Upgrade from 1.6.656 to 1.6.658 failed: command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3-base58=1.0.0 python3-pyzmq=17.0.0 libselinux1=2.1.9 python3:any=3.3.2-2~ lsb-base=3.2-14 python3-orderedset=2.0 libnfnetlink0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-jsonpickle=0.9.6 python3-intervaltree=2.1.0 python3-pygments=2.2.0 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:26:15 77b37e7c01e6 env[1610]: Trying to rollback to the previous version command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3-base58=1.0.0 python3-pyzmq=17.0.0 libselinux1=2.1.9 python3:any=3.3.2-2~ lsb-base=3.2-14 python3-orderedset=2.0 libnfnetlink0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-jsonpickle=0.9.6 python3-intervaltree=2.1.0 python3-pygments=2.2.0 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:27:26 77b37e7c01e6 env[1610]: Upgrade from 1.6.656 to 1.6.656 failed: command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-pygments=2.2.0 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3:any=3.3.2-2~ libnfnetlink0 libselinux1=2.1.9 python3-base58=1.0.0 lsb-base=3.2-14 python3-orderedset=2.0 python3-pyzmq=17.0.0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-intervaltree=2.1.0 python3-jsonpickle=0.9.6 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:27:26 77b37e7c01e6 env[1610]: Trying to rollback to the previous version command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-pygments=2.2.0 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3:any=3.3.2-2~ libnfnetlink0 libselinux1=2.1.9 python3-base58=1.0.0 lsb-base=3.2-14 python3-orderedset=2.0 python3-pyzmq=17.0.0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-intervaltree=2.1.0 python3-jsonpickle=0.9.6 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:36:18 77b37e7c01e6 env[1610]: Upgrade from 1.6.656 to 1.6.658 failed: command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3-base58=1.0.0 python3-pyzmq=17.0.0 libselinux1=2.1.9 python3:any=3.3.2-2~ lsb-base=3.2-14 python3-orderedset=2.0 libnfnetlink0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-jsonpickle=0.9.6 python3-intervaltree=2.1.0 python3-pygments=2.2.0 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:36:18 77b37e7c01e6 env[1610]: Trying to rollback to the previous version command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3-base58=1.0.0 python3-pyzmq=17.0.0 libselinux1=2.1.9 python3:any=3.3.2-2~ lsb-base=3.2-14 python3-orderedset=2.0 libnfnetlink0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-jsonpickle=0.9.6 python3-intervaltree=2.1.0 python3-pygments=2.2.0 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:37:29 77b37e7c01e6 env[1610]: Upgrade from 1.6.656 to 1.6.656 failed: command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-pygments=2.2.0 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3:any=3.3.2-2~ libnfnetlink0 libselinux1=2.1.9 python3-base58=1.0.0 lsb-base=3.2-14 python3-orderedset=2.0 python3-pyzmq=17.0.0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-intervaltree=2.1.0 python3-jsonpickle=0.9.6 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:37:29 77b37e7c01e6 env[1610]: Trying to rollback to the previous version command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-pygments=2.2.0 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3:any=3.3.2-2~ libnfnetlink0 libselinux1=2.1.9 python3-base58=1.0.0 lsb-base=3.2-14 python3-orderedset=2.0 python3-pyzmq=17.0.0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-intervaltree=2.1.0 python3-jsonpickle=0.9.6 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:46:21 77b37e7c01e6 env[1610]: Upgrade from 1.6.656 to 1.6.658 failed: command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3-base58=1.0.0 python3-pyzmq=17.0.0 libselinux1=2.1.9 python3:any=3.3.2-2~ lsb-base=3.2-14 python3-orderedset=2.0 libnfnetlink0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-jsonpickle=0.9.6 python3-intervaltree=2.1.0 python3-pygments=2.2.0 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100
Nov 07 12:46:21 77b37e7c01e6 env[1610]: Trying to rollback to the previous version command apt-cache show python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-six=1.5 python3-sha3=0.2.1 python3-prompt-toolkit=0.57-1 libxtables11=1.6.0-2ubuntu3 python3-six=1.11.0 python3-lazy-object-proxy python3-portalocker=0.5.7 libpam0g=0.99.7.1 python3-pympler=0.5 python3-indy-crypto=0.4.5 python3-base58=1.0.0 python3-pyzmq=17.0.0 libselinux1=2.1.9 python3:any=3.3.2-2~ lsb-base=3.2-14 python3-orderedset=2.0 libnfnetlink0 python3-ioflo=1.5.4 python3-sortedcontainers=1.5.7 python3-libnacl=1.6.1 python3-base58 python3-pip=10.0.0 tzdata libc6=2.14 init-system-helpers=1.18~ python3-ujson=1.33-1build1 python3-msgpack=0.4.6-1build1 python3-psutil=5.4.3 python3-charm-crypto python3-leveldb python3-dateutil=2.6.1 python3-jsonpickle=0.9.6 python3-intervaltree=2.1.0 python3-pygments=2.2.0 libpam-runtime=1.0.1-11 python3-semver=2.7.9 returned 100{code}
*Expected Results:*
Pool should be upgraded.;;;","08/Nov/18 11:14 PM;ozheregelya;*Environment:*
indy-node 1.6.631 -> 1.6.661 -> 1.6.662 -> sovrin 1.1.85
docker pool of 10 nodes

*Steps to Reproduce:*
1. Setup the pool.
2. Perform following upgrades:
indy-node 1.6.631 -> 1.6.661 -> 1.6.662 -> sovrin 1.1.85
3. Check upgrade logs and journalctl.

*Actual Results:*
Upgrade successfully completed in 4 minutes.

*Additional Information:*
Ticket for improvement of logging to journalctl: INDY-1834.;;;",,,,,,,,,,,,,,,,,,,,
Need to have tests to verify that AWS cross-region logic works,INDY-1782,34906,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/Oct/18 6:31 PM,10/Nov/18 1:17 AM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,In scope of INDY-1622 molecule integration tests were not updated to check that instances are created in different regions. Need to add necessary tests.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzwen:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to migrate molecule integration tests from boto to boto3,INDY-1783,34907,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/Oct/18 6:41 PM,07/Nov/18 1:06 AM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"Currently molecule integration tests use boto, migration to boto3 is highly recommended since its interfaces as declared are always up to date with AWS services.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzwev:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove security groups at tear-down phase for both tests and playbooks,INDY-1784,34908,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,26/Oct/18 6:58 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,"Currently integration tests don't remove dynamically created AWS security groups.

'aws_manage' playbook might fail during security groups removal as well.

Need to fix that since it pollutes the list of AWS account's used resources.",,,,,,,,,,,,,,,,,,,,,,,,INDY-1817,,,,,INDY-1622,,,,,INDY-1891,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzx53:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"07/Nov/18 12:19 AM;andkononykhin;As it was explored currently there is a logic of removing security groups but they fail because linked to EC2 instances that are still in shutdown phase. Thus,

*PoA*:
 * during aws instances termination wait until it completed;;;","07/Nov/18 12:21 AM;andkononykhin;PR is under review:
https://github.com/hyperledger/indy-node/pull/1007;;;","10/Nov/18 2:16 AM;andkononykhin;*Problem reason*:

security groups were not removed properly

*Changes*:

Improved logic of custom ansible module to ensure that all terminated instances are finished termination process. In such case security groups are removed without any issues.


*Committed into*:

https://github.com/hyperledger/indy-node/pull/1023

*Risk factors*:

Nothing is expected.

*Risk*:

Low
 

*Covered with test*s:

integration tests have been extended

*Recommendations for QA*:
 * since QA for doesn't have ability to run aws related routine no validation from them are expected
 * was tested by me and Sergey Khoroshavin using our accounts on AWS;;;",,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need to have a possibility to use spot instances for testing,INDY-1785,34909,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,andkononykhin,andkononykhin,26/Oct/18 7:03 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,devops,,,Currently pool automation scripts can provision only on-demand AWS EC2 instances. It might be much cheaper to utilize spot instances as well (instead).,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx1z:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ozheregelya,,,,,,,,,,,"05/Dec/18 8:26 PM;andkononykhin;*PoA*:
 * read AWS docs regarding spot instances utilization rules
 * explore AWS SDK for python to choose most convenient API to manage spots
 * write tests to check:
 ** instances launched as spots
 ** max prices set properly
 ** during termination related spot requests cancelled as well
 * add options to stateful_set module:
 ** to switch between on-demand and spot instances
 ** to set max price
 * update playbook to pass spot options from their level;;;","06/Dec/18 11:01 PM;andkononykhin;*Problem reason*:

AWS provides ability to use spot instances that cost much less than on-demand. The feature wasn't supported by pool automation scripts.

*Changes:*
 * improved logic of ec2 instances provisioning with ability to switch to use spot instances and to specify max price
 * added related options for provisioning role
 * spot instances are default

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1074]

*Risk factors*:

Nothing is expected.

*Risk*:

Low
  

*Covered with tests*:
 * tests for stateful_set ansible module

*Recommendations for QA*:

Note:
 * you might encounter errors like  *MaxSpotInstanceCountExceeded* for some regions. Seems it usually appear for new accounts or accounts that just start using spots. To unblock yourself you may try to limit set of regions to _us-east-1, us-east-2, eu-west-1_ which should have higher limit values.

Test cases
 *  spots
 ** create inventory directory using namespace-config.py script (keep defaults for instance type = t2.micro for testing purposes)
 ** run provisioning playbook
 ** using AWS console check that instances' parameter ""Lifecycle"" is ""spot""
 ** you can check that there is a respective spot request: max price should match current priceces for on-demand instances ([https://aws.amazon.com/ec2/pricing/on-demand/])
 ** destroy instances
 * spots with specified max price
 ** create inventory with additional option *--localhosts.aws_ec2_spot_max_price=0.01* (it should be acceptable for all regions even for most expensive like Tokyo)
 ** do all the steps as before
 ** check spot requests for created instances: max price should be lower than before
 ** destroy instances
 * on-demand:
 ** create inventory with switched on spots: *--localhosts.aws_ec2_market_spot false*
 ** provision and check that all instances have _normal_ value for Lifecycle parameter (means on-demand)
 ** destroy instances;;;","07/Dec/18 7:55 AM;ozheregelya;*Steps to Validate:*
1. Create default inventory.
=>  spot instances created by default.
2. Create inventory with *--localhosts.aws_ec2_spot_max_price*.
=> spot instances created.
3. Create inventory with *--localhosts.aws_ec2_market_spot false*.
=> on-demand instances created.

*Actual Results:*
Creation of spot instances supported, enabled by default, and works as expected.;;;",,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need to have a possibility to scale AWS EC2 instances sizes,INDY-1786,34910,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/Oct/18 7:12 PM,26/Oct/18 7:51 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,Currently there is no support for scaling sizes for existing instance types (for example from small to large and back). It would be nice to have such a feature for some tests scenarios.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzwf3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need to have a possibility to combine aws_manage with other roles in one playbook,INDY-1787,34911,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/Oct/18 7:29 PM,07/Dec/18 9:39 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,Currently aws_manage role doesn't register nodes in in-memory inventory which prevents combining aws_manage and pool_install in one playbook.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx4f:5",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need to be able to refer different groups in the same namespace using one inventory,INDY-1788,34912,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,26/Oct/18 7:32 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,devops,,,Currently creation of different groups in the same tagged namespace leads to creation of different inventory dirs. Having one inventory dir per namespace would be highly preferable.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,INDY-1626,,,,INDY-1792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxa0:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,sergey.khoroshavin,,,,,,,,,,,"09/Nov/18 11:32 PM;sergey.khoroshavin;This relates closely to INDY-1626 and most probably can be considered done when INDY-1626 is done.;;;",,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need to be able to parametrize search criteria for AWS AMI,INDY-1789,34913,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,26/Oct/18 7:43 PM,26/Oct/18 7:50 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"Currently AMI search criteria is hardcoded: it uses latest Ubuntu 16.04 image from Canonical. It would be useful to make that configurable, in particular:
 * AMI owner
 * AMI specification (OS, disk, ...)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzwfr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update documentation regarding aws_manage cross region feature,INDY-1790,34914,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,andkononykhin,andkononykhin,andkononykhin,26/Oct/18 7:48 PM,13/Feb/19 9:50 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,In scope of INDY-1622 it was implemented logic of cross region nodes provisioning. Need to update docs regarding that.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1622,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxga:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a dev/QA I need know how much test pools cost during given period,INDY-1791,34915,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,andkononykhin,sergey.khoroshavin,sergey.khoroshavin,26/Oct/18 8:05 PM,16/Nov/18 6:44 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"This can be done using AWS cost explorer [API|https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ce.html#CostExplorer.Client.get_cost_and_usage]

The main goal is to automate checks for monthly charges performed before/after ansible roles running to warn if we are going out of the budget.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1813,INDY-1840,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx5z:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve usability of current pool automation PoC,INDY-1792,34916,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,sergey.khoroshavin,sergey.khoroshavin,26/Oct/18 8:17 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Current PoC pool automation require changing root playbooks in order to alter configuration parameters, which is inconvenient and error prone.
It would be much better to have following workflow:
* create experiment directory using some tool (probably bash/python script or yet another playbook)
** invocation of tool should be as simple as something like _init_pool <experiment-name>_
** directory should be populated with some sensible default configuration, including number and type of instances created and versions of software installed
** ideally experiment directory should be usable as ansible inventory (so later ansible invocations could look as simple as _ansible-playbook -i <experiment-name> <playbook-name>_)
** probably it would be also good to have some software configuration files (like indy_config.py) in experiment directory as well
** subsequent invocation of tool with same experiment name should NOT overwrite existing directory
* modify parameters/configuration files in experiment directory
* run pool_create/pool_install/whatever playbook passing experiment directory (or some subdirectory) as inventory
* playbooks are allowed to modify experiment directory (for example to update information about hosts or retrieve logs)
* when done run pool_destroy (which should shut down AWS instances), attach retrieved data to JIRA issue if needed and delete experiment directory

Probably this better be done in conjunction with INDY-1788",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1788,,,,,INDY-1827,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxbr:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,sergey.khoroshavin,,,,,,,,,,,"07/Nov/18 12:27 AM;andkononykhin;*PoA*:
 # create script that:
 ** explores roles' variables using defaults
 ** provide command line API that allows user to specify custom values for explored variables
 ** creates inventory directory with hierarchy [acceptable for ansible|https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#splitting-out-host-and-group-specific-data%C2%A0]
 ** dumps yamls with variables with custom values and commented defaults
 # improve roles to check that required variables are defined;;;","07/Nov/18 12:27 AM;andkononykhin;PR is under review:
https://github.com/hyperledger/indy-node/pull/1007;;;","08/Nov/18 9:07 PM;andkononykhin;*Problem reason*:

there was no way how to easy configure ansible roles without patching them

*Changes*:

improved bootstrap routine: renamed common role to ansible_bootstrap and leave there only bootstrapping logic
 added docker as default molecule engine for all configuration roles (ansible_bootstrap, node_install, pool_install)
 * created script _scripts/inventory-init.py_ that:
 ** creates inventory directory
 ** explores available roles for defaults
 ** provides API to pass custom values for variables
 ** dumps config file as group_vars
 ** Inventory's group_vars might be passed to playbooks as well as to molecule.

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1007]

*Risk factors*:

Nothing is expected.


 *Risk*:

Low
  

*Covered with tests*:

existent tests were extended a bit, no new functionality in role to test


 *Recommendations for QA*:
 * since QA for doesn't have ability to run aws related routine no validation from them are expected
 * was tested by me and [~sergey.khoroshavin] using our accounts on AWS;;;",,,,,,,,,,,,,,,,,,,,,,
node_control should permit upgrading a steward multiple times,INDY-1793,34955,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,smithbk,smithbk,29/Oct/18 11:29 PM,14/Jan/19 11:11 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Currently, when the node_control tool upgrades a steward to a new version of sovrin,
it marks the steward as being upgraded to that version and will not upgrade it again.   However, we need node_control to permit upgrading our steward multiple times because we are running our steward as a docker container. 

Consider the following sequence of events:
1) Our steward image is built with the latest version of sovrin (e.g. version 1)
2) Our steward is running
3) Sovrin updates to version 2
4) Our steward is upgraded via node_control to version 2
5) Our steward container fails and is restarted from the steward image (version 1)
6) Our steward will not be upgraded again because it was already upgraded once


",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwnb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,smithbk,,,,,,,,,,,"05/Dec/18 11:32 PM;esplinr;Thank you for the suggestion.

We explicitly disallowed ""upgrading"" to the same version, and need to research why we did that.

There is also a ""reinstall"" command that you can try investigating.;;;","14/Jan/19 11:11 AM;smithbk;Where can I find the reinstall command?  Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,
Configure APT streams that contain latest packages for Client and Server,INDY-1794,34957,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Invalid,,benjsmi,benjsmi,30/Oct/18 12:26 AM,11/May/19 11:40 AM,28/Oct/23 2:47 AM,11/May/19 11:40 AM,,,,,,0,build-system,should,Usability,"At the moment, [https://sovrin.slack.com/archives/C5FJY7F1Q/p1540391559000100,] for a validator to join a Sovrin Network (i.e. TestNet, MainNet), its administrator must know the version of Indy that the network is running/supports and also the package version for *all* the apt packages that are relied upon by that version.

For example, for the v1.6.73, upgrade, this was the install string we were required to use:
{code:java}
sudo apt install indy-anoncreds=1.0.11 indy-node=1.6.73 indy-plenum=1.6.51 indy-plenum=1.6.51 python3-base58=1.0.0 python3-dateutil=2.6.1 python3-indy-crypto=0.4.3 python3-intervaltree=2.1.0 python3-ioflo=1.5.4 python3-jsonpickle=0.9.6 python3-libnacl=1.6.1 python3-msgpack=0.4.6-1build1 python3-orderedset=2.0 python3-portalocker=0.5.7 python3-prompt-toolkit=0.57-1 python3-psutil=5.4.3 python3-pygments=2.2.0 python3-pympler=0.5 python3-pyzmq=17.0.0 python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-semver=2.7.9 python3-sha3=0.2.1 python3-six=1.11.0 python3-sortedcontainers=1.5.7 python3-ujson=1.33-1build1 python3-timeout-decorator=0.4.0 indy-node=1.6.73 indy-plenum=1.6.51 indy-plenum=1.6.51 python3-base58=1.0.0 python3-dateutil=2.6.1 python3-indy-crypto=0.4.3 python3-intervaltree=2.1.0 python3-ioflo=1.5.4 python3-jsonpickle=0.9.6 python3-libnacl=1.6.1 python3-msgpack=0.4.6-1build1 python3-orderedset=2.0 python3-portalocker=0.5.7 python3-prompt-toolkit=0.57-1 python3-psutil=5.4.3 python3-pygments=2.2.0 python3-pympler=0.5 python3-pyzmq=17.0.0 python3-rlp=0.5.1 python3-rocksdb=0.6.9 python3-semver=2.7.9 python3-sha3=0.2.1 python3-six=1.11.0 python3-sortedcontainers=1.5.7 python3-ujson=1.33-1build1 sovtoken=0.9.3+12.58 sovrin=1.1.24 sovtoken=0.9.3+12.58 sovtokenfees=0.9.3+13.58 sovrin=1.1.24{code}
 

This is a bit unwieldy.  So this issue is to track the work of being able to point at a specific version of either the _Sovrin_ or _Indy-Node_ packages and get all the correctly depended upon packages.  However, along these same lines, there's another requirement here:

For those users who are not currently on the Sovrin Slack, which may be quite a few, it can be difficult to tell if you're going to get the _correct_ version of Indy to join, say, the TestNet, if you just execute:
{code:java}
apt install -y sovrin{code}
Or maybe even just:
{code:java}
apt install -y indy-node{code}
There are already in place, master and stable branches in the Sovrin/Indy apt repository, so my proposal is that there be two more branches: testnet and mainnet.  These branches would *always* point to the versions of the packages that a person needs to install to join the corresponding network.

These fixes help address the situations where:
 # An Sovrin Validator administrator is trying to follow the latest instructions on deploying their validator, and the packages that they install are incompatible with what's running on the network they would like to join (note that they _must_ join the TestNet before being allowed on the MainNet, so the when the versions are different, this is problematic).
 # The MainNet and TestNet are running different versions, as they are at the time of writing, without having to specify a long list of package versions, we can just use apt's built-in functionality for this situation to make it effortless to join any/either network.
 # Automated tools are being built to build, for example, Docker images, or virtual machine appliances/images, and maintaining the list of packages is a long, complicated process.  Instead, what if the build engine could just use a branch of the apt repo and automatically be ensured compatibility with the desired network.

I'm not proposing that we remove or forget the stable, master, or RC branches – these definitely have their purpose as is already demonstrated by the community.  I just find that the situation comes up a lot on Sovrin Slack and it could be ameliorated by this already existing feature of apt.

I'm aware that a process already exists in which the Trustees push a transaction to the config ledger or poor ledger that causes the validator nodes to automatically update in place.  That's fine.  When that is executed would be a good time to update the pointer on that network to point at the version that will be installed when the upgrade is complete.  A Steward (or maybe even a _trust anchor_) who is not attached to the network at the time of the transaction has no idea what version they need to use in order to even *read* these transactions though, and we definitely saw the transaction structure change with recent upgrades, and I am sure that will happen again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1825,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwnr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),benjsmi,esplinr,,,,,,,,,,,"05/Dec/18 11:29 PM;esplinr;Thanks for the issue. We are surprised that you had to use such a complicated command to upgrade. Our intention is for the process is to be as simple as you described.

There are a few reasons why it might be necessary to explicitly list all the version dependencies:
* The Sovrin Network often waits to deploy the latest Indy release to complete more testing, and so they are cautious to ensure they don't accidentally grab something newer than they intend.
* The Sovrin Test Net and the Sovrin Main Net are frequently running different versions of the Indy code. When a node moves between them, it has to downgrade, so we are careful to explicitly list all version numbers.
* During the most recent upgrade of the Sovrin Network, we hit some bugs that took some complicated gymnastics to work through.

Which network are you talking about having to use that command? Where did you receive the instruction to use that command?;;;","06/Dec/18 2:50 AM;benjsmi;Thanks for responding! I'll put my responses in-line.
 * Which network are you talking about having to use that command?

At one point or another for both/either network.
 * Where did you receive the instruction to use that command?

[~mgbailey]

But Mike told me to specify those package that precisely because otherwise my node wouldn't function on the network properly. [The instructions|https://docs.google.com/document/d/1AH618bj4q9U8FS1uyoIgbcvwNzaghBCQ1v44tNpZ2OU/edit#heading=h.a2y2mmlyqj2t] say to just install either the indy-node or sovrin packages. This didn't get my newly-created node into a situation where it could join the network. There were problems reading the genesis transactions document, etc. I had explanations from the Sovrin Slack channels that this shouldn't be a problem because I ""should already have certain packages installed."" That's not a good way to run an apt repository – especially because given the experience I had, a new Steward who is trying to set up his/her validator would have failed.
 * The Sovrin Network often waits to deploy the latest Indy release to complete more testing, and so they are cautious to ensure they don't accidentally grab something newer than they intend.

This makes sense and is a standard practice for continuous release cycles. I'm glad Sovrin/Indy does this. However, that's not mutually exclusive to having a stream/branch on the apt repository that would allow a user to pull the exact version that he/she needs to use to join the STN or MainNet. You/devs/release manager would update the packages pointed to/being used by that network when ready.
 * The Sovrin Test Net and the Sovrin Main Net are frequently running different versions of the Indy code. When a node moves between them, it has to downgrade, so we are careful to explicitly list all version numbers.

Same answer as above. Apt can be configured to allow for these types of situations. In fact, you already have a ""stable"" and ""master"" branch... I'm just asking to add ""testnet"" and ""mainnet"" and update their version metadata independently based on what's actually running in-network.
 * During the most recent upgrade of the Sovrin Network, we hit some bugs that took some complicated gymnastics to work through.

Agreed. I'm aware that there are growing pains and I'm sympathetic. I'm actually offering this suggestion/feature request as a way to try to *help* mitigate some of these kinds of gymnastics/issues.

Thank you for taking the time to consider my POV on this and to follow up with thoughtful questions!

Ben;;;","11/May/19 11:39 AM;esplinr;I created a story for this issue in the tracker for issues specific to the Sovrin Network:
https://sovrin.atlassian.net/browse/SN-3

Because this isn't expected to require changes to the Indy project, I'll close this issue. All watchers of this issue should subscribe to the new issue.;;;",,,,,,,,,,,,,,,,,,,,,,
Adjust last_ordered_3pc and perform GC when detecting lag in checkpoints on backup,INDY-1795,34979,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,spivachuk,spivachuk,30/Oct/18 10:57 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"Currently, when a lag in checkpoints is detected on a backup replica, only the watermarks are shifted. This is insufficient to resume ordering of 3PC-batches. To make it possible to resume ordering starting from the 3PC-batch next to the last quorumed checkpoint, {{last_ordered_3pc}} must be also adjusted and the messages below its new value should be removed (i.e. garbage collection should be performed). In scope of this task add these actions to the logic of handling a lag in checkpoints on a backup replica.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1772,,,,,INDY-1843,,,,,,,,,,,,,,,"08/Nov/18 7:01 PM;VladimirWork;INDY-1795.PNG;https://jira.hyperledger.org/secure/attachment/16238/INDY-1795.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzzws7:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),spivachuk,VladimirWork,zhigunenko.dsr,,,,,,,,,,"03/Nov/18 12:33 AM;spivachuk;*Changes:*
- Added a test verifying resumption of ordering 3PC-batches on a backup replica on detection of a lag in checkpoints (parametrized for primary and non-primary replicas).
- Added a variant of the test verifying resumption of ordering in case the lag is detected after some batch in the next checkpoint has already been committed but cannot be ordered out of turn.
- Added a variant of the test verifying resumption of ordering in case the lag is detected after some 3PC-messages related to the next checkpoint have already been stashed as laying outside of the watermarks.
- Implemented resumption of ordering 3PC-batches on a backup replica on detection of a lag in checkpoints.

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/964
- https://github.com/hyperledger/indy-node/pull/1009

*Version:*
- indy-node 1.6.653-master
- indy-plenum 1.6.578-master

*Risk factors:*
- Nothing is expected.

*Risk:*
- Low

*Covered with tests:*
- {{test_backup_replica_resumes_ordering_on_lag_in_checkpoints}}
- {{test_backup_replica_resumes_ordering_on_lag_if_checkpoints_belate}}
- {{test_stashed_messages_processed_on_backup_replica_ordering_resumption}};;;","06/Nov/18 4:56 PM;zhigunenko.dsr;*Steps to Validate* (from INDY-1574):
0. Preload pool with 300k txns.
1. Run 20 writing and 200 reading txns/sec load test.
2. Check validator-info/logs/metrics.;;;","08/Nov/18 7:01 PM;VladimirWork; !INDY-1795.PNG|thumbnail! ;;;","08/Nov/18 9:32 PM;VladimirWork;Build Info:
indy-node 1.6.669

Steps to Validate:
1. Run 20 nyms/sec for 15 hours.
2. Check metrics and logs for OOM reasons at any node.

Actual Results:
Pool works normally. Only 8th node breaks due to OOM.
All logs and journals are in ev@evernymr33:logs/1795-1683.tar.gz;;;",,,,,,,,,,,,,,,,,,,,,
Unable to write node data to pool ledger,INDY-1796,35004,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Invalid,,mgbailey,mgbailey,31/Oct/18 11:54 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"When attempting to write node data to the pool ledger, the transaction is rejected by the pool.  Here is a test submission, for example:
{code:java}
pool(testnet):wallet(testnet_wallet):did(6fe...zSJ):indy> ledger node target=1232LjQ2fwszJRSdZqg53q5e6ayScmtpeZyPGgKDs123 node_ip=192.168.0.0 node_port=9797 client_ip=192.168.0.0 client_port=9797 alias=testValidator services=VALIDATOR blskey=t5jtREu8au2dwFwtH6QWopmTGxu6qmJ3iSnk321yLgeu7mHQRXf2ZCBuez8KCAQvFZGqqAoy2FcYvDGCqQxRCz9qXKgiBtykzxjDjYu87JECwwddnktz5UabPfZmfu6EoDn4rFxvd4myPu2hksb5Z9GT6UeoEYi7Ub3yLFQ3xxaQXd
Transaction has been rejected: A Proof of possession must be provided with BLS key
{code}
This is on the TestNet, running 1.6.78. I have tried this with indy-cli versions 1.6.6 and 1.6.7, but I think the problem is in the node code, not the cli. The rejection message is showing up in the node logs, one of which I have attached.

We have 4 new stewards trying to onboard right now, and multiple are running into this problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1389,IS-1055,,,,,,,,,,,,,,"31/Oct/18 11:53 PM;mgbailey;brazil_log.tgz;https://jira.hyperledger.org/secure/attachment/16192/brazil_log.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwxj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,mgbailey,,,,,,,,,,,"01/Nov/18 12:46 AM;mgbailey;A new field was added that is optional - That is it is only required if the BLS key is supplied. Since the BLS key should always be used on real networks, it is silly that it is optional.

Anyway, the new field is not documented in the indy-cli help screens. I will replace this ticket with one to fix that problem.;;;","01/Nov/18 1:26 AM;esplinr;If blskey is specified in node transaction, blskey_pop should be specified as well. blskey_pop can be found in output of init_indy_node script.;;;",,,,,,,,,,,,,,,,,,,,,,,
What is the use of the clients added to the Test Network ?,INDY-1797,35008,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,burdettadam,burdettadam,01/Nov/18 3:03 AM,01/Nov/18 3:10 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Hi,

I am new to Indy and I am trying to understand better the structure of the whole system.
I understand what is a Trustee, Steward and Trust Anchor.

I was reading the 'indy-plenum/plenum/common/test_network_setup.py' script and I saw this piece of code

 
{code:python|borderStyle=solid}
for cd in client_defs:
            txn = Member.nym_txn(cd.nym, verkey=cd.verkey, creator=trustee_def.nym,
                                 seq_no=seq_no,
                                 protocol_version=genesis_protocol_version)
{code}
What are these clients for ? what can they do ?

Thanks in advance for your answer !",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwy7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting up and deploying indy for global scale?,INDY-1798,35009,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:13 AM,11/Oct/19 8:44 PM,28/Oct/23 2:47 AM,11/Oct/19 8:44 PM,,,,,,0,,,,"h3. *[nathanawmk|https://github.com/nathanawmk]* commented [on Aug 9|https://github.com/hyperledger/indy-plenum/issues/865#issue-349091902]

I am looking to setup up and deploying a decentralized identity platform on a global scale and I am looking to indy. Has anyone setup indy for global scale?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwyf:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,esplinr,,,,,,,,,,,"11/Oct/19 8:44 PM;esplinr;The Sovrin Network has deployed Indy on a global scale.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Test sovrin-client ERROR TypeError: Can't mix strings and bytes in path components,INDY-1799,35011,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:26 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"h3. *[xzcccc|https://github.com/xzcccc]* commented [on Jun 4|https://github.com/hyperledger/indy-plenum/issues/723#issue-329062107]

I install the sovrin-node by $ pip install -U --no-cache-dir sovrin-client . (And this command is follow the guide form [https://www.evernym.com|https://www.evernym.com/])
However, when I run the sovrin, it was something error. As follow is the error code:

 
{code:xml}
 

` $ sovrin
Loading module /usr/local/lib/python3.5/dist-packages/config/config-crypto-example1.py
Module loaded.

Sovrin-CLI (c) 2017 Evernym, Inc.
Node registry loaded.
EvernymV1: 52.33.22.91:9721
EvernymV2: 52.38.24.189:9723
RespectNetwork: 34.200.79.65:9729
BULLDOG: 52.56.74.57:9746
Type 'help' for more information.
Running Sovrin 0.3.23

Saved keyring ""Default"" restored (/home/pi/.sovrin/keyrings/no-env/default.wallet)
Active keyring set to ""Default""
sovrin> prompt ALICE
ALICE> connect test
Active keyring ""Default"" saved (/home/pi/.sovrin/keyrings/no-env/default.wallet)
Current active keyring got moved to 'test' environment. Here is the detail:
keyring name: Default
old location: /home/pi/.sovrin/keyrings/no-env/default.wallet
new location: /home/pi/.sovrin/keyrings/test/default.wallet

Saved keyring ""Default"" restored (/home/pi/.sovrin/keyrings/test/default.wallet)
Active keyring set to ""Default""
Error while running coroutine shell: TypeError(""Can't mix strings and bytes in path components"",)
Traceback (most recent call last):
File ""/usr/local/bin/sovrin"", line 78, in <module>
run_cli()
File ""/usr/local/bin/sovrin"", line 56, in run_cli
looper.run(cli.shell(*commands))
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 254, in run
return self.loop.run_until_complete(what)
File ""/usr/lib/python3.5/asyncio/base_events.py"", line 466, in run_until_complete
return future.result()
File ""/usr/lib/python3.5/asyncio/futures.py"", line 293, in result
raise self._exception
File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
result = coro.send(None)
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 245, in wrapper
raise ex
File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 233, in wrapper
results.append(await coro)
File ""/usr/local/lib/python3.5/dist-packages/plenum/cli/cli.py"", line 1119, in shell
self.parse(c)
File ""/usr/local/lib/python3.5/dist-packages/plenum/cli/cli.py"", line 1902, in parse
r = action(matchedVars)
File ""/usr/local/lib/python3.5/dist-packages/sovrin_client/cli/cli.py"", line 1611, in _connectTo
self._buildClientIfNotExists(self.config)
File ""/usr/local/lib/python3.5/dist-packages/plenum/cli/cli.py"", line 543, in _buildClientIfNotExists
self.newClient(clientName=name, config=config)
File ""/usr/local/lib/python3.5/dist-packages/sovrin_client/cli/cli.py"", line 382, in newClient
client = super().newClient(clientName, config=config)
File ""/usr/local/lib/python3.5/dist-packages/plenum/cli/cli.py"", line 1039, in newClient
config=config)
File ""/usr/local/lib/python3.5/dist-packages/sovrin_client/client/client.py"", line 51, in __init__
sighex)
File ""/usr/local/lib/python3.5/dist-packages/plenum/client/client.py"", line 85, in __init__
if self.exists(self.stackName, basedirpath):
File ""/usr/local/lib/python3.5/dist-packages/plenum/client/client.py"", line 211, in exists
os.path.exists(os.path.join(basedirpath, name))
File ""/usr/lib/python3.5/posixpath.py"", line 89, in join
genericpath._check_arg_types('join', a, *p)
File ""/usr/lib/python3.5/genericpath.py"", line 145, in _check_arg_types
raise TypeError(""Can't mix strings and bytes in path components"") from None
TypeError: Can't mix strings and bytes in path components`
{code}
I dont know how to solve this question....

Python version:3.5
Pip version:9.0.1
OS version:Debian 9

THANKS

{{}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwyv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 3:27 AM;burdettadam;ashcherbakov commented on Jun 7
I think instruction from Evernym.com is very outdated. There is no sovrin-client repo.
If you want to have indy-node (and indy-plenum) for development and local running of tests, then please follow the instructions from [https://github.com/hyperledger/indy-node/blob/master/docs/setup-dev.md]

If you just want to run a pool, then you can use Docker: [https://github.com/hyperledger/indy-node/tree/master/environment/docker/pool];;;",,,,,,,,,,,,,,,,,,,,,,,,
Tests using the sdk fail on the master branch ,INDY-1800,35012,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:31 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"lovesh commented on Jun 1
Tests (majority) on master branch that require indy-sdk fail on the call to open_pool_ledger which is calling indy_open_pool_ledger. Easiest way to reproduce is to run plenum/test/sdk/test_sdk_bindings.py. The same tests work on older branches, the difference being the format of genesis files.
Failing test have this genesis file (new txn format)
{code}
{""reqSignature"":{},""txn"":{""data"":{""data"":{""alias"":""Alpha"",""blskey"":""p2LdkcuVLnqidf9PAM1josFLfSTSTYuGqaSBx2Dq72Z5Kt2axQicYyqkQ6ZfcwzHpmLevmcXVwD4EC32wTbusvYxb5D1MJBfu67SQqxRTcK7pRBQXYiaPrzqUo9odhAgrwNPSbHcBJM6s5cNUPvjZZDuSJvhjC7tKFV9FGqyX4Zs4u"",""client_ip"":""127.0.0.1"",""client_port"":6152,""node_ip"":""127.0.0.1"",""node_port"":6151,""services"":[""VALIDATOR""]},""dest"":""JpYerf4CssDrH76z7jyQPJLnZ1vwYgvKbvcp16AB5RQ""},""metadata"":{""from"":""MSjKTWkPLtYoPEaTF1TUDb""},""type"":""0""},""txnMetadata"":{""seqNo"":1,""txnId"":""b1a96dd646bccaa24cef7a3db22a6f995f05658f4f1c3272913e258c03e6fb24""},""ver"":""1""}
{""reqSignature"":{},""txn"":{""data"":{""data"":{""alias"":""Beta"",""blskey"":""2JY8jXAiy3ffLu1ggSaiFTBpmb9X7wUZEedg7G3mJSU1vCnqFzYAofGR9SGEvb1C3p88Kdm2CPAdMyMc5v9KxL26vfeeHzRa2N5EHwV1JpPH5kcdYYkFhgNf8wxFAvJ9vPS1aCVms41ZC17GeovJLh4L2iACNd7ttPyS5M6a9Uux9oz"",""client_ip"":""127.0.0.1"",""client_port"":6154,""node_ip"":""127.0.0.1"",""node_port"":6153,""services"":[""VALIDATOR""]},""dest"":""DG5M4zFm33Shrhjj6JB7nmx9BoNJUq219UXDfvwBDPe2""},""metadata"":{""from"":""E4rYSWBUA12j5ScG6mie1p""},""type"":""0""},""txnMetadata"":{""seqNo"":2,""txnId"":""703390318bd55aef50b7823d2b90a846debff99e6e3d401a24a921b733912a6d""},""ver"":""1""}
{""reqSignature"":{},""txn"":{""data"":{""data"":{""alias"":""Gamma"",""blskey"":""1JwRChBPGQTtp4m4aNBRrf2kG3mzgxtRUTAscx8iV9uDih34pKWnEA54CoNq3DhAgEURQCN6VKrSZUb6zzzLBHhQt7HBdw2kbfUR3Fap2jqE6TEDamFQpqced2GRVcDo5wgVVKydsf1rFundAk7jMSk7mLrf7zBBN9xBx2yaUkvueN"",""client_ip"":""127.0.0.1"",""client_port"":6156,""node_ip"":""127.0.0.1"",""node_port"":6155,""services"":[""VALIDATOR""]},""dest"":""AtDfpKFe1RPgcr5nnYBw1Wxkgyn8Zjyh5MzFoEUTeoV3""},""metadata"":{""from"":""QxzxUA7gePtb9t46n1YgsC""},""type"":""0""},""txnMetadata"":{""seqNo"":3,""txnId"":""a8ab3d5805c9214bc66b794f599cbccd5a5958dc5a6a322ee81e3a68344c6db7""},""ver"":""1""}
{""reqSignature"":{},""txn"":{""data"":{""data"":{""alias"":""Delta"",""blskey"":""4kkk7y7NQVzcfvY4SAe1HBMYnFohAJ2ygLeJd3nC77SFv2mJAmebH3BGbrGPHamLZMAFWQJNHEM81P62RfZjnb5SER6cQk1MNMeQCR3GVbEXDQRhhMQj2KqfHNFvDajrdQtyppc4MZ58r6QeiYH3R68mGSWbiWwmPZuiqgbSdSmweqc"",""client_ip"":""127.0.0.1"",""client_port"":6158,""node_ip"":""127.0.0.1"",""node_port"":6157,""services"":[""VALIDATOR""]},""dest"":""4yC546FFzorLPgTNTc6V43DnpFrR8uHvtunBxb2Suaa2""},""metadata"":{""from"":""WMStfRmANynUmdpa1QYKDw""},""type"":""0""},""txnMetadata"":{""seqNo"":4,""txnId"":""18833da39fb9b7f8c917fe0220daf9cf12e6524df8fb16e39f04dbe827e2d200""},""ver"":""1""}
{code}
Passing tests have this genesis file (old txn format)
{code}
{""data"":{""alias"":""Alpha"",""blskey"":""p2LdkcuVLnqidf9PAM1josFLfSTSTYuGqaSBx2Dq72Z5Kt2axQicYyqkQ6ZfcwzHpmLevmcXVwD4EC32wTbusvYxb5D1MJBfu67SQqxRTcK7pRBQXYiaPrzqUo9odhAgrwNPSbHcBJM6s5cNUPvjZZDuSJvhjC7tKFV9FGqyX4Zs4u"",""client_ip"":""127.0.0.1"",""client_port"":6144,""node_ip"":""127.0.0.1"",""node_port"":6143,""services"":[""VALIDATOR""]},""dest"":""JpYerf4CssDrH76z7jyQPJLnZ1vwYgvKbvcp16AB5RQ"",""identifier"":""MSjKTWkPLtYoPEaTF1TUDb"",""type"":""0""}
{""data"":{""alias"":""Beta"",""blskey"":""2JY8jXAiy3ffLu1ggSaiFTBpmb9X7wUZEedg7G3mJSU1vCnqFzYAofGR9SGEvb1C3p88Kdm2CPAdMyMc5v9KxL26vfeeHzRa2N5EHwV1JpPH5kcdYYkFhgNf8wxFAvJ9vPS1aCVms41ZC17GeovJLh4L2iACNd7ttPyS5M6a9Uux9oz"",""client_ip"":""127.0.0.1"",""client_port"":6146,""node_ip"":""127.0.0.1"",""node_port"":6145,""services"":[""VALIDATOR""]},""dest"":""DG5M4zFm33Shrhjj6JB7nmx9BoNJUq219UXDfvwBDPe2"",""identifier"":""E4rYSWBUA12j5ScG6mie1p"",""type"":""0""}
{""data"":{""alias"":""Gamma"",""blskey"":""1JwRChBPGQTtp4m4aNBRrf2kG3mzgxtRUTAscx8iV9uDih34pKWnEA54CoNq3DhAgEURQCN6VKrSZUb6zzzLBHhQt7HBdw2kbfUR3Fap2jqE6TEDamFQpqced2GRVcDo5wgVVKydsf1rFundAk7jMSk7mLrf7zBBN9xBx2yaUkvueN"",""client_ip"":""127.0.0.1"",""client_port"":6148,""node_ip"":""127.0.0.1"",""node_port"":6147,""services"":[""VALIDATOR""]},""dest"":""AtDfpKFe1RPgcr5nnYBw1Wxkgyn8Zjyh5MzFoEUTeoV3"",""identifier"":""QxzxUA7gePtb9t46n1YgsC"",""type"":""0""}
{""data"":{""alias"":""Delta"",""blskey"":""4kkk7y7NQVzcfvY4SAe1HBMYnFohAJ2ygLeJd3nC77SFv2mJAmebH3BGbrGPHamLZMAFWQJNHEM81P62RfZjnb5SER6cQk1MNMeQCR3GVbEXDQRhhMQj2KqfHNFvDajrdQtyppc4MZ58r6QeiYH3R68mGSWbiWwmPZuiqgbSdSmweqc"",""client_ip"":""127.0.0.1"",""client_port"":6150,""node_ip"":""127.0.0.1"",""node_port"":6149,""services"":[""VALIDATOR""]},""dest"":""4yC546FFzorLPgTNTc6V43DnpFrR8uHvtunBxb2Suaa2"",""identifier"":""WMStfRmANynUmdpa1QYKDw"",""type"":""0""}
{code}
It looks like indy-sdk is not able to parse new txn format

By using the libindy.so and libindy-crypto.so build from master branch from respective repositories, i get the error _load_cdll: Can't load libindy: .... libindy.so: undefined symbol: crypto_pwhash while running plenum tests (try running the file mentioned above). I have libsodium18 and libsodium18-dev installed, do i need a newer version of libsodium for crypto_pwhash?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwz3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 3:33 AM;burdettadam;ashcherbakov commented on Jun 4
What version of libindy do you use? Please note, that SDK needs to be updated locally from time to time. The version is pinned in setup files.
The required version of SDK:

- libindy: 1.4.0~511 (https://github.com/hyperledger/indy-plenum/blob/master/ci/ubuntu.dockerfile#L13)
- Python wrapper: python3-indy==1.4.0-dev-511 (https://github.com/hyperledger/indy-plenum/blob/master/setup.py#L33);;;",,,,,,,,,,,,,,,,,,,,,,,,
Run sovrin error,INDY-1801,35013,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:37 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"xzcccc commented on Jun 1
I install the sovrin-node by $ pip install -U --no-cache-dir sovrin-client . (And this command is follow the guide form https://www.evernym.com)
However, when I run the sovrin, it was something error. As follow is the error code:
{code}
(sovrin) [xzc@xzc ~]$ sovrin
Loading module /home/xzc/anaconda3/lib/python3.6/site-packages/config/config-crypto-example1.py
Module loaded.
Traceback (most recent call last):
  File ""/home/xzc/anaconda3/bin/sovrin"", line 37, in <module>
    from sovrin_client.cli.cli import SovrinCli
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/sovrin_client/cli/cli.py"", line 17, in <module>
    from plenum.cli.cli import Cli as PlenumCli
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/plenum/cli/cli.py"", line 82, in <module>
    from plenum.server.node import Node
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/plenum/server/node.py"", line 71, in <module>
    from plenum.server.monitor import Monitor
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/plenum/server/monitor.py"", line 24, in <module>
    pluginManager = PluginManager()
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/plenum/server/notifier_plugin_manager.py"", line 35, in __init__
    self.importPlugins()
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/plenum/server/notifier_plugin_manager.py"", line 79, in importPlugins
    plugins = self._findPlugins()
  File ""/home/xzc/anaconda3/lib/python3.6/site-packages/plenum/server/notifier_plugin_manager.py"", line 105, in _findPlugins
    for pkg in pip.utils.get_installed_distributions()
AttributeError: module 'pip' has no attribute 'utils'
{code}
Howeverm,when i go to python console by type python and import pip.utils, it works ok.
{code}
(sovrin) [xzc@xzc ~]$ python
Python 3.6.4 |Anaconda, Inc.| (default, Jan 16 2018, 18:10:19) 
[GCC 7.2.0] on linux
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import pip.utils
>>> pip.utils
<module 'pip.utils' from '/home/xzc/.virtualenvs/sovrin/lib/python3.6/site-packages/pip/utils/__init__.py'>
>>> pip.utils.get_installed_distributions()
[wheel 0.31.1 (/home/xzc/.virtualenvs/sovrin/lib/python3.6/site-packages), setuptools 39.2.0 (/home/xzc/.virtualenvs/sovrin/lib/python3.6/site-packages), pip 9.0.1 (/home/xzc/.virtualenvs/sovrin/lib/python3.6/site-packages)]
{code}
Python version:3.6.4
Pip version:9.0.1
OS version:Centos7

THANKS",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwzb:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot run tests on Mac because of _lzma module,INDY-1802,35014,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:38 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"kdenhartog commented on May 31
Here's the stack trace I'm getting when trying to run tests from Master build. I believe it is specific to my python build because it's not able to find this standard package which is supposed to built into python3. I am currently installing a python3 (3.5.2) instance through pyenv. Any suggestions on how to resolve this?

Traceback (most recent call last):
File ""/Applications/PyCharm.app/Contents/helpers/pydev/pydev_run_in_console.py"", line 52, in run_file
pydev_imports.execfile(file, globals, locals) # execute the script
File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
exec(compile(contents+""\n"", file, 'exec'), glob, loc)
File ""/Users/kyle/garage/indy/indy-plenum/plenum/test/primary_election/test_primary_election_case2.py"", line 3, in 
from stp_core.loop.eventually import eventually
File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 20, in do_import
module = self._system_import(name, *args, **kwargs)
File ""/Users/kyle/garage/indy/indy-plenum/stp_core/loop/eventually.py"", line 9, in 
from stp_core.common.log import getlogger
File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 20, in do_import
module = self._system_import(name, *args, **kwargs)
File ""/Users/kyle/garage/indy/indy-plenum/stp_core/common/log.py"", line 6, in 
from stp_core.common.logging.CompressingFileHandler import CompressingFileHandler
File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 20, in do_import
module = self._system_import(name, *args, **kwargs)
File ""/Users/kyle/garage/indy/indy-plenum/stp_core/common/logging/CompressingFileHandler.py"", line 4, in 
import lzma
File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 20, in do_import
module = self._system_import(name, *args, **kwargs)
File ""/Users/kyle/.pyenv/versions/3.5.2/lib/python3.5/lzma.py"", line 26, in 
from _lzma import *
File ""/Applications/PyCharm.app/Contents/helpers/pydev/_pydev_bundle/pydev_import_hook.py"", line 20, in do_import
module = self._system_import(name, *args, **kwargs)
ImportError: No module named '_lzma'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwzj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 3:39 AM;burdettadam;ashcherbakov commented on Jun 7
We don't support Mac now, only Ubuntu 16.;;;","01/Nov/18 3:39 AM;burdettadam;kdenhartog commented on Jun 8
Figured out an alternative way using the docker script. It was confusing to know the version for the build with build-indy-plenum-docker.sh because I had to include a version. Also, running the container was confusing, so I'm going to make an PR to the documentation for that and help my team migrate to developing within the docker instances. docker run -it --name indy-plenum -v <local repo location>/indy-plenum:/home/indy-dev indy-plenum-build-u1604 bash;;;",,,,,,,,,,,,,,,,,,,,,,,
request to remove stale branches from this repo,INDY-1803,35015,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,burdettadam,burdettadam,01/Nov/18 3:41 AM,01/Nov/18 3:41 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"kdenhartog commented on Apr 17
Can we remove branches other than Master and Stable from this repo and have them moved to forks? I would make a PR to do this, but I'm not sure which ones are still being used and which are stale.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwzr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
No module named 'indy.pool',INDY-1804,35016,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:46 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"amber-zhang commented on Feb 5
Hi,

I am trying to run 'python -m plenum.test' under indy-plenum project, but got error: No module named 'indy.pool'. Please give suggestions on where to compile the module and install it. Thank you!

/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/config.py:329: in _getconftestmodules
return self._path2confmods[path]
E KeyError: local('/sovrin/build/indy-plenum/plenum/test')

During handling of the above exception, another exception occurred:
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/config.py:360: in _importconftest
return self._conftestpath2mod[conftestpath]
E KeyError: local('/sovrin/build/indy-plenum/plenum/test/conftest.py')

During handling of the above exception, another exception occurred:
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/config.py:366: in _importconftest
mod = conftestpath.pyimport()
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/py/_path/local.py:668: in pyimport
import(modname)
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/pytest/assertion/rewrite.py:213: in load_module
py.builtin.exec(co, mod.dict)
plenum/test/conftest.py:16: in 
from indy.pool import create_pool_ledger_config, open_pool_ledger, close_pool_ledger
E ImportError: No module named 'indy.pool'

During handling of the above exception, another exception occurred:
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/py/_path/common.py:377: in visit
for x in Visitor(fil, rec, ignore, bf, sort).gen(self):
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/py/_path/common.py:429: in gen
for p in self.gen(subdir):
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/py/_path/common.py:418: in gen
dirs = self.optsort([p for p in entries
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/py/_path/common.py:419: in 
if p.check(dir=1) and (rec is None or rec(p))])
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/main.py:411: in _recurse
ihook = self.gethookproxy(path)
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/main.py:315: in gethookproxy
my_conftestmodules = pm._getconftestmodules(fspath)
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/config.py:343: in _getconftestmodules
mod = self._importconftest(conftestpath)
/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/_pytest/config.py:368: in _importconftest
raise ConftestImportFailure(conftestpath, sys.exc_info())
E _pytest.config.ConftestImportFailure: ImportError(""No module named 'indy.pool'"",)
E File ""/root/.virtualenvs/indyVenv/lib/python3.5/site-packages/pytest/assertion/rewrite.py"", line 213, in load_module
E py.builtin.exec(co, mod.dict)
E File ""/sovrin/build/indy-plenum/plenum/test/conftest.py"", line 16, in 
E from indy.pool import create_pool_ledger_config, open_pool_ledger, close_pool_ledger",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwzz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 3:46 AM;burdettadam;amber-zhang commented on Feb 5
it has been solved after run these two commands. so closing it.
pip install python3-indy
pip install pytest-xdist;;;",,,,,,,,,,,,,,,,,,,,,,,,
problem regarding the sha3.,INDY-1805,35017,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 3:48 AM,06/Nov/19 6:51 PM,28/Oct/23 2:47 AM,11/Oct/19 8:42 PM,,,,,,0,,,,"WandyLau commented on Feb 1 
Hi maintainer,

As I found there are two module for sha3, pysha3 and sha3.
sha3: https://github.com/moshekaplan/python-sha3
pysha3: https://pypi.python.org/pypi/pysha3

The latter one has not updated since 2016. Why we use this ?
Can we just replace it with pysha3?",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx07:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,burdettadam,huxd,,,,,,,,,,"01/Nov/18 3:50 AM;burdettadam;lovesh commented on Feb 2
@WandyLau I see we use https://github.com/bjornedstrom/python-sha3. This is a pure python implementation so better chances of it working on various platforms. Do you see some issues with the library except that it has not had recent commits (it is fine for such projects).;;;","01/Nov/18 3:50 AM;burdettadam;WandyLau commented on Feb 3
Yes, I can not install this in s390. It complains the error.
The real link should be https://github.com/bjornedstrom/python-sha3

src/64/KeccakF-1600-opt64.c:250:2: error: #error ""Not yet implemented"".

So I just wonder if I can just replace this python-sha3 with pysha?
pysha is also on based of the Keccak.;;;","01/Nov/18 3:50 AM;burdettadam;lovesh commented on Feb 3
It will not be a drop-in replacement since python-sha3 does not patch hashlib like pysha3, so its a little more work. Raising an issue at python-sha3 with your error would be helpful;;;","01/Nov/18 3:51 AM;burdettadam;WandyLau commented on Feb 3
Does that mean pysha is more complete that python-sha? Shall we consider this replacement? Just
ask.;;;","01/Nov/18 3:56 AM;burdettadam;lovesh commented on Feb 5 
I can't say which is more complete or better. Also i don't know if there are platforms out there that will not run pysha3 but run python-sha3. This has to explored. We won't say no to any help on this :);;;","01/Nov/18 3:56 AM;burdettadam;WandyLau commented on Feb 5
As I see from the code, pysha3 just walk around this big endian problem, which is not implemented yet. And python-sha3, pysha3 use the same code of KeccakCodePackage. So I think it is the problem of KeccakCodePackage. And pysha3 may be a option. Not sure if this can work for us.
I will explore it.;;;","01/Nov/18 3:57 AM;burdettadam;kdenhartog commented on Apr 17
As far as I can tell, this issue comes from if you are running python 3.5 vs python3.6. I've been able to resolve this issue by building from python3.5 as python 3.6 doesn't have the right sha3 package anymore. I didn't investigate more into this.;;;","01/Nov/18 3:57 AM;burdettadam;ryanwest6 commented on May 17
I've discovered this issue as well, the sha3 package we're currently using seems to break with python3.6 (may be a difference in Cython calling C-code, not sure). This ticket in JIRA has a lot more info and differences in sha between 3.5 and 3.6: https://jira.hyperledger.org/browse/INDY-1308. Would it be feasible to rely on hashlib's SHA3-256 (introduced in python3.6) method instead of a package dependency, or do we need to keep plenum compatible with 3.5 and earlier?;;;","01/Nov/18 3:57 AM;burdettadam;ryanwest6 commented on May 22
I'd like to get plenum to work with python3.6 and I can integrate either pysha3 instead of sha3, or use python3.6's new built in SHA3-256 to do so. Any thoughts on which is best though?;;;","01/Nov/18 3:58 AM;burdettadam;lovesh commented on May 23 
@ryanwest6 You can add something like this to make it work both on python 3.5 and 3.6
{code}
try:
     # For python 3.6
     from hashlib import sha3_256              # sha3_256 requires bytes    
except ImportError:
     # For python 3.5
     import sha3   // or pysha3 if you need
     sha3_256 = lambda x: sha3.sha3_256(x.decode())   # _sha3.sha3_256 requires a string
{code};;;","01/Nov/18 3:58 AM;burdettadam;WandyLau commented on May 28
Hi, this problem has been resolved. But the patch is in master branch and it is not released so far. If
you want to use it, please clone the master branch.;;;","01/Nov/18 3:58 AM;burdettadam;ryanwest6 commented on May 29
I just recloned and downloaded master but still get the same issue on python 3.6.5: 'AttributeError: module '_sha3' has no attribute 'sha3''. Which commit to master made the fixes?;;;","11/Oct/19 8:42 PM;ashcherbakov;This has been already fixed;;;","06/Nov/19 6:51 PM;huxd;I still encounter this issue on s390 when using latest master branch code;;;",,,,,,,,,,,
Is there any replacement for leveldb?,INDY-1806,35018,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 4:00 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"WandyLau commented on Feb 1
Can I just replace leveldb with other? If so, please give some choices. Thanks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx0f:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 4:01 AM;burdettadam;lovesh commented on Feb 2 
@WandyLau Various usages of leveldb in the project extend a key value store interface KeyValueStorage at storage/kv_store.py. We wanted to support rocksdb too, here KeyValueStorageRocksdb at storage/kv_store_rocksdb.py. But leveldb is the only implemented option as of now;;;","01/Nov/18 4:01 AM;burdettadam;amber-zhang commented on Feb 4
@lovesh
Hi, I am trying to run indy-plenum in Ubuntu container based on s390x platform. Now I installed python-leveldb manually in Ubuntu container based on s390x platform. Please see below. Also I changed the ""import leveldb' into 'import python-leveldb/xenial as leveldb'. When I tried to run 'python3 -m plenum.test', I got errors "" _pytest.config.ConftestImportFailure: SyntaxError('invalid syntax', ('/sovrin/build/indy-plenum/storage/kv_store_leveldb_int_keys.py', 5, 18, ' import python-leveldb/xenial as leveldb\n'))"". Any ideas to import the manually installed leveldb? thanks

======python-leveldb installation info =========
apt list python-leveldb
Listing... Done
python-leveldb/xenial,now 0~svn68-2build4 s390x [installed];;;","01/Nov/18 4:02 AM;burdettadam;lovesh commented on Feb 5
@amber-zhang You shouldn't need to replace import leveldb with anything? python-leveldb is the name of the ubuntu package, the python package would still be called leveldb;;;","01/Nov/18 4:02 AM;burdettadam;WandyLau commented on Feb 5
I have enable leveldb in s390 and pull a new request. let's close this issue.;;;",,,,,,,,,,,,,,,,,,,,,
README.md out of date - init_plenum_raet_keep parameters ,INDY-1807,35019,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,burdettadam,burdettadam,01/Nov/18 4:03 AM,11/Oct/19 8:43 PM,28/Oct/23 2:47 AM,11/Oct/19 8:43 PM,,,,,,0,,,,"elBradford commented on Jan 20, 2017
The keep key gen commands listed in the readme(https://github.com/evernym/plenum/blob/master/README.md) have incorrect parameters -

init_plenum_raet_keep --name Alpha --seeds 000000000000000000000000000Alpha Alpha000000000000000000000000000 --force

The current code (installed using pip) asks for a --seed parameter instead, and will only accept a single seed:

init_plenum_raet_keep --name Alpha --seed 000000000000000000000000000AlphaAlpha000000000000000000000000000 --force

I'm not sure what the implications of the change are - whether it would use the two seeds to generate the key pair whereas it now uses the same seed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx0n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,burdettadam,,,,,,,,,,,"01/Nov/18 4:04 AM;burdettadam;lovesh commented on Jan 23, 2017 
Sorry for the trouble, the README was updated for correct changes, looks like it accidentally got overridden here(https://github.com/evernym/plenum/commit/311dd922b740cd2bb8919b89cc509d7c5c09bc67#diff-04c6e90faac2675aa89e2176d2eec7d8L109). We still generate 2 keypairs but one keypair is derived from the other so we just need one seed (even that is optional). We will get the correct changes back;;;","11/Oct/19 8:43 PM;ashcherbakov;`init_plenum_raet_keep` is not used anywhere now. Moreover, the ticket points to Evernym's internal repository, not Hyperledger one.;;;",,,,,,,,,,,,,,,,,,,,,,,
"'NoneType' object has no attribute 'split'""",INDY-1808,35020,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 4:06 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"AdrienPensart commented on Dec 12, 2016 
You might have a problem :
{code}
(env) > $ plenum                                                                                                         [±master ✓]

Plenum-CLI (c) 2016 Evernym, Inc.
Node registry loaded.
    Alpha: 127.0.0.1:9701
    Beta: 127.0.0.1:9703
    Gamma: 127.0.0.1:9705
    Delta: 127.0.0.1:9707
Type 'help' for more information.
plenum> new node all
plugin FirebaseStatsConsumer successfully loaded from module plugin_firebase_stats_consumer
Delta added replica Delta:0 to instance 0 (master)
Delta added replica Delta:1 to instance 1 (backup)
Delta listening for other nodes at 127.0.0.1:9707
Delta disconnected node is joined
Delta disconnected node is joined
Delta looking for Alpha at 127.0.0.1:9701
Delta starting key sharing
plugin FirebaseStatsConsumer successfully loaded from module plugin_firebase_stats_consumer
Beta added replica Beta:0 to instance 0 (master)
Beta added replica Beta:1 to instance 1 (backup)
Beta listening for other nodes at 127.0.0.1:9703
Beta disconnected node is joined
Beta disconnected node is joined
Beta looking for Alpha at 127.0.0.1:9701
Beta starting key sharing
plugin FirebaseStatsConsumer successfully loaded from module plugin_firebase_stats_consumer
Gamma added replica Gamma:0 to instance 0 (master)
Gamma added replica Gamma:1 to instance 1 (backup)
Gamma listening for other nodes at 127.0.0.1:9705
Gamma disconnected node is joined
Gamma disconnected node is joined
Gamma starting key sharing
plugin FirebaseStatsConsumer successfully loaded from module plugin_firebase_stats_consumer
Alpha added replica Alpha:0 to instance 0 (master)
Alpha added replica Alpha:1 to instance 1 (backup)
Alpha listening for other nodes at 127.0.0.1:9701
Alpha first time running; waiting for key sharing...
Alpha starting key sharing
Alpha looking for Gamma at 127.0.0.1:9705
Gamma now connected to Alpha
Alpha now connected to Gamma
Alpha msg validated ({'ledgerType': 1, 'txnSeqNo': 0, 'merkleRoot': '47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU='}, 'Gamma')
Gamma msg validated ({'ledgerType': 1, 'txnSeqNo': 0, 'merkleRoot': '47DEQpj8HBSa+/TImW+5JCeuQeRkm5NMpJWZG3hSuFU='}, 'Alpha')
plenum> status
Nodes: Beta, Delta, Gamma, Alpha
Clients: No clients are running. Try typing 'new client <name>'.
f-value (number of possible faulty nodes): 1
Instances: 2
Error while running coroutine shell: AttributeError(""'NoneType' object has no attribute 'split'"",)
Traceback (most recent call last):
  File ""/home/crunch/projects/route/env/bin/plenum"", line 53, in <module>
    run_cli()
  File ""/home/crunch/projects/route/env/bin/plenum"", line 49, in run_cli
    looper.run(cli.shell(*commands))
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/common/looper.py"", line 251, in run
    return self.loop.run_until_complete(what)
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 466, in run_until_complete
    return future.result()
  File ""/usr/lib/python3.5/asyncio/futures.py"", line 293, in result
    raise self._exception
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
    result = coro.send(None)
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/common/looper.py"", line 242, in wrapper
    raise ex
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/common/looper.py"", line 230, in wrapper
    results.append(await coro)
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/cli/cli.py"", line 966, in shell
    self.parse(c)
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/cli/cli.py"", line 1277, in parse
    r = action(matchedVars)
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/cli/cli.py"", line 976, in _simpleAction
    self.getStatus()
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/cli/cli.py"", line 711, in getStatus
    format(Replica.getNodeName(mPrimary)))
  File ""/home/crunch/projects/route/env/lib/python3.5/site-packages/plenum/server/replica.py"", line 234, in getNodeName
    return replicaName.split("":"")[0]
AttributeError: 'NoneType' object has no attribute 'split'
`
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx0v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 4:07 AM;burdettadam;lovesh commented on Dec 19, 2016
@AdrienPensart Thanks for pointing this out. The problem is that you are entering status command too early. If you wait for nodes to connect and do the election so you see messages like Beta:1 selected primary ..., Gamma:1 selected primary ..., etc and then enter status, it should work. This issue would be resolved in the next release though, the CLI would be handling this gracefully and not crashing.;;;","01/Nov/18 4:07 AM;burdettadam;AdrienPensart commented on Dec 20, 2016
Yes, it should not crash in all cases, but don't you know if it will be faster to bootstrap plenum on localhost in the next release ?;;;","01/Nov/18 4:07 AM;burdettadam;lovesh commented on Dec 20, 2016
We might be able to make small gains in speed but i dont think much can be done in the next release, when plenum bootstraps, a leader election happens which means 4 nodes with each sending/receiving a bunch of messages to/from others before they are ready to start accepting requests from clients. But it takes about 3 seconds on my machine to bootstrap, does it take much longer on your machine?;;;",,,,,,,,,,,,,,,,,,,,,,
Running Error,INDY-1809,35021,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 4:08 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"musquash commented on Sep 29, 2016
Hey guys.

I was trying to install plenum like you explained in the introduction. Unfortunately I get an error when I try to run it.

The error message:
`(evernym) musquash@ubuntu:~$ start_plenum_node Alpha

530 DEBUG Using selector: EpollSelector
535 DEBUG Starting ledger...
Keys exists for remote role EvernymV1
Keys exists for remote role EvernymV2
Keys exists for remote role WSECU
Keys exists for remote role BIG
Keys exists for remote role RespectNetwork
540 INFO Looper shutting down now...
551 INFO Looper shut down in 0.011 seconds.
Traceback (most recent call last):
File ""/home/musquash/evernym/bin/start_plenum_node"", line 22, in 
node = Node(selfName, nodeRegistry=None, basedirpath=keepDir)
File ""/home/musquash/evernym/lib/python3.5/site-packages/plenum/server/node.py"", line 113, in init
self.poolManager.nodeReg)
File ""/home/musquash/evernym/lib/python3.5/site-packages/plenum/common/stacked.py"", line 302, in init
self._name = stackParams[""name""]
TypeError: 'NoneType' object is not subscriptable
607 DEBUG Close <_UnixSelectorEventLoop running=False closed=False debug=True>
(evernym) musquash@ubuntu:~$
`

I am using a xubuntu vm (20GB and 1 GB ram, Ubuntu 16.06)

I hope you can help me.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx13:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,"01/Nov/18 4:08 AM;burdettadam;lovesh commented on Oct 2, 2016
@musquash This error appeared because you tried to run a node with a name (Alpha) that was not present in the existing node registry. As i understand you are trying plenum out and want to run a test cluster to see if it works. We have updated our code substantially in the past few days and you can use the script generate_plenum_pool_transactions to generate a genesis transaction file for a test cluster. You can go through the Readme to see examples of this script;;;",,,,,,,,,,,,,,,,,,,,,,,,
getting started without documentation?,INDY-1810,35022,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,burdettadam,burdettadam,01/Nov/18 4:10 AM,11/Oct/19 8:42 PM,28/Oct/23 2:47 AM,11/Oct/19 8:42 PM,,,,,,0,,,,"tomdavidson commented on Sep 9, 2016
@jasonalaw Thank you for the presentation and discussion at BYU OIT today.

Using Plenum/ledger outside of Sovrin might accomplish some of your outreach goals. For example, I'm after a distributed, append only, and ordered, object repository (avoiding rdbms) to use as an event store. More docs would be nice, esspeically comparted to the some of the hyperledger project's docs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx1b:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,burdettadam,esplinr,,,,,,,,,,"06/Nov/18 5:31 PM;ashcherbakov;[~burdettadam] Can you please provide more info on what docs do we expect?
We have [https://github.com/hyperledger/indy-node/blob/master/README.md] as an index of docs including Getting Started guide: https://github.com/hyperledger/indy-sdk/blob/master/doc/getting-started/getting-started.md;;;","11/Oct/19 8:42 PM;esplinr;We know that we need more documentation, but we don't understand what this specific person needs. We have recently published additional documentation on the design of the ledger. We can raise new issues if specific additional items are needed.;;;",,,,,,,,,,,,,,,,,,,,,,,
Zeno cli name,INDY-1811,35023,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,burdettadam,burdettadam,01/Nov/18 4:12 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"SmithSamuelM commented on Feb 16, 2016
I suggest changing /scripts/cli to /scripts/zeno as the cli when installed in /usr/local/bin/cli might be ambiguous but /usr/local/bin/zeno won't be",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzx1j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),burdettadam,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to write transaction to pool ledger,INDY-1812,35007,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,mgbailey,mgbailey,mgbailey,01/Nov/18 2:44 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"The following transaction was written by a steward to the ledger at 16:30:
{code:java}
ledger node target=2bDviHYdDiTjyXYXEW92zQHEf1C1QsbFatJ6uSYuYrHh node_ip=87.98.136.246 node_port=9781 client_ip=178.32.102.66 client_port=9782 alias=NodeTwinPeek services=VALIDATOR blskey=Jrbf7k1xgkbhfKAmVXqfLLmFieGrxL1f1H6WRBZVB4Rvh8uCHGVoVzMppygH2XPLK4n1cnaBKe7zYxftgMaYXka1HLaScfsVCGqpkSa7d2hzerpcvPQMvo9TCCTP3jWb6uC9kVUHZkVqVvecMDtRkVqr3ZChUAoTM2e4UGmgqvE3Zk blskey_pop=RY3ZXV5WoHWMM631ov7ZMWoTX41Cnah4CrwQnXFrPHt49ajB8b5AjnrSDxCb9JEhC8WLVveuQMH7p6FJfoQHRaG2tR9pQLgLCXvbDozYPin4LwVzV3Wh2LNMorAtJgr3PfqxzUmFNEkbiGAbzMdBS1EXbDya9exgrLkrMLuLG1crLw
{code}
After a long delay, this was returned to the indy-cli:


{code:java}
Transaction response has not been received
{code}
I see the transaction in the node logs, at 16:30 to 16:33, but the transaction is not in the ledger. We need to determine why, and fix it. 

The ledger is attached.

The compressed logs are too large to attach, so they can be accessed here:
https://drive.google.com/file/d/1k7FqF8TZAVqlCh9kgWLXTdgDD4puzQSX/view?usp=sharing","TestNet, running 1.6.78, indy-cli client running 1.6.7",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/18 2:40 AM;mgbailey;pool_ledger.tgz;https://jira.hyperledger.org/secure/attachment/16193/pool_ledger.tgz","01/Nov/18 10:07 PM;zhigunenko.dsr;screenshot-1.png;https://jira.hyperledger.org/secure/attachment/16202/screenshot-1.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzwxz:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,mgbailey,zhigunenko.dsr,,,,,,,,,,"01/Nov/18 4:37 AM;mgbailey;Update: A different steward was able to write his node data to the ledger successfully.

 ;;;","01/Nov/18 4:30 PM;ashcherbakov;[~mgbailey] [~krw910]
 According to logs:
{code:java}
`UnauthorizedClientRequest('TK4JebQqeqq5t6x2bCwnD7 is not a steward so cannot add a new node',) while processing Request: {'signature': '3o3K9VqZAoiVMXEVph2RHVZax1ePtAFTSLZTXEBXeiEwa6w3TwBMcUqD2yCMvArfut48T5NqRGb6PtsqKq3QfXAh', 'protocolVersion': 2, 'reqId': 1541003412216431500, 'identifier': 'TK4JebQqeqq5t6x2bCwnD7', 'operation': {'dest': '2bDviHYdDiTjyXYXEW92zQHEf1C1QsbFatJ6uSYuYrHh', 'type': '0', 'data':
{'alias': 'NodeTwinPeek', 'client_port': 9782, 'blskey_pop': 'RY3ZXV5WoHWMM631ov7ZMWoTX41Cnah4CrwQnXFrPHt49ajB8b5AjnrSDxCb9JEhC8WLVveuQMH7p6FJfoQHRaG2tR9pQLgLCXvbDozYPin4LwVzV3Wh2LNMorAtJgr3PfqxzUmFNEkbiGAbzMdBS1EXbDya9exgrLkrMLuLG1crLw', 'client_ip': '178.32.102.66', 'blskey': 'Jrbf7k1xgkbhfKAmVXqfLLmFieGrxL1f1H6WRBZVB4Rvh8uCHGVoVzMppygH2XPLK4n1cnaBKe7zYxftgMaYXka1HLaScfsVCGqpkSa7d2hzerpcvPQMvo9TCCTP3jWb6uC9kVUHZkVqVvecMDtRkVqr3ZChUAoTM2e4UGmgqvE3Zk', 'services': ['VALIDATOR'], 'node_ip': '87.98.136.246', 'node_port': 9781}
}}, will reject`
  {code}
So, it looks like the transaction is sent by not a Steward.

 

 ;;;","01/Nov/18 4:48 PM;ashcherbakov;From the Node side, the Reject message has also been sent in 12 sec :
{code:java}
2018-10-31 16:30:12,703|TRACE|zstack.py|brazilC transmitting b'{""reqId"":1541003580271403400,""identifier"":""TK4JebQqeqq5t6x2bCwnD7"",""op"":""REJECT"",""reason"":""client request invalid: UnauthorizedClientRequest(\'TK4JebQqeqq5t6x2bCwnD7 is not a steward so cannot add a new node\',)""}' to b'/RIt8vcNesd.tVeyJ#@7EoaJ}6V0a$9Ww=joW.65' through listener socket {code}
We should probably check on SDK/CLI side if Rejects are shown in CLI.;;;","01/Nov/18 10:11 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.78
indy-cli                   1.6.7 

*Actual Results:*
 !screenshot-1.png|thumbnail! 
In ""happy path"" case corresponding error message has been shown.
""Transaction response has not been received"" means that response has been received too late in case the transaction has been ordered or rejected, no matter.;;;","01/Nov/18 10:21 PM;ashcherbakov;[~mgbailey] We should either close this ticket as Won't Fix, or get all the logs from all the nodes, plus CLI logs.

 ;;;","02/Nov/18 1:01 AM;mgbailey;I re-submitted the transaction on the testnet with the DID of a trust anchor and got back an error message that makes more sense than a simple time out:
{code:java}
pool(testnet):wallet(testnet_wallet):did(UJq...G15):indy> ledger node target=2bDviHYdDiTjyXYXEW92zQHEf1C1QsbFatJ6uSYuYrHh node_ip=87.98.136.246 node_port=9781 client_ip=178.32.102.66 client_port=9782 alias=NodeTwinPeek services=VALIDATOR blskey=Jrbf7k1xgkbhfKAmVXqfLLmFieGrxL1f1H6WRBZVB4Rvh8uCHGVoVzMppygH2XPLK4n1cnaBKe7zYxftgMaYXka1HLaScfsVCGqpkSa7d2hzerpcvPQMvo9TCCTP3jWb6uC9kVUHZkVqVvecMDtRkVqr3ZChUAoTM2e4UGmgqvE3Zk blskey_pop=RY3ZXV5WoHWMM631ov7ZMWoTX41Cnah4CrwQnXFrPHt49ajB8b5AjnrSDxCb9JEhC8WLVveuQMH7p6FJfoQHRaG2tR9pQLgLCXvbDozYPin4LwVzV3Wh2LNMorAtJgr3PfqxzUmFNEkbiGAbzMdBS1EXbDya9exgrLkrMLuLG1crLw Transaction has been rejected: client request invalid: InsufficientCorrectSignatures(0, 1)
{code}
so I am unable to duplicate the timeout seen by the steward. I will mark it wont fix.;;;",,,,,,,,,,,,,,,,,,,
AWS tags for pool automation AWS resources,INDY-1813,35040,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,01/Nov/18 10:07 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,devops,,,"We need to add a set of tags.

To track charges:
 * Project (might be discussed) to track AWS charges using AWS Cost Explorer

Other:   
 * Department
 * Expiry
 * Name
 * Owner
 * Purpose",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1791,,,,,INDY-1828,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx9b:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"07/Nov/18 1:12 AM;andkononykhin;*PoA*:
 # extend logic of the custom aws provisioning plugin to add 'Project' tag with value 'Pool Automation'
 # rename existent tags keys to start with capital letters
 # Set Name tag for ec2 instances and security groups as well
 # provide API to pass additional custom tags;;;","12/Nov/18 5:41 PM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1028;;;","14/Nov/18 5:47 PM;andkononykhin;*Problem reason*:

* we had no tad to filter out all resources created in scope of pool automation project, that made very hard to estimate costs (in future)
* also there was no an API to set some additional tags (like Expiry, Owner, Purpose) that might be necessary in some cases

*Changes*:

* added general tag Project with default value Indy-PA to cover all resources where it is possible
* set first letters capital for all used predefined tags
* set lowercase for all resource names
* added support for additional custom tags

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1028]

*Risk factors*:

Nothing is expected.


 *Risk*:

Low
  

*Covered with tests*:

* tests for stateful_set.py module were extended

 *Recommendations for QA*:
 * since QA for doesn't have ability to run aws related routine no validation from them are expected
 * was tested by me and [~sergey.khoroshavin] using our accounts on AWS;;;",,,,,,,,,,,,,,,,,,,,,,
validator-info default response missing key fields,INDY-1814,35055,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,esplinr,esplinr,02/Nov/18 6:21 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,1.6.78,1.6.79,validator-info,,,0,EV-CS,help-wanted,,"The default response from validator-info should include the following additional fields:
* Transaction Count on all ledgers
* Current time the tool was run, including timezone
* BLS key
* IP addresses of the node and client port
* The listing of node names that are reachable and unreachable
* Software versions of indy-node and sovrin

Goals:
* The critical information necessary to understand the health of a steward
* Brief enough that it can be easily understood in one page of text
* The default information should be useful enough that it is unusual to need to run the tool in verbose mode.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1174,INDY-967,INDY-1841,INDY-1854,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx27:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,esplinr,VladimirWork,,,,,,,,,,"29/Nov/18 3:41 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/1063

Recommendations to QA
sovrin and node should be tested separately
case with additional ledgers should be tested as well

fix for this issue affects all related issues INDY-976 INDY-1841 INDY-1854

Node version 707;;;","29/Nov/18 7:40 PM;VladimirWork;There are issues with validator-info normal mode, waiting for fix.;;;","30/Nov/18 5:24 PM;VladimirWork;Build Info:
indy-node 1.6.709

Steps to Validate:
1. Run `validator-info` against node/sovrin/sovtoken packages installed.

Actual Results:
* Transaction Count on all ledgers (/) [sovtoken ledger count is displayed as `Total 1001 Transactions: X` since it is extraneous ledger for indy-node and it is identified by id (1001) only]
* Current time the tool was run, including timezone (/)
* BLS key (/)
* IP addresses of the node and client port (/)
* The listing of node names that are reachable and unreachable (/)
* Software versions of indy-node and sovrin (/);;;","30/Nov/18 6:07 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/1066

Node v710;;;",,,,,,,,,,,,,,,,,,,,,
New instance was removed after creating,INDY-1815,35065,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,Toktar,Toktar,03/Nov/18 12:26 AM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"When adding a new node to the pool and changing the value f, a new instance is created. After enabling the logic for removing replicas on performance degradaded, a problem was found with removing a replica immediately after creation.

*Acceptance criteria:*
 * Add a test for identifying a problem
 * Correct the problem. When adding a new replica it will not be removed on performance degradation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1872,,,,,,,,,,,,,,,"07/Nov/18 12:37 AM;zhigunenko.dsr;1815.tar.gz;https://jira.hyperledger.org/secure/attachment/16229/1815.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1684,,,No,,Unset,No,,,"1|hzwxdr:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,zhigunenko.dsr,,,,,,,,,,,"06/Nov/18 6:25 PM;Toktar;Problem reason:
 -  A new replica it will be removed on performance degradation. In start replics has value 0, but it should be None

Changes:
 - Changed RevivalSpikeResistantEMAThroughputMeasurement strategy for detecting instance degradation. Now in safe start (window_size * min_cnt) this strategy return None for getting throughput value.
 - Add tests

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/963]
 * [https://github.com/hyperledger/indy-node/pull/1011]

Version:
 * indy-node 1.6.654 -master
 * indy-plenum 1.6.579  -master

Risk factors:
 - No risk

Risk:
 - Low

Covered with tests:
 * test_instances_not_degraded_on_new_instance
 * test_rsr_ema_tm_throughput_in_safe_start
 * test_rsr_ema_tm_past_windows_processed_on_get_throughput_after_safe_start

Recommendations for QA:
 * Create a docker pool for 6 nodes
 * Add a new node.
 * Send any transaction.
 * Check that new instance (which created automaticaly after adding new node) correctly ordered transaction and no replica has been removed
 * Check that master didn't degrade (view change didn't happen);;;","07/Nov/18 12:30 AM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.656 
 
 *Steps to Validate:*
1) Create a docker pool for 6 nodes
2) Add the 7th node.
3) Send any transaction.

*Expected results:*
 * New instance (which created automaticaly after adding new node) correctly ordered transaction
 * No replica has been removed
 * Master didn't degrade (view change didn't happen)

*Actual results:*
 * New instance hasn't been created - FAIL
 * Some replica has been removed - FAIL
 * View change didn't happen - OK

*Additional info:*
Logs and validator-info:  [^1815.tar.gz];;;","07/Nov/18 7:36 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/967;;;","07/Nov/18 8:33 PM;Toktar;Problem reason:
 -  A backup replica used latency value for detecting areBackupsDegraded().  This parameter is incorrect in the backup replica start and shouldn't use for backup replicas.

Changes:
 - Remove latency check from areBackupsDegraded()
 - Add test

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/967]
 * 

Version:
 * indy-node 1.6.661 -master
 * indy-plenum 1.6.583 -master

Risk factors:
 - Replicas will be removed after adding a new node

Risk:
 - Low

Covered with tests:
 * [test_replica_removing_with_backup_degraded.py|https://github.com/hyperledger/indy-plenum/pull/967/files#diff-4f7376795ddaf94804fd3efc795afe2d]

Recommendations for QA:
 * Retest previous case;;;","08/Nov/18 7:35 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.663  

*Steps to Validate:*
1) Create a docker pool for 6 nodes
2) Add the 7th node.
3) Send any transaction.

*Actual results:*
- New instance (which created automaticaly after adding new node) correctly ordered transaction
- No replica has been removed
- Master didn't degrade (view change didn't happen);;;",,,,,,,,,,,,,,,,,,,,
Investigate slowness on TestNet due to demotion,INDY-1816,35125,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ashcherbakov,ashcherbakov,06/Nov/18 6:24 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,1.6.78,,,,,0,,,,"During testing of Indy Node on the Sovrin Test Network, we experienced some slowness.

It turned out that there were 12 Nodes on STN, and 3 of them were inactive, so we had exactly n-f nodes. This can be one of the sources (quite expected) of slowness. We want to do sufficient testing to verify that we don't have a serious problem.

*Acceptance criteria*
* Test a similar environment with unavailable nodes to see if performance is acceptable.
** If not, create a ticket in IS Jira (Indy-SDK) if needed to see how can we deal with unavailable nodes more efficiently.
* Test the performance of STN to verify that current performance is acceptable.
** If not, create a ticket to create an action plan.

*Notes*
 * Also it's not yet clear whether the slowness is noticed for write or read requests.
 If we had 3 nodes unreachable, then yes, the pool is very fragile, and we require all nodes for consensus. One or more nodes might have been struggling causing a possible temporary loss in consensus. This can be the starting point for searching the logs.
 * Doug was using the tests from [https://github.com/hyperledger/indy-sdk/tree/master/vcx/wrappers/python3/demo]
 * If this is for reads, then please note, that read request is sent to 1 node only selected randomly by SDK, so if it turned out that the request is sent to an unreachable node, then response can be quite slow.
* The logs can be found at [https://drive.google.com/open?id=1ZR2pmtfL5nOhWNrkQ-F2VYDeo2EIzDk8]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-1066,,,,,IS-1075,,,,,,,,,,,,,,,"20/Nov/18 8:27 PM;VladimirWork;1816_DEBUG_LOG.txt;https://jira.hyperledger.org/secure/attachment/16299/1816_DEBUG_LOG.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxav:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,VladimirWork,,,,,,,,,,"19/Nov/18 6:41 PM;ashcherbakov;*PoA:*
 * Start indy-cli
 * get all nodes in the STN using pool ledger from https://jira.hyperledger.org/browse/INDY-1842
 * send 10 GET_SCHEMA and 10 GET_CRED_DEF reqs to each node 1 by 1
 * Calculate the average time of response for every node
 * check if we got fast reply from all the nodes
 * if not, then some nodes in STN are unavailable, and we need to investigate why;;;","19/Nov/18 10:36 PM;VladimirWork;Build Info:
STN pool
libindy \ indy-cli 1.6.7~850 master

Steps to Reproduce:
1. Run GET_NYM txn using CLI against the pool connected with one pre-ordered node for each node of the pool (13).
2. Run GET_SCHEMA txn using CLI against the pool connected with one pre-ordered node for each node of the pool (13).

Actual Results:
There is less than 1 second response time for GET_NYM txn (for nonexistent in the ledger NYM).
There is from ~1 to ~5 seconds response time for GET_SCHEMA txn (for existing in the ledger SCHEMAs with seqNo 9726 and 9738).;;;","19/Nov/18 11:16 PM;VladimirWork;If we also want to check GET_CRED_DEF txns we should know their tags and origins (maker's DIDs).;;;","20/Nov/18 8:27 PM;VladimirWork;Build Info:
STN pool
libindy \ indy-cli 1.6.7~850 master

Steps to Reproduce:
1. Run schema and cred_def getting test for each node as pre-ordered multiple times to check timing for get requests.
2. Run schema and cred_def getting test for any single node as pre-ordered with client debug logging to check action sequence and timing for each action.
3. Run schema and cred_def getting test for random five nodes except 'dativa_validator' (unreachable) to check timing for get requests.

Autotest to Reproduce:
https://github.com/VladimirWork/tests/blob/3b6bdeb984275c0d36abbd8e80ec5176a07ff853/test_misc.py#L90-L115

Actual Results:
1. We have unexpected big response time even if we send request to reachable preoredered node and we have additional 20 seconds (default timeout) if there was unreachable one in the next 4 entries of pre-ordered list tail:
{noformat}
ITERATION:  0 NODE:  NodeTwinPeek SCHEMA BUILD TIME:  0.0007428050157614052 SCHEMA SUBMIT TIME:  6.747999059967697
ITERATION:  0 NODE:  NodeTwinPeek CRED DEF BUILD TIME:  0.005250868038274348 CRED DEF SUBMIT TIME:  7.223027279949747
ITERATION:  0 NODE:  RFCU SCHEMA BUILD TIME:  0.0005116660031490028 SCHEMA SUBMIT TIME:  6.563439471006859
ITERATION:  0 NODE:  RFCU CRED DEF BUILD TIME:  0.0004822569899260998 CRED DEF SUBMIT TIME:  26.230110621021595
ITERATION:  0 NODE:  australia SCHEMA BUILD TIME:  0.00046328100143000484 SCHEMA SUBMIT TIME:  27.83221844898071
ITERATION:  0 NODE:  australia CRED DEF BUILD TIME:  0.00045420601963996887 CRED DEF SUBMIT TIME:  10.201110676978715
ITERATION:  0 NODE:  brazil SCHEMA BUILD TIME:  0.0021047089830972254 SCHEMA SUBMIT TIME:  3.969035710033495
ITERATION:  0 NODE:  brazil CRED DEF BUILD TIME:  0.004435696988366544 CRED DEF SUBMIT TIME:  1.1642644249950536
ITERATION:  0 NODE:  canada SCHEMA BUILD TIME:  0.0006781350239180028 SCHEMA SUBMIT TIME:  8.935560951998923
ITERATION:  0 NODE:  canada CRED DEF BUILD TIME:  0.0006650919676758349 CRED DEF SUBMIT TIME:  24.076956228993367
ITERATION:  0 NODE:  england SCHEMA BUILD TIME:  0.00040230300510302186 SCHEMA SUBMIT TIME:  5.816180295951199
ITERATION:  0 NODE:  england CRED DEF BUILD TIME:  0.0003891229862347245 CRED DEF SUBMIT TIME:  26.58307822298957
ITERATION:  0 NODE:  ibmTest SCHEMA BUILD TIME:  0.00041242700535804033 SCHEMA SUBMIT TIME:  5.056306323967874
ITERATION:  0 NODE:  ibmTest CRED DEF BUILD TIME:  0.0005199289880692959 CRED DEF SUBMIT TIME:  27.814586043998133
ITERATION:  0 NODE:  korea SCHEMA BUILD TIME:  0.0005641109892167151 SCHEMA SUBMIT TIME:  6.0966269439668395
ITERATION:  0 NODE:  korea CRED DEF BUILD TIME:  0.0004341330495662987 CRED DEF SUBMIT TIME:  26.55214010697091
ITERATION:  0 NODE:  lab10 SCHEMA BUILD TIME:  0.0005217199795879424 SCHEMA SUBMIT TIME:  4.680699171032757
ITERATION:  0 NODE:  lab10 CRED DEF BUILD TIME:  0.0005471630138345063 CRED DEF SUBMIT TIME:  1.5173250649822876
ITERATION:  0 NODE:  singapore SCHEMA BUILD TIME:  0.00035651400685310364 SCHEMA SUBMIT TIME:  4.467870036023669
ITERATION:  0 NODE:  singapore CRED DEF BUILD TIME:  0.0004103579558432102 CRED DEF SUBMIT TIME:  1.3965592519962229
ITERATION:  0 NODE:  virginia SCHEMA BUILD TIME:  0.0014563830336555839 SCHEMA SUBMIT TIME:  26.497049494995736
ITERATION:  0 NODE:  virginia CRED DEF BUILD TIME:  0.0003750630421563983 CRED DEF SUBMIT TIME:  6.159671655972488
ITERATION:  0 NODE:  vnode1 SCHEMA BUILD TIME:  0.0003925340133719146 SCHEMA SUBMIT TIME:  28.38528107397724
ITERATION:  0 NODE:  vnode1 CRED DEF BUILD TIME:  0.002173338958527893 CRED DEF SUBMIT TIME:  25.24589255900355
ITERATION:  0 NODE:  xsvalidatorec2irl SCHEMA BUILD TIME:  0.0005049919709563255 SCHEMA SUBMIT TIME:  24.317779342003632
ITERATION:  0 NODE:  xsvalidatorec2irl CRED DEF BUILD TIME:  0.00040826998883858323 CRED DEF SUBMIT TIME:  25.69578657101374
ITERATION:  1 NODE:  NodeTwinPeek SCHEMA BUILD TIME:  0.0006035700207576156 SCHEMA SUBMIT TIME:  26.326870853023138
ITERATION:  1 NODE:  NodeTwinPeek CRED DEF BUILD TIME:  0.0004900849889963865 CRED DEF SUBMIT TIME:  8.417115608986933
ITERATION:  1 NODE:  RFCU SCHEMA BUILD TIME:  0.0003499230369925499 SCHEMA SUBMIT TIME:  5.841705403989181
ITERATION:  1 NODE:  RFCU CRED DEF BUILD TIME:  0.0003617530455812812 CRED DEF SUBMIT TIME:  26.091539084969554
ITERATION:  1 NODE:  australia SCHEMA BUILD TIME:  0.00042970303911715746 SCHEMA SUBMIT TIME:  26.30412693496328
ITERATION:  1 NODE:  australia CRED DEF BUILD TIME:  0.0003939750022254884 CRED DEF SUBMIT TIME:  28.584586251992732
ITERATION:  1 NODE:  brazil SCHEMA BUILD TIME:  0.00037106999661773443 SCHEMA SUBMIT TIME:  6.658427752030548
ITERATION:  1 NODE:  brazil CRED DEF BUILD TIME:  0.000631252012681216 CRED DEF SUBMIT TIME:  6.67594048299361
ITERATION:  1 NODE:  canada SCHEMA BUILD TIME:  0.0006730200257152319 SCHEMA SUBMIT TIME:  26.454359997005668
ITERATION:  1 NODE:  canada CRED DEF BUILD TIME:  0.000525923038367182 CRED DEF SUBMIT TIME:  27.552015827968717
ITERATION:  1 NODE:  england SCHEMA BUILD TIME:  0.0009118529851548374 SCHEMA SUBMIT TIME:  30.226472131034825
ITERATION:  1 NODE:  england CRED DEF BUILD TIME:  0.0005074020009487867 CRED DEF SUBMIT TIME:  25.69875141500961
ITERATION:  1 NODE:  ibmTest SCHEMA BUILD TIME:  0.00042560294969007373 SCHEMA SUBMIT TIME:  6.649003875034396
ITERATION:  1 NODE:  ibmTest CRED DEF BUILD TIME:  0.00038876099279150367 CRED DEF SUBMIT TIME:  8.237650988041423
ITERATION:  1 NODE:  korea SCHEMA BUILD TIME:  0.00039756897604092956 SCHEMA SUBMIT TIME:  8.15620059001958
ITERATION:  1 NODE:  korea CRED DEF BUILD TIME:  0.00034331600181758404 CRED DEF SUBMIT TIME:  7.019242018985096
ITERATION:  1 NODE:  lab10 SCHEMA BUILD TIME:  0.00036118796560913324 SCHEMA SUBMIT TIME:  5.59081495803548
ITERATION:  1 NODE:  lab10 CRED DEF BUILD TIME:  0.0003800999838858843 CRED DEF SUBMIT TIME:  6.191560307983309
ITERATION:  1 NODE:  singapore SCHEMA BUILD TIME:  0.0015356229851022363 SCHEMA SUBMIT TIME:  5.790958840050735
ITERATION:  1 NODE:  singapore CRED DEF BUILD TIME:  0.0004955320036970079 CRED DEF SUBMIT TIME:  8.70689561899053
ITERATION:  1 NODE:  virginia SCHEMA BUILD TIME:  0.0005372230079956353 SCHEMA SUBMIT TIME:  24.28011533000972
ITERATION:  1 NODE:  virginia CRED DEF BUILD TIME:  0.00045196397695690393 CRED DEF SUBMIT TIME:  5.5334381670109
ITERATION:  1 NODE:  vnode1 SCHEMA BUILD TIME:  0.000547519011888653 SCHEMA SUBMIT TIME:  27.127038303995505
ITERATION:  1 NODE:  vnode1 CRED DEF BUILD TIME:  0.0003628379781730473 CRED DEF SUBMIT TIME:  24.95290285698138
ITERATION:  1 NODE:  xsvalidatorec2irl SCHEMA BUILD TIME:  0.000395810988266021 SCHEMA SUBMIT TIME:  6.546137069992255
ITERATION:  1 NODE:  xsvalidatorec2irl CRED DEF BUILD TIME:  0.0004190409672446549 CRED DEF SUBMIT TIME:  6.459483966056723
ITERATION:  2 NODE:  NodeTwinPeek SCHEMA BUILD TIME:  0.00038561999099329114 SCHEMA SUBMIT TIME:  5.363953482010402
ITERATION:  2 NODE:  NodeTwinPeek CRED DEF BUILD TIME:  0.0004304379690438509 CRED DEF SUBMIT TIME:  28.477671108033974
ITERATION:  2 NODE:  RFCU SCHEMA BUILD TIME:  0.00040399900171905756 SCHEMA SUBMIT TIME:  26.042088654008694
ITERATION:  2 NODE:  RFCU CRED DEF BUILD TIME:  0.0003950360114686191 CRED DEF SUBMIT TIME:  7.361472717020661
ITERATION:  2 NODE:  australia SCHEMA BUILD TIME:  0.00035825197119265795 SCHEMA SUBMIT TIME:  25.825570926012006
ITERATION:  2 NODE:  australia CRED DEF BUILD TIME:  0.00037559698102995753 CRED DEF SUBMIT TIME:  10.436239817005116
ITERATION:  2 NODE:  brazil SCHEMA BUILD TIME:  0.0003781939740292728 SCHEMA SUBMIT TIME:  23.99490367702674
ITERATION:  2 NODE:  brazil CRED DEF BUILD TIME:  0.0004229489713907242 CRED DEF SUBMIT TIME:  6.475907411018852
ITERATION:  2 NODE:  canada SCHEMA BUILD TIME:  0.00041440397035330534 SCHEMA SUBMIT TIME:  5.294093917007558
ITERATION:  2 NODE:  canada CRED DEF BUILD TIME:  0.0003922300529666245 CRED DEF SUBMIT TIME:  9.017383576952852
ITERATION:  2 NODE:  england SCHEMA BUILD TIME:  0.0003447070484980941 SCHEMA SUBMIT TIME:  7.1536829869728535
ITERATION:  2 NODE:  england CRED DEF BUILD TIME:  0.0004314139951020479 CRED DEF SUBMIT TIME:  6.70012780604884
ITERATION:  2 NODE:  ibmTest SCHEMA BUILD TIME:  0.0003784529981203377 SCHEMA SUBMIT TIME:  27.339088546985295
ITERATION:  2 NODE:  ibmTest CRED DEF BUILD TIME:  0.0004486889811232686 CRED DEF SUBMIT TIME:  8.554044977005105
ITERATION:  2 NODE:  korea SCHEMA BUILD TIME:  0.0006445790058933198 SCHEMA SUBMIT TIME:  8.09631814394379
ITERATION:  2 NODE:  korea CRED DEF BUILD TIME:  0.0005855240160599351 CRED DEF SUBMIT TIME:  27.640831064025406
ITERATION:  2 NODE:  lab10 SCHEMA BUILD TIME:  0.0004396539879962802 SCHEMA SUBMIT TIME:  5.3167560120346025
ITERATION:  2 NODE:  lab10 CRED DEF BUILD TIME:  0.0007042420329526067 CRED DEF SUBMIT TIME:  12.470109855988994
ITERATION:  2 NODE:  singapore SCHEMA BUILD TIME:  0.0003895619884133339 SCHEMA SUBMIT TIME:  7.138290948001668
ITERATION:  2 NODE:  singapore CRED DEF BUILD TIME:  0.0003913450054824352 CRED DEF SUBMIT TIME:  26.39183896803297
ITERATION:  2 NODE:  virginia SCHEMA BUILD TIME:  0.00042729696724563837 SCHEMA SUBMIT TIME:  5.164014468027744
ITERATION:  2 NODE:  virginia CRED DEF BUILD TIME:  0.0007933389861136675 CRED DEF SUBMIT TIME:  6.977192924998235
ITERATION:  2 NODE:  vnode1 SCHEMA BUILD TIME:  0.0006357830134220421 SCHEMA SUBMIT TIME:  5.26071064901771
ITERATION:  2 NODE:  vnode1 CRED DEF BUILD TIME:  0.00040973396971821785 CRED DEF SUBMIT TIME:  6.362914098019246
ITERATION:  2 NODE:  xsvalidatorec2irl SCHEMA BUILD TIME:  0.0004897679900750518 SCHEMA SUBMIT TIME:  5.905505531001836
ITERATION:  2 NODE:  xsvalidatorec2irl CRED DEF BUILD TIME:  0.0006524560158140957 CRED DEF SUBMIT TIME:  9.286057751975022
{noformat}

2. We *discard valid state proofs as invalid* during state proof check and fall into consensus reading way (see log file for more info). [^1816_DEBUG_LOG.txt] 
3. First and second items are proved by this case - we have no timeout entries (20+ seconds) if we have no unreachable nodes in the first five entries of pre-ordered list:
{noformat}
ITERATION:  0    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00071936403401196     SCHEMA SUBMIT TIME:  10.674949111999013
ITERATION:  0    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0008700769976712763     CRED DEF SUBMIT TIME:  15.243298737041187
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  1    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0004818529705516994   SCHEMA SUBMIT TIME:  15.210919820005074
ITERATION:  1    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0017988579929806292     CRED DEF SUBMIT TIME:  12.489982932980638
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  2    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00038085499545559287      SCHEMA SUBMIT TIME:  9.789947590965312
ITERATION:  2    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0005322149954736233     CRED DEF SUBMIT TIME:  10.98095233400818
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  3    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00041271501686424017      SCHEMA SUBMIT TIME:  9.11812746297801
ITERATION:  3    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.00039006100269034505    CRED DEF SUBMIT TIME:  12.334778853983153
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  4    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0015980950556695461   SCHEMA SUBMIT TIME:  8.034286240988877
ITERATION:  4    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0005512820207513869     CRED DEF SUBMIT TIME:  12.350479895947501
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  5    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0004467319813556969   SCHEMA SUBMIT TIME:  8.49566237104591
ITERATION:  5    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0007589649758301675     CRED DEF SUBMIT TIME:  8.675111969001591
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  6    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0004481080104596913   SCHEMA SUBMIT TIME:  6.685532119998243
ITERATION:  6    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0009117420413531363     CRED DEF SUBMIT TIME:  7.367479664972052
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  7    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.000632074021268636    SCHEMA SUBMIT TIME:  6.785294690984301
ITERATION:  7    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0004784189513884485     CRED DEF SUBMIT TIME:  8.227590443042573
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  8    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00046138401376083493      SCHEMA SUBMIT TIME:  7.372021807997953
ITERATION:  8    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.00039455003570765257    CRED DEF SUBMIT TIME:  8.3099751159898
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  9    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00047961296513676643      SCHEMA SUBMIT TIME:  7.599210688029416
ITERATION:  9    NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.00042169197695329785    CRED DEF SUBMIT TIME:  7.6668213860248215
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  10   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0005553200026042759   SCHEMA SUBMIT TIME:  7.3137635180028155
ITERATION:  10   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0004580419627018273     CRED DEF SUBMIT TIME:  8.760852812032681
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  11   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0003568860120140016   SCHEMA SUBMIT TIME:  9.630284291983116
ITERATION:  11   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0003572659916244447     CRED DEF SUBMIT TIME:  7.676376082992647
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  12   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00041842699283733964      SCHEMA SUBMIT TIME:  7.292544230003841
ITERATION:  12   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.000414306006859988      CRED DEF SUBMIT TIME:  9.098214313969947
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  13   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0003788279718719423   SCHEMA SUBMIT TIME:  8.337549212970771
ITERATION:  13   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0004444450023584068     CRED DEF SUBMIT TIME:  8.011969352024607
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  14   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0003712259931489825   SCHEMA SUBMIT TIME:  7.516635962005239
ITERATION:  14   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0003985959920100868     CRED DEF SUBMIT TIME:  8.016342504997738
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  15   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0004995050258003175   SCHEMA SUBMIT TIME:  6.590662899019662
ITERATION:  15   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0005240379832684994     CRED DEF SUBMIT TIME:  8.55104955600109
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  16   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.00038845796370878816      SCHEMA SUBMIT TIME:  7.00814802601235
ITERATION:  16   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0005056080408394337     CRED DEF SUBMIT TIME:  7.852209976990707
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  17   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0005048540187999606   SCHEMA SUBMIT TIME:  7.684812290011905
ITERATION:  17   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0005052060005255044     CRED DEF SUBMIT TIME:  8.639466541993897
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  18   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0004395670257508755   SCHEMA SUBMIT TIME:  6.781523112964351
ITERATION:  18   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.000595680030528456      CRED DEF SUBMIT TIME:  8.58199124597013
{""preordered_nodes"": [""australia"", ""brazil"", ""canada"", ""england"", ""korea""], ""genesis_txn"": ""/home/indy/stn_genesis""}
ITERATION:  19   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    SCHEMA BUILD TIME:  0.0004052059957757592   SCHEMA SUBMIT TIME:  6.98533082898939
ITERATION:  19   NODE:  ['australia', 'brazil', 'canada', 'england', 'korea']    CRED DEF BUILD TIME:  0.0004211320192553103     CRED DEF SUBMIT TIME:  9.060357490961906
{noformat}

;;;","21/Nov/18 12:51 AM;ashcherbakov;*The cause of the issue*
 * The Main reason
 ** Inconsistency in communication between libindy and indy-node.
 It turned out that libindy and indy-node use different values for the state keys in Schemas, Nyms and CredDefs which leads to State Proof verification failure and a lot of re-sending.
 *** After txn format migration, indy-node started using string values for txn types for Nyms, Schemas and CredDefs, while libindy still uses hex values.
 *** So, State proof validation always fails, and client sends requests one by one to every node and validates state proofs and BLS multi-sigs until it receives f+1 equal replies.

 * Additional reason
 ** It looks like there is a new (14th) Validator Node added to STN recently: *dativa_validator* which doesn't reply to read requests.
 So, if a request is sent to this node (1/14 probability), it will wait for 20 secs before sending to the next node

 * Example:
 ** Send RequestX to Node1 => Reply1 => validate BLS-multi-sig (OK) => Validate State Proof (FAIL)
 ** Send RequestX to Node2 => Reply2 => validate BLS-multi-sig (OK) => Validate State Proof (FAIL)
 ** ....
 ** _<If NodeY=unreachable node, then the next re-sent is done in 20 secs only!!!>_
 ** ....
 ** Send RequestX to NodeF => ReplyF => validate BLS-multi-sig (OK) => Validate State Proof (FAIL)
 ** Send RequestX to NodeF+1 => ReplyF+1 => validate BLS-multi-sig (OK) => Validate State Proof (FAIL)
 ** Got F+1 equal replies => SUCCESS (but too long!)

 

*How to fix*

We need to decide if we go with Option1 or Option2.

Option3 must be done in any case, but it may mitigate the slowness issue in a short term.
 * Option1: in Indy-Node
 ** What needs to be done:
 *** Change state keys to use hex values for Nyms, Schemas and ClaimDefs (that is revert it back)
 *** Create a migration to delete domain state tree (it will be restored from the ledger)
 ** Pros:
 *** No changes in libindy and other apps
 ** Cons:
 *** A need for migration and restoring State Trie from the ledger on all nodes (may take some time)
 *** A need to upgrade STN
 *** libindy will still need to deal with non-string values in keys (which are public IDs)
 * Option2: in libindy
 ** What needs to be done:
 *** Use string values instead of hex when verifying state proofs
 ** Pros:
 *** It's better to have string values there, since state key is a public ID used in many places, so it makes sense to consider this change in any case
 *** No need for STN upgrade and migration
 ** Cons:
 *** A need to update all apps including libsovtoken, Verity, Agency, Connect.me

 * Option3: in VCX
 ** What needs to be done:
 *** Cache Schemas and CredDefs and do not request them from the Ledger on each proof generation
 ** Pros:
 *** Needs to be done in any case
 *** Touches less dependencies than a fix in libindy
 ** Cons:
 *** Still requires update of a lot of apps (Verity, Agency, Connect.me)

*When to fix*
 * Option A: A hotfix ASAP (either for Option1 or Option2)
 * Option B: In the next Service Pack release next Sprint (on Dec, 07)

*Action Items*
 * Create deterministic tests to make sure that BLS multi-signature and State Proofs work as expected (asking only 1 node)

 * 
 ** it turned out that we already had such a test, but it was incorrect
 ** fixed in [https://github.com/hyperledger/indy-node/pull/1046]
 * It was decided to go with Option2+A, that is created a hotfix libindy 1.6.8 (IS-1066);;;","21/Nov/18 1:04 AM;esplinr;We will also work on Option 3 in IS-1075.;;;",,,,,,,,,,,,,,,,,,,
Improve tests for 'stateful_set' to always cleanup all temporary created test resources,INDY-1817,35138,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,andkononykhin,andkononykhin,07/Nov/18 1:04 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,devops,,,Currently 'test_stateful_set.py'  in both failure and success cases doesn't remove AWS security groups that have been created during tests running.,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1784,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzxpj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Init Indy Node should output Base58-encrypted verkey already,INDY-1818,35148,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,benjsmi,benjsmi,07/Nov/18 7:42 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,documentation,,,0,useful,,,"[In the Steward Getting Started Guide|https://docs.google.com/document/d/1AH618bj4q9U8FS1uyoIgbcvwNzaghBCQ1v44tNpZ2OU/edit#heading=h.votw8a9lan5j]

Shows that the verkey will come out looking something like this:
{quote}{{bfede8c4581f03d16eb053450d103477c6e840e5682adc67dc948a177ab8bc9b}}
{quote}
Which to the best of my knowledge is a hexlified version of the verkey.  Later on, [in the guide|https://docs.google.com/document/d/1AH618bj4q9U8FS1uyoIgbcvwNzaghBCQ1v44tNpZ2OU/edit#heading=h.eg8r1qxkvix7], to send the the client to add the Steward's node to the ledger, they must execute:
{quote}python3 -c ""from plenum.common.test_network_setup import TestNetworkSetup; print(TestNetworkSetup.getNymFromVerkey(str.encode('bfede8c4581f03d16eb053450d103477c6e840e5682adc67dc948a177ab8bc9b')))""
{quote}
If they send the hex version of their verkey, the transaction would be rejected.

I've been able to produce the Base 58 version of the verkey in Node.js with just a simple Base 58 library, decoding the hex verkey into a string and then encoding it with the library.

This way when the user gets to this step:
{quote}indy> ledger node target=<validator_verkey_in_Base58> node_ip=<validator_node_ip_address> node_port=<node_port> client_ip=<validator_client_ip_address> client_port=<client_port> alias=<validator_alias> services=VALIDATOR blskey=<validator_bls_key> blskey_pop=<validator_bls_key_pop>
{quote}
They then use this base58 key they obtained above.

Can we make it so that the base58-encrypted key comes out when you run init_indy_node?

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-209,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwyn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,benjsmi,,,,,,,,,,,"08/Nov/18 7:01 PM;ashcherbakov;Duplicates INDY-209.;;;","09/Nov/18 10:32 PM;ashcherbakov;Done in the scope of INDY-209;;;","05/Dec/18 4:30 PM;ashcherbakov;[~mgbailey] [~TechWritingWhiz]
Since the init script now outputs the keys in base58 format, we need to update the Guide.;;;",,,,,,,,,,,,,,,,,,,,,,
Exposed validator-info ... info through a web-accessible API,INDY-1819,35149,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,benjsmi,benjsmi,07/Nov/18 8:04 AM,11/May/19 11:19 AM,28/Oct/23 2:47 AM,,,,validator-info,,,0,help-wanted,quality,,"Obviously we'd want to provide security around this, but it gets somewhat cumbersome to have to exec or ssh into my node to run the validator-info command just to check to see what's going on.  I've heard there was a discussion around making the indy-cli (I'm guessing the SDK-based CLI here) capable of querying node status... and that this feature would be included in libindy, so therefore accessible to any and all apps.

But I think any Steward, in order to setup monitoring, PagerDuty, support, etc, is going to want to be able to quickly and easily get the status (FULL status) of their Node without having to rely on SSH or other methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-715,,,,,INDY-1844,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzxrr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),benjsmi,mgbailey,,,,,,,,,,,"10/Nov/18 7:09 AM;mgbailey;[~benjsmi], I posted instructions on doing this from the new indy-cli to you today. Does this satisfy this ticket?;;;","13/Nov/18 1:45 AM;benjsmi;[~mgbailey], our issues with getting the indy-cli version of this working notwithstanding, it would not satisfy this issue as far as I'm concerned.

In my view, there would be a publicly accessible port beside the validator's client port that would HTTP access that would return a JSON rendering of the contents of validator-info, minus any info you think might be too useful to an attacker. Status though, at minimum, seems like something we shouldn't need to resort to compiling our own SDK to get at though.

Even *better* would be if I could browse to that Node's IP and see a HTML dashboard that gave me a ""prettier"" rendering of the info in validator-info.;;;",,,,,,,,,,,,,,,,,,,,,,,
Node service stops during node key validation,INDY-1820,35158,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,zhigunenko.dsr,zhigunenko.dsr,07/Nov/18 5:50 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,TShirt_M,,,"*Environment:*
indy-node 1.6.78 (stable)
sovrin 1.1.30 (stable)

*Steps to Reproduce:*
1) Prepare pool with 4 Nodes only
2) Send transaction for node adding
3) Demote new nonexistent node

*Actual Results:*
All nodes stop their services with error
{code:java}
Nov 07 07:45:08 1c31f9a17571 env[7897]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/network/keep_in_touch.py"", line 45, in serviceLifecycle
Nov 07 07:45:08 1c31f9a17571 env[7897]:     self.maintainConnections()
Nov 07 07:45:08 1c31f9a17571 env[7897]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/kit_zstack.py"", line 64, in maintainConnections
Nov 07 07:45:08 1c31f9a17571 env[7897]:     missing = self.connectToMissing()
Nov 07 07:45:08 1c31f9a17571 env[7897]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/kit_zstack.py"", line 120, in connectToMissing
Nov 07 07:45:08 1c31f9a17571 env[7897]:     self.connect(name, ha=self.registry[name])
Nov 07 07:45:08 1c31f9a17571 env[7897]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/zstack.py"", line 599, in connect
Nov 07 07:45:08 1c31f9a17571 env[7897]:     publicKeyRaw) if publicKeyRaw else self.getPublicKey(name)
Nov 07 07:45:08 1c31f9a17571 env[7897]:   File ""/usr/local/lib/python3.5/dist-packages/stp_zmq/zstack.py"", line 892, in getPublicKey
Nov 07 07:45:08 1c31f9a17571 env[7897]:     raise PublicKeyNotFoundOnDisk(self.name, name)
Nov 07 07:45:08 1c31f9a17571 env[7897]: stp_core.network.exceptions.PublicKeyNotFoundOnDisk: Node2 could not get Node8's public key from disk. Make sure the keys are initialized for this r
Nov 07 07:45:08 1c31f9a17571 systemd[1]: indy-node.service: Main process exited, code=exited, status=1/FAILURE
Nov 07 07:45:08 1c31f9a17571 systemd[1]: indy-node.service: Unit entered failed state.
Nov 07 07:45:08 1c31f9a17571 systemd[1]: indy-node.service: Failed with result 'exit-code'.
Nov 07 07:45:18 1c31f9a17571 systemd[1]: indy-node.service: Service hold-off time over, scheduling restart.
Nov 07 07:45:18 1c31f9a17571 systemd[1]: Stopped Indy Node.
{code}

*Additional Info:*
Logs, ledgers and journalctl here  [^INDY-1820.tar.gz] ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/18 5:55 PM;zhigunenko.dsr;INDY-1820.tar.gz;https://jira.hyperledger.org/secure/attachment/16230/INDY-1820.tar.gz","20/Nov/18 7:29 PM;zhigunenko.dsr;indy-1820.tar.gz;https://jira.hyperledger.org/secure/attachment/16298/indy-1820.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx8n:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,zhigunenko.dsr,,,,,,,,,,,"20/Nov/18 3:45 PM;dsurnin;PR
https://github.com/hyperledger/indy-plenum/pull/981

Added exception handling for keys loading;;;","20/Nov/18 9:38 PM;dsurnin;Node version 687;;;","20/Nov/18 10:43 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.687
indy-cli                   1.6.7~854

*Steps to Validate:*
{code}
pool create p1 gen_txn_file=pool_transactions_genesis
pool connect p1
wallet create w1 key=1
wallet open w1 key=1
did new seed=000000000000000000000000Trustee1 metadata=""TRUSTEE""
did use V4SGRU86Z58d6TV7PBUe6f
did new
ledger nym did=FFWqdhX4Yg6KrKGkChdc8f verkey=~NSqQZYcsiTVxNnFEdRkhWC role=STEWARD
did use FFWqdhX4Yg6KrKGkChdc8f
ledger node target=BeLdccGjNcCDpeNCbxCFb4CfZWGxLCSM4CfGFFoZ443V client_port=9702 client_ip=10.0.0.9 alias=Node8 node_ip=10.0.0.9 node_port=9701 services=VALIDATOR blskey=oApJaiaS9tKPGxb7svdDWvRSKYKFE8mFJVQyEQsRRqUMDeFQwLqRgrtxNfCtYrunCjzFmaMjFDnSy8a5n1ZCpp3TGH8oXJ8s4i7H9Yiy5hz2uPc3ManS99iTvQE3TcBfxkuXswmR3Esy9Qi7LUjCuWGoi7fkwB3wATcLMJ5AuYr8mB blskey_pop=QqURauAZ5zhjW9yVMtGxTLLDfnAxAhavzmuSUmMosmVZLSkcYEcywHHaxi7axkpRJmsg4kmeZk1tzC4zUQrRDLc7FgcfCuTN15ub2JkynAh29x76nr7KxeHWLwMZVMyzMc5fiUfxWk2RbChitZmbbzqTVQSpjodh8TJgZX4b5p9ap7
{code}

*Actual Results:*
Transaction response has  been received, pool can write;;;",,,,,,,,,,,,,,,,,,,,,,
Run very long load test on a small local pool,INDY-1821,35162,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,07/Nov/18 7:13 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"*Acceptance Criteria:*
* Create 4-node docker pool
* Run load script with 10 NYMs per second for as long as possible
* Check graphs for memory consumption

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1774,,,,,,,,,,,,,,,,,,,,"07/Nov/18 7:27 PM;sergey.khoroshavin;Screenshot from 2018-11-07 12-18-26.png;https://jira.hyperledger.org/secure/attachment/16231/Screenshot+from+2018-11-07+12-18-26.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzzx5j:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,"07/Nov/18 7:33 PM;sergey.khoroshavin; !Screenshot from 2018-11-07 12-18-26.png|thumbnail! 
It can be seen that:
* memory consumption continues to grow even after almost week
* over time growth becomes slower
* RocksDB flushes write buffers at fixed threshold
* for some tables there is significant growth of index and filters memory

*Next things to do*
* continue load test for one more week
* add block cache metrics (this will require patching python rocksdb wrapper);;;",,,,,,,,,,,,,,,,,,,,,,,,
Limit RocksDB memory consumption,INDY-1822,35168,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,07/Nov/18 9:33 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"In the scope of INDY-1821 it was found that RockDB's readers-mem is increasing.

This looks like the main cause of slow memory increasing during long load tests.

*Acceptance criteria*
 * Find out RocksDB config to limit readers-mem
 * Find out if RocksDB wrapper keeps Python objects not-GCed
 * [Optional] Get metrics about BlockCache (it may require changes in wrapper)
 * [Optional] Explicitly set limits for BlockCache",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/18 7:36 PM;sergey.khoroshavin;Screenshot from 2018-11-21 12-25-39.png;https://jira.hyperledger.org/secure/attachment/16303/Screenshot+from+2018-11-21+12-25-39.png","21/Nov/18 7:45 PM;sergey.khoroshavin;Screenshot from 2018-11-21 12-26-35.png;https://jira.hyperledger.org/secure/attachment/16304/Screenshot+from+2018-11-21+12-26-35.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwx7r:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"10/Nov/18 1:57 AM;sergey.khoroshavin;*PoA*
* [optional] implement block cache size metric in RocksDB python wrapper
* [optional] add block cache metric to set of metrics in plenum
* explicitly set limits to block cache and max_files_open, run load test and check results
* implement a way to destroy and recreate RocksDB instances (so all memory gets dropped)
* run load test in docker with some nodes periodically resetting their RocksDB instances and check both memory consumption and GC metrics;;;","21/Nov/18 8:01 PM;sergey.khoroshavin;Due to found and fixed memory leak in scope of INDY-1747 it was decided to run plain load test in docker, with altered block_cache and max_files_open limits. During load test run in docker nodes were configured as follows:
* Node1 & 2: default config
* Node3: _max_files_open_ set to 10 (default is 5000)
* Node4: _block_cache_size_ set to 1 Mb (default is 8 Mb)

Default config (node2) vs _max_files_open_ set to 10 (node3):
 !Screenshot from 2018-11-21 12-25-39.png|thumbnail! 

Default config (node2) vs _block_cache_size_ set to 1 Mb (node4):
 !Screenshot from 2018-11-21 12-26-35.png|thumbnail! 

It can be seen that:
* even with default config memory consumption became more or less stable after first 24 hours passed
* reduced _max_files_open_ affects _estimate-table-readers-mem_ metric, however total memory consumption change was not so visible
* reduced _block_cache_size_ didn't have any visible effects
* number of objects tracked by GC is not increasing

Preliminary conclusions:
* rocksdb memory consumption becomes more or less stable after initial warmup
* most probably it will still increase during long run, but not significantly

Additional things to do:
* run load test for a couple more days to make sure that memory consumption is really stable and see if difference between node2 and node3 becomes more pronounced
* revert configuration of node4 to default and do periodic restarts to check warmup behavior with large ledger (there are already over 1.5M txns written);;;","21/Nov/18 11:45 PM;sergey.khoroshavin;Also, according to RocksDB [documentation|https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB#indexes-and-filter-blocks] there is no simple way to limit memory taken by indexes and filters. It can be either moved to block cache (so it will page out when hitting limit) or number of max open files can be limited (which we did in our test), however both cases are not recommended unless most data is cold and there is only limited number of hot keys, otherwise performance issues may appear. In fact, recommendation is the opposite - make max number of open files unlimited if possible. 

So, given that during moderate load memory consumption is very modest it makes sense to keep RocksDB configuration default for now.;;;",,,,,,,,,,,,,,,,,,,,,,
Pool stops writing during load testing against domain and pool ledgers together,INDY-1823,35171,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,VladimirWork,VladimirWork,07/Nov/18 11:13 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,TShirt_L,,,"Build Info:
indy-node 1.6.648

Steps to Reproduce:
1. Run writing load test with 10 nyms/sec.
2. Run writing load test with 1 config txn/sec from another client and stop it after 10..15 minutes.
3. Run writing load test with 1 pool txn/sec from Step 2 client.
4. Check load script output/logs/metrics.

Actual Results:
Both load scripts start throwing Timeout errors after ~250 pool txns written, ~10:25 in metrics and logs (1) pool has stopped to write any txns, ~10:45 load script was stopped manually (2), after that the whole pool was restarted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1432,,,,,INDY-1846,,,,,,,,,,,,,,,"07/Nov/18 11:13 PM;VladimirWork;INDY-1432.PNG;https://jira.hyperledger.org/secure/attachment/16233/INDY-1432.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx8v:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"07/Nov/18 11:19 PM;VladimirWork;All logs\journals\metrics are in ev@evernymr33:logs/1823.7z;;;","13/Nov/18 11:02 PM;Toktar;*PoA:*
 Consider the case when 
 Node1 has committed_pool_root_hash_1,
 Node2 has committed_pool_root_hash_2
 It's correct situation because in one moment nodes can ordered different counts of batches.
 Now Node1 is sending a commit message and using committed_pool_root_hash_1 and data from PrePrepare to sign it.
 Node2 receives the commit from Node1 and validate it with its current committed_pool_root_hash, which is equal to committed_pool_root_hash_2 and different from the one used during signing. So, pool root hashes are different, and, although commit is correct, BLS signature verification fails, Commit is considered as invalid, and this batch will never be ordered.
 In order to fix the bug, we need to do the following:
 *  Add the current uncommitted pool state root hash to ""POOL_STATE_ROOT_HASH"" to PrePrepare message (Put it before PLUGIN_FIELDS for correct plugins work. Plugins add this field in the end of PrePrepare message)
 * Extend validation of received PrePreapares to check, that POOL_STATE_ROOT_HASH is equal to the node's uncommitted state root hash. If they are not equal - raise a suspicious error.
 * Change commit BLS signature creating. Now it should use pool state root hash from PrePrepare message instead committed state root hash. We can do it because pool state root hash from PrePrepare has already been verified in PrePrepare validation.
 * Change validation of a commit BLS signature. Now it should use pool state root hash from PrePrepare message instead committed state root hash.
 * Change logging message in case with incorrect BLS signature for commit message. Add information about an incorrect Commit message
 * Change using BLS key in creating signature from committed state to using key from uncommitted state.
 * Add unit tests:
 ** test for create3PCBatchwith for full requests queues
 ** test for create3PCBatchwith for pool_ledger (signature will not be added)
 ** test for successful finish for create3PCBatch (signature added, _bls_latest_multi_sig==None) 
 ** test for update_commit with _can_process_ledger() = false
 ** test for update_commit with _bls_latest_multi_sig = false
 ** test for successful finish for update_commit (signature added) 
 ** test for validate_commit with no BLS_MULTI_SIG in pre_prepare
 ** test for validate_commit with invalid pool_state_root_hash
 ** test for validate_commit with correct pool_state_root_hash
 ** test for validate_commit with no pool_state_root_hash
 ** test for processPrePrepare with no POOL_STATE_ROOT_HASH
 ** test for processPrePrepare with invalid POOL_STATE_ROOT_HASH
 ** test for processPrePrepare with correct POOL_STATE_ROOT_HASH
 ** test for processPrePrepare for indirect order of PrePrepares (PP2, PP1, check successful process for PP1, PP2 )
 * Add integration test:

 ** All nodes receive PrePrepare1(txn1 for pool_ledger)
 ** Nodes 1, 2 ordered txn1 and nodes 3, 4 did not.
 ** All nodes  receive PrePrepare2(txn2 for domain_ledger)
 ** Nodes 3, 4 receive commits from nodes 1, 2
 ** Nodes 1, 2 ordered txn1
 ** Check that all nodes ordered txn2;;;","15/Nov/18 8:11 PM;VladimirWork;Unit and integration tests were reviewed and discussed. Fix will be implemented in scope of INDY-1846.;;;",,,,,,,,,,,,,,,,,,,,,,
"Validator on Sovrin MainNet fails to upgrade, then fails to revert.",INDY-1824,35177,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,mgbailey,mgbailey,08/Nov/18 1:42 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"The attached journalctl logs show that during yesterday's automated upgrade, the correct packages appear to have been downloaded, but the migration script failed to run. If the steward tries to manually run the migration script, he gets the same error. It appears that the package updates did not run properly, even though dpkg -l returns the correct 1.6.78 version information.

Question 1: How can we recover from this?

After the failed migration, the logs show that an attempt to roll back the node packages failed. There are no apt holds of packages on this node. 

Question 2: Correct upgrade logic to fix failed roll back.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1848,,,,,,,,,,,,,,,"08/Nov/18 1:42 AM;mgbailey;BIG_Journal_1701.txt;https://jira.hyperledger.org/secure/attachment/16234/BIG_Journal_1701.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx3b:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,mgbailey,,,,,,,,,,,"08/Nov/18 6:42 PM;ashcherbakov;I can see the following in the provided log file:
{code:java}
Nov 06 17:01:19 ip-172-16-1-25 env[29650]: 2018-11-06 17:01:19,989 | DEBUG    | node_control_tool.py ( 170) | _create_backup | Creating backup for 1.3.62
Nov 06 17:01:20 ip-172-16-1-25 env[29650]: 2018-11-06 17:01:20,036 | INFO     | migration_tool.py    (  52) | migrate | Migrating from 1.3.62 to 1.6.78 on Ubuntu
Nov 06 17:01:20 ip-172-16-1-25 env[29650]: 2018-11-06 17:01:20,037 | DEBUG    | migration_tool.py    (  54) | migrate | Found migration scripts: ['1_0_28_to_1_0_29', '1_1_37_to_1_1_38', '1_1_43_to_1_2_44', '1_2_44_to_1_2_45', '1_2_50_to_1_2_51', '1_2_51_to_1_2_52', '1_3_396_to_1_3_397', '1_3_428_to_1_3_429', '1_3_433_to_1_3_434', '1_4_500_to_1_4_501', 'disabled_1_0_29_to_1_0_28', 'helper_1_0_28_to_1_0_29', 'helper_1_1_37_to_1_1_38']
Nov 06 17:01:20 ip-172-16-1-25 env[29650]: 2018-11-06 17:01:20,038 | INFO     | migration_tool.py    (  61) | migrate | Following migrations will be applied: ['1_3_396_to_1_3_397', '1_3_428_to_1_3_429', '1_3_433_to_1_3_434', '1_4_500_to_1_4_501']
Nov 06 17:01:20 ip-172-16-1-25 env[29650]: 2018-11-06 17:01:20,038 | INFO     | migration_tool.py    (  63) | migrate | Applying migration 1_3_396_to_1_3_397
Nov 06 17:01:20 ip-172-16-1-25 env[29650]: 2018-11-06 17:01:20,038 | INFO     | migration_tool.py    (  35) | _call_migration_script | script path /usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_396_to_1_3_397.py
Nov 06 17:01:21 ip-172-16-1-25 env[29650]: Traceback (most recent call last):
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/data/migrations/deb/1_3_396_to_1_3_397.py"", line 14, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from storage.kv_store_leveldb_int_keys import KeyValueStorageLeveldbIntKeys
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/storage/kv_store_leveldb_int_keys.py"", line 1, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from storage.helper import integer_comparator
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/storage/helper.py"", line 5, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from plenum.common.config_util import getConfig
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/__init__.py"", line 73, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     setup_plugins()
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/__init__.py"", line 57, in setup_plugins
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     plugin = find_and_load_plugin(plugin_name, plugin_root, installed_packages)
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/__init__.py"", line 29, in find_and_load_plugin
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     plugin = importlib.import_module(plugin_name)
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/lib/python3.5/importlib/__init__.py"", line 126, in import_module
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     return _bootstrap._gcd_import(name[level:], package, level)
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/sovtokenfees/__init__.py"", line 4, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from sovtokenfees.messages.fields import TxnFeesField
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/sovtokenfees/messages/fields.py"", line 5, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from sovtoken.messages.fields import PublicInputsField, \
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/sovtoken/messages/fields.py"", line 4, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from sovtoken.util import decode_address_to_vk_bytes
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/sovtoken/util.py"", line 8, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from plenum.server.domain_req_handler import DomainRequestHandler
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/domain_req_handler.py"", line 15, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from plenum.server.ledger_req_handler import LedgerRequestHandler
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/ledger_req_handler.py"", line 8, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from plenum.common.ledger import Ledger
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/ledger.py"", line 5, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from ledger.ledger import Ledger as _Ledger
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:   File ""/usr/local/lib/python3.5/dist-packages/ledger/ledger.py"", line 14, in <module>
Nov 06 17:01:21 ip-172-16-1-25 env[29650]:     from storage.helper import initKeyValueStorageIntKeys
Nov 06 17:01:21 ip-172-16-1-25 env[29650]: ImportError: cannot import name 'initKeyValueStorageIntKeys' {code}
So, it looks like a Steward has Sovrin plugins installed? How is it happened on SLN? It looks like this is the source of the issue.
I think we need to 
1) remove Sovrin packages
2) try migration again;;;","08/Nov/18 6:42 PM;ashcherbakov;[~mgbailey] [~esplinr] ^^;;;","09/Nov/18 12:27 AM;mgbailey;Thanks for the guidance, [~ashcherbakov]. I have arranged to get with him later today. For question #2, why was the upgrade not rolled back properly?;;;","15/Nov/18 6:06 PM;ashcherbakov;As for the rolling back the Upgrade, this is because of base58 dependency: 1.3.62 depends on 0.2.4 while 1.6.78 on 1.0.0. base58 wasn't explicitly downgraded during upgrade, that's why roll-back failed...

I created a bug INDY-1848 to fix downgrade.

As for this ticket, is the Node repaired and functional now? Can we close the ticket? [~mgbailey]?;;;","16/Nov/18 12:47 AM;mgbailey;[~ashcherbakov], yes, thank you. Please close the ticket.;;;",,,,,,,,,,,,,,,,,,,,
Production-ready Docker images for the End User,INDY-1825,35193,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,benjsmi,benjsmi,08/Nov/18 6:21 AM,08/Nov/18 6:22 AM,28/Oct/23 2:47 AM,,,,,,,0,Usability,,,"I realize there are Dockerfiles scattered throughout the various Indy repositories, but this proposal is to actually have an official (signed) Dockerhub repository for/from Hyperledger Indy.  Officially sanctioned images.

Separate image builds/streams for:
 * ""base"" aka the dependencies for all Indy componentry
 * ""node"" for a potential Steward
 * ""cli"" for clients
 * ""sdk"" as a base for folks wanting to incorporate the Indy SDK into their apps

Note that I am *not* referring here to ""you can get this up and running by following this 20 page guide and debugging the mistakes that happen along the way.""

I'm talking the level of image that you get from one of the core or OSS-contributed libraries on Dockerhub to where the image is _minimized_ (meaning only the production dependencies are included) and streamlined.

Basically, imho, a user should be able to say
{quote}$ docker run -it hyperledger-indy/node 
{quote}
And have a basically configured node up and running. Now, obviously they will need to customize it beyond that and it won't connect to a network, but my point is that they shouldn't need to build images themselves.

Similarly, it'd be wonderful if a user could go:
{quote}$ docker run -it hyperledger-indy/cli 
{quote}
And get the indy> prompt, ready to use his/her wallet to start running transactions.

Similar to INDY-1794, it would be nice if I want to run a Node on the Sovrin network, I should be able to run:
{quote}{{$ docker run -it hyperledger-indy/validator:mainnet}}
{quote}
Which should contain the build with the package levels required to comply/function on the Provisional network.

It looks like maybe some work was started on this here:

[https://github.com/hyperledger/indy-node/blob/master/ci/ubuntu.dockerfile]

But I guess to me it would be way more useful if Dockerhub's free CI (Travis) hosting was used to automatically push out the streams for these builds for available for all to pull and use in docker, docker-compose, kubernetes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1794,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzxzj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),benjsmi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to securely automate SSH authenticity checking,INDY-1826,35200,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,08/Nov/18 5:54 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,devops,,,"Currently pool_create playbook provisions hosts and provides ssh keys to reach them.  Initially these hosts are unknown, thus ssh does host checks which requires user interaction and usually would happen during pool_install playbook. It is not the desired behavior and should be automated in a secure way. 

 Useful links:
 * [https://stackoverflow.com/questions/32297456/how-to-ignore-ansible-ssh-authenticity-checking]
 * [https://stackoverflow.com/questions/23074412/how-to-set-host-key-checking-false-in-ansible-inventory-file]
 * [https://docs.ansible.com/ansible/latest/user_guide/intro_getting_started.html#host-key-checking]
 * [https://man.openbsd.org/ssh-keyscan]
 * [https://blog.0xbadc0de.be/archives/300]

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx9j:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"09/Nov/18 2:13 PM;andkononykhin;*PoA*:
 # add tasks to perform *ssh-keyscan* which would add host keys to:
 ** _aws_inventory_dir/.ssh/known_hosts_: to make an inventory portable to other environments
 ** (option 1) _~/.ssh/known_host_s: to skip ssh host checks in pool_install.yml playbook
 ** (options 2) use ssh option _UserKnownHostsFile=aws_inventory_dir/.ssh/known_hosts_
 # if option 1 is the choice: use _ssh-keygen -R_ to remove keys at tear-down phase
 # (optionally) check how it works for docker/vagrant and turn off ssh hosts checks if needed;;;","12/Nov/18 8:01 PM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1029;;;","14/Nov/18 5:51 PM;andkononykhin;*Problem reason*:
 * aws provisioned hosts keys were unknown for ssh clients and required user interaction during first attempt to reach them breaking automated workflow of Ansible scripts

*Changes*:
 * added logic of scanning hosts for their ssh keys and putting them to custom known_hosts file
 * ansible ssh connections were pointed to that known_hosts file

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1029]

*Risk factors*:

Nothing is expected.


 *Risk*:

Low
  

*Covered with tests*:

no, tested manually


 *Recommendations for QA*:
 * since QA for doesn't have ability to run aws related routine no validation from them are expected
 * was tested by me and [~sergey.khoroshavin] using our accounts on AWS;;;",,,,,,,,,,,,,,,,,,,,,,
Need to fix 'aws_manage' playbook that fails when inventory directory is not specified,INDY-1827,35201,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,08/Nov/18 6:01 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,There is an infinite loop in variable expansion logic in aws_manage's defaults which appears only if inventory directory is not defined (e.g. not specified as command line argument for pool_create playbook),,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1792,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzzx5r:",,,,Unset,Unset,Ev 18.22,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"08/Nov/18 8:30 PM;andkononykhin;*Problem reason:*

an infinite loop in variable expansion logic in aws_manage role defaults led to failures when no inventory directory was specified

*Changes:*

Fixed variables relationship

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1019]

*Risk factors:*

Nothing is expected.

*Risk:*

Low

*Recommendations for QA:*
 * since QA for doesn't have ability to run aws related routine no validation from them are expected
 * was tested locally by simply running
{code:java}
ansible-playbook pool_create.yml {code}
that shoudn't fail at the very beginning;;;",,,,,,,,,,,,,,,,,,,,,,,,
Need to add Names to AWS ec2 instances and security groups,INDY-1828,35206,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,08/Nov/18 6:13 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Currently AWS ec2 instances and security groups are not provided with Name (which is a tag).

 

*PoA*:
 # add Name tag to instances and groups basing on other tags: Project, Namespace, Role etc.

 ",Currently AWS EC2 instances and security gorups are only tagged but Name fields are empty. Need to improve since it seems badly.,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1813,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx9r:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"12/Nov/18 5:42 PM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1028;;;","14/Nov/18 5:47 PM;andkononykhin;*Problem reason*:

* _Name_ tag was missed for ec2 instances and security groups

*Changes*:

* set missed Name tags for mentioned resources basing on project, namespace and role


*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1028]

*Risk factors*:

Nothing is expected.


 *Risk*:

Low
  

*Covered with tests*:

* tests for stateful_set.py module were extended

 *Recommendations for QA*:
 * since QA for doesn't have ability to run aws related routine no validation from them are expected
 * was tested by me and [~sergey.khoroshavin] using our accounts on AWS;;;",,,,,,,,,,,,,,,,,,,,,,,
Load and performance testing of GET_TXN,INDY-1829,35217,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,alphax0111,ashcherbakov,ashcherbakov,08/Nov/18 8:09 PM,10/Nov/20 6:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"As of now, GET_TXN doesn't support State (Audit) Proofs, and requires to be sent to all nodes and waiting for f+1 equal replies.

GET_TXN may be used a lot in some cases, so we need to evaluate max performance the pool can handle with GET_TXN.

*Acceptance criteria*
 * Run load tests sending GET_TXN, find out max performance and evaluate stability
 ** Start with sending 10 GET_TX per sec
 ** Increment the load sequentially",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/18 5:01 PM;zhigunenko.dsr;image-2018-11-29-11-01-17-298.png;https://jira.hyperledger.org/secure/attachment/16327/image-2018-11-29-11-01-17-298.png","06/Dec/18 10:18 PM;zhigunenko.dsr;image-2018-12-06-16-17-56-413.png;https://jira.hyperledger.org/secure/attachment/16362/image-2018-12-06-16-17-56-413.png","07/Dec/18 11:01 PM;zhigunenko.dsr;image-2018-12-07-17-01-30-296.png;https://jira.hyperledger.org/secure/attachment/16365/image-2018-12-07-17-01-30-296.png","07/Dec/18 11:39 PM;zhigunenko.dsr;image-2018-12-07-17-39-04-739.png;https://jira.hyperledger.org/secure/attachment/16367/image-2018-12-07-17-39-04-739.png","07/Dec/18 11:42 PM;zhigunenko.dsr;image-2018-12-07-17-42-16-687.png;https://jira.hyperledger.org/secure/attachment/16366/image-2018-12-07-17-42-16-687.png","19/Nov/18 5:48 PM;zhigunenko.dsr;screenshot-1.png;https://jira.hyperledger.org/secure/attachment/16278/screenshot-1.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-781,,,No,,Unset,No,,,"1|hzwx0n:",,,,Unset,Unset,EV 18.24,Ev 18.25,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,zhigunenko.dsr,,,,,,,,,,,"19/Nov/18 5:51 PM;zhigunenko.dsr;*Actual Results:*
 * get_nym
 indy-node 1.6.700 (25 nodes)
 domain ledger - 110k, payment ledger - 154k

fact performance (total from 25 nodes): up to 7300 read/sec (potentially up to 8200 and more in case of bigger client number)
 * get_txn (domain)
 indy-node 1.6.703 (25 nodes)
 domain ledger - 10k

fact performance (total from 25 nodes): up to 250 txn/sec (current SDK connection sustentation policy leads to dropping new connections after 500 already existing)
!image-2018-11-29-11-01-17-298.png|width=100%, height=100%!
 
 * get_txn (payment)
 fact performance (total from 25 nodes): up to 400 read/sec
Step-by-step increasing
 !image-2018-12-06-16-17-56-413.png|width=100%, height=100%!
 !image-2018-12-07-17-39-04-739.png|width=100%, height=100%!
avg_connected_clients_num  
----
*Numerous client load*
 !image-2018-12-07-17-01-30-296.png|width=100%, height=100%!
 !image-2018-12-07-17-42-16-687.png|width=100%, height=100%!
avg_connected_clients_num 

 * get_txn (pool)
 fact performance (total from 25 nodes): TDB

*Additional Info:*
For getting maximum performance load must be increased slowly. Otherwise instant initial spike (with no view_change) will create too many timeouts;;;",,,,,,,,,,,,,,,,,,,,,,,,
Research Stellar Protocol,INDY-1830,35218,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,08/Nov/18 8:16 PM,12/Feb/20 2:08 AM,28/Oct/23 2:47 AM,12/Feb/20 2:08 AM,,2.0,,,,0,,,,"[https://www.stellar.org/papers/stellar-consensus-protocol.pdf]

Find out if Stellar protocol looks good for Indy and is better than RBFT.
 Timebox the effort to 2 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community. Discussion should include:
 ** Pros and Cons
 ** Characteristics of current production implementations of the protocol
 * A recommendation on whether to research more or kill the proposal
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqe9",,,,Unset,Unset,Ev-Node 20.01,Ev-Node 20.2,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"29/Jan/20 7:01 PM;ashcherbakov;Research results are tracked here: https://docs.google.com/document/d/1pme6fIpo3FcB7SLpxyyf28yOV5DOU27zgu3brZdvXpY/edit#;;;","04/Feb/20 12:57 AM;ashcherbakov;First-priority questions:
 * Liveness:
 ** Does every node need to change quorum slices if the network got stuck?
 ** Is stuck state unstable?
 * Leader:
 ** Is it selected in every slot/ballot?
 ** Does it depend on quorum slices so that nodes with equal quorums select leaders deterministicly?
 * Is it possible to support custom transaction?
 * Is it possible to get rid of FEEs?
 * Is it possible to pay FEEs in other currencies (not lumens / Stellar tokens)?
 * Is Stellar with symmetric quorum slices same as PBFT-like protocols? 
 * What deployment options do we have, 
 ** Use existing Stellar nodes
 ** Create a dedicated federation of Sovrin nodes on existing code
 ** Create a dedicated federation of Sovrin nodes on forked/enhanced code with support of Indy txns
* How many nodes do we need in deployment, and how quorum slices should look like?
** symmetric quorum slices
** asymmetric quorum slices 
* How can we organize nodes joining and leaving the pool?



 ;;;","06/Feb/20 2:44 AM;sergey.khoroshavin;Glossary:
* *Liveness* - property of system that _something good eventually happens_
* *Stuck network* - network failing to satisfy *liveness* condition
* *Stable stuck state* - stuck state which is persistent once entered, and requires manual intervention to make progress
* *Unstable stuck state* - stuck state which has a chance of disappearing without manual intervention given long enough time

Answers to first-priority questions:
* Liveness:
** Does every node need to change quorum slices if the network got stuck?
*** [ALMOST CERTAIN] *No.* It looks like stuck state can result only if selected quorum slices are unsafe, and amount of nodes which need to adjust their quorum slices to make network safe can easily be less than amount of all (or even 2/3) nodes.
** Is stuck state unstable?
*** [ALMOST CERTAIN] *Yes, it is unstable*, if quorum sets are safe. Nodes can neutralize stuck ballots indefinitely and try again, while nomination protocol tries to converge, and nomination protocol should eventually converge unless we miss something.
** Can stuck state happen if less than F (F_Liveness) nodes are malicious/crashed? 
*** [ALMOST CERTAIN] *It doesn't seem so*, however if quorums are asymmetric and network is large it could be quite hard to calculate minimum possible dispensable set (in other words - set of nodes that cannot affect consensus if all other nodes are live and honest).
*** [NOT CERTAIN] If there is some malicious scheduler that reorders inter-node messages over majority of network at his will, then *it might be possible* that network become stuck. I've seen mentions of such attack in randomized BFT protocols like Honeybadger (and they do have protection against it through common coin based on threshold crypto), as well as studies of Avalanche protocol. _We did not construct such an attack against Stellar protocol, and not sure if it is really possible - more research is needed here._
* Leader:
** Is it selected in every slot/ballot?
*** [CERTAIN] *Yes, it does,* however if nomination protocol fails to converge on first round next leader is selected in addition to existing one, and eventually all nodes in a given quorum set could become leaders at the same time.
** Does it depend on quorum slices so that nodes with equal quorums select leaders deterministicly?
*** [CERTAIN] *Yes,* nodes having equal quorum sets will select same leaders
* Is it possible to support custom transaction?
** [ALMOST CERTAIN] *No*, unless we fork Stellar code. It is possible to write custom data, but it is not possible to add custom validation.
* Is it possible to get rid of FEEs?
** [ALMOST CERTAIN] *No,* at least not without making our own fully independent deployment. Also code selecting transaction set for block is quite dependent on fees, especially when there is high load.
* Is it possible to pay FEEs in other currencies (not lumens / Stellar tokens)?
** [CERTAIN] *No,* documentation explicitely says that fees must be payed in lumens. It is possible to create tokens (""custom assets"" in stellar terminology) though, but they need to be traded for lumens before becoming usable as fees.
* Is Stellar with symmetric quorum slices has same safety and liveness properties as PBFT-like protocols?
** [ALMOST-ALMOST CERTAIN] *Yes,* it seems like they share safety and liveness properties with PBFT if all nodes select their quorum sets as >2/3 of all validator nodes. Also, it looks like DoS strategy for PBFT-like protocols when it is sufficient to shoot down only one node (current leader) at one time is not possible in Stellar, since it allows multiple leaders and grows set of current leaders as long as nomination at some slot fails to converge, however it also seems like shooting down 2-3 leaders can still severely slow down network.
* What deployment options do we have
** [CERTAIN] *Create a dedicated federation of Sovrin nodes on forked code with support of Indy txns*
* How many nodes do we need in deployment?
** [CERTAIN] I'd use *25* as initial guess, but it really depends on performance and diversity requirements, as well as performance of Stellar protocol itself.
* How quorum slices should look like?
** [CERTAIN] *Symmetric is preferred* (there are some works proving that no asymmetric quorums can be more reliable than symmetric), however it might not be possible depending on answer for next question.
* How can we organize nodes joining and leaving the pool?
** [OPTION 1] Force all nodes (using special transaction, similar to existing node promotion/demotion mechanic in Indy) to have same quorum set, consisting of all validator nodes with threshold of 2/3. However, this rejects many properties of Stellar, including possibility to effectively demote some bad validators despite network being stuck.
** [OPTION 2] Provide recommended quorum set through some side channel (like sovrin.org website?), but let stewards decide themselves whether they agree to apply it to configuration of their nodes. This makes it possible to repair stuck network much easier, however it adds risks that stewards could end up with some really unsafe configuration, since real quorum set configuration is up to them (and this is what production Stellar network seems to struggle with).;;;","12/Feb/20 2:08 AM;sergey.khoroshavin;Final results of research are available [here|https://docs.google.com/document/d/1pme6fIpo3FcB7SLpxyyf28yOV5DOU27zgu3brZdvXpY];;;",,,,,,,,,,,,,,,,,,,,,
Research Honeybadger protocol,INDY-1831,35219,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,08/Nov/18 8:17 PM,16/Jan/20 11:22 PM,28/Oct/23 2:47 AM,,,2.0,,,,0,,,,"[https://eprint.iacr.org/2016/199.pdf]

Find out if Honeybadger protocol looks good for Indy and is better than RBFT.
 Timebox the effort to 2 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community. Discussion should include:
 ** Pros and Cons
 ** Characteristics of current production implementations of the protocol
 ** Rough estimate of work required to bring implement Stellar in Plenum
 * A recommendation on whether to research more or kill the proposal
 * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwx4f:2rzm",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Research Stellar Implementation,INDY-1832,35220,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,08/Nov/18 8:19 PM,12/Feb/20 2:09 AM,28/Oct/23 2:47 AM,12/Feb/20 2:09 AM,,2.0,,,,0,,,,"[https://github.com/stellar/stellar-core]

We need to find out how stable Stellar Implementation is.
 Timebox the effort to 4 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** Characteristics of current deployments in production usage
 ** Current project roadmap
 ** Health of the open source community / level of investment
 ** Rough estimate of work required to bring Stellar to the same level as Plenum
 ** Rough understanding of performance
 ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Stellar
 * A recommendation on whether to research more or kill the proposal
 * [|https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i41ismqei",,,,Unset,Unset,Ev-Node 20.2,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"06/Feb/20 11:35 PM;ashcherbakov;Questions to be answered:
- How does writing to the ledger work?
- How does reading from the ledger work?
- How easy is it to implement tokens on top of Stellar?
- How easy is it to implement Indy transactions on top of Stellar?
- Is it possible to send Indy transactions to Stellar network without any FEE in Stellar tokens?
- How big Stellar community is? How responsive the community is?
- How many known bugs/issues are in Stellar?
- What is a quality of Stellar code? 
- How Stellar code is tested? What types of tests are there?
- What is Stellar performance?
- What is Stellar roadmap?
;;;","12/Feb/20 2:09 AM;sergey.khoroshavin;Final results of research are available [here|https://docs.google.com/document/d/1pme6fIpo3FcB7SLpxyyf28yOV5DOU27zgu3brZdvXpY];;;",,,,,,,,,,,,,,,,,,,,,,,
Research Honeybadger implementation,INDY-1833,35221,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,08/Nov/18 8:21 PM,16/Jan/20 11:22 PM,28/Oct/23 2:47 AM,,,2.0,,,,0,,,,"[https://github.com/poanetwork/hbbft]

We need to find out how stable Honeybadger Implementation is.
 Timebox the effort to 4 engineering days.

Acceptance Criteria
 * Brief summary of findings evaluating for review by Richard, Nathan, Daniel, Jason, and the broader community.
 Discussion should include:
 ** Pros and Cons
 ** Characteristics of current deployments in production usage
 ** Current project roadmap
 ** Health of the open source community / level of investment
 ** Rough estimate of work required to bring Honeybadger to the same level as Plenum
 ** Rough understanding of performance
 ** Rough understanding of the difficulty of migrating from the existing Plenum to an implementation based on Honeybadger
 * A recommendation on whether to research more or kill the proposal
 * Put findings to [https://docs.google.com/spreadsheets/d/1-GuJuew1oUvnlzU7gBPZkF5Ongu92voojPHAPc-pUu8/edit#gid=1455070692]

Things to consider:
 * Research how clients are authorized by ledger
 * How does this relate to work with observers
 * How much better is performance
 * How many nodes would be practical (50? 500? 5000?)
 * Get feedback from community
 * Notice: One of PoC goals should be smooth upgrade path
 * Notice: it would be great to be possible to make the client-to-node communication use A2A. We could even go one step further and do the same thing from node to node, though this might be harder.

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1691,,,No,,Unset,No,,,"1|hzwx4f:2rzm4",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not enough information about upgrade in journalctl,INDY-1834,35223,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ozheregelya,ozheregelya,08/Nov/18 11:05 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,TShirt_S,,,"*Environment:*
 indy-node 1.6.631 -> 1.6.661 -> 1.6.662
 docker pool of 10 nodes

*Steps to Reproduce:*
 1. Setup the pool with indy-node 1.6.631.
 2. Upgrade it to indy-node 1.6.661.
 => Pool successfully upgraded, following information is shown in journalctl:
{code:java}
Nov 07 16:01:33 69b5c1da28c9 env[83]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Nov 07 16:05:03 69b5c1da28c9 env[83]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Nov 07 16:06:59 69b5c1da28c9 env[83]: + deps='indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661'
Nov 07 16:06:59 69b5c1da28c9 env[83]: + '[' -z 'indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661' ']'
Nov 07 16:06:59 69b5c1da28c9 env[83]: + echo 'Try to donwload indy version indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661'
Nov 07 16:06:59 69b5c1da28c9 env[83]: Try to donwload indy version indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661
Nov 07 16:06:59 69b5c1da28c9 env[83]: + apt-get -y update
Nov 07 16:07:00 69b5c1da28c9 env[83]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Nov 07 16:07:00 69b5c1da28c9 env[83]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 07 16:07:00 69b5c1da28c9 env[83]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Nov 07 16:07:00 69b5c1da28c9 env[83]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Nov 07 16:07:02 69b5c1da28c9 env[83]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 07 16:07:02 69b5c1da28c9 env[83]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 07 16:07:07 69b5c1da28c9 env[83]: Fetched 107 kB in 7s (14.9 kB/s)
Nov 07 16:07:17 69b5c1da28c9 env[83]: Reading package lists...
Nov 07 16:07:18 69b5c1da28c9 env[83]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661
Nov 07 16:07:28 69b5c1da28c9 env[83]: Reading package lists...
Nov 07 16:07:30 69b5c1da28c9 env[83]: Building dependency tree...
Nov 07 16:07:30 69b5c1da28c9 env[83]: Reading state information...
Nov 07 16:07:32 69b5c1da28c9 env[83]: indy-anoncreds is already the newest version (1.0.32).
Nov 07 16:07:32 69b5c1da28c9 env[83]: python3-dateutil is already the newest version (2.6.1).
Nov 07 16:07:32 69b5c1da28c9 env[83]: python3-indy-crypto is already the newest version (0.4.5).
Nov 07 16:07:32 69b5c1da28c9 env[83]: python3-timeout-decorator is already the newest version (0.4.0).
Nov 07 16:07:32 69b5c1da28c9 env[83]: The following held packages will be changed:
Nov 07 16:07:32 69b5c1da28c9 env[83]:   indy-node indy-plenum
Nov 07 16:07:32 69b5c1da28c9 env[83]: The following packages will be upgraded:
Nov 07 16:07:32 69b5c1da28c9 env[83]:   indy-node indy-plenum
Nov 07 16:07:35 69b5c1da28c9 env[83]: 2 upgraded, 0 newly installed, 0 to remove and 15 not upgraded.
Nov 07 16:07:35 69b5c1da28c9 env[83]: Need to get 1385 kB of archives.
Nov 07 16:07:35 69b5c1da28c9 env[83]: After this operation, 101 kB of additional disk space will be used.
Nov 07 16:07:35 69b5c1da28c9 env[83]: Get:1 https://repo.sovrin.org/deb xenial/master amd64 indy-node amd64 1.6.661 [592 kB]
Nov 07 16:07:36 69b5c1da28c9 env[83]: Get:2 https://repo.sovrin.org/deb xenial/master amd64 indy-plenum amd64 1.6.582 [793 kB]
Nov 07 16:07:36 69b5c1da28c9 env[83]: Fetched 1385 kB in 3s (438 kB/s)
Nov 07 16:07:36 69b5c1da28c9 env[83]: Download complete and in download only mode
Nov 07 16:07:36 69b5c1da28c9 env[83]: + ret=0
Nov 07 16:07:36 69b5c1da28c9 env[83]: + '[' 0 -ne 0 ']'
Nov 07 16:07:36 69b5c1da28c9 env[83]: + '[' '' = '' ']'
Nov 07 16:07:36 69b5c1da28c9 env[83]: + echo 'Stop indy-node'
Nov 07 16:07:36 69b5c1da28c9 env[83]: Stop indy-node
Nov 07 16:07:36 69b5c1da28c9 env[83]: + systemctl stop indy-node
Nov 07 16:07:36 69b5c1da28c9 systemd[1]: Stopping Indy Node...
Nov 07 16:07:37 69b5c1da28c9 systemd[1]: Stopped Indy Node.
Nov 07 16:07:37 69b5c1da28c9 env[83]: + echo 'Run indy upgrade to indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661'
Nov 07 16:07:37 69b5c1da28c9 env[83]: Run indy upgrade to indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661
Nov 07 16:07:37 69b5c1da28c9 env[83]: + apt-get -y --allow-downgrades --allow-change-held-packages --reinstall install indy-anoncreds=1.0.32 python3-indy-crypto=0.4.5 python3-dateutil=2.6.1 indy-plenum=1.6.582 python3-timeout-decorator=0.4.0 indy-node=1.6.661
Nov 07 16:07:44 69b5c1da28c9 env[83]: Reading package lists...
Nov 07 16:07:46 69b5c1da28c9 env[83]: Building dependency tree...
Nov 07 16:07:46 69b5c1da28c9 env[83]: Reading state information...
Nov 07 16:07:47 69b5c1da28c9 env[83]: The following held packages will be changed:
Nov 07 16:07:47 69b5c1da28c9 env[83]:   indy-node indy-plenum
Nov 07 16:07:47 69b5c1da28c9 env[83]: The following packages will be upgraded:
Nov 07 16:07:47 69b5c1da28c9 env[83]:   indy-node indy-plenum
Nov 07 16:07:49 69b5c1da28c9 env[83]: 2 upgraded, 0 newly installed, 4 reinstalled, 0 to remove and 15 not upgraded.
Nov 07 16:07:49 69b5c1da28c9 env[83]: Need to get 248 kB/1634 kB of archives.
Nov 07 16:07:49 69b5c1da28c9 env[83]: After this operation, 101 kB of additional disk space will be used.
Nov 07 16:07:49 69b5c1da28c9 env[83]: Get:1 https://repo.sovrin.org/deb xenial/master amd64 indy-anoncreds amd64 1.0.32 [40.9 kB]
Nov 07 16:07:49 69b5c1da28c9 env[83]: Get:2 https://repo.sovrin.org/deb xenial/master amd64 python3-dateutil amd64 2.6.1 [195 kB]
Nov 07 16:07:50 69b5c1da28c9 env[83]: Get:3 https://repo.sovrin.org/deb xenial/master amd64 python3-indy-crypto amd64 0.4.5 [6012 B]
Nov 07 16:07:50 69b5c1da28c9 env[83]: Get:4 https://repo.sovrin.org/deb xenial/master amd64 python3-timeout-decorator amd64 0.4.0 [6698 B]
Nov 07 16:07:50 69b5c1da28c9 env[83]: debconf: delaying package configuration, since apt-utils is not installed
Nov 07 16:07:50 69b5c1da28c9 env[83]: Fetched 248 kB in 2s (114 kB/s)
Nov 07 16:07:51 69b5c1da28c9 env[83]: [614B blob data]
Nov 07 16:07:51 69b5c1da28c9 env[83]: Preparing to unpack .../indy-anoncreds_1.0.32_amd64.deb ...
Nov 07 16:07:51 69b5c1da28c9 env[83]: Unpacking indy-anoncreds (1.0.32) over (1.0.32) ...
Nov 07 16:07:51 69b5c1da28c9 env[83]: Preparing to unpack .../indy-node_1.6.661_amd64.deb ...
Nov 07 16:07:52 69b5c1da28c9 env[83]: Unpacking indy-node (1.6.661) over (1.6.631) ...
Nov 07 16:07:53 69b5c1da28c9 env[83]: Preparing to unpack .../indy-plenum_1.6.582_amd64.deb ...
Nov 07 16:07:54 69b5c1da28c9 env[83]: Unpacking indy-plenum (1.6.582) over (1.6.564) ...
Nov 07 16:07:57 69b5c1da28c9 env[83]: Preparing to unpack .../python3-dateutil_2.6.1_amd64.deb ...
Nov 07 16:07:57 69b5c1da28c9 env[83]: Unpacking python3-dateutil (2.6.1) over (2.6.1) ...
Nov 07 16:07:58 69b5c1da28c9 env[83]: Preparing to unpack .../python3-indy-crypto_0.4.5_amd64.deb ...
Nov 07 16:07:58 69b5c1da28c9 env[83]: Unpacking python3-indy-crypto (0.4.5) over (0.4.5) ...
Nov 07 16:07:58 69b5c1da28c9 env[83]: Preparing to unpack .../python3-timeout-decorator_0.4.0_amd64.deb ...
Nov 07 16:07:58 69b5c1da28c9 env[83]: Unpacking python3-timeout-decorator (0.4.0) over (0.4.0) ...
Nov 07 16:07:59 69b5c1da28c9 env[83]: Setting up indy-anoncreds (1.0.32) ...
Nov 07 16:08:01 69b5c1da28c9 env[83]: Setting up python3-indy-crypto (0.4.5) ...
Nov 07 16:08:01 69b5c1da28c9 env[83]: Setting up python3-dateutil (2.6.1) ...
Nov 07 16:08:03 69b5c1da28c9 env[83]: Setting up indy-plenum (1.6.582) ...
Nov 07 16:08:15 69b5c1da28c9 env[83]: Setting up python3-timeout-decorator (0.4.0) ...
Nov 07 16:08:15 69b5c1da28c9 env[83]: Setting up indy-node (1.6.661) ...
Nov 07 16:08:16 69b5c1da28c9 env[83]: /var/lib/dpkg/info/indy-node.postinst: line 132: /etc/supervisor/indy-node.conf: No such file or directory
Nov 07 16:08:22 69b5c1da28c9 env[83]: + ret=0
Nov 07 16:08:22 69b5c1da28c9 env[83]: + '[' 0 -ne 0 ']'
Nov 07 16:08:23 69b5c1da28c9 env[83]: + '[' '' = '' ']'
Nov 07 16:08:23 69b5c1da28c9 env[83]: + systemctl daemon-reload
Nov 07 16:08:23 69b5c1da28c9 systemd[1]: Reloading.
Nov 07 16:08:24 69b5c1da28c9 env[83]: + systemctl reset-failed
Nov 07 16:08:24 69b5c1da28c9 env[83]: + echo 'Starting indy-node'
Nov 07 16:08:24 69b5c1da28c9 env[83]: Starting indy-node
Nov 07 16:08:24 69b5c1da28c9 env[83]: + systemctl start indy-node
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Started Indy Node.
Nov 07 16:08:24 69b5c1da28c9 env[83]: + echo 'Restarting agent'
Nov 07 16:08:24 69b5c1da28c9 env[83]: Restarting agent
Nov 07 16:08:24 69b5c1da28c9 env[83]: + systemctl restart indy-node-control
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Stopping Indy Node...
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Stopping Service for upgrade of existing Indy Node and other operations...
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Stopped Service for upgrade of existing Indy Node and other operations.
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Stopped Indy Node.
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Started Indy Node.
Nov 07 16:08:24 69b5c1da28c9 systemd[1]: Started Service for upgrade of existing Indy Node and other operations.
{code}
3. On part of the nodes change sovrin repo from master to RC to get next upgrade failed.
 4. Upgrade indy-node to 1.6.662 version.

Actual Results:
 Upgrade completed on part of the nodes. 
 In journalctl of successfully upgraded nodes:
{code:java}
Nov 08 13:22:08 21a0845c5ffc systemd[1]: Stopping Indy Node...
Nov 08 13:22:09 21a0845c5ffc systemd[1]: Stopped Indy Node.
Nov 08 13:22:28 21a0845c5ffc systemd[1]: Reloading.
Nov 08 13:22:42 21a0845c5ffc systemd[1]: Reloading.
Nov 08 13:22:44 21a0845c5ffc systemd[1]: Reloading.
Nov 08 13:23:06 21a0845c5ffc systemd[1]: Reloading.
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Started Indy Node.
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Stopping Service for upgrade of existing Indy Node and other operations...
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Stopping Indy Node...
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Stopped Indy Node.
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Stopped Service for upgrade of existing Indy Node and other operations.
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Started Service for upgrade of existing Indy Node and other operations.
Nov 08 13:23:07 21a0845c5ffc systemd[1]: Started Indy Node.
{code}
In journalctl of failed nodes:
{code:java}
Nov 08 13:21:31 5088f23c2c74 env[1606]: Upgrade from 1.6.661 to 1.6.662 failed: command upgrade_indy_node ""indy-node=1.6.662"" returned 1
Nov 08 13:21:31 5088f23c2c74 env[1606]: Trying to rollback to the previous version command upgrade_indy_node ""indy-node=1.6.662"" returned 1
Nov 08 13:22:51 5088f23c2c74 systemd[1]: Stopping Indy Node...
Nov 08 13:22:52 5088f23c2c74 systemd[1]: Stopped Indy Node.
Nov 08 13:23:16 5088f23c2c74 systemd[1]: Reloading.
Nov 08 13:23:19 5088f23c2c74 systemd[1]: Reloading.
Nov 08 13:23:21 5088f23c2c74 systemd[1]: Reloading.
Nov 08 13:23:22 5088f23c2c74 systemd[1]: Reloading.
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Started Indy Node.
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Stopping Indy Node...
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Stopping Service for upgrade of existing Indy Node and other operations...
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Stopped Indy Node.
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Stopped Service for upgrade of existing Indy Node and other operations.
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Started Service for upgrade of existing Indy Node and other operations.
Nov 08 13:23:23 5088f23c2c74 systemd[1]: Started Indy Node.
{code}
*Expected Results:*
 Need to specify reason of upgrade failure in journalctl in case of failed upgrade and to put more information about successful upgrade.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx93:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,ozheregelya,,,,,,,,,,,"09/Nov/18 4:41 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/1024

first Node version with fix if v670;;;","15/Nov/18 7:07 PM;ozheregelya;Environment:
indy-node 1.6.675 -> sovrin 1.1.89 -> sovrin 1.1.90
docker pool

Steps to Validate:
1. Setup the pool of 10 nodes with indy-node 1.6.675 and sovrin 1.1.62 (last not pinned version).
2. Send pool upgrade transaction to sovrin 1.1.89 (depending on indy-node 1.6.675).
=> Upgrade completed successfully, information about upgrade logged to journalctl.
3. Send pool upgrade transaction to sovrin 1.1.90 (depending on indy-node 1.6.678).
=> Upgrade failed due to INDY-1850, information about problem logged to journalctl.

Actual Results:
Information about successful upgrade:
{code:java}
Nov 14 16:42:05 8fcd6b2e84ed env[4169]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Nov 14 16:42:06 8fcd6b2e84ed env[4169]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Nov 14 16:42:06 8fcd6b2e84ed env[4169]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 14 16:42:06 8fcd6b2e84ed env[4169]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Nov 14 16:42:06 8fcd6b2e84ed env[4169]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Nov 14 16:42:08 8fcd6b2e84ed env[4169]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 14 16:42:08 8fcd6b2e84ed env[4169]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 14 16:42:28 8fcd6b2e84ed env[4169]: Reading package lists...
Nov 14 16:42:31 8fcd6b2e84ed env[4169]: Building dependency tree...
Nov 14 16:42:31 8fcd6b2e84ed env[4169]: Reading state information...
Nov 14 16:42:32 8fcd6b2e84ed env[4169]: 30 packages can be upgraded. Run 'apt list --upgradable' to see them.
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: at
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: indy-anoncreds
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: indy-node
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: indy-plenum
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: iptables
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: libindy-crypto
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: libsodium18
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: python3-indy-crypto
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: sovrin
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: + deps=sovrin=1.1.89
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: + '[' -z sovrin=1.1.89 ']'
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: + echo 'Try to donwload indy version sovrin=1.1.89'
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: Try to donwload indy version sovrin=1.1.89
Nov 14 16:43:39 8fcd6b2e84ed env[4169]: + apt-get -y update
Nov 14 16:43:40 8fcd6b2e84ed env[4169]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Nov 14 16:43:40 8fcd6b2e84ed env[4169]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 14 16:43:41 8fcd6b2e84ed env[4169]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Nov 14 16:43:41 8fcd6b2e84ed env[4169]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Nov 14 16:43:43 8fcd6b2e84ed env[4169]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 14 16:43:43 8fcd6b2e84ed env[4169]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 14 16:44:00 8fcd6b2e84ed env[4169]: Reading package lists...
Nov 14 16:44:01 8fcd6b2e84ed env[4169]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovrin=1.1.89
Nov 14 16:44:15 8fcd6b2e84ed env[4169]: Reading package lists...
Nov 14 16:44:17 8fcd6b2e84ed env[4169]: Building dependency tree...
Nov 14 16:44:17 8fcd6b2e84ed env[4169]: Reading state information...
Nov 14 16:44:20 8fcd6b2e84ed env[4169]: The following held packages will be changed:
Nov 14 16:44:20 8fcd6b2e84ed env[4169]:   sovrin
Nov 14 16:44:20 8fcd6b2e84ed env[4169]: The following packages will be upgraded:
Nov 14 16:44:20 8fcd6b2e84ed env[4169]:   sovrin
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: 1 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: Need to get 10.4 kB of archives.
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: After this operation, 0 B of additional disk space will be used.
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: Get:1 https://repo.sovrin.org/deb xenial/master amd64 sovrin amd64 1.1.89 [10.4 kB]
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: Fetched 10.4 kB in 2s (4932 B/s)
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: Download complete and in download only mode
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: + ret=0
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: + '[' 0 -ne 0 ']'
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: + '[' '' = '' ']'
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: + echo 'Stop indy-node'
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: Stop indy-node
Nov 14 16:44:23 8fcd6b2e84ed env[4169]: + systemctl stop indy-node
Nov 14 16:44:23 8fcd6b2e84ed systemd[1]: Stopping Indy Node...
Nov 14 16:44:24 8fcd6b2e84ed systemd[1]: Stopped Indy Node.
Nov 14 16:44:24 8fcd6b2e84ed env[4169]: + echo 'Run indy upgrade to sovrin=1.1.89'
Nov 14 16:44:24 8fcd6b2e84ed env[4169]: Run indy upgrade to sovrin=1.1.89
Nov 14 16:44:24 8fcd6b2e84ed env[4169]: + apt-get -y --allow-downgrades --allow-change-held-packages --reinstall install sovrin=1.1.89
Nov 14 16:44:32 8fcd6b2e84ed env[4169]: Reading package lists...
Nov 14 16:44:34 8fcd6b2e84ed env[4169]: Building dependency tree...
Nov 14 16:44:34 8fcd6b2e84ed env[4169]: Reading state information...
Nov 14 16:44:35 8fcd6b2e84ed env[4169]: The following held packages will be changed:
Nov 14 16:44:35 8fcd6b2e84ed env[4169]:   sovrin
Nov 14 16:44:35 8fcd6b2e84ed env[4169]: The following packages will be upgraded:
Nov 14 16:44:35 8fcd6b2e84ed env[4169]:   sovrin
Nov 14 16:44:35 8fcd6b2e84ed env[4169]: debconf: delaying package configuration, since apt-utils is not installed
Nov 14 16:44:36 8fcd6b2e84ed env[4169]: 1 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.
Nov 14 16:44:36 8fcd6b2e84ed env[4169]: Need to get 0 B/10.4 kB of archives.
Nov 14 16:44:36 8fcd6b2e84ed env[4169]: After this operation, 0 B of additional disk space will be used.
Nov 14 16:44:36 8fcd6b2e84ed env[4169]: [614B blob data]
Nov 14 16:44:36 8fcd6b2e84ed env[4169]: Preparing to unpack .../sovrin_1.1.89_amd64.deb ...
Nov 14 16:44:36 8fcd6b2e84ed env[4169]: Unpacking sovrin (1.1.89) over (1.1.62) ...
Nov 14 16:44:37 8fcd6b2e84ed env[4169]: Setting up sovrin (1.1.89) ...
Nov 14 16:44:40 8fcd6b2e84ed env[4169]: + ret=0
Nov 14 16:44:40 8fcd6b2e84ed env[4169]: + '[' 0 -ne 0 ']'
Nov 14 16:44:42 8fcd6b2e84ed env[4169]: + '[' '' = '' ']'
Nov 14 16:44:42 8fcd6b2e84ed env[4169]: + systemctl daemon-reload
Nov 14 16:44:42 8fcd6b2e84ed systemd[1]: Reloading.
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: + systemctl reset-failed
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: + echo 'Starting indy-node'
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: Starting indy-node
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: + systemctl start indy-node
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Started Indy Node.
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: + echo 'Restarting agent'
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: Restarting agent
Nov 14 16:44:43 8fcd6b2e84ed env[4169]: + systemctl restart indy-node-control
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Stopping Indy Node...
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Stopping Service for upgrade of existing Indy Node and other operations...
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Stopped Service for upgrade of existing Indy Node and other operations.
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Stopped Indy Node.
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Started Indy Node.
Nov 14 16:44:43 8fcd6b2e84ed systemd[1]: Started Service for upgrade of existing Indy Node and other operations.
Nov 14 16:45:16 8fcd6b2e84ed env[5196]: indy-anoncreds was already set on hold.
Nov 14 16:45:16 8fcd6b2e84ed env[5196]: indy-plenum was already set on hold.
Nov 14 16:45:16 8fcd6b2e84ed env[5196]: indy-node was already set on hold.
Nov 14 16:45:16 8fcd6b2e84ed env[5196]: python3-indy-crypto was already set on hold.
Nov 14 16:45:16 8fcd6b2e84ed env[5196]: libindy-crypto was already set on hold.
Nov 14 16:45:16 8fcd6b2e84ed env[5196]: sovrin set on hold.
{code}

Information about failed upgrade:
{code:java}
Nov 14 16:47:03 8fcd6b2e84ed env[5196]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Nov 14 16:47:04 8fcd6b2e84ed env[5196]: Hit:1 http://security.ubuntu.com/ubuntu xenial-security InRelease
Nov 14 16:47:04 8fcd6b2e84ed env[5196]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 14 16:47:04 8fcd6b2e84ed env[5196]: Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Nov 14 16:47:04 8fcd6b2e84ed env[5196]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Nov 14 16:47:06 8fcd6b2e84ed env[5196]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 14 16:47:06 8fcd6b2e84ed env[5196]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 14 16:47:12 8fcd6b2e84ed env[5196]: Fetched 216 kB in 8s (25.0 kB/s)
Nov 14 16:47:25 8fcd6b2e84ed env[5196]: Reading package lists...
Nov 14 16:47:27 8fcd6b2e84ed env[5196]: Building dependency tree...
Nov 14 16:47:28 8fcd6b2e84ed env[5196]: Reading state information...
Nov 14 16:47:28 8fcd6b2e84ed env[5196]: 30 packages can be upgraded. Run 'apt list --upgradable' to see them.
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: at
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: indy-anoncreds
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: indy-node
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: indy-plenum
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: iptables
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: libindy-crypto
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: libsodium18
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: python3-indy-crypto
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: sovrin
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: + deps=sovrin=1.1.90
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: + '[' -z sovrin=1.1.90 ']'
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: + echo 'Try to donwload indy version sovrin=1.1.90'
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: Try to donwload indy version sovrin=1.1.90
Nov 14 16:48:36 8fcd6b2e84ed env[5196]: + apt-get -y update
Nov 14 16:48:37 8fcd6b2e84ed env[5196]: Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Nov 14 16:48:37 8fcd6b2e84ed env[5196]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 14 16:48:37 8fcd6b2e84ed env[5196]: Hit:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease
Nov 14 16:48:37 8fcd6b2e84ed env[5196]: Hit:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease
Nov 14 16:48:39 8fcd6b2e84ed env[5196]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 14 16:48:39 8fcd6b2e84ed env[5196]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 14 16:48:43 8fcd6b2e84ed env[5196]: Get:7 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [104 kB]
Nov 14 16:48:43 8fcd6b2e84ed env[5196]: Get:8 http://security.ubuntu.com/ubuntu xenial-security/universe amd64 Packages [506 kB]
Nov 14 16:48:44 8fcd6b2e84ed env[5196]: Get:9 http://security.ubuntu.com/ubuntu xenial-security/multiverse amd64 Packages [4027 B]
Nov 14 16:48:46 8fcd6b2e84ed env[5196]: Fetched 721 kB in 9s (73.5 kB/s)
Nov 14 16:48:59 8fcd6b2e84ed env[5196]: Reading package lists...
Nov 14 16:48:59 8fcd6b2e84ed env[5196]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovrin=1.1.90
Nov 14 16:49:12 8fcd6b2e84ed env[5196]: Reading package lists...
Nov 14 16:49:15 8fcd6b2e84ed env[5196]: Building dependency tree...
Nov 14 16:49:15 8fcd6b2e84ed env[5196]: Reading state information...
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: Some packages could not be installed. This may mean that you have
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: requested an impossible situation or if you are using the unstable
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: distribution that some required packages have not yet been created
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: or been moved out of Incoming.
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: The following information may help to resolve the situation:
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: The following packages have unmet dependencies:
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: sovrin : Depends: indy-node (= 1.6.678) but 1.6.675 is to be installed
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: E: Unable to correct problems, you have held broken packages.
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + ret=100
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + '[' 100 -ne 0 ']'
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + echo 'Failed to obtain sovrin=1.1.90'
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: Failed to obtain sovrin=1.1.90
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + exit 1
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: Upgrade from 1.1.89 to 1.1.90 failed: Command 'upgrade_indy_node ""sovrin=1.1.90""' returned non-zero exit status 1
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: Trying to rollback to the previous version Command 'upgrade_indy_node ""sovrin=1.1.90""' returned non-zero exit status 1{code}
 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Enable PreViewChange Strategy ,INDY-1835,35240,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,09/Nov/18 4:59 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,,,,"A PreViewChange strategy was implemented in the scope of INDY-1687 to speed-up and improve stability of the view change. It's currently disabled by default.

*Acceptance criteria*
 * Make sure that it doesn't have any side-affects
 * Do more load testing (with regular view changes) with strategy enabled:
 ** do 10 NYMs per sec 
 ** check that most of view changes are quite fast
 ** check that view changes don't lead to broken ledgers (all ledger are in sync eventually)
 ** enable the strategy by default",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/18 5:46 PM;anikitinDSR;image.png;https://jira.hyperledger.org/secure/attachment/16270/image.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwwy7:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,,,,,,,,,,,"12/Nov/18 4:49 PM;ashcherbakov;We need to get the following information from load test runs:
1) How many txns pool was able to write (on every node)
2) Whether the pool continues to write txns
3) Was there any failures in jornalctl; did the node restart by OOM or other errors
4) How many view changes completed
5) How much time each View Change took
6) Whether View Changes were finished by last_prepared_certificate, or by catchup
7) Whether the ledger is broken on any nodes
8) Provide graphs for metrics;;;","12/Nov/18 9:13 PM;anikitinDSR;As a result of logs investigation:
 1. On all nodes except 4, 13 and 19 wrote the same count of transaction, about 2.2 million. Nodes 4, 13 and 19 had OOM errors and was not restarted (process is alive, but there is no any activities from it, like log writing or etc)
 2. All the time, during 3 days, pool wrote transactions
 3. There is several OOM errors in journalctl. Also, all of the nodes were restarted after this and continued to accept transactions after initial catchup, except 3 nodes
 4. Before stopping the test 106 view_change rounds were completed (view change was forced each 1 hour).
 5. Average time for view_change was about several seconds (5-10). Also there was some long view_change, like 5 minutes. The reason why is that half of prepared certificate was (33, 2640) and from other was (33, 2641). In that case we will do catchup rounds until timeout estimation.
 6. The most of view_changes were completed by last_prepared_certificate (103 of 106).
 7. There was some ""Incorrect state tree errors"", but it was after node restarted by OOM error.
 Other comments:
 - it looks, like VCStartStrategy works fine, because of the most of view changes were completed by last_prepared_certificate.
 - time, which was spent for strategy working is quite small in comparison of all view_change time (about 100 ms vs 5 min for whole view_change);;;","15/Nov/18 5:46 PM;anikitinDSR;During log analizing was found a bug, which associated with batch handling. There is a lot of errors in logs, like
 ""discard batch ""<some message>"" because Expected object or value"", that means, that error was raised on string 'pi' or 'po' serialization. In this case, all batch will be discarded with some other messages inside, like Propogate, for example. This behaivour was catched in a previous logs too, therefore it was not impacted by view_change strategy. For this issue was created a bug INDY-1849.

As for strategy working:
 As i described above, the most of view_changes were completed by last_prepared_certificate. Also there is some spikes on throughput graph that means that strategy can give some real chance to ordering before view_change starting or get the most suitable last_prepared_certificate.
 !image.png|thumbnail!;;;",,,,,,,,,,,,,,,,,,,,,,
Enable Clear Request Queue strategy,INDY-1836,35241,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,ashcherbakov,ashcherbakov,09/Nov/18 5:03 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"A sanity check to remove old requests from queues was implemented in the scope of INDY-1780. It's disabled by default.

*Acceptance criteria*
 * Make sure that it doesn't have any side-affects
 * Do load testing with enabled strategy
 * Enable it by default",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1898,,,,,,,,,,,,,,,"06/Dec/18 7:57 PM;VladimirWork;1836_15node_1case.png;https://jira.hyperledger.org/secure/attachment/16359/1836_15node_1case.png","06/Dec/18 7:57 PM;VladimirWork;1836_1node_1case.png;https://jira.hyperledger.org/secure/attachment/16358/1836_1node_1case.png","06/Dec/18 7:57 PM;VladimirWork;1836_25node_1case.png;https://jira.hyperledger.org/secure/attachment/16360/1836_25node_1case.png","10/Dec/18 7:15 PM;VladimirWork;INDY-1836_1.PNG;https://jira.hyperledger.org/secure/attachment/16372/INDY-1836_1.PNG","13/Dec/18 11:53 PM;sergey-shilov;INDY-1836_1.png;https://jira.hyperledger.org/secure/attachment/16418/INDY-1836_1.png","13/Dec/18 12:07 AM;sergey-shilov;INDY-1836_1.png;https://jira.hyperledger.org/secure/attachment/16405/INDY-1836_1.png","11/Dec/18 7:16 PM;VladimirWork;INDY-1836_1_2nd.PNG;https://jira.hyperledger.org/secure/attachment/16388/INDY-1836_1_2nd.PNG","14/Dec/18 6:04 PM;VladimirWork;INDY-1836_1_3rd.PNG;https://jira.hyperledger.org/secure/attachment/16422/INDY-1836_1_3rd.PNG","10/Dec/18 7:15 PM;VladimirWork;INDY-1836_25.PNG;https://jira.hyperledger.org/secure/attachment/16373/INDY-1836_25.PNG","13/Dec/18 11:53 PM;sergey-shilov;INDY-1836_25.png;https://jira.hyperledger.org/secure/attachment/16419/INDY-1836_25.png","13/Dec/18 12:07 AM;sergey-shilov;INDY-1836_25.png;https://jira.hyperledger.org/secure/attachment/16406/INDY-1836_25.png","11/Dec/18 7:17 PM;VladimirWork;INDY-1836_25_2nd.PNG;https://jira.hyperledger.org/secure/attachment/16389/INDY-1836_25_2nd.PNG","14/Dec/18 6:04 PM;VladimirWork;INDY-1836_25_3rd.PNG;https://jira.hyperledger.org/secure/attachment/16423/INDY-1836_25_3rd.PNG","12/Jan/19 7:53 PM;anikitinDSR;node1.png;https://jira.hyperledger.org/secure/attachment/16531/node1.png","12/Jan/19 7:55 PM;anikitinDSR;node15.png;https://jira.hyperledger.org/secure/attachment/16532/node15.png","19/Dec/18 9:12 PM;ozheregelya;prod1836node13_18_12_2018.png;https://jira.hyperledger.org/secure/attachment/16456/prod1836node13_18_12_2018.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwvif:00006",,,,Unset,Unset,Ev 18.23,EV 18.24,Ev 18.25,Ev 19.1,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,ozheregelya,sergey-shilov,VladimirWork,zhigunenko.dsr,,,,,,,"30/Nov/18 6:46 PM;sergey-shilov;The list of proposed tests:

* Drop request on propagate phase (primary and non-primary):
** Propagate is sent
** Propagates from other nodes are delayed more than drop time out
** Request is dropped before finalization
** Then Propagates are received
** Request should be ordered

* Drop finalized request before Pre-Prepare (non-primary only):
** Request is finalized
** Pre-Prepare from primary is delayed more than drop time out
** Request is dropped
** Then Pre-Prepare, Prepares and Commits are received
** Request should be ordered

* Drop finalized request before Prepares and Commits received (primary and non-primary):
** Request is finalized, Pre-Prepare:
*** is sent (primary)
*** is received (non-primary)
** Prepares and Commits from other nodes are delayed more than drop time out
** Request is dropped
** Then Prepares and Commits are received
** Request should be ordered

* Drop finalized request before Commits received (primary and non-primary):
** Request is finalized, Pre-Prepare:
*** is sent (primary)
*** is received (non-primary)
** Prepares are received
** Commits from other nodes are delayed more than drop time out
** Request is dropped
** Then Commits are received
** Request should be ordered
;;;","05/Dec/18 5:25 PM;sergey-shilov;The list of currently implemented tests:
* Drop request on propagate phase (primary and non-primary)
* Drop finalized request before Prepares and Commits received (primary and non-primary)
* Drop finalized request before Commits received (primary and non-primary)

PR:
https://github.com/hyperledger/indy-plenum/pull/984;;;","05/Dec/18 10:36 PM;sergey-shilov;*Problem state / reason:*

For now there is a memory leak in propagator's requests queue in case of requests that are not committed for a very long time. We need a mechanism to drop such requests.

*Changes:*

Two new time outs were added:
1) A time out for propagate phase (i.e. before request is finalised)
2) A time out for ordering phase (i.e. after request is finalised)

The first time out starts when the request is added to the propagator's requests queue. When request is finalised (i.e. quorum of propagates is reached) the second time out starts and the first time out is not used further.
Request is dropped from the requests queue if one of these time outs exceeded.

This is a kind of a sanity check, not a regular action. Proposed time outs are very high (10 hours for propagates phase and 20 hours for ordering phase) so we do not expect dropping of requests from the propagate's requests queue on a regular basis.

*Committed into:*

    https://github.com/hyperledger/indy-plenum/pull/984
    https://github.com/hyperledger/indy-node/pull/1078
    indy-node 1.6.717-master

*Risk factors:*

    Unstable behaviour of the node if requests are dropped.

*Risk:*

    Medium

*Recommendations for QA:*

Firstly, enable this strategy by setting in the indy config file:
_OUTDATED_REQS_CHECK_ENABLED = True_
_OUTDATED_REQS_CHECK_INTERVAL = 10_

The run acceptance mix twice:
* the first run with parameters:
** PROPAGATES_PHASE_REQ_TIMEOUT = 60  # seconds
** ORDERING_PHASE_REQ_TIMEOUT = 72000  # seconds
* the second run with parameters:
** PROPAGATES_PHASE_REQ_TIMEOUT = 36000  # seconds
** ORDERING_PHASE_REQ_TIMEOUT = 120  # seconds
;;;","06/Dec/18 8:43 PM;VladimirWork;Build Info:
indy-node 1.6.718

Steps to Reproduce:
1. Run production load without fees with this parameters:
{noformat}
OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 10
PROPAGATES_PHASE_REQ_TIMEOUT = 60 # seconds
ORDERING_PHASE_REQ_TIMEOUT = 72000 # seconds
{noformat}
2. Check client output/logs/metrics.

Actual Results:
Writing client has the next stats: {code:java}Time 55390.65 Clients 0/10 Sent: 547931 Succ: 416561 Failed: 128300 Nacked: 3070 Rejected: 0{code}
Pool has stopped ordering txns and some less than f nodes have View 1 (i.e. 1st node). Metrics from 1st, 15th and 25th nodes: !1836_1node_1case.png|thumbnail!  !1836_15node_1case.png|thumbnail!  !1836_25node_1case.png|thumbnail! 

All nodes' logs\validator-info\detailed client output are in ev@evernymr33:logs/1836_1case.tar.gz and ev@evernymr33:logs/1836_1case_writing_client_data.tar.gz
;;;","07/Dec/18 9:18 PM;sergey-shilov;New indy-node master build: *1.6.725-master*

Please run tests described in
https://jira.hyperledger.org/browse/INDY-1836?focusedCommentId=54375&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-54375;;;","10/Dec/18 7:15 PM;VladimirWork;Build Info:
indy-node 1.6.725

Steps to Reproduce:
1. Run production load without fees with this parameters:
{noformat}
OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 10
PROPAGATES_PHASE_REQ_TIMEOUT = 60 # seconds
ORDERING_PHASE_REQ_TIMEOUT = 72000 # seconds
{noformat}
2. Check client output/logs/metrics.

Actual Results:
Pool has stopped ordering txns at ~219k domain txns written.  !INDY-1836_1.PNG|thumbnail!  !INDY-1836_25.PNG|thumbnail! 

All logs and metrics are in ev@evernymr33:logs/1836_08_12_2018.tar.gz and ev@evernymr33:logs/1836_08_12_2018_metrics.tar.gz;;;","11/Dec/18 7:17 PM;VladimirWork;Build Info:
indy-node 1.6.725

Steps to Reproduce:
1. Run production load without fees with this parameters:
{noformat}
OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 10
PROPAGATES_PHASE_REQ_TIMEOUT = 36000 # seconds
ORDERING_PHASE_REQ_TIMEOUT = 120 # seconds
{noformat}
2. Check client output/logs/metrics.

Actual Results:
Pool has written ~200k domain txns sucessfully and has continued ordering until load test stopping. !INDY-1836_1_2nd.PNG|thumbnail!  !INDY-1836_25_2nd.PNG|thumbnail! 

All logs and metrics are in ev@evernymr33:logs/1836_11_12_2018.tar.gz and ev@evernymr33:logs/1836_11_12_2018_metrics.tar.gz;;;","13/Dec/18 12:08 AM;sergey-shilov;Seems like we need to re-test the first run. Our persistent pool works faster than we thought and no one time out on propagate phase occurred during normal work. After several working hours all nodes went out of disk space and then time outs started triggering while the pool was inoperable.
So we need to reduce propagates phase time out and solve disk space problem before test run.
!INDY-1836_1.png|thumbnail!  !INDY-1836_25.png|thumbnail! 

Proposed parameters:

OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 10
PROPAGATES_PHASE_REQ_TIMEOUT = 5 # seconds
ORDERING_PHASE_REQ_TIMEOUT = 72000 # seconds;;;","13/Dec/18 11:56 PM;sergey-shilov;Seems like we need to re-test the second run too, no one ordering time out occurred during normal work, the pool works pretty fast!
 We see time outs at the end of the test run, but they happen due to started view change by ""Primary disconnected"" reason. Primary was disconnected as it could not bind its' listener as address was already in use when it tried to restart the client stack. Client stack restart was triggered on all nodes due to unexpected spike in clients connections, but four nodes (including primary) could not bind their listeners in 1 second.
 !INDY-1836_1.png|thumbnail! !INDY-1836_25.png|thumbnail!

So there are two action items here:
 # Increase interval between bind re-tries to 1 second to have 5 seconds overall for trying to bind
 # Increase primary disconnected time out (2 seconds is very short time out for production network)

Proposed parameters:

OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 10
PROPAGATES_PHASE_REQ_TIMEOUT = 36000 # seconds
ORDERING_PHASE_REQ_TIMEOUT = 7 # seconds;;;","14/Dec/18 6:05 PM;VladimirWork;Steps to Reproduce:
1. Run production load without fees with this parameters:
{noformat}
OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 10
PROPAGATES_PHASE_REQ_TIMEOUT = 5 # seconds
ORDERING_PHASE_REQ_TIMEOUT = 72000 # seconds
{noformat}
2. Check client output/logs/metrics.

Actual Results: Pool has written 100k+ domain txns sucessfully and has continued ordering until load test stopping.  !INDY-1836_1_3rd.PNG|thumbnail!  !INDY-1836_25_3rd.PNG|thumbnail! 

All logs and metrics are in ev@evernymr33:logs/1836_13_12_2018.tar.gz and ev@evernymr33:logs/1836_13_12_2018_metrics.tar.gz;;;","14/Dec/18 6:42 PM;sergey-shilov;We need to do the following test: emulation of stopped ordering replica.

Test parameters:

OUTDATED_REQS_CHECK_ENABLED = True
OUTDATED_REQS_CHECK_INTERVAL = 2
PROPAGATES_PHASE_REQ_TIMEOUT = 3
ORDERING_PHASE_REQ_TIMEOUT = 120
REPLICAS_REMOVING_WITH_DEGRADATION = None
REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None

Before load is started it is needed to stop some non-master primary (for example, Node3) and leave it stopped during whole test.
;;;","15/Dec/18 12:52 AM;VladimirWork;Logs and metrics from the last run described above are in ev@evernymr33:logs/1836_14_12_2018.tar.gz and ev@evernymr33:logs/1836_14_12_2018_metrics.tar.gz;;;","18/Dec/18 10:11 PM;sergey-shilov;Please re-test the case described in

https://jira.hyperledger.org/browse/INDY-1836?focusedCommentId=54799&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-54799

with a new indy-node 1.6.738-master.;;;","19/Dec/18 9:14 PM;ozheregelya;Retested with indy-node 1.6.738.
 Pool stopped writing after ~12 hours of load.
 !prod1836node13_18_12_2018.png|thumbnail!

Logs and metrics: s3://qanodelogs/indy-1836/prod_load_18_12_2018
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1836/prod_load_18_12_2018/ /home/ev/logs/indy-1836/prod_load_18_12_2018/

UPD: Node3 was not stopped before load by mistake.;;;","20/Dec/18 8:14 AM;ozheregelya;Logs for correct test (without metrics and validator-info history):
 s3://qanodelogs/indy-1836/logs-only-19-12-2018

Logs for second run (with metrics and validator-info):
s3://qanodelogs/indy-1836/logs-and-metrics-20-12-2018;;;","25/Dec/18 9:00 PM;sergey-shilov;Please re-test the case described in

https://jira.hyperledger.org/browse/INDY-1836?focusedCommentId=54799&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-54799

with a new indy-node 1.6.745-master.
;;;","26/Dec/18 3:58 PM;zhigunenko.dsr;Logs and metrics for 1.6.745 are available here:
/home/ev/logs/INDY-1836/1836_25_12_2018



Node3 was stopped during test;;;","27/Dec/18 9:04 PM;sergey-shilov;Please re-test the case described in

https://jira.hyperledger.org/browse/INDY-1836?focusedCommentId=54799&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-54799

with a new indy-node 1.6.747-master.
;;;","28/Dec/18 7:57 PM;ozheregelya;Logs and metrics sources for 1.6.747:
 s3://qanodelogs/indy-1836/prod_load_28_12_2018

Metrics .csv files:
 s3://qanodelogs/indy-1836/prod_load_28_12_2018/metrics_csv

Note that in this test writing load was started not from the beginning test (it's caused by load script issue), so, first it was reading load only during several hours.

At the end of the test pool stopped writing.
Following nodes were lagged during test:
 persistent_node16 6697
 persistent_node10 11140
 persistent_node23 26379
 persistent_node1 16119
 persistent_node19 31722
 persistent_node7 15672
 the rest nodes have 33520 domain txns.;;;","12/Jan/19 1:31 AM;VladimirWork;Build Info:
indy-node 1.6.752

Steps to Reproduce:
0. Set the next parameters in config file:
{noformat}
OUTDATED_REQS_CHECK_INTERVAL = 120  # seconds
PROPAGATES_PHASE_REQ_TIMEOUT = 300  # seconds
ORDERING_PHASE_REQ_TIMEOUT = 600 # seconds
REPLICAS_REMOVING_WITH_DEGRADATION = None
REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None
GC_STATS_REPORT_INTERVAL=300
{noformat}
1. [17:30] Run production load without fees (10 writes and ~30 reads per seconds since I fell into an issue with `too many files opened` error).
2. [17:45] Stop 3rd node (primary of the 2nd instance).
3. [19:00] Stop the load.

Actual Results:
All nodes except stopped one have the same amount of txns in domain and sovtoken ledgers. All journals\logs\metrics are sent to [~anikitinDSR].;;;","12/Jan/19 8:00 PM;anikitinDSR;After logs and metrics analyzing from the last load_test launching we got the next results:

!node1.png|thumbnail!

  !node15.png|thumbnail!

As we can see on ""Request queues"" we have increased count of unordered request because of stopped 3rd node (primary of the 2nd instance). But this queue does not raise extremely and ""Clear Request Queue strategy"" periodically delete requests from queue.;;;",,,,
Intermmitent test failure: test_view_change_on_quorum_of_master_degraded,INDY-1837,35246,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,09/Nov/18 8:18 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/18 8:18 PM;ashcherbakov;test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-8475.txt;https://jira.hyperledger.org/secure/attachment/16246/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-8475.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzwx5j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermmitent test failure: test_stashed_messages_processed_on_backup_replica_ordering_resumption[non-primary] ,INDY-1838,35247,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,09/Nov/18 8:20 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Nov/18 8:20 PM;ashcherbakov;test-result-plenum-1.ubuntu-05.txt;https://jira.hyperledger.org/secure/attachment/16247/test-result-plenum-1.ubuntu-05.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzwx5r:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
3rd party open source manifest,INDY-1839,35250,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,esplinr,esplinr,esplinr,09/Nov/18 10:21 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Hints*
 * A spreadsheet with dependencies about 1.5 year old: [https://docs.google.com/spreadsheets/d/1vz3kVt281EF-qp_tKB5z7zJTviI9YPV0NO4x0DsLSuk/edit#gid=0]
 * We should have a look at setup.py in indy-plenum and indy-node (in particular, install-requires section)
 * We may also install indy-node deb package, and get all dependencies for Node and Plenum using `apt-cache show`
 ** we already have utilities to get dependency tree that we use during Upgrade
 ** have a look at https://github.com/hyperledger/indy-node/blob/master/indy_node/utils/node_control_utils.py#L111

*Acceptance criteria:*
 * A spreadsheet with all direct dependencies for Indy-Node containing the following information about each dependency:
 ** Name
 ** License
 ** Version
** Reason for usage

Destination spreadsheet:
https://drive.google.com/drive/folders/1crXe2FSzaojc_QWfsl0jYfYCliVly4jj",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-1068,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxaf:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,dsurnin,esplinr,,,,,,,,,,"15/Nov/18 9:16 PM;dsurnin;Table filled
https://docs.google.com/spreadsheets/d/1L4C5qHAlmkR6Fko2sY0jRCgF1KvWKlMaIfJlg5dmyyo/edit#gid=0;;;","17/Nov/18 4:12 AM;esplinr;This looks good. Thank you.;;;",,,,,,,,,,,,,,,,,,,,,,,
Need to have ec2 volumes tagged as well,INDY-1840,35256,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,andkononykhin,andkononykhin,10/Nov/18 1:16 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,devops,,,"As of now EC2 volumes are created automatically during instances creation and are not tagged Thus, it would be hard to track costs for them.

Acceptance criteria:
 # volumes should have tags
 # tags should be added at creation time to make creation and tagging ""atomic"" operation",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1791,,,,,INDY-1877,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwvjk:",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,,"07/Dec/18 10:48 PM;andkononykhin;*PoA*:
 * add tests to check that instances have expected tags
 * improve  [boto3 create_instances api|https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances] usage to pass tags for attached volumes
 * no additional variables for playbook are expected since tags would be the same as for instances;;;","07/Dec/18 10:48 PM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1085;;;","10/Dec/18 4:49 PM;andkononykhin;*Problem reason*:

AWS EC2 Volumes for instances were created implicitly without any tags specific to the project.

*Changes*:
 * volumes received the same tags as instances

*Committed into*:

https://github.com/hyperledger/indy-node/pull/1085

*Risk factors*:

Nothing is expected.

*Risk*:

Low

*Covered with tests*:
 * extended tests for _stateful_set.py_ ansible module

*Recommendations for QA*:
 * create test inventory using _namespace-config.py_
 * run _provision.yml_ playbook
 * check that all volumes created for instances are tagged
 * destroy;;;","11/Dec/18 12:29 AM;VladimirWork;Steps to Validate:
1. Create test inventory using namespace-config.py.
2. Run provision.yml playbook.
3. Check that all volumes created for instances are tagged.
4. Destroy the pool.

Actual Results:
All volumes created for instances are tagged.;;;",,,,,,,,,,,,,,,,,,,,,
Additional data needed from validator-info,INDY-1841,35263,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,mgbailey,mgbailey,10/Nov/18 7:27 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,validator-info,,,0,,,,"The following should be displayed in all outputs of validator-info (regular, -v and --json):

The BLS key

The date (including timezone or UTC offset) that the info was last refreshed.

The software package versions of indy-node and sovrin (this was there previously)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-967,,,,,INDY-1814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx2n:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,mgbailey,ozheregelya,VladimirWork,,,,,,,,,"30/Nov/18 12:56 AM;ozheregelya;*Environment:*
 indy-node 1.6.709

*Steps to Reproduce:*
 1. Run validator-info.
 2. Run validator-info -v.
 3. Run validator-info --json.

*Actual Results:*
 Date and sovrin version are not specified in verbose and json modes.

*Expected Results:*
 Named fields should present in all modes.;;;","30/Nov/18 6:08 PM;dsurnin;Update date added to json output as well as packets versions

PR
https://github.com/hyperledger/indy-node/pull/1066

Node v710;;;","30/Nov/18 9:11 PM;VladimirWork;Build Info:
indy-node 1.6.710
sovrin 1.1.97

Steps to Validate:
1. Run validator-info.
2. Run validator-info -v.
3. Run validator-info --json.
4. Check that all outputs above show date and sovrin version.

Actual Results:
Date and sovrin version are specified in all modes.;;;",,,,,,,,,,,,,,,,,,,,,,
Node that is not on network is shown as 'unreachable',INDY-1842,35264,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,mgbailey,mgbailey,mgbailey,10/Nov/18 8:40 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.79,,,,0,TShirt_M,,,"This behavior has manifest on two new validators that were onboarded to TestNet this week, but not on the other validators on that network. TestNet is running indy-node=1.6.78.

valNode01 is a node that was once on the TestNet, but it has been removed with a ledger entry with services=[] written to the ledger. The issue is that in spite of this, this node is shown as if it is an active node that is unreachable in validator-info. In the logs, you can see that the new validators are attempting to contact it as well. This occurs on these 2 new validators, but not on the other  ones in the pool. At least for these 2 validators, this is a drag on consensus, and could also influence client behavior as well.

The pool ledgers are in sync on all nodes.

validator-info output,  the pool ledger, and logs from one of the new nodes are attached.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Nov/18 8:39 AM;mgbailey;xsvalidatorec2irl_log_1.zip;https://jira.hyperledger.org/secure/attachment/16250/xsvalidatorec2irl_log_1.zip","10/Nov/18 8:39 AM;mgbailey;xsvalidatorec2irl_log_2.zip;https://jira.hyperledger.org/secure/attachment/16251/xsvalidatorec2irl_log_2.zip","10/Nov/18 8:39 AM;mgbailey;xsvalidatorec2irl_pool_ledger.txt;https://jira.hyperledger.org/secure/attachment/16249/xsvalidatorec2irl_pool_ledger.txt","10/Nov/18 8:40 AM;mgbailey;xsvalidatorec2irl_validator_info.json;https://jira.hyperledger.org/secure/attachment/16248/xsvalidatorec2irl_validator_info.json",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxan:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,dsurnin,esplinr,krw910,mgbailey,,,,,,,"13/Nov/18 2:54 PM;esplinr;From Mike:
All new nodes coming onto the network show this behavior. In addition, the address they are trying to contact it on is the loopback address (127.0.0.1);;;","14/Nov/18 1:28 AM;ashcherbakov;[~mgbailey]
Is `valNode01` part of genesis file?;;;","14/Nov/18 4:42 AM;mgbailey;It is not in the genesis. It was added by a node transaction, which was later updated, and then updated again to remove the validator service tag.

 ;;;","15/Nov/18 5:35 PM;ashcherbakov;[~mgbailey]
1) Do we understand correctly, that the pool stability is not affected by this issue, and all nodes are in consensus?

2) Is the issue still there if a new node is restarted?;;;","16/Nov/18 2:42 AM;mgbailey;[~ashcherbakov] This issue is not preventing consensus, but it still merits looking at since we are experiencing intermittent slowness from clients for unknown reasons. There are 4 of 13 nodes that are currently experiencing this issue. I have asked the stewards of these nodes to restart the indy-node service on them, but due to their time zones, we may not hear back from them today.

 ;;;","17/Nov/18 3:46 AM;mgbailey;Update:
 # Restarting the nodes does appear to make them stop attempting to connect to the phantom node.
 # All the nodes in the pool started today reporting ""sovrin.sicpa.com"" as unreachable. Once again, this node's transaction on the pool ledger has been previously disabled with a services=<blank> write to the ledger. Once again, a restart of the nodes makes them stop trying to reach the phantom node.
 # My concern is rising. Could clients, which share code libraries with indy-node, also be attempting to contact demoted nodes?;;;","18/Nov/18 3:04 AM;krw910;[~esplinr][~mgbailey][~ashcherbakov]
I changed this from a task to a bug since it is an issue and not an exploratory type ticket.

I agree with Mike that this is a concern and I would say a big one for me. We cannot keep restarting nodes to fix issues. 
I see two possible things here that both are concerning.
1. Nodes are ignoring the demoted state of a node services=[]
2. The nodes that are trying to contact those in a demoted state do not have the transaction in their ledger showing the node has been demoted. 

Both possibilities are concerning.;;;","19/Nov/18 4:14 PM;ashcherbakov;Agree, it needs to be investigated.

[~mgbailey]
{quote}been previously disabled with a services=<blank>
{quote}
 So, did we use `services=<blank>` or `services=[]`?
{quote}Could clients, which share code libraries with indy-node, also be attempting to contact demoted nodes?
{quote}
The only client that shares codebase with indy-node is the old deprecated indy-cli shipped with indy-node. Does anyone still use it?;;;","20/Nov/18 12:56 AM;mgbailey;[~ashcherbakov] The transaction to demote a node in the new indy-cli is:
{code:java}
ledger node target=7wetNy5AJpHfXvhTx4okZokSw5mcrgWgVZ8jJ3WHgrmd alias=trusted_you services=
{code}
I assume that libindy and indy-node share some common code. Maybe this is incorrect.;;;","21/Nov/18 2:08 AM;esplinr;[~mgbailey] Indy Node and Indy SDK only share the crypto library.

We believe that addressing the concern in IS-1066 and INDY-1816 mitigates most of the symptoms of this ticket. We will continue our investigation to verify that there are no additional changes necessary.

Downstream applications need to ensure that they are refreshing their list of pool nodes on a regular basis to remove stale nodes. cc [~krw910] and [~jrayback];;;","22/Nov/18 7:07 PM;Derashe;This problem was caused because of double demotion problem (earlier double demotion resulted in connection to node). This was fixed after stable release 1.6.78 was published.

Ticket: https://jira.hyperledger.org/browse/INDY-1506

PR: https://github.com/hyperledger/indy-plenum/pull/922

 

 ;;;","22/Nov/18 7:11 PM;dsurnin;The last demotion of the node valNode01 was made with node ip changed to 127.0.0.1. That is why new nodes tried to connect to localhost.;;;","24/Nov/18 1:05 AM;mgbailey;[~Derashe] [~ashcherbakov] I am seeing this problem in systems that are running 1.6.78. How can we say that it is fixed?;;;","24/Nov/18 1:36 AM;Derashe;[~mgbailey] Because this fix was not included in 1.6.78. And was fixed after it has been published.;;;",,,,,,,,,,,
Investigate reasons of OOM crashes,INDY-1843,35285,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,12/Nov/18 11:06 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,,,,"We should investigate logs and metrics gathered in scope of INDY-1795 to find reasins of OOM crashes.

Steps to Reproduce:
1. Run 20 nyms/sec for 15 hours.
2. Check metrics and logs for OOM reasons at any node.

Actual Results:
Pool works normally. Only 8th node breaks due to OOM.
All logs and journals are in ev@evernymr33:logs/1795-1683.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1795,,,,,,,,,,,,,,,,,,,,"12/Nov/18 11:07 PM;VladimirWork;OOM.png;https://jira.hyperledger.org/secure/attachment/16254/OOM.png","05/Dec/18 6:42 PM;VladimirWork;image (3).png;https://jira.hyperledger.org/secure/attachment/16343/image+%283%29.png","05/Dec/18 6:42 PM;VladimirWork;image (4).png;https://jira.hyperledger.org/secure/attachment/16344/image+%284%29.png","05/Dec/18 6:42 PM;VladimirWork;image (5).png;https://jira.hyperledger.org/secure/attachment/16345/image+%285%29.png","05/Dec/18 6:42 PM;VladimirWork;image (6).png;https://jira.hyperledger.org/secure/attachment/16346/image+%286%29.png","13/Nov/18 6:03 PM;Toktar;image-2018-11-13-12-04-18-072.png;https://jira.hyperledger.org/secure/attachment/16256/image-2018-11-13-12-04-18-072.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa6:s",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"13/Nov/18 6:03 PM;Toktar;In 00:00 on the Node8 the node stack had a high load. With INFO logs we don't know what messages were it. But with metrics we see that it were messages with type MessageRep.

!image-2018-11-13-12-04-18-072.png|width=573,height=293!
 In this metric we see count of processing messages per second for all types of messages witch can be asked via message requests (MessageReq). Node8 asked Propagate messages.
 * red line - count of processed propagates per second
 * orange line - count of processed message replies per second

As a result, Node8 is slow due to the large number of processed messages. It does not have time to order incoming messages and after 2 hours the node shot down with out of memory. And the instance 7 with primary Node8 has been removed with backup primary degradation.;;;","21/Nov/18 7:08 PM;Toktar;Logs with level INFO and csv from 1, 6, 8, 16 nodes are not enough. 
 We know that in 22:00 nodes processed a lot of request messages. We don't know which node send it but message requests ask propagates messages.
 Node24 received twice PrePrepares from Node9 (instance 8) at 23:59:29. It means that some other node may already ordered this batch and asked propagate from other nodes.
 What can we do:
 # Fix problem with requesting Propagates messages for already ordered batches (INDY-1709)
 # Add logging for all message requesting.
 # Retest with Debug logging level and new feature.;;;","30/Nov/18 6:15 PM;Toktar;Similar problem with propagates re asking was found in the logs for another case with the same INDY-1865 load and 400 MAX_CONNECTED_CLIENTS_NUM \ 500 iptables client connections limit restrictions: ev@evernymr33:logs/1865_23_11_2018_400_500_restrictions.7z;;;","30/Nov/18 6:44 PM;Toktar;Problem reason:
 - 4 nodes restarted with out of memory and have a problem with ordering. In theory this problem relates to so big traffic with Propagate messages re-asking. In this case slow nodes should receive a lot of Pre-Prepare messages with already ordered requests.  

Changes:
 - In this step bug with re-asking ordered requests is fixed. If it solves problem with the pool in this ticket, task can be closed. 

PR:
 * [https://github.com/hyperledger/indy-node/pull/1067]
 * [https://github.com/hyperledger/indy-plenum/pull/990]

Version:
 * indy-node 1.6.711 -master
 * indy-plenum 1.6.617  -master

Risk factors:
 - Node ordering, backup instances can be stop ordering in view change or catchup if task is fixed incorrect.

Risk:
 - Low

Covered with tests:
 * [test_replica_received_preprepare_with_unknown_request.py|https://github.com/hyperledger/indy-plenum/pull/990/files#diff-4a9676e3338b499da78166553598c538]
 * test_process_pre_prepare_with_not_final_request
 * test_process_pre_prepare_with_ordered_request

Recommendations for QA:
 * Repeat test from this task with DEBUG logging level for 8-15 hours.
 If logs will contain ""Pre-Prepare message has already ordered requests"" and no one node will not restart with out of memory, ticket may be close.;;;","05/Dec/18 6:43 PM;VladimirWork;Build Info:
indy-node 1.6.713

Steps to Validate:
1. Run 20 nyms/sec for ~8 hours with debug logs.
2. Run 20 nyms/sec for ~12 hours with info logs.
3. Check metrics and logs for OOM reasons at any node.

Actual Results:
It looks like there are no ""Pre-Prepare message has already ordered requests"" entries in the logs in both cases. Pool has been broken it the first case but since the second case works well the root cause of the issue is debug level logs. !image (3).png|thumbnail!  !image (4).png|thumbnail!  !image (5).png|thumbnail!  !image (6).png|thumbnail! ;;;","05/Dec/18 10:57 PM;Toktar;In the first test with debug logs we have a lot of view changes, because master instance writes more logs in debug logging level then backup instances and works slower.
 * 1-3 View changes happened with master instance degraded
 * 4-7 View changes happened because a previous View change didn't finish in time. 

In this test, message requests for Propagates were found but it's an expected behavior. If Node2 is farther from primary node than from other nodes, it can receives PrePrepare message early than will has a quorum of Propogates messages to finalize request.;;;",,,,,,,,,,,,,,,,,,,
Be able to see the status of a Sovrin Network live without using SDK,INDY-1844,35294,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,Done,,benjsmi,benjsmi,13/Nov/18 1:53 AM,11/Oct/19 10:20 PM,28/Oct/23 2:47 AM,11/Oct/19 10:19 PM,,,,,,0,,,,"Thanks so much to those in the Indy community who hang out on Slack and RocketChat to let us know things like when updates are coming and also the live status of the networks – is there a problem with STN? Slack is the best place to check.

But I think the users of these networks would prefer to be able to get either a JSON or HTML-ified display of the health of the network at live status.

If JSON output is allowed, it would be very easy for folks who rely on the network to connect in their support team's existing alerting systems to this endpoint and get alerted whenever there's a network-level problem.

As per a few other issues I've opened, I don't think this should require compiling the Indy SDK and writing your own script. Yes, that's possible, but as service providers these days, people are expecting more.

INDY-1819 should (in the related for this issue) should help achieve this goal. If each validator node can be monitored without a specialized SDK, then the monitoring of the network becomes a much easier task.

Some may argue or feel that this issue is more related to Sovrin or the Sovrin Networks specifically as opposed to the Indy Node. I've not seen a project or JIRA board where I can open issues related to the Sovrin network directly, so I'm using this forum.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1819,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-2,,Network-wide Browser-viewable Status,Done,No,,Unset,No,,,"1|hzzyif:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),benjsmi,esplinr,,,,,,,,,,,"11/Oct/19 10:19 PM;esplinr;Improvements to validator-info allow monitoring the node status. Other stewards have integrated this information with browser based status reports. If there are additional features needed, we can raise specific issues.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Remove exclusive reliance on systemd,INDY-1845,35295,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,benjsmi,benjsmi,13/Nov/18 2:02 AM,13/Nov/18 2:02 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Systemd is the standard control bug for daemons and long-running background processes in Ubuntu, to be sure.  However, as we move forward, folks may want to run their validators in Docker containers. Systemd is unavailable in Docker.  Instead, many who write Docker images but want to provide on/off and management capabilities to the services inside their Docker containers use [Supervisor|https://supervisord.org].

There are others, of course, such as launchd.  The point of this task/issue is to remove hard-coded instances of OS/shell-out calls to systemd and replace them with an abstraction that would allow pluggable interfacing with whichever process control management system may be appropriate for the determined environment.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzyin:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),benjsmi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Change pool state root hash for BLS-signature in Commit messages,INDY-1846,35312,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Toktar,Toktar,13/Nov/18 11:25 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"Change pool_state_root_hash in Commit MultiSignatureValue to pre_prepare.pool_state_root_hash.
*Acceptance criteria*
 * Add field ""POOL_STATE_ROOT_HASH"" to PrePrepare message (Put it before PLUGIN_FIELDS)
 * Add validation of new PrePrepare format. Its POOL_STATE_ROOT_HASH should be included in the uncommitted state root hash
 * Change commit BLS signature creating. Now it should use pool state root hash from PrePrepare message instead committed state root hash.
 * Change validation of a commit BLS signature. Now it should use pool state root hash from PrePrepare message instead committed state root hash.
 * Change logging message in case with incorrect BLS signature for commit message. Add information about an incorrect Commit message
 * Add tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1823,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwz3:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,zhigunenko.dsr,,,,,,,,,,"23/Nov/18 10:50 PM;Toktar;Problem reason:
 - Node1 is sending a commit message and using committed_pool_root_hash_1 and data from PrePrepare to sign it.
Node2 receives the commit from Node1 and validate it with its current committed_pool_root_hash, which is equal to committed_pool_root_hash_2 and different from the one used during signing. So, pool root hashes are different, and, although commit is correct, BLS signature verification fails, Commit is considered as invalid, and this batch will never be ordered.

Changes:
 - Add the current uncommitted pool state root hash to ""POOL_STATE_ROOT_HASH"" to PrePrepare message 
 -  Extend validation of received PrePreapares to check, that POOL_STATE_ROOT_HASH is equal to the node's uncommitted state root hash. 
 -  Change commit BLS signature creating. Now it should use pool state root hash from PrePrepare message instead committed state root hash. We can do it because pool state root hash from PrePrepare has already been verified in PrePrepare validation.
 -  Change validation of a commit BLS signature. Now it should use pool state root hash from PrePrepare message instead committed state root hash.
 -  Change logging message in case with incorrect BLS signature for commit message. Add information about an incorrect Commit message
 -  Change using BLS key in creating signature from committed state to using key from uncommitted state.
 - Add tests

PR:
 * [https://github.com/hyperledger/indy-node/pull/1055]
 * [https://github.com/hyperledger/indy-plenum/pull/980]

Version:
 * indy-node 1.6.700 -master
 * indy-plenum 1.6.604  -master

Risk factors:
 - Node ordering

Risk:
 - Low

Covered with tests:
 * test for create3PCBatchwith for empty requests queues - {{test_create_3pc_batch_with_empty_requests}}

 *  test for successful finish for create3PCBatch (POOL_STATE_ROOT_HASH added) - {{test_create_3pc_batch}}

 *  test for update_commit with _can_process_ledger() = false - {{test_update_commit_pool_ledger}}

 *  test for update_commit with can_sign_bls() = false - {{test_update_commit_without_bls_crypto_signer}}

 *  test for successful finish for update_commit (signature added) - {{test_update_commit}}

 *  test for validate_commit with no BLS_MULTI_SIG in pre_prepare - {{test_validate_commit_correct_sig_first_time}}

 *  test for validate_commit with invalid pool_state_root_hash - {{test_validate_commit_signature_without_pool_state_root}}

 *  test for validate_commit with correct pool_state_root_hash - {{test_validate_commit_correct_sig_second_time}}

 *  test for validate_commit without pool_state_root_hash - {{test_validate_commit_signature_without_pool_state_root}}

 *  test for success processPrePrepare - {{test_process_pre_prepare_validation}}

 *  test for processPrePrepare without old PrePrepare schema - {{test_process_pre_prepare_validation_old_schema}}

 *  test for processPrePrepare with invalid POOL_STATE_ROOT_HASH - {{test_process_pre_prepare_with_pool_state_root}}

 *  test for processPrePrepare with correct POOL_STATE_ROOT_HASH - {{test_process_pre_prepare_with_incorrect_pool_state_root}}

 *  integration test - {{test_commit_signature_validation_integration}} :

 * test_validate_commit_does_not_use_committed_pool_state
 * test_validate_pre_prepare_does_not_use_committed_pool_state

Recommendations for QA:
 * Repeat test from INDY-1823;;;","27/Nov/18 10:44 PM;zhigunenko.dsr;*Environment*
indy-node 1.6.700
logLevel = debug
 
*Step to Reproduce*
live_agent4: perf_processes.py -g pool -c 1 -n 1 -l 5 -k nym -y one -b 10
live_agent5: perf_processes.py -g pool -c 1 -n 1 -l 1 -k ""demoted_node"" -y one -b 10

*Actual Results:*
Pool started 
sudo read_ledger --type=domain --count >> 55622
sudo read_ledger --type=pool --count >> 13015
""View_No"":      22
""Last_complete_view_no"":  22;;;","28/Nov/18 5:58 PM;VladimirWork;Build Info:
indy-node 1.6.701

Steps to Validate:
1. Run production load without fees against 25 nodes pool for 16+ hours.
2. Check logs for ""Commit message has invalid BLS signature"" entries.

Actual Results:
Pool works normally. There are no ""Commit message has invalid BLS signature"" entries in the logs.;;;",,,,,,,,,,,,,,,,,,,,,,
Add dates to the release notes,INDY-1847,35365,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,esplinr,esplinr,15/Nov/18 7:40 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,,,,The release notes for Indy Node and Sovrin should mention the date of each release.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvig:",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,zhigunenko.dsr,,,,,,,,,,,"18/Dec/18 9:43 PM;zhigunenko.dsr;PR: https://github.com/hyperledger/indy-node/pull/1099

*Additional Info:*
Some versions are omitted in Release Notes or non-identical version numbers on https://repo.sovrin.org/lib/apt/xenial/stable/ ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Downgrade (roll-back) should work properly in case one of the dependencies is changed,INDY-1848,35368,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Deferred,,ashcherbakov,ashcherbakov,15/Nov/18 6:06 PM,11/Oct/19 7:10 PM,28/Oct/23 2:47 AM,11/Oct/19 7:10 PM,,,,,,0,EV-CS,,,"If upgrade from version X to Y fails, it tries to roll-back everything by performing downgrade to version X.

If there is a dependency (not indy one, but,for example, base58), so that X depends on version Z1, and Y depends on version Z2,  then downgrade fails since Z1 is not listed explicitly during downgrade (apt can't install old earlier version of a dependency if there is a newer one in the repo).

*Acceptance criteria*
 * Downgrade and roll-back should be possible
 * If a dependency version changed, then it should properly take it into account and downgrade the dependency explicitly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1824,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-738,,,No,,Unset,No,,,"1|hzwxe0:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:10 PM;esplinr;This is an usual case, very difficult to fix, and would probably result in a situation that could be recovered by steward intervention on each node. So we will focus on other priorities.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Expected object or value error during batch handling,INDY-1849,35369,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,anikitinDSR,anikitinDSR,15/Nov/18 6:25 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"During log analizing from INDY-1835 was found a bug, which associated with batch handling. There is a lot of errors in logs, like
 ""discard batch ""<some message>"" because Expected object or value"", that means, that error was raised on string 'pi' or 'po' serialization. In this case, all batch will be discarded with some other messages inside, like Propogate, for example.

*Acceptance criteria:*
 * Investigate 'batch creation' related source code
 * Write unit and integration tests, which emulate error raising
 * Create a fix for avoiding this error and normal batch handling",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxb3:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,ozheregelya,,,,,,,,,,"19/Nov/18 11:18 PM;anikitinDSR;Was added small fix for this issue in PR:

[https://github.com/hyperledger/indy-plenum/pull/978]

But also, it needs to be investigated more deeper.;;;","22/Nov/18 7:57 PM;ashcherbakov;Indy-node 1.6.695;;;","22/Nov/18 8:58 PM;anikitinDSR;Reasons:
 * need to exclude batch into batch creating

Changes:
 * fix resending messages from stashed_to_disconnected queue

Version:
 * indy-node: 1.6.695
 * indy-plenum: 1.6.599

Recomendation for QA:
 * Need to run load_test (acceptance for example) with enabled heartbeats (set ENABLE_HEARTBEATS = True into /etc/indy/indy_config.py ) and check, that there is no any warning messages like ""Got error <some error> while processing <some message> message"".;;;","23/Nov/18 12:16 AM;ozheregelya;*Environment:*
indy-node 1.6.695

*Steps to Validate:*
1. Setup the pool with ENABLE_HEARTBEATS = True.
2. Run load test with production load (10 writes, 100 reads per sec).
3. Check logs for warnings.

*Actual Results:*
No warnings like ""Got error <some error> while processing <some message> message"" appeared.;;;",,,,,,,,,,,,,,,,,,,,,
Sovrin package can't be upgraded,INDY-1850,35370,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,ozheregelya,ozheregelya,ozheregelya,15/Nov/18 6:51 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.79,,,,0,TShirt_M,,,"*Environment:*
indy-node 1.6.675 -> sovrin 1.1.89 -> sovrin 1.1.90
docker pool

*Steps to Reproduce:*
1. Setup the pool of 10 nodes with indy-node 1.6.675 and sovrin 1.1.62 (last not pinned version).
2. Send pool upgrade transaction to sovrin 1.1.89 (depending on indy-node 1.6.675).
3. Send pool upgrade transaction to sovrin 1.1.90 (depending on indy-node 1.6.678).

*Actual Results:*
Upgrade failed due to following error:
{code:java}
Nov 14 16:49:16 8fcd6b2e84ed env[5196]: The following packages have unmet dependencies:
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: sovrin : Depends: indy-node (= 1.6.678) but 1.6.675 is to be installed
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: E: Unable to correct problems, you have held broken packages.
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + ret=100
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + '[' 100 -ne 0 ']'
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + echo 'Failed to obtain sovrin=1.1.90'
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: Failed to obtain sovrin=1.1.90
Nov 14 16:49:17 8fcd6b2e84ed env[5196]: + exit 1
{code}
*Expected Results:*
Sovrin should be upgraded.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwyv:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,ozheregelya,VladimirWork,,,,,,,,,,"19/Nov/18 5:19 AM;VladimirWork;It looks like we have the same issue with indy-node package upgrade (1.6.681 -> 1.6.682):
{noformat}
Nov 18 19:53:38 4db03710d62f env[331]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Nov 18 19:53:38 4db03710d62f env[331]: Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Nov 18 19:53:38 4db03710d62f env[331]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 18 19:53:38 4db03710d62f env[331]: Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Nov 18 19:53:39 4db03710d62f env[331]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Nov 18 19:53:39 4db03710d62f env[331]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 18 19:53:40 4db03710d62f env[331]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 18 19:53:40 4db03710d62f env[331]: Fetched 323 kB in 1s (205 kB/s)
Nov 18 19:53:41 4db03710d62f env[331]: Reading package lists...
Nov 18 19:53:41 4db03710d62f env[331]: Building dependency tree...
Nov 18 19:53:41 4db03710d62f env[331]: Reading state information...
Nov 18 19:53:41 4db03710d62f env[331]: 13 packages can be upgraded. Run 'apt list --upgradable' to see them.
Nov 18 19:53:47 4db03710d62f env[331]: indy-anoncreds
Nov 18 19:53:47 4db03710d62f env[331]: indy-node
Nov 18 19:53:47 4db03710d62f env[331]: indy-plenum
Nov 18 19:53:47 4db03710d62f env[331]: libindy-crypto
Nov 18 19:53:47 4db03710d62f env[331]: python3-indy-crypto
Nov 18 19:53:47 4db03710d62f env[331]: + deps=indy-node=1.6.682
Nov 18 19:53:47 4db03710d62f env[331]: + '[' -z indy-node=1.6.682 ']'
Nov 18 19:53:47 4db03710d62f env[331]: + echo 'Try to donwload indy version indy-node=1.6.682'
Nov 18 19:53:47 4db03710d62f env[331]: Try to donwload indy version indy-node=1.6.682
Nov 18 19:53:47 4db03710d62f env[331]: + apt-get -y update
Nov 18 19:53:47 4db03710d62f env[331]: Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Nov 18 19:53:47 4db03710d62f env[331]: Hit:2 http://archive.ubuntu.com/ubuntu xenial InRelease
Nov 18 19:53:47 4db03710d62f env[331]: Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Nov 18 19:53:47 4db03710d62f env[331]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Nov 18 19:53:48 4db03710d62f env[331]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Nov 18 19:53:48 4db03710d62f env[331]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Nov 18 19:53:48 4db03710d62f env[331]: Fetched 323 kB in 1s (206 kB/s)
Nov 18 19:53:49 4db03710d62f env[331]: Reading package lists...
Nov 18 19:53:49 4db03710d62f env[331]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install indy-node=1.6.682
Nov 18 19:53:50 4db03710d62f env[331]: Reading package lists...
Nov 18 19:53:50 4db03710d62f env[331]: Building dependency tree...
Nov 18 19:53:50 4db03710d62f env[331]: Reading state information...
Nov 18 19:53:51 4db03710d62f env[331]: Some packages could not be installed. This may mean that you have
Nov 18 19:53:51 4db03710d62f env[331]: requested an impossible situation or if you are using the unstable
Nov 18 19:53:51 4db03710d62f env[331]: distribution that some required packages have not yet been created
Nov 18 19:53:51 4db03710d62f env[331]: or been moved out of Incoming.
Nov 18 19:53:51 4db03710d62f env[331]: The following information may help to resolve the situation:
Nov 18 19:53:51 4db03710d62f env[331]: The following packages have unmet dependencies:
Nov 18 19:53:51 4db03710d62f env[331]:  indy-node : Depends: indy-plenum (= 1.6.593) but 1.6.590 is to be installed
Nov 18 19:53:51 4db03710d62f env[331]: E: Unable to correct problems, you have held broken packages.
Nov 18 19:53:51 4db03710d62f env[331]: + ret=100
Nov 18 19:53:51 4db03710d62f env[331]: + '[' 100 -ne 0 ']'
Nov 18 19:53:51 4db03710d62f env[331]: + echo 'Failed to obtain indy-node=1.6.682'
Nov 18 19:53:51 4db03710d62f env[331]: Failed to obtain indy-node=1.6.682
Nov 18 19:53:51 4db03710d62f env[331]: + exit 1
Nov 18 19:53:51 4db03710d62f env[331]: Upgrade from 1.6.681 to 1.6.682 failed: Command 'upgrade_indy_node ""indy-node=1.6.682""' returned non-zero exit status 1
Nov 18 19:53:51 4db03710d62f env[331]: Trying to rollback to the previous version Command 'upgrade_indy_node ""indy-node=1.6.682""' returned non-zero exit status 1
{noformat}
;;;","20/Nov/18 10:31 PM;dsurnin;PR
https://github.com/hyperledger/indy-node/pull/1044

first Node version with fix is 688;;;","24/Nov/18 1:20 AM;ozheregelya;*Environment:*
 indy-node 1.6.634 -> 1.6.688 -> 1.6.699
 sovrin 1.1.62 -> 1.1.91 -> 1.1.93

*Steps to Validate:*
 1. Check upgrade to new version.
 2. Check upgrade from new version.
3. Check rollback after failed upgrade and manual upgrade.
 (see [https://docs.google.com/spreadsheets/d/1ZeeTy0iwP2UhU-y2nJF3HNGlx0NZoLTgIEe9EwxQVQ8/edit#gid=1122130859] for details about upgrade cases)

*Actual Results:*
 Upgrade works.;;;",,,,,,,,,,,,,,,,,,,,,,
Plenum Consensus Protocol Diagram,INDY-1851,35381,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,ashcherbakov,ashcherbakov,15/Nov/18 9:45 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Create a diagram for request ordering by a Node and replicas (validation, 3PC batch creation, 3PC protocol, BLS multi-sigs), see [https://github.com/hyperledger/indy-plenum/blob/master/docs/request_handling.md]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1202,,,No,,Unset,No,,,"1|hzwx33:",,,,Unset,Unset,Ev 18.23,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"22/Nov/18 10:02 PM;ashcherbakov;https://github.com/hyperledger/indy-plenum/pull/989;;;",,,,,,,,,,,,,,,,,,,,,,,,
Request handlers should be pluggable,INDY-1852,35383,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,15/Nov/18 11:09 PM,11/Oct/19 7:14 PM,28/Oct/23 2:47 AM,09/Oct/19 7:09 PM,,,,,,0,,,,"Symptom:
The plugins used by the Sovrin network overrode the config handler which prevented non-forced upgrades. The plugin architecture should not allow this.

Notes:
* POA:
https://docs.google.com/document/d/10j1PV6miXOGrPzDe8wKptkpOzbqBYi0LS1GXF4Dwdo0/edit#heading=h.z1u0dzf62loi
* Sovrin Issue:
https://sovrin.atlassian.net/browse/ST-484",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-11,,Pluggable Request Handlers,Done,No,,Unset,No,,,"1|hzzyzr:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Detailed plan for resolving plugin upgrades,INDY-1853,35384,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,KitHat,esplinr,esplinr,15/Nov/18 11:11 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"*Acceptance Criteria*
* Create the issues needed in INDY-1852",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwx2v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Verbose response from validator-info should have a predictable order,INDY-1854,35407,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,16/Nov/18 6:58 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,validator-info,,,0,EV-CS,,,Alphabetize by key,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1814,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx2f:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,VladimirWork,,,,,,,,,,,"30/Nov/18 5:13 PM;VladimirWork;Build Info:
indy-node 1.6.709

Steps to Validate:
1. Run `validator-info -v` at all nodes of the pool.
2. Check keys' order and output consistency.

Actual Results:
All keys are sorted alphabetically.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Tasks related to moving from inheritance to composition in request handlers,INDY-1855,35441,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,KitHat,KitHat,19/Nov/18 6:28 PM,09/Oct/19 7:04 PM,28/Oct/23 2:47 AM,09/Oct/19 7:04 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-3,,Request Handlers,Done,No,,Unset,No,,,"1|hzzzd3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),KitHat,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Define interfaces for handlers for transactions, queries, actions and batches",INDY-1856,35446,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,KitHat,KitHat,19/Nov/18 6:39 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"See the step 1 in solution 2 in this google document:
https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvis:",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,KitHat,,,,,,,,,,"13/Dec/18 6:35 PM;ashcherbakov;# DatabaseFacade - container for all databases
 ** ledgers
 ** states
 ** idr_cache
 ** ts_store
 ** bls_store
 # Interfaces for each txn (write req):
 ** AbstractWriteReqHandler(db_facade):
 *** static_validation
 *** dynamic_validation
 *** apply_request
 *** revert_request
 ** NymRequestHandler
 ** NodeRequestHandler
 ** ....
 # Interfaces for each req request:
 ** AbstractReadReqHandler(db_facade):
 *** static_validation
 *** get_result
 ** GetNymReqHandler
 ** ...
 # Interfaces for each action request:
 ** AbstractActionReqHandler
 # Interafces for BatchReqHandler(db_facade):
 ** AbstratcBatchReqHandler(db_facade):
 *** post_apply_req_batch 
 *** commit_req_batch
 *** revert_req_batch
 ** DomainBatchReqHandler
 ** PoolBatchReqHandler
 ** ....
 # WriteRequestManager():
 ** self.write_req_handler: txn_type - > List[AbstractWriteReqHandler]
 ** self.batch_req_handler: ledger_id - > List[AbstratcBatchReqHandler]
 ** append_write_req_handler(txn_id, ReqHandler)
 ** remove_write_req_handlers(txn_id)
 ** static_validation
 ** dynamic_validation
 ** apply_request
 ** revert_request
 ** post_apply_req_batch
 ** commit_req_batch
 ** revert_req_batch
 ** for every method:
 *** for handler in self.<>_req_handler:
 **** handler.<>
 # ReadRequestManager
 ** self.read_req_handler: txn_type - > AbstractReadReqHandler
 # ActionReqManager
 ** self.action_req_handler: txn_type - > AbstractActionReqHandler

 

What Plenum/Node should implement:
 * WriteReqHandler for every txn type
 * ReadReqHandler for every query
 * ActionReqHandler for every action
 * IdrCacheBatchReqHandler - registered for DOMAIN_LEDGER
 * DomainBatchReqHandler - registered for DOMAIN_LEDGER
 * PoolBatchReqHandler - registered for POOL_LEDGER
 * ConfigBatchReqHandler - registered for CONFIG_LEDGER

 

What Plugins can implement:
 * reqHandler for new txn/query
 * batch req handler for new ledger
 * FeesReqHandler for every domain txn type
 * FeesBatchReqHandler for DOMAIN_LEDGER;;;","18/Dec/18 6:13 PM;Derashe;PRs:
 * plenum
 ** [https://github.com/hyperledger/indy-plenum/pull/1030]
 ** [https://github.com/hyperledger/indy-plenum/pull/1031]
 * node
 ** https://github.com/hyperledger/indy-node/pull/1090;;;",,,,,,,,,,,,,,,,,,,,,,,
Move logic from ReqHandler in DomainReqHandler,INDY-1857,35447,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,KitHat,KitHat,19/Nov/18 6:45 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"Move logic from DomainReqHandler (both in node and plenum) to separate implementations of QueryHandler, TransactionHandler and BatchHandler (according to step 2 of solution 2 by the link: https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#)",,,,,,,,,,,,,,,,,,,,,INDY-1858,INDY-1859,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Dec/18 12:19 AM;Derashe;diagram.png;https://jira.hyperledger.org/secure/attachment/16489/diagram.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00005",,,,Unset,Unset,Ev 18.25,Ev 19.1,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,KitHat,,,,,,,,,,,"25/Dec/18 12:15 AM;Derashe;PRs:
 * plenum https://github.com/hyperledger/indy-plenum/pull/1038
 * node https://github.com/hyperledger/indy-node/pull/1111;;;","25/Dec/18 12:19 AM;Derashe;Class diagram

!diagram.png|thumbnail!;;;",,,,,,,,,,,,,,,,,,,,,,,
Move logic from PoolReqHandler,INDY-1858,35448,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,KitHat,KitHat,19/Nov/18 6:49 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.9.0,,,,0,,,,"Move logic from PoolReqHandler (both in node and plenum) to separate implementations of QueryHandler, TransactionHandler, BatchHandler and ActionHandler (according to step 2 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",,,,,,,,,,,,,,,,,,,,,,,INDY-1857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00007i",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,KitHat,,,,,,,,,,,"14/Jan/19 11:30 PM;Derashe;PRs:
 * plenum: [https://github.com/hyperledger/indy-plenum/pull/1052]
 * node: [https://github.com/hyperledger/indy-node/pull/1125];;;",,,,,,,,,,,,,,,,,,,,,,,,
Move logic from ConfigReqHandler,INDY-1859,35449,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,KitHat,KitHat,19/Nov/18 6:53 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.9.0,,,,0,,,,"Move logic from ConfigReqHandler (both in node and plenum) to separate implementations of QueryHandler, TransactionHandler and BatchHandler (according to step 2 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",,,,,,,,,,,,,,,,,,,,,INDY-1860,,INDY-1857,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00007o",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,KitHat,,,,,,,,,,,"16/Jan/19 5:09 PM;Derashe;PR: https://github.com/hyperledger/indy-node/pull/1129;;;","16/Jan/19 5:12 PM;Derashe;We need to define what can we do with node_upgrade txn. Cause it has separated specific logic.

We can try to implement handling through hanlder or leave it as is.;;;",,,,,,,,,,,,,,,,,,,,,,,
Create Builders for handlers,INDY-1860,35450,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,KitHat,KitHat,19/Nov/18 7:00 PM,07/Jun/19 7:01 PM,28/Oct/23 2:47 AM,07/Jun/19 7:01 PM,,1.9.0,,,,0,,,,Incorporate request handler creation as said in step 4 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#],,,,,,,,,,,,,,,,,,,,,INDY-1861,,INDY-1859,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bjhx",,,,Unset,Unset,Ev-Node 19.11,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,KitHat,,,,,,,,,,,"31/May/19 5:45 PM;anikitinDSR;h2. PoA
h3. What we need to do

We need to make bootstrap process more clearnly and predicable. As of now, we have a couple of strongly constrained things. For example, PoolManager need to be initiated before bls_bft, but PoolManager requires a PoolReqHandler, which requires write_req_validator, which requires config_state... More clearnly schema is:
 * init request managers
 * init storages, ledgers and states
 * init write_request_validator (which is used on indy-node side for AUTH_RULE validation)
 * init request handlers
 * init batch handlers
 * restore states from corresponded ledgers if needed

For now, there is a couple of problems related to strongly integrated order of initialization.

The most appropriated approach for this is a ""Factory"" schema. We can init all stuffs firstly and setup all other things them.;;;","07/Jun/19 7:00 PM;anikitinDSR;Reason:
 * Need to create bootstraping procedure for pluggable request handlers

Changes:
 * Created methods for initiating storages and registering handlers.

PRs:
 * indy-plenum: [https://github.com/hyperledger/indy-plenum/pull/1222]
 * indy-node: [https://github.com/hyperledger/indy-node/pull/1332]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Integrate new handlers into the codebase,INDY-1861,35452,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,KitHat,KitHat,19/Nov/18 7:23 PM,21/Jun/19 5:00 PM,28/Oct/23 2:47 AM,21/Jun/19 4:04 PM,,1.9.0,,,,0,,,,Make united places for all request handlers to be created (according to step 3 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#]),,,,,,,,,,,,,,,,,,,,,,,INDY-1860,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bje",,,,Unset,Unset,Ev-Node 19.12,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,KitHat,,,,,,,,,,"21/Jun/19 4:04 PM;ashcherbakov;Main integration is done.

*PRs:*
 * [https://github.com/hyperledger/indy-node/pull/1352]
 * [https://github.com/hyperledger/indy-node/pull/1352]

*Tech debt*
 * There are some clean-up tasks that needs to be finished as well - INDY-2154

*Validation*
 * Will be done in the scope of INDY-2153 (see comments there);;;","21/Jun/19 5:00 PM;anikitinDSR;Versions:
indy-node: 1.9.0~dev1007
indy-plenum: 1.9.0~dev822

 ;;;",,,,,,,,,,,,,,,,,,,,,,,
Node does not validate CLAIM_DEF's filed ref,INDY-1862,35456,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,Derashe,Derashe,19/Nov/18 10:21 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.79,,,,0,,,,"We can send CLAIM_DEF's with ref field of any integer number. Node must validate that ref is a valid seq_no of SCHEMA txn.

Examples: 

test_send_claim_def_fails_if_ref_is_seqno_of_non_schema_txn

test_send_claim_def_fails_if_ref_is_not_existing_seqno",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa4:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,zhigunenko.dsr,,,,,,,,,,,"27/Nov/18 9:21 PM;Derashe;PR: [https://github.com/hyperledger/indy-node/pull/1056]

Indy-node version: 1.6.703;;;","28/Nov/18 5:24 PM;zhigunenko.dsr;*Environment:*
indy-node 1.6.703
indy-plenum 1.6.608
indy-cli 1.6.8~858

*Steps to Validate:*
 1) send cred-def with non-schema SeqNo
 2) send cred-def with non-existing SeqNo
 3) send cred-def with existing SeqNo
 4) send custom transations with non-schema SeqNo
 5) send custom transations with non-existing SeqNo

*Actual Results:*
 Cases 1,2,4,5 - corresponding CLI error
 Case 3 - expected success;;;",,,,,,,,,,,,,,,,,,,,,,,
Limit number of 3PC batches in flight,INDY-1863,35457,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,19/Nov/18 10:25 PM,23/Apr/19 11:47 PM,28/Oct/23 2:47 AM,23/Apr/19 11:47 PM,,1.7.1,,,,0,,,,"Currently batches are created as soon as there are _Max3PCBatchSize_ requests in queue OR _Max3PCBatchWait_ seconds passed since last batch was created. When under high load it can lead to creation of batches faster than they are ordered leading to uncontrollable growth of node traffic and consensus related data structures. At the same time batch size is not maxed out, which means that nodes are not using resources optimally (actually, the bigger batch the lower overhead of consensus).

*Proposed solution*
* Add _Max3PCBatchesInFlight_ parameter
* Create batches as soon as there are _Max3PCBatchSize_ requests in queue OR (_Max3PCBatchWait_ seconds passed AND there are less than _Max3PCBatchesInFlight_ batches in flight)

*Expected improvements*
Under high load bigger batches will be created, reducing number of consensus messages per request, so throughput and probably latency are improved",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1910,,,,,INDY-2064,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwvif:00001yw9609",,,,Unset,Unset,Ev-Node 19.08,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,,"18/Apr/19 4:33 PM;ashcherbakov;Fixed in Build: indy-node 1.7.0.dev895 ;;;","18/Apr/19 8:01 PM;sergey.khoroshavin;*Problem reason*
Actually there are two problems here:
* performance - as written in description
* stability - due to how current view change is implemented it is better to have as little as possible batches in flight, otherwise even small time shift between view changes on different nodes can lead to some nodes ordering more batches than others and falling out of consensus (more details in INDY-1296 and INDY-1303)

*Changes made*
Added _Max3PCBatchesInFlight_ parameter, defaulted to None (meaning no limit, i.e. old behavior). When _Max3PCBatchesInFlight_ is a number then primary won't try to create new batches unless:
* current number of batches in flight is less than that defined number
* we need to send a new batch just after view change
Also this limit won't prevent from creating as many freshness batches as needed when we already successfully entered batch creation logic.

*Version*
indy-node 1.7.0.dev895 

*PR*
https://github.com/hyperledger/indy-plenum/pull/1163

*Covered by tests*
test_max_3pc_batches_in_flight
test_can_send_multiple_3pc_batches
test_can_send_multiple_3pc_batches_below_limit
test_cannot_send_multiple_3pc_batches_above_limit
test_can_send_multiple_3pc_batches_in_next_view

*Risk*
Medium

*Risk factors*
Limiting number of batches in flight can lead to increased batch sizes with longer noninterruptible processing time. This can potentially lead to instabilities, although they can be addressed by INDY-1651.

*Recommendations for QA*
Run load tests on AWS pool with Max3PCBatchesInFlight set to 4 on all nodes:
* normal production load - to see if we are still stable
* production load with forced view changes (with period set to 1800 seconds) - to see if we get any better with reduced number of batches in flight;;;","19/Apr/19 8:20 PM;VladimirWork;Build Info:
indy-node 1.7.0.dev896

Steps to Reproduce:
1. Run production load test with `Max3PCBatchesInFlight = 4` for about 1 day.
2. Stop the load to try to catch up stalled nodes not under the load.
3. Run low rate load test for a half an hour to try to catch up stalled nodes by ordering batches.

Actual Results:
At least 3 nodes stalled by ViewNo (0 against 8 at the rest ones) and by all ledgers. They don't catch up and order at steps 2 and 3 too.

Logs and metrics:
ev@evernymr33:logs/INDY-1863_19_04_2019_case_1_logs.tar.gz
ev@evernymr33:logs/INDY-1863_19_04_2019_case_1_metrics.tar.gz;;;","22/Apr/19 6:25 PM;VladimirWork;Build Info:
indy-node 1.7.0.dev896

Steps to Reproduce:
1. Run production load test with `Max3PCBatchesInFlight = 4` and forced VCs for 8+ hours.
2. Stop the load to try to catch up stalled nodes not under the load.

Actual Results:
Pool has stopped writing due to long several sequential VCs. Nodes have ViewNo from 0 to 30. Pool has written only 41k txns into domain and 99k txns into sovtoken ledger. Also there are some stacktraces in logs and journalctl (see more info in the attachments).

Logs and metrics:
ev@evernymr33:logs/INDY-1863_22_04_2019_case_2_logs.tar.gz
ev@evernymr33:logs/INDY-1863_22_04_2019_case_2_metrics.tar.gz;;;","22/Apr/19 8:48 PM;VladimirWork;All issues found were reported as INDY-2064.;;;","23/Apr/19 5:22 PM;VladimirWork;Build Info:
indy-node 1.7.0.dev900

Steps to Reproduce:
1. Run production load test without fees with `Max3PCBatchesInFlight = 4` and forced VCs every 1800 seconds.

Actual Results:
All nodes are in sync by ledger size and ViewNo but there are too few txn written into sovtoken ledger (110k vs 558k in domain ledger).

Logs and metrics:
ev@evernymr33:logs/INDY-1863_23_04_2019_case_3_logs.tar.gz
ev@evernymr33:logs/INDY-1863_23_04_2019_case_3_metrics.tar.gz;;;","23/Apr/19 8:46 PM;VladimirWork;Build Info:
indy-node 1.7.0.dev900

Steps to Reproduce:
1. Run load test with 40 nyms/sec with `Max3PCBatchesInFlight = 4`.

Actual Results:
Pool has performed several VCs right after start of the load and has been broken after 141k txns written.

Logs and metrics:
ev@evernymr33:logs/INDY-1863_23_04_2019_case_4_logs.tar.gz
ev@evernymr33:logs/INDY-1863_23_04_2019_case_4_metrics.tar.gz;;;","23/Apr/19 11:46 PM;VladimirWork;Build Info:
indy-node 1.7.0.dev900

Steps to Reproduce:
1. Run load test with 40 nyms/sec with default config.

Actual Results:
Pool has performed several VCs right after start of the load and has been broken after 110k txns written so it looks like we survive longer with batches in flight limitation (at least results are not worse with this limitation than without it) but we need more test runs to prove it for sure.

Logs and metrics:
ev@evernymr33:logs/INDY-1863_23_04_2019_case_5_logs.tar.gz
ev@evernymr33:logs/INDY-1863_23_04_2019_case_5_metrics.tar.gz;;;",,,,,,,,,,,,,,,,,
Client request schema ignore unknown parameters passend in operation,INDY-1864,35492,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Derashe,Derashe,20/Nov/18 5:33 PM,20/Nov/18 5:33 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"If we pass parameter, which is unknown for certain type of request, then this request will be ordered and written in ledger. We must return ReckNack if encounter such a parameter.

Example: testSendNodeHasInvalidSyntaxIfUnknownParameterIsPassed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzzzmn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Two nodes were lagged after production load without fees,INDY-1865,35497,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Toktar,ozheregelya,ozheregelya,20/Nov/18 11:31 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,TShirt_M,,,"*Environment:*
 indy-node 1.6.682 
 libindy 1.6.7~816

*Steps to Reproduce:*
 1. Setup the pool.
 2. Run reading test on one client machine:
{code:java}
perf_processes.py -g live_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":10,\""addr_mint_limit\"":10,\""mint_by\"":10,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k ""[{\""get_nym\"":{\""count\"": 9}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}, {\""get_revoc_reg_def\"":{\""count\"": 1}}, {\""verify_payment\"":{\""count\"": 1}}]"" -c 900 -b 100 -l 100{code}
3. Run writing load test from another machine:
{code:java}
perf_processes.py -g live_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000,\""mint_by\"":500,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 10 -b 100 -l 10{code}
*Actual Results:*
 Most of the nodes wrote 277570 txns. Two nodes were lagged:
 live_node4 | 205588
 live_node5 | 189321

Logs and metrics: s3://qanodelogs/indy-1865
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1865/ /home/ev/logs/indy-1865/

 

Logs and metrics: evernymr33:/home/ev/logs/indy-1865/

Correct logs: s3://qanodelogs/INDY-1865-correct",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/18 5:46 PM;Toktar;image-2018-11-29-11-46-51-396.png;https://jira.hyperledger.org/secure/attachment/16328/image-2018-11-29-11-46-51-396.png","29/Nov/18 5:47 PM;Toktar;image-2018-11-29-11-47-38-347.png;https://jira.hyperledger.org/secure/attachment/16329/image-2018-11-29-11-47-38-347.png","29/Nov/18 5:51 PM;Toktar;image-2018-11-29-11-52-26-512.png;https://jira.hyperledger.org/secure/attachment/16330/image-2018-11-29-11-52-26-512.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwzr:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,VladimirWork,,,,,,,,,,"23/Nov/18 10:56 PM;VladimirWork;Logs for another case with the same load and 400 MAX_CONNECTED_CLIENTS_NUM \ 500 iptables client connections limit restrictions: ev@evernymr33:logs/1865_23_11_2018_400_500_restrictions.7z;;;","30/Nov/18 6:23 PM;Toktar;logs/1865_23_11_2018_400_500_restrictions.7z was added to INDY-1843 because it has the same problem with Propagate messages.

Problem in the first logs in this task relates to processing checkpoint messages from the next view in View change. It will be fixed in the task INDY-1876;;;",,,,,,,,,,,,,,,,,,,,,,,
Possible memory leak during catchup,INDY-1866,35514,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,21/Nov/18 10:17 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,,,,"During long sustained load test one of nodes was restarted, successfully completed short catchup and continued ordering. However after that all other nodes increased memory consumption and number of live objects, but number of requests in their queues (which is major contributor to number of live objects) was not increased. This increase in number of live objects seems constant. There is probability that node has memory leak related to catchup and/or other activity related to return of some node to consensus.

*PoA*
* run load test in docker with on of nodes continuosly restarted
* check memory consumption and number of live objects on all nodes except one being restarted
* if there is increase in memory consumption try to identify its source (logging top 50 types of live objects can help here)
* if there is an identified leak either fix it or create issue dedicated to fix (if fix would take more than 1 day)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Nov/18 10:05 PM;sergey.khoroshavin;Screenshot from 2018-11-21 15-53-40.png;https://jira.hyperledger.org/secure/attachment/16305/Screenshot+from+2018-11-21+15-53-40.png","27/Nov/18 6:13 PM;sergey.khoroshavin;Screenshot from 2018-11-27 12-10-59.png;https://jira.hyperledger.org/secure/attachment/16318/Screenshot+from+2018-11-27+12-10-59.png","06/Dec/18 8:18 PM;sergey.khoroshavin;Screenshot from 2018-12-06 12-37-27.png;https://jira.hyperledger.org/secure/attachment/16361/Screenshot+from+2018-12-06+12-37-27.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1721,,,No,,Unset,No,,,"1|hzwx13:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,"27/Nov/18 6:21 PM;sergey.khoroshavin; !Screenshot from 2018-11-27 12-10-59.png|thumbnail! 
Seems like there is indeed memory leak in nodes when one of them restarts while under load. Furthermore, when node was stopped and started after some time it's memory grew much more. It seemed like memory growth during restart is proportional to number of transactions missed by node, which makes catchup code prime suspect. Top live objects showed that growth was in number of _dict_ and _set_ objects. Further investigation will be performed.;;;","30/Nov/18 11:00 PM;sergey.khoroshavin;Following experiments were performed in docker pool with one node constantly restarting:
* load test with one node ignoring catchup requests, but there was no difference in memory consumption compared to normal nodes
* load test with one node ignoring both ledger status and catchup requests, but there was no difference in memory consumption compared to normal nodes
* no load showed no memory growth;;;","06/Dec/18 8:16 PM;sergey.khoroshavin;*Problem reason*
Rebooting node was propagating requests that were already ordered by other nodes. While these propagates were discarded requests were still accumulating in ReqAuthenticator during message authentication, which takes place before actual processing.

*Solution*
Make sure request is deleted from ReqAuthenticator when discarding PROPAGATE during actual processing.

*Version*
indy-plenum 1.6.625

*PR*
https://github.com/hyperledger/indy-plenum/pull/1012

*Covered by tests*
https://github.com/hyperledger/indy-plenum/pull/1012/files#diff-bd3503579ad656b2b2f2726298ed1602R85

*Risk*
Low

*Recommendations for QA*
Continuously restart one of non-primary nodes while under load test for at least 4 hours. Look for number of objects tracked by GC in metrics - it shouldn't increase in long term.;;;","06/Dec/18 8:18 PM;sergey.khoroshavin;Actually some validation was already done using docker pool with Node2 having this fix and other nodes not:
!Screenshot from 2018-12-06 12-37-27.png|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,
Pool stopped writing after production load with fees,INDY-1867,35517,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Derashe,ozheregelya,ozheregelya,21/Nov/18 10:29 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,TShirt_L,,,"Environment:
indy-node 1.6.690
libindy 1.6.7~816

Steps to Reproduce:
1. Setup the pool.
2. Run reading load test from 2 machines:
{code:java}
perf_processes.py -g live_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":10,\""addr_mint_limit\"":10,\""mint_by\"":10,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k ""[{\""get_nym\"":{\""count\"": 9}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}, {\""get_revoc_reg_def\"":{\""count\"": 1}}, {\""verify_payment\"":{\""count\"": 1}}]"" -c 450 -b 100 -l 50 --short_stat{code}
3. Run writing load test from 3rd machine:
{code:java}
perf_processes.py -g live_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":1000,\""addr_mint_limit\"":1000,\""mint_by\"":500,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4, \""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 10 -b 10 -l 10{code}
 

*Actual Results:*
Pool stopped writing after ~30K domain txns written.
17 nodes wrote 30616 txns, 8 nodes (11, 14, 1, 12, 9, 19, 20) wrote 30700.

Logs and metrics: s3://qanodelogs/indy-1770/live-20-11-18-production-mix-fees
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1770/live-20-11-18-production-mix-fees/ /home/ev/logs/indy-1770/live-20-11-18-production-mix-fees/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwwzz:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,ozheregelya,zhigunenko.dsr,,,,,,,,,"26/Nov/18 5:47 PM;Derashe;Problem reason:
 * Pool stopped work

Findings:
 * Unfortunately, logs were deleted until last few hours, when nodes just tried to reconnect to each other
 * But using validator_info we saw that part of nodes with 30700 has unequeal commited_state_root_hash and uncommited_state_root_hash of plugin ledger. Also we've mentioned that view_change happened right in the moment when pool stopped ordering. 
 * After investigating plugin's code, we've understand that probably one does't have code to revert unapplied state

Details:
 * If plugin need to be integrated in plenum codebase, it must implement several feautres (for example feature of applying and reverting state)
 * Plugins can use abstractions of Ledger, PrunningState and reqHandler (like sovtoken plugin do) or they can implement such a features by themselves (like sovtokenfees do).
 * In second case it seems that sovtokenfees use POST_REQUEST_APPLICATION hook to apply state, but in the same time it do not have functionality to revert state in case of need.
 * That probably led us to state_root_hash inconsistency

Action item:
 * Start a ticket in plugin's code. ;;;","26/Nov/18 8:14 PM;zhigunenko.dsr;*Steps to Reproduce:*
agent1, agent2
{code:java}
perf_processes.py -g pool -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":3,\""addr_mint_limit\"":10,\""mint_by\"":10,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4}"" -k ""[{\""get_nym\"":{\""count\"": 9}}, {\""get_schema\"":{\""count\"": 2}}, {\""get_attrib\"":{\""count\"": 2}}, {\""get_cred_def\"":{\""count\"": 2}}, {\""get_revoc_reg_def\"":{\""count\"": 1}}, {\""verify_payment\"":{\""count\"": 1}}]"" -c 450 -b 100 -l 50 --short_stat
{code}

agent3
{code:java}
perf_processes.py -g pool -m t -n 1 -y one -s 000000000000000000000000Trustee1 -s 000000000000000000000000Trustee2 -s 000000000000000000000000Trustee3 -s 000000000000000000000000Trustee4 --ext ""{\""payment_addrs_count\"":10,\""addr_mint_limit\"":1000,\""mint_by\"":500,\""payment_method\"":\""sov\"",\""plugin_lib\"":\""libsovtoken.so\"",\""plugin_init\"":\""sovtoken_init\"",\""trustees_num\"":4, \""set_fees\"":{\""1\"":1,\""100\"":1,\""101\"":1,\""102\"":1,\""113\"":1,\""10001\"":1}}"" -k ""[{\""nym\"":{\""count\"": 4}}, {\""schema\"":{\""count\"": 1}}, {\""attrib\"":{\""count\"": 3}}, {\""cred_def\"":{\""count\"": 1}}, {\""revoc_reg_def\"":{\""count\"": 1}}, {\""payment\"":{\""count\"": 9}}]"" -c 100 -b 10 -l 10 &
{code}

agent4
{code:java}
perf_processes.py -g pool -c 1 -n 1 -l 1 -k ""demoted_node"" -y one -b 100
{code}

*Actual Results:*
Pool lost its consensus about view_no
sudo read_ledger --type=domain --count 32700
sudo read_ledger --type=pool --count 14806
sudo read_ledger --type=sovtoken --count 78381

*Additional Info:*
Logs and metrics wiil be available on ~/logs/24-25.11/ soon;;;","29/Nov/18 12:52 AM;Derashe;During analyze of logs with load of nym+fees and node transactions, pool crashed. That happened because of slowness of ordering pool (node) transactions. 
 * A few nodes got behind pool in different moments, which led to OOM (because of stashing incoming requests). Somewhy, this slowness happens to happen in different time at different nodes. This led pool to consensus loss.
 * It was also mentioned that requests did not stashed, while behind nodes were catching up ledger.

Action items:
 * Retest this case, when fixes for optimizing pool txns will be done.
 * Define if we have problem with catchup. And if we do, then create ticket to fix that.

Upd:
 * After brief analisys it was revealed that we have some problems with catchup logic. That needs to be solved in scope of another ticket;;;","29/Nov/18 4:04 PM;ashcherbakov;So, the issues is caused by the following tickets:
 * INDY-1879: Don't use all pool ledger data in NODE transactions ordering - this is why NODE txns are slow which leads to View Changes and general performance degradation
 * INDY-1874: Stub: Stability with fees plugins - a bug in plugins leading to losing consensus after the view change
 * INDY-1876: A Node needs to be able to order requests received during catch-up - this is why node may go to infinite catchup and can not order
 * INDY-1881: Change catchup logic for correctly clearing requests queues - this is why requests are not cleared after catch-ups which may lead to OOM;;;",,,,,,,,,,,,,,,,,,,,,
Write and Read request flow,INDY-1868,35544,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,22/Nov/18 5:00 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,Create Activity diagrams for write and read request flows including optimistic and negative scenarios.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1202,,,No,,Unset,No,,,"1|hzwx1b:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"28/Nov/18 12:13 AM;ashcherbakov;PR: https://github.com/hyperledger/indy-plenum/pull/997;;;",,,,,,,,,,,,,,,,,,,,,,,,
Create Catch-up Sequence Diagram,INDY-1869,35545,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,ashcherbakov,ashcherbakov,22/Nov/18 5:02 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Create diagram(s) for Catch-up logic including
 * catch-up from client point of view
 * catch-up from node-to-node point of view",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1202,,,No,,Unset,No,,,"1|hzwx1j:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"29/Nov/18 3:59 PM;ashcherbakov;PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/996]
 * [https://github.com/hyperledger/indy-plenum/pull/1003];;;",,,,,,,,,,,,,,,,,,,,,,,,
Create a Diagram for Components,INDY-1870,35546,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,22/Nov/18 5:04 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Create Components Diagram for all Ledgers, States and other storages.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1202,,,No,,Unset,No,,,"1|hzwx1r:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"28/Nov/18 12:14 AM;ashcherbakov;PRs:
 * [https://github.com/hyperledger/indy-plenum/pull/997]
 * [https://github.com/hyperledger/indy-plenum/pull/999]

Changes:
 * Updated `storages.md`
 * Created a deployment diagras for storages;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermmitent test failure: test_view_change_after_back_to_quorum_with_disconnected_primary,INDY-1871,35553,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,22/Nov/18 8:18 PM,11/Oct/19 7:22 PM,28/Oct/23 2:47 AM,11/Oct/19 7:22 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/18 8:18 PM;ashcherbakov;test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-2507.txt;https://jira.hyperledger.org/secure/attachment/16306/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-2507.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzwx5w:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"11/Oct/19 7:22 PM;esplinr;The current Jenkins builds are sufficiently reliable, though we still see intermittent test failures. We expect to transition away from Jenkins toward a solution like GitLab CI soon.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Intermmitent test failure: test_primary_selection_increase_f ,INDY-1872,35554,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,Toktar,Toktar,22/Nov/18 11:06 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"When adding a new node to the pool and changing the value f, a new instance is created. After enabling the logic for removing replicas on performance degradaded, a problem was found with removing a replica immediately after creation.

[^test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-2554.txt]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1815,,,,,,,,,,,,,,,,,,,,"23/Nov/18 9:25 PM;ashcherbakov;test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-2778.txt;https://jira.hyperledger.org/secure/attachment/16314/test-result-plenum-2.prd-ubuntu1604-indy-x86_64-4c-16g-2778.txt","22/Nov/18 11:09 PM;Toktar;test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-2554.txt;https://jira.hyperledger.org/secure/attachment/16307/test-result-plenum-3.prd-ubuntu1604-indy-x86_64-4c-16g-2554.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1488,,,No,,Unset,No,,,"1|hzwxa5:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,,"28/Nov/18 12:12 AM;ashcherbakov;Fixed in https://github.com/hyperledger/indy-plenum/pull/992;;;",,,,,,,,,,,,,,,,,,,,,,,,
Different amount of protocol instances on different nodes,INDY-1873,35566,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,23/Nov/18 6:52 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.79,,,,0,,,,"Build Info:
indy-node 1.6.694

Steps to Reproduce:
1. Start a pool of 7 nodes.
2. Send some write requests.
3. Ensure that the batches are ordered in the instances 1 and 2.
4. Stop the primary of the instance 2 (3rd node).
5. Stop the master's primary (1st node).
6. Wait for a view change is started and completed.
7. Start the last stopped node (1st node).
8. Start the first stopped node (3rd node) and ensure that it is the primary of the instance 1 now.
9. Send 1 write request.
10. Verify that the new batches are ordered in the instances 1 and 2.

Actual Results:
There are 3 primaries and protocol instances (0, 1, 2) on the 1st and 3rd node and only 2 primaries and protocol instances (0, 2) on the other 5 nodes. The issue persists after another txns ordering and over time.

Expected Results:
We should have the same amount of protocol instances and primaries on each node in the pool.

Additional Info:
REPLICAS_REMOVING_WITH_DEGRADATION = None
REPLICAS_REMOVING_WITH_PRIMARY_DISCONNECTED = None

There is no issue with this parameters so it looks like replicas' killing logic should be improved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1759,,,,,,,,,,,,,,,,,,,,"23/Nov/18 6:49 PM;VladimirWork;1759_logs_to_the_new_ticket.tar.gz;https://jira.hyperledger.org/secure/attachment/16313/1759_logs_to_the_new_ticket.tar.gz","26/Nov/18 9:17 PM;VladimirWork;INDY-1873.PNG;https://jira.hyperledger.org/secure/attachment/16317/INDY-1873.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwwz8:",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"23/Nov/18 9:06 PM;Toktar;After view change and node retarting replica 1 should be removed. But it didn't happen because first 240 sec (window_size * min_cnt) throughput is not measurable and after we check are new requests ordered. After 240 sec there were no new requests and throughput was not measurable too.;;;","26/Nov/18 9:17 PM;VladimirWork;Build Info:
indy-node 1.6.700

Steps to Validate:
1. Start a pool of 7 nodes.
2. Send some write requests.
3. Ensure that the batches are ordered in the instances 1 and 2.
4. Stop the primary of the instance 2 (3rd node).
5. Stop the master's primary (1st node).
6. Wait for a view change is started and completed.
7. Start the last stopped node (1st node).
8. Start the first stopped node (3rd node) and ensure that it is the primary of the instance 1 now.
9. Send some write requests.
10. Verify that the new batches are ordered in the instances 0 and 2 and there is *no 1 instance* at any node.

Actual Results:
All batches are ordered successfully for 0 and 2 instances. There is no 1 instance at any node after Step 9. !INDY-1873.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,,
Stub: Stability with fees plugins,INDY-1874,35612,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,esplinr,esplinr,26/Nov/18 11:48 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,TShirt_L,,,"Work on the issue tracked in the Sovrin JIRA

https://sovrin.atlassian.net/browse/ST-497

POA: Pool ran into an incosistent state after the view change",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1901,,,No,,Unset,No,,,"1|hzwvif:00007",,,,Unset,Unset,EV 18.24,Ev 18.25,Ev 19.1,Ev-Node 19.02,Ev-Node 19.03,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,esplinr,VladimirWork,,,,,,,,,,"14/Feb/19 6:51 PM;VladimirWork;Fix will be tested in scope of INDY-1994.;;;","14/Feb/19 7:07 PM;anikitinDSR;Reasons:
 * Need to fix batch reverting for txns with fees, when we doing catchup (revert_unordered_batches)

Changes:
 * added LedgerUncommittedTracker for tracking uncommitted states (root_hash and uncommitted ledger size)

 * added batch reverting to state from tracker

 * added a couple of tests and fixed fixture building

PRs:
 * indy-plenum: [https://github.com/hyperledger/indy-plenum/pull/1084] , [https://github.com/hyperledger/indy-plenum/pull/1072], [https://github.com/hyperledger/indy-plenum/pull/1070], [https://github.com/hyperledger/indy-plenum/pull/1062]

 * token-plugin: [https://github.com/sovrin-foundation/token-plugin/pull/194]

Recomendation for QA:
 * Fix will be tested in scope of INDY-1994;;;",,,,,,,,,,,,,,,,,,,,,,,
Backup a running node,INDY-1875,35616,,Story,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,mgbailey,mgbailey,27/Nov/18 3:07 AM,18/Apr/19 11:59 PM,28/Oct/23 2:47 AM,,,,,,,1,EV-CS,help-wanted,,"*Story*
As a steward of node on an Indy network, I need to backup the indy database without stopping the service so that in the event of a failure I can quickly restore in order to meet up-time requirements.

*Notes*
* Synchronizing the Sovrin Test Net (currently less than 100,000 transactions) takes over an hour on a fresh system.
* RocksDb APIs exist for backing up the live database. It would help to have scripts that will backup and restore the database when commanded by an admin, without requiring that the service be stopped first. These scripts should be able to be run in non-interactive mode, so that they in turn may scripted.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwy50:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,mgbailey,xnopre,,,,,,,,,,"09/Jan/19 1:28 AM;xnopre;I need this script to backup node data in my ""connected"" node, and put and restore this data in my ""backup failover"" server, to keep this last up-to-date, to have a quick start in case I have to switch to this backup server. Can this ticket be put in higher priority ? Thanks;;;","28/Feb/19 8:06 AM;esplinr;We investigated this last summer when we were trying to take a snapshot of a node that we could restore for debugging purposes. You can see our proof of concept here:

https://github.com/hyperledger/indy-node/tree/master/tools/diagnostics

It worked in some situations, but we found that there were a lot of structures in memory that weren't being persisted.

With the addition of the BLS signatures, the freshness checks, and the audit ledger, that situation is likely to have changed.

Evernym won't be able to come back to this project until later in the year, but we would love to discuss strategies with someone who is able to dig in sooner.;;;",,,,,,,,,,,,,,,,,,,,,,,
A Node needs to be able to order requests received during catch-up,INDY-1876,35626,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,27/Nov/18 8:21 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"As of now, Replicas discard PrePrepares received during catch-up.

So, if catch-up is long enough, it may lead to infinite catch-ups and a lot of requests for missing PrePrepares, since during a high load the pool may order a lot.

Also we should not process Checkpoints during catch-up

*Acceptance criteria*
 * Add tests to make sure that a Node can order and apply all requests and 3PC messages received during catch-up
 * Simplify processing of 3PC messages during catch-up:
 ** Stash all 3PC messages and Checpoints during catch-up
 ** Once catch-up is finished - process them",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1949,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwvif:00004",,,,Unset,Unset,Ev 18.25,Ev 19.1,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,Toktar,,,,,,,,,,"28/Nov/18 6:40 PM;ashcherbakov;*PoA:*
 # Write tests simulating active ordering while a node does a catchup. We expect that all newly ordered requests need to be applied after the cacthup immediately.
 # Create a a separate 3PC Message Validator class and cover by unit tests. Each Replica should have its own 3PC Validator instance.
 The Validator returns one of the following values: [PROCESS, STASH, DISCARD]
 ** Validate method for 3PC messages:
 *** Check if viewNo is correct.
 **** If it's less than view_no - 1, then discard
 **** If it's more than view_no, then stash (see next sections)
 **** If it's equal to view_no - 1, and view change is in progress, then check if it's less than last_prepared_certificate
 **** If it's equal to viewNo, and view change is in progress - stash
 **** Otherwise - Process
 *** Check if isParticipating
 **** If not - stash
 *** Check if already ordered
 **** If yes - discard
 *** Сheck if within watermarks
 **** If not - stash
 ** Validator for Checkpoint messages
 *** If already stable - discard
 *** If not participating - stash
 # Create a stasher for 3PC messages attached to the Replica.
 ** All messages for which STASH is returned by the Validator go to the Stasher
 ** The stasher should have a limit to not stash too much
 ** All messages from the stasher are trying to be re-applied after
 *** Catch-up is finished
 *** View Change is finished
 *** watermarks are chamged
 # Write the following integration tests:

 ** Continue ordering while catching-up
 *** Expected result: a node applies stashed messages which were ordered during catchup, and will be up-to-date with the pool
 ** Continue ordering and stabilize checkpoints during catch-up
 *** Expected result - the node doesn't start catch-up again due to stashed checkpoints, and will be up-to-date after catch-up.
 ** A node starts view change latter, while other nodes already finished it and started ordering
 *** Expected result - the node successfully finished view change, and applied stashed messages that were ordered during view chnage, and will be up-to-date with the pool;;;","18/Dec/18 6:02 PM;ashcherbakov;The current work is in https://github.com/ashcherbakov/indy-plenum/tree/catchup-fixes;;;","12/Jan/19 3:54 AM;Toktar;Problem reason:
 - Current stash mechanism has a lot of bugs:
 ** process 3pc messages and checkpoints in catchup
 ** process checkpoints in view change
 ** late processing of unstashed messages 

Changes:
 - Refactoring stashing mechanism. Create 3 types of stashing:
 ** queue for 3pc messages and checkpoints received in catchup
 ** for messages from a next view
 ** for out of watermarks messages
 - Add tests

PR:
 * [https://github.com/hyperledger/indy-node/pull/1122]
 * [https://github.com/hyperledger/indy-plenum/pull/1046]

Version:
 * indy-node 1.6.753 -master
 * indy-plenum 1.6.652 -master

Risk factors:
 - Problem with ordering in catchup, view change. Performance degradation.

Risk:
 - Medium

Covered with tests:
 * [test_stashing_3pc_while_catchup.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-294ec0b032203ef2cf0abd48be770ead]
 * [test_stashing_3pc_while_catchup_checkpoints.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-0a9bf2d99bd0ce64e892a15b2b4701e8]
 * [test_replica_stasher.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-460da6f43d0f54da3852d5ad80984255]
 * [test_replica_unstashing.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-14832af9d700da535ee6d89e7691f4e8]
 * [test_stash_future_view.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-0e50e46f125b200e799d3067fa4759f9]
 * [test_stash_out_of_watermarks.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-e2d82babc11799caee9fe49e5c4be0a8]
 * [test_unstash_after_catchup_in_view_change.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-b83b05d0f6744d42770207355e481ed7]
 * [test_3pc_messages_validation.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-95600fdf1137ea91d61ab28d161944b2]
 * [test_replica_3pc_validation.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-e82220bb9f03171e61bf26ab539b2d6a]
 * [test_replica_checkpoint_validation.py|https://github.com/hyperledger/indy-plenum/pull/1046/files#diff-92507caf8a7c21885a02ab11416aab86]
 * and others

Recommendations for QA:

Production load. Check that performance are not degraded.;;;","14/Jan/19 9:01 PM;ozheregelya;Environment:
indy-node 1.6.753

Steps to Validate:
1. Setup the pool.
2. Run load test with production load (10 writes, 100 reads per sec).

Actual Results:
Part of requests were failed with CommonIOError from libindy. Results of load script:
{code:java}
Time 57774.64 Clients 10/10 Sent: 375618 Succ: 231415 Failed: 144150 Nacked: 0 Rejected: 0{code}
Errors in load script output: 
{code:java}
2019-01-13 20:03:39,058|ERROR|libindy.py|indy.libindy.native.indy.errors.indy| src/errors/indy.rs:73 | Casting error to ErrorCode: Plugged method error. Consider the error code.{code}
As for pool, it haven't stopped writing, but part of the nodes were lagged. Ledger sizes (domain | sovtoken):
persistent_node13 31576 | 31872    <<< the only node which have more sovtoken txns than domain ones
persistent_node14 42565 | 41774
persistent_node18 58636 | 56223
persistent_node20 58642 | 56242
persistent_node25 58727 | 56312
persistent_node19 58727 | 56312
persistent_node1 59599 | 57094
the rest ones 125755 | 116642

Logs and output of writing load script: s3://qanodelogs/indy-1876/load_script_out_14_01_2019
 Logs and metrics from the nodes: s3://qanodelogs/indy-1876/prod_load_14_01_2019
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1876/prod_load_14_01_2019/ /home/ev/logs/indy-1876/prod_load_14_01_2019/;;;","16/Jan/19 6:23 PM;Toktar;The main problem was in the incorrect client.
A new version
 * indy-node 1.6.759 -master
 * indy-plenum 1.6.654 -master;;;","18/Jan/19 12:47 AM;ozheregelya;*Environment:*
indy-node 1.6.759 (with disabled freshness)

*Steps to Validate:*
1. Setup the pool.
2. Run production load.

*Actual Results:*
Pool was able to write during 23 hours.
Tickets for the issues which were found during load test: INDY-1949, INDY-1955.;;;",,,,,,,,,,,,,,,,,,,
Support volume size customization in pool automation scripts,INDY-1877,35632,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,27/Nov/18 11:51 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,We should support volume size customization in pool automation scripts because now all volumes are created with default size according to instance type.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1840,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwvji:",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,VladimirWork,,,,,,,,,,,"07/Dec/18 10:45 PM;andkononykhin;*PoA*:
 * add tests to check that instances have volume with cusome size
 * improve  [boto3 api create_instances|https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances]  usage  to pass parameters for attached volumes
 * add relative variable to aws_manage playbook;;;","07/Dec/18 10:46 PM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1085;;;","10/Dec/18 4:49 PM;andkononykhin;*Problem reason*:

There was no way to customize size of  AWS EC2 volumes that are created for instances.

*Changes*:
 * added parameter _aws_ebs_volume_size_ for aws_manage role with default value of 16Gb

*Committed into*:

https://github.com/hyperledger/indy-node/pull/1085

*Risk factors*:

Nothing is expected.

*Risk*:

Low

*Covered with tests*:
 * extedned tests for _stateful_set.py_ ansible module

*Recommendations for QA*:
 * create test inventory using *namespace-config.py* with options for ebs volumes (e.g clients
{noformat}
--aws_clients_provisioner.aws_ebs_volume_size 10{noformat}
and nodes 

{noformat}
--aws_nodes_provisioner.aws_ebs_volume_size 17{noformat}

 * run _provision.yml_ playbook
 * check that all volumes created for clients and nodes have expected volumes
 * destroy;;;","11/Dec/18 1:12 AM;VladimirWork;Steps to Validate:
1. Create test inventory using namespace-config.py with options for ebs volumes (e.g clients
`--aws_clients_provisioner.aws_ebs_volume_size 10`
and nodes 
`--aws_nodes_provisioner.aws_ebs_volume_size 17`
2. Run provision.yml playbook.
3. Check that all volumes created for clients and nodes have expected volumes.
4. Destroy the pool.

Actual Results:
All volumes created for clients and nodes have expected volumes.;;;",,,,,,,,,,,,,,,,,,,,,
Errors 'Version not found' appear in journalctl during upgrade,INDY-1878,35636,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,ozheregelya,ozheregelya,28/Nov/18 1:43 AM,28/Nov/18 1:43 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Environment:
indy-node 1.6.688 -> 1.6.699

Steps to Reproduce:
1. Setup the pool with indy-node 1.6.688.
2. Perform upgrade to indy-node 1.6.699.
3. Look through journalctl.

Actual Results:
Following messages appear in journalctl:
{code:java}
Nov 23 15:00:40 fd033de9defa env[1644]: Building dependency tree...
Nov 23 15:00:40 fd033de9defa env[1644]: Reading state information...
Nov 23 15:00:40 fd033de9defa env[1644]: 15 packages can be upgraded. Run 'apt list --upgradable' to see them.
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '0.99.7.1' for 'libpam0g' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '2.1.9' for 'libselinux1' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '10.0.0' for 'python3-pip' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '1.0.1-11' for 'libpam-runtime' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '3.2-14' for 'lsb-base' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '3.3.2-2~' for 'python3' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '1.18~' for 'init-system-helpers' was not found
Nov 23 15:01:46 fd033de9defa env[1644]: E: Version '2.14' for 'libc6' was not found
Nov 23 15:02:04 fd033de9defa env[1644]: E: Version '5.2' for 'libstdc++6' was not found
Nov 23 15:02:04 fd033de9defa env[1644]: E: Version '25' for 'python3-setuptools' was not found
Nov 23 15:02:04 fd033de9defa env[1644]: E: Version '0.5' for 'debconf' was not found
Nov 23 15:02:04 fd033de9defa env[1644]: E: Version '1:3.0' for 'libgcc1' was not found
Nov 23 15:02:04 fd033de9defa env[1644]: E: Version '3.6' for 'python3' was not found
Nov 23 15:02:04 fd033de9defa env[1644]: E: Version '1.5' for 'python3-six' was not found
Nov 23 15:02:19 fd033de9defa env[1644]: E: Version '2.14' for 'libc6' was not found
Nov 23 15:02:19 fd033de9defa env[1644]: E: Version '3.3.2-2~' for 'python3' was not found
Nov 23 15:02:19 fd033de9defa env[1644]: E: Version '1:4.1.1' for 'libgcc1' was not found
Nov 23 15:02:34 fd033de9defa env[1644]: + deps='libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 libsodium18 at iptables indy-plenum=1.6.603 indy-node=1.6.699 sovrin=1.1.93'
Nov 23 15:02:34 fd033de9defa env[1644]: + '[' -z 'libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 libsodium18 at iptables indy-plenum=1.6.603 indy-node=1.6.699 sovrin=1.1.93' ']'
Nov 23 15:02:34 fd033de9defa env[1644]: + echo 'Try to donwload indy version libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 libsodium18 at iptables indy-plenum=1.6.603 indy-node=1.6.699 sovrin=1.1.93'
Nov 23 15:02:34 fd033de9defa env[1644]: Try to donwload indy version libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 libsodium18 at iptables indy-plenum=1.6.603 indy-node=1.6.699 sovrin=1.1.93
Nov 23 15:02:34 fd033de9defa env[1644]: + apt-get -y update
{code}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i000cn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't use all pool ledger data in NODE transactions ordering,INDY-1879,35655,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,Toktar,Toktar,28/Nov/18 10:25 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,,,,"Method processPoolTxn() is called in each ordering of transactions for pool ledger. The method calls ledger.getAllTxn() which returns all transactions in the ledger. This takes a very long time in proportion to the size of the ledger.

*Acceptance Criteria:*
 * Change call getNodeInfoFromLedger() in processPoolTxn() to faster way.
 * Change call getNodeInfoFromLedger() in nodeServiceChange() to faster way.
 * Add tests",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/18 9:04 PM;zhigunenko.dsr;image.png;https://jira.hyperledger.org/secure/attachment/16416/image.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwvif:i",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,zhigunenko.dsr,,,,,,,,,,,"01/Dec/18 1:15 AM;Toktar;PoA:

Pool transactions are ordered so long because in process  ledger.getAllTxn() is called. It's a very long operation because it returns all transactions from ledger. We need change it to faster way.

getNodeInfoFromLedger is called from:
 # getNodeData - doesn't use and can be pass
 # nodeServicesChanged - uses in onPoolMembershipChange for getting all services and compare with new services. I think we should add a new field to store services information. In theory, we can change logic of nodeServicesChanged but code will be not obvious. 
 # onPoolMembershipChange - check new node or not via field to store last services. For checking logical bug with unordered txn SeqNoDB will be used.
 # getNodeName - uses in blskeys, HA, keys changing. Will change to using date from node.nodestack.remotes

Tests:
 * Unit test for onPoolMembershipChange. Ledger contains a lot of transactions. Check that method works fast and getAllTxn is never called.
 * Integration test for write pool transactions in a big ledger (or long getAllTxn). Check time of execute operation.;;;","11/Dec/18 10:49 PM;Toktar;Problem reason:
 - Method  ledger.getAllTxn() is called in each ordering of transactions for pool ledger. And pool txns order very slow for a big Pool ledger.

Changes:
 - Remove uses of getAllTxn in nodeServicesChanged, onPoolMembershipChange and getNodeName 
 - Remove methods which have never used.
 - Add tests

PR:
 * [https://github.com/hyperledger/indy-node/pull/1091]
 * [https://github.com/hyperledger/indy-plenum/pull/1010]

Version:
 * indy-node 1.6.731 -master
 * indy-plenum 1.6.632 -master

Risk factors:
 - Problem with pool transactions ordering

Risk:
 - Low

Covered with tests:
 * test_on_pool_membership_changes

Recommendations for QA:
 * Load test with 10 NODE txn/sec for 4 hours. (Please, send me a metric from any one node to detect looper work time);;;","13/Dec/18 9:05 PM;zhigunenko.dsr;*Environment:*
node 1.6.371 (25 nodes)

*Steps to Validate:*
Load test with 10 NODE txn/sec for 4 hours

*Actual Results:*
There is no failures or view changes
 !image.png|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,
Don't use all pool ledger data in place where this can be avoided,INDY-1880,35657,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,Toktar,Toktar,28/Nov/18 10:36 PM,13/Feb/19 10:02 PM,28/Oct/23 2:47 AM,,,1.16.0,,,,0,,,,Find all usages of ledger.getAllTxn() and change it to faster way if this can be avoided.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-775,,,No,,Unset,No,,,"1|hzwx4f:2rzlk",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,,"01/Dec/18 2:19 AM;Toktar;PoA:

Places with ledger.getAllTxn()  calls with should be changed for faster way:
 * getNodeServices should be redefined. New method should be cashing nodes services. Use new field from INDY-1879.
 * Cashing self node id in pool manager (Can dest be changes?). It fixes problem with using rank.

Validator info uses getAllTxn too. But it happens every minute and it's not so often.;;;","06/Dec/18 9:34 PM;ashcherbakov;There are also the following places which should also be handled:
 * *TxnStackManager.getNodesServices - on every pool upgrade??? we should use state instead*
 * TxnStackManager._parse_pool_transaction_file  - we should use stat instead
 * TxnStackManager.nodeExistsInLedger - can be removed
 * TxnStackManager.nodeIds - looks like it's used in tests only?
 * Plenum's DomainReqHandler.countStewards - looks like isn't used in indy-node (overriden) - should remove then?
 * *Node.addGenesisNyms - iterates through the whole Domain ledger during each restart of the node!!!!*
 * *Node.collectNodeInfo - on every view change and restart!!!!*
 * pool_manager.id
 * *IndyNode: NodeAuthNr - on every upgrade???*
 * *IndyNode: PoolConfig.processLedger - after every catch up!!!!*;;;",,,,,,,,,,,,,,,,,,,,,,,
Change catchup logic for correctly clearing requests queues,INDY-1881,35685,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,Derashe,Derashe,29/Nov/18 3:48 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,,,,"For now after we finish catchup we do not correctly clear our request queues (node.requests, replica.requestQueues, etc..). In scope of this ticket, we need PoA and fix for this problem.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwvif:0c",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,,,,,,,,,,,"29/Nov/18 5:52 PM;Derashe;*PoA:*

We need to define ways how can we clear collections related to requests in node and replicas. Also we need to write tests to demonstrate that we are able to clear both finalized and non-finalized requests correctly.

To correctly clear all collections we must take into account all processes that happens after ordering (including hooks).
 * We can start with already implemented mechanism of clearing replica's queues.
 ** On master replica we already collecting all ""written"" pre-prepares and clear every included related requests. We could delete these requests from node.requests right away.
 ** We can add clearing of requestsQueues on backup replicas, because we clearing monitor statistic right after that. 
 * For master, the above relates to finalized requests, that were included in 3pc. But we must also consider finalized requests that were not included in 3pc (which must be stored in replica.requestQueues and replica.inBox, node.requests, node.requestSender) and non-finalized requests (which store in node.requests, node.requestSender). One of the ways we can do this by iterating throught node.requests and looking in seqNoDb. If request was placed in DB, than it was catched up, so we can delete it, following the above algorithm.
 * We must also consider correctly syncronizing token and domain ledger.;;;","29/Nov/18 10:01 PM;Derashe;As a result of a discussion we are going to make such a changes:
 * for requests which were included in prePrepares and sentPrePrepares on master replica with 3PC keys <= last_caught_up_3PC: delete them from requestQueues of master replica _(already implemented)_ and call node.requests.free() for them
 * for requests in requestQueues of master replica which presented in seqNoDb: delete them from requestQueues of master replica and call node.requests.free() for them
 * delete those non-finalized requests from node.requests which presented in seqNoDb

We are also going to test at least 6 cases to test postive and negative sides of the above changes:
 * freeing of finalized sent requests which were caughtup
 * freeing of finalized non-sent requests which were caughtup
 * deletion of non-finalized requests which were caughtup
 * no freeing of finalized sent requests which >= last_caught_up_3PC
 * no freeing of finalized non-sent requests which >= last_caught_up_3PC
 * no deletion of non-finalized requests which is not in ledger;;;","06/Dec/18 10:23 PM;Derashe;Build info:
 * indy-node [1.6.720|https://github.com/hyperledger/indy-node/releases/tag/1.6.720-master]
 * indy-plenum 1.6.624

Committed into:
 * [https://github.com/hyperledger/indy-plenum/pull/1004]

Covered with tests:
 * [test_clearing_requests_after_catchup.py|https://github.com/hyperledger/indy-plenum/pull/1004/files#diff-eec3d66b372d1ed53aecbff6040d454c]

Recommendations for QA: 
 * Check that catchup procedure works without an anomalies

P.S: Unfortunatelly, it seems it's too hard to reproduce tricky integration test situations described above. But me must ensure that catchup is still fucntioning as follows;;;","11/Dec/18 3:58 AM;ozheregelya;*Environment:*
 indy-node 1.6.725

*Steps to Validate:*
 1. Setup the pool.
 2. Demote one of nodes.
 3. Run load test.
 4. Promote demoted node back.
 5. Make sure that node completed catch up successfully and can write after catch up.

*Actual Results:*
 Catch up basically works. Catch up of large ledger will be tested during of load testing.;;;",,,,,,,,,,,,,,,,,,,,,
Diagram for Checkpoints and Watermarks,INDY-1882,35691,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,spivachuk,ashcherbakov,ashcherbakov,29/Nov/18 5:00 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1202,,,No,,Unset,No,,,"1|hzwxa6:i",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,spivachuk,,,,,,,,,,,"05/Dec/18 6:30 PM;spivachuk;PR containing a diagram showing the checkpoints feature and a diagram demonstrating checkpoint-based catch-up trigger and resumption of checkpoints stabilization after catch-up:
https://github.com/hyperledger/indy-plenum/pull/1007;;;",,,,,,,,,,,,,,,,,,,,,,,,
Documentation about Message Requests,INDY-1883,35693,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,29/Nov/18 5:00 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-792,,,No,,Unset,No,,,"1|hzwvif:0i",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,,"12/Dec/18 7:46 PM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/1027;;;",,,,,,,,,,,,,,,,,,,,,,,,
Trial of additional ledger catchup,INDY-1885,35717,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,zhigunenko.dsr,zhigunenko.dsr,30/Nov/18 2:22 AM,09/Oct/19 6:16 PM,28/Oct/23 2:47 AM,09/Oct/19 6:16 PM,,,,,,0,,,,"As Maintainer of Node
I need to be sure that in case of single node addition or promotion for:
1) pool ledger
2) config ledger
3) payment ledger
that there are no issues to catchup corresponding ledger if:
a) pool is idle
b) pool is under low load with domain transactions
c) pool is under low load with transactions whose specific for this ledger

Points of interest:
* memory consumption
* possible crashes or issues
* adequate lead-time

Order of magnitude: 10^6 transactions (for payment), 10^4 (for pool and config)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1118,,,No,,Unset,No,,,"1|i000uv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,zhigunenko.dsr,,,,,,,,,,,"09/Oct/19 6:16 PM;ashcherbakov;We have system tests for catchup and promotion/demotion;;;",,,,,,,,,,,,,,,,,,,,,,,,
Implement automated tests for validator-info script,INDY-1886,35759,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,dsurnin,dsurnin,01/Dec/18 12:31 AM,08/Jan/19 4:26 AM,28/Oct/23 2:47 AM,,,,validator-info,,,0,,,,"The to add automated tests for scripts started in PR 
https://github.com/hyperledger/indy-node/pull/1068

validator-info script should be splitted into several parts: argument parsing, environment interaction (fs check, user check, etc), gathering info, format info to simplify testing.

script testing should be enabled in build pipeline.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i0013j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),dsurnin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve ansible role tests,INDY-1887,35764,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,01/Dec/18 2:19 AM,01/Dec/18 2:19 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Now for each configuration role there are two scenarios - base (for testing role in isolation) and integration (which brings up lots of virtual machines), and those integration tests share a lot between roles. It would much better to reorganize tests so that roles have only base isolated tests, and create dummy role for running set of integration tests in one environment.

Also it would be great to finally revive molecule tests for aws_manage role

*Acceptance criteria*
* create dummy role of set of integration tests
* remove integration tests from configuration roles
* make molecule tests for aws_manage role actual",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|i0014f:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix throughput class creation bug,INDY-1888,35801,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,Toktar,Toktar,03/Dec/18 10:14 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.80,,,,0,,,,"Parameter {{start_ts}} in function {{create_throughput_measurement}} is defined by default as {{start_ts=time.perf_counter()}}. In python this definition will lead to the start initialization of the parameter. If we wil call this method in further without the parameter, {{start_ts}} will be equal to the first value.


*Acceptance Criteria*
 * Move the definition of the parameter start_ts to the function body.
 * Add test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1684,,,No,,Unset,No,,,"1|hzwxa6:w",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,zhigunenko.dsr,,,,,,,,,,,"04/Dec/18 8:30 PM;Toktar;Problem reason:
 - In first unmeasured time new replicas has throughput ratio 0, because start replica time was incorrect calculated. 

Changes:
 - The bug with calculation replica start time in create_throughput_measurement is fixed.

PR:
 * [https://github.com/hyperledger/indy-node/pull/1071]
 * [https://github.com/hyperledger/indy-plenum/pull/1005]

Version:
 * indy-node 1.6.714 -master
 * indy-plenum 1.6.619  -master

Risk factors:
 - In new instance start all replicas in this instance can be removed.

Risk:
 - Low

Covered with tests:
 * test_instances_not_degraded_on_new_instance

Recommendations for QA:
 * Case1: 
 ** Start 6 nodes in Docker
 ** +Wait 5 min+
 ** Start the 7th node
 ** Wait 5 min
 ** Check that no replica deleted.
 * Case2:
 ** Start 6 nodes in Docker
 ** +Wait 1 min+
 ** Start the 7th node
 ** Wait 5 min
 ** Check that no replica deleted.;;;","06/Dec/18 10:58 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.717
indy-plenum                1.6.621

*Steps to Validate:*
Case1: 
1) Start 6 nodes in Docker
2) Wait 5 min
3) Start the 7th node
4) Wait 5 min
5) Check that no replica deleted.

Case2:
1) Start 6 nodes in Docker
2) Wait 1 min
3) Start the 7th node
4) Wait 5 min
5) Check that no replica deleted.

*Actual Results:*
No replica deleted;;;",,,,,,,,,,,,,,,,,,,,,,,
Cleanup configurable parameters of ansible roles in pool automation,INDY-1889,35826,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,03/Dec/18 11:05 PM,03/Dec/18 11:05 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Currently inventory-init script exports way too many parameters to group and host vars from ansible roles. Actually there are many exported parameters that probably will never need override. Also, some of exported parameters has Jinja templates as a value which also reduces readability of resulting configuration.

*Acceptance criteria*
* Decide on a list of most important parameters for roles which will often require configuration
* Restrict parameters exported to group/host vars to this list
* [Optional] Allow export of less important parameters to separate group vars files",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|i001h3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.79,INDY-1890,35839,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,ozheregelya,ozheregelya,04/Dec/18 5:29 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,07/Dec/18 12:00 AM,0,,,,"*Version Information*
 indy-node 1.6.79
 indy-plenum 1.6.54
 sovrin 1.1.32

*Notices for Stewards:*
 (!) *Warning:* Embedded command-line tool _indy_ is *no longer available.* For further pool interaction use _indy-cli_ package [https://github.com/hyperledger/indy-sdk/tree/master/cli]
 (!) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*
 (!) *Validator-info output has been changed.* If you use validator-info as data source make sure that you have done necessary changes for compatibility
 (!) *The INDY-1818 (**Init Indy Node should output Base58-encrypted verkey already) affects nodes adding.* 
 (!) *Pool upgrade to sovrin 1.1.32 should be performed simultaneously for all nodes due to txn format changes.*
 *There are still some issues with token functionality*

*Major Fixes*
 INDY-1872 - Intermittent test failure: test_primary_selection_increase_f
 INDY-1867 - Pool stopped writing after production load with fees
 INDY-1862 - Node does not validate CLAIM_DEF's filed ref
 INDY-1850 - Sovrin package can't be upgraded
 INDY-1849 - Expected object or value error during batch handling
 INDY-1842 - Node that is not on network is shown as 'unreachable'
 INDY-1834 - Not enough information about upgrade in journalctl
 INDY-1827 - Need to fix 'aws_manage' playbook that fails when inventory directory is not specified
 INDY-1824 - Validator on Sovrin MainNet fails to upgrade, then fails to revert
 INDY-1823 - Pool stops writing during load testing against domain and pool ledgers together
 INDY-1820 - Node service stops during node key validation
 INDY-1818 - Init Indy Node should output Base58-encrypted verkey already
 INDY-1816 - Investigate slowness on TestNet due to demotion
 INDY-1815 - New instance was removed after creating
 INDY-1781 - Upgrade from 1.6.645+ version result errors about packages versions in journalctl
 INDY-1765 - RequestQueue in Replica doesn't clear after View Change

*Changes and Additions*
 INDY-1870 - Create a Diagram for Components
 INDY-1869 - Create Catch-up Sequence Diagram
 INDY-1868 - Write and Read request flow
 INDY-1851 - Plenum Consensus Protocol Diagram
 INDY-1846 - Change pool state root hash for BLS-signature in Commit messages
 INDY-1839 - 3rd party open source manifest
 INDY-1835 - Enable PreViewChange Strategy
 INDY-1828 - Need to add Names to AWS ec2 instances and security groups
 INDY-1826 - Need to securely automate SSH authenticity checking
 INDY-1822 - Limit RocksDB memory consumption
 INDY-1821 - Run very long load test on a small local pool
 INDY-1813 - AWS tags for pool automation AWS resources
 INDY-1795 - Adjust last_ordered_3pc and perform GC when detecting lag in checkpoints on backup
 INDY-1792 - Improve usability of current pool automation PoC
 INDY-1788 - As a dev/QA I need to be able to refer different groups in the same namespace using one inventory
 INDY-1784 - Remove security groups at tear-down phase for both tests and playbooks
 INDY-1780 - Clear Requests queue periodically
 INDY-1776 - Test ZMQ Memory Consumption with restarting of listener on every X connections
 INDY-1775 - Get information about how many client connections is usually in progress
 INDY-1774 - Do a long test with a load pool can handle
 INDY-1773 - Find out why validation of PrePrepares with Fees takes so long
 INDY-1772 - Check why backup instances stop ordering so often
 INDY-1771 - As a dev I need to be able to perform tests on docker
 INDY-1770 - Test ZMQ Memory Consumption with restricted number of client connections
 INDY-1769 - Run load tests with file storages
 INDY-1762 - Change dependency building for upgrade procedure
 INDY-1759 - Use persisted last_pp_seq_no for recovery of backup primaries
 INDY-1756 - Extend Load Script with GET_TXN
 INDY-1753 - Avoid redundant static validation during signature verification
 INDY-1747 - Find out why max node prod time increases during long load test

*Sovrin-related Issues:*
 INDY-1764 - Upgrade to latest Sovrin doesn't work if it depends on a newer IndyNode
 INDY-1763 - Verify Upgrade of the MainNet to the latest IndyNode
 INDY-1748 - Failed to upgrade from Sovrin 1.1.24 to Sovrin 1.1.26",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:03",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,TechWritingWhiz,VladimirWork,,,,,,,,,,"07/Dec/18 8:42 AM;TechWritingWhiz;Moved the due date due to missing information: indy-anoncreds # & Release version number. I can't submit a PR without these.;;;","07/Dec/18 5:19 PM;VladimirWork;Sovrin package version has been added. There will be no indy-anoncreds version anymore since we got rid of this dependency.;;;","11/Dec/18 1:50 AM;TechWritingWhiz;The PR is here: [https://github.com/sovrin-foundation/sovrin/pull/133];;;",,,,,,,,,,,,,,,,,,,,,,
Need to verify that network interface is deleted before removing security group,INDY-1891,35857,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,andkononykhin,andkononykhin,04/Dec/18 8:13 PM,06/Dec/18 9:44 PM,28/Oct/23 2:47 AM,,,,,,,0,devops,,,"Sometime tests for stateful_set module fail during security group termination logging  ""_DependencyViolation: DeleteSecurityGroup operation has a dependent object_"" even if associated instances have been terminated.

As it was explored the most possible reason is that network interfaces attached to some terminated instance still available and have association with security group which is going to be removed.

Suggested fix:
 * during ec2 instance termination detach it from the interface
 * during security group termination filter and delete all network interfaces that linked with the group and don't have ec2 instances attachment",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1784,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwx4f:6",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deletion of rests of an old client,INDY-1892,35859,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,Derashe,Derashe,04/Dec/18 10:02 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"We've mentioned that indy-cli folder appears. We need to clear the rest of client.

{{root@bfa3e734eb50:/home/indy# ll}}
{{ total 24}}
{{ drwxr-xr-x 5 indy indy 4096 Dec 1 15:57 ./}}
{{ drwxr-xr-x 6 root root 4096 Dec 1 15:57 ../}}
{{ -rw-r--r-- 1 indy indy 220 Aug 31 2015 .bash_logout}}
{{ -rw-r--r-- 1 indy indy 3771 Aug 31 2015 .bashrc}}
{{ drwxrwxr-x 6 indy indy 4096 Dec 1 15:57 .indy-cli/}}
{{ -rw-r--r-- 1 indy indy 655 May 16 2017 .profile}}
{{ root@bfa3e734eb50:/home/indy# ll .indy-cli/}}
{{ total 16}}
{{ drwxrwxr-x 6 indy indy 4096 Dec 1 15:57 ./}}
{{ drwxr-xr-x 5 indy indy 4096 Dec 1 15:57 ../}}
{{ drwxrwxr-x 4 indy indy 4096 Dec 1 15:57 networks/}}
{{ drwxrwxr-x 2 indy indy 4096 Nov 30 13:57 wallets/}}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa6:y",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,zhigunenko.dsr,,,,,,,,,,,"04/Dec/18 10:04 PM;Derashe;PRs:

https://github.com/hyperledger/indy-node/pull/1072
[https://github.com/hyperledger/indy-plenum/pull/1009]

Recomendation for QA:

Check that there is no indy-cli folder.;;;","05/Dec/18 9:24 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.717
indy-plenum                1.6.621
libindy                    1.6.8~869

 *Steps to Validate:*
1) look for artifacts in home directory
2) look for artifacts in config directory
3) try to launch _indy_
4) make sure that _pool_start.sh_ / _client_start.sh_ scripts are working

*Actual Results:*
No traces of deprecated CLI;;;",,,,,,,,,,,,,,,,,,,,,,,
Validator stuck in continual view change,INDY-1893,35882,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,,mgbailey,mgbailey,05/Dec/18 8:40 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,,,,"One of the steward validators on MainNet (indy-node 1.6.78) has been stuck in a view change for an extended time. This first came to notice when attempting to retrieve its status using get-validator-info from a client:
{code:java}
pool(mainnet):wallet(mainnet_wallet):did(J4N...Y5q):indy> ledger get-validator-info nodes=NewtonD
Validator Info:
Transaction has been rejected: Client request is discarded since view change is in progress
{}
{code}
This is the only node that experienced this issue. All other nodes view and primary information was consistant. A transaction was posted to the ledger, and this node did not post it in its database.

The node was able to recover following a restart of the service. Please examine the attached logs and validator-info dump (before the restart) to see if there is information that can be used to improved the stability of the product.

*UPDATE:*

At about 10:45 MST 6 nodes on TestNet owned by Evernym were rebooted simultaneously to apply a kernel update. One of these nodes was the current primary node. As a result of this, a view change was initiated, which was not completed properly on all nodes. 2 of the Evernym nodes selected a different primary than the rest, and 10 of the nodes owned by other stewards got into the same state that we see in this ticket: a never-completed view change.
{code:java}
pool(testnet):wallet(testnet_wallet):did(6fe...zSJ):indy> ledger get-validator-info
Validator Info:
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
Transaction has been rejected: Client request is discarded since view change is in progress
...{code}
I will be attaching logs from all of the nodes that I can get my hands on.

Restarting effected nodes clears the issue, but takes a lot of time since it must be done manually by our dispersed stewards. (see INDY-1896) This causes much concern on TestNet these days. Updating the priority accordingly.

 ","MainNet, indy-node 1.6.78",,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1896,,,,,,,,,,,,,,,,,,,,"05/Dec/18 8:36 AM;mgbailey;NewtonD_20181203.tar.gz;https://jira.hyperledger.org/secure/attachment/16341/NewtonD_20181203.tar.gz","05/Dec/18 8:40 AM;mgbailey;NewtonD_config_before;https://jira.hyperledger.org/secure/attachment/16340/NewtonD_config_before","06/Dec/18 8:44 AM;mgbailey;NodeTwinPeek.tar.gz;https://jira.hyperledger.org/secure/attachment/16356/NodeTwinPeek.tar.gz","06/Dec/18 8:38 AM;mgbailey;australia.tgz;https://jira.hyperledger.org/secure/attachment/16353/australia.tgz","06/Dec/18 8:38 AM;mgbailey;brazil.tgz;https://jira.hyperledger.org/secure/attachment/16352/brazil.tgz","06/Dec/18 8:38 AM;mgbailey;canada.tgz;https://jira.hyperledger.org/secure/attachment/16354/canada.tgz","06/Dec/18 8:38 AM;mgbailey;england.tgz;https://jira.hyperledger.org/secure/attachment/16349/england.tgz","06/Dec/18 8:38 AM;mgbailey;korea.tgz;https://jira.hyperledger.org/secure/attachment/16351/korea.tgz","06/Dec/18 8:38 AM;mgbailey;singapore.tgz;https://jira.hyperledger.org/secure/attachment/16350/singapore.tgz","11/Dec/18 6:39 AM;mgbailey;validator-info.log-20181207.gz;https://jira.hyperledger.org/secure/attachment/16384/validator-info.log-20181207.gz","06/Dec/18 8:38 AM;mgbailey;virginia.tgz;https://jira.hyperledger.org/secure/attachment/16348/virginia.tgz","06/Dec/18 8:44 AM;mgbailey;xsvalidatorec2irl.log.tgz;https://jira.hyperledger.org/secure/attachment/16355/xsvalidatorec2irl.log.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:06",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,mgbailey,,,,,,,,,,"06/Dec/18 9:15 AM;mgbailey;More logs that are too big to attach: 

[https://drive.google.com/file/d/1lInLBdrhl8ANyLvcmi6Ex2A-gAPZLc7I/view?usp=sharing]

[https://drive.google.com/file/d/1gKRINx3Sy3r8urqOw0HZp-z1KWj_Wdc4/view?usp=sharing]

https://drive.google.com/file/d/14MjSfhyX9UiAokG8SiJsHLdzBhbuqWtN/view?usp=sharing;;;","06/Dec/18 4:01 PM;ashcherbakov;Currently client requests are not processed during a view change. I think we should keep this for write requests only, while read requests need to be processed even during the view change.;;;","06/Dec/18 6:03 PM;ashcherbakov;{quote}At about 10:45 MST 6 nodes on TestNet owned by Evernym were rebooted simultaneously to apply a kernel update.
{quote}
[~mgbailey]
 Taken into account that the size of the pool is 18 (so f = 5), then 6 nodes is f+1.
 Restarting f+1 nodes at the same time (including Primary) means that the other nodes will start a View Change that can not be finished (since there are only n-f-1 nodes are left, which is less than minimal number of working nodes).

We will investigate exact reasons for this and do necessary fixes if needed, but in general I think we need to follow the rule, that *we should not have more than f nodes down (unless the whole pool is restarted) at the same time*. It's better to restart the nodes one by one.

 ;;;","06/Dec/18 8:22 PM;Derashe;After inverstigating the attached logs, we've found the next situation happened:
 * During restart after disconnecting of 6 nodes from pool (including primary) view change was inited on the rest of the pool (at 17:47). Quorum of Instance_changes (n-f) was collected because few nodes (of those 6) sent Instance_change before they've been restarted (because of slowness reason). Other Instance_change messages were collected when primary disconnected.
 * As a result 18-6 = 12 nodes stuck in unfinished view_change with view_no = 3 and no primary.
 * After ""6 group"" brought back to live, it saw primary as connected and did not do a view_change (stayed on view_no = 2).
 * So we have groups of 6 and 12 nodes. Neither of both have quorum to commit or end view_change;;;","06/Dec/18 8:32 PM;ashcherbakov;So, the following fixes will be done as a result of analysis:
 # INDY-1897: View Change needs to be triggered in BFT way
 # INDY-1896: process read requests and actions (POOL_RESTART) during view change.;;;","12/Dec/18 7:17 AM;mgbailey;Another log: https://drive.google.com/file/d/17Nj54NUAdeKyn2Jx3POa86rmeeEH5AFO/view?usp=sharing;;;",,,,,,,,,,,,,,,,,,,
Remove Windows compatibility component from Ubuntu installment,INDY-1894,35891,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,zhigunenko.dsr,zhigunenko.dsr,05/Dec/18 9:03 PM,06/Dec/18 5:35 AM,28/Oct/23 2:47 AM,,,,,,,0,,,,"There is unnecessary executable file in root home directory after indy-node installation

root@f8fe7f742be5:~# ll .indy
drwxr-xr-x 2 root root   4096 Dec  5 10:33 ./
drwx------ 6 root root   4096 Dec  5 11:12 ../
-rw-r--r-- 1 root root 331264 Dec  5 10:01 nssm_original.exe","indy-node                  1.6.717
indy-plenum                1.6.621
libindy                    1.6.8~869
libindy-crypto             0.4.5
python3-indy-crypto        0.4.5",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx64:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),zhigunenko.dsr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parametrize Docker file to work better with the System Tests,INDY-1895,35893,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,05/Dec/18 9:58 PM,09/Oct/19 11:20 PM,28/Oct/23 2:47 AM,09/Oct/19 11:20 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-781,,,No,,Unset,No,,,"1|hzwx4f:2x",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"09/Oct/19 11:20 PM;ashcherbakov;Done while working on system tests;;;",,,,,,,,,,,,,,,,,,,,,,,,
Issuing a pool restart does not work if there is no consensus,INDY-1896,35902,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,esplinr,esplinr,06/Dec/18 5:32 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.80,,,,0,,,,"The Sovrin Test Net hit a problem where 10 of 18 nodes were attempting a view change.

We attempted to restore consensus by putting a restart transaction on the ledger. The transaction was not executed. Instead we had to restart each node manually.

Instructions for a restart command are here:
https://github.com/hyperledger/indy-node/blob/5d4088f70f69113e6f1ae31373e3d201b8273fe9/design/pool_restart_txn.md",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1893,,,,,,,,,,,,,,,"06/Dec/18 6:28 AM;mgbailey;pool_restart.txt;https://jira.hyperledger.org/secure/attachment/16347/pool_restart.txt",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:09",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,mgbailey,Toktar,VladimirWork,,,,,,,,"06/Dec/18 6:28 AM;mgbailey;It is possible that the issue is not consensus, but rather that the nodes themselves rejected the instruction because they were in ""view change"". But this is unacceptable as well. I am attaching a file with the output of the attempt to post the transaction to TestNet: [^pool_restart.txt];;;","06/Dec/18 4:01 PM;ashcherbakov;Yes, we need to accept PoolRestart in any state, and being able to do it even during the view change.;;;","11/Dec/18 12:42 AM;Toktar;Problem reason:
 - All client requests are discarded in a View change process. In a long View change when something broke, we should have able to restart pool, upgrade pool and receive replies for read transactions requests.

Changes:
 - Allow processing of query requests, get transaction requests, action requests (restart pool, validator info) and force upgrade requests.

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1021]
 * [https://github.com/hyperledger/indy-plenum/pull/1016]
 * [https://github.com/hyperledger/indy-node/pull/1081]

Version:
 * indy-node 1.6.728 -master
 * indy-plenum 1.6.631 -master

Risk factors:
 - In view change requests that need a quorum will processed
 - In view change requests that not need a quorum will not processed

Risk:
 - Medium

Covered with tests:
 * [test_force_upgrade_process_in_view_change.py|https://github.com/hyperledger/indy-node/pull/1081/files#diff-6e1c3461db8df1fbe38b55bb0b788139]
 * test_pool_restart_in_view_change
 * test_client_read_request_not_discard_in_view_change_with_dict
 * test_client_write_request_discard_in_view_change_with_dict
 * test_client_get_request_not_discard_in_view_change_with_dict
 * test_client_msg_discard_in_view_change_with_request

Recommendations for QA:
 * Check replies for different requests types  in a view change process. For example, start load test on a docker pool (4 nodes will be enough) with get, query, writes requests and force view change every 1 or 2 minute. Write requests should be nacked in view change, other should work correctly
 * For a long view change disconnect first (f - 2) nodes.  Send request with POOL_RESTART, POOL_UPGRADE, VALIDATOR_INFO in view change. All requests should  work correctly.
  ;;;","11/Dec/18 11:53 PM;VladimirWork;Build Info:
indy-node 1.6.728

Steps to Validate:
1. Check replies for different requests types in a view change process.
2. For a long view change disconnect first (f - 2) nodes.
3. Send request with POOL_RESTART, POOL_UPGRADE, VALIDATOR_INFO in view change.

Actual Results:
There are no REQNACK get requests during VC in progress and write requests return REQNACK or REJECT at the same time.
POOL_RESTART, POOL_UPGRADE, VALIDATOR_INFO work during VC in progress.;;;",,,,,,,,,,,,,,,,,,,,,
PoA: View Change needs to be triggered in BFT way,INDY-1897,35923,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,06/Dec/18 8:30 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"The issue caused problems from INDY-1893.
 * As of now, INSTANCE_CHANGE message doesn't have instance change ID as required by RBFT paper (see IV.D).
 * So, a node can process ALL instance change messages, including very old ones (from the nodes which are already not available).
 * So, a node may start a view change receiving INSTANCE_CHANGE from less than n-f nodes, if it's stashed some old INSTANCE_CHANGEs from nodes that didn't send INSTANCE_CHANGE now.
 * This breaks BFT principles, that either all nodes should start the view change, or all nodes should not start the view change.

 

*Acceptance criteria*
 * Create a test reproducing a problem
 ** Send INSTANCE_CHANGE by f+1 nodes only, so that other nodes don't accept it, and don't start the view change.
 One of this nodes must be a master primary.
 ** Stop these f+q nodes
 ** Wait until other nodes send INSTANCE_CHANGE because they lost primary
 ** *Make sure they don't start view change (this will fail with the current code)*
 ** Restart f+1 nodes
 ** make sure the pool is functional.
 * Explore options of how we can fix it
 * Create necessary tickets for implementation

*One of the possible options*
 * Add instance change ID into INSTANCE_CHANGE message
 ** See [https://pakupaku.me/plaublin/rbft/5000a297.pdf,] IV.D. - *looks like there is a bug there*
 ** *Every new INSTANCE_CHANGE needs to be sent with a new (incremented) ID - this is not explicitly said in RBFT paper (is it a bug there?)*
 ** Think about whether we need a new protocol version for nodes (most probably no, since we can add it as an optional field at the end).
 * Change INSTANCE_CHANGE processing logic to start view change only if there is a quorum of INSTANCE_CHANGEs with the same ID and viewNo.
 ** Has an INSTANCE_CHANGE ID parameter for the node.
 ** Discard (stash?) INSTANCE_CHANGE  messages with ID less than the current node's one
 ** Check and send INSTANCE_CHANGE to others if INSTANCE_CHANGE ID is greater than the current one

 * Think about restoring INSTANCE_CHANGE ID
 ** Either clear it after the view change, or make it persistent and increment during the whole life of the pool
 ** If clear it after a view change / restart, then a node needs to start a view change if it gets a quorum of INSTANCE_CHANGE with ids less than this node's one.
 This is needed to handle the situation when all nodes re-started except the current one.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1903,,,,,INDY-1973,INDY-1984,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwvif:000000o",,,,Unset,Unset,Ev 18.25,Ev 19.1,Ev-Node 19.02,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"12/Dec/18 11:52 PM;sergey.khoroshavin;Some results of *preliminary analysis*:
* while INSTANCE_CHANGE messages implemented in plenum don't have ID, however they have viewNo (which isn't explicitely required by RBFT paper)
* RBFT paper states, that INSTANCE_CHANGE ID should be incremented only when _n-f_ messages with current ID are collected
* however the very same event (_n-f_ INSTANCE_CHANGE messages) triggers start of a view change, which increments _viewNo_
* so current implementation with _viewNo_ actually does the same thing as RBFT paper requires
* and it doesn't prevent incidents like INDY-1903

However we could try to come up with a different algorithm for assigning IDs to INSTANCE_CHANGE messages, which would fix an issue. Description already contains some of options which can be tried. One more option is to actually instantiate some binary byzantine agreement protocol (not unlike ABA subprotocol from HoneyBadger) to make sure that enough nodes actually came to agreement before starting a view change, but this also requires research.

So, *current plan of attack* is:
* implement simple testable models of different strategies to assign IDs to INSTANCE_CHANGE messages
* implement a number of randomized stress tests for these models
* exercise tests on models to pick the most suitable one (or find that no model is robust enough)
* if good enough model is found describe it here and implement it in main codebase;;;","28/Jan/19 11:00 PM;sergey.khoroshavin;1. Adding additional ID to INSTANCE_CHANGE and discarding messages with id < current_id - 1 leads to following failure scenario:
* network outage between primary and Node2 happens
** Node2 broadcasts INSTANCE_CHANGE(view_no=1, id=0)
** Node2 sets current_id to 1
* primary is restarted
** Node2 broadcasts INSTANCE_CHANGE(view_no=1, id=1)
** Node2 sets current_id to 2
** Node3 and Node4 broadcast INSTANCE_CHANGE(view_no=1, id=0)
** Node3 and Node4 set current_id to 1
** Node2 discards INSTANCE_CHANGE from Node3 and Node4 because its current_id is already 2
** Node3 and Node4 start view change because they have enough INSTANCE_CHANGE messages (from Node3, Node4 and old message from Node2)
* Two nodes are in view change, two are not - consensus lost

2. Not discarding INSTANCE_CHANGE messages with too low IDs restores old failure case:
* network outage between primary and Node2 happens
** Node2 broadcasts INSTANCE_CHANGE(view_no=1, id=0)
* Node3 is restarted, discarding INSTANCE_CHANGE from Node2
* primary is restarted
** Node2 broadcasts INSTANCE_CHANGE(view_no=1, id=1)
** Node3 and Node4 broadcast INSTANCE_CHANGE(view_no=1, id=0)
** Node2 and Node4 start view change because they have enough INSTANCE_CHANGE messages (from Node3, Node4 and old message from Node2)
** Node3 doesn't start view change
* Two nodes are in view change, two are not - consensus lost

3. Idea with incrementing current_id and broadcasting INSTANCE_CHANGE upon receiving INSTANCE_CHANGE with id higher that current_id might work, however this makes it possible to trigger view change by just one node (which is a serious security breach) and greatly increases chance of view change:
* some node broadcasts INSTANCE_CHANGE(id=1000)
* all other nodes find id greater than their current_id, increment it and also broadcast INSTANCE_CHANGE(id=1000)
* view change happens, triggered by just one node

4. Attempt to mitigate situation in 3 by requiring f+1 quorum of INSTANCE_CHANGE messages with high id is almost equivalent to allowing just f+1 INSTANCE_CHANGE messages to start a view change for reasons described in 3. Further increasing required quorum to n-f INSTANCE_CHANGE messages with high id doesn't make sense because if we already got this many messages then we should start view change anyways.  

All in all, this (together with previous comment) leads to following *conclusions*:
* current implementation is equivalent to what is described in RBFT paper
* just adding some additional id to INSTANCE_CHANGE won't help, and can actually make system less stable
* however if >f and <n-f nodes start view change then ordering will be halted, which theoretically should trigger view change on remaining nodes, however this was not the case in INDY-1903. Ordering halt detection should be reliably addressed by INDY-1911
* if we need to _reliably_ decide by all honest to either start a view change or not without relying on some ""primary"" then some form of Asynchronous Byzantine Binary Agreement should be implemented, like ones in Honeybadger or Algorand, however this is quite complex task and out of scope of this issue;;;","30/Jan/19 4:12 PM;ashcherbakov;{quote}3. Idea with incrementing current_id and broadcasting INSTANCE_CHANGE upon receiving INSTANCE_CHANGE with id higher that current_id might work, however this makes it possible to trigger view change by just one node (which is a serious security breach) and greatly increases chance of view change:
{quote}
Why other nodes broadcast INSTANCE_CHANGE  if they don't think that view change needs to be triggered? IIRC, Instance Change is broadcasted on receiving Instance Change from another node only if the receiver think that the view change needs to happen.
Another possible condition is if the receiver gets n-f instance changes.;;;","30/Jan/19 7:14 PM;sergey.khoroshavin;{quote}Why other nodes broadcast INSTANCE_CHANGE  if they don't think that view change needs to be triggered?{quote}

Because of this requirement

{quote}Check and send INSTANCE_CHANGE to others if INSTANCE_CHANGE ID is greater than the current one{quote}

Yes, probably I missed ""check"" part, I'll improve tests and see how it goes.

;;;","31/Jan/19 7:44 PM;sergey.khoroshavin;After implementing 
{quote}Check and send INSTANCE_CHANGE to others if INSTANCE_CHANGE ID is greater than the current one{quote}
the following failure case was found:
* Network outage between primary and Node4 happens
** Node4 broadcasts INSTANCE_CHANGE(view_no=1, id=0) and sets current_id to 1
* Primary gets restarted
** Node2 and Node3 broadcast INSTANCE_CHANGE(view_no=1, id=0) and set current_id to 1
** Node4 broadcasts INSTANCE_CHANGE(view_no=1, id=1) and set current_id to 2
** Node2 and Node3 get each others INSTANCE_CHANGE(view_no=1, id=0) message, have quorum (due to old INSTANCE_CHANGE from Node4) and start a view change, incrementing their view_no
** Node4 discards INSTANCE_CHANGE(view_no=1, id=0) messages from Node2 and Node3 because it already sent INSTANCE_CHANGE(view_no=1, id=1)
** Node2 and Node3 discard INSTANCE_CHANGE(view_no=1, id=1) message from Node4 because they are already in a view change
* Two nodes are in view change, two are not - consensus lost;;;","01/Feb/19 4:49 PM;ashcherbakov;The result of this research, is that in order to have really predictable and BFT way of starting a view change, we need to implement at least a simple version of Binary Byzantine Agreement.
We created INDY-1973 for this;;;","01/Feb/19 6:47 PM;sergey.khoroshavin;Also some failure cases were already covered by tests in https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/view_change/test_view_change_with_different_ic.py;;;",,,,,,,,,,,,,,,,,,
Statistics for outdated requests,INDY-1898,35930,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,sergey-shilov,sergey-shilov,sergey-shilov,06/Dec/18 9:13 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"In scope of INDY-1836 we enabled a strategy for dropping outdated requests. Now we need to have a statistics regarding requests dropped on propagates and ordering phases to diagnose pool's health.

*Acceptance criteria:*
* Separate counters for requests dropped on propagates phase and ordering phase
* Show these counters:
** as a part of extended validator info
** as a part of diagnostic metrics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1836,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa6:z",,,,Unset,Unset,EV 18.24,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provision playbook fails during adding hosts to known_hosts file,INDY-1899,35935,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,andkononykhin,andkononykhin,06/Dec/18 11:24 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,devops,,,"Initial run almost always fails with the following error:

 
{code:java}
TASK [aws_manage : Add host keys into '/home/xop/Projects/indy-dev/indy-node/pool_automation/review/inventory/../ssh/known_hosts.clients'] *****************************************************************
 ...
 ""msg"": ""No key specified when adding a host""{code}
 Repeated runs finally pass but it should be fixed anyway.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1826,,,,,INDY-1905,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxa6:ry",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ozheregelya,,,,,,,,,,,"11/Dec/18 12:39 AM;andkononykhin;*PoA:*
 * debug _ssh-keyscan_ command execution results as it seems it doesn't provide any keys and doesn't fail at the same time

 ;;;","11/Dec/18 12:57 AM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1087;;;","11/Dec/18 4:35 PM;andkononykhin;*Problem reason*:

Provision playbook failed during adding hosts to ssh known_hosts file. The error was about missed host keys.

*Changes*:
 * added logic with retries for ssh-keyscan command since it doesn't return any error in failure cases
 * improved ssh-keyscan conditional check for failure result

*Committed into:*

https://github.com/hyperledger/indy-node/pull/1087

*Risk factors*:

Nothing is expected.

*Risk*:

Low
 

*Covered with tests*:

no, manually tested. Automated tests for aws-manage role are not maintained for now (INDY-1782)


*Recommendations for QA*:
 * create namespace directory
 * run provision playbook
 * ensure that the error doesn't appear any more
 ** there might be a case when ssh-keyscan fails even after all multiple tries (for now: 5 tries with 5 seconds delay). In such case the playbook will fail at that stage instead of later host adding logic as it was before. Such a failure might be treated as expected. Possible solutions:
 *** restart provision playbook
 *** create a new task to tune default retry and delay parameters since they likely depend on AWS infrastructure and network state
 * destroy the pool;;;","12/Dec/18 8:10 AM;ozheregelya;*Steps to Validate:*
1. Run provision and destroy playbooks several times.

*Actual Results:*
After several runs of provision and destroy playbooks named error was not appeared.;;;",,,,,,,,,,,,,,,,,,,,,
Stub: PoA for combining Domain and Payment ledgers,INDY-1900,35983,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ashcherbakov,ashcherbakov,07/Dec/18 9:21 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,,,,,0,,,,"This is a stub for ST-501.

https://sovrin.atlassian.net/browse/ST-501",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1901,,,No,,Unset,No,,,"1|hzwvif:0003r",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"20/Dec/18 8:08 PM;ashcherbakov;This will be addressed in INDY-1921;;;",,,,,,,,,,,,,,,,,,,,,,,,
Stubs for Sovrin Jira,INDY-1901,35984,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,07/Dec/18 9:35 PM,09/Oct/19 12:26 AM,28/Oct/23 2:47 AM,09/Oct/19 12:26 AM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-2,,ST,Done,No,,Unset,No,,,"1|i0024n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepare a doc with useful info about ZeroMQ,INDY-1902,35994,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey-shilov,sergey-shilov,sergey-shilov,08/Dec/18 1:04 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,,0,,,,ZeroMQ has several unclear features that affects consumer application. In scope of this ticket I'm going to describe known features/issues/limitation of ZeroMQ that were found during our testing as a part of knowledge sharing.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-792,,,No,,Unset,No,,,"1|hzwxa6:ri",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,"18/Dec/18 6:34 PM;sergey-shilov;Corresponding doc created:
https://github.com/hyperledger/indy-plenum/blob/master/docs/misc/zeromq_features.md;;;",,,,,,,,,,,,,,,,,,,,,,,,
Consensus is lost on SLN,INDY-1903,36019,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,10/Dec/18 3:38 PM,24/Mar/20 11:56 PM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,,,,The log files can be found in https://drive.google.com/open?id=12sGi0D5cJTJp83FyBByF-fSu3mOcD_cN,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1984,,,,,INDY-1909,INDY-1910,INDY-1911,INDY-1897,,,,,,,,,,,,"11/Dec/18 5:58 AM;mgbailey;OASFCU.tgz;https://jira.hyperledger.org/secure/attachment/16377/OASFCU.tgz","11/Dec/18 5:58 AM;mgbailey;ServerVS.tgz;https://jira.hyperledger.org/secure/attachment/16374/ServerVS.tgz","11/Dec/18 5:58 AM;mgbailey;Stuard.logs.tgz;https://jira.hyperledger.org/secure/attachment/16378/Stuard.logs.tgz","12/Dec/18 12:52 AM;mgbailey;TNO.tgz;https://jira.hyperledger.org/secure/attachment/16395/TNO.tgz","11/Dec/18 5:58 AM;mgbailey;danube.tgz;https://jira.hyperledger.org/secure/attachment/16379/danube.tgz","11/Dec/18 5:58 AM;mgbailey;digitalbazaar_logs_20181210.tgz;https://jira.hyperledger.org/secure/attachment/16380/digitalbazaar_logs_20181210.tgz","12/Dec/18 12:52 AM;mgbailey;esatus_AG.tgz;https://jira.hyperledger.org/secure/attachment/16396/esatus_AG.tgz","11/Dec/18 6:35 AM;mgbailey;ev1_log_20181210.tgz;https://jira.hyperledger.org/secure/attachment/16383/ev1_log_20181210.tgz","11/Dec/18 5:58 AM;mgbailey;findentity.log.20181210.gz;https://jira.hyperledger.org/secure/attachment/16382/findentity.log.20181210.gz","12/Dec/18 2:08 AM;mgbailey;ibm-mainnet-validator-logs.tar.gz;https://jira.hyperledger.org/secure/attachment/16397/ibm-mainnet-validator-logs.tar.gz","11/Dec/18 5:58 AM;mgbailey;pcValidator01.tgz;https://jira.hyperledger.org/secure/attachment/16381/pcValidator01.tgz","11/Dec/18 5:58 AM;mgbailey;royal_sovrin.tgz;https://jira.hyperledger.org/secure/attachment/16376/royal_sovrin.tgz","11/Dec/18 5:58 AM;mgbailey;trustscience-validator02.tgz;https://jira.hyperledger.org/secure/attachment/16375/trustscience-validator02.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:06i",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,mgbailey,peacekeeper,RyanWest,sergey.khoroshavin,,,,,,,,"10/Dec/18 7:47 PM;sergey.khoroshavin;Investigation of log files from _ev1_ node showed the following:
* ordering was performed on all instances except 3 and 5 until 2018-12-08 05:24:43
* 05:25:37: first INSTANCE_CHANGE(viewNo: 3, reason: 26) messages started to appear signaling that nodes lost connection to master primary
* 05:25:54: _ev1_ also lost connection to master primary (which was _danube_)
* 05:25:56: first message requests of LEDGER_STATUS started to appear, which could be indication that some nodes started view change with following catch up
* 05:25:56: _ev1_ also decided to send INSTANCE_CHANGE(viewNo: 3, reason: 26), but quorum still wasn't reached
* 05:25:57: first VIEW_CHANGE_DONE(viewNo: 3, name 'royal_sovrin') messages started to appear signaling that nodes are trying to finish view change
* 05:26:00: there was single INSTANCE_CHANGE(viewNo: 4, reason: 26) from _iRespond_, which means that _iRespond_ lost connection to new master primary which is _royal_sovrin_
* 05:26:12: connection to _danube_ was reestablished, and _danube_ was seemingly restored to have viewNo 2
* 05:29:43: request from client was received (seems like it was monitoring ATTRIB transaction), and there was attempt to order it in view 2, but quorum wasn't reached
* 05:32:56: INSTANCE_CHANGE(viewNo: 4, reason: 28) messages started to appear signaling that nodes that started view change to view 3 were not able to finish it (most probably due to lack of quorum of VIEW_CHANGE_DONE messages) and trying to move to next view, but quorum wasn't reached
* after that nothing happened except periodic failed attempts to order incoming requests

Seems like there are *24* active validator nodes, so *f=7* and *n-f=17*
Before view change 3PC consensus messages were coming from all nodes.
Instance change request to view 3 was received from following 15 nodes (16 if counting _ev1_, *1 less than needed to reach quorum*):
* Aalto
* amihan-sovrin (18 seconds earlier than others)
* atbsovrin
* BIGAWSUSEAST1-001
* digitalbazaar (18 seconds earlier than others)
* DustStorm
* ibm
* icenode
* NewtonD (18 seconds earlier than others)
* prosovitor (simultaneous with ledger status request)
* ServerVS
* Stuard
* TNO
* VeridiumIDC
* zaValidator

Ledger status requests and view change done messages were received from following nodes 16 nodes (*1 less than needed to reach quorum*):
* Aalto
* atbsovrin
* BIGAWSUSEAST1-001
* digitalbazaar
* esatus_AG
* findentity
* icenode
* iRespond (later than others, simultaneous with IC(4, 26))
* OASFCU
* pcValidator01
* prosovitor
* royal_sovrin
* ServerVS
* Stuard
* trustscience-validator02
* VeridiumIDC

When trying to order new incoming requests following 7 (8 if counting _ev1_) nodes were sending 3PC consensus messages related to view 2 (so they were definitely not in view change):
* amihan-sovrin
* danube
* DustStorm
* ibm
* NewtonD
* TNO
* zaValidator
;;;","11/Dec/18 6:35 AM;mgbailey;Uploading updated ev1 logs: [^ev1_log_20181210.tgz];;;","11/Dec/18 8:32 PM;sergey.khoroshavin;Analysis of logs from *danube* showed that it was indeed a short network outage and not a restart, as originally thought. Also not all connections were disrupted long enough, so just 16 nodes (which is less than _n-f_) should have sent INSTANCE_CHANGE(3, 26), which *correlates with previous observations*.
{code}
2018-12-08 05:25:53,947|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from VeridiumIDC
2018-12-08 05:25:54,059|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from Aalto
2018-12-08 05:25:54,059|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from Stuard
2018-12-08 05:25:54,059|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from icenode
2018-12-08 05:25:54,059|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from TNO
2018-12-08 05:25:54,059|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from ServerVS
2018-12-08 05:25:54,240|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from BIGAWSUSEAST1-001
2018-12-08 05:25:54,260|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from digitalbazaar
2018-12-08 05:25:54,314|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from ibm
2018-12-08 05:25:54,435|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from zaValidator
2018-12-08 05:25:54,435|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from iRespond
2018-12-08 05:25:54,435|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from prosovitor
2018-12-08 05:25:54,435|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from DustStorm
2018-12-08 05:25:54,453|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from atbsovrin
2018-12-08 05:25:54,566|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from ev1
2018-12-08 05:25:54,819|NOTIFICATION|keep_in_touch.py|CONNECTION: danube disconnected from amihan-sovrin
2018-12-08 05:26:18,926|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to digitalbazaar
2018-12-08 05:26:18,926|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to BIGAWSUSEAST1-001
2018-12-08 05:26:19,356|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to atbsovrin
2018-12-08 05:26:19,448|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to iRespond
2018-12-08 05:26:21,475|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to zaValidator
2018-12-08 05:26:21,476|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to TNO
2018-12-08 05:26:21,703|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to Aalto
2018-12-08 05:26:22,177|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to ibm
2018-12-08 05:26:25,929|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to Stuard
2018-12-08 05:26:26,357|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to DustStorm
2018-12-08 05:26:28,119|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to icenode
2018-12-08 05:26:29,191|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to ev1
2018-12-08 05:26:33,294|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to VeridiumIDC
2018-12-08 05:26:33,324|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to ServerVS
2018-12-08 05:26:33,442|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to prosovitor
2018-12-08 05:26:33,593|NOTIFICATION|keep_in_touch.py|CONNECTION: danube now connected to amihan-sovrin
{code}

Analysis of logs from *trustscience-validator02* showed that there were some INSTANCE_CHANGE messages stashed on this node since last restart and view change, which led to reaching INSTANCE_CHANGE quorum even though number of nodes affected by connectivity issues with *danube* was less than _n-f_.
{code}
2018-11-16 17:28:45,626|INFO|node.py|trustscience-validator02 started participating
2018-11-20 02:04:47,908|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-11-21 06:28:59,483|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-11-21 06:29:39,192|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-11-21 06:57:02,945|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-11-21 12:18:49,536|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 12:35:25,731|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 12:37:24,494|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 12:38:36,052|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:10:56,172|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:11:56,186|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:12:33,498|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:12:36,527|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from atbsovrin
2018-11-21 17:37:41,063|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 20:47:27,820|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 20:47:31,092|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-21 20:48:27,838|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 20:48:31,106|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-21 21:28:17,666|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 21:28:37,668|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-21 21:29:17,676|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 21:29:37,698|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-25 06:30:49,558|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-27 19:13:48,993|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from zaValidator
2018-11-27 19:14:49,002|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from zaValidator
2018-11-29 00:17:27,639|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-29 00:17:35,991|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from OASFCU
2018-11-29 00:17:35,993|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-11-29 00:18:27,647|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-29 00:18:35,998|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from OASFCU
2018-11-29 00:18:36,019|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-11-29 00:19:27,662|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-29 00:19:36,015|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from OASFCU
2018-11-29 06:31:55,613|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-12-01 06:31:36,454|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-12-01 22:03:59,513|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from TNO
2018-12-02 07:24:45,461|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from digitalbazaar
2018-12-07 02:26:32,277|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-12-08 05:27:21,267|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from digitalbazaar
2018-12-08 05:27:21,777|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-12-08 05:27:22,018|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from NewtonD
2018-12-08 05:27:39,892|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ServerVS
2018-12-08 05:27:39,913|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from VeridiumIDC
2018-12-08 05:27:39,949|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from TNO
2018-12-08 05:27:39,998|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-12-08 05:27:40,004|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-12-08 05:27:40,010|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Aalto
2018-12-08 05:27:40,045|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from BIGAWSUSEAST1-001
2018-12-08 05:27:40,185|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ibm
2018-12-08 05:27:40,192|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from atbsovrin
2018-12-08 05:27:40,284|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-12-08 05:27:40,291|INFO|view_changer.py|trustscience-validator02 received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from prosovitor
2018-12-08 05:27:40,291|NOTIFICATION|view_changer.py|VIEW CHANGE: trustscience-validator02 initiating a view change to 3 from 2
{code}
;;;","11/Dec/18 9:27 PM;sergey.khoroshavin;Logs from *findentity* show same situation as in *trustsience-validator02*:
{code}
2018-11-16 17:27:19,684|INFO|node.py|findentity started participating
2018-11-20 02:03:19,330|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-11-21 06:27:29,974|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-11-21 06:28:09,679|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-11-21 06:55:33,420|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-11-21 12:17:19,822|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 12:33:56,012|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 12:35:54,763|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 12:37:06,324|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:09:26,438|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:10:26,454|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:11:03,754|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-11-21 13:11:06,791|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from atbsovrin
2018-11-21 17:36:11,182|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 20:45:57,821|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 20:46:01,091|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-21 20:46:57,832|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 20:47:01,103|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-21 21:26:47,644|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 21:27:07,643|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-21 21:27:47,658|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-21 21:28:07,668|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-11-25 06:29:16,664|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-27 19:12:13,956|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from zaValidator
2018-11-27 19:13:13,976|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from zaValidator
2018-11-29 00:15:51,589|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-29 00:15:59,942|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-11-29 00:15:59,944|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from OASFCU
2018-11-29 00:16:51,616|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-29 00:16:59,955|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from OASFCU
2018-11-29 00:16:59,974|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-11-29 00:17:51,612|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-11-29 00:17:59,969|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from OASFCU
2018-11-29 06:30:19,339|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from esatus_AG
2018-12-01 06:29:58,517|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-12-01 22:02:21,007|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from TNO
2018-12-02 07:23:06,645|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from digitalbazaar
2018-12-07 02:24:24,415|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-12-08 05:25:37,465|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from digitalbazaar
2018-12-08 05:25:37,976|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-12-08 05:25:38,208|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from NewtonD
2018-12-08 05:25:56,101|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ServerVS
2018-12-08 05:25:56,106|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from VeridiumIDC
2018-12-08 05:25:56,165|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from TNO
2018-12-08 05:25:56,183|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-12-08 05:25:56,202|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-12-08 05:25:56,220|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Aalto
2018-12-08 05:25:56,239|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from BIGAWSUSEAST1-001
2018-12-08 05:25:56,367|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ibm
2018-12-08 05:25:56,386|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from atbsovrin
2018-12-08 05:25:56,471|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-12-08 05:25:56,491|INFO|view_changer.py|findentity received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from prosovitor
2018-12-08 05:25:56,491|NOTIFICATION|view_changer.py|VIEW CHANGE: findentity initiating a view change to 3 from 2
{code};;;","12/Dec/18 1:52 AM;mgbailey;[~sergey.khoroshavin] More ev1 logs are [here|https://drive.google.com/file/d/1AiCbNrEY5ONsy73ppE3AU9bnN-5Msis4/view?usp=sharing];;;","12/Dec/18 2:08 AM;mgbailey;[~sergey.khoroshavin] attached are the logs for one of the nodes that did not come into consensus. This is for ibm, which is a non-standard node running supervisord (docker) instead of systemd. It is likely that the restart command malfunctioned on this node because of that. [^ibm-mainnet-validator-logs.tar.gz]

P.S. They have restarted their node, and it is in consensus now. ;;;","12/Dec/18 12:36 PM;peacekeeper;All this was caused by a network outage of less than 1 minute? If it helps, I can say that we certainly didn't restart or otherwise do anything special on the *danube* node around that time.;;;","12/Dec/18 8:12 PM;sergey.khoroshavin;[~peacekeeper]
Well, actually this was caused by different events that were happening throughout 3 weeks period (more precisely 2018-11-16 17:27 UTC till 2018-12-08 05:25 UTC), and last network outage was just a final trigger for the incident. Also from my point of view incident happened not because *danube* was somewhat misbehaving (on purpose or not), but because there is a serious shortcoming in current protocol implementation that was there from the very beginning, and it was just a big misfortune that it was uncovered only now and in such unpleasant circumstances. More information can be found in already posted [incident report|https://forum.sovrin.org/t/sovrin-main-net-outage-december-2018/1010], and I'm going to add some more details here soon.;;;","12/Dec/18 9:15 PM;sergey.khoroshavin;Analysis of logs from *TNO* (which was one of nodes that didn't enter view change) showed that it was indeed restarted during timeframe when *danube* was master primary, so it flushed previously accumulated INSTANCE_CHANGE messages, which resulted in  not achieving quorum during last network outage
{code}
2018-11-30 21:33:02,311|INFO|node.py|TNO reseting...
2018-11-30 21:33:13,913|INFO|node.py|TNO started participating
2018-12-01 06:30:15,711|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-12-02 07:23:20,155|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from digitalbazaar
2018-12-08 05:25:52,239|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from digitalbazaar
2018-12-08 05:25:52,796|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from amihan-sovrin
2018-12-08 05:25:53,008|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from NewtonD
2018-12-08 05:26:10,785|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ServerVS
2018-12-08 05:26:10,798|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from VeridiumIDC
2018-12-08 05:26:10,876|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Stuard
2018-12-08 05:26:10,903|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from icenode
2018-12-08 05:26:10,906|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from Aalto
2018-12-08 05:26:11,021|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from BIGAWSUSEAST1-001
2018-12-08 05:26:11,182|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ibm
2018-12-08 05:26:11,223|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from atbsovrin
2018-12-08 05:26:11,296|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from DustStorm
2018-12-08 05:26:11,323|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from prosovitor
2018-12-08 05:26:11,363|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from zaValidator
2018-12-08 05:26:11,516|INFO|view_changer.py|TNO received instance change request: INSTANCE_CHANGE{'reason': 26, 'viewNo': 3} from ev1
{code};;;","12/Dec/18 9:57 PM;sergey.khoroshavin;*Conclusion*
There is serious protocol shortcoming which allows situations where less than _n-f_ nodes enter view change, basically bringing pool into read-only state until manual emergency restart of all nodes. *Steps to reproduce* basically boils down to following:
# Given working pool
# Make randomly selected half of nodes broadcast INSTANCE_CHANGE (for example, by selectively limiting their connectivity to primary node)
# Restart another randomly selected set of half nodes (but not including current master primary)
# Make another half of nodes that were not selected in (2) to broadcast INSTANCE_CHANGE messages
# This will lead to nodes that were not restarted in (3) to enter a view change, with their number not enough to finish it
# At the same time number of remaining nodes that didn't enter a view change will be not enough to continue normal request processing
# So pool is non operational until manual restart of all nodes

 The problem is basically the same as found in INDY-1893.

There are several possible *options to fix this*:
* Discard old INSTANCE_CHANGE messages on some predefined timeout. It won't fix issue, just reduce probability of running into this problem, but it can be done really quickly. This will be addressed in INDY-1909.
* Introduce some unique _ID_ into INSTANCE_CHANGE messages and come up with a way to assign it in a way that will prevent this problem altogether. It's not yet known whether such protocol exists and if it is really simple to implement, but research (which includes writing tests for checking different hypothesis) is already ongoing and results will be provided in INDY-1897
* As per original PBFT paper (not an RBFT, on which Indy Plenum is based) nodes should start view change when they detect that some transactions are not ordered for long enough time. While it can be quickly implemented almost as is and it will definitely fix this issue it will make pool much more susceptible to DDoS. This will be addressed in INDY-1910.
* Following previous idea, if INDY-933 gets implemented nodes can vote for view change if state doesn't get updated long enough. I believe this option is actually the most robust solution in the long run, however it will take some time to implement it. Also it increases probability of view change, and given that Indy Plenum has problems (INDY-1296 and all related issues) with current implementation it introduces additional risks until these problems get fixed. This will be addressed in INDY-1911.
;;;",,,,,,,,,,,,,,,
Part of the nodes are lagged under production load before the View Change,INDY-1904,36027,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,,ozheregelya,ozheregelya,10/Dec/18 8:41 PM,10/Apr/19 6:09 PM,28/Oct/23 2:47 AM,10/Apr/19 6:09 PM,,,,,,0,,,,"*Environment:*
indy-node 1.6.726

*Steps to Reproduce:*
1. Setup the pool of 25 nodes.
2. Run load test with production load + 1 revoc_reg_entry / min.

*Actual Results:*
Most of the nodes have 795416 domain txns, but some nodes were lagged:
- node4 | 196731
- node14 | 701050
- node25 | 542967
- node1 | 791528
- node24 | 792286

Symptoms are the same as in INDY-1865, but in there issue started to appear only after View Change. In this case Node4 and Node25 were lagged before View Change.

*Logs and metrics:* s3://qanodelogs/indy-1904/lagged_nodes_10_12_18
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1904/lagged_nodes_10_12_18/ /home/ev/logs/indy-1904/lagged_nodes_10_12_18/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw9i",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,,,,,,,,,,,"10/Apr/19 6:09 PM;ashcherbakov;This issue is quite old and a lot of view change fixes have been done. Let's continue track the view change progress in the scope of the plan described in INDY-2035.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Unable to use numeric namespace name using pool automation scripts,INDY-1905,36059,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,VladimirWork,VladimirWork,11/Dec/18 4:23 PM,11/Dec/18 5:46 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"Steps to Reproduce:
1. Run `python ./scripts/namespace-config.py conf --localhosts.aws_regions us-east-1 us-west-2 --aws_clients_provisioner.aws_instance_count 1 --aws_clients_provisioner.aws_ec2_type t3.micro --aws_clients_provisioner.aws_ebs_volume_size 8 --aws_nodes_provisioner.aws_instance_count 4 --aws_nodes_provisioner.aws_ec2_type m5.large --aws_nodes_provisioner.aws_ebs_volume_size 24 --nodes.indy_node_channel master --clients.perf_scripts_libindy_ver 1.6.8~884 --clients.indy_cli_ver 1.6.8~884 --nodes.indy_node_ver 1.6.725 --nodes.indy_plenum_ver 1.6.628 --nodes.python_indy_crypto_ver 0.4.5 --nodes.libindy_crypto_ver 0.4.5`
2. Run `ansible-playbook -i conf/inventory provision.yml` several times.

Actual Results:
There are ansible errors about node ssh key during first run, about client ssh key second time and about `Unexpected Exception, this is probably a bug: expected string or buffer` the third and all the next times.

Expected Results:
There should be no errors or we should forbid numeric namespaces' names.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1899,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|i002jz:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to choose volume type using pool automation scripts,INDY-1906,36060,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,VladimirWork,VladimirWork,11/Dec/18 4:31 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"Actual Results:
Now we unable to choose volume type using pool automation scripts and all volumes have standart (magnetic) type but when we create instances manually all volumes have gp2 (ssd) type.

Expected Results:
It would be nice to configure volume type the same as its size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|hzwxa6:rz",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ozheregelya,VladimirWork,,,,,,,,,,"11/Dec/18 6:35 PM;andkononykhin;As it was discovered 'standard' type is default for part of regions, for other ones - 'gp2' is default. 

[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSVolumeTypes.html]

*PoA*:
 * add tests to check custom volume type for _stateful_set_ module
 * implement support for the volume type in the module
 * add relative option to _aws_manage_ role;;;","11/Dec/18 6:50 PM;andkononykhin;PR: https://github.com/hyperledger/indy-node/pull/1088;;;","11/Dec/18 8:55 PM;andkononykhin;*Problem reason*:

There was no way to customize type of  AWS EC2 volumes that are created for instances.

*Changes*:

added parameter aws_ebs_volume_type for aws_manage role with default value of _gp2_

*Committed into*:

https://github.com/hyperledger/indy-node/pull/1088

*Risk factors*:

Nothing is expected.

*Risk*:

Low

*Covered with tests*:

extended tests for stateful_set.py ansible module


*Recommendations for QA*:
 * create test inventory using namespace-config.py with options for ebs volumes (optionally you may try default settings), e.g.

{noformat}
--aws_clients_provisioner.aws_ebs_volume_size gp2 --aws_nodes_provisioner.aws_ebs_volume_size standard{noformat}

 * run provision.yml playbook
 * check that all volumes created for clients and nodes have expected types
 * destroy;;;","12/Dec/18 8:04 AM;ozheregelya;*Steps to Validate:*
1. Initialize inventory with custom 'aws_ebs_volume_size' and 'aws_ebs_volume_type'

*Actual Results:*
Custom volume sizes were set for both node and client.
Default volume type is 'gp2' (same as before), 'standard' can be set with aws_ebs_volume_type parameters.;;;",,,,,,,,,,,,,,,,,,,,,
Unable to install not the latest packages on clients using pool automation scripts,INDY-1907,36062,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,VladimirWork,VladimirWork,11/Dec/18 4:40 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Steps to Reproduce:
1. Configure client to install not the latest libindy and indy-cli versions:
`python ./scripts/namespace-config.py conf --localhosts.aws_regions us-east-1 us-west-2 --aws_clients_provisioner.aws_instance_count 1 --aws_clients_provisioner.aws_ec2_type t3.micro --aws_clients_provisioner.aws_ebs_volume_size 8 --aws_nodes_provisioner.aws_instance_count 4 --aws_nodes_provisioner.aws_ec2_type m5.large --aws_nodes_provisioner.aws_ebs_volume_size 24 --nodes.indy_node_channel master --clients.perf_scripts_libindy_ver 1.6.8~881 --clients.indy_cli_ver 1.6.8~881 --nodes.indy_node_ver 1.6.725 --nodes.indy_plenum_ver 1.6.628 --nodes.python_indy_crypto_ver 0.4.5 --nodes.libindy_crypto_ver 0.4.5`
2. Provision the client.
3. Try to configure the client.

Actual Results:
{noformat}
TASK [perf_scripts : Install pip and other required packages] **********************************************************************************************************************************************
fatal: [conf_clients1]: FAILED! => {""cache_update_time"": 1544452950, ""cache_updated"": true, ""changed"": false, ""msg"": ""'/usr/bin/apt-get -y -o \""Dpkg::Options::=--force-confdef\"" -o \""Dpkg::Options::=--force-confold\""     install 'libindy=1.6.8~881' 'python3-pip' 'python-setuptools' 'virtualenv'' failed: E: Packages were downgraded and -y was used without --allow-downgrades.\n"", ""rc"": 100, ""stderr"": ""E: Packages were downgraded and -y was used without --allow-downgrades.\n"", ""stderr_lines"": [""E: Packages were downgraded and -y was used without --allow-downgrades.""], ""stdout"": ""Reading package lists...\nBuilding dependency tree...\nReading state information...\nThe following additional packages will be installed:\n  binutils build-essential cpp cpp-5 dpkg-dev fakeroot g++ g++-5 gcc gcc-5\n  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n  libasan2 libatomic1 libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdpkg-perl\n  libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-5-dev libgomp1\n  libisl15 libitm1 liblsan0 libmpc3 libmpx0 libpython3-dev libpython3.5-dev\n  libquadmath0 libstdc++-5-dev libtsan0 libubsan0 linux-libc-dev make\n  manpages-dev python-pip-whl python-pkg-resources python3-dev\n  python3-setuptools python3-virtualenv python3-wheel python3.5-dev\nSuggested packages:\n  binutils-doc cpp-doc gcc-5-locales debian-keyring g++-multilib\n  g++-5-multilib gcc-5-doc libstdc++6-5-dbg gcc-multilib autoconf automake\n  libtool flex bison gdb gcc-doc gcc-5-multilib libgcc1-dbg libgomp1-dbg\n  libitm1-dbg libatomic1-dbg libasan2-dbg liblsan0-dbg libtsan0-dbg\n  libubsan0-dbg libcilkrts5-dbg libmpx0-dbg libquadmath0-dbg glibc-doc\n  libstdc++-5-doc make-doc python-setuptools-doc\nThe following NEW packages will be installed:\n  binutils build-essential cpp cpp-5 dpkg-dev fakeroot g++ g++-5 gcc gcc-5\n  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl\n  libasan2 libatomic1 libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdpkg-perl\n  libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-5-dev libgomp1\n  libisl15 libitm1 liblsan0 libmpc3 libmpx0 libpython3-dev libpython3.5-dev\n  libquadmath0 libstdc++-5-dev libtsan0 libubsan0 linux-libc-dev make\n  manpages-dev python-pip-whl python-pkg-resources python-setuptools\n  python3-dev python3-pip python3-setuptools python3-virtualenv python3-wheel\n  python3.5-dev virtualenv\nThe following packages will be DOWNGRADED:\n  libindy\n0 upgraded, 49 newly installed, 1 downgraded, 0 to remove and 26 not upgraded.\n"", ""stdout_lines"": [""Reading package lists..."", ""Building dependency tree..."", ""Reading state information..."", ""The following additional packages will be installed:"", ""  binutils build-essential cpp cpp-5 dpkg-dev fakeroot g++ g++-5 gcc gcc-5"", ""  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl"", ""  libasan2 libatomic1 libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdpkg-perl"", ""  libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-5-dev libgomp1"", ""  libisl15 libitm1 liblsan0 libmpc3 libmpx0 libpython3-dev libpython3.5-dev"", ""  libquadmath0 libstdc++-5-dev libtsan0 libubsan0 linux-libc-dev make"", ""  manpages-dev python-pip-whl python-pkg-resources python3-dev"", ""  python3-setuptools python3-virtualenv python3-wheel python3.5-dev"", ""Suggested packages:"", ""  binutils-doc cpp-doc gcc-5-locales debian-keyring g++-multilib"", ""  g++-5-multilib gcc-5-doc libstdc++6-5-dbg gcc-multilib autoconf automake"", ""  libtool flex bison gdb gcc-doc gcc-5-multilib libgcc1-dbg libgomp1-dbg"", ""  libitm1-dbg libatomic1-dbg libasan2-dbg liblsan0-dbg libtsan0-dbg"", ""  libubsan0-dbg libcilkrts5-dbg libmpx0-dbg libquadmath0-dbg glibc-doc"", ""  libstdc++-5-doc make-doc python-setuptools-doc"", ""The following NEW packages will be installed:"", ""  binutils build-essential cpp cpp-5 dpkg-dev fakeroot g++ g++-5 gcc gcc-5"", ""  libalgorithm-diff-perl libalgorithm-diff-xs-perl libalgorithm-merge-perl"", ""  libasan2 libatomic1 libc-dev-bin libc6-dev libcc1-0 libcilkrts5 libdpkg-perl"", ""  libexpat1-dev libfakeroot libfile-fcntllock-perl libgcc-5-dev libgomp1"", ""  libisl15 libitm1 liblsan0 libmpc3 libmpx0 libpython3-dev libpython3.5-dev"", ""  libquadmath0 libstdc++-5-dev libtsan0 libubsan0 linux-libc-dev make"", ""  manpages-dev python-pip-whl python-pkg-resources python-setuptools"", ""  python3-dev python3-pip python3-setuptools python3-virtualenv python3-wheel"", ""  python3.5-dev virtualenv"", ""The following packages will be DOWNGRADED:"", ""  libindy"", ""0 upgraded, 49 newly installed, 1 downgraded, 0 to remove and 26 not upgraded.""]}
	to retry, use: --limit @/home/indy/indy-node/pool_automation/configure.retry

PLAY RECAP *************************************************************************************************************************************************************************************************
conf_clients1              : ok=14   changed=0    unreachable=0    failed=1  
{noformat}

Expected Results:
We should be able to install not the latest versions of client packages the same as node packages.

",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1641,,,No,,Unset,No,,,"1|i002kn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,"11/Dec/18 5:10 PM;VladimirWork;Can be configured by `--clients.indy_cli_libindy_ver 1.6.8~881 --clients.indy_cli_ver 1.6.8~881`.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Some nodes unable to catch up others,INDY-1908,36093,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,,VladimirWork,VladimirWork,12/Dec/18 5:41 PM,01/Aug/19 9:58 PM,28/Oct/23 2:47 AM,17/May/19 5:04 PM,,,,,,0,,,,"Build Info:
indy-node 1.6.728

Steps to Reproduce:
1. Run reading and writing load test against 25 nodes pool.
2. Force VCs by primaries restarting each 5-10 minutes.
3. Reset View to 0 by the whole pool restarting.
4. Stop the load test.
5. Stop 1..6 nodes to start long VC process.
6. Send pool restart and pool upgrade (to the same version with reinstall) txns to ledger (successfully).
7. Run reading and writing load test again.
8. Start 1..6 nodes (they don't catch up under load).
9. Stop the load test.
10. Check 1..6 nodes for catch-up (1,4,6 nodes don't catch up).
11. Restart 1,4,6 nodes.

Actual Results:
2,3,5 nodes catch up one by one (not together) and make it slowly (about 1.5..2 hours to catch up form 25k to 40k for 3 nodes).
1,4,6 nodes don't catch up until restart and also it takes about the same time (1..1.5 hours) to catch up.

All logs and validator-info are in ev@evernymr33:logs/catchup_12_12_2018.tar.gz

Expected Results:
Catch-up should perform faster and all nodes should catch up successfully.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001ywbq",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"17/May/19 5:04 PM;ashcherbakov;Closing as out of date;;;",,,,,,,,,,,,,,,,,,,,,,,,
Discard old INSTANCE_CHANGE messages,INDY-1909,36097,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,sergey.khoroshavin,sergey.khoroshavin,12/Dec/18 11:14 PM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.82,,,,0,,,,"*Acceptance criteria*
* Tests should be written that confirm that old enough INSTANCE_CHANGE messages are discarded
* Accumulated INSTANCE_CHANGE messages should be discarded based on some configurable timeout
* Also it should be possible to turn of this functionality from config file

*Additional notes*
* This won't totally prevent incidents like INDY-1903, just reduce probability of such situation
* This can potentially introduce other failure modes
* Most probably this should be turned off when proper fix is delivered
* Main reason why this task exists is because it's one of the simplest and fastest ways to significantly reduce chance of incidents like INDY-1903",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1903,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwxa6:rzc",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,Toktar,zhigunenko.dsr,,,,,,,,,,"14/Dec/18 12:12 AM;Toktar;PR: https://github.com/hyperledger/indy-plenum/pull/1028;;;","18/Dec/18 1:19 AM;Toktar;Problem reason:
 - A node accumulates old InstanceChange messages and use it for quorum for view change. 

Changes:
 - Discard old InstanceChange messages
 - Add tests

PR:
 * [https://github.com/hyperledger/indy-node/pull/1096]
 * [https://github.com/hyperledger/indy-plenum/pull/1028]

Version:
 * indy-node 1.6.736 -master
 * indy-plenum 1.6.638 -master

Risk factors:
 - Problem with view change

Risk:
 - Low

Covered with tests:
 * [test_models.py|https://github.com/hyperledger/indy-plenum/pull/1028/files#diff-b7328ffeb9e512539a4abe4c1f10eb03]
 * [test_old_instance_change_discarding.py|https://github.com/hyperledger/indy-plenum/pull/1028/files#diff-37714c8f48307db3ea816980a2522bc5]

Recommendations for QA:
 * Start pool for 4 nodes on Docker
 * Selectively limiting the Node4 connectivity to primary node
 * Restart Node2 and Node3
 * Wait config.OUTDATED_INSTANCE_CHANGES_CHECK_INTERVAL  (2 hours by default)
 * Selectively limiting Node2 and Node3 connectivity to primary node
 * Write any txn and check that the reply is successfully received.;;;","18/Dec/18 9:13 PM;zhigunenko.dsr;*Environment:*
indy-node                  1.6.736
indy-plenum                1.6.638

*Steps to Validate:*
1) Start pool for 4 nodes on Docker, set node1 as primary
2) Block incoming packets from node4 to node1 (primary)
3) Unblock incoming packets from node4 to node1 (primary) after ""INSTANCE_CHANGE{'viewNo': 1, 'reason': 26} but did not find the master to be slow""
4) Restart services on Node2 and Node3
5) Wait config.OUTDATED_INSTANCE_CHANGES_CHECK_INTERVAL==2min
6) Block incoming packets from node3 and node2 to node1 (primary)
7) Write nym
8) Wait another config.OUTDATED_INSTANCE_CHANGES_CHECK_INTERVAL==2min
9) Unblock incoming packets from node3 and node2 to node1 (primary)
10) Write another nym again

*Actual Results:*
There is no view change. Pool orders new transactions
;;;",,,,,,,,,,,,,,,,,,,,,,
Send INSTANCE_CHANGE if some request doesn't get ordered by master Primary for long enough,INDY-1910,36098,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,12/Dec/18 11:17 PM,08/Jan/20 5:56 PM,28/Oct/23 2:47 AM,,,,,,,0,,,,"*Acceptance criteria*
* Tests should be implemented which show that if some request doesn't get ordered for long enough for some reason then view change starts and successfully completes
* Broadcasting INSTANCE_CHANGE when some request is stuck for long enough should be implemented
* Threshold timeout should be configurable, and it should be possible to turn off this functionality
* Extensive testing should be performed to make sure that this won't provoke perpetual view change under high enough load

*Additional notes*
* This will make sure that if more than _f_ and less than _n-f_ nodes enter view change then if (and as soon as) there are any incoming write requests then remaining nodes that are not yet in view change will also enter view change allowing it to finish
* In other words it should reliably fix problem occured in INDY-1903
* Also this is basically what original PBFT (not RBFT) paper requires
* This will also provide pretty good resistance to censorship attacks
* However there is a high risk that this change can provoke perpetual view change under high enough load, so extensive testing is required, as well as probably some more research. Implementation of INDY-1863 _might_ also help here
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1903,INDY-1911,,,,INDY-1863,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ibx",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Send INSTANCE_CHANGE if state doesn't get updated long enough,INDY-1911,36099,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,sergey.khoroshavin,sergey.khoroshavin,12/Dec/18 11:18 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.7.1,,,,0,,,,"*Acceptance criteria*
* Tests should be implemented which show that if some request doesn't get ordered for long enough for some reason then view change starts and successfully completes (probably can be shared with INDY-1911)
* Tests should be implemented which show that if more than _f_ and less than _n-f_ nodes entered view change then remaining honest will also enter and successfully complete a view change, regardless of presence or absence of incoming requests
* Implement broadcasting of INSTANCE_CHANGE message when state doesn't get updated long enough
* Threshold timeout should be configurable, and it should be possible to turn off this functionality

*Additional notes*
* This will make sure that if more than _f_ and less than _n-f_ nodes enter view change then if (and as soon as) there are any incoming write requests then remaining nodes that are not yet in view change will also enter view change allowing it to finish
* In other words it should reliably fix problem occured in INDY-1903
* Unlike INDY-1910 this doesn't increase risk of perpetual view change under high load
* There should be functionality to periodically update state even if there are no incoming requests
* In other words this is blocked by INDY-933
",,,,,,,,,,INDY-933,,,,,,,,,,,,,,,,,,,INDY-1903,,,,,INDY-1910,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwvif:0000024",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,sergey.khoroshavin,,,,,,,,,,,"01/Feb/19 10:46 PM;sergey.khoroshavin;*Analysis*
* >f and <n-f nodes start a view change
** View change will not finish unless more nodes also enter view change
** But since ordering will be stalled state signatures won't be updated so other nodes will send more INSTANCE_CHANGE messages, which should help them reach quorum and also join view change
** There was a concern that by the time other nodes find out that they should also enter view change nodes that already entered view change will time out and start next view change, making this fix futile. However this is not the case, since in order to switch to next view change they need to reach quorum of n-f INSTANCE_CHANGE messages for next view, which is not possible since there are less than n-f nodes stuck in a view change in the first place
* <=f nodes start a view change
** View change will not finish unless more nodes also enter view change
** Since ordering won't be stalled state signatures will be updated as usual so other nodes won't have incentive to join view change
** However this is not a problem since ordering will continue as usual
** And if something stops ordering freshness check will kick in, forcing more nodes to join and finally finish view change
** However since there was ordering in progress and a lot of time might have passed between start of view change by different nodes its possible that small number of nodes that started view change earlier might end up with different ledger states

*Plan of Attack*
* Add one more reason code for INSTANCE_CHANGE (signatures_are_not_fresh)
* Implement periodic check of state signatures timestamps, send INSTANCE_CHANGE if signatures are not fresh enough
* Remove skip mark from test_view_change_with_different_ic, check that it passes
* Implement test to check behavior of case when <=f nodes start a view change much earlier

;;;","02/Feb/19 1:02 AM;sergey.khoroshavin;*Failing case found*
* Network outage happens between primary and nodes 2 and 3
** Nodes 2 and 3 broadcast INSTANCE_CHANGE messages, which get accumulated on nodes 2, 3 and 4
* Node 4 gets restarted, losing accumulated INSTANCE_CHANGE messages
* Network outage happens between primary and node 4
** Node 4 broadcasts INSTANCE_CHANGE
** Node 2 and 3 now have quorum of INSTANCE_CHANGE messages, which start a view change
** Node 4 has only INSTANCE_CHANGE from self, which is insufficient to start a view change
* Time passes and freshness check is triggered on Node1 and Node4
** Nodes 1 and 4 broadcast INSTANCE_CHANGE, which get accumulated on nodes 1 and 4
** However this is still insufficient to start a view change on Node4 and Node1
* Consensus is lost

Possible solutions are:
* when performing freshness check send INSTANCE_CHANGE even if view change to this view is already in progress
* don't do anything with freshness, just periodically resend INSTANCE_CHANGE while view change is in progress
;;;","09/Feb/19 1:30 AM;sergey.khoroshavin;*Changes*
* Added new reason for INSTANCE_CHANGE: state signatures are not fresh enough, code 43
* Implemented sending INSTANCE_CHANGE messages when node discover that state of some of its ledgers doesn't have fresh enough signatures for too long
* Implemented contiguous sending of INSTANCE_CHANGE (with same target view_no) until all ledgers have fresh enough signature

*PR*
https://github.com/hyperledger/indy-plenum/pull/1078

*Versions*
indy-plenum 1.6.675
indy-node 1.6.799

*Covered with tests*
* [unit tests + test_view_change_doesnt_happen_if_pool_is_left_alone|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-286c6136421042b6056063c91c03f22e]
* [test_freshness_instance_changes_are_sent_continuosly|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-c5870aa7a382d90b9bffe7b56baf4928]
* [test_view_change_happens_if_ordering_is_halted|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-9f6c8921187de21f7703e7dbf60b4c8e]
* [test_view_change_happens_if_primary_is_slow_to_update_freshness|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-e2fcfc49802c877d17fd4a19fbe8179a]
* [test_view_change_not_happen_if_ic_is_discarded|https://github.com/hyperledger/indy-plenum/pull/1078/files#diff-4151554ee1cac10387e880c08964071a]

*Risk*
Medium

*Risk factors*
Additional reason for view change can lead to more view changes, watch out for false or even perpetual view changes

*Recommendations for QA*
Run load test on 25-nodes pool with moderate load - pool should work.
Try enabling regular view change (every 30 minutes) - pool should still work under load.
Stop load script
Try slowing down primary - pool should do a view change even without load. 
Try reproducing INDY-1903 - pool should recover after entering a view change on only some of nodes, even without load.;;;","12/Feb/19 8:19 PM;sergey.khoroshavin;*Additional changes*
* Respect ACCEPTABLE_FRESHNESS_INTERVALS_COUNT when detecting whether to send INSTANCE_CHANGE
* This greatly reduces chance of false view change since previous coefficient was 1.2 and default value of ACCEPTABLE_FRESHNESS_INTERVALS_COUNT is 2

*PR*
https://github.com/hyperledger/indy-plenum/pull/1085;;;","15/Feb/19 1:33 AM;ozheregelya;Environment:
indy-node 1.6.803

Case 1:
Steps to Validate:
1. Setup the pool with default config.
2. Run production load.
Actual Results:
550K txns written, pool works, all nodes are in sync. 

Case 2:
Steps to Validate:
1. Setup the pool with forced View Changes each 0.5h.
2. Run production load.
Actual Results:
400K txns written, 42 view changes happened, pool works, all nodes are in sync. 

Case 3:
{quote}
Try slowing down primary - pool should do a view change even without load. 
{quote}
Can't be reproduced on real environment.

Case 4:
Steps to Validate:
1. Setup the pool.
2. Block primary on half of nodes using iptables (iptables -A INPUT -s 54.176.193.152 -j DROP).
3. When the rest nodes will get INSTANCE_CHANGEs with 26 reason, unblock primary.
4. Restart the rest half of nodes and block primary on them.
Actual Results:
View Change was completed because of 43 reason.;;;",,,,,,,,,,,,,,,,,,,,
Hotfix 1.6.80,INDY-1912,36122,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,13/Dec/18 3:34 PM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.80,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa6:rzo",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.80,INDY-1913,36133,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,ozheregelya,ozheregelya,13/Dec/18 9:04 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,,,,14/Dec/18 12:00 AM,0,,,,"*Version Information*
 indy-node 1.6.80
 indy-plenum 1.6.55
 sovrin 1.1.33

*Notices for Stewards*
 *(!) There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*
 *(!)* *Pool upgrade to sovrin 1.1.32 and above should be performed simultaneously for all nodes due to txn format changes.*

*Major Fixes*
 INDY-1896 - Issuing a pool restart does not work if there is no consensus
 INDY-1872 - Intermittent test failure: test_primary_selection_increase_f
 INDY-1888 - Fix throughput class creation bug",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i002zj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,TechWritingWhiz,,,,,,,,,,,"15/Dec/18 1:32 AM;TechWritingWhiz;The pull request is here: https://github.com/sovrin-foundation/sovrin/pull/135;;;",,,,,,,,,,,,,,,,,,,,,,,,
Limit the number of attributes in schema,INDY-1914,36160,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,KitHat,KitHat,14/Dec/18 9:13 PM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,,,,"This story is an outcome of https://jira.hyperledger.org/browse/IS-1102
We should limit our number of attributes in schema to 125 to make cred def pass the request limit check.

*Acceptance Criteria*
* Clearly document the limit
* Implement a check for the limit
* Test that a transaction with 125 attributes with names of the maximum length will succeed
* Test that a transaction with 126 attributes will fail

Excluded:
* The performance of a transaction with 125 attributes does not need to be tested with this story",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-1102,IS-1117,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0003",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,KitHat,ozheregelya,,,,,,,,,,"10/Jan/19 1:40 AM;Derashe;PR: https://github.com/hyperledger/indy-node/pull/1119;;;","10/Jan/19 7:58 PM;Derashe;Recomendation for QA:

Test edge cases like:
 * schema txn with a 125 attributes with names of the maximum length will succeed
 * schema txn with a 126 attributes will fail;;;","12/Jan/19 1:38 AM;ozheregelya;*Environment:*
indy-node 1.6.752
libindy 1.7.0~916
libindy 1.7.0~924

*Steps to Validate:*
1. Using old libindy send schema with 126 attributes.
=> Node replied with ""InvalidClientRequest(""validation error [SchemaField]: length should be at most 125 (attr_names=[...])"",""
2. Using new libindy and indy-cli send schema with 125 attributes with max length of attributes names.
=> Schema successfully written.
3. Using new libindy and indy-cli send schema with 126 attributes.
=> ""CommonInvalidStructure"" is returned by libindy.
=> ""Invalid format of command params. Please check format of posted JSONs, Keys, DIDs and etc..."" appear in indy-cli.

*Actual Results:*
Amount of attributes is limited by 125.
Restriction is documented on both of node and sdk sides. It's mentioned in indy-cli help as well.;;;",,,,,,,,,,,,,,,,,,,,,,
POA: track same transactions with different multi-signatures,INDY-1915,36211,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,esplinr,17/Dec/18 11:57 PM,17/May/19 4:54 PM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.7.1,,,,0,,,,"As of now transaction's digest is used to uniquely identify the transaction.

Signature (as well as multi-signature) is not part of the digest.
This is not a problem for a common (single) signature, but nay cause issues in case of multi-signature:
 * There is a transaction signed by Trustees 1,2,3, and by Trustees 3,4,5.
 * Both Trustee 1 and Trustee 4 send this txn
 * Since multi-signature is not part of the digest, the pool will not be able to distinguish if this is txn signed by 1,2,3, or by 3,4,5.
 * Some nodes in the pool may select for ordering a txn signed by 1,2,3, and some signed by 3,4,5. Since multi-sig is part of the txn in the Ledger, it will lead to different txn root hashes and different ledgers, so the pool may lost consensus.


*Acceptance Criteria*
* Design a solution to the above problem.
* Get feedback from the architects.
* Create relevant issues in this project.",,,,,,,,,,,,,,,,,,,,,,,INDY-1757,,,,,,INDY-1674,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-2093,,,No,,Unset,No,,,"1|hzwvif:000007z",,,,Unset,Unset,Ev-Node 19.06,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,sergey.khoroshavin,,,,,,,,,,"28/Mar/19 9:50 PM;sergey.khoroshavin;*Problem analysis*
* Transactions in ledger include signatures, therefore same transactions with different signatures should really be treated as different transactions during ordering. This can be achieved by including signatures into transaction digest, so they are easily distinguishable during ordering
* Once transaction is written to ledger it should not be replayable - for example, it should be impossible to add another valid signature to existing set of signatures and send it as another one. So it should be possible to easily detect if transaction with same payload but different signatures was already ordered. This can be achieved by having digest of just transaction payload. Actually this is digest we already have.

*Plan of Attack*
* Rename current digest into _payload_digest_
* Introduce another transaction digest, called _digest_, which includes both transaction payload and signatures
* Use new _digest_ as is without modifying majority of old code (for example this includes PREPREPARE messages and seqNoDB)
* Introduce another index _payload_digest_ -> _digest_ and modify logic of handling existing transactions (look for getReplyFromLedgerForRequest in node.py) as follows:
** if there are no transactions written with same _payload_digest_ - put transaction into ordering as usual
** if there was a transaction with same _payload_digest_ and _digest_ - return REPLY as if transaction was just ordered
** if there was a transaction with same _payload_digest_ but different _digest_ - return REQNACK stating that same transaction was already ordered with different signatures

Another option is to avoid adding new index by modifying seqNoDB to use _payload_digest_ as key and tuple (_digest_, _seq_no_) as value, however this will require more changes in existing code and can be actually more error prone both to implement and maintain. Also this option can turn out to be more costly performance wise.

Also it might be desirable to add digest checks into dynamic validation as well to get rid of situations when two requests with same payload but different signatures end up in one batch.;;;","29/Mar/19 3:48 PM;ashcherbakov;UPD:
the signatures needs to be deterministically sorted when calculating digest.;;;",,,,,,,,,,,,,,,,,,,,,,,
Network maintenance role,INDY-1916,36227,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,esplinr,esplinr,18/Dec/18 4:42 AM,13/Jul/19 6:50 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.6.83,validator-info,,,0,,,,"*Story*
As a systems administrator responsible for solutions built on top of an Indy network, I want to be able to monitor the health of the network to ensure that my solutions will be reliable.

As a steward, I want to be able to setup network maintenance without having to use my steward keys in a dangerous manner.

Notes:
* This user is not a steward on the network.
* It is reasonable to require that this user has a trusted position on the network.
* This user is very technical and motivated to build a monitoring system.
* The monitoring system requires:
** The information returned by validator-info across the various stewards, but today this information is only available to users with a steward role.
** Other information that can be provided by querying the ledger. This does not require any changes.

*Acceptance Criteria*
* A non-steward role exists by which a trusted administrator can access validator-info across all stewards in a consensus pool.
* This role is called ""Network Monitor"".
* The only permissions this role has beyond an unprivileged user is to access validator-info.
* A user can be assigned this role by a steward (no multi-signature necessary).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,IS-1123,,,,,INDY-1281,INDY-1928,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00026",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,esplinr,VladimirWork,,,,,,,,,,"09/Jan/19 7:29 PM;anikitinDSR;As i understand from description, in the scope of this ticket we whould add new role, which named ""NETWORK_MONITOR"", with the next options:
 * User with this role can be added only by steward, that means, that we should add rule for _*adding*_ and _*deleting*_ new NYM with role ""NETWORK_MONITOR""
 * User with this role can only perform a GET_VALIDATOR_INFO requests;;;","09/Jan/19 8:33 PM;anikitinDSR;[~esplinr], should we add a possibility to adding and deleting users with role NETWORK_MONITOR for TRUSTEE too? In other words, STEWARDs and TRUSTEEs can add and delete NYM with role ""NETWORK_MONITOR"".

 ;;;","10/Jan/19 12:07 AM;esplinr;Thank you [~anikitinDSR] for finding this mistake in the requirements. You are correct that TRUSTEES should also be able to create a NETWORK_MONITOR role. Both STEWARDs and TRUSTEEs should be able to create the role without requiring a second signature.

Your summary of the implementation plan matches my understanding. A STEWARD or TRUSTEE should be able to create a NYM with the role ""NETWORK_MONITOR"" to grant the ability, and should also be able to remove it.;;;","10/Jan/19 12:21 AM;anikitinDSR;Also, supposed constant for this role is:

NETWORK_MONITOR = ""201""

PR:
[https://github.com/hyperledger/indy-node/pull/1117];;;","10/Jan/19 6:59 PM;anikitinDSR;Reasons:
 * Need to add new role ""NETWORK_MONITOR"", which can perform get-validator-info action

Changes:
 * add the rule for *adding* and *deleting*  NYM with role ""NETWORK_MONITOR"" (only TRUSTEE and STEWARD can create this)
 * modify existing rule for get-validator-info action for allowing users with role NETWORK_MONITOR perform it

Version:
 * indy-node 1.6.750

PR:
 * in indy-node  [https://github.com/hyperledger/indy-node/pull/1117]

For QA:
 * need to check, that only TRUSTEE and STEWARD can create NYM with role ""NETWORK_MONITOR"". TRUST_ANCHOR for example can't do this.
 * create user with role NETWORK_MONITOR and do get-validator-info action. Check that user with this role can do this and get the same info as STEWARD
 * check, that only TRUSTEE and STEWARD can delete user with NETWORK_MONITOR role (it means, that change ""role"" field from ""NETWORK_MONITOR"" to """")
 * check, that user with role ""NETWORK_MONITOR"" can't do other things like create other users with role ""NETWORK_MONITOR"", do restart or config actions.

 

 

 ;;;","11/Jan/19 6:10 PM;VladimirWork;System test is ready but testing is blocked by IS-1123.;;;","12/Jan/19 6:36 AM;esplinr;A concern was raised that adding this role centralizes maintenance of the network.

This role does not give privileged access to maintain the network. The name NETWORK_MONITOR is meant to communicate that it gives additional people the ability to monitor the network, whereas today only a STEWARD key can access validator-info.

When we better understand the implications of this monitoring on network performance and security, we might decide that this information should be available to anyone without a special role.;;;","14/Jan/19 6:46 PM;anikitinDSR;[~esplinr] as of now, do we stay on current position, that only TRUSTEE and STEWARD can create NETWORK_MONITOR role and only get-validator-info cmd can be allowed for new role? And also we take into account that permissions for user with role NETWORK_MONITOR can be changed in future or this role can be not used.;;;","16/Jan/19 1:22 AM;VladimirWork;Build Info:
indy-node 1.6.757

Steps to Validate:
1. Need to check, that only TRUSTEE and STEWARD can create NYM with role ""NETWORK_MONITOR"". TRUST_ANCHOR for example can't do this.
2. Create user with role NETWORK_MONITOR and do get-validator-info action. Check that user with this role can do this and get the same info as STEWARD.
3. Check, that only TRUSTEE and STEWARD can delete user with NETWORK_MONITOR role (it means, that change ""role"" field from ""NETWORK_MONITOR"" to """").
4. Check, that user with role ""NETWORK_MONITOR"" can't do other things like create other users with role ""NETWORK_MONITOR"", do restart or config actions.

Actual Results:
New role works as expected (temporary test due to absent SDK support here: https://github.com/VladimirWork/tests/blob/f3196e77aef6c4e155098ad43e91971f317de09d/test_misc.py#L230-L332)
New role is still not supported by libindy and indy-cli so this functionality will be retested in scope of IS-1123 by https://github.com/VladimirWork/tests/blob/f3196e77aef6c4e155098ad43e91971f317de09d/test_misc.py#L154-L226.;;;","18/Jan/19 9:45 AM;esplinr;Some clarifications:
* The NETWORK_MONITOR role should be created by any TRUSTEE or STEWARD, and can also be revoked by any TRUSTEE or STEWARD (not just the one that created it).
* The role only allows running the get-validator-info command
* The network can function without this role, so no deployment of an Indy network is required to use it.
* We do not expect the scope of the NETWORK_MONITOR role to expand, but it could happen in future tickets.

My understanding of Vladimir's comment is that the current implementation reflects these expectations. If they do not, we should raise a new ticket (this one can continue to be considered ""done"").;;;",,,,,,,,,,,,,,,
Nodes have different amount of replicas after nodes adding/promotion,INDY-1917,36229,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ozheregelya,ozheregelya,18/Dec/18 5:05 AM,30/Mar/19 5:35 AM,28/Oct/23 2:47 AM,30/Mar/19 5:35 AM,,1.7.1,,,,0,TShirt_M,,,"*Case 1:* promotion of stopped node
 *Steps to Reproduce:*
 1. Setup pool of 4 nodes.
 2. Initialize 3 more nodes.
 3. Start 5 and 6 nodes.
 4. Add 5 - 7 nodes (node7 is stopped).
 5. Wait for some time.

*Actual Results:*
 3rd replica was removed on all nodes exclude Node1 and Node3. After starting Node7 and writing several txns nothing was changed.

*Expected Result:*
 Replicas should not be removed without any reason.

 

*Case 2:* node demotion and promotion back
 Steps to Reproduce:
 1. Initiate View Change to make replicas count consistent on all nodes after Case 1.
 2. Demote Node7.
 => Replica was removed on all nodes (include Node7).
 3. Promote Node7 back.

*Actual Results:*
 After promotion replicas were added on all nodes exclude Node7. After writing several txns nothing was changed.

*Expected Results:*
 Replicas count should be consistent on all nodes.


*Logs:* s3://qanodelogs/indy-1917
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1917/ /home/ev/logs/indy-1917/",indy-node 1.6.80,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1720,,,,,,,,,,,,,,,,,,,,"12/Feb/19 12:26 AM;VladimirWork;INDY-1917_case1.PNG;https://jira.hyperledger.org/secure/attachment/16762/INDY-1917_case1.PNG","12/Feb/19 12:26 AM;VladimirWork;INDY-1917_case2.PNG;https://jira.hyperledger.org/secure/attachment/16763/INDY-1917_case2.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000002b",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,VladimirWork,,,,,,,,,,"06/Feb/19 1:57 AM;Derashe;In case 1, we send txn with 7-th node and that caused new replica to appear. But because load on pool was not high, this replica did not get high throughtput values. And when throughtput comparision activated, it just deleted new replica. Deletion was unequeal because of little bug in code.

In case 2, we needed to restart node after demotion. That would probably let us aviod anomaly.

PR: [https://github.com/hyperledger/indy-plenum/pull/1076];;;","07/Feb/19 11:52 PM;Derashe;h4. [Indy Node 1.6.799 |https://github.com/hyperledger/indy-node/releases/tag/1.6.799-master] contains this fix;;;","12/Feb/19 12:26 AM;VladimirWork;Build Info:
indy-node 1.6.800

Steps to Valdiate:
1. Setup pool of 4 nodes.
2. Initialize 3 more nodes.
3. Start 5 and 6 nodes.
4. Add 5 - 7 nodes (node7 is stopped).
5. Wait for some time and check replicas at all nodes.
6. Start Node7.
7. Wait for some time and check replicas at all nodes.
8. Demote Node7.
9. Promote Node7 back *and restart service*.
10. Wait for some time and check replicas at all nodes.

Actual Results:
All nodes have the same primaries for each instance during test. !INDY-1917_case1.PNG|thumbnail!  !INDY-1917_case2.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,,
Node that does not upgrade spams the config ledger,INDY-1918,36232,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,mgbailey,mgbailey,18/Dec/18 7:21 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,TShirt_L,,,"On the Sovrin TestNet, a trustee posted an upgrade transaction for 18:05 on Friday, Dec. 14. Most nodes upgraded properly, but one of them (named sovrin.sicpa.com) did not and instead got into a state where it continually wrote ""in-progress"" transactions to the config ledger, at a rate of about 1 every 3 seconds. This continued until a trustee wrote a transaction to the ledger restarting the indy-node service on the problem node.

Looking at the logs from the node (attached), it appears that perhaps the indy-node-control service was not running. The steward says that there were no entries in journalctl from indy-node-control, which might support this. We will debug this further with the steward, and findings will be added here as we proceed.

Meanwhile, the problem where the condition results in spamming of the ledger should be remedied, regardless of the cause of the failure to upgrade.

Attaching the config ledger from another node as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1922,INDY-1958,,,,,,,,,,,,,,"15/Jan/19 10:05 PM;VladimirWork;INDY-1918.PNG;https://jira.hyperledger.org/secure/attachment/16548/INDY-1918.PNG","18/Dec/18 7:30 AM;mgbailey;config_ledger.txt;https://jira.hyperledger.org/secure/attachment/16443/config_ledger.txt","18/Dec/18 7:28 AM;mgbailey;korea.log.93.xz;https://jira.hyperledger.org/secure/attachment/16442/korea.log.93.xz","18/Dec/18 4:56 AM;mgbailey;sovrin-2018-12-14.log.gz;https://jira.hyperledger.org/secure/attachment/16441/sovrin-2018-12-14.log.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-738,,,No,,Unset,No,,,"1|hzwvif:00009",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,mgbailey,VladimirWork,,,,,,,,,,"18/Dec/18 7:29 AM;mgbailey;Logs from working node ""korea"" also attached.;;;","19/Dec/18 3:32 AM;mgbailey;Ok, it looks like we found the root cause. Their iptables was configured in a way that was preventing loopback, and apparently the indy-node-control service uses loopback on port 30003 to communicate with indy-node.

So we upgraded the node manually and it is back online.  The steward is redoing his iptable settings. *We still need to fix the continuous repeated retries to upgrade*, though. Another consequence of this is that now the client command to get-validator-info is not working for this node because the response message is to large  to send (the message contains upgrade attempt information). A separate ticket will be filed on this.;;;","29/Dec/18 10:42 PM;Derashe;Result of inverstigation:

Function that handled failed callback of node_controll_tool is postConfigLedgerCatchup. This function scans ledger for last POOL_UPGRADE transaction and tries to apply it again. And so it resulted in recursive call and that was the reason of the problem.

Simplest fix that can be applied here is to prevent such a behaviour in failed_callback function.;;;","29/Dec/18 10:51 PM;Derashe;PR: https://github.com/hyperledger/indy-node/pull/1116;;;","14/Jan/19 4:15 PM;Derashe;Rec for QA:
 * Run 4-nodes pool.
 * Choose one node and set it's iptables setting in such a way, that it prevent communication with node_control_tool (localhost:30003 by default).
 * After that send upgrade txn to the pool with pretty soon upgrade time (like 30 sec). (if you want to check this faster , you can set MinSepBetweenNodeUpgrades setting to low value, like 10 sec.)
 * After planned upgrade finish, check that every node, except chosen, upgraded to a newer version.
 * Get validator-info from all nodes. Check, that chosen node do not have huge upgrade log (it must have less records that other nodes). (provide validator info here if possible);;;","15/Jan/19 10:05 PM;VladimirWork;Build Info:
indy-node 1.6.754

Steps to Validate:
1. Run 4-nodes pool.
2. Choose one node and set it's iptables setting in such a way, that it prevent communication with node_control_tool (localhost:30003 by default).
3. After that send upgrade txn to the pool with pretty soon upgrade time (like 30 sec). (if you want to check this faster , you can set MinSepBetweenNodeUpgrades setting to low value, like 10 sec.)
4. After planned upgrade finish, check that every node, except chosen, upgraded to a newer version (1.6.755).
5. Check upgrade logs and config ledger at all nodes.

Actual Results:
All nodes has 8 entries in config ledger. There are 2 entries in upgrade log at the blocked node and 3 entries at the others right after upgrade. After sometime there are also 3 entries (entry about failed upgrade is added) at the blocked node. !INDY-1918.PNG|thumbnail! 
;;;",,,,,,,,,,,,,,,,,,,
Node on Sovrin TestNet did not upgrade automatically,INDY-1919,36233,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,mgbailey,mgbailey,18/Dec/18 7:53 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,TShirt_M,,,"A node named NodeTwinPeek did not upgrade properly. In its journalctl logs, the process appears to proceed normally until this line:
{code:java}
Dec 14 19:05:19 sovrin-validator.twinpeek.net env[18880]: Copying last_version failed due to [Errno 2] No such file or directory: '/var/lib/indy/sandbox/last_version'{code}
The steward manually ran ""sudo apt upgrade"", and the service appears to have come up and is operating normally.

Please determine why the upgrade failed so that we can prevent this issue in the future.

Logs from other nodes during this upgrade can be found on INDY-1918.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/18 7:52 AM;mgbailey;NodeTwinPeek_log (1).tgz;https://jira.hyperledger.org/secure/attachment/16445/NodeTwinPeek_log+%281%29.tgz","18/Dec/18 7:52 AM;mgbailey;indy-node-control.service.output;https://jira.hyperledger.org/secure/attachment/16444/indy-node-control.service.output",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-738,,,No,,Unset,No,,,"1|hzwvif:0000i",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,mgbailey,VladimirWork,,,,,,,,,,"19/Dec/18 4:07 AM;mgbailey;The steward has verified that the proper permissions are set on /var/lib/indy/sandbox, so write access is not preventing the file from being created:

ls -l -d /var/lib/indy/sandbox
drwxrwxr-x 4 indy indy 4096 Dec 14 20:04 /var/lib/indy/sandbox;;;","28/Dec/18 6:15 PM;Derashe;[~mgbailey] We've discovered strange error appeared:
 * Upgrade from 1.1.30 to 1.1.33 failed: Platform is not supported

That means that python script didn't recognize current platform as Ubuntu.

Can you, please, clarify with steward of this node, if there some unusual startup environment (docker, Debian probably)?;;;","11/Jan/19 2:22 AM;mgbailey;From the steward:
{code:java}
Our node is running on a dedicated server (bare-metal).  
$ lsb_release -a 
  No LSB modules are available. 
  Distributor ID: Ubuntu 
  Description: Ubuntu 16.04.5 LTS 
  Release: 16.04 
  Codename: xenial
{code}
So the OS looks correct. [~Derashe] Is it possible that the error is because it is running on a dedicated server instead of in a VM? This is our first steward that is doing this.;;;","11/Jan/19 2:38 AM;Derashe;[~mgbailey]  Can this steward run python lines of code below on this dedicated server and provide us the output?
h4. {{_import platform_}}
h4. {{_platform.uname()_}};;;","11/Jan/19 11:56 PM;mgbailey;[~Derashe] Result from steward:
{code:java}
uname_result(system='Linux', node='sovrin-validator.twinpeek.net', release='4.9.132-xxxx-std-ipv6-64', version='#406590 SMP Wed Oct 10 08:31:23 UTC 2018', machine='x86_64', processor='x86_64')
{code};;;","12/Jan/19 1:14 AM;Derashe;Ok, the problem is that python's platform.uname() can't identify this platform as Ubuntu. We've replaced it with a modern _distro_ util. We will push it to the master. It would be also very nice, if this steward install _distro_ and provide us the output of code below, so we can be sure, that this will work.

In console:
h4.  _pip install distro_

Python scipt:
h4. _distro.linux_distribution()_

PR: https://github.com/hyperledger/indy-node/pull/1121;;;","14/Jan/19 8:43 PM;Derashe;Rec for QA: 
 * Set up pool
 * Send and perform upgrade txn;;;","15/Jan/19 12:14 AM;mgbailey;[~Derashe] here is the result on NodeTwinPeek:

>>> distro.linux_distribution()

('Ubuntu', '16.04', 'Xenial Xerus')

 ;;;","15/Jan/19 12:21 AM;Derashe;[~mgbailey] Perfect! That means our fix will work for this steward.;;;","15/Jan/19 8:00 PM;VladimirWork;Build Info:
1.6.755

Steps to Reproduce:
1. Run upgrade from 1.6.755 to 1.6.756 against 4 nodes docker pool.
2. Check upgrade results and journalctl.

Actual Results:
It looks like distro package broke upgrading:
{noformat}
Jan 15 10:20:16 e83cd99f5e18 env[63]:  indy-node : Depends: distro (= 1.3.0) but it is not installable
Jan 15 10:20:16 e83cd99f5e18 env[63]: E: Unable to correct problems, you have held broken packages.
Jan 15 10:20:16 e83cd99f5e18 env[63]: + ret=100
Jan 15 10:20:16 e83cd99f5e18 env[63]: + '[' 100 -ne 0 ']'
Jan 15 10:20:16 e83cd99f5e18 env[63]: + echo 'Failed to obtain libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-
Jan 15 10:20:16 e83cd99f5e18 env[63]: Failed to obtain libindy-crypto=0.4.5 python3-indy-crypto=0.4.5 indy-plenum=1
Jan 15 10:20:16 e83cd99f5e18 env[63]: + exit 1
Jan 15 10:20:16 e83cd99f5e18 env[63]: Upgrade from 1.6.755 to 1.6.756 failed
{noformat}

Expected Results:
Upgrade should perform normally.;;;","15/Jan/19 10:16 PM;Derashe;Fixed in PR: [https://github.com/hyperledger/indy-node/pull/1126]

Use indy-node 1.6.757 master;;;","16/Jan/19 9:34 PM;VladimirWork;Build Info:
indy-node 1.6.757

Steps to Validate:
1. Install indy-node 1.6.757.
2. Upgrade 1.6.757 to 1.6.758 (version with distro -> version with distro).
3. Install indy-node 1.6.750.
4. Upgrade 1.6.750 to 1.6.757 (version without distro -> version with distro).

Actual Results:
Installation and both upgrades works normally.;;;",,,,,,,,,,,,,
"Upgrade appears to have broken ""validator-info --nagios""",INDY-1920,36235,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,nzHiggy,nzHiggy,18/Dec/18 11:11 AM,30/Mar/19 5:33 AM,28/Oct/23 2:47 AM,30/Mar/19 5:33 AM,1.6.80,1.6.83,validator-info,,,0,TShirt_S,,,"+*Running:*+

validator-info --nagios

*+Output:+* 
Traceback (most recent call last):
 File ""/usr/local/bin/validator-info"", line 866, in <module>
 sys.exit(main())
 File ""/usr/local/bin/validator-info"", line 850, in main
 print(get_stats_from_file(json_data, args.verbose, args.json, args.nagios))
 File ""/usr/local/bin/validator-info"", line 589, in get_stats_from_file
 return nagios(vstats)
 File ""/usr/local/bin/validator-info"", line 574, in nagios
 ] + [
TypeError: 'TransactionsStats' object is not subscriptable
Error in sys.excepthook:
Traceback (most recent call last):
 File ""/usr/lib/python3/dist-packages/apport_python_hook.py"", line 63, in apport_excepthook
 from apport.fileutils import likely_packaged, get_recent_crashes
 File ""/usr/lib/python3/dist-packages/apport/__init__.py"", line 5, in <module>
 from apport.report import Report
 File ""/usr/lib/python3/dist-packages/apport/report.py"", line 30, in <module>
 import apport.fileutils
 File ""/usr/lib/python3/dist-packages/apport/fileutils.py"", line 23, in <module>
 from apport.packaging_impl import impl as packaging
 File ""/usr/lib/python3/dist-packages/apport/packaging_impl.py"", line 23, in <module>
 import apt
 File ""/usr/lib/python3/dist-packages/apt/__init__.py"", line 34, in <module>
 apt_pkg.init_config()
SystemError: E:Opening configuration file /etc/apt/apt.conf.d/01turnkey - ifstream::ifstream (13: Permission denied)

Original exception was:
Traceback (most recent call last):
 File ""/usr/local/bin/validator-info"", line 866, in <module>
 sys.exit(main())
 File ""/usr/local/bin/validator-info"", line 850, in main
 print(get_stats_from_file(json_data, args.verbose, args.json, args.nagios))
 File ""/usr/local/bin/validator-info"", line 589, in get_stats_from_file
 return nagios(vstats)
 File ""/usr/local/bin/validator-info"", line 574, in nagios
 ] + [
TypeError: 'TransactionsStats' object is not subscriptable",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jan/19 8:49 AM;wombletron;sparknz-validator-info.log.tgz;https://jira.hyperledger.org/secure/attachment/16600/sparknz-validator-info.log.tgz","22/Jan/19 5:59 AM;wombletron;validator-info.json;https://jira.hyperledger.org/secure/attachment/16605/validator-info.json",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-984,,,No,,Unset,No,,,"1|hzwvif:000001",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),nzHiggy,Toktar,VladimirWork,wombletron,,,,,,,,,"15/Jan/19 6:36 PM;Toktar;We try to run  script with different valid and invalid input json but can't catch this bug.
[~nzHiggy] Have you the input log file that reproduce the bug?;;;","16/Jan/19 9:29 AM;wombletron;[~Toktar] Kevin is away at the moment. I can run commands though to capture what may be needed.

When I run /usr/local/bin/validator-info --json or -v the command works as expected.
When i run /usr/local/bin/validator-info --nagios i get the error below:

pault@capliam19:/usr/local/bin$ sudo ./validator-info --nagios
Traceback (most recent call last):
File ""./validator-info"", line 866, in <module>
sys.exit(main())
File ""./validator-info"", line 850, in main
print(get_stats_from_file(json_data, args.verbose, args.json, args.nagios))
File ""./validator-info"", line 589, in get_stats_from_file
return nagios(vstats)
File ""./validator-info"", line 574, in nagios
] + [
TypeError: 'TransactionsStats' object is not subscriptable

Do you want anything specific checked?;;;","17/Jan/19 7:21 AM;Toktar;Hi [~wombletron]!
Thank you for your answer. Could you send me a file 'validator-info.log' from directory '/var/lib/indy' on node where you are running script? It help me reproduce the bug.;;;","17/Jan/19 9:12 AM;wombletron;Shall I just attach it here, or email it to you? its nice and small.;;;","17/Jan/19 10:10 PM;Toktar;If it's nice and small I think will be better to attach it here for a history.  Please, archive the file if it will be more than Jira can upload.;;;","19/Jan/19 8:49 AM;wombletron;Here you go.

[^sparknz-validator-info.log.tgz];;;","21/Jan/19 7:50 PM;Toktar;Thank you very much! But I'm so sorry, could you attach validator-info.json too?;;;","22/Jan/19 6:19 AM;wombletron;I take it you mean the output of validator-info --json

[^validator-info.json];;;","23/Jan/19 7:59 AM;Toktar;Thank you! We found the problem and fixed it.

--nagios will work correctly in a new version 1.6.83;;;","23/Jan/19 6:48 PM;Toktar;*Problem reason:*
 - TransactionsStats has another structure than other BaseUnknown elements. In --nagios option,  'transaction-count' call is incorrect and causes an error.

*Changes:*
 - Added methods to correct handling this way of calls to TransactionsStats.

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1137]

*Version:*
 * indy-node 1.6.769 -master (indy-plenum 1.6.658 -master)

*Risk factors:*
 - Problem with validator-info script work.

*Risk:*
 - Low for validator-info, no risk for system

*Recommendations for QA:*

Test validator-info script with
 * – nagios
 * --json
 * -v
 * without parameters;;;","25/Jan/19 12:04 AM;VladimirWork;Build Info:
indy-node 1.6.770

Steps to Validate:
1. Test validator-info script with:
1.1. --nagios
1.2. --json
1.3. -v
1.4. without parameters

Actual Results:
Validator-info works as expected.;;;",,,,,,,,,,,,,,
Pool was broken after load with 30+ nyms/sec,INDY-1921,36243,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ozheregelya,ozheregelya,18/Dec/18 9:19 PM,01/Aug/19 9:57 PM,28/Oct/23 2:47 AM,17/May/19 4:59 PM,,,,,,0,,,,"*Environment:*
 indy-node 1.6.734

*Steps to Reproduce:*
 1. Setup the pool.
 2. Run following load from 2 clients:
{code:java}
perf_processes.py -g persistent_transactions_genesis -m t -n 1 -y one -s 000000000000000000000000Truste
e1 -k nym -c 10 -b 1 -l 15
{code}
 

3. Run stepwise load:
{code:java}
  spike:                                    
    req_kind: nym
    mode: stepwise                          
    step_value: 1                           
    step_time_in_seconds: 1800              
    step_initial_load_rate: 1
{code}
*Actual Results:*
 Pool stopped working after ~10 hours of load (most of the time only 30 nyms/sec were send). No ViewChanges were happened. According to validator-info, only two nodes (Node1 and Node7) have not deleted replicas.

*Logs and metrics:* s3://qanodelogs/indy-1921
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1921/ /home/ev/logs/indy-1921/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywbj",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,,,,,,,,,,,"17/May/19 4:59 PM;ashcherbakov;Closing as out-of-date;;;",,,,,,,,,,,,,,,,,,,,,,,,
validator-info to client times out if there are many upgrade attempts by node,INDY-1922,36264,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,mgbailey,mgbailey,19/Dec/18 8:10 AM,30/Mar/19 5:34 AM,28/Oct/23 2:47 AM,30/Mar/19 5:34 AM,,1.6.83,validator-info,,,1,TShirt_S,,,"A node, sovrin.sicpa.com, had an issue that prevented it from upgrading properly during the last upgrade via trustee transaction. A side effect of this failure was that a thousand or so attempts to upgrade were done by this node before we were able to stop it. That is the subject of a different ticket (INDY-1918). 

Following remediation of this issue and manual upgrade of the validator, attempting get-validator-info fails to respond. The logs on the validator show a warning that the message is too large to transmit. I believe that the underlying issue is that one of the items reported in validator-info is a table of upgrade attempts. If, as I suppose, this is causing the message to become too large if there are many upgrade attempts, the table should be limited or truncated before transmission to prevent this message size overflow.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1918,,,,,,,,,,,,,,,,,,,,"16/Jan/19 5:43 AM;mgbailey;sicpa_logs.tgz;https://jira.hyperledger.org/secure/attachment/16559/sicpa_logs.tgz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-984,,,No,,Unset,No,,,"1|hzwvif:0003i",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,lbendixsen,mgbailey,ozheregelya,,,,,,,,,"09/Jan/19 1:48 AM;lbendixsen;[~esplinr] The node on the test net that has this failure is causing my monitoring script not to function properly.  I can't monitor the node that is having this problem because my script depends on the results returning from validator-info.  I need this fixed as soon as you can. ;;;","11/Jan/19 6:15 PM;Derashe;We've reproduced situation from INDY-1918 and check what problem with validator_info appears. It seems to be the same that described in description of ticket. We've made fixes for this problem in PR below. I think that this would be enough for closing this ticket. But it would be nice if [~lbendixsen] or [~mgbailey] provide any logs from node or validator_info history, so we ensure that this is the right problem.

PR: https://github.com/hyperledger/indy-plenum/pull/1050;;;","14/Jan/19 4:53 PM;Derashe;Rec for QA: 
 * Run 4 nodes pool with config VALIDATOR_INFO_UPGRADE_LOG_SIZE = 2. For best case you can set different size on different node
 * Schedule and perform few upgrades (>2)
 * Get validator info from nodes and check that upgrade log size cropped and is not more than config value.;;;","16/Jan/19 1:19 AM;ozheregelya;*Environment:*
indy-node 1.6.753 -> 1.6.754

*Steps to Validate:*
1. Set up the pool of 4 nodes.
2. Schedule several upgrades and wait when they will be applied.
3. Set different values of VALIDATOR_INFO_UPGRADE_LOG_SIZE (2, 5, 0, default) in nodes configs.
4. Run ledger get-validator-info from indy-cli.

*Actual Results:*
If VALIDATOR_INFO_UPGRADE_LOG_SIZE=X is specified in config and X value differs from 0, than X rows are shown in validator-info reply.
If VALIDATOR_INFO_UPGRADE_LOG_SIZE=0 is specified in config, whole upgrade log is shown in validator-info reply.
If VALIDATOR_INFO_UPGRADE_LOG_SIZE is not specified in config, 10 rows are shown in validator-info reply.;;;","16/Jan/19 5:43 AM;mgbailey;[~Derashe]  The steward has provided logs, which are attached here.;;;","16/Jan/19 3:16 PM;Derashe;Thanks.;;;",,,,,,,,,,,,,,,,,,,
Hotfix 1.6.82,INDY-1923,36274,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,zhigunenko.dsr,ashcherbakov,ashcherbakov,19/Dec/18 4:46 PM,30/Mar/19 5:32 AM,28/Oct/23 2:47 AM,30/Mar/19 5:32 AM,,1.6.82,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa6:rzu",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PoA: ledger synchronization,INDY-1924,36276,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,ashcherbakov,ashcherbakov,19/Dec/18 6:06 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Acceptance criteria:*

Create a PoA for the following requirements:

Requirement 1: No synchronization issues between any ledgers

Requirement 2: Auditability/history. Must have for domain/config. Nice to have for all combinations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:00008",,,,Unset,Unset,Ev 18.25,Ev 19.1,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,,,,,,,,,,,"26/Dec/18 8:28 PM;sergey.khoroshavin;PR: https://github.com/hyperledger/indy-plenum/pull/1044;;;",,,,,,,,,,,,,,,,,,,,,,,,
Fix documentation related to old client,INDY-1925,36280,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,ashcherbakov,ashcherbakov,19/Dec/18 9:25 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"*Acceptance criteria*
 # Remove any mentioning of old client from [https://github.com/hyperledger/indy-node/blob/master/environment/docker/pool/README.md]
 # Update [https://github.com/hyperledger/indy-node/blob/master/README.md#how-to-install-a-test-network]
 # Mention at the beginning of [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md] that the recommended way of starting a Pool is to use Docker (with the corresponding reference to Docker doc)
 # Mention at the beginning of [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md] that the recommended way of starting a Pool is to use Docker (with the corresponding reference to Docker doc)
 # Mention at the beginning of [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md|https://github.com/hyperledger/indy-plenum/blob/master/docs/start_nodes.md] that the recommended way of starting a Pool is to use Docker (with the corresponding reference to Docker doc)
 # Remove  indy-running-locally.md from indy node docs
 # Remove cluster-simulation.md from indy node docs",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-792,,,No,,Unset,No,,,"1|hzwxa6:rzy",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,,,,,,,,,,,"20/Dec/18 1:13 AM;Derashe;PRs:
 * plenum: https://github.com/hyperledger/indy-plenum/pull/1040
 * node: https://github.com/hyperledger/indy-node/pull/1103;;;",,,,,,,,,,,,,,,,,,,,,,,,
Replica's primaryName is not checked for None during logging,INDY-1926,36285,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,sergey-shilov,sergey-shilov,sergey-shilov,19/Dec/18 11:40 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"It is a normal situation if replica's primaryName is None (for example, during the view change process). But there is a place where it is not checked during logging.

Back trace:

Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 110, in wrapper
Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]:     action()
Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/monitor.py"", line 418, in check_unordered
Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]:     handler(new_unordereds)
Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 227, in unordered_reques
t_handler_logging
Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]:     .format(reqId, duration, replica.primaryName.split(':')[0], prepre_sender,
Dec 19 02:20:53 irelandQALive3.qatest.evernym.com env[2638]: AttributeError: 'NoneType' object has no attribute 'split'",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwxa6:rzv",,,,Unset,Unset,Ev 18.25,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey-shilov,,,,,,,,,,,,"20/Dec/18 8:43 PM;sergey-shilov;*Problem state / reason:*

It is a normal situation if replica's primaryName is None (for example, during the view change process). But there is a place where it is not checked during logging.

*Changes:*

Added check for None.

*Committed into:*

    https://github.com/hyperledger/indy-plenum/pull/1039
    https://github.com/hyperledger/indy-node/pull/1104
    indy-node <to be built>

*Risk factors:*

    Nothing is expected.

*Risk:*

    Low

*Recommendations for QA:*

The fix is trivial, there is nothing for QA here.;;;",,,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.82,INDY-1927,36350,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,krw910,zhigunenko.dsr,zhigunenko.dsr,20/Dec/18 10:57 PM,30/Mar/19 5:32 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"*Version Information*
 indy-node 1.6.82
 indy-plenum 1.6.57
 sovrin 1.1.34

*Notices for Stewards*
 *(!) There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*
*(!)* *Pool upgrade to sovrin 1.1.32 and above should be performed simultaneously for all nodes due to txn format changes.***
 (!) indy-node 1.6.82 isn't using the latest versions of libindy-crypto (must be 0.9.5) and python3-indy-crypto(must be 0.9.5). Set these packages versions in case of new node manual installation

*Changes*
 INDY-1909 - Add old instance change messages discarding
 INDY-1836 - Increase ToleratePrimaryDisconnection and bind re-try time
 INDY-1926 - Add check for None of replica's primary name during logging",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i0046v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),TechWritingWhiz,zhigunenko.dsr,,,,,,,,,,,"25/Dec/18 12:38 AM;TechWritingWhiz;This is done. The pull request is here: [https://github.com/sovrin-foundation/sovrin/pull/137]

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,
"As a user of Valdiator Info script, I need to know whether the pool has write consensus and when the state was updated the last time",INDY-1928,36362,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,21/Dec/18 5:29 PM,13/Jul/19 6:53 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,1.6.83,validator-info,,,0,,,,"*Acceptance Criteria:*
 * Add the time when state was updated for every ledger (assuming that INDY-933 is done) into validator info output
 * This needs to be present in the verbose and json output, but not in the default output.",,,,,,,,,,INDY-933,,,,,,,,,,,,,,,,,,,INDY-1916,INDY-1281,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-56,,,No,,Unset,No,,,"1|hzwvif:0000006",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,esplinr,VladimirWork,,,,,,,,,"21/Dec/18 10:16 PM;esplinr;[~ashcherbakov]: [~mgbailey] requested that this not appear in the default output. We can move it to the default output in the future if he finds that he uses it regularly.;;;","11/Jan/19 7:03 PM;anikitinDSR;h2. *Plan of attack*

*Requirement*

We need to add ""pool in write consensus"" indication and the last time when the state was updated into output of validator info script.

*What to do*

In the scope of already implemented INDY-933 task, we have a ""freshness"" mechanism which can give last_updated time for each ledger. Summarizing  we need to:
 * get the last_updated value from FreshnessState for each ledger in ledger_ids
 * compare last_updated value with value from the last check. If difference beetween the current last_updated value and the current time less then 2*FRESHNESS_TIMEOUT, then we don't have a write consensus for this ledger (setup corresponded flag in False).
 *  extend section ""Node_info""to format, like:
  ""Freshness_status"":
 {
 ""ledger_id"": \{         ""Last_updated_time"": ""time when state was updated"",         ""Has_write_consensus"": ""True or False""      }
}

*How to do*

This suggestion will be integrated in existing validator-info dumping procedure.;;;","24/Jan/19 6:30 PM;anikitinDSR;Reasons:
 * need to add Freshness_status into validator-info output

Changes:
 * changed last_updated to timestamp representation
 * added method for ppTime and last_updated in freshness
 * added Freshness section into validator-info output with structure like in PoA

Version:
 * indy-node: 1.6.770

Recomendation for QA:
 * check, that there is section Freshness_status into Node_info section of 'validator-info -v' output
 * check, that get-validator-info cmd include Freshness_status info
 * check, that Freshness_status info will changed when freshness updates
 * stop 2 nodes from 4, wait for 2 Freshness timeouts and check, that Has_write_consensus changed to False

 

 ;;;","28/Jan/19 5:01 PM;VladimirWork;Build Info:
indy-node 1.6.771

Steps to Validate:
1. Check, that there is section Freshness_status into Node_info section of 'validator-info -v' output.
2. Check, that get-validator-info cmd include Freshness_status info.
3. Check, that Freshness_status info will changed when freshness updates.
4. Stop 2 nodes from 4, wait for 2 Freshness timeouts and check, that Has_write_consensus changed to False.

Actual Results:
Freshness status is displayed and is actual.;;;",,,,,,,,,,,,,,,,,,,,,
Implementation: Apply new request handlers approach to Token Plugins,INDY-1929,36363,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,21/Dec/18 5:36 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,Stub for ST-510,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00001yu",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implementation: Apply new multi-signature approach to Token Plugins,INDY-1930,36364,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,21/Dec/18 5:37 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,Stub for ST-509,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:000006r20e6",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sovrin can't be upgraded to not the latest version,INDY-1931,36372,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,,ozheregelya,ozheregelya,22/Dec/18 1:12 AM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"*Environment*:
 indy-node 1.6.80, sovrin 1.1.12 -> 1.1.33

*Steps to Reproduce:*
 1. Setup the pool with indy-node 1.6.80 RC with sovrin 1.1.12 (not pinned to the node version).
 2. Send upgrade transaction to sovrin 1.1.33 RC.

*Actual Results:*
 Sovrin can't be upgraded to not the latest version since newer versions of plugins present in the repo.
{code:java}
Dec 21 14:43:26 3f2ade248020 env[67]: sovrin : Depends: sovtoken (= 0.9.7) but it is not going to be installed 
Dec 21 14:43:26 3f2ade248020 env[67]: Depends: sovtokenfees (= 0.9.7) but it is not going to be installed 
Dec 21 14:43:26 3f2ade248020 env[67]: E: Unable to correct problems, you have held broken packages.
{code}
*Expected Results:*
 It should pe possible to upgrade to not the latest version of sovrin.

*Journalctl* after upgrade:
[^Node1_journalctl.out] [^Node2_journalctl.out] [^Node3_journalctl.out] [^Node4_journalctl.out] [^Node5_journalctl.out] [^Node6_journalctl.out]",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Dec/18 8:28 PM;ozheregelya;Node1_journalctl.out;https://jira.hyperledger.org/secure/attachment/16483/Node1_journalctl.out","24/Dec/18 8:28 PM;ozheregelya;Node2_journalctl.out;https://jira.hyperledger.org/secure/attachment/16484/Node2_journalctl.out","24/Dec/18 8:28 PM;ozheregelya;Node3_journalctl.out;https://jira.hyperledger.org/secure/attachment/16485/Node3_journalctl.out","24/Dec/18 8:28 PM;ozheregelya;Node4_journalctl.out;https://jira.hyperledger.org/secure/attachment/16486/Node4_journalctl.out","24/Dec/18 8:28 PM;ozheregelya;Node5_journalctl.out;https://jira.hyperledger.org/secure/attachment/16487/Node5_journalctl.out","24/Dec/18 8:28 PM;ozheregelya;Node6_journalctl.out;https://jira.hyperledger.org/secure/attachment/16488/Node6_journalctl.out","24/Dec/18 4:09 PM;zhigunenko.dsr;indy-1931.tar.gz;https://jira.hyperledger.org/secure/attachment/16482/indy-1931.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i004an:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,zhigunenko.dsr,,,,,,,,,,"24/Dec/18 8:53 PM;zhigunenko.dsr;*Step to Reproduce:*
Upgrade Sovrin 1.1.32 to Sovrin 1.1.33

*Actual Results:*
{code:java}
Dec 24 09:55:00 51cae9acf69f env[1765]: WARNING: apt does not have a stable CLI interface. Use with caution in scripts.
Dec 24 09:55:00 51cae9acf69f env[1765]: Get:1 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Dec 24 09:55:03 51cae9acf69f env[1765]: Hit:2 https://repo.sovrin.org/deb xenial InRelease
Dec 24 09:55:03 51cae9acf69f env[1765]: Hit:3 https://repo.sovrin.org/sdk/deb xenial InRelease
Dec 24 09:55:03 51cae9acf69f env[1765]: Hit:4 http://archive.ubuntu.com/ubuntu xenial InRelease
Dec 24 09:55:03 51cae9acf69f env[1765]: Get:5 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Dec 24 09:55:04 51cae9acf69f env[1765]: Get:6 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Dec 24 09:55:05 51cae9acf69f env[1765]: Fetched 323 kB in 4s (69.5 kB/s)
Dec 24 09:55:11 51cae9acf69f env[1765]: Reading package lists...
Dec 24 09:55:12 51cae9acf69f env[1765]: Building dependency tree...
Dec 24 09:55:12 51cae9acf69f env[1765]: Reading state information...
Dec 24 09:55:12 51cae9acf69f env[1765]: 18 packages can be upgraded. Run 'apt list --upgradable' to see them.
Dec 24 09:55:25 51cae9acf69f env[1765]: /bin/sh: 1: Syntax error: ""("" unexpected
Dec 24 09:55:25 51cae9acf69f env[1765]: E: No packages found
Dec 24 09:55:25 51cae9acf69f env[1765]: E: No packages found
Dec 24 09:55:25 51cae9acf69f env[1765]: E: No packages found
Dec 24 09:55:31 51cae9acf69f env[1765]: + deps='sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33'
Dec 24 09:55:31 51cae9acf69f env[1765]: + '[' -z 'sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33' ']'
Dec 24 09:55:31 51cae9acf69f env[1765]: + echo 'Try to donwload indy version sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33'
Dec 24 09:55:31 51cae9acf69f env[1765]: Try to donwload indy version sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33
Dec 24 09:55:31 51cae9acf69f env[1765]: + apt-get -y update
Dec 24 09:55:31 51cae9acf69f env[1765]: Hit:1 http://archive.ubuntu.com/ubuntu xenial InRelease
Dec 24 09:55:31 51cae9acf69f env[1765]: Get:2 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [109 kB]
Dec 24 09:55:31 51cae9acf69f env[1765]: Get:3 http://security.ubuntu.com/ubuntu xenial-security InRelease [107 kB]
Dec 24 09:55:32 51cae9acf69f env[1765]: Get:4 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [107 kB]
Dec 24 09:55:33 51cae9acf69f env[1765]: Hit:5 https://repo.sovrin.org/deb xenial InRelease
Dec 24 09:55:33 51cae9acf69f env[1765]: Hit:6 https://repo.sovrin.org/sdk/deb xenial InRelease
Dec 24 09:55:34 51cae9acf69f env[1765]: Fetched 323 kB in 2s (126 kB/s)
Dec 24 09:55:40 51cae9acf69f env[1765]: Reading package lists...
Dec 24 09:55:40 51cae9acf69f env[1765]: + apt-get --download-only -y --allow-downgrades --allow-change-held-packages install sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33
Dec 24 09:55:46 51cae9acf69f env[1765]: Reading package lists...
Dec 24 09:55:47 51cae9acf69f env[1765]: Building dependency tree...
Dec 24 09:55:47 51cae9acf69f env[1765]: Reading state information...
Dec 24 09:55:48 51cae9acf69f env[1765]: libindy-crypto is already the newest version (0.4.5).
Dec 24 09:55:48 51cae9acf69f env[1765]: Some packages could not be installed. This may mean that you have
Dec 24 09:55:48 51cae9acf69f env[1765]: requested an impossible situation or if you are using the unstable
Dec 24 09:55:48 51cae9acf69f env[1765]: distribution that some required packages have not yet been created
Dec 24 09:55:48 51cae9acf69f env[1765]: or been moved out of Incoming.
Dec 24 09:55:48 51cae9acf69f env[1765]: The following information may help to resolve the situation:
Dec 24 09:55:48 51cae9acf69f env[1765]: The following packages have unmet dependencies:
Dec 24 09:55:48 51cae9acf69f env[1765]:  sovrin : Depends: sovtoken (= 0.9.7) but 0.9.8 is to be installed
Dec 24 09:55:48 51cae9acf69f env[1765]:  sovtoken : Depends: indy-plenum (= 1.6.57) but 1.6.55 is to be installed
Dec 24 09:55:48 51cae9acf69f env[1765]: E: Unable to correct problems, you have held broken packages.
Dec 24 09:55:48 51cae9acf69f env[1765]: + ret=100
Dec 24 09:55:48 51cae9acf69f env[1765]: + '[' 100 -ne 0 ']'
Dec 24 09:55:48 51cae9acf69f env[1765]: + echo 'Failed to obtain sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33'
Dec 24 09:55:48 51cae9acf69f env[1765]: Failed to obtain sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33
Dec 24 09:55:48 51cae9acf69f env[1765]: + exit 1
Dec 24 09:55:48 51cae9acf69f env[1765]: Upgrade from 1.1.32 to 1.1.33 failed: Command 'upgrade_indy_node ""sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33""' returned non-zero exit status 1
Dec 24 09:55:48 51cae9acf69f env[1765]: Trying to rollback to the previous version Command 'upgrade_indy_node ""sovtoken indy-plenum=1.6.55 sovtoken=0.9.7 indy-node=1.6.80 libindy-crypto=0.4.5 sovtokenfees=0.9.7 sovrin=1.1.33""' returned non-zero exit status 1
{code}
;;;","24/Dec/18 9:54 PM;ashcherbakov;Sovrin 1.1.33 wasn't accepted and will be removed from Stable. PLugions 0.9.7 are not in Stable at all.;;;",,,,,,,,,,,,,,,,,,,,,,,
Warning message 'invalid BLS signature' appears in logs after several hours of load,INDY-1932,36424,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ozheregelya,ozheregelya,28/Dec/18 8:22 PM,26/Dec/19 10:29 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,"Environment:
indy-node 1.6.738

Steps to Reproduce:
1. Setup the pool with config described in [comment|https://jira.hyperledger.org/browse/INDY-1836?focusedCommentId=54799&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-54799] to INDY-1836, but do not stop any nodes.
2. Run the production load.

Actual Results:
Pool stopped working after 12 hours of load because of INDY-1926, but before this following warnings were appeared in logs:
{code:java}
2018-12-19 02:11:16,815|WARNING|node.py|Node20 raised suspicion on node Node10 for Commit message has invalid BLS signature; suspicion code is 31
....
2018-12-19 02:11:17,188|WARNING|node.py|Node20 raised suspicion on node Node17 for Commit message has invalid BLS signature; suspicion code is 31
{code}
Note that test was started at 2018-12-18 16:08:00 and pool worked without any issues during ~10 hours.

Need to understand why these warnings were appeared.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1935,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c92z",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,Toktar,VladimirWork,,,,,,,,,,"22/Aug/19 9:04 PM;Toktar;Log files where this problem was reproduced on the Node3. 
 ev@evernymr33:logs/21_08_2019_consensus_issues.tar.gz;;;","09/Sep/19 9:54 PM;VladimirWork;The issue reproduces against indy-node 1.10.0~dev1077 and plugins 1.0.3~dev86 during prodcution load test:
ev@evernymr33:logs/2183_prod_load_with_stopped_node.tar.gz
ev@evernymr33:logs/performance-results__09-09-2019__14-01.tar.gz;;;",,,,,,,,,,,,,,,,,,,,,,,
Pool stopped writing after several days under spike load test,INDY-1933,36599,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ozheregelya,ozheregelya,09/Jan/19 8:52 PM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,1.7.1,,,,0,TShirt_L,,,"*Environment:*
indy-node 1.6.747

*Steps to Reproduce:*
1. Run following load tests from 4 machines:
client 1: writing 10 txns/sec during 1 hour and sleep for 2 hours (production mix);
client 2: writing 1 attrib/min, like on live pool;
client 3 and client 4: reading 50 txns/sec from each machine.

*Actual Results:*
Pool stopped writing after several days of spike load.
Most part of the nodes wrote 338512 txns in domain ledger, 3 nodes were lagged:
persistent_node1 331698
persistent_node5 331457
persistent_node9 307915

*Additional Information:*
Note that first attempt to run spike load test was unsuccessful and spike load was started later than the rest ones.

Metrics from Node 13:
!w2.png|thumbnail!

Logs and full metrics: s3://qanodelogs/indy-1933
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1933/ /home/ev/logs/indy-1933/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/19 7:35 PM;VladimirWork;INDY-1933.PNG;https://jira.hyperledger.org/secure/attachment/16803/INDY-1933.PNG","09/Jan/19 9:51 PM;ozheregelya;w2.png;https://jira.hyperledger.org/secure/attachment/16512/w2.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0000028",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,VladimirWork,,,,,,,,,,"11/Feb/19 6:29 PM;Derashe;Founded problem:

When few backup replicas stop ordering at the same time, MedianLowStrategy is incorrect way to measure backup replica faulty, because it may cause situation when we can not distinguish if replicas degraded or not. 

Solution:

We can solve this problem, by using MedianMediumStrategy for measuring backups and left MedianLowStrategy only for master

PR: [https://github.com/hyperledger/indy-plenum/pull/1080];;;","12/Feb/19 8:40 PM;Derashe;Indy Node version: 802;;;","13/Feb/19 12:39 AM;Derashe;Req for QA:
 * Run pool of 7 nodes (f = 2, r = 3)
 * Send 1 txn
 * Load backup primaries nodes with some task ( ""stress -c 10 -i 10 -m 10"" for example. stress can be installed from apt)
 * Run a little load test (~200 txns)
 * If there were no view_change, ensure that every node deleted 1 and 2 replicas ( and only master - 0 left) else talk with me about changing test case;;;","13/Feb/19 7:35 PM;VladimirWork;Build Info:
indy-node 1.6.803

Steps to Validate:
1. Run pool of 7 nodes (f = 2, r = 3).
2. Send 1 txn.
3. Load backup primaries nodes with some task.
4. Run a little load test (~200 txns).

Actual Results:
There are no VCs and both backup replicas are deleted on each node. !INDY-1933.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,
validator-info field key has space in it,INDY-1934,36618,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,mgbailey,mgbailey,10/Jan/19 4:29 AM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,,,,,0,TShirt_S,,,"One of the fields that was recently added to validator-info is ""Update time"". The space in this key name  in the JSON output breaks some of our parsing. Please rename it to ""Update_time"", to match the convention used throughout our JSON output.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/19 4:17 PM;VladimirWork;INDY-1934.PNG;https://jira.hyperledger.org/secure/attachment/16538/INDY-1934.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0003c",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,mgbailey,VladimirWork,,,,,,,,,,"10/Jan/19 8:32 PM;anikitinDSR;Reasons:
 * need to change field ""Update time"" to ""Update_time"" in the output of validator-info json file

Changes:
 * replaced whitespace in field to ""_"" (for now should be ""Update_time"")

PR:
 * indy-node: [https://github.com/hyperledger/indy-node/pull/1120]

Version:
 * indy-node: 1.6.752

For QA:
 * start up indy-node and check that validator-info's json file includes a ""Update_time"" field and don't this field with whitespace

 ;;;","11/Jan/19 8:02 PM;VladimirWork;Build Info:
indy-node 1.6.752

Steps to Validate:
1. Run `validator-info --json | grep Update_time`.
2. Run `validator-info --json | grep 'Update time'`.

Actual Results:
Field ""Update time"" is renamed to ""Update time"" (for both --json and -v modes).;;;","12/Jan/19 12:15 AM;mgbailey;Another test that I would like validated:
 # Using indy-cli from a client, attach to the network as a steward. 
 # Run 'ledger get-validator-info'

Verify that the key in the json output has changed to Update_time;;;","14/Jan/19 4:18 PM;VladimirWork;'ledger get-validator-info' returns 'Update_time' for both Trustee and Steward users. !INDY-1934.PNG|thumbnail! ;;;",,,,,,,,,,,,,,,,,,,,,
A node may use incorrect BLS key for signature,INDY-1935,36730,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,14/Jan/19 5:32 PM,26/Dec/19 10:29 PM,28/Oct/23 2:48 AM,,,,,,,0,TShirt_L,,,"*Problem:***
 * Incorrect BLS multi-sig appears in the load test sometimes.

*Problem Reason:*
 * The reason is that BLS multi-signature is created against the current uncommitted pool state (the one sent in PrePrepare), but nodes use the BLS keys for signature according to their committed pool state (Node's BLS keys are updated only when a 3PC batch is ordered, that is the state is committed).

*Test scenario:*
 * A Primary sends a PRE-PREPARE with BLS keys update against POOL_STATE1
 * A Primary sends a PRE-PREPARE with another BLS keys update against POOL_STATE2 *before* the firest PRE-PREPARE is committed and ordered (just delay Ordered msgs on all nodes)
 * {color:#de350b}For the second PRE-PREPARE, all nodes use BLS keys for signature against POOL_STATE1, but the signature will be validated by others against POOL_STATE2{color}

*{color:#172b4d}Acceptance criteria{color}*
 * {color:#172b4d}Write the integration test for the test case above
{color}
 * {color:#172b4d}Write a PoA and fix the problem{color}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1932,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w4c93",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,,,,,,,,,,,"22/Aug/19 9:03 PM;Toktar;Log files where this problem was reproduced on the Node3. 
 ev@evernymr33:logs/21_08_2019_consensus_issues.tar.gz;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool config transaction returns wrong result code,INDY-1936,36735,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,VladimirWork,VladimirWork,14/Jan/19 8:00 PM,14/Jan/19 8:00 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,"Build Info:
indy-node 1.6.752

Steps to Reproduce:
1. Send `pool config` txn using DID that isn't allowed to perform it.
2. Send any other txn the same way (`pool restart` for example).
3. Compare both txns' results.

Actual Results:
Pool restart: *REJECT* with 'client request invalid: UnauthorizedClientRequest(\\'role is not accepted\\',)' message.
Pool config: *REQNACK* with 'client request invalid: UnauthorizedClientRequest(\\'role is not accepted\\',)' message.

Expected Results:
Pool config txn should be validated the same way as all other txns (so it should return REQNACK for static validation and REJECT for dynamic validation).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/19 8:00 PM;VladimirWork;Inkedvalidation_LI.jpg;https://jira.hyperledger.org/secure/attachment/16540/Inkedvalidation_LI.jpg",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i006lj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stable release 1.6.83,INDY-1937,36739,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,ashcherbakov,ashcherbakov,14/Jan/19 8:34 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000001cr",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Dynamically enable/disable debug logging,INDY-1938,36743,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,smithbk,smithbk,14/Jan/19 10:22 PM,27/Mar/20 10:09 PM,28/Oct/23 2:48 AM,,,1.16.0,libsovrin,,,0,,,,"In order to support indy in production, we need to be able to dynamically enable and disable debug logging without restarting the indy-node process.  The logLevel can currently only be controlled via a config parameter, and changing this requires restarting the indy-node process.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywbk",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),smithbk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Trustee cannot demote Steward added by another Trustee,INDY-1939,36744,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,anikitinDSR,VladimirWork,VladimirWork,14/Jan/19 10:40 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,,,,,0,TShirt_S,,,"Build Info:
indy-node 1.6.755

Steps to Reproduce:
1. Add new Trustee by default Trustee.
2. Add Steward by new Trustee.
3. Remove role from Steward by default Trustee.

Actual Results:
{'op': 'REJECT', 'reqId': 1547466365636457443, 'reason': ""client request invalid: UnauthorizedClientRequest('actor must be owner',)"", 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'}

Expected Results:
Roles adding and removing should be performed according to https://github.com/hyperledger/indy-node/blob/master/docs/auth_rules.md

Additional Info:
We have the same issue with new NETWORK_MONITOR role so it looks like the whole roles' logic should be fixed.

For QA:
Implement system test according to https://github.com/hyperledger/indy-node/blob/master/docs/auth_rules.md to validate this ticket and run during acceptance testing (manually / in CD).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1730,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0003k",,,,Unset,Unset,Ev 19.1,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,VladimirWork,,,,,,,,,,,"16/Jan/19 9:19 PM;anikitinDSR;h3. *Problem reason*

**We had a test, which create a new Trustee by default Trustee, then new trustee create a steward and default trustee try to demote it. According to rules [auth_rules|https://github.com/hyperledger/indy-node/blob/master/docs/auth_rules.md] this action should be performed. 
h3. *Test describing*

This test sent NYM transction for demote steward using verkey field inside. In this case, from indy-node's side, we try to change field ROLE to None and field VERKEY from previous value to the same. But KeyRotation operation requeres that actor must owner of this NYM and corresponded error will be raised.
For case, if verkey field is not presented in NYM txn, then, from indy-node's side, actor is trying to change field ROLE to None and this operation will be performed, because TRUSTEE can demote any STEWARDs.
Previously, if we sent a NYM txn with the same ROLE and VERKEY field as in ledger, then we got 'REPLY' that means, that 'all fine' and write txn was performed, but it's wrong. 
h3. *Resume*

If we want to perform demote as described in ticket, then we should send NYM txn without verkey field. 

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,
validator-info should show the individual consensus status of each node,INDY-1940,36796,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,esplinr,esplinr,16/Jan/19 5:26 AM,16/Jan/19 5:26 AM,28/Oct/23 2:48 AM,,,,validator-info,,,0,,,,"The debug response from validator-info should show the individual consensus status of each node.
{code}
Node 1 got votes from: 2, 4, 15
{code}

A compact view on the default response could also show the consensus status:
{code}
Node 1 [.X.X..........X.........]
{code}
Which shows that there were votes from node 2, 4, and 15 and no votes from nodes the other nodes 1 through 24.

TODO: Verify whether this is useful for the consumers of validator-info",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i006yn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sovrin MainNet lost consensus,INDY-1941,36799,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,anikitinDSR,mgbailey,mgbailey,16/Jan/19 6:07 AM,30/Mar/19 5:32 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,TShirt_M,,,"This morning at 11:34 am MST, an automated process detected that it was no longer able to write a transaction to MainNet (indy-node version 1.6.80). A check of validator-info showed that the majority of the nodes were reporting that no primary was selected, although all nodes had the same transaction counts:
{code:java}
[all]> show primary
{
    ""Aalto"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""Aalto:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""BIGAWSUSEAST1-001"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""BIGAWSUSEAST1-001:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""DustStorm"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""DustStorm:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""NewtonD"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""NewtonD:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""OASFCU"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""OASFCU:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""ServerVS"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""ServerVS:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""Stuard"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""Stuard:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""TNO"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""TNO:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""VeridiumIDC"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""VeridiumIDC:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""amihan-sovrin"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""amihan-sovrin:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""atbsovrin"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""atbsovrin:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""danube"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""danube:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""digitalbazaar"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""digitalbazaar:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""esatus_AG"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""esatus_AG:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""ev1"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""ev1:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""findentity"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""findentity:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""iRespond"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""iRespond:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""ibm"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""ibm:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""icenode"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""icenode:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""pcValidator01"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""pcValidator01:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""prosovitor"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""prosovitor:0"": {
                    ""Primary"": null
                }
            }
        }
    },
    ""royal_sovrin"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""royal_sovrin:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""trustscience-validator02"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""trustscience-validator02:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    },
    ""zaValidator"": {
        ""Node_info"": {
            ""Replicas_status"": {
                ""zaValidator:0"": {
                    ""Primary"": ""ev1:0""
                }
            }
        }
    }
}
{code}
After getting a Sovrin Foundation trustee online, we were able to post a pool_restart transaction, which resulted in a view change and all nodes selected zaValidator as the new primary.

We now need to determine why the nodes lost their primary, and fell out of consensus, in order to prevent this behavior going forward. We will be posting logs to this ticket, as we receive them from stewards.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/19 4:29 AM;mgbailey;2019-01-16-digitalbazaar.log.xz;https://jira.hyperledger.org/secure/attachment/16590/2019-01-16-digitalbazaar.log.xz","23/Jan/19 5:51 AM;mattnorton;TNO_2019_01_14_onwards.log;https://jira.hyperledger.org/secure/attachment/16609/TNO_2019_01_14_onwards.log","22/Jan/19 8:32 AM;mgbailey;TNO_2019_01_14_onwards.log;https://jira.hyperledger.org/secure/attachment/16606/TNO_2019_01_14_onwards.log","17/Jan/19 4:29 AM;mgbailey;esatus_AG.tgz;https://jira.hyperledger.org/secure/attachment/16588/esatus_AG.tgz","22/Jan/19 8:32 AM;mgbailey;ev1.log.81.xz;https://jira.hyperledger.org/secure/attachment/16607/ev1.log.81.xz","16/Jan/19 6:07 AM;mgbailey;ev1_log.tgz;https://jira.hyperledger.org/secure/attachment/16568/ev1_log.tgz","17/Jan/19 4:29 AM;mgbailey;zaValidator.log.zip;https://jira.hyperledger.org/secure/attachment/16589/zaValidator.log.zip",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0000003",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,mgbailey,,,,,,,,,,,"17/Jan/19 4:35 AM;mgbailey;Some logs are attached. If there is a particular node you need logs for, please indicate it and I will try to get it from that steward.;;;","18/Jan/19 11:12 PM;anikitinDSR;[~mgbailey] Can you please add logs from ev1 for day 2019-01-15.
Attached log for ev1 starts from 2019-01-15 20:48:37 and pool was already restart at this time.;;;","22/Jan/19 8:32 AM;mgbailey;[~anikitinDSR] adding [^ev1.log.81.xz] ;;;","23/Jan/19 7:20 PM;anikitinDSR;After logs investigation was noticed, that this situation is quite similar with INDY-1903.
There is old INSTANCE_CHANGE message about primary disconnection and less than _n-f_ nodes enter view change. This case brings pool into read_only mode and automated process detected that consensus was lost. More details described into comment:

https://jira.hyperledger.org/browse/INDY-1903?focusedCommentId=54713&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-54713
This issue was fixed in INDY-1909 and will be presented in the next release.;;;",,,,,,,,,,,,,,,,,,,,,
Transaction Author Agreement,INDY-1942,36800,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,16/Jan/19 7:44 AM,13/Dec/19 6:49 AM,28/Oct/23 2:48 AM,09/Oct/19 12:44 AM,,,,,,0,,,,"*Story*
As a steward of an Indy network, I want transaction authors to agree that the information they submit to the ledger meets the requirements outlined by ledger governance for the current transaction to minimize the liability that stewards accept when storing the data on an immutable ledger.

*Acceptance Criteria*
* An author agreement which covers all transaction types is anchored to the domain ledger or plugin ledgers. (Might also include the hash to save on future computation.)
* The text is available to be looked up from the ledger for audit purposes.
* When the author agreement is written to the ledger, it must be signed by a configurable number of TRUSTEEs.
* New versions of the agreement can be added to the ledger, but previous versions are retained.
* The transaction format on the domain ledger has a field that contains a hash indicating the acceptance of the author agreement. This hash is signed when the transaction is signed with the private key of the DID that will make the future writes to the ledger.
* The digital signature and hash is created by the transaction author who is submitting the transaction and explicitly ties the transaction to the specific agreement that was accepted in order to submit the transaction.
* The transaction will only be accepted with the current version of the agreement.
* The transaction format should record the timestamp when the agreement was accepted by a user in a UI.
* The timestamp must be in a UTC ISO format. It should pass basic validation: that it is not more than two minutes in the future (allowing for minor clock skew), or more than two minutes before the time the current agreement was submitted to the ledger. If it has an invalid time, the transaction should be rejected with a clear error: ""Transaction rejected because the timestamp for accepting the author agreement is invalid.""
* The transaction format should include a field that contains the label of one of the agreement acceptance mechanisms enumerated on the list of valid values.
* The list of valid agreement acceptance mechanisms must be anchored on the config ledger. Each mechanism includes a label (max 64 characters) and a description (max 256 characters).
Initial mechanisms will include:
** Label: Product EULA, Description: Included in the EULA for the product being used.
** Label: Service Agreement, Description: Included in the agreement with the service provider managing the transaction.
** Label: Click Agreement, Description: Agreed through the UI at the time of submission.
** Label: Session Agreement, Description: Agreed at wallet instantiation or login.
* The list of valid agreement acceptance mechanisms includes a link to an off-ledger document containing additional information about the acceptance mechanisms such as how the agreement must be displayed in order to be legally meaningful.
* Documentation exists for how to put the transaction author agreement and agreement acceptance mechanism list onto the config ledger.
* If there is no Transaction Author Agreement registered on the ledger, then transactions should be accepted with an empty hash, empty acceptance timestamp, and empty agreement mechanism. (This is the new developer testing use case.)
* If there is a Transaction Author Agreement registered on the ledger, but no list of valid agreement mechanisms, then all transactions should be rejected.

*Questions*
Q. Is it sufficient to accept the agreement at the time a new DID is written to the ledger rather than for each transaction?
A. No it is not sufficient. We need confidence that users remember their acceptance for each transaction they right. We also want to avoid having to create a mechanism for having users re-accept the agreement when it changes.

Q. Is the signature of acceptance of the transaction author agreement an additional signature, or is the current transaction signature sufficient? Is it a new set of keys, or is it the same set of keys necessary for submitting the transaction?
A. The transaction author agreement is signed with the same set of keys that signs the transaction for submission to the ledger. As a result, the current transaction signature is sufficient. When we separate transaction authors from endorsers (INDY-1999), we will have to ensure that the author key is still used to sign the author agreement.

Q. Do we need acceptance for the Transaction Author Agreement for every transaction type, or just transactions that allow writing arbitrary data to the ledger?
A. We need acceptance of the agreement for all transaction types, as encryption keys are considered personal data and require acceptance of the agreement.

Q. Do plugin ledgers require the same agreement as writes to the domain ledger?
A. Yes. We expect to have a single agreement for all transactions for the foreseeable future.

Architecture question:
* What transaction type is best?
* Which ledger is best to store this transaction?",,,,,,,,,,,,,,,,,,,,,IS-1147,,,,,,,,INDY-634,IS-1148,IS-1234,IS-1298,INDY-2302,INDY-1999,INDY-2313,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,Transaction Author Agreement,Done,No,,Unset,No,,,"1|hzwvif:00001yw969",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),8.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,sergey.khoroshavin,sergey.minaev,,,,,,,,,"24/Apr/19 4:02 PM;ashcherbakov;[~esplinr]
 Questions about requirements:
 # Should the GET_TRANSACTION_AUTHOR_AGREEMENT request support getting of old versions of the agreement, or only the current (latest) one?
 # If GET_TRANSACTION_AUTHOR_AGREEMENT needs to support getting of old versions, then do we have any requirements about what should be used as an ID for old agreements? Should we have an explicit `version` field for every agreement, and make sure (during validation of the agreement on the ledger) that the next version is greater than the previous one, so that the version is a unique identifier? Can we use the version in GET_TRANSACTION_AUTHOR_AGREEMENT request then? If the version is empty, then the latest agreement will be returned.
 # What can be the maximum size of the Agreement? The thing is that we have a limitation for the maximum size of a transaction/request to the Ledger, and this is 128 KB.
 # As I understand, we should have a GET_TRANSACTION_AUTHOR_AGREEMENT_MECHANISM request (in addition to mechaism write transaction).
Should the request support getting of old versions of the mechanisms list, or only the current (latest) one?
 # If GET_TRANSACTION_AUTHOR_AGREEMENT_MECHANISM  needs to support getting of old versions, then do we have any requirements about what should be used as an ID for old lists?;;;","24/Apr/19 6:43 PM;esplinr;1 and 2: It should be possible to look up a TAA by the hash used to sign a transaction. The text is versioned, so it would be helpful to store an explicit version and do a lookup by version, if it is easy to do so. It is not necessary to store the version of the text in the transactions that are written to the ledger—the hash is sufficient.

3: The current size of the agreement proposed by the Sovrin Foundation is ~15K. I believe a 128KB limit is reasonable.

4 and 5: When examining a transaction, it will occasionally be necessary to look up the transaction author agreement mechanism that was in force at the time the transaction was written in order to get the description associated with the label, and the URL that points to additional information. Given that the list is expected to always be short, it is reasonable to return the entire list. This implies that there needs to be an explicit version number associated with the list of agreement mechanisms.

Q6: Are the versions of the TAA and the Agreement Mechanism List assigned by the ledger, or explicitly set by the user?
A6: It is must useful if they are set by the user. Any unique string should be allowed, so the user can name the ""version"" in a way that is natural for their use case of the ledger.

Q7: Is it possible to update just part of the list of agreement mechanisms? For example, edit a description of a single entry, or only update the URL.
A7: No. The entire list must be updated in order to make any changes. The list is short enough that this is practical.;;;","25/Apr/19 1:25 AM;ashcherbakov;[~esplinr] [~sergey.minaev]

More questions that may affect the decision whether we would like to combine TRANSACTION_AUTHOR_AGREEMENT and TRANSACTION_AGREEMENT_MECHANISM txns into one:
 * Q8: Should the user be aware of the description/URL of the selected acceptance mechanism? Should the description/URL be signed by the user?
 * Q9: Should the user be aware of all acceptance mechanisms when signing TAA? Should all the acceptance mechanisms be signed by the user? 

TA signing for XFER txns:
 * Q10: There is no DID-based signature for XFER txn. We have input-address-based signature only. Should we sign TA fields by input-address-based signature?;;;","25/Apr/19 6:56 AM;esplinr;A8: The description / URL of the acceptance mechanisms does not need to be signed by the user as part of the transaction, but the API should make it easy for an application developer to display it to the user.

A9: The acceptance mechanism is selected by the application developer, who chooses the mechanism that best matches the UX being used by the application to obtain user agreement before submission of the transaction to the ledger.

A10: Acceptance of the Transaction Author Agreement is indicated by including the hash of the agreement in the signed transaction that is submitted to the ledger. The format by which the transaction is currently signed is sufficient to demonstrate that agreement.

Q11: How likely is it that the list of acceptance mechanisms would change separate from the agreement? (If they always change together, they could conceivably be tracked on the ledger as the same transaction.)
A11: It is rare that the list of acceptance mechanisms would change separate from the agreement. But if such a change  was required, bundling the list along with the Transaction Author Agreement would change the hash of the agreement and force all users in the network to re-accept the agreement the next time they try to write to the ledger. That would be an important UX cost.

Additional thoughts on A4-5: Assuming that the list of acceptance mechanisms is separate from the transaction author agreement, then we will need an API to map a transaction on the domain ledger to the associated list of the acceptance mechanisms. Possible ways to do the mapping include:
* Always traversing through the audit ledger (because it should be rare)
* Including the sequence number of the list in the transaction format on the domain ledger
* Including an explicit version string in the list on the config ledger, and then including that version string in the transaction on the domain ledger.;;;","25/Apr/19 5:31 PM;sergey.minaev;Q12: Is it correct assumption about TAA: TAA text doesn't contain ANY assumptions, limitations etc about acceptance mechanisms list (AML). And it's fully independent of the AML?
 Q13: Is correct (in general) to include all details of acceptance mechanisms into TAA text?
 Q14: Does ""Additional thoughts on A4-5"" above means that user may not know actual mechanism and including label of mechanism under signature has not law impact;;;","25/Apr/19 5:41 PM;sergey.minaev;Q15: If TAA and AML are separate entities, must the latest version of AML be used by user?;;;","25/Apr/19 11:40 PM;esplinr;A12: Correct. The legal text of the author agreement is not dependent on the list of acceptance mechanisms.

A13: Including the details of the acceptance mechanisms in the TAA text doesn't hurt the legal agreement, but it could force users to re-accept the agreement if it is necessary to change the list of acceptance mechanisms (even in trivial ways such as updating the URL or fixing a typo).

A14: The mechanism the user follows to review and accept the agreement is integral to the software the user selects for interacting with the ledger. The developer of that software will design the UX to follow one of the acceptable approaches for getting the user's agreement. The developer will then hard-code into the software the label describing that approach (hopefully as a configuration parameter to make it easy to change should the network administrator ever choose to change the label). The software includes that label into the transaction on behalf of the user simply so that in the future there is an indication of how the user reviewed the agreement without exposing the user to the correlation risk that would come with recording the specific software package that was used.

A15: When submitting a transaction to the ledger, it is only necessary to have an acceptance mechanism label that is still one of the valid mechanisms in the list on the ledger. The description and associated URL are guides for the developer, and can be used in the future to describe what interaction the user probably had when accepting the agreement, but is not binding in the same way as the text of the agreement.

More additional thoughts on A4-5: Though we have to ensure that a transaction submitted to the ledger is using an acceptance mechanism label that is currently part of the approved list, it is not very important to get an old list of acceptance mechanisms. The list of acceptance mechanisms can change without the software being updated, so long as the label is still valid. It is a requirement for the network administrator to change the labels if the implied user interaction is different enough to invalidate software that is currently in use in the ecosystem. For this reason, looking up an old list of acceptance mechanisms does not need to be a fast operation and could require traversing ledger history based on the datetime of the transactions or by referencing the audit ledger. An explicit version number for the list of acceptance mechanisms might be helpful, but is not required.

Based on our conversations, we see three approaches:
1. Include  the acceptance mechanism list as part of the transaction author agreement text.
  Pro: Strong legal assurance that the user understood the acceptance mechanism, and that the label matches a specific description at a point in time.
  Con: Could require users to re-accept the agreement more often than necessary.
  Note: Our legal council said that the acceptance mechanism is a guide, and it is not necessary to have a strong mapping between the label and the description.

2. Include the acceptance mechanism list as separate fields in the same ledger transaction as the author agreement.
  Pro: Easier development, while allowing the mechanisms to be changed without changing the hash of the agreement.
  Con: The author agreement and the acceptance mechanism list are separate concerns, so combining them seems unnatural.

3. Have a separate ledger transaction for the author agreement and the acceptance mechanism list.
  Pro: This is the most flexible and natural approach.
  Con: It is the most work.

We will pursue the third approach unless it seems to be too expensive.;;;","27/Apr/19 2:34 AM;ashcherbakov;Design: https://github.com/hyperledger/indy-node/blob/master/design/txn_author_agreement.md;;;","30/Apr/19 11:58 PM;sergey.khoroshavin;[~esplinr]
Q16: Should there be a way to disable Transaction Author Agreement mechanism after it is enabled by sending first TAA transaction?;;;",,,,,,,,,,,,,,,,
Add audit ledger,INDY-1944,36810,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,sergey.khoroshavin,sergey.khoroshavin,16/Jan/19 4:06 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.7.1,,,,0,,,,"*Acceptance criteria*
* New ledger should be created with new AUDIT_LEDGER_ID
* Each transaction should include dictionary ledger_id : last_seq_no
* For each 3PC batch ordered on master instance transaction should be written to audit ledger
* Audit ledger should participate in catch-up
* Relevant tests should be implemented

*Additional notes*
* This is based on design described in https://github.com/hyperledger/indy-plenum/blob/master/design/catchup-and-audit-proposal.md
",,,,,,,,,,,,,,,,,INDY-1945,INDY-1946,INDY-1993,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/19 10:10 PM;VladimirWork;INDY-1944.PNG;https://jira.hyperledger.org/secure/attachment/16863/INDY-1944.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:000002o",,,,Unset,Unset,Ev-Node 19.03,Ev-Node 19.04,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,nanspro,ozheregelya,sergey.khoroshavin,Toktar,VladimirWork,,,,,,,"08/Feb/19 11:08 PM;ashcherbakov;PoA:

1) Register the new AUDIT ledger
 - no state is needed
 - make sure it's present in the list of ledgers

2) Audit Ledger catchup
 - catch-up audit ledger first
 - make sure it's caught up

3) Write to audit ledger for every Ordered 3PC Batch
 - make sure that audit ledger is updated after every ordered 3pc batch
 - check it for different combinations of 3PC batches from different ledgers and different 3PC batch sizes
 - put all required data into the txn and test the format

4) Include audit ledger txn root hashes into PrePrepare
 - add it as a last (optional) field
 - make sure it doesn't break compatibility
 - apply audit ledger txn during validation of PrePrepare and compare the root
 - raise Suspicious error if they are not equal

5) Update Documentation

6) [Optional] Move all catch-up related logic to Catchup Helper class from the Node monolith;;;","12/Feb/19 5:09 PM;ashcherbakov;Audit Ledger txn format:
{code:java}
{
    ""ver"": ""1"",
    ""txn"": {
        ""type"": ""2"", # AUDIT
        ""protocolVersion"": 2, # CURRENT_PROTOCOL_VERSION

        ""data"": {
            ""ver"": ""1"",
            ""viewNo"": <...>, # view no of last 3PC batch
            ""ppSeqNo"": <...>, # ppSeqNo of last 3PC batch
            ""ledgerSize"": { # a map of ledger_id -> size
                0: <...>, # pool ledger size
                1: <...>, # domain ledger size
                2: <...>, # config ledger size
                1001: <...>, # plugin ledger size
            },
            ""ledgerRoot"": {  # either a root hash as base58, or a delta, that is a difference between the current audit txn seqno and a seq_no of audit txn where it was changed last time
                0: <...>, # pool ledger root (hash or seqno delta)
                1: <...>, # domain ledger root (hash or seqno delta)
                2: <...>, # config ledger root (hash or seqno delta)
                1001: <...>, # plugin ledger root (hash or seqno delta)
                # -1: <...>, in case of shared root for all ledgers 
            },
           ""stateRoot"": {   # a state root hash as base58 for the ledgers that changed the state in this 3PC
                0: <...>, # pool state root hash
                1: <...>, # domain state root hash
                2: <...>, # config state root hash
                1001: <...>, # plugin state root hash
                # -1: <...>, in case of shared root for all ledgers  
            }    
        },
        ""metadata"": {
        },
    },
    ""txnMetadata"": {
        ""txnTime"": <...>,
        ""seqNo"": <...>,
    },
    ""reqSignature"": {
    }
} {code}
Please note, that `ledgerRoot` and `stateRoot` have values for all ledgers (or one `-1` value in future once we have a combined root).
 `ledgerRoot` value for every ledger can be either
 * ledger/state root hash as base58, if this ledger was changed in the 3PC batch the audit txn is created for
 * delta, that is a difference between the current audit txn seq_no and a seq_no of audit txn where it was changed last time

If the ledger is never changed, then the corresponding ledger_id is not present at all.

This is done so for
 # Easy way to verify the data consistency when doing external audit of the ledger. One can compare the root hash after applying each 3PC batch
 # Easy way to find out a txn (3PC batch) from audit ledger where a ledger had the given root hash. This will be needed for a new protocol of BLS multi-sig using audit ledger.
 # More compact data than if we stored all root hashes all the time (even if it's not changed).

Use case: Get the value at the given time
 - ts and ledger_id as an input
 - find an audit ledger txn at the given timestamp (using ts_store index)
 - if the ledger_id's value is a root has is present in the audit txn - return it
 - if the ledger_id's value is a delta (an integer), then get the audit txn for the `seq_no=current_audit_seq_no-delta` and get the root hash of the ledger from there;;;","12/Feb/19 5:44 PM;ashcherbakov;h2. *Examples of 2 audit txn.*

Sizes of ledgers before: ""0"": 10, ""1"": 5, ""2"": 15, ""1001"": 100,
h3. *Batch1: DOMAIN ledger (15 txns)*
{code:java}
{
 ""ver"": ""1"",
 ""txn"": {
  ""type"": ""2"", 
  ""protocolVersion"": 2, 
  ""data"": { 
    ""ver"": ""1"", 
    ""viewNo"": 0,
    ""ppSeqNo"": 1, 
    ""ledgerSize"": {
       0: 10,
       1: 20,
       2: 15,
       1001: 100, 
    },
    ""ledgerRoot"": {
         0: 2,
         1: ""domain_ledger_root_hash_1"", 
         2: 1,
    },
    ""stateRoot"": {
         1: ""domain_state_root_hash_1"" 
    }
   },
   ""metadata"": {
   },
 },
 ""txnMetadata"": {
   ""txnTime"": 100001,
   ""seqNo"": 10, 
 },
 ""reqSignature"": {
 }
}
 {code}
h3. *Batch2: POOL ledger (1 txn)*
{code:java}
{
  ""ver"": ""1"",
  ""txn"": {
    ""type"": ""2"",
    ""protocolVersion"": 2,
    ""data"": {
      ""ver"": ""1"",
      ""viewNo"": 0,
      ""ppSeqNo"": 2,
      ""ledgerSize"": {
        0: 11,
        1: 20,
        2: 15,
        1001: 100, 
      },
     ""ledgerRoot"":{ 
        0: ""pool_ledger_root_hash_2"",
        1: 1,
        2: 2
     },
     ""stateRoot"":{
       0: ""pool_state_root_hash_2"" 
     }   
    },
    ""metadata"": {
    },
   },
  ""txnMetadata"": {
     ""txnTime"": 100005,
     ""seqNo"": 11,
  },
  ""reqSignature"": {
  }
}{code}
 

 ;;;","22/Feb/19 10:18 PM;ashcherbakov;*Changes:*
 * Added Audit ledger as the first ledger for catchup
 * Audit ledger is updated for each 3PC batch (see format above)
 * Audit ledger txn root is included into PrePrepare and validated by others when processing PrePrepare
 * Audit Ledger Handler is implemented taking into account Pluggable Request Handlers approach
 * A couple of re-factorings and improvements have been done

*Covered by tests*
 * plenum/tests/audit_ledger folder

*PR*
 * [https://github.com/hyperledger/indy-plenum/pull/1091]
 * [https://github.com/hyperledger/indy-node/pull/1182]
 * https://github.com/hyperledger/indy-plenum/pull/1100

*Risk*
 * Medium

*Build*
 * 1.6.832

*Recommendation for QA:*
 * Test that audit ledger is updated during the load (audit ledger ID is 3)
 * Audit ledger as of now does not fix any known issues. But it should nor introduce any new issues not affect performance a lot.
 * So, the main task here is a regression testing
 ** Run acceptance load test (10 writes, 100 reads per sec)
 ** Test that catchup of all ledgers work correctly
 ** Test that view change works correctly
 ** Test that Audit ledger is shown properly in validator info;;;","23/Feb/19 12:03 AM;ashcherbakov;BTW the next Pool upgrade (to 1.7) can be forced to avoid some issues with audit ledger.;;;","25/Feb/19 8:19 PM;VladimirWork;We have a number instead of ""audit"" word in validator-info:
{noformat}
  Total pool Transactions:  7
  Total ledger Transactions:  18
  Total config Transactions:  0
  Total 3 Transactions:  0
{noformat}
;;;","25/Feb/19 10:12 PM;VladimirWork;Build Info:
indy-node 1.6.832

Steps to Reproduce:
1. Run 7 nodes pool.
2. Run slight load about 1 writing txn per second.
3. Stop the 7th node.
4. Stop the load.
5. Start the 7th node.
6. Check catch-up for all ledgers.

Actual Results:
Audit ledger doesn't catch up at 7th node (but domain ledger does) and it grows at 1-6 nodes. !INDY-1944.PNG|thumbnail!
Logs: ev@evernymr33:logs/1944_logs.tar.gz

Expected Results:
All ledgers should catch up successfully.;;;","26/Feb/19 12:10 AM;Toktar;""Audit"" in validator-info added in scope of the task INDY-2008;;;","26/Feb/19 12:31 AM;VladimirWork;Validator-info fixes will be tested in scope of INDY-2008. Catch-up will be tested in scope of INDY-1945 and INDY-1946.;;;","26/Feb/19 12:31 AM;ozheregelya;Load test results for indy-node 1.6.832:
Pool worked under production load *without plugins* during two days. 2394800 txns were written, all nodes are in sync, but two nodes failed with 'No space left'. So, load test result looks good enough.
Load *with plugins* is blocked by ST-511.;;;",,,,,,,,,,,,,,,
Implementation: Improve catch-up to use audit ledger for consistency,INDY-1945,36811,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,sergey.khoroshavin,sergey.khoroshavin,sergey.khoroshavin,16/Jan/19 4:07 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,1.7.1,,,,0,,,,"*Acceptance criteria*
* Catch-up should be improved roughly as described in https://github.com/hyperledger/indy-plenum/blob/master/design/catchup-and-audit-proposal.md#catch-up:
** LEDGER_STATUS is asked for Audit ledger only, returning its last seq_no (let’s call it audit_seq_no)
** after figuring out that catch-up is needed node sends REQUEST_CONSISTENCY_PROOF consisting of:
*** ledger_id
*** minimum/maximum audit_seq_no
** node that receives REQUEST_CONSISTENCY_PROOF:
*** given min/max audit_seq_no finds corresponding min/max seq_no inside ledger defined by ledger_id (thanks to Audit ledger it’s O(1) operation)
*** sends CONSISTENCY_PROOF message as usual
** then catch-up proceeds as usual
* It might make sense to try to be closer to current logic by sending CONSISTENCY_PROOFs in response to LEDGER_STATUS (thus not requiring explicit REQUEST_CONSISTENCY_PROOF message), however this will require sending multiple CONSISTENCY_PROOFs per one LEDGER_STATUS, which feels like a questionable design decision and can potentially lead to some unexpected edge cases. This requires some additional analysis as part of PoA
",,,,,,,,,,INDY-1944,,,,,,,INDY-1993,,,,,,,,,,,,INDY-1946,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:000005r",,,,Unset,Unset,Ev-Node 19.04,Ev-Node 19.05,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,,"06/Feb/19 11:18 PM;VladimirWork;(i) QA: we should perform INDY-1985 steps during production load test *with and without* payment txns at 10w / 100r load rate.;;;","25/Feb/19 10:25 PM;sergey.khoroshavin;*Plan of Attack*
 * [done] Implement catchup utils module
 ** CatchupDataProvider - catchup specific minimalistic interface to node
 ** Some of catchup message builders and validators
 * [done] Implement ClientSeederService and NodeSeederService
 ** Depends on CatchupDataProvider
 ** Doesn't have internal state other than injectected through data provider
 ** Handles LEDGER_STATUS and CATCHUP_REQ messages
 ** Makes it possible to move most of logic not depending on current catchup state out of LedgerManager
 * [done] Implement ConsProofService (and probably come up with a better name for it)
 ** Main purpose is to find out catchup till seq_no by gathering quorum of CONSISTENCY_PROOFS
 ** Depends on CatchupDataProvider
 ** Each instance is tied to specific ledger
 ** Has method start to start subprotocol and some form of callback when it comes up with a safe seq_no that can be used by CatchupRepService
 ** Handles LEDGER_STATUS and CONSISTENCY_PROOF messages
 * [done] Implement CatchupRepService
 ** Main purpose is to actually gather transactions from other nodes once catchup till seq_no is known from ConsProofService
 ** Depends on CatchupDataProvider
 ** Each instance is tied to specific ledger
 ** Has method start(seq_no, root_hash) to start subprotocol and some form of callback to notify NodeOneLedgerLeecherService that ledger catchup is finished
 ** Handles CATCHUP_REP messages
 * [done] Implement LedgerLeecherService
 ** Depends on CatchupDataProvider
 ** Each instance is tied to specific ledger
 ** Aggregates ConsProofService and CatchupRepService
 ** Has method start with optional parameter last_txn
 *** If last_txn is provided then it will start CatchupRepService
 *** If last_txn is not provided then it will first try to get it using ConsProofService
 ** Has some form of callback to notify NodeLeecherService that ledger catchup is finished
 ** Makes it possible to move individual ledger catchup logic out of LedgerManager
 * [done] Implement NodeLeecherService
 ** Depends on CatchupDataProvider
 ** Aggregates multiple LedgerLeecherServices
 ** Has explicit state
 *** CatchingUpPoolLedger
 *** CatchingUpAuditLedger
 *** CatchingUpNormalLedger
 *** Done
 ** Has potential to catch up ledgers in parallel in CatchingUpNormalLedger state
 ** Has method start which enters CatchingUpAuditLedger and starts catchup of audit ledger
 ** When audit ledger is caught up catch up other ledgers using received data
 ** If audit ledger catch up fails under some timeout catch up other ledgers the old way
 * [optional] Implement CatchupService
 ** Aggregates ClientSeederService, NodeSeederService and NodeLeecherService
 ** Just a simple wrapper around multiple catchup-related services
 ** Replaces most of functionality of LedgerManager

*Integration and testing strategy* 
 After implementing each of new services (with corresponding unit tests) delete their functionality from LedgerManager and fix all failing integration tests.
 Pros:
 * code gets integrated to main codebase ASAP
 * existing integration tests can catch problems that may slip through unit tests
 * integrating code early can help detect design flaws and other problems early
 * gradually reducing amount of code in LedgerManager will make it more manageable, which in turn can help;;;","15/Mar/19 6:39 PM;sergey.khoroshavin;*Problem reason*
Old catchup logic could lead to failures when there were writes to several ledgers during catchup. This was because size to catch up was queried independently and at different times  for each ledger.

*Changes*
New catchup logic is implemented. It catches up audit ledger using old logic, but sizes to catch up to for other ledgers are determined from audit ledger, so in effect size to catch up for all ledgers is determined atomically, which solves problem. Furthermore, in many cases catch up should happen significantly faster since it needs to request and gather ledger statuses and consistency proofs only for audit ledger (previously it did this for all ledgers). Also in process old code was refactored quite heavily.

*PRs:*
https://github.com/hyperledger/indy-plenum/pull/1098
https://github.com/hyperledger/indy-plenum/pull/1101
https://github.com/hyperledger/indy-plenum/pull/1104
https://github.com/hyperledger/indy-plenum/pull/1108
https://github.com/hyperledger/indy-plenum/pull/1109
https://github.com/hyperledger/indy-plenum/pull/1111
https://github.com/hyperledger/indy-plenum/pull/1113
https://github.com/hyperledger/indy-plenum/pull/1114
https://github.com/hyperledger/indy-plenum/pull/1115
https://github.com/hyperledger/indy-plenum/pull/1117
https://github.com/hyperledger/indy-plenum/pull/1118
https://github.com/hyperledger/indy-plenum/pull/1120
https://github.com/hyperledger/indy-plenum/pull/1123

*Version:*
indy-plenum: 1.6.726-master
indy-node: 1.6.861-master

*Risk:*
Medium

*Covered with tests:*
https://github.com/hyperledger/indy-plenum/blob/1.6.726-master/plenum/test/node_catchup_with_3pc/test_slow_catchup_while_ordering.py
Also lots of existing integration tests were applicable, since majority of work was just a refactoring

*Recommendations for QA*
This is going to be tested in scope of INDY-1993;;;",,,,,,,,,,,,,,,,,,,,,,
Implementation: Restore current 3PC state from audit ledger,INDY-1946,36812,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,sergey.khoroshavin,sergey.khoroshavin,16/Jan/19 4:09 PM,10/Apr/19 6:05 PM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.7.1,,,,0,,,,"*Acceptance criteria*
* viewNo and ppSeqNo should be included into transactions in audit ledger
* current viewNo and ppSeqNo should be restored from audit ledger

*Additional notes*
* This is based on design described in https://github.com/hyperledger/indy-plenum/blob/master/design/catchup-and-audit-proposal.md
* It might be safer to implement this after INDY-1945, but probably they can be done in parallel",,,,,,,,,,INDY-1944,,,,,,,INDY-1720,INDY-2025,,,,,,,,,,,,,,,,INDY-1945,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:00007xxw",,,,Unset,Unset,Ev-Node 19.04,Ev-Node 19.05,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,sergey.khoroshavin,,,,,,,,,,"19/Feb/19 2:31 AM;Derashe;*PoA:*

We need to define how would we save and load view number for node and seq_no for primary replica from audit ledger.

*Save*

Saving will be performed by request ordering. During apply process, called from master replica, we will pass view number and seq_no and save txn to audit ledger.

*Load*

For now, we have a mechanism of restoring view_number, using CurrentState message, when new node connects to pool. We can replace it with restoring view_number from last audit txn as we finish catchup. Restoring must be performed as if view_change happened.

But CurrentState message also contains logic for restoring correct primaries, so we must take into account PoA of https://jira.hyperledger.org/browse/INDY-1720. (option #3);;;","22/Feb/19 6:33 PM;Derashe;For correct primary setting and avoid problems, mentioned in 1720, we'll organize primary selection as follows:
 * select_primaries - function, that recalculate primaries, basing on our current pool state. If node have some existing primaries set, this function will not touch it. It will adjust primaries that needed.
 ** called when pool started, to set initial primaries
 ** called when node promotion/demotion lead to increacing/reducing replicas count
 ** called when view_change happened
 * apply_primaries_from_audit - function that applies primaries from last audit ledger txn to current state
 ** called at the end of catchup, if this catchup wasn't part of cases described in the paragraph above

This solves problem INDY-1720, because recalculation of primaries can be done only at in-sync nodes.

 *Note.* In select primaries method, existing state case will work only in one case. This case is node demotion/promotion. Other cases intend that replicas restored and their primaries flushed before start of selection.;;;","22/Feb/19 8:12 PM;Derashe;For purpose of reducing ledger size, audit ledger will store primaries list or seq_no DELTA between current txn and last txn with primaries list.;;;","22/Feb/19 8:24 PM;Derashe;*Tests.* As for now, we are planning to implement folowing tests:
 * Tests for correct functionality of storing primaries field
 * Tests for nodes connectivity
 ** pool start (and correct state after start)
 ** catchup after node ""turned off"" and activated in the same view (with/without txns made during this node abscence)
 ** catchup after node ""turned off"" and activated with on/few view_changes happened (with/without txns made during this node abscence)
 ** ""turned off"" - expands to three cases - shotdown, network disconnection from pool,demotion
 * Tests for correct view setting (probably only change behaviour in existing view setting tests);;;","26/Feb/19 4:44 PM;ashcherbakov;*Low-level PoA*

1) Add `on_catchup_done` method into PrimarySelector - DONE
 2) instead of calling `select_primaries` at the end of `allLedgersCaughtup`, do the following - DONE:
  - if audit ledger is empty, then 
    - `select_primaries`
    - viewNo=0
    - lastOrdered=(0, 0)
  - else
    - `select_primaries_after_catchup`
    - set viewNo from the audit ledger
    - set lastOrdered from the audit ledger
 3) remove propagate_primary logic from ViewChanger and other places - DONE
 4) call propagate primary logic (like setting watermarks etc.) at the end of the first catch-up - DONE
 5) add a 3d condition to send a 3PC batch into `create_3pc_batch` in Replica (in addition to common ordering and freshness), so that a 3PC batch with new primaries will be sent once primaries are changed - DONE
 6) add information about primaries into audit ledger - DONE
 7) do `select_primaries` after any changes of the number of replicas (not only adding new replicas, but also removing) - DONE
 8) remove the code related to recovering of  last ordered during catch-up (see `three_phase_key_for_txn_seq_no`) since it will be recovered from the audit ledger instead. - DONE

9) If a Pool txn is applied (in PrePrepare creation or processing) which leads to changes of the number of instances (and hence re-selection), then preliminary calculate the new primaries for each replica and add them to the corresponding audit txn - DONE
 10) Use primaries from the corresponding audit txn when creating Ordered msg - DONE
 11) do view change once the number of replicas has changed - DONE

 ;;;","26/Feb/19 4:58 PM;ashcherbakov;*Tests*

0) Make sure existing tests pass - IN PROGRESS, will be continued in INDY-2025
 1) update unit tests for audit ledger handler taking into account the new field - DONE
 3) integration test: correct primaries, viewNo and ppSeqNo after catchup - DONE
 4) integration test: correct primaries, viewNo and ppSeqNo after changing of number of replicas - DONE
 5) integration test: correct primaries, viewNo and ppSeqNo after view change - DONE
 6) integration test: start catchup of a node when there were no ordering, but a couple of freshness updates (check that catchup is successful, primaries, viewNo and ppSeqNo restored properly, and the node can participate in consensus) - DONE
 7) integration test: check that correct primaries are put into audit when applying stashed batches during catchup - DONE
 Case1:
 - Apply a PP1 for 3PC with pool txn changin F (adding 7th node) - do not order it yet!
 - Apply a PP2 for 3PC with a domain txn - do not order it yet!
 - Create ordered msg for PP1 - do not execute (commit) by Node yet!
 - Create ordered msg for PP2 - do not execute (commit) by Node yet!
 - Start catchup (so that uncommitted state reverted)
 - make sure that forceOrdered is called and both Ordered msgs are processed, that is they are -reapplied and committed
 - make sure that the state and ledger is equal and correct on all nodes

Case1:
 - Apply a PP1 for 3PC with pool txn changin F (adding 7th node) - do not order it yet!
 - Apply a PP2 for 3PC with a domain txn - do not order it yet!
 - Order and execute (commit) for PP1
 - Create ordered msg for PP2 - do not execute (commit) by Node yet!
 - Start catchup (so that uncommitted state reverted)
 - make sure that forceOrdered is called and PP2's Ordered, that is it is re-applied and committed
 - make sure that the state and ledger is equal and correct on all nodes

 

8) unit tests for primary selection in PrePrepare - DONE

 ;;;","15/Mar/19 10:02 PM;ashcherbakov;PR: [https://github.com/hyperledger/indy-plenum/pull/1096]

The PR contains implementation and most of the test fixes.

There are still some failures of tests (~10-15). Fixing of the tests and validation will be continued in https://github.com/hyperledger/indy-plenum/pull/1096;;;",,,,,,,,,,,,,,,,,,
Move logic from ActionReqHandler,INDY-1947,36815,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,Derashe,Derashe,16/Jan/19 5:31 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.9.0,,,,0,,,,"Move logic from ActionReqHandler(both in node and plenum) to separate implementations of QueryHandler, TransactionHandler, BatchHandler and ActionHandler (according to step 2 of solution 2 by the link: [https://docs.google.com/document/d/1V4UYvXxPsPQLZmfZatWs1wDehTlyoNfyl9JAGdDmTE8/edit#])",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1852,,,No,,Unset,No,,,"1|hzwvif:00007u",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,,,,,,,,,,,,"16/Jan/19 6:21 PM;Derashe;PR: https://github.com/hyperledger/indy-node/pull/1132;;;",,,,,,,,,,,,,,,,,,,,,,,,
Audit Ledger related tasks,INDY-1948,36816,,Epic,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,sergey.khoroshavin,sergey.khoroshavin,16/Jan/19 6:20 PM,09/Oct/19 12:26 AM,28/Oct/23 2:48 AM,27/May/19 5:30 PM,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,ghx-label-13,,Audit Ledger,Done,No,,Unset,No,,,"1|i0071z:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),nanspro,sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node stops working without any services failure,INDY-1949,36825,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,Derashe,ozheregelya,ozheregelya,16/Jan/19 11:50 PM,30/Mar/19 5:32 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,TShirt_L,,,"*Environment:*
 indy-node 1.6.753, 1.6.759 (w/o freshness)

*Steps to Reproduce:*
 1. Setup the pool.
 2. In case of indy-node 1.6.759 set UPDATE_STATE_FRESHNESS = False in config.
 3. Restart pool.
 4. Start production load.

*Actual Results:*
 * Two nodes were lagged on ~3K txns in domain ledger.
 * On both of lagged nodes there are no logs and metrics and validator-info updates after lagging:
{code:java}
ubuntu@tokyoQALive18:~$ date && tail /var/log/indy/sandbox/Node18.log
Wed Jan 16 14:39:39 UTC 2019
2019-01-16 13:16:04,968|INFO|replica.py|Node18:3 processing checkpoint CHECKPOINT{'digest': 'd7d7366160e4cafa60a245ec0355cb187a8a07a2539749467592a330284bfa8a', 'instId': 3, 'viewNo': 1, 'seqNoEnd': 1100, 'seqNoStart': 1001} from Node19
2019-01-16 13:16:04,968|INFO|replica.py|Node18:3 processing checkpoint CHECKPOINT{'digest': 'd7d7366160e4cafa60a245ec0355cb187a8a07a2539749467592a330284bfa8a', 'instId': 3, 'viewNo': 1, 'seqNoEnd': 1100, 'seqNoStart': 1001} from Node24
2019-01-16 13:16:04,969|INFO|replica.py|Node18:3 processing checkpoint CHECKPOINT{'digest': 'd7d7366160e4cafa60a245ec0355cb187a8a07a2539749467592a330284bfa8a', 'instId': 3, 'viewNo': 1, 'seqNoEnd': 1100, 'seqNoStart': 1001} from Node4
2019-01-16 13:16:04,969|INFO|replica.py|Node18:3 processing checkpoint CHECKPOINT{'digest': 'd7d7366160e4cafa60a245ec0355cb187a8a07a2539749467592a330284bfa8a', 'instId': 3, 'viewNo': 1, 'seqNoEnd': 1100, 'seqNoStart': 1001} from Node21
2019-01-16 13:16:04,969|INFO|replica.py|Node18:3 processing checkpoint CHECKPOINT{'digest': 'd7d7366160e4cafa60a245ec0355cb187a8a07a2539749467592a330284bfa8a', 'instId': 3, 'viewNo': 1, 'seqNoEnd': 1100, 'seqNoStart': 1001} from Node7
2019-01-16 13:16:04,969|INFO|replica.py|Node18:3 set watermarks as 1100 1400
2019-01-16 13:16:04,969|INFO|replica.py|Node18:3 removing stashed checkpoints: viewNo=1, seqNoStart=1001, seqNoEnd=1100
2019-01-16 13:16:04,969|INFO|replica.py|Node18:3 cleaning up till (1, 1100)
2019-01-16 13:16:04,979|INFO|replica.py|Node18:3 marked stable checkpoint (1001, 1100)
2019-01-16 13:16:04,979|INFO|replica.py|Node18:3 processed 16 stashed checkpoints for (1001, 1100), 16 of them were stashed again{code}
 

 * Uptime of indy-node services according to systemctl status is the same as on another nodes.
 * On not lagged nodes following warnings constantly appear:
{code:java}
2019-01-16 14:09:19,243|WARNING|zstack.py|Node13 could not transmit message to Node3
2019-01-16 14:09:19,246|WARNING|zstack.py|Node13 could not transmit message to Node18
2019-01-16 14:09:19,314|WARNING|zstack.py|Node13 could not transmit message to Node3
2019-01-16 14:09:19,317|WARNING|zstack.py|Node13 could not transmit message to Node18{code}

 * The last message from lagged node in alive node logs:
{code:java}
2019-01-16 13:16:04,025|INFO|replica.py|Node13:2 processing checkpoint CHECKPOINT{'digest': '9464514b3f4125249233b7c19418a19f1223195e669fc1c6305089cdddbcdcc4', 'seqNoEnd': 1100, 'viewNo': 1, 'instId': 2, 'seqNoStart': 1001} from Node18{code}

 * There are no any errors or warnings in journalctl.

*Expected Results:*
 * Node should not lag.
 * Even if node lags, it should continue writing logs and metrics.

*Logs and metrics:* 
1) s3://qanodelogs/indy-1876/prod_load_14_01_2019 (Node13 is lagged here)
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1876/prod_load_14_01_2019/ /home/ev/logs/indy-1876/prod_load_14_01_2019/
 2) s3://qanodelogs/indy-1949/prod_load_17_01_2019 
At the end of this test following nodes were lagged:
Node3 3717
Node13 63086
Node17 73303
Node2 56375
Node25 28932
Node18 3414
Node20 69748",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1876,,,,,INDY-1959,INDY-1965,,,,,,,,,,,,,,"17/Jan/19 1:11 AM;ozheregelya;111.png;https://jira.hyperledger.org/secure/attachment/16579/111.png","24/Jan/19 12:23 AM;Derashe;2_node.PNG;https://jira.hyperledger.org/secure/attachment/16612/2_node.PNG","24/Jan/19 1:27 AM;Derashe;image-2019-01-23-19-31-40-410.png;https://jira.hyperledger.org/secure/attachment/16613/image-2019-01-23-19-31-40-410.png","24/Jan/19 1:30 AM;Derashe;image-2019-01-23-19-34-22-909.png;https://jira.hyperledger.org/secure/attachment/16614/image-2019-01-23-19-34-22-909.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0000001",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,ozheregelya,sergey.khoroshavin,,,,,,,,,"17/Jan/19 1:12 AM;ozheregelya;Process is still alive:
!111.png|thumbnail!;;;","17/Jan/19 7:14 PM;sergey.khoroshavin;Looked at Node3 and found very similar logs in the end:
{code}
2019-01-16 13:16:52,297|INFO|replica.py|Node3:7 set last ordered as (1, 1200)
2019-01-16 13:16:52,297|INFO|replica.py|Node3:7 ordered batch request, view no 1, ppSeqNo 1200, ledger 1, state root None, txn root None, requests ordered 5, discarded 0
2019-01-16 13:16:52,297|INFO|replica.py|Node3:7 sending Checkpoint (1101, 1200) view 1 checkpointState digest 687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301. Ledger 1 txn root hash None. Committed state root hash None Uncommitted state root hash None
2019-01-16 13:16:52,297|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node14
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node13
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node21
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node23
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node20
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node19
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node1
2019-01-16 13:16:52,298|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node15
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node12
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node25
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node4
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node10
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node7
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node17
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node2
2019-01-16 13:16:52,299|INFO|replica.py|Node3:7 processing checkpoint CHECKPOINT{'digest': '687892c94e23725c077ee96072824a2a3d18eb28edd740fe1147db26fc6d2301', 'seqNoEnd': 1200, 'seqNoStart': 1101, 'instId': 7, 'viewNo': 1} from Node6
2019-01-16 13:16:52,300|INFO|replica.py|Node3:7 set watermarks as 1200 1500
2019-01-16 13:16:52,300|INFO|replica.py|Node3:7 removing stashed checkpoints: viewNo=1, seqNoStart=1101, seqNoEnd=1200
2019-01-16 13:16:52,300|INFO|replica.py|Node3:7 cleaning up till (1, 1200)
2019-01-16 13:16:52,311|INFO|replica.py|Node3:7 marked stable checkpoint (1101, 1200)
2019-01-16 13:16:52,312|INFO|replica.py|Node3:7 processed 16 stashed checkpoints for (1101, 1200), 16 of them were stashed again
{code}

Attaching to hung process with gdb showed that there is an activity in main thread, and when stopping at random times stack trace were very similar.
One stack trace example:
{code}
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 68, in handleSync
    if isinstance(msg, tuple) and len(
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 94, in handleSync
    super().handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2343, in dequeue_commits
    self.threePhaseRouter.handleSync((commit, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1572, in addToPrepares
    self.dequeue_commits(prepare.viewNo, prepare.ppSeqNo)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1173, in processPrepare
    self.addToPrepares(prepare, sender)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 94, in handleSync
    super().handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2316, in dequeue_prepares
    self.threePhaseRouter.handleSync((prepare, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1469, in addToPrePrepares
    self.dequeue_prepares(*key)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1036, in _process_valid_preprepare
    self.addToPrePrepares(pre_prepare)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1078, in processPrePrepare
    self._process_valid_preprepare(pre_prepare, sender)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 94, in handleSync
    super().handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1021, in process_three_phase_msg
    self.threePhaseRouter.handleSync((msg, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 114, in handleAllSync
    self.handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 995, in serviceQueues
    r += self.inBoxRouter.handleAllSync(self.inBox, limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 97, in <genexpr>
    sum(replica.serviceQueues(limit) for replica in self._replicas.values())
  <built-in method sum of module object at remote 0x7ffbb83145e8>
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 97, in service_inboxes
    sum(replica.serviceQueues(limit) for replica in self._replicas.values())
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1389, in _process_replica_messages
    inbox_processed = self.replicas.service_inboxes(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1386, in serviceReplicas
    return self._process_replica_messages(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 367, in wrapper
    return await f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1356, in prod
    c += await self.serviceReplicas(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 367, in wrapper
    return await f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 313, in prod
    c = await super().prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
    s += await n.prod(limit)
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
    msgsProcessed = await self.prodAllOnce()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
    await self.runOnceNicely()
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
    result = coro.send(None)
  File ""/usr/lib/python3.5/asyncio/tasks.py"", line 307, in _wakeup
    self._step()
  File ""/usr/lib/python3.5/asyncio/events.py"", line 125, in _run
    self._callback(*self._args)
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1312, in _run_once
    handle._run()
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 345, in run_forever
    self._run_once()
  File ""/usr/lib/python3.5/asyncio/base_events.py"", line 375, in run_until_complete
    self.run_forever()
  File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
    return self.loop.run_until_complete(what)
  File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
    looper.run()
  File ""/usr/local/bin/start_indy_node"", line 19, in <module>
    client_ip=sys.argv[4], client_port=int(sys.argv[5]))
{code}

Another (a bit trimmed) example:
{code}
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2324, in enqueue_commit
    ""Request {} from {}"".format(request, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1650, in validateCommit
    self.enqueue_commit(commit, sender)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1197, in processCommit
    if self.validateCommit(commit, sender):
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 94, in handleSync
    super().handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2343, in dequeue_commits
    self.threePhaseRouter.handleSync((commit, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1572, in addToPrepares
    self.dequeue_commits(prepare.viewNo, prepare.ppSeqNo)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1173, in processPrepare
    self.addToPrepares(prepare, sender)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 94, in handleSync
    super().handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 2316, in dequeue_prepares
    self.threePhaseRouter.handleSync((prepare, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1469, in addToPrePrepares
    self.dequeue_prepares(*key)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1036, in _process_valid_preprepare
    self.addToPrePrepares(pre_prepare)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1078, in processPrePrepare
    self._process_valid_preprepare(pre_prepare, sender)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 94, in handleSync
    super().handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 1021, in process_three_phase_msg
    self.threePhaseRouter.handleSync((msg, sender))
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 70, in handleSync
    return self.getFunc(msg[0])(*msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 114, in handleAllSync
    self.handleSync(msg)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 995, in serviceQueues
    r += self.inBoxRouter.handleAllSync(self.inBox, limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
    return f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 97, in <genexpr>
    sum(replica.serviceQueues(limit) for replica in self._replicas.values())
  <built-in method sum of module object at remote 0x7ffbb83145e8>
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 97, in service_inboxes
    sum(replica.serviceQueues(limit) for replica in self._replicas.values())
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1389, in _process_replica_messages
    inbox_processed = self.replicas.service_inboxes(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1386, in serviceReplicas
    return self._process_replica_messages(limit)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 367, in wrapper
    return await f(self, *args, **kwargs)
  File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1356, in prod
{code};;;","17/Jan/19 7:17 PM;sergey.khoroshavin;*Preliminary hypothesis:* during processing of PREPREPARE some PREPAREs and COMMITs are getting processed, but then stashed again in an infinite loop.;;;","19/Jan/19 9:53 PM;ozheregelya;*Environment:*
 indy-node 1.6.761

*Steps to Reproduce:*
 1. Set up the pool.
 2. Run production load.

*Actual Results:*
 *Case 1:*
 Pool stopped writing after ~85K txns in domain and ~80K in sovtoken ledgers. 
 Hanged nodes were not noticed, but Node23 was stopped earlier than the others with following errors in journalctl:
{code:java}
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]: --- Logging error ---
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]: Traceback (most recent call last):
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/logging/handlers.py"", line 72, in emit
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self.doRollover()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/common/logging/CompressingFileHandler.py"", line 40, in doRollover
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self.compressor.start()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/multiprocessing/process.py"", line 105, in start
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self._popen = self._Popen(self)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/multiprocessing/context.py"", line 212, in _Popen
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return _default_context.get_context().Process._Popen(process_obj)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/multiprocessing/context.py"", line 267, in _Popen
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return Popen(process_obj)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/multiprocessing/popen_fork.py"", line 20, in __init__
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self._launch(process_obj)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/multiprocessing/popen_fork.py"", line 67, in _launch
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self.pid = os.fork()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]: OSError: [Errno 12] Cannot allocate memory
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]: Call stack:
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     looper.run()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return self.loop.run_until_complete(what)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 375, in run_until_complete
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self.run_forever()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 345, in run_forever
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self._run_once()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 1312, in _run_once
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     handle._run()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/events.py"", line 125, in _run
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self._callback(*self._args)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 307, in _wakeup
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self._step()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     result = coro.send(None)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     await self.runOnceNicely()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     msgsProcessed = await self.prodAllOnce()
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     s += await n.prod(limit)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 313, in prod
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     c = await super().prod(limit)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 367, in wrapper
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return await f(self, *args, **kwargs)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1356, in prod
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     c += await self.serviceReplicas(limit)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 367, in wrapper
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return await f(self, *args, **kwargs)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1386, in serviceReplicas
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return self._process_replica_messages(limit)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1389, in _process_replica_messages
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     inbox_processed = self.replicas.service_inboxes(limit)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 97, in service_inboxes
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     sum(replica.serviceQueues(limit) for replica in self._replicas.values())
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replicas.py"", line 97, in <genexpr>
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     sum(replica.serviceQueues(limit) for replica in self._replicas.values())
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 210, in wrapper
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return f(self, *args, **kwargs)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 995, in serviceQueues
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     r += self.inBoxRouter.handleAllSync(self.inBox, limit)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 114, in handleAllSync
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     self.handleSync(msg)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/router.py"", line 72, in handleSync
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     return self.getFunc(msg)(msg)
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/replica.py"", line 982, in readyFor3PC
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]:     'until a primary is chosen'.format(self))
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]: Message: 'Node23:1 is getting requests but still does not have a primary so the replica will not process the request until a primary is chosen'
Jan 19 09:36:37 virginaQALive23.qatest.evernym.com env[15739]: Arguments: ()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]: Traceback (most recent call last):
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/bin/start_indy_node"", line 19, in <module>
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     client_ip=sys.argv[4], client_port=int(sys.argv[5]))
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/utils/node_runner.py"", line 54, in run_node
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     looper.run()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 263, in run
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     return self.loop.run_until_complete(what)
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/base_events.py"", line 387, in run_until_complete
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     return future.result()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/futures.py"", line 274, in result
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     raise self._exception
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/lib/python3.5/asyncio/tasks.py"", line 239, in _step
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     result = coro.send(None)
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 227, in runForever
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     await self.runOnceNicely()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 210, in runOnceNicely
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     msgsProcessed = await self.prodAllOnce()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/stp_core/loop/looper.py"", line 152, in prodAllOnce
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     s += await n.prod(limit)
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/indy_node/server/node.py"", line 313, in prod
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     c = await super().prod(limit)
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/common/metrics_collector.py"", line 367, in wrapper
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     return await f(self, *args, **kwargs)
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 1360, in prod
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     c += self._serviceActions()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 100, in _serviceActions
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     action()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/has_action_queue.py"", line 110, in wrapper
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     action()
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:   File ""/usr/local/lib/python3.5/dist-packages/plenum/server/node.py"", line 2851, in flush_metrics
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]:     self.metrics.add_event(MetricsName.GC_TRACKED_OBJECTS, len(gc.get_objects()))
Jan 19 09:36:38 virginaQALive23.qatest.evernym.com env[15739]: MemoryError
{code}
*Logs and metrics for Case 1:* s3://qanodelogs/indy-1949/not_completed_VC_19_01_2019
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1949/not_completed_VC_19_01_2019/ /home/ev/logs/qanodelogs/indy-1949/not_completed_VC_19_01_2019/

 

*Case 2:*
 When the test was stopped, pool was in View Change. 8 of 25 nodes were lagged:
persistent_node17 80154
 persistent_node23 70777
 persistent_node21 95689
 persistent_node11 119120
 persistent_node14 328697 (no space left)
 persistent_node10 557561 (no space left)
 persistent_node2 582729 (no space left)
 persistent_node13 618651 (no space left)

One node have 2 txns more in comparison with the main part of nodes:
 persistent_node4 778778

The rest nodes have 778776 txns.

*Logs and metrics for Case 2:* s3://qanodelogs/indy-1949/successful_20_01_2019
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1949/successful_20_01_2019/ /home/ev/logs/qanodelogs/indy-1949/successful_20_01_2019/;;;","23/Jan/19 1:16 AM;ozheregelya;*Case 3* (with default config):

No View Changes, but 8 nodes were lagged at the moment when the test was stopped:
{code:java}
persistent_node1 421566
persistent_node2 421566
persistent_node3 269964
persistent_node4 412970
persistent_node5 421566
persistent_node6 421566
persistent_node7 421566
persistent_node8 421566
persistent_node9 75345
persistent_node10 421566
persistent_node11 421566
persistent_node12 421566
persistent_node13 328253
persistent_node14 280839
persistent_node15 309501 
persistent_node16 421566
persistent_node17 164241
persistent_node18 421566
persistent_node19 421566
persistent_node20 421566
persistent_node21 421566
persistent_node22 193836
persistent_node23 421566
persistent_node24 421566
persistent_node25 421566{code}
Several hours later part of the nodes completed catch up:
{code:java}
persistent_node1 421566
persistent_node2 421566
persistent_node3 269964
persistent_node4 421566
persistent_node5 421566
persistent_node6 421566
persistent_node7 421566
persistent_node8 421566
persistent_node9 75345
persistent_node10 421566
persistent_node11 421566
persistent_node12 421566
persistent_node13 421566
persistent_node14 280839
persistent_node15 421566
persistent_node16 421566
persistent_node17 164241
persistent_node18 421566
persistent_node19 421566
persistent_node20 421566
persistent_node21 421566
persistent_node22 193836
persistent_node23 421566
persistent_node24 421566
persistent_node25 421566{code}
 *Logs and metrics:* s3://qanodelogs/indy-1949/default_config_22_01_2019
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1949/default_config_22_01_2019/ /home/ev/logs/qanodelogs/indy-1949/default_config_22_01_2019/;;;","23/Jan/19 10:25 PM;ashcherbakov;h2. Initial problem (case 1)

*Problem reason:*
 * There can be prepares and commits without pre-prepares
 * Once corresponding pre-prepares are received, prepares and commits are processed
 * Processing of commits can lead to checkpoint stabilization
 * During checkpoint stabilization we call GC and remove this PrePrepare
 * The code was written in a way, that after removing this PrePrepare the Commit is stashed again since its doesn't have a PrePrepare.

*Fix:*
 * The code processing stashed prepare/commits because of missing PrePrepares was calling process methods directly, that is our in-one-place validation (ReplicaValidator) wasn't called.
 * This is fixed so that we always call validation first, so that Commits are not stashed again, but discarded as already ordered.

*PR:*
 * [https://github.com/hyperledger/indy-plenum/pull/1057]**
 * **

*Version:*
 * indy-node 1.6.761;;;","24/Jan/19 1:30 AM;Derashe;h2. Case 2:
h3. Research result:
 * Node2 had a connection problem form a very beginning !2_node.PNG|thumbnail! . Noticeable things there:

 ** node's request queue more that finalized request queue. That means, that this node did not get enought propagates.
 ** avg_node_stack_messages_processed is on it's maximum - 100 requests. 
 ** looper run time values higher that on other nodes
 * These problems led Node2 to slowed ordering.
 * Node2 was primary for 1 instance, since view_no was 0, and so, ordering on 1 instance for all nodes was slowed. 
 * That caused requests to delay on every node. And this called OOM exception on some nodes. !image-2019-01-23-19-31-40-410.png|thumbnail!
 * At some time few view_changes happened, so lagged node were not primary anymore.
 * The nodes, that did not yet go down, continued ordering successfully !image-2019-01-23-19-34-22-909.png|thumbnail!;;;","28/Jan/19 4:58 PM;ashcherbakov;Since the initial issue if fixed, this ticket can be closed.

The issues found in cases 2 and 3 look like the new ones. They will be addressed in the scope of INDY-1965.;;;",,,,,,,,,,,,,,,,,
Rename TRUST_ANCHOR to ENDORSER,INDY-1950,36826,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,esplinr,esplinr,16/Jan/19 11:52 PM,20/Jun/19 10:16 PM,28/Oct/23 2:48 AM,17/Jun/19 3:55 PM,,1.9.0,,,,0,,,,"The term TRUST_ANCHOR is no longer being used to refer to permission to write to the ledger. Instead it is used to refer to the organization sponsoring a governance framework around credentials that are trusted by a specific industry. (See the Sovrin Foundation glossary for more details.)

The role required to write to the ledger will now be called ENDORSER.

*Acceptance Criteria*
* TRUST_ANCHOR role is called ENDORSER

Not included:
* Changes to the ENDORSER role to allow write delegation to transaction authors. The expectation is that all transaction authors must have the endorser role in order to write.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609r",,,,Unset,Unset,Ev-Node 19.12,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,esplinr,,,,,,,,,,,"14/Jun/19 10:21 PM;Derashe;Changes:
 - TRUST_ANCHOR renamed to ENDORSER both in docs and code

PR:
 - [https://github.com/hyperledger/indy-node/pull/1346]

 

Sovrin glossary: [https://docs.google.com/document/d/1gfIz5TT0cNp2kxGMLFXr19x1uoZsruUe_0glHst2fZ8/];;;",,,,,,,,,,,,,,,,,,,,,,,,
Explore strange long VC appeared during sytem tests running,INDY-1951,36853,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,17/Jan/19 4:50 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,Explore strange long VC appeared during sytem tests running.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00000303",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,"22/Jan/19 6:04 PM;VladimirWork;The issue was caused by 60 seconds timeout for primary disconnection (so VC will trigger only after this interval), so tests are refactored according to this changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Pool config txn brokes the pool,INDY-1952,36867,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Invalid,,VladimirWork,VladimirWork,17/Jan/19 10:28 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Build Info:
indy-node 1.6.760

Steps to Reproduce:
0. Send nym txn to domain ledger.
1. Send node txn to pool ledger.
2. Send pool config txn to config ledger with {""force"":false,""writes"":true} parameters (that doesn't change pool working capacity).
3. Send another nym txn to domain ledger.
4. Repeat steps 0-3 two times.

Actual Results:
First 0-3 steps run works, but we get PoolLedgerTimeout at step 2 during second 0-3 steps run and PoolLedgerTimeout at step 0 during third 0-3 steps run.

Expected Results:
This sequence should work any number of times and should not break the pool.

Client's and nodes' logs:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i007cv:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,"18/Jan/19 3:44 PM;VladimirWork;It's ok since during second test run we have 6 nodes in the pool (f=1) and 2 of them are unavailable.;;;",,,,,,,,,,,,,,,,,,,,,,,,
Incorrect pool upgrade txn validation,INDY-1953,36868,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,17/Jan/19 10:39 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,TShirt_M,,,"Build Info:
indy-node 1.6.760

Steps to Reproduce:
1. Send pool upgrade txn with time span with 1 day / 1 month / 1 year (but with the same HH:MM:SS time).

Actual Results:
{'reqId': 1547728351112051493, 'reason': 'client request invalid: InvalidClientRequest(""{\'Gw6pDLhcBcoQesN72qfotTgFa7cbuqZpkX3Xo6pLhPhv\': \'2021-10-12T12:32:31+0000\', \'4PS3EDQ3dW1tci1Bp6543CfuuebjFrg36kLAUcskGfaA\': \'2021-10-15T12:32:31+0000\', \'8ECVSk179mjsjKRLWiQtssMLgp6EPhWXtaYyStWPSGAb\': \'2021-10-13T12:32:31+0000\', \'DKVxG2fXXTU8yT5N7hGEbXB3dfdAnYv1JczDUHpmDxya\': \'2021-10-14T12:32:31+0000\'} {color:red}not a valid schedule since time span between upgrades is 0 seconds which is less than specified in the config{color}"",)', 'op': 'REQNACK', 'identifier': 'V4SGRU86Z58d6TV7PBUe6f'}

Expected Results:
Pool upgrade command should be validated correctly.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000000c",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,Derashe,VladimirWork,,,,,,,,,,"18/Jan/19 9:35 PM;anikitinDSR;Bug in line:
https://github.com/hyperledger/indy-node/blob/f394697cc8b728a98856d998f5db8ef71f3d6492/indy_node/server/upgrader.py#L451;;;","21/Jan/19 6:17 PM;Derashe;PR: [https://github.com/hyperledger/indy-node/pull/1135]

Rec for QA: retest case from description

Node ver: 764;;;","22/Jan/19 5:29 PM;VladimirWork;Build Info:
indy-node 1.6.764 -> 1.6.765

Steps to Validte:
1. Schedule upgrade commands with valid time span (1 hour, 1 day, 1 week, 1 month) with the same minutes and seconds.
2. Check that common time span upgrade also schedules and triggers and there are no regressions.

Actual Results:
Upgrade command validation works as expected.;;;",,,,,,,,,,,,,,,,,,,,,,
"As a user, I need to be able to know what was the last update time of the ledger when querying a txn via GET_TXN request",INDY-1954,36869,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,VladimirWork,VladimirWork,17/Jan/19 11:01 PM,28/Aug/19 10:27 PM,28/Oct/23 2:48 AM,28/Aug/19 10:27 PM,,1.9.2,,,,0,,,,"Build Info:
 indy-node 1.6.760

Steps to Reproduce:
 1. Send GET_TXN to any ledger (including domain) for any txn.
 2. Check reply.

Actual Results:
 There is no 'state_proof' field in the reply so state proof and freshness functionality doesn't affect pool and config ledgers since there are no special get txns for this ledgers.
Actually since GET_TXN is based on Ledger's Merkle Tree, not Patricia State Trie, the more correct term here is `audit_proof`.

Expected Result:
- There should be 'audit_proof' field in the reply for GET_TXN (see `{color:#333333}handle_get_txn_req{color}` in `node.py`).
- The field content needs to be the same as for `state_proof` in other get requests (see [https://github.com/hyperledger/indy-node/blob/master/docs/source/requests.md#reply-structure-for-read-requests).] 
- {{`proof_nodes}}` need to be the actual audit proof which can be obtained by `{color:#333333}merkleInfo{color}` method in `ledger.py`

Notes:
* Ensure that the pluggable request handler for GET_TXN is properly called.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-933,,,,,IS-1216,IS-1316,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969v986149",,,,Unset,Unset,Ev-Node 19.17,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,KitHat,RyanWest,VladimirWork,,,,,,,,,"21/Mar/19 12:56 AM;RyanWest;PoA (in progress): https://docs.google.com/document/d/1SoqihiVr-AkrM2GulbWyapIPnbyHv1XC61UX09A9trk/edit?usp=sharing;;;","27/Aug/19 12:43 AM;ashcherbakov;Build: 1.9.2.dev1065 -master;;;","27/Aug/19 4:10 PM;KitHat;Problem reason: 
- We have all of the data to make audit proof for GET_TXN but not providing signature and ledger size to verify it

Changes: 
- added some new fields for GET_TXN transaction

PR:
- https://github.com/hyperledger/indy-plenum/pull/1303


Version:
- node master #1965
- plenum master #873

Risk factors:
- Read of GET_TXN can get slower in case of regression

Risk:
- Low

Covered with tests:
- https://github.com/KitHat/indy-plenum/blob/ef367a6d0e4b34393fb473b4b9603ed377087c20/plenum/test/test_get_txn_state_proof.py

Recommendations for QA
- common test -- read from one node;;;","28/Aug/19 10:26 PM;VladimirWork;Build Info:
indy-node 1.9.2~dev1067
plugins 1.0.2~dev81

Steps to Validate:
https://github.com/VladimirWork/indy-test-automation/blob/bc266b089bf10838b2e81c29a6740ab16b39703c/system/draft/test_misc.py#L95

Actual Results:
GET_TXN from all ledgers except audit reads from 1 node successfully the same as verify_payment.

Additional Info:
Intermmittent unclear behaviour was appeared during state proof reading of domain genesis txn (seqNo 1) - it returned PoolLedgerTimeout instead of REPLY once in a while. It will be investigated separately.;;;",,,,,,,,,,,,,,,,,,,,,
Node can't order after view change and catch up,INDY-1955,36881,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,ozheregelya,ozheregelya,18/Jan/19 12:40 AM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,TShirt_L,,,"*Environment:*
indy-node 1.6.759

*Steps to Reproduce:*
1. Set up the pool.
2. Run load test with load near to production.
3. Stop several nodes (Node13 and Node20 for example).
4. Stop the primary to initiate View Change.
5. Start stopped nodes (one by one, including primary).
6. Reduce load rate of script.
7. Wait for the end of catch up on nodes which were stopped.
8. Try to write several txns.

*Actual Results:*
Pool can write, but nodes which were stopped don't order txns. The last message with ""ordered batch request"" was written to logs on view 0:
2019-01-17 12:58:04,538|INFO|replica.py|Node13:6 ordered batch request, view no 0, ppSeqNo 92, ledger 1001, state root None, txn root None, requests ordered 2, discarded 0

*Expected Results:*
Nodes should order after View Change and Catch Up.

 *Logs and metrics*: s3://qanodelogs/indy-1876/not_ordering_after_VC_and_catchup_17_01_2019
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1876/not_ordering_after_VC_and_catchup_17_01_2019/ /home/ev/logs/qanodelogs/indy-1876/not_ordering_after_VC_and_catchup_17_01_2019/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1966,INDY-1985,,,,,,,,,,,,,,"30/Jan/19 5:59 PM;VladimirWork;INDY-1955_25_01_2019.PNG;https://jira.hyperledger.org/secure/attachment/16644/INDY-1955_25_01_2019.PNG","30/Jan/19 5:59 PM;VladimirWork;INDY-1955_25_01_2019_another_nodes.PNG;https://jira.hyperledger.org/secure/attachment/16645/INDY-1955_25_01_2019_another_nodes.PNG","30/Jan/19 5:59 PM;VladimirWork;INDY-1955_29_01_2019.PNG;https://jira.hyperledger.org/secure/attachment/16646/INDY-1955_29_01_2019.PNG","01/Feb/19 4:13 PM;VladimirWork;INDY-1955_31_01_2019.PNG;https://jira.hyperledger.org/secure/attachment/16652/INDY-1955_31_01_2019.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:0000002",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,Toktar,VladimirWork,,,,,,,,,"23/Jan/19 11:15 PM;ozheregelya;New test-run results:
indy-node 1.6.769

Steps to reproduce are exactly the same as in description. Stopped nodes: Node 19, Node 24.
At the end of the test Node1 (ex-primary) still didn't completed catch up, but the initial problem is reproducing well on Node 24.

Logs and metrics: s3://qanodelogs/indy-1955/additional_logging_23_01_2019
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1955/additional_logging_23_01_2019/ /home/ev/logs/qanodelogs/indy-1955/additional_logging_23_01_2019/;;;","25/Jan/19 4:26 PM;ashcherbakov;*Problem reason*
 * One of the reasons for this is that if there are a lot of txns ordered by the pool when a Node is doing the view change (and hence a catch-up), then this Node may get enough Checkpoints to start catch-up.
 * Once the view-change/catch-up is finished, the Checkpoints were processed and a new catch-up was started.
 * However, there is no need to start a new catch-up since actually the quorum of stashed checkpoints is for the already ordered or caught up transactions.

*Fix*
 * Remove a stashed Checkpoint if it's already ordered

*PR*
 * [https://github.com/hyperledger/indy-plenum/pull/1061]

*Tests*
 * test_3pc_while_catchup_with_chkpoints_only
 * test_checkpoints_after_view_change

*Build*
 * 1.6.771

 ;;;","25/Jan/19 10:50 PM;Toktar;Another problem was found.

*Problem reason*
 * In catch-up a node sends CatchupReq to all nodes and doesn't check them for disconnection and malicious behavior. The catch-up will be finished but after a long time.

*Fix*
 * A node shouldn't send CatchupReq to disconnected nodes and nodes which didn't answer CatchupRep in the previous round of transactions request.

*PR*
 * [https://github.com/hyperledger/indy-plenum/pull/1063]

*Tests*
 * [test_catchup_with_one_slow_node.py|https://github.com/hyperledger/indy-plenum/pull/1063/files#diff-a9bd82da7b85c96e8a463678809ac513]
 * [test_catchup_with_disconnected_node.py|https://github.com/hyperledger/indy-plenum/pull/1063/files#diff-6290ecccaaac916bdcab5264aee89cf3]

*Build*
 * 1.6.774;;;","30/Jan/19 5:59 PM;VladimirWork;Build Info
Case 1: indy-node 1.6.772
Case 2: indy-node 1.6.774

Steps to Reproduce:
1. Run load test with load near to production.
2. Stop several nodes (Node20 and Node25).
3. Stop the primary (Node2 in Case 1 and Node1 in Case 2) to initiate View Change.
4. Start stopped nodes (one by one, including primary).
5. Stop the load.
6. Wait for the end of catch up on nodes which were stopped.

Actual Results:
Stopped nodes don't order after start (but catch up txns).
Case 1:  !INDY-1955_25_01_2019.PNG|thumbnail!  !INDY-1955_25_01_2019_another_nodes.PNG|thumbnail! 
Case 2:  !INDY-1955_29_01_2019.PNG|thumbnail! 

Logs and metrics:
Case 1:
ev@evernymr33:logs/1949_25_01_2019_metrics.tar.gz
ev@evernymr33:logs/1949_25_01_2019.tar.gz
Case 2:
ev@evernymr33:logs/1955_29_01_2019_metrics.tar.gz
ev@evernymr33:logs/1955_29_01_2019_logs.tar.gz;;;","31/Jan/19 9:09 PM;Toktar;*Problem reason*
 * Nodes remove first messages in stash deque upon reaching the limits.

*Fix*
 * Don't add messages in stash deque when it more than limit.

*PR*
 * [https://github.com/hyperledger/indy-node/pull/1148]
 * [https://github.com/hyperledger/indy-plenum/pull/1067]

*Tests*
 * [test_limited_stashing_3pc_while_catchup.py|https://github.com/hyperledger/indy-plenum/pull/1067/files#diff-aa7ee199f8b26780f7fd6b64bc93e06f]

*Build*
 * 1.6.776;;;","31/Jan/19 11:11 PM;VladimirWork;Build Info
indy-node 1.6.776

Steps to Reproduce:
1. Run load test with load near to production.
2. Stop several nodes (Node20 and Node25).
3. Stop the primary (Node1) to initiate View Change.
4. Start stopped nodes (one by one, including primary).
5. Stop the load.
6. Wait for the end of catch up on nodes which were stopped.

Actual Results:
Stopped nodes *don't order* after start but catch up successfully (except Node20). !INDY-1955_31_01_2019.PNG|thumbnail!

ev@evernymr33:logs/1955_31_01_2019_logs.tar.gz
ev@evernymr33:logs/1955_31_01_2019_metrics.tar.gz
;;;","01/Feb/19 9:27 PM;VladimirWork;All stopped and started nodes perform catch-up but still don't order so https://jira.hyperledger.org/browse/INDY-1983 has been reported for additional fixes.;;;",,,,,,,,,,,,,,,,,,
Remove ANYONE_CAN_WRITE,INDY-1956,36920,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,esplinr,esplinr,18/Jan/19 9:24 AM,01/Oct/20 8:21 AM,28/Oct/23 2:48 AM,14/Jun/19 8:23 PM,,1.9.0,,,,0,,,,"The flag for ANYONE_CAN_WRITE is currently tracked in the configuration file deployed to each node. If an admin were to change this flag on a single node, bad things would happen.

It is preferable to have this option set on a network-wide basis using the config ledger.

UPD: The new Multi-signature approach doesn't require this flag at all.

*Acceptance criteria:*
 * Either remove this flag from code (most likely) or move it to config ledger if it's really needed

 

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1528,INDY-1527,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001yw9609o",,,,Unset,Unset,Ev-Node 19.12,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,esplinr,ozheregelya,,,,,,,,,"21/Jan/19 4:19 PM;ashcherbakov;[~esplinr]
I'm not sure that this task is really critical because of 2 reasons:
1) If a Steward changes this flag intentionally, then he or she behaves like a malicious actor.
So, if the number of such Stewards is less than F, we are fine. If this number is more than F, then RBFT just doesn't work anymore (the same with any other malicious activity).

2) I'm not sure that we need this flag at all, or at least should keep it working the current way.
The new approach for multi-signature support should also provide a lot of flexibility for write authorization policy. So that we can control what's required to write a txn to the ledger just by configuring this policies (in config ledger or just in code for the first phase).
For example, Indy code can have some default trust-anchor based policies for every operation.
Other deployments (for example Sovrin) can extend/override this policies.
Indy may require that only a Trust Anchor can write to the ledger. Sovrin can extend the policy saying that either a Trust Anchor OR a user with tokens can write to the ledger.
ANYONE_CAN_WRITE will not be required for this anymore.;;;","12/Mar/19 10:57 PM;ozheregelya;(i) For QA:
 check that following AUTH_RULE command will be rejected:
{code:java}
ledger custom {""reqId"":101,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}},""field"":""role"",""auth_type"":""1"",""new_value"":""*"",""auth_action"":""ADD""},""protocolVersion"":2} sign=true
{code}
See Case 2 from this comment https://jira.hyperledger.org/browse/INDY-1995?focusedCommentId=57973&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-57973 for details.;;;","13/Jun/19 10:53 PM;Derashe;Problem reason/description: 
- We do not need ANYONE_CAN_WRITE flag anymore

Changes: 
- ANYONE_CAN_WRITE flag and according functionality excluded

PR:
- [https://github.com/hyperledger/indy-node/pull/1345]

Version:
- will be marked later

 

Covered with tests:
- Test ref on github

Recommendations for QA
- Edit acceptance test cases to exclude ANYONE_CAN_WRITE cases

- Check that manual setting of ANYONE_CAN_WRITE do not make any sense to authorisation;;;",,,,,,,,,,,,,,,,,,,,,,
Confirm that Trust Anchor role works as intended,INDY-1957,36921,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,esplinr,esplinr,18/Jan/19 9:35 AM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,1.6.83,,,,0,,,,"Our requirements around the Trust Anchor role has evolved a lot over the last year, and we need to confirm that the current implementation will meet the needs of the organizations who are now ready to write to the Sovrin Ledger.

*Acceptance Criteria*
 Confirm that the Trust Anchor role has the following behaviors, and raise tickets for any functionality that deviates from these requirements:
 * Any steward or trustee can create the role.
 * Any trustee can revoke the role.
 * Holders of this role can demote themselves (remove the role).
 * Anyone with the role should be able to write to the ledger without proof of payment.
 * If a ANYONE_CAN_WRITE (INDY-1528) is false, this role is required to write to the ledger.
 * If ANYONE_CAN_WRITE is true, then anyone can write to the ledger with or without this role.
 * The plugin interface supports plugins such that if ANYONE_CAN_WRITE is true, writes can be allowed without this role only if payment is attached, and can be allowed without a payment if the author has this role. (Check the Sovrin payment plugins to ensure they work this way. Issues with the plugin implementation should be raised in the Sovrin jira.)
 * This document reflects these rules:
 [https://github.com/hyperledger/indy-node/blob/master/docs/auth_rules.md]
 * Make sure that all necessary integration and system tests exist

*Notes*
 * The current Trust Anchor role will soon be called Endorser (INDY-1950)
 * ANYONE_CAN_WRITE does not need to be tracked on the config ledger for now (INDY-1956)
 * This role does not need to be able to authorize 3rd party transaction authors; for now all authors must have this role. (INDY-1563)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1968,INDY-1969,INDY-1970,INDY-1528,INDY-1554,INDY-1967,INDY-1974,INDY-1975,INDY-1976,INDY-1977,INDY-1978,INDY-1979,INDY-1980,INDY-1981,INDY-1982,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000001c",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,esplinr,ozheregelya,,,,,,,,,"21/Jan/19 4:00 PM;ashcherbakov;[~esplinr]

The story is mostly about creation of new transactions (new SCHEMA, NYM, CRED_DEF, etc.). So, if `ANYONE_CAN_WRITE=False`, then a non-trust-anchor can not create new transactions.

What about modifying existing transactions, for example DID key rotation? As of now, only the owner can modify his or her data (SSI principles). So, if a non-trust-anchor user has a DID (created by some Trust Anchor), he or she can send any number of txns to rotate the key. Is it OK and intended?;;;","22/Jan/19 10:50 PM;ozheregelya;[~esplinr]

There is one difference between auth_rules.md and [old spreadsheet|https://docs.google.com/spreadsheets/d/1uSxqZ5tzAiervgrLQn4FGlUhD-5YbxAbZkG4GdzjPtM/edit#gid=0]. In current implementation and in auth_rules.md only Trustee can blacklist Trust Anchor, but in old spreadsheet Steward can do this as well. Long time ago we had several discussions with Kelly, but have not had final answer, if the spreadsheet actual or not. Which requirement is correct?;;;","25/Jan/19 11:28 PM;andkononykhin;PoA:
 # implement integration tests
 # fix failed tests that don't need much time for that,  create tasks for other;;;","25/Jan/19 11:36 PM;esplinr;[~ozheregelya] The old spreadsheet should be considered deprecated. We need to maintain auth_rules.md to match the code.;;;","28/Jan/19 8:24 PM;ozheregelya;[~esplinr], It's clear. I asked because it's looks a bit strange that STEWARD can create TRUST_ANCHOR, but can't blacklist it.;;;","29/Jan/19 5:15 AM;ashcherbakov;There are 3 main things we need to take into account when deciding if the current trust anchor's behavior is fine and sufficient for the current use cases:
 # Although only trust anchor can create new NYMs, SCHEMAs, ATTRIBs and CLAIM_DEFs, the owner of the NYM transaction can edit it (for example rotate the key) without any restrictions. So, at some degree there are write transactions that can be sent by a non-trust-anchor.
 # As of now, it's not possible to have either Permissions or fees-based authorization policies (that is be either a Trust Anchor OR pay). Tokens always require payment regardless of the role.
 This will be fixed once the new multi-signature approach is integrated in the scope of INDY-1930.
 # Revocation transactions can be sent by non-trust-anchors. This will be addressed in the scope of INDY-1554
 # As of now, no promotion of roles is possible, that is if a NYM is demoted (trust anchor role is removed), then it's removed forever.;;;","30/Jan/19 6:26 PM;andkononykhin;> Any steward or trustee can create the role.

works

> Any trustee can revoke the role.

woks

> Holders of this role can demote themselves (remove the role).

INDY-1968

> Anyone with the role should be able to write to the ledger without proof of payment.

Doesn't work. Plugins doesn't check any authorization rules for now. (INDY-1930)

> If a ANYONE_CAN_WRITE (INDY-1528) is false, this role is required to write to the ledger.

Doesn't work since:
 * NYM: by design anyone can modify its verkey if he/she owns the NYM (verkey is not None)
 * revocation txns: anyone can write (INDY-1554)

> If ANYONE_CAN_WRITE is true, then anyone can write to the ledger with or without this role.

Partly works. INDY-1967

> The plugin interface supports plugins such that if ANYONE_CAN_WRITE is true, writes can be allowed without this role only if payment is attached, and can be allowed without a payment if the author has this role. (Check the Sovrin payment plugins to ensure they work this way. Issues with the plugin implementation should be raised in the Sovrin jira.)

Doesn't work. (INDY-1930)

Additional notes (already mentioned):
 * there was found that demotion for did with no role (already demoted or never been promoted) is accepted and seems it is not as designed (INDY-1969)
 * previously demoted DID (or created without any role specified) can't be promoted (INDY-1970)
 * once INDY-1930 becomes done ANYONE_CAN_WRITE is going to be removed
 * created a set of tasks to improve test codebase to cover all possible cases related to auth rules: INDY-1974,  INDY-1975,  INDY-1976,  INDY-1977,  INDY-1978,  INDY-1979,  INDY-1980,  INDY-1981,  INDY-1982;;;","30/Jan/19 6:33 PM;ozheregelya;{quote}If ANYONE_CAN_WRITE is true, then anyone can write to the ledger with or without this role.
{quote}
Please note the that now there is a problem with ANYONE_CAN_WRITE = True - INDY-1967: ANYONE_CAN_WRITE = True breaks roles validation. 
So, anyone can write to the ledger with or without any role, but, for example, Identity Owner can create new Trustee or blacklist existing one.;;;","31/Jan/19 8:24 PM;ozheregelya;From QA side, all acceptance scenarios were reviewed and updated. INDY-1963 and INDY-1971 block updating expected results, so expected results will be updated in scope of these tickets.
Roles were re-tested and there were no issues besides INDY-1963, INDY-1967 and INDY-1971.;;;","01/Feb/19 5:29 PM;andkononykhin;*Problem reason*:
 * there is no assurance  that TRUST_ANCHOR permissions are set correctly
 * there are not enough integration tests to cover more cases (all would be the best)

*Changes*:
 * added integration tests
 * created set of tasks to fix encountered issues and improve test codebase (please refer to this comment)

*Committed into*:

[https://github.com/hyperledger/indy-plenum/pull/1068]

[https://github.com/hyperledger/indy-node/pull/1146|https://github.com/hyperledger/indy-node/pull/1029]

*Risk factors*:

no

*Risk*:

Low

*Covered with tests*:

yes

*Recommendations for QA*:
 * no acceptance is expected since changes were made only for test codebase only;;;",,,,,,,,,,,,,,,
Add docker node to persistent pool for upgrade tests,INDY-1958,36928,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,18/Jan/19 8:53 PM,30/Mar/19 5:32 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,1.6.83,,,,0,,,,The last two upgrades have had the IBM node spamming the other nodes config ledger. They have their node running in a docker. We need to add it to the pool.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007y",,,,Unset,Unset,Ev-Node 19.02,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,"24/Jan/19 5:48 PM;VladimirWork;Container and network are configured so connections to AWS nodes from genesis file are established successfully. Docker node will be added to persistent pool today to prove that all works as expected.;;;","24/Jan/19 9:00 PM;VladimirWork;Pool with 25 AWS nodes and 1 docker node works so we can test upgrade against this setup at the next acceptance testing.
{noformat}
root@8e4449e8309d:/home/indy# validator-info
Validator Node26 is running
Update time:     Thursday, January 24, 2019 11:58:14 AM +0000
Validator DID:    5gR8V32KnPfrYBbTsdu77TNGSKcsCh6igri7PEQkgY6H
Verification Key: 6SS8Hu4bt9F3pJvyLR31bxt4z2VXChS9d94V9qsMrosjnNsBzgcMGwq
BLS Key: 2iEuvYUPJZ4XSf4gsM1qrJ4wSELZTHGXRXqSb7ATtyb6xeyhgaXCqTuDnxN8uAbw5SuRrK9WPJusQPgh3RzCyVzbb6tZac8d7gex8eRkvHuFMcDdHY9oEkv8Jbskkz41K2T5PmiyyB1pYxKaiVsJSXny3TNEjUu5eF6p69Az5rYB7YJ
Node HA:        0.0.0.0:9701
Client HA:      0.0.0.0:9702
Metrics:
  Uptime: 18 minutes, 1 second
  Total pool Transactions:  26
  Total config Transactions:  0
  Total ledger Transactions:  231
  Read Transactions/Seconds:  0.00
  Write Transactions/Seconds: 0.01
Reachable Hosts:   26/26
  Node6	(5)
  Node8	(7)
  Node24	
  Node19	
  Node15	
  Node2	(1)
  Node21	
  Node18	
  Node13	
  Node12	
  Node22	
  Node7	(6)
  Node14	
  Node17	
  Node1	(0)
  Node16	
  Node10	
  Node20	
  Node5	(4)
  Node25	
  Node11	
  Node4	(3)
  Node26	
  Node9	(8)
  Node23	
  Node3	(2)
Unreachable Hosts: 0/26
Software Versions:
  indy-node: 1.6.761
{noformat}
;;;",,,,,,,,,,,,,,,,,,,,,,,
Node should be automatically restarted when hung,INDY-1959,36933,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,18/Jan/19 11:11 PM,18/Jan/19 11:12 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,"*Acceptance criteria*
* Node control tool should be able to detect when node becomes unresponsive, and force a restart after some timeout (default value of 600 looks nice)
* This behavior should be configurable (both whether restart is needed and value of timeout)

*Nice to have*
In case of forced restart it would be exceptionally useful to automatically gather stack trace of node for further analysis. It is possible using gdb with python debugging plugin. Following things a required:
* _gdb_ and _python3.5-dbg_ packages should be installed
* if using Ubuntu 16.04 plugin should be patched: 
{code}
/usr/share/gdb/auto-load/usr/bin/python3.5m-gdb.py:
- fields = gdb.lookup_type('PyUnicodeObject').target().fields()
+ fields = gdb.lookup_type('PyUnicodeObject').fields()
{code}
Probably it's possible to place patched plugin somewhere in home directory in order to avoid modifying system files.
* attach to process by running _sudo gdb python3.5 <pid>_
* get stack trace by running _py-bt_ inside gdb console
* if for some reason we need to keep process running _detach_ should be issued inside gdb console before closing gdb

A lot of useful information can be found here:
* https://wiki.python.org/moin/DebuggingWithGdb
* https://stackoverflow.com/questions/53084290/python-gdb-error-python-exception-class-runtimeerror-type-does-not-have-a-t
* http://grapsus.net/blog/?q=gdb",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1949,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i007q7:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Signature validation must be in-sync on all nodes,INDY-1960,36972,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,ashcherbakov,ashcherbakov,21/Jan/19 5:54 PM,08/Jan/20 5:12 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,"h2. *Problem description*
 * Signatures are validated by nodes as part of static validation, that is at the time when either a request or a first Propagate is received.
 * Signature validation depends on the current uncommitted state of the domain ledger, since it's validated against the keys associated with DIDs in domain ledger.
 * Different nodes may have different domain uncommitted state because of the lags in the network so that some nodes are behind.
 * So, some nodes may reject (NACK) the request because of invalid signature, and some nodes may accept it (ACK).
 * Also if request's signature is valid, it will not be re-checked anymore when corresponding Propagates are received.

h2. *Decision matrix*

 
| |Not propagated to Primary|Propagated to Primary|
|Propagated to
non-primary|Case 1, 2|OK|
|Not propagated to
non-primary|Case 3|Case 4, 5|

**

 
h2. *Consequences*
 * *Case 1: A request stays in Request queue and not ordered for some time*
 **  Description:
 *** A request is accepted (ACK) by a (non-master-primary) Node which is behind the primary (hasn't yet applied the recent uncommitted state where the request is rejected).
 *** Other nodes reject the request
 *** So, there is no quorum of PROPAGATEs
 *** The request on the node which accepted it will not be removed from the requests queue until it's cleared by timeout
 *** => potential risk of OOM
 **  Severity
 *** Low
 *** There are not so high chances for such situations, and the number of such requests is not significant, so the chances of OOM are pretty low
 *** Also we have a sanity check to clean the requests queue from time to time

 * *Case 2: Unequal number of requests ordered by Master and non-master Instances*
 **  Description:
 *** A request is rejected (NACK) by a Node which is behind the master primary (hasn't yet applied the recent uncommitted state where the request is rejected).
 *** This node is a primary on a backup instance
 *** Other nodes accept the request
 *** So, there is a quorum of PROPAGATEs
 *** The node that rejected the request rejects the PROPAGATEs as well since it's still behind the master primary
 *** The backup instance where this node is a primary will not order the request, while master will
 *** => the backup instance is faster
 **  Severity
 *** Low
 *** There are not so high chances for such situations, and the number of such requests is not significant
 *** However, this is a source of potential false-positive view changes

 * *Case 3: A client rotates the key and then sends the txn using the new key, and this txn is rejected with invalid signature*
 ** Descriptions:
 *** A request is accepted (ACK) by the Primary (BTW the Primary's uncommitted state is always up-to-date and can be used as a reference when validation signatures)
 *** The request is rejected (NACK) by n-f non-primaries which are behind the primary (haven't yet applied the recent uncommitted state where the request can be accepted).
 *** => Client receives a quorum of NACKs and assumes that request is not ordered.
 ** Severity:
 *** Medium
 *** If the client re-sends the same request, the request will be ordered successfully

 * *Case 4: Message Requests for Propagates*
 ** Description:
 *** A request is accepted (ACK) by the Primary (BTW the Primary's uncommitted state is always up-to-date and can be used as a reference when validation signatures)
 *** A request is accepted by n-f nodes in total and propagated to n-f nodes including the Primary
 *** A request is rejected and not propagated to f nodes which are behind the primary (haven't yet applied the recent uncommitted state where the request can be accepted).
 *** Primary starts ordering the request
 *** Non-primaries who haven't propagated the request send MessageRequest for the corresponding Propagates
 *** If the Propagates are received at the time when signature validation passes (the node applied the same state as the Primary), then the node can continue ordering.
 *** If the Propagates are received at the time when signature is still considered to be invalid (the node hasn't yet applied the same state as the Primary), then the Propagates will not be requested anymore, and the Node stops ordering until the quorum of stashed stable checkpoints and catch-up.
 ** Severity:
 *** Low
 *** The situation is quire rare and the message request saves the node

 * *Case 5: Node stops ordering until catch-up*
 ** Description:
 *** See Case 4
 **  Severity:
 *** High

h2. *How to fix*
 * Option1: move signature verification to dynamic validation, so that the same (proposed by the primary) uncommitted state is used
 ** Cons:
 *** A source of attacks, so that anyone can send a lot of invalid requests
 ** Pros:
 *** Easy to implement
 *** Fixes all the issues
 * Option 2: TBD",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49i59u",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update technical documentation with Audit Ledger concept,INDY-1961,36975,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,ashcherbakov,ashcherbakov,21/Jan/19 6:16 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,1.7.1,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:000006w",,,,Unset,Unset,Ev-Node 19.05,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,"15/Mar/19 8:46 PM;ashcherbakov;*Changes*
 * Added docs about Audit ledger (audit_ledger.md)
 * Updated docs about storages (sotrage.md and storages.puml)
 * Updates docs about catchup (catchup.md and sequence diagram catchup-procedure.puml) taking into account Audit Ledger and recent changes with stashing during catchup

*PRs:*
- https://github.com/hyperledger/indy-plenum/pull/1106
- https://github.com/hyperledger/indy-plenum/pull/1129;;;",,,,,,,,,,,,,,,,,,,,,,,,
Sample configuration for multi-sig permissions,INDY-1962,36993,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,esplinr,esplinr,21/Jan/19 11:51 PM,28/Aug/19 6:13 PM,28/Oct/23 2:48 AM,28/Jun/19 10:50 AM,,,,,,0,,,,"The default deployment of Indy should have liberal permissions so that developers do not have unnecessary hurdles to learning the system. No multi-signature should be required.

But Indy should include a sample transaction that configures the permissions in INDY-1729 in order demonstrate how multi-signature ledger permissions can be configured.

*TODO: Need to define*
 * A single trustee can setup a permission scheme
 * Once the permission scheme is established, it will define how many trustees are required to change the scheme (3 as the default in the sample transaction)

 

Examples of rules requiring a multi-signature (that can be used by default in Indy deployments such as Sovrin):
 * Three trustees are required to
 ** create a new trustee
 ** remove a trustee
 * Two trustees are required to:
 ** create a steward
 ** revoke the role of a steward
 ** demote a steward's nodes
 ** can write an upgrade transaction to the ledger",,,,,,,,,,INDY-1729,INDY-1732,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00001ywa9",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,lbendixsen,,,,,,,,,,"11/Jun/19 10:55 PM;esplinr;[~lbendixsen] Can you contribute the script you use to setup the permission scheme for the Sovrin network?

We will then want to update the documentation to clarify that the default deployment is open and easy to use for developers, but should be changed for production use cases because it is potentially insecure.;;;","13/Jun/19 6:09 AM;lbendixsen;If I am not mistaken, copying the following into  filename and then entering filename on the command line with indy-cli like

> indy-cli filename

will invoke all of the current settings that the Sovrin Foundation will use for its MainNet. (With a few exceptions as the following does not include FEEs)

If this does not satisfy the request let me know and I will do my best to remedy it.

Be careful when using the following to run tests as you could easily lock yourself out of your Network. Best practice for testing is to leave out the last line of the ""script"" so that you can still change auth_rules with only one trustee.

---------------

ledger auth-rule action=ADD field=role txn_type=NYM old_value=* new_value=0 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=role txn_type=NYM old_value=* new_value=2 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=role txn_type=NYM old_value=* new_value=101 constraint=""\{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=role txn_type=NYM old_value=* new_value=201 constraint=""\{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=role txn_type=NYM old_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false},\{""sig_count"":1,""role"":""101"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}]}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=0 new_value=2 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=0 new_value=101 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=0 new_value=201 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=0 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=2 new_value=0 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=2 new_value=101 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=2 new_value=201 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=2 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=101 new_value=0 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=101 new_value=2 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=101 new_value=201 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=101 constraint=""\{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=201 new_value=0 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=201 new_value=2 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=201 new_value=101 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM old_value=201 constraint=""\{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM new_value=0 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM new_value=2 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM new_value=101 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=role txn_type=NYM new_value=201 constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=verkey txn_type=NYM old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""*"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=ADD field=* txn_type=ATTRIB old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""*"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=* txn_type=ATTRIB old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""*"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=ADD field=* txn_type=SCHEMA old_value=* new_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false},\{""sig_count"":1,""role"":""101"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}]}""

ledger auth-rule action=ADD field=* txn_type=SCHEMA old_value=* new_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false},\{""sig_count"":1,""role"":""101"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}]}""

ledger auth-rule action=ADD field=* txn_type=CLAIM_DEF old_value=* new_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false},\{""sig_count"":1,""role"":""101"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}]}""

ledger auth-rule action=EDIT field=* txn_type=CLAIM_DEF old_value=* new_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true},\{""sig_count"":1,""role"":""101"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}]}""

ledger auth-rule action=ADD field=services txn_type=NODE old_value=* new_value=[VALIDATOR] constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=services txn_type=NODE old_value=* new_value=[] constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=services txn_type=NODE old_value=[VALIDATOR] new_value=[] constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":4,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}]}""

ledger auth-rule action=EDIT field=services txn_type=NODE old_value=[] new_value=[VALIDATOR] constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":4,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}]}""

ledger auth-rule action=EDIT field=node_ip txn_type=NODE old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=node_port txn_type=NODE old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=client_ip txn_type=NODE old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=client_port txn_type=NODE old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=blskey txn_type=NODE old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=ADD field=action txn_type=POOL_UPGRADE old_value=* new_value=start constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=action txn_type=POOL_UPGRADE old_value=start new_value=cancel constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=action txn_type=POOL_RESTART old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=EDIT field=action txn_type=POOL_CONFIG old_value=* new_value=* constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}""

ledger auth-rule action=ADD field=* txn_type=GET_VALIDATOR_INFO old_value=* new_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false},\{""sig_count"":1,""role"":""201"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}]}""

ledger auth-rule action=ADD field=* txn_type=REVOC_REG_DEF old_value=* new_value=* constraint=""{""constraint_id"":""OR"",""auth_constraints"":[

{""sig_count"":1,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}

,\{""sig_count"":1,""role"":""2"",""constraint_id"":""ROLE"",""need_to_be_owner"":false},\{""sig_count"":1,""role"":""101"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}]}""

ledger auth-rule action=EDIT field=* txn_type=REVOC_REG_DEF old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""*"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=ADD field=* txn_type=REVOC_REG_ENTRY old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""*"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=* txn_type=REVOC_REG_ENTRY old_value=* new_value=* constraint=""\{""sig_count"":1,""role"":""*"",""constraint_id"":""ROLE"",""need_to_be_owner"":true}""

ledger auth-rule action=EDIT field=* txn_type=AUTH_RULE old_value=* new_value=* constraint=""\{""sig_count"":3,""role"":""0"",""constraint_id"":""ROLE"",""need_to_be_owner"":false}"";;;","28/Jun/19 10:50 AM;esplinr;This is perfect. Thank you for sharing!;;;",,,,,,,,,,,,,,,,,,,,,,
Unclear error messages when Trustee send a NYM with the same verkey,INDY-1963,37028,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,22/Jan/19 4:12 PM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,1.6.83,,,,0,TShirt_M,,,"The current error message is `actor must be owner`, which is very unclear and not obvious.

*Acceptance criteria:*
 * Show more descriptive and clear message in *all cases* where `actor must be owner` is shown now.
For example, `Trustee can not touch verkey field since only the owner can modify it`


 * Update tests.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007x",,,,Unset,Unset,Ev-Node 19.02,Ev-Node 19.03,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,ozheregelya,VladimirWork,,,,,,,,,"30/Jan/19 11:11 PM;ozheregelya;Similar problem appear 
* in case of laсk of permissions when create a role:
{code:java}
Transaction has been rejected: role is not accepted{code}
* in case of almost all actions as Identity Owner:
{code:java}
Transaction has been rejected: There is no accepted constraint{code};;;","30/Jan/19 11:28 PM;ozheregelya;For QA:
! Note that this fix will require update of expected results for [acceptance batches|https://github.com/hyperledger/indy-node/tree/master/acceptance/indy-cli-batches].;;;","01/Feb/19 11:31 PM;anikitinDSR;Reasons:
 * need to make more cleanly error messages for authorization

Changes:
 * changed format of ""owner"" error messages 
 * changed ""There is no constraint"" messages
 * made other messages more cleanly

Version:
 * indy-node 1.6.779

Recomendation for QA:
 * raise ""owner"" message, for example change verkey by not owner, and check, that auth error looks like "" can not touch verkey field since only the owner can modify it""
 * try to ""There is no accepted constraint"" error, for example doing some actions by already suspended user. Check, that auth errors for this case looks like ""Rule for this action is <description of rule with requirements>"";;;","04/Feb/19 10:25 PM;VladimirWork;Build Info:
indy-node 1.6.779

Steps to Validate:
1. Raise ""owner"" message, for example change verkey by not owner, and check, that auth error looks like ""can not touch verkey field since only the owner can modify it"".
2. Try to ""There is no accepted constraint"" error, for example doing some actions by already suspended user. Check, that auth errors for this case looks like ""Rule for this action is <description of rule with requirements>"".

Actual Results:
All messages appear as expected.;;;",,,,,,,,,,,,,,,,,,,,,
"""Create a Network and Start Nodes"" echo command requires additional sudo",INDY-1964,37164,,Bug,Code Review,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,coderintherye,coderintherye,coderintherye,26/Jan/19 8:06 AM,26/Jan/19 8:11 AM,28/Oct/23 2:48 AM,,,,,,,0,,,,"The command:

sudo echo ""deb https://repo.sovrin.org/deb xenial stable"" >> /etc/apt/sources.list

present at [https://github.com/hyperledger/indy-node/blob/master/docs/start-nodes.md] does not actually work because appending gets done as regular user rather than as sudo, as seen:

$ sudo echo ""deb https://repo.sovrin.org/deb xenial stable"" >> /etc/apt/sources.list
bash: /etc/apt/sources.list: Permission denied

 

instead should use this command:

sudo bash -c 'echo ""deb https://repo.sovrin.org/deb xenial stable"" >> /etc/apt/sources.list'

 ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i0093b:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),coderintherye,,,,,,,,,,,,"26/Jan/19 8:11 AM;coderintherye;https://github.com/hyperledger/indy-node/pull/1141/files;;;",,,,,,,,,,,,,,,,,,,,,,,,
Some of the nodes lagging behind during the load test,INDY-1965,37176,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,28/Jan/19 4:57 PM,30/Mar/19 5:32 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,1.7.1,,,,0,TShirt_XL,,,"*Problem:*
 * Load acceptance test (see INDY-1949, cases 2 and 3)
 * Some of the nodes starting to be very slow
 * So, some instances order slower, and the requests queue grows
 * That leads to OOM eventually
 * It looks like one of the reason of the slowness of nodes is a lot of messages requests for Propagates at some point (that is a node receives PrePrepares with non-finalized requests).

*Acceptance criteria*
 * Find out why so many Messages Requests for Propagates is started to be sent
 * Check if this is the only source of the issue
 * Make a fix or create a separate ticket for the fix if needed",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1949,,,,,INDY-1985,,,,,,,,,,,,,,,"01/Feb/19 5:53 PM;VladimirWork;INDY-1965_01_02_2019.PNG;https://jira.hyperledger.org/secure/attachment/16653/INDY-1965_01_02_2019.PNG","06/Feb/19 12:22 AM;VladimirWork;INDY-1965_05_02_2019.PNG;https://jira.hyperledger.org/secure/attachment/16721/INDY-1965_05_02_2019.PNG","15/Feb/19 1:34 AM;ozheregelya;ddos1965.png;https://jira.hyperledger.org/secure/attachment/16814/ddos1965.png","12/Feb/19 12:18 AM;Derashe;image-2019-02-11-18-18-20-176.png;https://jira.hyperledger.org/secure/attachment/16761/image-2019-02-11-18-18-20-176.png","15/Feb/19 5:08 PM;Derashe;image-2019-02-15-11-08-33-682.png;https://jira.hyperledger.org/secure/attachment/16819/image-2019-02-15-11-08-33-682.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00000009",,,,Unset,Unset,Ev-Node 19.02,Ev-Node 19.03,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Derashe,ozheregelya,Toktar,VladimirWork,,,,,,,,"29/Jan/19 4:42 PM;VladimirWork;Build Info:
indy-node 1.6.772

Steps to Reproduce:
1. Run production load test for 8 hours.

Actual Results:
Pool was in consensus at the end of load test but logs and metrics should be investigated:
ev@evernymr33:logs/1965_28_01_2019_metrics.tar.gz
ev@evernymr33:logs/1965_28_01_2019_logs.tar.gz;;;","01/Feb/19 5:30 AM;Toktar;*Problem reason*
 * Nodes are working slow with lots of stashing messages in logs.

*Fix*
 * Move logs about stashing to TRACE from INFO

*PR*
 * [https://github.com/hyperledger/indy-plenum/pull/1069]

*Build*
 * 1.6.778 - indy-node - master
 * 1.6.669 - indy-plenum - master;;;","01/Feb/19 5:54 PM;VladimirWork;Build Info:
indy-node 1.6.776

Steps to Reproduce:
1. Run production load test for 8+ hours.

Actual Results:
Pool was in consensus at the end of load test but logs and metrics should be investigated:
ev@evernymr33:logs/1965_01_02_2019_metrics.tar.gz
ev@evernymr33:logs/1965_01_02_2019_logs.tar.gz
 !INDY-1965_01_02_2019.PNG|thumbnail! ;;;","05/Feb/19 1:40 AM;Derashe;While researching this issue we did some optimization to propagate handling process.

PR: [https://github.com/hyperledger/indy-plenum/pull/1071]

Node ver: 781;;;","06/Feb/19 12:22 AM;VladimirWork;Build Info:
indy-node 1.6.781

Steps to Reproduce:
1. Run production load test for 8+ hours.

Actual Results:
Pool was in consensus at the end of load test but logs and metrics should be investigated:
ev@evernymr33:logs/1965_05_02_2019_metrics.tar.gz
ev@evernymr33:logs/1965_05_02_2019_logs.tar.gz
 !INDY-1965_05_02_2019.PNG|thumbnail! ;;;","11/Feb/19 9:10 PM;ozheregelya;Environment:
indy-node 1.6.798

Steps to Reproduce:
1. Set up the pool.
2. Add to the node config
Case 1:
{code:java}
MAX_REQUEST_QUEUE_SIZE = 10000
NODE_TO_NODE_STACK_QUOTA = 1000{code}
Case 2:
{code:java}
MAX_REQUEST_QUEUE_SIZE = 10000
NODE_TO_NODE_STACK_QUOTA = 1000
PROPAGATE_REQUEST_DELAY = 300{code}
3. Run the production load without plugins.

Logs and metrics: s3://qanodelogs/indy-1965
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-1965/case1/ /home/ev/logs/qanodelogs/indy-1965/case1/
aws s3 cp --recursive s3://qanodelogs/indy-1965/case2/ /home/ev/logs/qanodelogs/indy-1965/case2/;;;","12/Feb/19 12:30 AM;Derashe;Case 1 showed us working pool with stable performance and no anomalyes. It seems, that raising our node stack quotas helped node to survive bounce of propagate requests.

Example of propagate bounce:  !image-2019-02-11-18-18-20-176.png|thumbnail! . In the top red circle we can see that node stack was bounced up to 600 node messages. After that we was able to handle all of these messages and continue stable ordering. 

As a result of testing, we've decided to set these configs as default:

_MAX_REQUEST_QUEUE_SIZE = 10000_
_NODE_TO_NODE_STACK_QUOTA = 1000_;;;","12/Feb/19 12:37 AM;Derashe;PR: https://github.com/hyperledger/indy-plenum/pull/1083;;;","13/Feb/19 12:16 AM;Derashe;Problem reason:
 * During the load, nodes starting to ask for propagates because of getting preprepare with unfinalized requests. Some nodes get enough propagates and continue ordering, but other part is starting to ask more and more propagates, what leads us to Node falling because of OOM. Such a nodes cannot participate in consensus.

Changes:
 * During inverstigation, we found, that raising bound for node stack messages allow nodes to survive and succesfully request all propagates.
 * Bound was raised up to 1000 requests per one iteration (was 100). That number based on successfull load testing results and our assumption of approximate propagates request count.
 * We left maximum message size for one prod run the same to avoid probable issues.

Committed into:
 * changed config https://github.com/hyperledger/indy-plenum/pull/1083

Risk:
 * There is a risk, that we can be spammed by some unexpected traffic (from malicious node for example). But as we left messages size bound the same, this risk is minimized

Covered with tests:
 * load tests above

Recommendations for QA: 
 * Retest the case above with plugins to ensure that they don't affect this problem
 * Retest the case of DDOS with plugins to look if there are any performance improvements;;;","14/Feb/19 4:41 PM;VladimirWork;Build Info:
indy-node 1.6.803

Steps to Validate:
1. Run prod load with payments with
{noformat}
MAX_REQUEST_QUEUE_SIZE = 10000
NODE_TO_NODE_STACK_QUOTA = 1000
{noformat}
in indy_config.py for 6+ hours.

Actual Results:
Pool was in consensus at the end of load, domain and token ledgers txn count was consistent. More info can be found in ev@evernymr33:logs/1965_13_02_2019_metrics.tar.gz;;;","15/Feb/19 1:40 AM;ozheregelya;Case 2: DDoS load

!ddos1965.png|thumbnail!

Load test was running with load rate 70 txns/sec (production mix) during 20 minutes.
77K txns were written, 26K txns were lost (19K of them since NACKs view change).
All txhs were written during ~1 hour since the start of test. After that pool worked under production load without any issues and all nodes were in sync.;;;","15/Feb/19 6:18 PM;VladimirWork;Build Info:
indy-node 1.6.803

Steps to Validate:
1. Run prod load with payments with

MAX_REQUEST_QUEUE_SIZE = 10000
NODE_TO_NODE_STACK_QUOTA = 1000
in indy_config.py for 10+ hours.

Actual Results:
Pool was in consensus at the end of load, domain and token ledgers txn count was consistent. More info can be found in ev@evernymr33:logs/1965_15_02_2019_metrics.tar.gz;;;","15/Feb/19 6:51 PM;Derashe;Here is result of 10 tps load. It looks pretty stable. We have propagate request bounce at the end. But, unfortunatelly, we turned pool off. 

Ticket can be closed because load became stable.

!image-2019-02-15-11-08-33-682.png|thumbnail!;;;","16/Feb/19 12:47 AM;Derashe;PR: [https://github.com/hyperledger/indy-plenum/pull/1090];;;",,,,,,,,,,,
Problem with VCStartMsgStrategy,INDY-1966,37188,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,Toktar,Toktar,28/Jan/19 10:55 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,1.7.1,,,,0,,,,"*Steps to Reproduce:*

1) A node is started.

2) The node receive InstanceChange for viewNo = 3 and collect Quorum to start View Change 2 -> 3. The node calls _on_verified_instance_change_msg() and starts prepare_view_change process.

3) The node begins to process FutureViewChangeDone (CurentState early) and starts propagate primary for view no 3.  View_change_in progress = True, catchup is started.

4) The node process ViewChangeContinueMessage in the method on_view_change_continued() but proposed_view_no = 3 and  replica.node.viewNo = 3. It means that message will not be processed and prepare_view_change  will not be finished. As a result a new InstanceChanges will be received but condition for a new view change will not be called because is_preparing is False still.

 

*Acceptance Criteria:*

To solve this problem we need to collect InstanceChanges but don't process them before propagate primary. And process InstanceChanges after.
 * Add test to reproduce this case
 * Fix a bug with processing InstanceChanges before propagate primary. If a node starts with a big lag and pool has 2 view changes one by one then the started node should does catch up, propagates primary, does view change for the last view and start ordering.

*Logs:*

ev@evernymr33:logs/1949_25_01_2019_metrics.tar.gz

ev@evernymr33:logs/1949_25_01_2019.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1955,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwvif:000001f",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),Toktar,VladimirWork,,,,,,,,,,,"08/Feb/19 10:38 PM;Toktar;*PoA:*
 +Write a test+ for reproduce this problem:
 1. Stop the node Delta
 2. Patch methods for processing VCStartMsgStrategy messages
 3. Delay CurrentState messages on Delta
 4. Start Delta
 5. Start view change with a maser degradation reason (from view 0 to 1)
 6. Check that Delta start VCStartMsgStrategy after quorum of InstanceChanges
 7. Reset delay for CurrentStates
 8. Check that propagate primary happened.
 9. Unpatch VCStartMsgStrategy methods and process catching messages.
 10. Start view change with a maser degradation reason (from view 1 to 2)
 11. Check that all nodes has viewNo = 2 and can order transactions.
 If test reproduced the problem +add a fix+. In the end of catchup call 
 VCStartMsgStrategy method which will back strategy's parameters to initial define values;;;","11/Feb/19 11:56 PM;Toktar;*Problem reason:*
 - If a node collect quorum of instance change messages before propagating primary and start pre-processing ViewChange messages, then after propagating primary the node can't do a new view change.

*Changes:*
 - Add a complete logic in a VCStartMsgStrategy before start view change.

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1167]
 * [https://github.com/hyperledger/indy-plenum/pull/1081]

*Version:*
 * indy-node 1.6.801 -master
 * (indy-plenum 1.6.676 -master)

*Risk factors:*
 - A node can't order after start.

*Risk:*
 - Low

*Test:*
 * [test_restart_node_with_view_changes.py|https://github.com/hyperledger/indy-plenum/pull/1081/files#diff-0c3695aa4d1c10233f315936e874a530]

*Recommendations for QA:*

Load with 10 txn/sec (any medium load), force view change every 1 hour. Try to restart not primary node in view change and just before view change.;;;","13/Feb/19 4:52 PM;VladimirWork;Build Info:
indy-node 1.6.802

Steps to Validate:
1. Load with 10 txn/sec (any medium load).
2. Force view change every 30 minutes.
3. Restart not primary nodes (5-10 one by one) in view change and just before view change.

Actual Results:
There are the same view no and primaries at the end of load test. Amount of txns in ledger also is the same.;;;",,,,,,,,,,,,,,,,,,,,,,
ANYONE_CAN_WRITE = True breaks roles validation,INDY-1967,37189,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,ozheregelya,ozheregelya,28/Jan/19 11:34 PM,14/Jun/19 11:20 PM,28/Oct/23 2:48 AM,14/Jun/19 11:20 PM,,1.9.0,,,,0,,,,"*Environment:*
 indy-node 1.6.772

*Steps to Reproduce:* 
 1. Set ANYONE_CAN_WRITE = True in node configs, restart the pool.
 2. Run acceptance batch [https://github.com/hyperledger/indy-node/blob/master/acceptance/indy-cli-batches/AS-03-01-identity-owner-anyone-can-write.batch]
 3. Compare actual and expected results.

*Actual Results:*
 Identity Owner can blacklist Trustee, create Steward etc.

*Expected Results:*
 ANYONE_CAN_WRITE = True should not break roles validation for NYM txn.

*Additional Information:*
 * This issue is actual not only for Identity Owner. For example, Steward can create Trustee:
{code:java}
pool(p4):wallet(AS-03-wallet-owner):did(Ahq...xCn):indy> ledger get-nym did=CbPwHxKEibPhV4pgXWpu26
Following NYM has been received.
Metadata:
+------------------------+-----------------+---------------------+---------------------+
| Identifier             | Sequence Number | Request ID          | Transaction time    |
+------------------------+-----------------+---------------------+---------------------+
| AhqUV2zHYdNaWLFCCe7xCn | 27              | 1548685898206190160 | 2019-01-28 14:13:16 |
+------------------------+-----------------+---------------------+---------------------+
Data:
+------------------------+------------------------+-------------------------+---------+
| Identifier             | Dest                   | Verkey                  | Role    |
+------------------------+------------------------+-------------------------+---------+
| LBbKEeczA9iL21p4Kgxcuf | CbPwHxKEibPhV4pgXWpu26 | ~MviYa49QADQXAM68WSiLPD | STEWARD |
+------------------------+------------------------+-------------------------+---------+
pool(p4):wallet(AS-03-wallet-owner):did(Ahq...xCn):indy> did use CbPwHxKEibPhV4pgXWpu26
Did ""CbPwHxKEibPhV4pgXWpu26"" has been set as active
pool(p4):wallet(AS-03-wallet-owner):did(CbP...u26):indy> ledger nym did=CbPwHxKEibPhV4pgXWpu11 role=TRUSTEE
Nym request has been sent to Ledger.
Metadata:
+------------------------+-----------------+---------------------+---------------------+
| From                   | Sequence Number | Request ID          | Transaction time    |
+------------------------+-----------------+---------------------+---------------------+
| CbPwHxKEibPhV4pgXWpu26 | 44              | 1548685914743831922 | 2019-01-28 14:31:54 |
+------------------------+-----------------+---------------------+---------------------+
Data:
+------------------------+---------+
| Did                    | Role    |
+------------------------+---------+
| CbPwHxKEibPhV4pgXWpu11 | TRUSTEE |
+------------------------+---------+
{code}

 * This issue is actual only for NYM txns, pool restart and get-validator-info works well.
 * Note that attached expected result accords to previous version of batch, so some cases absent there.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,"28/Jan/19 11:44 PM;ozheregelya;AS-03-01-identity-owner-anyone-can-write.actual;https://jira.hyperledger.org/secure/attachment/16642/AS-03-01-identity-owner-anyone-can-write.actual","28/Jan/19 11:44 PM;ozheregelya;AS-03-01-identity-owner-anyone-can-write.batch;https://jira.hyperledger.org/secure/attachment/16641/AS-03-01-identity-owner-anyone-can-write.batch","28/Jan/19 11:45 PM;ozheregelya;AS-03-01-identity-owner-anyone-can-write.expected;https://jira.hyperledger.org/secure/attachment/16643/AS-03-01-identity-owner-anyone-can-write.expected",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00rf6:",,,,Unset,Unset,Ev-Node 19.12,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,,,,,,,,,,,"29/Jan/19 4:26 PM;ashcherbakov;This is a nasty bug for sure.
However, we are going to remove this flag in the scope of INDY-1930 and INDY-1956.;;;","14/Jun/19 11:20 PM;ashcherbakov;The flag is removed in the scope of INDY-1956;;;",,,,,,,,,,,,,,,,,,,,,,,
As a TRUST_ANCHOR I should be able to revoke myself,INDY-1968,37224,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,30/Jan/19 12:58 AM,11/Oct/19 8:48 PM,28/Oct/23 2:48 AM,11/Oct/19 8:48 PM,,1.13.0,,,,0,,,,For now TRUST_ANCHOR can't revoke themselves.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9x",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"11/Oct/19 8:48 PM;ashcherbakov;Done as part of configurable auth rules;;;",,,,,,,,,,,,,,,,,,,,,,,,
It is possible to repeat the removal of a role from an Identity Owner,INDY-1969,37225,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,andkononykhin,andkononykhin,30/Jan/19 1:07 AM,13/Feb/19 9:51 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,"If a role is removed multiple times, the request is accepted and in case of repeated actions leads to ""duplicates"" (only _reqId_ differs) of NYM txns in domain ledger.

*Steps to Reproduce*
* Add a steward role to a DID
* Remove the steward role from the DID (set the DID role to None)
* Remove the steward role from the DID again

*Actual Behavior*
* One transaction is written to the ledger setting the DID role to Steward
* One transaction is written to the ledger setting the DID role to None
* A second transaction is written to the ledger setting the DID role to None

*Expected Behavior*
* One transaction should be written to the ledger setting the DID role to Steward
* One transaction should be written to the ledger setting the DID role to None
* The second attempt to remove the Steward role should generate an error ""The specified DID does not have the role""

Example cases:
 * trustee creates steward without verkey - demotes it - demotes it again
 * trustee creates identity owner without verkey - demotes it (trustee here can be replaced with steward or trust_anchor)

As a special case:
 * cidentity owner (with verkey) demotes themselves

Notes:
* This was found while working on implementation of new authentication rules in INDY-1730
* In general, it should not be possible to write a transaction that does not modify the current state.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1730,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwx5c:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
As a TRUSTEE/STEWARD/TRUST_ANCHOR I should be able to promote identity owners,INDY-1970,37243,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,andkononykhin,andkononykhin,30/Jan/19 6:38 PM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,1.6.83,,,,0,,,,"Currently it's not possible to promote any existing DID which doesn't have any role (e.g. created without it or demoted before).

Need to decide whether it's worth to have and design the rules. ",,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1971,,INDY-1957,INDY-1730,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000004",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"31/Jan/19 5:45 PM;ashcherbakov;Duplicates INDY-1971;;;",,,,,,,,,,,,,,,,,,,,,,,,
A role that has been removed can't be added back,INDY-1971,37256,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Highest,Done,ozheregelya,ozheregelya,ozheregelya,30/Jan/19 11:13 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.6.83,,,,0,TShirt_M,,,"*Environment:*
 indy-node 1.6.772

*Steps to Reproduce:*
 1. Set yourself as default Trustee.
 2. Create new Steward.
 3. Remove the newly created Steward role.
 4. Try to add the role back to the same DID.

*Actual Results:*
{code:java}
pool(p4):wallet(w):did(V4S...e6f):indy> ledger nym did=DjCtFTFy6znNPtZ551GbLU verkey=~17hichoWuRFo8y6RCowjcR role=STEWARD
Nym request has been sent to Ledger.
Metadata:
+------------------------+-----------------+---------------------+---------------------+
| From | Sequence Number | Request ID | Transaction time |
+------------------------+-----------------+---------------------+---------------------+
| V4SGRU86Z58d6TV7PBUe6f | 29 | 1548856166008667735 | 2019-01-30 13:49:26 |
+------------------------+-----------------+---------------------+---------------------+
Data:
+------------------------+-------------------------+---------+
| Did | Verkey | Role |
+------------------------+-------------------------+---------+
| DjCtFTFy6znNPtZ551GbLU | ~17hichoWuRFo8y6RCowjcR | STEWARD |
+------------------------+-------------------------+---------+
pool(p4):wallet(w):did(V4S...e6f):indy> ledger nym did=DjCtFTFy6znNPtZ551GbLU role=
Nym request has been sent to Ledger.
Metadata:
+------------------------+-----------------+---------------------+---------------------+
| From | Sequence Number | Request ID | Transaction time |
+------------------------+-----------------+---------------------+---------------------+
| V4SGRU86Z58d6TV7PBUe6f | 30 | 1548856181047273998 | 2019-01-30 13:49:41 |
+------------------------+-----------------+---------------------+---------------------+
Data:
+------------------------+------+
| Did | Role |
+------------------------+------+
| DjCtFTFy6znNPtZ551GbLU | - |
+------------------------+------+
pool(p4):wallet(w):did(V4S...e6f):indy> ledger nym did=DjCtFTFy6znNPtZ551GbLU role=STEWARD
Transaction has been rejected: Request can not be authorized as action is not allowed: Request: {'signature': 'j36LLmsnrbHqBZ2UZUYeuJAvr2jBUKzV3qPk53d8pgGG8GF5zk7mSqYiW911wKM2JZThtQKvHLGvv5nRa2g2K13', 'protocolVersion': 2, 'reqId': 1548856194280186477, 'identifier': 'V4SGRU86Z58d6TV7PBUe6f', 'operation': {'type': '1', 'dest': 'DjCtFTFy6znNPtZ551GbLU', 'role': '2'}}{code}
*Expected Results:*
 * A role can be added back
 * The rules for adding the role back are the same as for adding a new NYM with the corresponding role
 * auth_map.md needs to be updated

*Notes*
 * A different DID can be created and given the role.",,,,,,,,,,,,,,,,,,,,,,,,INDY-1970,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000001ci",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),Derashe,ozheregelya,,,,,,,,,,,"30/Jan/19 11:28 PM;ozheregelya;For QA:
! Note that this fix will require update of expected results for [acceptance batches|https://github.com/hyperledger/indy-node/tree/master/acceptance/indy-cli-batches].;;;","05/Feb/19 1:58 AM;Derashe;PR: [https://github.com/hyperledger/indy-node/pull/1153]

INDY Node version: 782

Rec for QA: 

Test all NYM cases from this table 

https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md;;;","06/Feb/19 1:57 AM;ozheregelya;*Environment:*
indy-node 1.6.782

*Steps to Reproduce:*
1. Run all acceptance batches for roles.

*Actual Results:*
Roles work as expected. Minor issue for messages: INDY-1987.

Expected results of acceptance batches were updated, mistake in scenario for steward was fixed.  PR: https://github.com/hyperledger/indy-node/pull/1156;;;",,,,,,,,,,,,,,,,,,,,,,
Stub: Stability fixes for plugins catchup and UTXO cache,INDY-1972,37328,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,31/Jan/19 10:08 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"A stub for ST-499

https://sovrin.atlassian.net/browse/ST-499",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000006r20eo",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace INSTANCE_CHANGE with simplified Binary Byzantine Agreement,INDY-1973,37335,,Task,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,,sergey.khoroshavin,sergey.khoroshavin,31/Jan/19 11:34 PM,31/Jan/19 11:34 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1897,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a13:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),sergey.khoroshavin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for NYM,INDY-1974,37362,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,andkononykhin,andkononykhin,andkononykhin,01/Feb/19 4:58 PM,30/Mar/19 5:32 AM,28/Oct/23 2:48 AM,30/Mar/19 5:32 AM,,,,,,0,,,,"In scope of INDY-1957 integrations tests were added but they don't cover all possible cases.

Should check all cobinations of the following:
 * signer's role:  any of indy_common.roles.Roles
 * signer is: the same as dest, creator, other
 * dest's role in ledger: any of indy_common.roles.Roles
 * dest's role in NYM: omitted, any of indy_common.roles.Roles
 * dest's verkey in ledger: None, val
 * dest's verkey in NYM: omitted, same as in ledger, new",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xi",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"06/Feb/19 7:49 PM;ashcherbakov;https://github.com/hyperledger/indy-node/pull/1159;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for ATTRIB,INDY-1975,37363,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,Derashe,andkononykhin,andkononykhin,01/Feb/19 5:02 PM,24/May/19 6:28 PM,28/Oct/23 2:48 AM,24/May/19 6:28 PM,,1.8.0,,,,0,,,,Add tests to ATTRIB's Auth Rules (the same way as in Done in https://jira.hyperledger.org/browse/INDY-2070),,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw9609bj29",,,,Unset,Unset,Ev-Node 19.10,,,,,,,(Please add steps to reproduce),1.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,Derashe,,,,,,,,,,,"23/May/19 10:21 PM;Derashe;Problem reason/description: 
- Lack of auth_rule tests with attrib txn

Changes: 
- Attrib txn tests added

PR:
- [https://github.com/hyperledger/indy-node/pull/1308]

Risk factors:
- no

Risk:
- Low

 ;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for SCHEMA,INDY-1976,37364,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:05 PM,17/May/19 6:04 PM,28/Oct/23 2:48 AM,17/May/19 6:04 PM,,,,,,0,,,,"There are integration tests for NYM: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for SCHEMA.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a7j:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:04 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for CLAIM_DEF,INDY-1977,37365,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:06 PM,17/May/19 6:04 PM,28/Oct/23 2:48 AM,17/May/19 6:04 PM,,,,,,0,,,,"There are integration tests for NYM: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for CLAIM_DEF.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a7r:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:04 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for NODE txn,INDY-1978,37367,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:07 PM,17/May/19 6:04 PM,28/Oct/23 2:48 AM,17/May/19 6:04 PM,,,,,,0,,,,"There are integration tests for NYM txn: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for NODE txn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a87:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:03 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for POOL_UPGRADE txn,INDY-1979,37368,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:08 PM,17/May/19 6:04 PM,28/Oct/23 2:48 AM,17/May/19 6:04 PM,,,,,,0,,,,"There are integration tests for NYM txn: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for POOL_UPGRADE txn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a8f:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:03 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for POOL_RESTART txn,INDY-1980,37369,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:09 PM,17/May/19 6:04 PM,28/Oct/23 2:48 AM,17/May/19 6:04 PM,,,,,,0,,,,"There are integration tests for NYM txn: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for POOL_RESTART txn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a8n:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:03 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for POOL_CONFIG txn,INDY-1981,37370,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:10 PM,17/May/19 6:04 PM,28/Oct/23 2:48 AM,17/May/19 6:04 PM,,,,,,0,,,,"There are integration tests for NYM txn: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for POOL_CONFIG txn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a8v:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:03 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
Integration tests to verify auth rules for VALIDATOR_INFO txn,INDY-1982,37371,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,01/Feb/19 5:12 PM,17/May/19 6:02 PM,28/Oct/23 2:48 AM,17/May/19 6:02 PM,,,,,,0,,,,"There are integration tests for NYM txn: [https://github.com/hyperledger/indy-node/blob/master/indy_node/test/nym_txn/test_nym_auth_rules.py]

Need similar ones for VALIDATOR_INFO txn.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1957,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00a93:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,,,,,,,,,,,"17/May/19 6:01 PM;ashcherbakov;Done in https://jira.hyperledger.org/browse/INDY-2070;;;",,,,,,,,,,,,,,,,,,,,,,,,
A Node need to be able to order stashed requests after long catch-ups,INDY-1983,37374,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,ashcherbakov,ashcherbakov,01/Feb/19 7:09 PM,15/Apr/19 5:01 PM,28/Oct/23 2:48 AM,15/Apr/19 5:01 PM,,1.7.1,,,,0,,,,"*Problem*
 * A node is doing a long catchup (>10mins) whike the pool continues ordering
 * The node successfully stashes all 3PC messages received during catch-up and tries to re-apply them once catch-up is finished
 * However, since catch-up took more than 10 mins, the current time on the node will be at least 10 mins greater than the one in stashed PrePrepares.
 * So, the PrePrepare will be discarded as having incorrect time.

*Acceptance criteria*
 * Write tests simulating the issue
 * The PrePrepares processed (unstashed) after the catch-up should be discarded or applied because of incorrect time the same way as if there were no catch-up.

*Possible fix*
 * Create a map PrePrepare -> receiving_time
 * Every PrePrepare stashed during the catch-up will be put there
 * Once a PrePrepare is being processed, we need to have a look if there is received time for it in the map, and, if so, pop it and use for the ppTime validation
 * Make sure that the map is cleared properly in GC. so there is no OOM",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2052,,,,,,,,,,,,,,,"12/Apr/19 9:41 PM;VladimirWork;1983_fail_case.tar.gz;https://jira.hyperledger.org/secure/attachment/17093/1983_fail_case.tar.gz","05/Feb/19 3:59 PM;VladimirWork;INDY-1983_04_02_2019.PNG;https://jira.hyperledger.org/secure/attachment/16717/INDY-1983_04_02_2019.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1377,,,No,,Unset,No,,,"1|hzwvif:000006r20ci",,,,Unset,Unset,Ev-Node 19.07,Ev-Node 19.08,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,ashcherbakov,VladimirWork,,,,,,,,,,"05/Feb/19 3:53 PM;VladimirWork;Build Info:
indy-node 1.6.779

Steps to Reproduce:
1. Run load test with 1 writing txn per second and 10 reading txns per second (1/10 of production load with the same txn types).
2. Stop several nodes (Node20 and Node25).
3. Stop the primary (Node1) to initiate View Change.
4. Start stopped nodes (one by one, including primary).
5. Stop the load.
6. Wait for the end of catch up on nodes which were stopped.
7. Start the same load for a short period to check ordering.

Actual Results:
There is the same issue as for full production load (also it looks like Node 25 has started ordering at Step 7 but Node 1 and Node 20 have not).
ev@evernymr33:logs/1983_04_02_2019_metrics.tar.gz
ev@evernymr33:logs/1983_04_02_2019_logs.tar.gz;;;","04/Apr/19 5:52 PM;andkononykhin;*PoA*
 # receiving timestamp is attached as *ts_rcv* attribute for any message read from network interface
 ** there would be a wrapper class (e.g. ZStackMessage) that hodls raw message and network metadata like sender and receiving timestamp
 # this value passed to upper stacks which handles incoming messages callbacks
 # Node class uses that value to associate with any node messages (including PrePrepares)
 ** there would another wrapper class that holds object presentation of the parsed message (MessageBase child) and network related metadata mentioned upper
 ** that would replace widely used tuples (msg, from)
 # once a PrePrepare is being processed that timestamp is used for estimation of message  obsolescence instead of now();;;","11/Apr/19 8:02 PM;andkononykhin;Problem reason:
 * stashed PrePrepares (e.g. received during catchup or view change) becomes obsolete if stashing time  is bigger then acceptable deviation time for PP
 * such case is wrong if incoming PP is not obsolete at the time when it comes

Changes:
 * PP obsolescence is estimated basing on the time it comes to replica instead of the time when the estimation happens

PR:
 * [https://github.com/hyperledger/indy-plenum/pull/1155]

Version:
 * indy-plenum: 1.7.0.dev754​
 * indy-node: 1.7.0.dev891

Risk factors:
 * nothing expected

Risk:
 * Low

Covered with tests:
 * [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence.py]
 * [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_fail_for_delayed.py]
 * [https://github.com/hyperledger/indy-plenum/blob/master/plenum/test/node_request/test_pre_prepare/test_pp_obsolescence_check_pass_for_stashed.py]

Recommendations for QA:
 * start test pool setting ACCEPTABLE_DEVIATION_PREPREPARE_SECS config parameter to lower value for one node
 * stop that node while other nodes are ordering
 * start the node after some time expecting that the node will perform catchup more than the time set for the mentioned config parameter
 * send set of txns
 * once the node completes catch up check that it's logs don't include records about suspicions on nodes with suspicion code 18;;;","12/Apr/19 9:40 PM;VladimirWork;Build Info:
indy-node 1.7.0~dev891

Steps to Reproduce:
1. Start test pool setting ACCEPTABLE_DEVIATION_PREPREPARE_SECS config parameter to lower value for one node (60).
2. Stop that node while other nodes are ordering (1 txn/sec).
3. Start the node after some time expecting that the node will perform catchup more than the time set for the mentioned config parameter
send set of txns.

Actual Results:
Node perfroms catch up under this load and orders but there are some suspicions codes appear (including code 18):
{noformat}
2019-04-12 11:47:25,275|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,280|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,281|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,288|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,289|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,295|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,295|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,303|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,303|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,310|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,311|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,317|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:25,318|WARNING|node.py|Node3 raised suspicion on node Node2 for PRE-PREPARE time not acceptable; suspicion code is 18
2019-04-12 11:47:29,526|WARNING|node.py|Node3 raised suspicion on node Node4 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:47:29,526|WARNING|node.py|Node3 raised suspicion on node Node1 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:47:29,529|WARNING|node.py|Node3 raised suspicion on node Node4 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:47:29,530|WARNING|node.py|Node3 raised suspicion on node Node1 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:47:29,533|WARNING|node.py|Node3 raised suspicion on node Node4 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:47:29,533|WARNING|node.py|Node3 raised suspicion on node Node2 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:47:29,534|WARNING|node.py|Node3 raised suspicion on node Node1 for COMMIT message has already received; suspicion code is 8
2019-04-12 11:54:24,646|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20
2019-04-12 11:54:24,646|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20
2019-04-12 11:54:27,371|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20
2019-04-12 11:54:27,371|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20
2019-04-12 11:57:43,736|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20
2019-04-12 11:57:43,736|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20
2019-04-12 11:57:44,141|WARNING|node.py|Node3 raised suspicion on node Node1 for Pre-Prepare message has incorrect reject; suspicion code is 20
2019-04-12 11:57:44,142|NOTIFICATION|node.py|VIEW CHANGE: Node3 got one of primary suspicions codes 20
{noformat}

Info logs:
 [^1983_fail_case.tar.gz] 

Debug logs:
ev@evernymr33:logs/1983_DEBUG.tar.gz;;;","13/Apr/19 1:54 AM;andkononykhin;Debug logs exploration results:
 # all PP timestamps checks failed on Node3:1 backup replica
 # view change didn't take place
 # suspicions for incorrect PP time were raised 103 times (lowest ppSeqNo = 242, highest = 561):
 ## *ppSeqNo=242*:
 *** *ts diff ~199* secs (sent by Node2:1 at 13:11:21, stashed due to catchup by Node3:1 at 13:14:40)
 *** *suspicion* was raised
 ## *ppSeqNo=401* (as a middle of the range):
 *** *ts diff ~112* secs (sent by Node2:1 at 13:13:10, stashed due to catchup by Node3:1 at 13:15:02)
 *** *suspicion* was raised
 ## *ppSeqNo=561*:
 *** *ts diff ~61* secs (sent by Node2:1 at 13:17:20, reported as received by Node3:1 at 13:18:21)
 *** *suspicion* was raised
 ## I checked also *ppSeqNo=563*:
 *** *ts diff ~59* secs (sent by Node2:1 at 13:17:22, reported as received by Node3:1 at 13:13:21)
 *** *no suspicion* was raised

Keeping in mind that pp obsolescence interval  (ACCEPTABLE_DEVIATION_PREPREPARE_SECS) was set to 60 secs I may say that all mentioned cases fit designed logic.;;;","13/Apr/19 2:08 AM;andkononykhin;[~ashcherbakov] [~VladimirWork] Please take a look at the comment above. I didn't find any issues related to the task and I don't think that it is necessary to tune the test scenario since in any case we would have probability of getting suspicions in logs. Seems the original recommendation for QA was not enough accurate: the task's goal was not to get rid of PP time suspicions but to make the PP times check more robust. Thus, the current tests results and the exploration I made are seemed as good verification for the task. Do you think we need more QA work here?;;;",,,,,,,,,,,,,,,,,,,
INSTANCE_CHANGE messages should be persisted between restarts,INDY-1984,37382,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ozheregelya,sergey.khoroshavin,sergey.khoroshavin,02/Feb/19 12:01 AM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.7.1,,,,0,,,,"As a result of investigation done in scope of INDY-1897 it was found that one way to avoid situations like in INDY-1903 is to actually persist INSTANCE_CHANGE messages between nodes restarts. However this can potentially bring problems if viewNo is not persisted as well, so this can be potentially blocked by INDY-1946.

*Acceptance criteria*
* Write tests checking for edge cases which can arise when INSTANCE_CHANGE messages are persisted and viewNo is not
* If it turns out that in order to pass these tests INDY-1946 is required then wait until this issue is done
* Implement and test persisting of received INSTANCE_CHANGE messages to some local database
* (Optional) Implement and test garbage collection of old INSTANCE_CHANGE messages so that database doesn't grow indefinitely",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1897,,,,,INDY-1903,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1376,,,No,,Unset,No,,,"1|hzwvif:000005",,,,Unset,Unset,Ev-Node 19.04,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,sergey.khoroshavin,Toktar,,,,,,,,,,"12/Feb/19 1:18 AM;sergey.khoroshavin;Required additional integration *tests scenarios*

*Configuration*
* Set primary disconnection timeout to some large value, so that view change is not triggered by primary restarts
* Set freshness check interval to 20-30 seconds, so that it will happen during test

*Scenario 1*
* Beta and Gamma send InstanceChange for all nodes.
* Restart Alpha
* Restart Gamma and Delta
* Send InstanceChange from Delta for all nodes
* At this point Delta will enter view change, while all other nodes continue normal ordering (unless InstanceChanges are persisted)
* Ensure that pool is still functional (ordering some transactions in process)
* Beta and Gamma send InstanceChange for all nodes again, which will lead to view change by other nodes
* Ensure that all nodes have same data and pool is functional (this will most probably fail)

*Scenario 2*
* Beta and Gamma send InstanceChange for all nodes.
* Restart Alpha (while setting primary disconnection timeout to some large value, so that view change is not triggered)
* Restart Gamma and Delta
* Send InstanceChange from Delta for all nodes
* At this point Delta will enter view change, while all other nodes continue normal ordering (unless InstanceChanges are persisted)
* Ensure that pool is still functional (ordering some transactions in process)
* Beta send InstanceChange for all nodes again
* Restart Alpha and Gamma
* Send InstanceChange from Gamma for all nodes
* At this point Gamma will enter another view change, leaving two nodes in view change which breaks consensus
* Ensure that all nodes have same data and pool is functional (this will most probably fail)
;;;","13/Feb/19 10:13 PM;Toktar;*PoA:*
 Instance Change messages are lost after a node restart. As a result, a restarted node can't start view change and will not order batches.
 This leads to case when a pool can't order because more then f+1 nodes can't order.
 To solve this problem InstanceChanges will store in a KeyValueStorageType.Rocksdb.
 InstanceChanges should be store in the follow structure:
{code:java}
{ <view_no>: {
              <node_name>: {<timestamp>,
                            <reason>},
              <node_name>: {<timestamp>,
                            <reason>},
             },
  ...
}{code}
+First option.+ Store serialized InstanceChangesVotes in Rocksdb and deserialize it in every case when we need to read value.
 +Second option.+ Deserialize InstanceChangesVotes from Rocksdb only in a node initialization and save it in an internal structure.

*Implementation.*
 Add two tests from the comment above.
 Create InstanceChangeProvider with public functions as in models.InstanceChanges:
 * add_vote
 * has_view
 * has_inst_chng_from
 * has_quorum
 *  + remove_votes - to remove votes for all last view before View Change

Implement in InstanceChangeProvider one of options to store InstanceChange messages and to remove messages when the quorum has been reached or InstanceChange was received too long ago.
 Integrate InstanceChangeProvider in the current system. Change calls models.InstanceChanges methods to calls same InstanceChangeProvider methods.;;;","20/Feb/19 9:38 PM;Toktar;*Problem reason:*
 - Nodes lose InstanceChange messages in restart and can't start a view change.

*Changes:*
 - Store  InstanceChange messages in nodeStatusDB

*PR:*
 * [https://github.com/hyperledger/indy-node/pull/1178]
 * [https://github.com/hyperledger/indy-plenum/pull/1087]

*Version:*
 * indy-node 1.6.811 -master
 * (indy-plenum 1.6.690 -master)

*Risk factors:*
 - More than f nodes will start view change and can't finish it.

*Risk:*
 - Medium

*Test:*
 * [test_vc_started_in_different_time.py|https://github.com/hyperledger/indy-plenum/pull/1087/files#diff-fe1d1e3ed51dc1314a76d1256ee9aa0e]
 * [test_vc_finished_when_less_than_quorum_started.py|https://github.com/hyperledger/indy-plenum/pull/1087/files#diff-683019ed07c17671377ff7b2bb70597d]

*Recommendations for QA:*

Docker pool for 4 nodes (can scale to more).
 * Node3, Node4 send InstanceChange with any reason
 * Restart Node2
 * Restart Node3
 * Restart Node4
 * Node2 send InstanceChange with any reason
 * Check that pool done view change after some time(a few minutes).;;;","22/Feb/19 1:56 AM;ozheregelya;*Environment:* 
indy-node 1.6.819

*Steps to Validate:*
1. Setup the pool.
2. Send n-f-1 instance changes.
3. Restart more than f+1 nodes.
4. Send 1 more instance change.

*Actual Results:*
View change was happened.;;;",,,,,,,,,,,,,,,,,,,,,
PrePrepares stashed in a catchup can't process after,INDY-1985,37383,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,Toktar,Toktar,02/Feb/19 12:34 AM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,1.7.1,,,,0,,,,"*Problem:*
 * Load acceptance test 
 * A node restarted
 * The node doing catchups one by one and can't order.

*Actual Results:*
 * The node can't order because the first stashed PrePrepair discarded due to
_'Pre-Prepare message has incorrect state trie root; suspicion code is 21'_

*Expected Results:*
 * The node doing start catchup and propagate primary
 * Stashed in catchup messages are successfully unstased and processed
 * The node doesn't need a catchup and starts usual ordering

*Logs:*
 * ev@evernymr33:logs/1965_01_02_2019_metrics.tar.gz
 * ev@evernymr33:logs/1965_01_02_2019_logs.tar.gz",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1955,INDY-1965,,,,,,,,,,,,,,,,,,,"06/Feb/19 9:18 PM;VladimirWork;INDY-1985_06_02_2019_bad_25th_node.PNG;https://jira.hyperledger.org/secure/attachment/16727/INDY-1985_06_02_2019_bad_25th_node.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1032,,,No,,Unset,No,,,"1|hzwvif:000001d",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,Toktar,VladimirWork,,,,,,,,,,"05/Feb/19 6:14 PM;VladimirWork;Build Info:
indy-node 1.6.779

Steps to Reproduce:
1. Run load test with 1 writing txn per second and 10 reading txns per second (1/10 of production load with the same txn types).
2. Stop several nodes (Node20 and Node25).
3. Stop the primary (Node1) to initiate View Change.
4. Start stopped nodes (one by one, including primary).
5. Stop the load.
6. Wait for the end of catch up on nodes which were stopped.
7. Start the same load for a short period to check ordering.

Actual Results:
There is the same issue as for full production load (also it looks like Node 25 has started ordering at Step 7 but Node 1 and Node 20 have not).
ev@evernymr33:logs/1983_04_02_2019_metrics.tar.gz
ev@evernymr33:logs/1983_04_02_2019_logs.tar.gz;;;","05/Feb/19 8:03 PM;ashcherbakov;*Problem reason*
 * This is caused by the fact that
 ** we write to multiple ledgers (domain and payment) during catch-up
 ** catch-up is not synced between ledgers
 ** last_ordered_3pc is set according to the last caught-up ledger
 * So, we caught-up domain ledgers first till last_ordered_3pc=(0, X), and then payment ledger till last_ordered_3pc=(0, X+Y).
 * Last ordered is set to (0, X+Y)
 * Once we try apply stashed 3pc messages for domain ledger, we will discard all 3pc messages below (0, X+Y), although they are not actually ordered
 * Once we start applying (0,X+Y+1) PrePrepare, state root will not be equal, since we actually have a gap

*Fixes*
 * Audit Ledger will synchronize the catch-up for all the ledgers and the issue will gone
 * It will be done in the scope of INDY-1945

 

*Proof from the logs about the reason:*

2019-02-04 14:33:14,755|INFO|node.py|Node25 is updating txn to batch seqNo map after catchup to *(1, 9562)* for *ledger_id 1*

2019-02-04 14:33:14,755|INFO|ledger_manager.py|CATCH-UP: Node25 completed catching up ledger 1, caught up 530 in total

.....
2019-02-04 14:35:44,476|INFO|node.py|Node25 is updating txn to batch seqNo map after catchup to *(1, 9712)* for *ledger_id 1001* 

2019-02-04 14:35:44,476|INFO|ledger_manager.py|CATCH-UP: Node25 completed catching up ledger 1001, caught up 480 in total
2019-02-04 14:35:44,476|INFO|node.py|Node25 caught up to 1010 txns in the last catchup

2019-02-04 14:35:44,476|INFO|replica.py|Node25:0 set last ordered as *(1, 9712)*

.....

2019-02-04 14:35:46,640|WARNING|node.py|Node25 raised suspicion on node Node2 for Pre-Prepare message has incorrect state trie root; suspicion code is 21;;;","05/Feb/19 11:00 PM;ashcherbakov;*Recommendation for QA*

In order to double-check the facts, I recommend the following load test:
 * Acceptance load with domain txns only (all domain txns): 1 txn per sec
 * Acceptance load with domain txns only (all domain txns): 10 txns per sec;;;","06/Feb/19 9:18 PM;VladimirWork;Build Info:
indy-node 1.6.781

Steps to Validate:
1. Acceptance load with domain txns only (all domain txns): 1 txn per sec + stop 20, 25 then stop primary then start all stopped.
2. Acceptance load with domain txns only (all domain txns): 10 txns per sec + stop 20, 25 then stop primary then start all stopped.

Actual Results:
1. All nodes catched up and ordered successfully. (/)
2. All nodes catched up and ordered successfully *except Node 25 that didn't catch up and didn't order*. (!) Logs and metrics:
ev@evernymr33:logs/1985_06_02_2019_metrics.tar.gz
ev@evernymr33:logs/1985_06_02_2019_logs.tar.gz
 !INDY-1985_06_02_2019_bad_25th_node.PNG|thumbnail! ;;;","06/Feb/19 11:09 PM;ashcherbakov;For some reasons, Node25 was disconnected from most of the nodes after restart, so that it wasn't able to start operating.

So, the initial issues is not reproduced (that proves the theory).;;;","06/Feb/19 11:16 PM;VladimirWork;Second case will be re-tested in scope of INDY-1945 after audit ledger implementation.;;;",,,,,,,,,,,,,,,,,,,
Investigate reasons of long VCs,INDY-1986,37439,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Won't Do,,VladimirWork,VladimirWork,05/Feb/19 9:47 PM,25/Oct/19 9:06 PM,28/Oct/23 2:48 AM,25/Oct/19 9:06 PM,,,,,,0,,,,"Build Info:
indy-node 1.6.782

Steps to Reproduce:
1. Run the pool of 4 nodes.
2. Force VC by primary stopping.
3. Start ex-primary back and check that pool can write and read nyms.
4. Force VC by primary demoting.
5. Promote ex-primary back and check that pool can write and read nyms.
6. Repeat steps 2-5 and check VC_in_progress time for each step.

Actual Results:
There are very long VCs during multiple test runs about 5-10 minutes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/19 11:23 PM;VladimirWork;1986_logs_and_metrics.tar.gz;https://jira.hyperledger.org/secure/attachment/16720/1986_logs_and_metrics.tar.gz","05/Feb/19 11:16 PM;VladimirWork;VC_wrong_backup_primary_at_1_node.PNG;https://jira.hyperledger.org/secure/attachment/16719/VC_wrong_backup_primary_at_1_node.PNG",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00akn:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,VladimirWork,,,,,,,,,,,"05/Feb/19 11:16 PM;VladimirWork;Also after 2 test runs (2-5) Node 2 has Node 3 as backup primary but all other nodes have Node 2 as backup primary: !VC_wrong_backup_primary_at_1_node.PNG|thumbnail! ;;;","25/Oct/19 9:06 PM;ashcherbakov;PBFT View Change is implemented to address the issue;;;",,,,,,,,,,,,,,,,,,,,,,,
Not consistent messages for not permitted actions,INDY-1987,37444,,Bug,New,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Low,,,ozheregelya,ozheregelya,06/Feb/19 1:01 AM,07/Feb/19 10:54 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,"*Environment:*
 indy-node 1.6.782

*Steps to Reproduce:*
 1. Run any acceptance batch for roles.

*Actual Results:*
 If operation is allowed only for only one user (i.e. for TRUSTEE), the message differs form the rest ones:
{code:java}
did use T6XTs3nSU3J7ptAcxSnaVo
Did ""T6XTs3nSU3J7ptAcxSnaVo"" has been set as active

- ledger nym did=WDLETDtBugFiJvtkghHoH role=TRUST_ANCHOR
Transaction has been rejected: Rule for this action is: 1 STEWARD signature is required OR 1 TRUSTEE signature is required

- ledger nym did=WDLETDtBugFiJvtkghHoH role=STEWARD
Transaction has been rejected: TRUST_ANCHOR can not do this action

- ledger nym did=WDLETDtBugFiJvtkghHoH role=TRUSTEE
Transaction has been rejected: TRUST_ANCHOR can not do this action

- ledger nym did=UR1jYsBfADbYK5FZoF76Mh role=NETWORK_MONITOR
Transaction has been rejected: Rule for this action is: 1 STEWARD signature is required OR 1 TRUSTEE signature is required
{code}
*Expected Results:*
 Messages like '<role> can not do this action' should be changed in consistency with the rest ones, e.g. 'Rule for this action is: 1 TRUSTEE signature is required'.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00alj:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ozheregelya,,,,,,,,,,,,"07/Feb/19 10:54 PM;ozheregelya;*Note for QA:* 
Need to double check expected results for acceptance batches for roles and update ones for nodes adding after fix of this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,
TRUSTEE cannot change trust anchor's nym role on ledger from TRUST_ANCHOR to <None>,INDY-1989,37460,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,,sklump,sklump,07/Feb/19 1:48 AM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,,plenum,,,0,node-SDK,quality,,"According to
https://github.com/hyperledger/indy-node/blob/master/docs/source/auth_rules.md

the TRUSTEE should be able to reset the role in an existing cryptonym on the ledger from TRUST_ANCHOR to <None>.

Instead, the transaction fails with error:
(1012) Ledger rejected transaction request: client request invalid: UnauthorizedClientRequest('actor must be owner',)

This behaviour is new. From the indy-pool docker files shipping in indy-sdk/ci/, 

Works:
ARG indy_plenum_ver=1.6.641
ARG indy_anoncreds_ver=1.0.32
ARG indy_node_ver=1.6.740
ARG python3_indy_crypto_ver=0.4.5
ARG indy_crypto_ver=0.4.5

Broken:
ARG indy_plenum_ver=1.6.656
ARG indy_anoncreds_ver=1.0.32
ARG indy_node_ver=1.6.761
ARG python3_indy_crypto_ver=0.4.5
ARG indy_crypto_ver=0.4.5
","ubuntu 16.04
indy-sdk 1.7.0-dev-955 or higher (same result on indy-sdk 1.8.0-dev-965)",28800,0,,0%,28800,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00ap3:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),esplinr,sklump,,,,,,,,,,,"07/Feb/19 11:11 PM;sklump;node 1.6.785 with plenum 1.6.672 fixes.

Kindly update indy-sdk/indy-pool.dockerfile and then close this ticket.;;;","18/Feb/19 11:59 PM;esplinr;Thanks [~sklump] for reporting the problem.

This issue only existing on Master as a result of our multi-signature work. We were unable to reproduce this issue on 1.6.83.;;;",,,,,,,,,,,,,,,,,,,,,,,
DOC: Request for release notes on Indy-node 1.6.83,INDY-1990,37523,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,mgbailey,VladimirWork,VladimirWork,08/Feb/19 6:55 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,,,,,0,,,,"*Version Information:*
indy-node 1.6.83
indy-plenum 1.6.58
sovrin 1.1.35

*Notices for Stewards:*
(!) *There are possible OOM issues during 3+ hours of target load or large catch-ups at 8 GB RAM nodes pool so 32 GB is recommended.*
(!) *Pool upgrade to sovrin 1.1.32 and above should be performed simultaneously for all nodes due to txn format changes.*

*Major Fixes:*
INDY-1922 - validator-info to client times out if there are many upgrade attempts by node
INDY-1919 - Node on Sovrin TestNet did not upgrade automatically
INDY-1918 - Node that does not upgrade spams the config ledger
INDY-1953 - Incorrect pool upgrade txn validation
INDY-1920 - Upgrade appears to have broken ""validator-info --nagios""
INDY-1955 - Node can't order after view change and catch up
INDY-1963 - Unclear error messages when Trustee send a NYM with the same verkey
INDY-1971 - A role that has been removed can't be added back

*Changes and Additions:*
INDY-1914 - Limit the number of attributes in schema
INDY-1836 - Enable Clear Request Queue strategy
INDY-1876 - A Node needs to be able to order requests received during catch-up
INDY-1916 - Network maintenance role
INDY-933 - There should always be fresh enough signature of a state
INDY-1949 - Node stops working without any services failure
INDY-1928 - As a user of Valdiator Info script, I need to know whether the pool has write consensus and when the state was updated the last time
INDY-1528 - Trust anchor permission not needed for ledger writes

*Known Issues:*
INDY-1965 - Some of the nodes lagging behind during the load test",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00b2f:",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),TechWritingWhiz,VladimirWork,,,,,,,,,,,"09/Feb/19 3:18 AM;TechWritingWhiz;[~VladimirWork] The current format of the Sovrin Release Notes includes sections for : `Major Fixes` and `Changes-Additions-Known Issues.`  The items in this ticket are all lumped together and listed as ""Changes."" Please separate them out appropriately so that each section may be clear and correct. 

 

[~krw910]

 ;;;","11/Feb/19 5:23 PM;VladimirWork;[~TechWritingWhiz] Done.;;;",,,,,,,,,,,,,,,,,,,,,,,
Failed indy-node system tests on jenkins,INDY-1991,37525,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,andkononykhin,andkononykhin,andkononykhin,08/Feb/19 9:58 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,,,,,0,devops,TShirt_S,,"System tests fails in indy-node CD pipeline ([https://ci.evernym.com/job/Indy-Node/job/Sovrin%20Node/job/master/787/console):]

{{The following packages have unmet dependencies: indy-node : Depends: indy-plenum (= 1.6.672) but it is not going to be installed}}

Seems it is related to recently released version of indy-crypto which is installed in system tests without explcite version specified.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xr",,,,Unset,Unset,Ev-Node 19.03,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"08/Feb/19 9:59 PM;andkononykhin;PoA:
 * updated CD pipeline to use explicit versions of indy-crypto and indy-sdk packages
 * versions should be the same that are specified in setup.py files of indy-node and indy-plenum 
 * verify on Jenkins;;;","08/Feb/19 10:02 PM;andkononykhin;PRs:
 * [https://github.com/hyperledger/indy-node/pull/1160]
 * [https://github.com/hyperledger/indy-node/pull/1161];;;","08/Feb/19 10:24 PM;andkononykhin;*Problem reason*:
 * system tests failed in CD pipeline for master branch

*Changes*:
 * added logic of resolving indy-sdk and indy-crypto versions from setup.py of indy-node and correspondent indy-plenum
 * use the versions during test environment installation routine
 * hardened logic of docker pool start

*Committed into*:

[https://github.com/hyperledger/indy-node/pull/1160|https://github.com/hyperledger/indy-node/pull/1029]

https://github.com/hyperledger/indy-node/pull/1161

*Risk factors*:

Changes impacts release logic.

*Risk*:

Medium

*Covered with tests*:

no, was tested manually on jenkins server

*Recommendations for QA*:
 * no acceptance is expected;;;",,,,,,,,,,,,,,,,,,,,,,
Implementation (not active): As a user/steward I want to have better understanding of release version and changelog ,INDY-1992,37528,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,,andkononykhin,andkononykhin,08/Feb/19 10:43 PM,25/Nov/20 2:02 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,1.7.1,,,,0,devops,,,"Currently release versions of indy-node looks like semver but actually doesn't follow it having build numbers as patch part of the version. 

Requirements:
 * 3rd number of the version should have a real patch meaning (follow semver)
 * versions of different branches of the code should be clearly related
 * indy-node should be easily linked to indy-plenum version for both dev and stable code
 * there should be clear relation between artifacts and code",,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-1652,,,,,INDY-1733,INDY-2019,INDY-2026,INDY-2037,INDY-2038,INDY-2039,INDY-2040,INDY-2041,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00000u",,,,Unset,Unset,Ev-Node 19.03,Ev-Node 19.04,Ev-Node 19.05,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),andkononykhin,,,,,,,,,,,,"12/Feb/19 10:29 PM;andkononykhin;A kind of summary of INDY-1652 design adapted for  indy-node and indy-plenum
h2. Drawbacks of current process
h3. branching and tags
 # *Stable branch is merged with not yet QA verified code* (QA test release candidate when release branch has been already merged into stable)
 # *Heve to bother with hotfixes in case of multiple release candidates to not freeze the master during release time*  (if release fails we need to create new rc branch, fix master and repeat the process, thus if we want to have just fixes we have to freeze master until release is finished, otherwise new rc might include some other changes besides fixes)
 # *There is no way to group git tags of the same release process* (multiple release candidates for the same release: they refer to different will refer to different release versions because of jenkins build number)

h3. Build numbers mess things:
 # *source code itself can’t be correlated with the version* (except you has checkouted by tag)
 # *Master and stable versions are loosely correlated* (Hard to say how to apply release notes to master)
 # *Master and stable have quite big diff* (which mostly is caused by different namings for migrations scripts which include version numbers in their names)
 # *Have to use different package names for master and stable indy-plenum*

h3. Release notes chaos
 # *Both indy-plenum and Indy-node changes are mixed and documented in sovrin-foundation/sovrin*
 # *No separate changelog for indy-plenum and indy-node*

h2. What we want to keep
 * indy-node should be easily linked to indy-plenum version for both dev and stable code
 * there should be clear relation between artifacts and code
 * keep the development and release processes simple
 ** there should be as less manual work as possible
 ** all branching (and other GitHub) routine should be very straightforward to reduce number of human mistakes

h2. What we want to improve
h4. versioning
 * all parts of version related to source code have to presented in the source code repository
 * 3rd number of the version should have a real patch meaning (follow semver)
 * build related numbers shouldn't be presented in the source code repository
 * versions of different branches of the code should be clearly related
 * stable should include only code that is really tested using auto tests and accepted by QA
 * codebase shouldn't be change between QA approval and final release steps (git tagging, official artifacts publishing)

(nice to have)
 * each installed package should be self-explained regarding its version
 * Jira integration (names of branches, commit comments)

h2. Proposed changes
h3. branching
 * master: development
 * release (rc): release preparation and testing
 * stable: releases

h3. tags
 * created only for stable and rc

h3.  versioning
 * X, Y, Z - MAJOR, MINOR, PATCH
 * N - source revision number (release candidate number, master increment for merges)
 ** starts from 1
 ** not for stable
 ** incremented each (and ONLY) time related source code base branch receives update
 * R - package revision number
 ** incremented each time packaging process replayed
 * B - pipeline build number
 * SHA1 - sha1 for accordant source code state

h3. source code: semver
 - master: X.Y.0-N
 - rc: X.Y.Z-rc.N
 - stable: X.Y.Z

h3. packaging: semver compartible (as close as package repository type allows)
 - PyPi:
 ** master: X.Y.0.devN
 ** rc: X.Y.Z.rcN
 ** stable: X.Y.Z
 - debian:
 ** master: X.Y.0~N
 ** rc: X.Y.Z~rc.N
 ** stable: X.Y.Z

h3. indy-node & indy-plenum correlation
 - indy-plenum is released independently (doesn't wait for QA approval)
 - indy-node refers only to 'indy-plenum' package as python dependency

h3. development workflow
 - version: X.Y.0-N
 ** ??? N starts from 1 and is incremented automatically each time new merge to master happens
 ** Z is 0 since usually we don't expect that all changes in next release are about bugfixes only,
 thus we increment at least Y (X is possible as well if major changes are planned) compared to last release

 - development cycle
 ** start:
 *** initial commit: version is bumped to X.Y.0-1
 ** in-progress:
 *** PRs are merged
 *** N is incremented automatically for each commit by bot user from pipeline
 (commits from bot user are skipped)
 *** changelog is updated in top 'dev' part
 ** finish:
 *** no finalization is expected, new development cycle just starts with initial commit

h3. release workflow
 - version: X.Y.0[-rc.N]
 ** ??? N starts from 1 and might be incremented automatically for release branches (as for master)
 ** Z is 0 meaning initial release for X.Y, Z > 0 implies hotfix workflow which is slightly different and
 is described below

 - release process:
 ** initial phase
 *** initial commit tomaster for new development cycle is created
 *** branch `release-X.Y.0` is created from the master's SHA1 previous to the just created
 *** initial commit for release: bump version to X.Y.0-rc.1
 *** changelog is updated: dev replaced with X.Y.0
 *** tag is created: vX.Y.0-rc.1
 ** QA phase, repeatable
 *** CD: rc packages are published
 *** QA approves/denies
 *** if issues:
 **** PRs are created to release branch
 **** release notes are updated in X.Y.Z header
 **** finally version is invremented: X.Y.0-rc.(N+1)
 **** new tag is created: vX.Y.0-rc.(N+1)
 **** repeate QA phase
 *** else:
 **** done here, go to final phase
 ** Final phase
 *** merge release branch to stable with fast-forward only: --ff-only X.Y.0-rc.N
 *** bump version to X.Y.0
 *** add tag vX.Y.0
 *** push to origin:
 *** PR would be marked as merged
 *** stable CD should be triggered and packages are created and published
 *** merge to master (resolve possible conflicts in changelog and version)
 *** delete release branch

h3. hotfix workflow


 - version: X.Y.(Z+1) [-rc.N] 
 ** ??? N starts from 0 since no changes have been pushed yet
 - release process:
 ** initial phase
 *** branch `hotfix-X.Y.0` is created from tag X.Y.Z
 *** initial commit for release: bump version to X.Y.(Z+1)-rc.0
 *** changelog is updated: X.Y.(Z+1) is added
 *** tag is not created
 *** push to origin
 ** bugfixing phase
 *** fix commits are added (TODO PRs or not)
 *** release notes are updated in X.Y.(Z+1)
 *** when ready for QA:
 **** version is bumped to X.Y.(Z+1)-rc.1
 **** tag vX.Y.(Z+1)-rc.1 is created
 ** QA phase, repeatable
 *** the same as for release workflow
 ** Final phase
 *** the same as for release workflow

h2. Migration from current state


h3. indy-plenum
 # synchronize master and stable
 ** migrations should keep stable numbers
 ** new version should be 1.7.0 (to decrease possibility of messing master and stable versions)
 # adjust metadata file for new version scheme
 #  changelog: add and start to manage

h3. indy-node
 # synchronize master and stable
 ** indy-plenum dependency should keep name from stable ('indy-plenum')
 # adjust metadata file for new version scheme
 # changelog: add and start to manage;;;","12/Feb/19 11:21 PM;andkononykhin;(Keeping in mind the comment above)

*PoA*:
 # *jenkins shared library*
 ** packages versioning (N - source code revision number, B - jenkins build number):
 *** PyPI:
 **** master X.Y.0.devB
 **** release candidate X.Y.Z.rcN (N starts with 1)
 **** stable X.Y.Z
 *** debian:
 **** master X.Y.0~devB
 **** rc X.Y.Z~rcN
 **** stable X.Y.Z
 ** tags for master are not created (INDY-1733)
 #  *indy-plenum*
 ## improve metadata:
 *** bumps indy-plenum version to one that is higher than both current master's and stable's (1.7)
 *** adds 3rd PATCH number (1.7.0)
 *** adds pre-release part to use for master and rc to distinguish it from stable (e.g. 1.7.0.dev0)
 ## improve release nodes managements:
 *** -add CHANGELOG.md- (not needed for now)
 *** dev as top header, would receive updates related to merged changes
 ## improve CD pipeline:
 *** corresponds to changes in shared library
 # *indy-node*
 ## improve metadata:
 *** the same as for plenum, except CHANGELOG.md is presented already
 *** (plus) name for indy-plenum package in setup.py should become 'indy-plenum' for both master and stable
 ## adjust upgrade logic to support upgrade from debian repo's rc and master
 ## changes for CD pipeline
 *** corresponds to changes in shared library
 # *development releases*
 ** for any push to master CD is triggered
 ** CD pipeline:
 *** (optionally) tests
 *** bumps version to replace 0 with job build number B: X.Y.0.devB
 *** publich to PiPY as X.Y.0.devB
 *** publish to debian as X.Y.0~devB
 # *release process (indy-plenum)*
 ** similar to indy-node (see below) with only difference: no QA approval step
 # *release process (indy-node)*
 ** option 1:
 *** during release candidate preparation:
 **** need to set pre-release part for rc in metadata (X.Y.Z.rc1)
 **** need to update changelog: add header  'X.Y.Z - date' below 'dev'
 *** hotfixes (second and further release candidates) should increment N (source code release revision number) and update changelog below 'dev' header
 *** after QA approval:
 **** stable merges changes using fast-forward only strategy
 **** pre-release part is removed from metadata
 **** changes are pushed to origin
 **** new packages are published to PyPI and debian
 ** option 2:
 *** the same as option 1 but:
 **** pre-release metadata is removed at the beginning of the release process, thus:
 **** rc packages are published with rc suffixes anyway
 **** stable merges release candidate without any additional commits 
 ** option3:
 *** maintainer (have write access to GitHub repo) creates new branch release-X.Y.0 from master
 *** any contributor create a PR to that branch with the following changes:
 **** stable is merged in
 **** version is bumped to X.Y.0.rc1
 **** changelog is finalized
 *** once PR is passed CI and merged CD pipeline starts and does:
 **** (optionally) tests
 **** prepare package
 ***** adds manifest file
 **** (optionally) publish to PyPI as X.Y.0.rc1
 **** bumps version to X.Y.0
 **** does git commit with release comment
 **** publish to debian as X.Y.0~rc1
 **** wait for QA
 **** if denied:
 ***** new turn of release candidate starting with new PR to release-X.Y.0 that contains version bump X.Y.0.rcN and necessary changes required by QA
 **** if approved:
 ***** (optionally) set tag X.Y.0
 ***** push to release-X.Y.0
 ***** (merge option 1, if bot has rights) fast-forward merge to stable and push
 *** maintainer:
 **** merge option 2:
 ***** create PR to stable
 ***** merge after CI is passed
 **** merge options 3:
 ***** do merge to stable with --ff-only option without PR
 *** CD pipeline for stable is triggered and does:
 **** repack debian X.Y.0~rc.1 and publish as X.Y.0
 **** publish to PyPI
 *** PR to master for back merge
 *** hotfix release process is similar but with the following differences:
 **** release-X.Y.Z is created from stable commit X.Y.(Z-1)
 **** master is not merged into

Seems Option 3 is the best one:
 * it solves the problem with non-stable code merged in to stable: stable receives changes only after QA approval
 * it keeps the same codebase in debian artifacts between rc and stable (debian repack)
 * we can (if needed) publish to PyPI different packages for rc and stable

Drawbacks:
 * master's source code version doesn't follow the development progress (pre-release part is always '-0'), hard to understand which version of the code is installed withing python program

 ;;;","06/Mar/19 1:15 PM;andkononykhin;||Subtask||Status||
|-(indy-plenum) python and shell API for version management-|done|
|-(indy-node) python and shell API for version management-|done|
|-(indy-plenum/indy-node) abstracts for version objects with API to parse and compare different version schemes (PyPI, SemVer, Debian)-|done|
|(indy-node) make changes in upgrade logic to support new versioning scheme and operate with newly created version abstracts|code-review|
|(jenkins-shared) update scripts to use new shell / python API for version management
 * bump version using python API
 * for dev releases: bump version to set tilde and build number X.Y.0~devB
 * for rc release: bump version to set tilde X.Y.Z~rc2|in-progress (70%)|
|(jenkins-shared) split general pipeline to branches and adjust according to PoA|todo|
|-(jenkins-shared) add re-pack logic for release candidate debian artifacts-|done|
|-(testing) set up local jenkins and test GitHub repos to test new API and new release logic in a sandbox-|done|
|(testing) debug API and new release logic in a sandbox environment|in-progress|
|(testing) debug API and new release logic on the production build server|todo|;;;","15/Mar/19 6:48 PM;andkononykhin;Problem reason: 
 * versioning logic doesn't follow SemVer
 * coming release version is not 100% predictable since depends on build server specific

Changes: 


 * added abstracts version to encapsulate validation and presentation
 * separated upstream (source code) and package versioning
 * made related changes for node upgrade logic
 * tests coverage

PRs:
 * indy-plenum: 
 ** [https://github.com/hyperledger/indy-plenum/pull/1110]
 ** [https://github.com/hyperledger/indy-plenum/pull/1116|https://github.com/hyperledger/indy-plenum/pull/1110]
 ** [https://github.com/hyperledger/indy-plenum/pull/1119|https://github.com/hyperledger/indy-plenum/pull/1110]
 * indy-node
 ** https://github.com/hyperledger/indy-plenum/pull/1771


Risk factors:
- upgrade logic was impacted

Risk:

- Med

Covered with tests:
- yes

Recommendations for QA
 * new logic of versioning and build pipelines for release is not activated yet since it needs additional testing and debugging and will be done in scope of INDY-2019
 * validation here is necessary mostly to check stability of upgrade/restart logic

 ;;;",,,,,,,,,,,,,,,,,,,,,
Debug and Validation: Audit Ledger and improving catch-up to use audit ledger for consistency,INDY-1993,37651,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,13/Feb/19 9:23 PM,30/Mar/19 5:34 AM,28/Oct/23 2:48 AM,30/Mar/19 5:34 AM,,1.7.1,,,,0,,,,See details in INDY-1945,,,,,,,,,,INDY-1945,INDY-1944,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1948,,,No,,Unset,No,,,"1|hzwvif:000007i",,,,Unset,Unset,Ev-Node 19.06,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,sergey.khoroshavin,VladimirWork,,,,,,,,,,"19/Mar/19 12:15 AM;VladimirWork;PoA:

AWS QA Live Pool:

1. Production load with payments + one node big catch-up not under load. (done) (/)
{noformat}
ev@evernymr33:logs/1993_19_03_2019_prod_logs.tar.gz
ev@evernymr33:logs/1993_19_03_2019_prod_metrics.tar.gz
{noformat}

2. Production load with payments + one node big catch-up + one node small catch-up under load. (done) (x)
3rd node breaks with stacktrace (fixed in 869 master). 2, 10, 15, 20, 25 nodes (that were stopped and started during the load) don't catchup (even after service restart). 1st node was started and stopped and performs catch up successfully.
{noformat}
ev@evernymr33:logs/1993_20_03_2019_prod_catchup_under_load_logs.tar.gz
ev@evernymr33:logs/1993_20_03_2019_prod_catchup_under_load_metrics.tar.gz
{noformat}

2.2. *From 1 to 10  txn per sec* load *without* payments with pool and config txns + small catchups (1, 2, 5, 10 minutes) of a single node. (done) (!)
*Node doesn't catch up if it was off for 10 minutes under 10 txns/sec (all except payments) load.* There is a constant difference from 7k to 14k between catch-up attempts.

3. Production load with payments + forced VC + pool and config txns. (done) (!)
5th node has View 6 (and not all ledgers are catched up in this node) after the load but all other nodes have View 42.
{noformat}
ev@evernymr33:logs/1993_20_03_2019_prod_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_20_03_2019_prod_view_changes_metrics.tar.gz
{noformat}

3.2. Production load *without* payments + forced VC + pool and config txns. (done) (x)
All nodes have different amount of txns in ledgers at the end of load and there were different ViewNo on different nodes during load. During the night the most of nodes restarted and changed the ViewNo to 0. There are also lagged nodes against 872-master.
{noformat}
ev@evernymr33:logs/1993_21_03_2019_10_all_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_21_03_2019_10_all_view_changes_metrics.tar.gz

ev@evernymr33:logs/1993_22_03_2019_view_changes_logs.tar.gzes_logs.tar.gz
{noformat}

4. Production load with payments and fees + one node big catch-up + one node small catch-up under load.
5. DDOS.
6. Production load with payments and fees + forced VC + pool and config txns.
7. One, two, multiple (f changing) nodes adding (catch-up) + production load.

Docker Pool:
 1. Test catch-up for all 5 ledgers (pool, config, domain, audit, token) manually and with system test (already implemented).
 2. One, two, multiple (f changing) nodes adding (catch-up).;;;","19/Mar/19 12:44 AM;sergey.khoroshavin;*Updated version*
indy-node: 1.6.866-master
indy-plenum: 1.6.729-master

*PR*
https://github.com/hyperledger/indy-plenum/pull/1130

*Changes and reason*
New catch-up logic first catches up audit ledger and only after that pool and other ledgers. This can lead to much slower catch up if node was absent for too long and lots of new nodes joined in the meantime. So this patch changes catch-up logic to include initial pre-sync of pool ledger before starting catching up audit ledger and all other ledgers (including pool again).;;;","19/Mar/19 10:43 PM;sergey.khoroshavin;Analysis of logs from *case 1* showed that:
1. At one point nodes paused ordering and then resumed with slightly degraded performance of one of backup instances, however there were no catchups during this event and generally this is unrelated to catch-up logic
2. No node lagged behind during load, so catch-up under load apparently didn't happen
3. Node20 was manually turned off during load and turned on back later. It catched up successfully.

All in all everything looks good for now, there are no apparent regressions in catch-up logic, but more tests are needed (as per PoA).
;;;","20/Mar/19 7:14 AM;sergey.khoroshavin;*Updated version*
indy-node: 1.6.869-master
indy-plenum: 1.6.730-master

*PR*
https://github.com/hyperledger/indy-plenum/pull/1132

*Changes and reason*
Fixes crash when node receives message request for consistency proof with seqNoEnd larger than current ledger size. Problem was caused by unhandled exception raised from build_consistency_proof when replying to message requests for consistency proofs.;;;","21/Mar/19 5:46 PM;sergey.khoroshavin;*Analysis of logs*

*Case 2*
Nodes were not able to sync and continue ordering due to:
1) after long catchups (>10 minutes longs) PREPREPARES were too old to process - this is a known problem to be addressed in INDY-1983
2) due to bug in processing message requests for consistency proofs (present in load test, but fixed in 1.6.869-master) lagging nodes were crashing when other nodes started catch ups, so they were unable to finish catch up normally

*Case 3*
Node 5 went out of sync during view change and apparently had corrupted ledger. Rough timeline was:
* node 5 restarted, started catch up and caught up till (5, 1639), at this point pool reached (5, 1766)
* node 5 started new round of catch up, caught up till (5, 1771), at this point pool reached (5, 2751)
* node 5 started applying stashed 3PC messages, at the same time other nodes in pool performed quick view change to 6 and continued ordering
* node 5 ordered till  (5, 2748) and also entered view change (10 minutes late than other nodes in pool)
* node 5 ordered till (5, 2751) during view change, reverted 2 unordered batches and started catch up
* when trying to apply catch up replies node 5 found that resulting root hashes were different from what was expected and blacklisted all other nodes
* why root hashes were different is *TDB*

Also apparently some nodes were receiving large catchup requests and for some reason were unable to split reply so that message would fit into network message size limits:
{code}
2019-03-19 16:59:58,855|INFO|seeder_service.py|Node6 received catchup request: CATCHUP_REQ{'ledgerId': 0, 'catchupTill': 7110, 'seqNoStart': 26, 'seqNoEnd': 7110} from b']kGFQrntZEcKJpn2QL:3TImpiGVKH9<cSx(CS6s&'
2019-03-19 16:59:59,174|WARNING|zstack.py|CONNECTION: Cannot transmit message. Error InvalidMessageExceedingSizeException('Message len 4293000 exceeded allowed limit of 131072',)
{code}

*Case 3.2*
Many nodes went out of sync during view changes with symptoms very similar to *case 3*. Investigation is ongoing.
;;;","27/Mar/19 8:30 PM;ashcherbakov;*Test cases where the issues reproduced*
The issues are reproduced only when we do forced view change tests and write to domain, pool and config ledgers at the same time.
If we write to domain ledger only, the pool successfully survived 120 forced view changes and continues ordering. 
I think the main difference here is that when we write to multiple ledgers, we created multiple PrePrepares (for each ledger) almost at the same time which increases a risk of out-of-order messages.

*Current problems:*
1) Some nodes have ""phantom"" audit txns that other nodes don't have. Moreover, there is no corresponding main ledger txn written to the ledger for these nodes, so it looks like it's written to audit ledger only.
Moreover, there is not Ordered msg in longs corresponding to this phantom audit txn. So, it looks like it was applied, but not committed explicitly. 
2) Audit ledger tried to revert txns while it's empty (LogicError). It happens at the same time, as issue 1, so it looks like it's related.
3) There are a lot of warnings `The first created batch has not been committed or reverted and yet another batch is trying to be committed` from idr_cahce (first time it's raised for Pool ledger batch). So, it looks like idr_cahce is broken.

*Important notes*
1) Issues 1 and 2 are reproduced from a very long sequence of view changes, where it's called from the PreViewChange strategy. The specific here is that view change (and hence catchup, and hence revert of unordered batches) is called from Replica, not from Node (see node's `prod`).
It also leads to the fact that `forceProcessedOredered` ordered non-zero count of Ordered messages before starting a view change.
2) There were cases when we were processing a lot of 3PC messages in one run of replica's service method, and stabilized checkpoints at the same time without letting a chance to execute ordered batches. 
3) A lot of PrePrepares are discarded because if incorrect time (because the node was lagging behind). But the corresponding batches were eventually ordered. It looks like the time was accepted eventually because a quorum of Prepares was gathered. ;;;","27/Mar/19 8:36 PM;ashcherbakov;BTW one issue with audit ledger has been fixed:
*Problem*
If there was audit ledger catch-up (catching up more than zero txns), then multiple 3PC batches were applied (uncommitted), and then all of then needs to be reverted, then the revert of the first batch will not happen.

*Problem reason*
This is because last_committed wasn't updated in audit ledger's uncommitted batch tracker (used for apply-revert-commit logic) after catchup

*PR*
- fix: https://github.com/hyperledger/indy-plenum/pull/1138
- more tests: https://github.com/hyperledger/indy-plenum/pull/1136, https://github.com/hyperledger/indy-node/pull/1221

 ;;;","28/Mar/19 12:43 AM;VladimirWork;The last runs' results (forced VCs each 1800 seconds / domain+pool+config txns):

874-master:
{noformat}
ev@evernymr33:logs/1993_25_03_2019_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_25_03_2019_view_changes_metrics.tar.gz
{noformat}

876-master:
{noformat}
ev@evernymr33:logs/1993_27_03_2019_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_27_03_2019_view_changes_metrics.tar.gz
{noformat}

83-stable:
{noformat}
ev@evernymr33:logs/1993_28_03_2019_view_changes_logs.tar.gz
ev@evernymr33:logs/1993_28_03_2019_view_changes_metrics.tar.gz
{noformat}
;;;","28/Mar/19 7:18 PM;VladimirWork;Results summary:
We sustain production load (domain and payment txns) for 12+ hours and catch up ledgers under load successfully if nodes are disconnected less than 10 minutes. There are no performance degradation found and audit ledger behaves as expected at this load level.
All issues found during load testing with pool and config txns / forced VCs are reported as separate tickets:
INDY-2032
INDY-2034
INDY-2035;;;",,,,,,,,,,,,,,,,
Stub: Load tests with FEEs,INDY-1994,37678,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Invalid,,ashcherbakov,ashcherbakov,14/Feb/19 6:42 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,This is a stub for https://sovrin.atlassian.net/browse/ST-507,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000006r09",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),5.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debug and validation: Move the auth_map structure to the config ledger,INDY-1995,37682,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,VladimirWork,ashcherbakov,ashcherbakov,14/Feb/19 9:12 PM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,1.7.1,,,,0,,,,See INDY-1732 for details,,,,,,,,,,INDY-2002,INDY-2001,INDY-2003,,,,,,,,,,,,,,,,,,,,,,INDY-2024,,,,,,,,,,,,,,,"01/Mar/19 8:52 PM;anikitinDSR;add_rules_to_config_state.py;https://jira.hyperledger.org/secure/attachment/16887/add_rules_to_config_state.py","13/Mar/19 1:27 AM;ozheregelya;nym_perf_11032019_2.png;https://jira.hyperledger.org/secure/attachment/16917/nym_perf_11032019_2.png",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,INDY-1727,,,No,,Unset,No,,,"1|hzwvif:00000a",,,,Unset,Unset,Ev-Node 19.05,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),anikitinDSR,ashcherbakov,ozheregelya,VladimirWork,,,,,,,,,"01/Mar/19 5:29 PM;anikitinDSR;The last changes was added into indy-node 1.6.837
 Also, was added a debug logging when we use auth rule from state. Please take it into account.
recommendation for QA.

For testing INDY-2003:
 * setup pool of 4 nodes
 * check, that current rule is worked (for example for adding new steward we must be a trustee)
 * change constraint for this rule by sending AUTH_RULE txn (not implemented yet from indy-sdk side). For example, only steward can create steward
 * check, that step 2 is not worked now and only steward can create new stewards
 * check, that reject will be returned if we try to change rule not from local map
 * check, that only trustee can send AUTH_RULE txn

For testing INDY-2002 and INDY-2001:
 * setup pool of 4 nodes
 * run add_rules_to_config_state scripts for all the nodes (stops all nodes before)
 * run load_test with several type of transactions (add NYM, ATTRIB, SCHEMA, CLAIM_DEF, NODE, editing NODE txn for example)
 * check, that there is strings in logs, like ""Using auth constraint from state""

Some acceptance testing for checking that there is no any degradations or regressions:
 * setup 25 nodes pool
 * run add_rules_to_config_state on all nodes
 * run acceptance load_test;;;","07/Mar/19 11:14 PM;ozheregelya;*-Case 1:-* verified on indy-node 1.6.855
 There is no validation for ""role"" value in constraint:
{code:java}
pool(p1):wallet(w):did(V4S...e6f):indy> ledger custom {""reqId"":23,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""ROLE"",""role"":""ololo"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}},""field"":""role"",""auth_type"":""1"",""new_value"":"""",""auth_action"":""ADD""},""protocolVersion"":2} sign=true
Response: 
{""result"":{""txnMetadata"":{""seqNo"":11,""txnTime"":1551967509},""reqSignature"":{""type"":""ED25519"",""values"":[{""from"":""V4SGRU86Z58d6TV7PBUe6f"",""value"":""4k6fXFoyJF8YueKt6pc6TizgfTHQ931ryBwEVEWi8X5dMNdTdsLoJT8xTHAcxSVhwHfCzX3WxYPTn97gCEwEbwfm""}]},""auditPath"":[""DzveqQuop7uS1werAeUggpUrS4sjhr3UmKHsYtNJrjTb"",""CbvSdfmqEwaGX2nsdoETsbhbpk7pMBumjMYop8AjUN8c""],""rootHash"":""G62oxG7xeALd1Zt9BBPhypEyHxzpJpbpeTGCbN5B7GSd"",""ver"":""1"",""txn"":{""data"":{""auth_action"":""ADD"",""field"":""role"",""auth_type"":""1"",""new_value"":"""",""constraint"":{""need_to_be_owner"":false,""constraint_id"":""ROLE"",""metadata"":{},""sig_count"":1,""role"":""ololo""}},""type"":""120"",""protocolVersion"":2,""metadata"":{""digest"":""7f7d698d5ce73862ee122a213e904efc76002b7285a49b58791d4668561e871a"",""from"":""V4SGRU86Z58d6TV7PBUe6f"",""reqId"":23}}},""op"":""REPLY""}{code}
Here: ""constraint_id"":""ROLE"",{color:#ff0000}""role"":""ololo""{color}  is invalid value and transaction should not be written.

-*Case 2:*-
 -""*"" value works inconsistently for ""role"" in ""constraint"" and for ""field"":""role"",""auth_type"":""1"",""new_value"":""*"".-

-*Steps to Reproduce:*-
 -1. Send following rule (Trust Anchor can add ""*""):-
{code:java}
ledger custom {""reqId"":101,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}},""field"":""role"",""auth_type"":""1"",""new_value"":""*"",""auth_action"":""ADD""},""protocolVersion"":2} sign=true
{code}
-2. Set yourself as TRUST_ANCHOR.- 
 -3. Check which roles can be added by TRUST_ANCHOR.-

-*Actual Results:*- 
 -""*"" mean only adding of identity owner here.-
{code:java}
 
pool(p1):wallet(wallolo):did(2Qg...VrF):indy> ledger nym did=2QgL6eshMaLCaPcjvu4111
Nym request has been sent to Ledger.
Metadata:
+------------------------+-----------------+---------------------+---------------------+
| From                   | Sequence Number | Request ID          | Transaction time    |
+------------------------+-----------------+---------------------+---------------------+
| 2QgL6eshMaLCaPcjvu4VrF | 18              | 1551969878495845634 | 2019-03-07 14:44:38 |
+------------------------+-----------------+---------------------+---------------------+
Data:
+------------------------+------+
| Did                    | Role |
+------------------------+------+
| 2QgL6eshMaLCaPcjvu4111 | -    |
+------------------------+------+
pool(p1):wallet(wallolo):did(2Qg...VrF):indy> ledger nym did=2QgL6eshMaLCaPcjvu4111 role=TRUST_ANCHOR
Transaction has been rejected: Rule for this action is: 1 STEWARD signature is required OR 1 TRUSTEE signature is required
{code}
-*Expected Results:*-
 -""*"" for new_value of role field should work on the same manner as for ""role"" in constraint (i.e. should mean all possible roles including None role), or should not be a valid value if it's impossible.-

*Case 2* won't fix since it caused by ANYONE_CAN_WRITE flag and ANYONE_CAN_WRITE flag will be removed in INDY-1956;;;","11/Mar/19 5:47 PM;anikitinDSR;About ""Case: 2"". In the first step was changed rule for adding new NYM. After this txn we expect, that users with role ""101"" (TRUST_ANCHOR) can *_add_* NYM txn with anyone value for field ""role"". Then, on step 3, we create a identity owner without defining role's field and then we send a NYM txn with *_editing_* role's field. Rule for editing existing NYM wasn't changed, that  means, that transaction rejecting is an expected behaviour;;;","11/Mar/19 8:54 PM;VladimirWork;Build Info:
indy-node 1.6.846

Steps to Reproduce:
1. Try to send node txn (*to add new node*, not to demote existing one) with empty 'services' field.

Actual Results:
There is an error: client request invalid: UnauthorizedClientRequest(\""Request can not be authorized as action is not allowed.

Expected Results:
We could add new nodes with empty services field using old auth map and we should keep this behaviour.;;;","12/Mar/19 10:53 PM;ozheregelya;-Case 4:- verified on indy-node 1.6.855
 """" is invalid value for old_value field.

 
{code:java}
pool(p1):wallet(w):did(V4S...e6f):indy> ledger custom {""reqId"":11,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""0"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}},{""constraint_id"":""ROLE"",""role"":""2"",""sig_count"":1,""need_to_be_owner"":true,""metadata"":{}},{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":true,""metadata"":{}}]},""field"":""role"",""auth_type"":""1"",""auth_action"":""EDIT"",""old_value"":"""",""new_value"":""101""},""protocolVersion"":2} sign=true
Transaction has been rejected: validation error [ClientAuthRuleOperation]: empty string (old_value=)
{code}
 

"""" should be valid value (for example, for <None> role, in case of changing identity owned to any other role).;;;","13/Mar/19 1:27 AM;ozheregelya;Load test results look good (when the auth rules are used from state, there is no performance degradation in comparison with indy-node 1.6.738: pool still is able to write 33 - 35 txns/sec).

!nym_perf_11032019_2.png|thumbnail!;;;","13/Mar/19 8:18 PM;ozheregelya;Case 5:
In case of Node txn validation for STEWARD role still presents even if the rule was changed.
{code:java}
pool(p1):wallet(w):did(V4S...e6f):indy> ledger custom {""reqId"":111,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}}]},""field"":""services"",""auth_type"":""0"",""auth_action"":""EDIT"",""old_value"":""['VALIDATOR']"",""new_value"":""[]""},""protocolVersion"":2} sign=true
pool(p1):wallet(w):did(V4S...e6f):indy> ledger custom {""reqId"":111,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}}]},""field"":""services"",""auth_type"":""0"",""auth_action"":""EDIT"",""old_value"":""[]"",""new_value"":""['VALIDATOR']""},""protocolVersion"":2} sign=true
pool(p1):wallet(w):did(UhQ...WAf):indy> ledger node target=4Tn3wZMNCvhSTXPcLinQDnHyj56DTLQtL61ki4jo2Loc alias=Node5 services=
Transaction has been rejected: UhQ8PZG8muEPRYvMs9wWAf is not a steward so cannot update a node{code}
Note that the problem appears only for demote/promote of nodes. Similar case for adding node works well.;;;","14/Mar/19 6:49 PM;VladimirWork;Build Info:
indy-node 1.6.858

Steps to Validate:
1. Try to send node txn (to add new node, not to demote existing one) with empty 'services' field.

Actual Results:
Nodes with empty 'services' field are added successfully.;;;","15/Mar/19 8:46 PM;VladimirWork;Case 5:
Build Info:
indy-node 1.6.861

Actual Results:
{noformat}
pool(docker):wallet(docker):did(V4S...e6f):indy> ledger custom {""reqId"":111,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}}]},""field"":""services"",""auth_type"":""0"",""auth_action"":""EDIT"",""old_value"":""['VALIDATOR']"",""new_value"":""[]""},""protocolVersion"":2} sign=true
Response: 
{""op"":""REPLY"",""result"":{""rootHash"":""42TDiKJNQiBegte8KMeQaCAMsHk1JjvqCHfgqkHnQUPx"",""txnMetadata"":{""txnTime"":1552647677,""seqNo"":1},""ver"":""1"",""txn"":{""protocolVersion"":2,""type"":""120"",""data"":{""auth_action"":""EDIT"",""auth_type"":""0"",""constraint"":{""auth_constraints"":[{""metadata"":{},""role"":""101"",""need_to_be_owner"":false,""constraint_id"":""ROLE"",""sig_count"":1}],""constraint_id"":""OR""},""field"":""services"",""old_value"":""['VALIDATOR']"",""new_value"":""[]""},""metadata"":{""reqId"":111,""from"":""V4SGRU86Z58d6TV7PBUe6f"",""digest"":""5d8ec44a9bda929d49ae1200b6a0a110526c6de07ea0259ee50d5c821e11ccb8""}},""auditPath"":[],""reqSignature"":{""type"":""ED25519"",""values"":[{""value"":""3mFrSzSnr9ob3jrxsX2etSxpk953EnM629z2W3NeudkM2tdQZoJvWayf5QBTvzYoXfRkomrfDHcnTiVbjLBqXXzv"",""from"":""V4SGRU86Z58d6TV7PBUe6f""}]}}}
pool(docker):wallet(docker):did(V4S...e6f):indy> ledger custom {""reqId"":111,""identifier"":""V4SGRU86Z58d6TV7PBUe6f"",""operation"":{""type"":""120"",""constraint"":{""constraint_id"":""OR"",""auth_constraints"":[{""constraint_id"":""ROLE"",""role"":""101"",""sig_count"":1,""need_to_be_owner"":false,""metadata"":{}}]},""field"":""services"",""auth_type"":""0"",""auth_action"":""EDIT"",""old_value"":""[]"",""new_value"":""['VALIDATOR']""},""protocolVersion"":2} sign=true
Response: 
{""result"":{""auditPath"":[""42TDiKJNQiBegte8KMeQaCAMsHk1JjvqCHfgqkHnQUPx""],""txn"":{""data"":{""constraint"":{""auth_constraints"":[{""metadata"":{},""need_to_be_owner"":false,""constraint_id"":""ROLE"",""sig_count"":1,""role"":""101""}],""constraint_id"":""OR""},""old_value"":""[]"",""auth_type"":""0"",""auth_action"":""EDIT"",""field"":""services"",""new_value"":""['VALIDATOR']""},""protocolVersion"":2,""metadata"":{""reqId"":111,""digest"":""2c9b6753fed0ab65c6b4868b3cd7b8ab9826f9b65223177723638e8ee3ad7c6c"",""from"":""V4SGRU86Z58d6TV7PBUe6f""},""type"":""120""},""reqSignature"":{""type"":""ED25519"",""values"":[{""value"":""4oxvQ8m4q8tadAiNkqy42j8tzrTMqSRNeMAKB4He4tyh99yZyJnBmuLKxKo6inABQDJTK4U2AAeBzDzumpWXE5aq"",""from"":""V4SGRU86Z58d6TV7PBUe6f""}]},""ver"":""1"",""txnMetadata"":{""txnTime"":1552647697,""seqNo"":2},""rootHash"":""GGM8cUuqNnXT7Y6HYdgbMg8N9P592WTWfouAuX8D2EXo""},""op"":""REPLY""}
pool(docker):wallet(docker):did(V4S...e6f):indy> did use VnxoLY944SQjpctrfzWGaz
Did ""VnxoLY944SQjpctrfzWGaz"" has been set as active
pool(docker):wallet(docker):did(Vnx...Gaz):indy> ledger node target=4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe alias=Node5 services=
Transaction has been rejected: VnxoLY944SQjpctrfzWGaz is not a steward of node 4SWokCJWJc69Tn74VvLS6t2G2ucvXqM9FDMsWJjmsUxe
{noformat}

Now we have another error here: new TA is not a Steward of Node5 but we have the rule that we should not be the owner to perform node demotion (just TA role is needed).

Expected Results:
Node demotion should be successfull in this case.;;;","15/Mar/19 10:03 PM;VladimirWork;Case 5 will be fixed in scope of INDY-2024, all other cases was verified above.;;;",,,,,,,,,,,,,,,
Stable release 1.7.0,INDY-1996,37685,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,anikitinDSR,ashcherbakov,ashcherbakov,14/Feb/19 11:18 PM,30/Mar/19 5:35 AM,28/Oct/23 2:48 AM,30/Mar/19 5:35 AM,,,,,,0,,,,The next Pool upgrade (to 1.7) can be forced to avoid some issues with audit ledger.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:000007w",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Improve load script for correct statistic about payment requests,INDY-1997,37703,,Task,To Develop,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,,VladimirWork,VladimirWork,VladimirWork,15/Feb/19 7:49 PM,06/Nov/19 9:41 PM,28/Oct/23 2:48 AM,,,,,,,0,,,,There are empty 'label' and 'client_sent' fields for get requests in load script (1.0.21 version) output and field 'id' contains something like payment address so it looks like some reading payment requests don't fill these fields. This should be investigated and fixed since it causes to errors in throughput\latency calculations.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001yw969w49ii",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add all system tests to Indy CD (preparation),INDY-1998,37772,,Task,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,High,Done,VladimirWork,VladimirWork,VladimirWork,18/Feb/19 11:01 PM,30/Mar/19 5:33 AM,28/Oct/23 2:48 AM,30/Mar/19 5:33 AM,,,,,,0,,,,"Things to do:

1. Add all existing system tests that can be run on each build in pipeline to Indy CD pipeline.
2. Run system tests against 7 nodes docker pool instead of 4 nodes docker pool (if possible).
3. Use ""docker_setup_and_teardown"" fixture to run separate pool for VC and consensus tests (if possible). ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,INDY-2016,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00007xy",,,,Unset,Unset,Ev-Node 19.04,,,,,,,(Please add steps to reproduce),3.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),VladimirWork,,,,,,,,,,,,"18/Feb/19 11:05 PM;VladimirWork;FYI [~ashcherbakov] [~andkononykhin];;;","19/Feb/19 11:27 PM;VladimirWork;Jenkinsfile.cd and system tests' folder structure have been updated.;;;","21/Feb/19 11:23 PM;VladimirWork;Final system tests changes have been added.;;;","22/Feb/19 7:03 PM;VladimirWork;Final Jenkinsfile changes have been added. Need to check all changes in pipeline.;;;","01/Mar/19 12:40 AM;VladimirWork;Fixes for CD pipeline: https://github.com/hyperledger/indy-test-automation/pull/13 need to be tested in pipeline. FYI [~andkononykhin];;;","01/Mar/19 10:13 PM;VladimirWork;All necessary changes in Jenkinsfile.cd and system tests are done, but we need to fix the issue with docker-to-docker connection in scope of INDY-2016.;;;",,,,,,,,,,,,,,,,,,,
POA: Transaction authors don't need to be endorsers,INDY-1999,37774,,Story,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Done,ashcherbakov,esplinr,esplinr,18/Feb/19 11:49 PM,28/Aug/19 6:14 PM,28/Oct/23 2:48 AM,26/Jul/19 12:36 AM,,1.9.1,,,,0,,,,"*Story*
As a transaction author, I need my transactions to be written to the ledger preserving me as the author without my needing to accept the responsibilities of an endorser so that I can focus on my business. Instead, I will have a business relationship with an endorser who will endorse my transactions.

*Goals*
* It is easy to tell from the ledger who is the endorser and who is the transaction author for each transaction.
* A transaction author can use a different transaction endorser for future transactions, including updates to attribs and key rotations.
* The transaction must use the author key to sign the transaction author agreement regardless of whether an endorser signature is also needed.
* If the endorser field is included in a transaction, then the ledger will reject the transaction if it is not signed by the endorser.

*Acceptance Criteria*
* Analyze the work required.
* Raise issues for completing the work.

*Notes*
* The transaction author field is intended to be used for all future permissions.
* The endorser field only needs to be recorded in order to:
** Allow auditability, that the transaction was written with correct permissions
** Allow the network administrator to bill the endorser for the write
* Sub-endorsers or chains of endorsers would be desirable for a global network, but are not required at this time. We recognize that in the future adding multiple levels of endorsers or delegated endorsers could be a breaking change.
** Endorsers could be a list, but it would still require the transaction author to know all endorsers at the time the transaction is created. That list could be built off-ledger by sharing a draft transaction.
* Suggested user flow:
** Transaction author identifies the need for creating a transaction
** Transaction author identifies which endorser they want to use
** Transaction author includes the DID of the endorser in the transaction, and signs the transaction.
** Transaction author passes the signed transaction to the endorser
** Endorser adds their signature
** Endorser submits the multi-signed transaction to the ledger
** If the endorser will not accept the transaction, and the transaction author wants to try a different endorser, the transaction author must recreate the transaction in order to include the new endorser DID.
* This process is separate from a potential process by which payment could be attached to a third party transaction so as to allow it to be written to the ledger. That potential feature of the Sovrin Network is being tracked in [ST-608|https://sovrin.atlassian.net/browse/ST-608]. In other words, transaction endorsers and authors don't need to be tracked separately for Sovrin XFER transactions.",,,,,,,,,,,,,,,,,,,,,,,,INDY-1693,INDY-1695,INDY-1694,,,INDY-1942,IS-1339,IS-1337,,,IS-1325,INDY-2170,INDY-2199,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|i00t7a:",,,,Unset,Unset,Ev-Node 19.15,,,,,,,(Please add steps to reproduce),2.0,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,esplinr,,,,,,,,,,,"19/Jul/19 9:31 PM;ashcherbakov;A link to previous versions of the PoA:
- https://docs.google.com/document/d/1jq9_Y2qF3tW-NtVN6mHb9PeoRTMjmx6hVoIjbO_N4as/edit
- https://jira.hyperledger.org/browse/INDY-1563;;;","23/Jul/19 8:15 PM;ashcherbakov;[~esplinr]
Do we have any assumptions about whether the original transaction author needs to have a DID on the ledger? What verkey the author should use for signing?;;;","23/Jul/19 10:20 PM;ashcherbakov;Another thing we need to state explicitly: if Endorser field is added, there must be a signature from the Endorser to make sure that Endorser approves the business relationship.

Also I believe this should be fine to assume that the author has a DID on the ledger (except creation of new DIDs , INDY-2171).
 But this may lead to a problem with a plugin transaction which doesn't require any DID-based signatures because of correlation. If we support a use case that this txn may have an Endorser, then it must be signed by both original author and the endorser, so the author has to have a signature on the ledger.

Another option is if we use cryptonyms in case there is no DID on the ledger.;;;","24/Jul/19 12:16 AM;esplinr;We currently expect that writes of schemas and cred defs should only be done by entities who already have a DID on the ledger. In INDY-2170 we considered relaxing that assumption, but decided not to change that assumption at this time. If we change it in the future, we expect that this behavior would also be impacted.

Endorsing the transactions of an author is only required when the author is not providing payment. I updated the notes in the story description to clarify that payment transactions handled by ledger plugins cannot be endorsed.;;;","24/Jul/19 9:31 PM;ashcherbakov;PR with Design: https://github.com/hyperledger/indy-node/pull/1395;;;",,,,,,,,,,,,,,,,,,,,
F nodes were lagged during load test,INDY-2000,37795,,Bug,Complete,INDY,Indy Node,software,nage,The server portion of an Indy identity network.,https://github.com/hyperledger/indy-node,Medium,Won't Do,,ozheregelya,ozheregelya,19/Feb/19 11:14 PM,11/Oct/19 8:51 PM,28/Oct/23 2:48 AM,11/Oct/19 8:51 PM,,1.13.0,,,,0,,,,"*Environment:*
 indy-node 1.6.806

*Steps to Reproduce:*
 1. Setup the pool.
 2. Add following to config:
{code:java}
NODE_TO_NODE_STACK_QUOTA = 1000
NODE_TO_NODE_STACK_SIZE = 10 * 1024 * 1024{code}
3. Restart the pool.
 4. Run load test with production load.

*Actual Results:*
 Most part of the nodes wrote 376016 domain txns, but part of them were lagged:
 node3 96201
 node13 149862
 node2 67272
 node11 87478
 node21 69519
 node25 67272
 node18 87457
 node20 366421 << situation from INDY-1965 was reproduced on this node.

*Expected Results:*
Need to recognize what happened on lagged nodes (exclude Node20).

*Logs and metrics*: s3://qanodelogs/indy-2000
 To get logs, run following command on log processor machine: 
 aws s3 cp --recursive s3://qanodelogs/indy-2000/prod_load_190219/ /home/ev/logs/qanodelogs/indy-2000/prod_load_190219/
 *CSVs and graphics*:
 s3://qanodelogs/indy-2000/prod_load_190219_metrics",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,(Please add high level design or a link to the design),Unset,Unset,Unset,,,,,No,,Unset,No,,,"1|hzwvif:00001ywb",,,,Unset,Unset,,,,,,,,(Please add steps to reproduce),,Unset,,,,,,Unset,,Unset,,(Please add usage information),ashcherbakov,ozheregelya,Toktar,,,,,,,,,,"21/Feb/19 12:53 AM;Toktar;*Problem 1:*
 * Node21, Node25 - can't start ordering after view change 1 -> 2
 * Node3, Node13 - can't start ordering after view change 2 -> 3

Nodes can't process stashed messages because the first PrePrepare discards with 
{code:java}
message has incorrect state trie root; suspicion code is 21
{code}
It should be fixed after implementation of audit ledger.

But in fact, nodes make catchup one by one and don't clean the requests queue. It leads to out of memory error.  And I think it should be explored.

*Problem 2:*

Node11, Node18 stopped with ZMQ error
{code:java}
zmq.error.ZMQError: Address already in use
{code}
when count of client connections were about 460 (459, 461). Nodes stopped and didn't restart.;;;","21/Feb/19 8:08 PM;ashcherbakov;The first issue will be addressed in the scope of INDY-1945

The second issue (ZMQ) needs to be investigated further.;;;","11/Oct/19 8:51 PM;ashcherbakov;The outdated test results;;;",,,,,,,,,,,,,,,,,,,,,,
